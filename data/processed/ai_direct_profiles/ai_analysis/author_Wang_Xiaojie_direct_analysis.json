{
  "raw_data": {
    "basic_info": {
      "standardized_name": "Wang Xiaojie",
      "name_variations": [
        "Xiaojie Wang"
      ],
      "total_papers": 25,
      "document_types": {
        "academic_paper": 25
      },
      "institutions": [
        "Center for Intelligence Science and Technology, School of Computer Science, Beijing University of Posts and Telecommunications",
        "Beijing University of Posts & Telecommunications",
        "Engineering Research Center of Information Networks, Ministry of Education, Beijing 100876, China",
        "Li Auto Inc.",
        "School of Artificial Intelligence, Beijing University of Posts and Telecommunications",
        "Beijing Academy of Artificial Intelligence, Beijing, China",
        "Beijing Natural Science Foundation",
        "Beijing University of Posts and Telecommunications, Beijing, China",
        "Beijing Academy of Artificial Intelligence",
        "Beijing University of Posts and Telecommunications",
        "High Performance Computing Platform of BUPT",
        "National Nature Science Foundation of China",
        "Center for Intelligence Science and Technology, Beijing University of Posts and Telecommunications",
        "School of Artificial Intelligence, Beijing University of Posts and Telecommunications, China",
        "Engineering Research Center of Information Networks, Ministry of Education, China",
        "BUPT",
        "School of Computers, Beijing University of Posts and Telecommunications, Beijing 100876, China"
      ],
      "papers": [
        "2504.15958v2",
        "1307.1275v1",
        "2023.acl_long.802",
        "2022.acl_long.212",
        "3664647.3681466",
        "2408.03632v3",
        "3394171.3416296",
        "3664647.3680897",
        "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
        "2820400",
        "0592",
        "4930_Article_Text_7995_1_10_20190709",
        "0628",
        "2647868.2654902",
        "2021.acl_long.494",
        "2022.emnlp_main.212",
        "2022.coling_1.234",
        "2808205",
        "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining",
        "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
        "3469877.3490585",
        "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
        "2025.coling_main.22",
        "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
        "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis"
      ]
    },
    "research_content": {
      "domains": [
        "Text-to-image generation, subject-driven image generation, training-free framework",
        "Personalized content creation",
        "Image synthesis",
        "Multi-modal learning, bimodal data representations, image-tags",
        "Predictive systems for word tags",
        "Structured Sentiment Analysis",
        "Opinion analysis",
        "Sentiment classification",
        "Natural language processing",
        "Aspect-based Sentiment Analysis (ABSA), Graph Convolutional Network",
        "Sentiment analysis",
        "Natural language processing",
        "Computer vision tasks, image harmonization",
        "Digital editing",
        "Image generation and editing tasks",
        "Text-to-image synthesis with multi-concept customization",
        "Image generation",
        "AI art",
        "Virtual reality",
        "Visual feature learning for image retrieval in e-commerce",
        "E-commerce",
        "Content-based image retrieval",
        "Phrase Grounding under weak supervision",
        "Multimedia and multimodal retrieval",
        "Weakly supervised phrase grounding",
        "Image-text alignment",
        "Multimodal information processing",
        "Deep Learning for Multimedia and Emotional and Social Signals in Multimedia",
        "Image captioning",
        "Computer Vision",
        "Natural Language Processing",
        "Visual Question Answering (VQA)",
        "Human-computer interaction",
        "Image and text understanding",
        "Stock Trend Prediction in Artificial Intelligence",
        "Intelligent Investment",
        "Financial Time Series Analysis",
        "Cross-modal retrieval",
        "Web image database",
        "Information retrieval",
        "Aspect-based sentiment analysis",
        "Natural language processing",
        "Sentiment analysis",
        "Aspect Sentiment Triplet Extraction, Machine Reading Comprehension, Fine-grained Aspect-based Sentiment Analysis",
        "Aspect-based Sentiment Analysis",
        "Natural Language Processing",
        "Distantly supervised relation extraction",
        "Natural Language Processing",
        "Cross-modal retrieval, deep learning, autoencoder",
        "Multimodal data retrieval",
        "Cross-lingual Learning, Pointwise Mutual Information (PMI), Hallucination, Large Language Models (LLMs)",
        "Cross-lingual transfer learning",
        "Language model pretraining",
        "Multimodal data analysis using deep neural learning",
        "Image and text analysis",
        "Complex systems",
        "Image paragraph captioning",
        "Computer vision tasks",
        "Aspect-Based Sentiment Analysis",
        "sentiment analysis",
        "recommendation",
        "advertisement computation",
        "Multimodal Aspect-Based Sentiment Analysis (MABSA)",
        "Social media sentiment analysis",
        "Image paragraph captioning",
        "Beijing Academy of Artificial Intelligence",
        "Computer Vision",
        "Natural Language Processing",
        "Text-to-image synthesis, Generative Adversarial Networks, Multi-modal disentangled representation learning",
        "Interactive art",
        "Computer-aided drawing"
      ],
      "methods": [
        "FreeGraftor, a training-free framework that uses cross-image feature grafting",
        "Hierarchical representations of bimodal data using MPEG-7, gist descriptors, RBMs, and a quasi-Siamese auto-encoder",
        "USSA, a unified 2D table-filling scheme that utilizes 13 relation types and a bi-axial attention module",
        "Enhanced Multi-Channel Graph Convolutional Network (EMC-GCN)",
        "Harmony-VAE, inverse harmonization diffusion model",
        "Concept Conductor framework with multipath sampling, layout alignment, and concept injection",
        "Utilizing product titles as supervised signals to learn image features, constructing an image classification dataset using n-grams from product titles, fine-tuning a pre-trained model, and extracting the basic max-pooling activation of convolutions (MAC) feature.",
        "Triple alignment strategies: Region-Text Alignment (RTA), Domain Alignment (DomA), and Category Alignment (CatA)",
        "Refinement-based approach using a detector-free phrase grounding model fine-tuned with a visual prompt from CLIP text-related representations",
        "Facial emotion recognition during speech; Correspondence autoencoders for cross-modal retrieval",
        "Topic-Oriented Multi-Sentence (TOMS) captioning model",
        "Differential Networks (DN) and DN-based Fusion (DF)",
        "Multi-scale Two-way Deep Neural Network (MTDNN), using eXtreme Gradient Boosting and Recurrent Convolutional Neural Network",
        "Correspondence autoencoder (Corr-AE), Corr-Cross-AE, and Corr-Full-AE",
        "DualGCN model",
        "COntext-Masked MRC (COM-MRC) framework",
        "BERT-based Graph Convolutional network Model (BGM)",
        "Correspondence Autoencoder (Corr-AE), integrating representation learning and correlation learning into a single process",
        "InfoLoss, a novel loss function for continual pretraining",
        "Bimodal Deep Architecture (BDA)",
        "Splitting to Tree Decoder (S2TD)",
        "DualGCN, integrating SynGCN and SemGCN through a mutual BiAffine module, with orthogonal and differential regularizers.",
        "COnditional Relation based Sentiment Analysis framework (CORSA), including a conditional relation detector (CRD) and a visual object localizer (VOL)",
        "DualRel model",
        "Modality disentangled discriminator, AttnGAN, DM-GAN"
      ],
      "keywords": [
        "Text-to-image generation",
        "Subject-driven image generation",
        "Cross-image feature grafting",
        "Semantic matching",
        "Efficiency",
        "Multi-subject generation",
        "Multi-modal learning",
        "Bimodal representations",
        "Image-tags",
        "Word tags",
        "Restricted Boltzmann Machines",
        "Siamese network",
        "Structured Sentiment Analysis",
        "USSA",
        "Bi-axial attention",
        "Overlap",
        "Discontinuity",
        "Aspect Sentiment Triplet Extraction",
        "Graph Convolutional Network",
        "Linguistic Features",
        "Refining Strategy",
        "image harmonization",
        "latent diffusion model",
        "VAE",
        "data augmentation",
        "inverse harmonization",
        "stable diffusion",
        "Text-to-image synthesis",
        "Personalization",
        "Concept Conductor",
        "Attribute leakage",
        "Layout confusion",
        "Visual feature learning",
        "Bag of n-grams",
        "Image retrieval",
        "CNN",
        "MAC",
        "Phrase Grounding",
        "weak supervision",
        "zero-shot learning",
        "alignment strategies",
        "CLIP",
        "Weakly supervised phrase grounding",
        "Visual prompt tuning",
        "CLIP",
        "Detector-free",
        "Multimedia",
        "Deep Learning",
        "Emotion Recognition",
        "Speech",
        "Correspondence Autoencoders",
        "Cross-Modal Retrieval",
        "Image captioning",
        "Topic-Oriented Multi-Sentence",
        "Latent Dirichlet Allocation",
        "Fusion Gate Unit",
        "Instance Coverage",
        "Visual Question Answering",
        "Differential Networks",
        "DN-based Fusion",
        "attention distribution",
        "Stock Trend Prediction",
        "Multi-scale Analysis",
        "Deep Neural Network",
        "Wavelet Transform",
        "Downsampling",
        "XGBoost",
        "RCNN",
        "Cross-modal retrieval",
        "Correspondence autoencoder",
        "Representation learning",
        "Correlation learning",
        "aspect-based sentiment analysis",
        "dual graph convolutional networks",
        "dependency parsing",
        "semantic correlations",
        "regularizers",
        "Aspect Sentiment Triplet Extraction",
        "Machine Reading Comprehension",
        "Context Masking",
        "Sentiment Analysis",
        "Distant supervision",
        "Relation extraction",
        "BERT",
        "Graph Convolutional Network",
        "Cross-entropy loss",
        "Cross-modal",
        "retrieval",
        "image and text",
        "deep learning",
        "autoencoder",
        "Cross-lingual Learning",
        "Pointwise Mutual Information (PMI)",
        "Hallucination",
        "Large Language Models (LLMs)",
        "Multimodal data",
        "Deep neural learning",
        "Similarity metric",
        "Bimodal deep architecture",
        "image captioning",
        "paragraph generation",
        "tree-structured decoder",
        "vision and language",
        "Aspect-Based Sentiment Analysis",
        "DualGCN",
        "graph convolutional networks",
        "dependency parsing",
        "semantic information",
        "Multimodal Aspect-Based Sentiment Analysis",
        "Conditional Relation",
        "CORSA framework",
        "Visual Object Localizer",
        "Image paragraph captioning",
        "Dual Relations",
        "Spatial Relation",
        "Semantic Relation",
        "Weakly Supervised",
        "Relation-aware Attention",
        "text-to-image synthesis",
        "generative adversarial networks",
        "multi-modal disentangled representation learning"
      ],
      "innovations": [
        "Training-free cross-image feature grafting",
        "Semantic matching",
        "Position-constrained attention fusion",
        "Noise initialization strategy for geometry priors",
        "Three-level representations in three stages",
        "Bimodal auto-encoder for level-3 representations",
        "Data-specific strategy for choosing correct tag words",
        "Unified 2D table-filling scheme",
        "Bi-axial attention module",
        "Addressing overlap and discontinuity in SSA",
        "Proposed EMC-GCN to exploit word relations",
        "Defined ten relation types",
        "Incorporated linguistic features",
        "Developed refining strategy for improved triplet extraction",
        "Harmony-VAE to enhance decoded image quality",
        "Inverse harmonization model for data augmentation",
        "Introduction of the Human Harmony dataset",
        "Training-free framework",
        "Isolates sampling processes",
        "Self-attention-based spatial guidance",
        "Shape-aware masks for concept injection",
        "Using product titles to guide the learning of visual features",
        "Conversion of product titles into discrete labels for supervised learning",
        "Bag of n-grams approach",
        "Triple alignment strategies for zero-shot Phrase Grounding under Weak Supervision",
        "CLIP-based heatmap generation",
        "Region-category relations consideration",
        "Use of similarity tokens for spatial information capture",
        "Detector-free network fine-tuning",
        "Latent Dirichlet Allocation (LDA) for topic mining",
        "Fusion Gate Unit (FGU) for sentence generation",
        "Multi-label logistic regression and softmax for training",
        "Propose Differential Networks (DN) module",
        "Introduce DN-based Fusion (DF) model for VQA",
        "Two-way end-to-end model",
        "Wavelet-based and downsampling-based scale information",
        "Enhancing stock trend prediction with multi-scale information",
        "Integrates representation and correlation learning into a single process",
        "Proposes a novel loss function",
        "Extends the Corr-AE to two other correspondence models",
        "DualGCN architecture",
        "SynGCN and SemGCN modules",
        "orthogonal and differential regularizers",
        "Context augmentation strategy",
        "Discriminative model",
        "Two-stage inference method",
        "Combining a Pretrained Language Model (PLM) and a Graph Convolutional Network (GCN)",
        "Using GCN to directly learn bag representations over instances",
        "Proposed the Corr-AE",
        "Integrates representation and correlation learning",
        "Two correspondence models: Corr-Cross-AE and Corr-Full-AE",
        "Proposal of InfoLoss for continually pretraining LLMs",
        "Mitigation of hallucinations in cross-lingual transfer setting",
        "Three interconnected components",
        "Stacked restricted Boltzmann machines",
        "Variant autoencoder with predefined loss function",
        "Tree-structured decoder",
        "Top-down binary tree expansion",
        "Split module",
        "Score module",
        "Tree structure loss",
        "DualGCN model",
        "SynGCN",
        "SemGCN",
        "Mutual BiAffine module",
        "orthogonal and differential regularizers",
        "Proposed CORSA framework",
        "Conditional Relation Detector (CRD)",
        "Visual Object Localizer (VOL)",
        "Captures spatial and semantic relations among objects",
        "Weakly supervised multi-label classifier",
        "Relation-aware attention",
        "Fusion Gates",
        "Proposed modality disentangled discriminator",
        "Separate classification of content and style",
        "Enhanced text-image correlation",
        "Style transfer"
      ],
      "technical_concepts": [
        "FreeGraftor",
        "Semantic-Aware Feature Grafting",
        "Structure-Consistent Initialization",
        "Multimodal-Diffusion Transformer",
        "U-Net",
        "Transformer",
        "Stable Diffusion",
        "FLUX.1",
        "MPEG-7",
        "gist descriptors",
        "Restricted Boltzmann Machines",
        "Siamese network",
        "auto-encoder",
        "Contrastive Divergence",
        "Structured Sentiment Analysis",
        "2D table-filling scheme",
        "Bi-axial attention module",
        "Bi-lexical dependency parsing",
        "EMC-GCN",
        "Aspect-based Sentiment Analysis",
        "Graph Convolutional Operations",
        "Linguistic Features",
        "Biaffine Attention Module",
        "Latent diffusion models",
        "Harmony-VAE",
        "Inverse harmonization model",
        "Denoising Diffusion Probabilistic Models (DDPMs)",
        "Stable Diffusion",
        "Text-to-image diffusion models",
        "Multipath sampling",
        "Layout alignment",
        "Concept injection",
        "VAE",
        "Image representations",
        "Visual content-based indexing and retrieval",
        "CNN",
        "MAC",
        "SPoC",
        "RMAC",
        "RAMAC",
        "MS-RMAC",
        "GRMAC",
        "Region-Text Alignment",
        "Domain Alignment",
        "Category Alignment",
        "Contrastive Language-Image Pre-Training (CLIP)",
        "heatmap generation",
        "CLIP",
        "Grounding model",
        "Similarity tokens",
        "Cosine similarity",
        "VGG16",
        "Facial emotion recognition",
        "Correspondence autoencoders",
        "Cross-modal retrieval",
        "LSTM",
        "FGU",
        "LDA",
        "Multi-label logistic regression",
        "Softmax",
        "BELU",
        "METEOR",
        "ROUGE L",
        "CIDEr",
        "Instance Coverage",
        "Differential Networks (DN)",
        "DN-based Fusion (DF)",
        "pair-wise feature elements",
        "attention-based models",
        "Neural Networks",
        "Support Vector Machine",
        "Random Forest",
        "DWT",
        "CNN",
        "GRU",
        "Cross-entropy Loss",
        "Autoencoders",
        "Correlation measure",
        "Loss function",
        "Deep architecture",
        "Canonical correlation analysis",
        "Graph Convolutional Networks",
        "BiLSTM",
        "BERT",
        "dependency probability matrix",
        "self-attention mechanism",
        "COntext-Masked MRC",
        "Aspect Extraction",
        "Opinion Extraction",
        "Sentiment Classification",
        "Aspect Detection",
        "BERT-based PLM",
        "Graph Convolutional Network (GCN)",
        "Bag representation",
        "Cross-entropy loss",
        "Gradient descent optimization",
        "Autoencoders",
        "Correlation learning",
        "Multimodal reconstruction",
        "CCA",
        "InfoLoss",
        "Cross-entropy loss",
        "Pointwise Mutual Information (PMI)",
        "Hallucinations",
        "Large Language Models (LLMs)",
        "Multimodal data",
        "Deep neural networks",
        "Restricted Boltzmann machines",
        "Autoencoder",
        "Similarity measure",
        "Computer vision tasks",
        "Split module",
        "Score module",
        "Word-level RNN",
        "Tree structure loss",
        "Cosine similarity",
        "LSTM",
        "Highway Network",
        "Sentence-BERT",
        "graph neural networks",
        "GCNs",
        "GATs",
        "syntax structures",
        "semantic correlations",
        "dependency trees",
        "Multimodal Aspect Term Extraction",
        "Multimodal Aspect-oriented Sentiment Classification",
        "Joint Multimodal Aspects of Sentiment Analysis",
        "UNINEXT",
        "YOLOv8",
        "DualRel model",
        "Faster R-CNN",
        "Relation Embedding Module",
        "Relation-aware Interaction Module",
        "Self-critical sequence training",
        "Generative Adversarial Networks (GANs)",
        "AttnGAN",
        "DM-GAN",
        "Inception Score (IS)",
        "Fréchet Inception Distance (FID)",
        "Modality disentangled representation"
      ]
    },
    "detailed_innovations": {
      "contributions_by_paper": {
        "2504.15958v2": [
          "training-free framework",
          "cross-image feature grafting",
          "structure-consistent initialization"
        ],
        "2023.acl_long.802": [
          "a bi-lexical dependency parsing graph converted to a unified 2D table filling scheme (USSA)",
          "an effective model that collaborates with USSA, utilizing the bi-axial attention module",
          "extensive experimental validation on benchmark datasets"
        ],
        "2022.acl_long.212": [
          "a novel EMC-GCN model",
          "a comprehensive exploitation of linguistic features",
          "an effective refining strategy"
        ],
        "3664647.3681466": [
          "Introduction of the Human Harmony dataset",
          "Harmony-VAE's effectiveness in enhancing the VAE component in latent diffusion models for image harmonization"
        ],
        "2408.03632v3": [
          "We introduce Concept Conductor, a training-free framework that ensures visual fidelity and correct layout in multi-concept customization."
        ],
        "3394171.3416296": [
          "method for learning product visual representations",
          "deep CNNs pre-trained and fine-tuned on this dataset can enhance feature effectiveness for product image retrieval"
        ],
        "3664647.3680897": [
          "提出了一种零样本短语接地的新框架",
          "引入了三重对齐策略"
        ],
        "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding": [
          "use of similarity tokens for spatial information capture",
          "detector-free network fine-tuning"
        ],
        "0592": [
          "a novel topic-oriented captioning model",
          "the FGU design",
          "extensive experimental evaluation"
        ],
        "4930_Article_Text_7995_1_10_20190709": [
          "We propose a general DN module and a new DF model for the VQA task."
        ],
        "0628": [
          "Proposing a Multi-scale Two-way Deep Neural Network (MTDNN) for stock trend prediction"
        ],
        "2647868.2654902": [
          "We propose the correspondence autoencoder (Corr-AE) based on two basic unimodal autoencoders.",
          "The Corr-AE integrates representation and correlation learning into a single process.",
          "A novel loss function is designed, incorporating the losses of autoencoders for all modalities and the loss of correlation between modalities."
        ],
        "2021.acl_long.494": [
          "提出了一种结合语法和语义特征的DualGCN模型",
          "设计了正交和微分正则化器以增强模型捕获语义关联的能力"
        ],
        "2022.emnlp_main.212": [
          "propose a COntext-Masked MRC (COM-MRC) framework for ASTE tasks",
          "alleviate interference in ASTE tasks",
          "the context augmentation strategy effectively expanding the training corpus"
        ],
        "2022.coling_1.234": [
          "提出了一种结合预训练语言模型和图卷积网络的简单模型"
        ],
        "2808205": [
          "We propose the correspondence autoencoder (Corr-AE), which integrates representation learning and correlation learning into a single process."
        ],
        "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining": [
          "the proposal of InfoLoss for continually pretraining LLMs",
          "the first attempt to mitigate hallucinations in a cross-lingual transfer setting",
          "extensive experiments on twelve benchmarks"
        ],
        "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture": [
          "proposes the BDA to measure similarity in multimodal systems",
          "combines feature extraction and deep neural networks",
          "demonstrates effectiveness in classifying image tags"
        ],
        "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis": [
          "proposal of DualGCN architecture",
          "integration of syntactic knowledge through SynGCN and semantic information through SemGCN",
          "orthogonal and differential regularizers"
        ],
        "2025.coling_main.22": [
          "propose the CORSA framework",
          "address the impact of irrelevant images",
          "localize condition-related visual regions"
        ],
        "Improving_Image_Paragraph_Captioning_with_Dual_Relations": [
          "提出DualRel模型，明确捕捉空间和语义关系以改进图像段落字幕生成"
        ],
        "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis": [
          "The proposed discriminator is the first to learn modality disentangled representation for text-to-image synthesis, enhancing discrimination of image-text correlation and facilitating image synthesis manipulation."
        ]
      },
      "novelty_claims_by_paper": {
        "2022.acl_long.212": [
          "Our proposed framework is EMC-GCN, which addresses Aspect and Opinion Term Co-Extraction (AOTE) and Aspect-Sentiment Pair Extraction (ASPE)",
          "We define ten types of relations between words for the ASTE task"
        ],
        "3394171.3416296": [
          "using product titles to guide the learning of visual features",
          "conversion of product titles into discrete labels for supervised learning"
        ],
        "3664647.3680897": [
          "在弱监督下实现零样本接地",
          "使用CLIP进行区域文本对齐"
        ],
        "0592": [
          "Our TOMS model differs by generating sentences from topics of interest, capturing linguistic distinctions in image descriptions.",
          "The Fusion Gate Unit (FGU) fuses three sources of representations: image, context, and topic."
        ],
        "4930_Article_Text_7995_1_10_20190709": [
          "原文明确声明的新颖性，如无则为空数组"
        ],
        "0628": [
          "Using wavelet-based and downsampling-based scale information",
          "Achieving state-of-the-art performance on FI-2010 and CSI-2016 datasets"
        ],
        "2021.acl_long.494": [
          "DualGCN模型的结构设计",
          "正交和微分正则化器的应用"
        ],
        "2022.emnlp_main.212": [
          "COM-MRC’s components work collaboratively",
          "the two-stage inference method reduces interference from other aspects"
        ],
        "2022.coling_1.234": [
          "首次使用GCN直接学习实例之间的包表示"
        ],
        "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture": [
          "the BDA's three components are designed to extract features, stack RBMs, and use a variant autoencoder for discriminative learning"
        ],
        "3469877.3490585": [
          "We propose Splitting to Tree Decoder (S2TD), a novel tree-structured decoder that models paragraph decoding as top-down binary tree expansion."
        ],
        "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis": [
          "DualGCN architecture",
          "orthogonal and differential regularizers"
        ],
        "Improving_Image_Paragraph_Captioning_with_Dual_Relations": [
          "DualRel模型考虑了特定的语义和空间关系，并使用关系感知交互"
        ]
      },
      "technical_relationships": {
        "2504.15958v2": {
          "base_methods": [
            {
              "method_name": "FLUX.1",
              "relationship_type": "基于",
              "evidence_text": "We build upon FLUX.1, where joint attention is computed as:"
            }
          ],
          "compared_methods": [
            {
              "method_name": "FreeCustom",
              "comparison_result": "FreeCustom suffers from severe attribute confusion",
              "evidence_text": "FreeCustom suffers from severe attribute confusion, while MS-Diffusion and OmniGen lack visual faithfulness for small objects."
            },
            {
              "method_name": "MS-Diffusion",
              "comparison_result": "MS-Diffusion and OmniGen lack visual faithfulness for small objects",
              "evidence_text": "FreeCustom suffers from severe attribute confusion, while MS-Diffusion and OmniGen lack visual faithfulness for small objects."
            },
            {
              "method_name": "OmniGen",
              "comparison_result": "OmniGen lacks visual faithfulness for small objects",
              "evidence_text": "FreeCustom suffers from severe attribute confusion, while MS-Diffusion and OmniGen lack visual faithfulness for small objects."
            }
          ]
        },
        "2023.acl_long.802": {
          "base_methods": [
            {
              "method_name": "bi-lexical dependency parsing",
              "relationship_type": "改进",
              "evidence_text": "We propose USSA, a unified 2D table-filling scheme that utilizes 13 relation types."
            }
          ],
          "compared_methods": [
            {
              "method_name": "RACL-BERT, Head-first, Head-final, Frozen PERIN, TGLS",
              "comparison_result": "USSA generally outperforms other baselines",
              "evidence_text": "Table 4 shows that USSA generally outperforms other baselines in terms of Span F1."
            }
          ]
        },
        "2022.acl_long.212": {
          "base_methods": [
            {
              "method_name": "Graph Convolutional Network (GCN)",
              "relationship_type": "扩展",
              "evidence_text": "Motivated by CNN, GCN is an efficient variant operating on graphs (Kipf and Welling, 2017)."
            }
          ],
          "compared_methods": [
            {
              "method_name": "GTS-BERT",
              "comparison_result": "Our EMC-GCN significantly surpasses GTS-BERT with an average of 1.96% and 2.61% F1-score on D1 and D2, respectively",
              "evidence_text": "Under the F1 metric, our EMC-GCN model outperforms all pipeline, end-to-end, and MRC-based methods on two groups of datasets."
            },
            {
              "method_name": "BMRC",
              "comparison_result": "The term 'light' is challenging to identify by GTS-BERT and BMRC, yet 'easy' is predicted correctly by all methods",
              "evidence_text": "The term 'light' is challenging to identify by GTS-BERT and BMRC, yet 'easy' is predicted correctly by all methods due to its closer proximity to 'transport' than 'light'."
            }
          ]
        },
        "3664647.3681466": {
          "base_methods": [
            {
              "method_name": "Stable Diffusion",
              "relationship_type": "基于",
              "evidence_text": "Harmony-VAE is a method designed for image harmonization, which includes repairing the content of the foreground area while maintaining a harmonized appearance. It builds upon the Stable Diffusion model"
            }
          ],
          "compared_methods": [
            {
              "method_name": "HDNet",
              "comparison_result": "DiffHarmony++ shows superior performance as the foreground proportion increases.",
              "evidence_text": "On the iHarmony4 dataset, HDNet512 outperforms DiffHarmony++ in samples with small foreground proportions (0% ∼5%), but DiffHarmony++ shows superior performance as the foreground proportion increases."
            }
          ]
        },
        "2408.03632v3": {
          "base_methods": [
            {
              "method_name": "Stable Diffusion",
              "relationship_type": "改进",
              "evidence_text": "Our method is implemented on Stable Diffusion v1.5, utilizing layout references from SDXL."
            },
            {
              "method_name": "ED-LoRA",
              "relationship_type": "结合",
              "evidence_text": "ED-LoRA, a method for single-concept customization, is used as our default single-concept model, involving learnable hierarchical text embeddings and low-rank adaptation (LoRA) applied to pre-trained weights."
            }
          ],
          "compared_methods": [
            {
              "method_name": "Custom Diffusion",
              "comparison_result": "原文无此信息",
              "evidence_text": "We compare our method against three baselines: Custom Diffusion, Cones 2, and Mix-of-Show, using their official code implementations."
            },
            {
              "method_name": "Cones 2",
              "comparison_result": "原文无此信息",
              "evidence_text": "We compare our method against three baselines: Custom Diffusion, Cones 2, and Mix-of-Show, using their official code implementations."
            },
            {
              "method_name": "Mix-of-Show",
              "comparison_result": "原文无此信息",
              "evidence_text": "We compare our method against three baselines: Custom Diffusion, Cones 2, and Mix-of-Show, using their official code implementations."
            }
          ]
        },
        "3394171.3416296": {
          "base_methods": [
            {
              "method_name": "Deep Convolutional Neural Networks (CNNs)",
              "relationship_type": "基于",
              "evidence_text": "Our method, detailed in Figure 2, addresses this by leveraging the titles of product images."
            }
          ],
          "compared_methods": [
            {
              "method_name": "SEResnet-152 and Densenet-201",
              "comparison_result": "The deepest model, Densenet-201, shows the best performance among the ImageNet-1k trained models, while the shallowest model, ResNet-50, performs the worst",
              "evidence_text": "Table 1 presents the results of five deep CNN models with four different pooling methods on the validation set."
            }
          ]
        },
        "3664647.3680897": {
          "base_methods": [
            {
              "method_name": "CLIP",
              "relationship_type": "基于",
              "evidence_text": "Our framework uses a novel PG framework using triple alignment strategies under weak supervision: 1) Region-Text Alignment (RTA) to associate region-level attributes based on Contrastive Language-Image Pre-Training (CLIP)."
            }
          ],
          "compared_methods": [
            {
              "method_name": "WWbl",
              "comparison_result": "Our CLIP-based heatmap surpasses the pseudo label used by WWbl, explaining a 9% increase in bbox accuracy.",
              "evidence_text": "Our CLIP-based heatmap surpasses the pseudo label used by WWbl, explaining a 9% increase in bbox accuracy."
            },
            {
              "method_name": "ZSGNet",
              "comparison_result": "Our approach exceeds previous methods and will explore interpretable solutions for grounding-related tasks.",
              "evidence_text": "Our approach exceeds previous methods and will explore interpretable solutions for grounding-related tasks."
            }
          ]
        },
        "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding": {
          "base_methods": [
            {
              "method_name": "Weakly-Supervised Grounding (WSG)",
              "relationship_type": "改进",
              "evidence_text": "We propose a refinement-based approach using a detector-free phrase grounding model fine-tuned with a visual prompt from CLIP text-related representations."
            },
            {
              "method_name": "CLIP",
              "relationship_type": "结合",
              "evidence_text": "Our approach leverages the relationship between CLIP and the grounding model, refining training through visual prompt tuning."
            }
          ],
          "compared_methods": [
            {
              "method_name": "WWbl",
              "comparison_result": "Our method shows improvements in metrics on Flickr30K and ReferIt",
              "evidence_text": "Our method is trained on COCO and VG train splits and evaluated on the test splits of Flickr30K, VG, and ReferIt. The performance of our method is compared with state-of-the-art DF-WSG methods quantitatively and qualitatively."
            },
            {
              "method_name": "Gbs",
              "comparison_result": "Our approach not only surpasses detector-free methods like Gbs and WWbl",
              "evidence_text": "Our approach not only surpasses detector-free methods like Gbs and WWbl but also outperforms detector-based methods on almost all categories for Flickr30K Entities."
            }
          ]
        },
        "0592": {
          "base_methods": [
            {
              "method_name": "Single-sentence (SS) captioning models",
              "relationship_type": "基于",
              "evidence_text": "Single-sentence (SS) captioning models have been developed using templates and neural network approaches, but they often provide incomplete descriptions."
            },
            {
              "method_name": "MS captioning methods",
              "relationship_type": "基于",
              "evidence_text": "MS captioning methods generate multiple sentences for a complete depiction, with some focusing on regions of interest."
            }
          ],
          "compared_methods": [
            {
              "method_name": "NIC",
              "comparison_result": "Our TOMS demonstrates improved performance, especially in terms of IC.",
              "evidence_text": "Our proposed Topic-Oriented Multi-Sentence (TOMS) captioning model incorporates topic embedding to guide the generation process... Our TOMS demonstrates improved performance, especially in terms of IC."
            },
            {
              "method_name": "ATT-FCN",
              "comparison_result": "原文无此信息",
              "evidence_text": "We compare our TOMS model with NIC and ATT-FCN for sentence level MS captioning"
            },
            {
              "method_name": "RTT-GAN",
              "comparison_result": "原文无此信息",
              "evidence_text": "We compare our TOMS model with NIC and ATT-FCN for sentence level MS captioning and with RTT-GAN, Regions-Hierarchical, and Sentence-Concat for paragraph level MS captioning."
            },
            {
              "method_name": "Regions-Hierarchical",
              "comparison_result": "原文无此信息",
              "evidence_text": "We compare our TOMS model with NIC and ATT-FCN for sentence level MS captioning and with RTT-GAN, Regions-Hierarchical, and Sentence-Concat for paragraph level MS captioning."
            },
            {
              "method_name": "Sentence-Concat",
              "comparison_result": "原文无此信息",
              "evidence_text": "We compare our TOMS model with NIC and ATT-FCN for sentence level MS captioning and with RTT-GAN, Regions-Hierarchical, and Sentence-Concat for paragraph level MS captioning."
            }
          ]
        },
        "4930_Article_Text_7995_1_10_20190709": {
          "base_methods": [
            {
              "method_name": "Attention-based models",
              "relationship_type": "基于",
              "evidence_text": "We review current VQA models, focusing on attention-based models and fusion strategies."
            }
          ],
          "compared_methods": [
            {
              "method_name": "HighOrderAtt",
              "comparison_result": "原文无此信息",
              "evidence_text": "HighOrderAtt (Schwartz, Schwing, and Hazan 2017) - - - - 69.4"
            },
            {
              "method_name": "MLB(7)",
              "comparison_result": "原文无此信息",
              "evidence_text": "MLB(7) (Kim et al. 2017) 66.77 84.54 39.21 57.81"
            },
            {
              "method_name": "Mutan(5)",
              "comparison_result": "原文无此信息",
              "evidence_text": "Mutan(5) (Ben-younes et al. 2017) 67.42 85.14 39.81 58.52"
            },
            {
              "method_name": "DualMFA",
              "comparison_result": "原文无此信息",
              "evidence_text": "DualMFA (Lu et al. 2018) 66.01 83.59 40.18 56.84 70.04"
            },
            {
              "method_name": "ReasonNet",
              "comparison_result": "原文无此信息",
              "evidence_text": "ReasonNet (Ilievski and Feng 2017) - - - - -"
            }
          ]
        },
        "0628": {
          "base_methods": [
            {
              "method_name": "Support Vector Machine and Neural Networks",
              "relationship_type": "基于",
              "evidence_text": "STP is a classification task traditionally tackled by Support Vector Machine and Neural Networks."
            },
            {
              "method_name": "Ensemble-based methods like Random Forest and deep learning models",
              "relationship_type": "基于",
              "evidence_text": "Ensemble-based methods like Random Forest and deep learning models have also been explored."
            }
          ],
          "compared_methods": [
            {
              "method_name": "XGBoost",
              "comparison_result": "原文无此信息",
              "evidence_text": "The data is treated as a non-stationary discrete signal and decomposed using DWT to obtain transformed multi-scale components. These are concatenated and input to an XGBoost model to ensemble multi-scale information and output category scores."
            },
            {
              "method_name": "CNN",
              "comparison_result": "原文无此信息",
              "evidence_text": "A key operation concatenates these features, and a GRU unit temporally cascades the information to output categories."
            },
            {
              "method_name": "RNN",
              "comparison_result": "原文无此信息",
              "evidence_text": "A key operation concatenates these features, and a GRU unit temporally cascades the information to output categories."
            },
            {
              "method_name": "RCNN",
              "comparison_result": "RCNN shows the best performance on most indices",
              "evidence_text": "In the multi-scale rows, variations are fed with DWT-based, downsampling-based, and CNN multi-kernel size scale-information. RCNN shows the best performance on most indices, while XGBoost is less effective."
            }
          ]
        },
        "2647868.2654902": {
          "base_methods": [
            {
              "method_name": "Correspondence LDA, deep autoencoders, deep belief networks, and deep Boltzmann Machines",
              "relationship_type": "基于",
              "evidence_text": "Shared layer models include topic models like Correspondence LDA and deep architectures such as deep autoencoders, deep belief networks, and deep Boltzmann Machines."
            }
          ],
          "compared_methods": [
            {
              "method_name": "CCA-based models and multi-modal models",
              "comparison_result": "Our correspondence autoencoders significantly outperform other models",
              "evidence_text": "Our correspondence autoencoders significantly outperform other models on both retrieval tasks across Wikipedia, Pascal, and NUS-WIDE-10k data sets."
            }
          ]
        },
        "2021.acl_long.494": {
          "base_methods": [
            {
              "method_name": "Graph Convolutional Network (GCN)",
              "relationship_type": "改进",
              "evidence_text": "We propose a GCN-based method that combines syntactic and semantic features."
            },
            {
              "method_name": "Attention-based LSTM",
              "relationship_type": "改进",
              "evidence_text": "Attention-based neural networks, including those proposed by Wang et al. (2016) and others, have been introduced to implicitly model the semantic relation between aspects and their context."
            },
            {
              "method_name": "Recursive neural network by Dong et al. (2014)",
              "relationship_type": "改进",
              "evidence_text": "Notable works include the recursive neural network by Dong et al. (2014)"
            }
          ],
          "compared_methods": [
            {
              "method_name": "ATAE-LSTM, IAN, RAM, MGAN, TNet, ASGCN, CDT, BiGCN, kumaGCN, InterGCN, R-GAT, DGEDT, BERT, R-GAT+BERT, DGEDT+BERT",
              "comparison_result": "Our DualGCN model consistently outperforms attention-based and syntax-based methods",
              "evidence_text": "We compare DualGCN with state-of-the-art baselines, including ATAE-LSTM, IAN, RAM, MGAN, TNet, ASGCN, CDT, BiGCN, kumaGCN, InterGCN, R-GAT, DGEDT, BERT, R-GAT+BERT, and DGEDT+BERT."
            }
          ]
        },
        "2022.emnlp_main.212": {
          "base_methods": [
            {
              "method_name": "Aspect Sentiment Triplet Extraction (ASTE)",
              "relationship_type": "基于",
              "evidence_text": "Aspect Sentiment Triplet Extraction (ASTE) extracts sentiment triplets from sentences and is formalized as an effective machine reading comprehension (MRC) framework."
            }
          ],
          "compared_methods": [
            {
              "method_name": "BMRC",
              "comparison_result": "Our COM-MRC outperforms other baselines in terms of Precision, Recall, and F1 scores on both D1 and D2 datasets",
              "evidence_text": "Our COM-MRC outperforms other baselines in terms of Precision, Recall, and F1 scores on both D1 and D2 datasets, as shown in Tables 3 and 4."
            },
            {
              "method_name": "EMC-GCN",
              "comparison_result": "Our COM-MRC outperforms EMC-GCN",
              "evidence_text": "Our COM-MRC outperforms EMC-GCN on datasets D1 and D2."
            }
          ]
        },
        "2022.coling_1.234": {
          "base_methods": [
            {
              "method_name": "BERT-based Graph Convolutional network Model (BGM)",
              "relationship_type": "结合",
              "evidence_text": "We introduce BGM, a model that combines a Pretrained Language Model (PLM) and a Graph Convolutional Network (GCN)"
            },
            {
              "method_name": "PCNN-based methods",
              "relationship_type": "基于",
              "evidence_text": "DS-RE methods can be categorized into PCNN-based and PLMs-based approaches"
            },
            {
              "method_name": "PLMs-based methods",
              "relationship_type": "基于",
              "evidence_text": "PCNN-based methods use various techniques to acquire effective bag representations, while PLMs-based methods have shown remarkable performance"
            }
          ],
          "compared_methods": [
            {
              "method_name": "multiple baseline methods",
              "comparison_result": "report better performance",
              "evidence_text": "We compare our BGM with multiple baseline methods and report better performance"
            },
            {
              "method_name": "BGM without GCN",
              "comparison_result": "performance drop",
              "evidence_text": "In the ablation study, we find that removing GCN results in a performance drop"
            },
            {
              "method_name": "BGM without EntCon",
              "comparison_result": "performance drop",
              "evidence_text": "In the ablation study, we find that removing entity connections results in a performance drop"
            }
          ]
        },
        "2808205": {
          "base_methods": [
            {
              "method_name": "autoencoder",
              "relationship_type": "改进",
              "evidence_text": "We propose several novel models based on different autoencoders, which correlate hidden representations of a pair of autoencoders."
            },
            {
              "method_name": "Canonical Correlation Analysis (CCA)",
              "relationship_type": "结合",
              "evidence_text": "The models are trained using a novel optimal objective that minimizes a linear combination of representation learning errors and correlation learning error."
            }
          ],
          "compared_methods": [
            {
              "method_name": "CCA-based models",
              "comparison_result": "Our models achieve substantial improvements in mAP scores.",
              "evidence_text": "Compared to CCA-AE, our Corr-AE enhances the average mAP by 53.6%, 81.5%, and 48.3% on the respective datasets."
            },
            {
              "method_name": "Bimodal AE, Bimodal DBN",
              "comparison_result": "Our Corr-AE also achieves notable improvements over Bimodal AE and Bimodal DBN.",
              "evidence_text": "Our Corr-AE also achieves notable improvements over Bimodal AE and Bimodal DBN."
            }
          ]
        },
        "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining": {
          "base_methods": [
            {
              "method_name": "Cross-lingual Continual Pretraining",
              "relationship_type": "扩展",
              "evidence_text": "Cross-Lingual Continual Pretraining addresses the limitation of LLMs [1, 2, 3, 4] in languages other than English."
            }
          ],
          "compared_methods": [
            {
              "method_name": "traditional cross-entropy loss",
              "comparison_result": "InfoLoss aims to avoid learning incorrect language distributions caused by noisy tokens",
              "evidence_text": "We compare our method, InfoLoss, with the traditional cross-entropy loss for training Llama 2."
            }
          ]
        },
        "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture": {
          "base_methods": [
            {
              "method_name": "deep neural learning",
              "relationship_type": "基于",
              "evidence_text": "Deep neural learning, inspired by biological propagation phenomena in the human brain, has seen rapid development since 2006 and has achieved success in tasks involving single modal data."
            },
            {
              "method_name": "topic models, joint models, undirected Markov random fields",
              "relationship_type": "扩展",
              "evidence_text": "Various approaches have been proposed for learning from cross-modal data. These include extensions of topic models, joint models, and undirected Markov random fields, but these single-hidden-layer models struggle with the complexity of images and text."
            }
          ],
          "compared_methods": [
            {
              "method_name": "Multilayer Perceptrons (MLP)",
              "comparison_result": "Our deep neural architecture outperforms an MLP-based system",
              "evidence_text": "Our deep neural architecture outperforms an MLP-based system with two hidden layers and a CCA-based system with two RBMs, achieving an accuracy of 88.96%."
            },
            {
              "method_name": "Canonical Correlation Analysis (CCA)",
              "comparison_result": "The CCA-based system performs better than the MLP-based system",
              "evidence_text": "The CCA-based system performs better than the MLP-based system."
            }
          ]
        },
        "3469877.3490585": {
          "base_methods": [
            {
              "method_name": "hierarchical and non-hierarchical decoders",
              "relationship_type": "基于",
              "evidence_text": "Existing approaches, such as hierarchical and non-hierarchical decoders, have limitations in managing visual observations and maintaining paragraph coherence."
            }
          ],
          "compared_methods": [
            {
              "method_name": "DAM-ATT, SCST, SCST+RP, CRL, OR-ATT, OR-ATT+RP",
              "comparison_result": "Our S2TD achieves competitive performance, especially in terms of BLEU-1 and CIDEr.",
              "evidence_text": "Performance evaluation compares S2TD with state-of-the-art sequential decoding methods, grouped into hierarchical and non-hierarchical methods."
            }
          ]
        },
        "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis": {
          "base_methods": [
            {
              "method_name": "GCN",
              "relationship_type": "改进",
              "evidence_text": "Incorporating syntactic dependency structure with graph convolutional networks (GCNs) has shown advantages, but performance depends on dependency parsers."
            }
          ],
          "compared_methods": [
            {
              "method_name": "ATAE-LSTM",
              "comparison_result": "DualGCN model consistently outperforms attention-based and syntax-based methods",
              "evidence_text": "The DualGCN model consistently outperforms attention-based and syntax-based methods on Restaurant14, Laptop14 and Twitter datasets."
            },
            {
              "method_name": "GCAE",
              "comparison_result": "DualGCN model outperforms",
              "evidence_text": "DualGCN model outperforms SynGCN-head on the Restaurant14 and Laptop14 datasets"
            },
            {
              "method_name": "R-GAT + BERT",
              "comparison_result": "DualGCN + BERT and DualGCN + BERT-PT show improved performance over the basic DualGCN model and other BERT-based methods",
              "evidence_text": "DualGCN + BERT and DualGCN + BERT-PT show improved performance over the basic DualGCN model and other BERT-based methods."
            }
          ]
        },
        "2025.coling_main.22": {
          "base_methods": [
            {
              "method_name": "Traditional methods",
              "relationship_type": "基于",
              "evidence_text": "Traditional methods assume the image contains objects referred to by the aspects, which is not always true."
            }
          ],
          "compared_methods": [
            {
              "method_name": "ChatGPT 3.5, Llama 2, VisualGLM-6B, Llava 1.5, MMICL, mPLUG-Owl2, GPT4V",
              "comparison_result": "our model outperforms",
              "evidence_text": "Comparative experiments with large language models (LLMs) and multi-modal LLMs (MLLMs) on the JMASA and MASC tasks demonstrate that our model outperforms ChatGPT 3.5, Llama 2, VisualGLM-6B, Llava 1.5, MMICL, mPLUG-Owl2, and GPT4V, showcasing the advantage of multimodal input and the effectiveness of our approach."
            }
          ]
        },
        "Improving_Image_Paragraph_Captioning_with_Dual_Relations": {
          "base_methods": [
            {
              "method_name": "Faster R-CNN",
              "relationship_type": "基于",
              "evidence_text": "We employ Faster R-CNN [19] to detect N objects in an image, represented as C={c1, · · · , cN}."
            }
          ],
          "compared_methods": [
            {
              "method_name": "Regions-Hierarchical",
              "comparison_result": "DualRel outperforms SCST on all metrics (except for a tie in METEOR) and the recent IMAP method.",
              "evidence_text": "Baselines: We compare DualRel with baselines including Regions-Hierarchical [1], RTT-GAN [5], DAM [7], SCST [8], DCPG-VAE [6], TMOS [27], CAE-LSTM [11], DHPV [9], CVAP [10], CRL [12], Dual-CNN [15], VREN [17], IMAP [14], S2TD [16], and OR-ATT [18]."
            },
            {
              "method_name": "SCST",
              "comparison_result": "DualRel outperforms SCST on all metrics (except for a tie in METEOR)",
              "evidence_text": "Results: Table 1 shows our DualRel method achieves the best scores in B@{1-4} and CIDEr. DualRel outperforms SCST on all metrics (except for a tie in METEOR) and the recent IMAP method."
            }
          ]
        },
        "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis": {
          "base_methods": [
            {
              "method_name": "AttnGAN",
              "relationship_type": "改进",
              "evidence_text": "The modality disentangled discriminator is integrated into two models: AttnGAN and DM-GAN."
            },
            {
              "method_name": "DM-GAN",
              "relationship_type": "改进",
              "evidence_text": "The modality disentangled discriminator is integrated into two models: AttnGAN and DM-GAN."
            }
          ],
          "compared_methods": [
            {
              "method_name": "GAN-INT-CLS",
              "comparison_result": "原文无此信息",
              "evidence_text": "In terms of style manipulation experiments on the CUB dataset, two types of experiments are conducted. The first involves style transfer, where the model uses modality-specific features for direct style transfer, achieving more accurate style reconstruction compared to GAN-INT-CLS."
            }
          ]
        }
      }
    },
    "collaboration_raw": {
      "collaborators_by_paper": {
        "2504.15958v2": [
          "Fangxiang Feng",
          "Chen Wei",
          "Xiaojie Wang",
          "Zebin Yao",
          "Huixing Jiang",
          "Ruifan Li",
          "Lei Ren"
        ],
        "1307.1275v1": [
          "Fangxiang Feng",
          "Xiaojie Wang",
          "Ruifan Li"
        ],
        "2023.acl_long.802": [
          "Zepeng Zhai",
          "Hao Chen",
          "Xiaojie Wang",
          "Ruifan Li"
        ],
        "2022.acl_long.212": [
          "Fangxiang Feng",
          "Xiaojie Wang",
          "Zepeng Zhai",
          "Hao Chen",
          "Ruifan Li"
        ],
        "3664647.3681466": [
          "Fangxiang Feng",
          "Xiaojie Wang",
          "Ruifan Li",
          "Guang Liu",
          "Pengfei Zhou"
        ],
        "2408.03632v3": [
          "Fangxiang Feng",
          "Xiaojie Wang",
          "Zebin Yao",
          "Ruifan Li"
        ],
        "3394171.3416296": [
          "Fangxiang Feng",
          "Xiaojie Wang",
          "Tianrui Niu",
          "Huixing Jiang",
          "Ruifan Li"
        ],
        "3664647.3680897": [
          "Fangxiang Feng",
          "Zhanyu Ma",
          "Xiaojie Wang",
          "Ruifei Zhang",
          "Zhihong Chen",
          "Guanbin Li",
          "Yuzhe Ji",
          "Zhihan Yu",
          "Yibing Song",
          "Pengyue Lin",
          "Ruifan Li",
          "Xiang Wan"
        ],
        "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding": [
          "Fangxiang Feng",
          "Xiaojie Wang",
          "Mingcong Lu",
          "Zhihan Yu",
          "Pengyue Lin",
          "Ruifan Li"
        ],
        "2820400": [
          "Fangxiang Feng",
          "George Toderici",
          "Xiaojie Wang",
          "Yelin Kim",
          "Emily Mower-Provost",
          "Hayley Hung",
          "Ruifan Li"
        ],
        "0592": [
          "Xiaojie Wang",
          "Ruifan Li",
          "Chang Zhou",
          "Yuzhao Mao"
        ],
        "4930_Article_Text_7995_1_10_20190709": [
          "Chenfei Wu",
          "Xiaojie Wang",
          "Ruifan Li",
          "Jinlai Liu"
        ],
        "0628": [
          "Qi Sun",
          "Xiaojie Wang",
          "Hailong Huang",
          "JianPing Shen",
          "Weiguo Gao",
          "Yuzhao Mao",
          "Guang Liu",
          "Ruifan Li",
          "Xuan Li"
        ],
        "2647868.2654902": [
          "Fangxiang Feng",
          "Xiaojie Wang",
          "Ruifan Li"
        ],
        "2021.acl_long.494": [
          "Fangxiang Feng",
          "Zhanyu Ma",
          "Xiaojie Wang",
          "Eduard Hovy",
          "Hao Chen",
          "Ruifan Li"
        ],
        "2022.emnlp_main.212": [
          "Fangxiang Feng",
          "Xiaojie Wang",
          "Zepeng Zhai",
          "Hao Chen",
          "Dongyan Zhao",
          "Feifan Fan",
          "Yansong Feng",
          "Ruifan Li"
        ],
        "2022.coling_1.234": [
          "Fangxiang Feng",
          "Xiaojie Wang",
          "Ruifan Li",
          "Ziqin Rao"
        ],
        "2808205": [
          "Fangxiang Feng",
          "XIAOJIE WANG",
          "RUIFAN LI",
          "Xiaojie Wang",
          "FANGXIANG FENG",
          "IBRAR AHMAD",
          "Ruifan Li"
        ],
        "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining": [
          "Xiaojie Wang",
          "Yuantao Fan",
          "Guangwei Zhang",
          "Chuan Shi",
          "Ruifan Li"
        ],
        "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture": [
          "Fangxiang Feng",
          "Bohan Li",
          "Xiaojie Wang",
          "Peng Lu",
          "Ruifan Li"
        ],
        "3469877.3490585": [
          "Fangxiang Feng",
          "Zhanyu Ma",
          "Yihui Shi",
          "Xiaojie Wang",
          "Yun Liu",
          "Ruifan Li"
        ],
        "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis": [
          "Fangxiang Feng",
          "Zhanyu Ma",
          "Xiaojie Wang",
          "Eduard Hovy",
          "T. Qian",
          "M. Zhang",
          "Hao Chen",
          "Ruifan Li"
        ],
        "2025.coling_main.22": [
          "Shuqin Ye",
          "Xiaojie Wang",
          "Xinjing Liu",
          "Guangwei Zhang",
          "Ruifan Li"
        ],
        "Improving_Image_Paragraph_Captioning_with_Dual_Relations": [
          "Fangxiang Feng",
          "Zhanyu Ma",
          "Yihui Shi",
          "Xiaojie Wang",
          "Yun Liu",
          "Ruifan Li"
        ],
        "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis": [
          "Fangxiang Feng",
          "Xiaojie Wang",
          "Tianrui Niu",
          "Member, IEEE",
          "Ruifan Li"
        ]
      },
      "institutional_networks": [
        "Center for Intelligence Science and Technology, School of Computer Science, Beijing University of Posts and Telecommunications",
        "Beijing University of Posts & Telecommunications",
        "Engineering Research Center of Information Networks, Ministry of Education, Beijing 100876, China",
        "Li Auto Inc.",
        "School of Artificial Intelligence, Beijing University of Posts and Telecommunications",
        "Beijing Academy of Artificial Intelligence, Beijing, China",
        "Beijing Natural Science Foundation",
        "Beijing University of Posts and Telecommunications, Beijing, China",
        "Beijing Academy of Artificial Intelligence",
        "Beijing University of Posts and Telecommunications",
        "High Performance Computing Platform of BUPT",
        "National Nature Science Foundation of China",
        "Center for Intelligence Science and Technology, Beijing University of Posts and Telecommunications",
        "School of Artificial Intelligence, Beijing University of Posts and Telecommunications, China",
        "Engineering Research Center of Information Networks, Ministry of Education, China",
        "BUPT",
        "School of Computers, Beijing University of Posts and Telecommunications, Beijing 100876, China"
      ]
    },
    "experimental_details": {
      "datasets_used": [
        {
          "paper": "2504.15958v2",
          "dataset": "DreamBench",
          "description": "原文无此信息"
        },
        {
          "paper": "2504.15958v2",
          "dataset": "CustomConcept101",
          "description": "原文无此信息"
        },
        {
          "paper": "2504.15958v2",
          "dataset": "Mix-of-Show",
          "description": "原文无此信息"
        },
        {
          "paper": "2023.acl_long.802",
          "dataset": "NoReCFine, MultiBEU, MultiBCA, MPQA, DSUnis",
          "description": "原文无此信息"
        },
        {
          "paper": "2022.acl_long.212",
          "dataset": "SemEval ABSA Challenges (Pontiki et al., 2014, 2015, 2016)",
          "description": "two ABSA datasets"
        },
        {
          "paper": "3664647.3681466",
          "dataset": "iHarmony4",
          "description": "Includes HCOCO, HFlickr, HAdobe5K, and Hday2night sub-datasets, consisting of 65,742 training and 7,404 testing pairs of composite and real images."
        },
        {
          "paper": "3664647.3681466",
          "dataset": "Human Harmony",
          "description": "Created by filtering the imaterialist-fashion-2020-fgvc7 dataset to exclude images with only products. The cleaned dataset contains 29,106 images."
        },
        {
          "paper": "2408.03632v3",
          "dataset": "Mix-of-show",
          "description": "原文无此信息"
        },
        {
          "paper": "2408.03632v3",
          "dataset": "DreamBooth",
          "description": "原文无此信息"
        },
        {
          "paper": "2408.03632v3",
          "dataset": "CustomConcept101",
          "description": "原文无此信息"
        },
        {
          "paper": "3394171.3416296",
          "dataset": "Perfect-500K",
          "description": "dataset from the 'AI Meets Beauty' challenge"
        },
        {
          "paper": "3664647.3680897",
          "dataset": "Flickr-Split-S0",
          "description": "零样本PG设置下的数据集"
        },
        {
          "paper": "3664647.3680897",
          "dataset": "Flickr-Split-S1",
          "description": "零样本PG设置下的数据集"
        },
        {
          "paper": "3664647.3680897",
          "dataset": "VG-Split-S2",
          "description": "零样本PG设置下的数据集"
        },
        {
          "paper": "3664647.3680897",
          "dataset": "VG-Split-S3",
          "description": "零样本PG设置下的数据集"
        },
        {
          "paper": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
          "dataset": "Flickr30K Entities",
          "description": "原文无此信息"
        },
        {
          "paper": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
          "dataset": "COCO",
          "description": "原文无此信息"
        },
        {
          "paper": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
          "dataset": "Visual Genome",
          "description": "原文无此信息"
        },
        {
          "paper": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
          "dataset": "ReferIt",
          "description": "原文无此信息"
        },
        {
          "paper": "0592",
          "dataset": "Flickr8k",
          "description": "原文无此信息"
        },
        {
          "paper": "0592",
          "dataset": "Flickr30k",
          "description": "原文无此信息"
        },
        {
          "paper": "0592",
          "dataset": "COCO",
          "description": "原文无此信息"
        },
        {
          "paper": "0592",
          "dataset": "paragraph dataset from Krause et al. (2017)",
          "description": "原文无此信息"
        },
        {
          "paper": "4930_Article_Text_7995_1_10_20190709",
          "dataset": "VQA 1.0",
          "description": "原文无此信息"
        },
        {
          "paper": "4930_Article_Text_7995_1_10_20190709",
          "dataset": "VQA 2.0",
          "description": "原文无此信息"
        },
        {
          "paper": "4930_Article_Text_7995_1_10_20190709",
          "dataset": "COCO-QA",
          "description": "原文无此信息"
        },
        {
          "paper": "4930_Article_Text_7995_1_10_20190709",
          "dataset": "TDIUC",
          "description": "原文无此信息"
        },
        {
          "paper": "0628",
          "dataset": "FI-2010",
          "description": "The first publicly available benchmark dataset of high-frequency Limit Order Book (LOB) data"
        },
        {
          "paper": "0628",
          "dataset": "CSI-2016",
          "description": "A dataset we collected from three one-minute stock index data"
        },
        {
          "paper": "2647868.2654902",
          "dataset": "Wikipedia, Pascal, NUS-WIDE-10k",
          "description": "These datasets vary in text modality, size, and category numbers."
        },
        {
          "paper": "2021.acl_long.494",
          "dataset": "Restaurant, Laptop, Twitter",
          "description": "三个公开数据集"
        },
        {
          "paper": "2022.emnlp_main.212",
          "dataset": "D1 (Wu et al., 2020a)",
          "description": "benchmark datasets from SemEval Challenges"
        },
        {
          "paper": "2022.emnlp_main.212",
          "dataset": "D2 (Xu et al., 2020)",
          "description": "benchmark datasets from SemEval Challenges"
        },
        {
          "paper": "2022.coling_1.234",
          "dataset": "NYT10",
          "description": "原文无此信息"
        },
        {
          "paper": "2022.coling_1.234",
          "dataset": "GDS",
          "description": "原文无此信息"
        },
        {
          "paper": "2808205",
          "dataset": "Wikipedia dataset",
          "description": "2,866 image/text pairs from ten semantic categories."
        },
        {
          "paper": "2808205",
          "dataset": "Pascal dataset",
          "description": "1,000 image/text pairs from twenty categories."
        },
        {
          "paper": "2808205",
          "dataset": "NUS-WIDE-10k dataset",
          "description": "1,000 image/text pairs per category from ten categories."
        },
        {
          "paper": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining",
          "dataset": "RedPajama-V2 and MAP-CC",
          "description": "We sample an equal proportion of 15 billion English and Chinese tokens from RedPajama-V2 and MAP-CC, respectively."
        },
        {
          "paper": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
          "dataset": "Small ESP Game dataset",
          "description": "contains 100,000 labeled images with corresponding tags"
        },
        {
          "paper": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
          "dataset": "MLC-2013 dataset",
          "description": "has 1,000 manually labeled images with two labels per image"
        },
        {
          "paper": "3469877.3490585",
          "dataset": "Stanford image paragraph benchmark dataset",
          "description": "原文无此信息"
        },
        {
          "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
          "dataset": "Restaurant14",
          "description": "All datasets have three sentimental polarities: positive, neutral, and negative"
        },
        {
          "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
          "dataset": "Laptop14",
          "description": "All datasets have three sentimental polarities: positive, neutral, and negative"
        },
        {
          "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
          "dataset": "Twitter",
          "description": "All datasets have three sentimental polarities: positive, neutral, and negative"
        },
        {
          "paper": "2025.coling_main.22",
          "dataset": "TWITTER-15 and TWITTER-17",
          "description": "benchmark datasets for MABSA tasks, specifically C-MABSA with conditional relations"
        },
        {
          "paper": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
          "dataset": "Stanford benchmark dataset",
          "description": "includes 14575/2487/2489 pairs for training/validation/test. The dataset comprises an average of 67.5 words per paragraph and 5.7 sentences."
        },
        {
          "paper": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
          "dataset": "CUB",
          "description": "原文无此信息"
        },
        {
          "paper": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
          "dataset": "Oxford-102",
          "description": "原文无此信息"
        },
        {
          "paper": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
          "dataset": "COCO",
          "description": "原文无此信息"
        }
      ],
      "evaluation_metrics": [
        {
          "paper": "2504.15958v2",
          "metric": "CLIP"
        },
        {
          "paper": "2504.15958v2",
          "metric": "DINOv2"
        },
        {
          "paper": "2023.acl_long.802",
          "metric": "Sentiment Graph F1 (SF1)"
        },
        {
          "paper": "2023.acl_long.802",
          "metric": "Holder F1"
        },
        {
          "paper": "2023.acl_long.802",
          "metric": "Target F1"
        },
        {
          "paper": "2023.acl_long.802",
          "metric": "Exp. F1"
        },
        {
          "paper": "2023.acl_long.802",
          "metric": "Nonpolarity Sentiment Graph F1 (NSF1)"
        },
        {
          "paper": "2022.acl_long.212",
          "metric": "F1"
        },
        {
          "paper": "3664647.3681466",
          "metric": "PSNR"
        },
        {
          "paper": "3664647.3681466",
          "metric": "MSE"
        },
        {
          "paper": "3664647.3681466",
          "metric": "fMSE"
        },
        {
          "paper": "2408.03632v3",
          "metric": "CLIP"
        },
        {
          "paper": "2408.03632v3",
          "metric": "ImageReward"
        },
        {
          "paper": "2408.03632v3",
          "metric": "Segmentation Similarity (SegSim)"
        },
        {
          "paper": "3394171.3416296",
          "metric": "Mean Average Precision (MAP)"
        },
        {
          "paper": "3664647.3680897",
          "metric": "IoU"
        },
        {
          "paper": "3664647.3680897",
          "metric": "bbox accuracy"
        },
        {
          "paper": "3664647.3680897",
          "metric": "recognition accuracy"
        },
        {
          "paper": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
          "metric": "pointing game accuracy"
        },
        {
          "paper": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
          "metric": "bounding box accuracy"
        },
        {
          "paper": "0592",
          "metric": "BELU"
        },
        {
          "paper": "0592",
          "metric": "METEOR"
        },
        {
          "paper": "0592",
          "metric": "ROUGE L"
        },
        {
          "paper": "0592",
          "metric": "CIDEr"
        },
        {
          "paper": "0592",
          "metric": "Instance Coverage (IC)"
        },
        {
          "paper": "4930_Article_Text_7995_1_10_20190709",
          "metric": "Acc(ans)"
        },
        {
          "paper": "0628",
          "metric": "F1 score"
        },
        {
          "paper": "0628",
          "metric": "accuracy"
        },
        {
          "paper": "2647868.2654902",
          "metric": "mean average precision (mAP)"
        },
        {
          "paper": "2647868.2654902",
          "metric": "top 20% percentage"
        },
        {
          "paper": "2021.acl_long.494",
          "metric": "accuracy"
        },
        {
          "paper": "2021.acl_long.494",
          "metric": "macro-averaged F1-score"
        },
        {
          "paper": "2022.emnlp_main.212",
          "metric": "Precision"
        },
        {
          "paper": "2022.emnlp_main.212",
          "metric": "Recall"
        },
        {
          "paper": "2022.emnlp_main.212",
          "metric": "F1 scores"
        },
        {
          "paper": "2022.coling_1.234",
          "metric": "P@N"
        },
        {
          "paper": "2022.coling_1.234",
          "metric": "AUC"
        },
        {
          "paper": "2022.coling_1.234",
          "metric": "Micro-F1 score"
        },
        {
          "paper": "2808205",
          "metric": "mean average precision (mAP)"
        },
        {
          "paper": "2808205",
          "metric": "top 20% percentage"
        },
        {
          "paper": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining",
          "metric": "average accuracy"
        },
        {
          "paper": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
          "metric": "accuracy"
        },
        {
          "paper": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
          "metric": "area under the ROC curve"
        },
        {
          "paper": "3469877.3490585",
          "metric": "BLEU-1"
        },
        {
          "paper": "3469877.3490585",
          "metric": "BLEU-2"
        },
        {
          "paper": "3469877.3490585",
          "metric": "BLEU-3"
        },
        {
          "paper": "3469877.3490585",
          "metric": "BLEU-4"
        },
        {
          "paper": "3469877.3490585",
          "metric": "METEOR"
        },
        {
          "paper": "3469877.3490585",
          "metric": "CIDEr"
        },
        {
          "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
          "metric": "LAL-Parser"
        },
        {
          "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
          "metric": "Glove vectors"
        },
        {
          "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
          "metric": "BiLSTM"
        },
        {
          "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
          "metric": "dropout"
        },
        {
          "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
          "metric": "Adam optimizer"
        },
        {
          "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
          "metric": "learning rate"
        },
        {
          "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
          "metric": "epochs"
        },
        {
          "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
          "metric": "batch size"
        },
        {
          "paper": "2025.coling_main.22",
          "metric": "Accuracy"
        },
        {
          "paper": "2025.coling_main.22",
          "metric": "F1 score"
        },
        {
          "paper": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
          "metric": "BLEU@{1, 2, 3, 4}"
        },
        {
          "paper": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
          "metric": "METEOR"
        },
        {
          "paper": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
          "metric": "CIDEr"
        },
        {
          "paper": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
          "metric": "BERTScore F metrics"
        },
        {
          "paper": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
          "metric": "Inception Score (IS)"
        },
        {
          "paper": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
          "metric": "Fréchet Inception Distance (FID)"
        },
        {
          "paper": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
          "metric": "R-Precision"
        }
      ],
      "performance_data": []
    }
  },
  "ai_analysis": {
    "academic_identity": {
      "primary_research_identity": "计算机视觉与自然语言处理",
      "research_signature": [
        "多模态学习",
        "图像生成",
        "情感分析"
      ],
      "expertise_domains": [
        "计算机视觉",
        "自然语言处理",
        "机器学习"
      ],
      "academic_stage": "成熟期",
      "unique_strengths": [
        "多模态学习",
        "图像生成",
        "情感分析"
      ]
    },
    "innovation_assessment": {
      "innovation_level": 8,
      "creativity_patterns": "持续创新，注重跨领域融合",
      "technical_depth": "技术深度较高，掌握多种深度学习模型",
      "methodological_innovation": "在多模态学习、图像生成等领域有创新方法",
      "breakthrough_potential": "具有突破性研究潜力",
      "innovation_consistency": "创新持续性较好"
    },
    "research_trajectory": {
      "development_pattern": "稳步发展，注重跨领域研究",
      "topic_evolution": "从计算机视觉到多模态学习、图像生成等",
      "productivity_trend": "产出稳定增长",
      "collaboration_evolution": "合作范围不断扩大",
      "impact_growth": "影响力持续提升"
    },
    "technical_capabilities": {
      "experimental_skills": "实验技能较强",
      "dataset_expertise": "熟练使用多种公开数据集",
      "evaluation_proficiency": "熟练运用多种评价指标",
      "technical_breadth": "技术广度较广",
      "implementation_ability": "技术实现能力较强"
    },
    "collaboration_analysis": {
      "network_position": "处于合作网络的中心位置",
      "collaboration_quality": "合作质量较高",
      "institutional_connections": "与多个机构保持良好合作关系",
      "leadership_potential": "具有学术领导潜力",
      "mentoring_ability": "指导能力较强"
    },
    "development_recommendations": {
      "strategic_directions": [
        "继续深耕多模态学习",
        "拓展图像生成新方向"
      ],
      "skill_enhancement": [
        "学习最新深度学习模型",
        "加强跨领域合作"
      ],
      "collaboration_targets": [
        "与顶级机构合作",
        "拓展国际合作"
      ],
      "research_priorities": [
        "多模态学习",
        "图像生成"
      ],
      "career_milestones": [
        "获得重要奖项",
        "发表顶级期刊论文"
      ],
      "risk_mitigation": [
        "关注最新研究动态",
        "加强团队建设"
      ]
    },
    "comparative_analysis": {
      "peer_comparison": "处于同领域研究者前列",
      "competitive_advantages": [
        "多模态学习",
        "图像生成",
        "情感分析"
      ],
      "improvement_areas": [
        "加强理论创新",
        "拓展应用场景"
      ],
      "market_position": "处于学术市场高端位置"
    }
  }
}