{
  "metadata": {
    "generation_time": "2025-08-03 16:11:56",
    "method": "direct_from_raw_data",
    "data_sources": [
      "概要层",
      "结构化层"
    ],
    "ai_model": "glm-4"
  },
  "data_statistics": {
    "summary_documents": 118,
    "structural_documents": 118,
    "total_unique_documents": 118,
    "name_standardization": {
      "original_names": 200,
      "standardized_names": 195
    }
  },
  "ai_insights": {
    "individual_analyses": {
      "李睿凡": {
        "raw_data": {
          "basic_info": {
            "standardized_name": "李睿凡",
            "name_variations": [
              "Li Ruifan",
              "Ruifan LI",
              "RUIFAN LI",
              "Ruifan Li",
              "LI Ruifan",
              "李睿凡"
            ],
            "total_papers": 64,
            "document_types": {
              "patent": 10,
              "academic_paper": 54
            },
            "institutions": [
              "School of Computer Sciences, Beijing University of Posts and Communications, Beijing 100876, China",
              "School of Information Engineering, Beijing University of Posts and Telecommunications, Beijing, China",
              "北京邮电大学人工智能学院",
              "Key Laboratory of Interactive Technology and Experience System, Ministry of Culture and Tourism, China",
              "School of Artificial Intelligence, Beijing University of Posts and Telecommunications, Beijing, China",
              "School of Artificial Intelligence, Beijing University of Posts and Communications, Beijing 100876, China",
              "交互技术与体验系统文化和旅游部重点实验室",
              "Beijing Natural Science Foundation",
              "Beijing University of Posts and Telecommunications",
              "National Nature Science Foundation of China",
              "Engineering Research Center of Information Networks, Ministry of Education",
              "School of Artiﬁcial Intelligence, Beijing University of Posts and Telecommunications, China",
              "Engineering Research Center of Information Networks, Ministry of Education, China",
              "中国民航大学通信工程系",
              "北京邮电大学计算机学院",
              "Engineering Research Center of Information Networks, Ministry of Education, Beijing 100876, China",
              "Li Auto Inc.",
              "School of Artificial Intelligence, Beijing University of Posts and Telecommunications",
              "教育部信息网络工程研究中心",
              "Beijing University of Posts and Telecommunications, Beijing, China",
              "School of Artificial Intelligence, Beijing University of Posts and Telecommunications, China",
              "北京科技大学信息工程学院",
              "School of Science, Yanshan University, Qinhuangdao 066004, China",
              "原文未明确提到",
              "兰州理工大学计算机与通信学院",
              "School of Artiﬁcial Intelligence, Beijing University of Posts and Telecommunications, Beijing, China",
              "北京林业大学信息学院",
              "School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China",
              "Beijing Academy of Artificial Intelligence",
              "School of Computer, Beijing University of Posts and Telecommunications, Beijing 100876, P. R. China",
              "北京邮电大学信息工程学院",
              "BUPT",
              "School of Computers, Beijing University of Posts and Telecommunications, Beijing 100876, China",
              "华中科技大学控制科学与工程系",
              "Center for Intelligence Science and Technology, School of Computer Science, Beijing University of Posts and Telecommunications",
              "Beijing University of Posts & Telecommunications",
              "Beijing Academy of Artificial Intelligence, Beijing, China",
              "Center for Intelligence Science and Technology, Beijing University of Posts and Telecommunications",
              "High Performance Computing Platform of BUPT"
            ],
            "papers": [
              "一种基于人工智能的数据安全风险监测追溯系统_黄永军",
              "多视图有监督的LDA模型",
              "基于统计和加权的提高击键认证识别方法(英文)",
              "2504.15958v2",
              "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning",
              "1307.1275v1",
              "FAIA_372_FAIA230600",
              "2023.acl_long.802",
              "3191835.3191989",
              "2022.acl_long.212",
              "3664647.3681466",
              "2408.03632v3",
              "3394171.3416296",
              "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction",
              "一种图像检索方法_鲁鹏",
              "Designing_a_Japanese_idiom_education_support_system_for_overseas_students",
              "基于采集搜索引擎数据的隐私信息评级方法_芦效峰",
              "978_3_642_23223_7_60",
              "3664647.3680897",
              "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding",
              "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
              "引入深度学习的人工智能类课程",
              "智能科学技术导论教学目的及策略",
              "Improved+Eavesdropping+Detection+Strategy+Based+on+Extended+Three_particle+Greenberger_Horne_Zeilinger+State+in+Two_step+Quantum+Direct+Communication+Protocol",
              "2820400",
              "0592",
              "基于词性和位置的特征关键词提取方法_芦效峰",
              "Dimensionality_reduction_for_text_using_LLE",
              "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification",
              "使用TAST码的BI_STCM_ID系统中的星座映射分析",
              "4930_Article_Text_7995_1_10_20190709",
              "0628",
              "Enhanced_Prompt_Learning_for_Few_shot_Text_Classification_Method",
              "考虑合成灰数灰度性质的改进区间灰数预测模型",
              "鲁棒局部保持投影的表情识别",
              "面向小样本图像分类的任务相关度量学习方法及装置_李晓旭",
              "3548636.3548646",
              "2647868.2654902",
              "2022.findings_emnlp.6",
              "基于文本生成图像的模型训练方法、设备和图像生成方法_冯方向",
              "2021.acl_long.494",
              "2022.emnlp_main.212",
              "基于多层神经网络的电力实体识别方法、存储介质和设备_刘子全",
              "2022.coling_1.234",
              "2808205",
              "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining",
              "融入类别自适应度量学习的小样本图像分类方法及装置_李晓旭",
              "面向_智能科学与技术_专业的C语言教学探讨",
              "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
              "1911.09359v1",
              "基于KFD_Isomap的人脸识别",
              "深度学习中卷积神经网络的教学探讨",
              "基于掩码上下文机器阅读理解的方面情感三元组抽取方法_李睿凡",
              "3469877.3490585",
              "计算机游戏中的智能技术",
              "使用LDC码的BI_STCM_ID系统中的星座映射分析",
              "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
              "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
              "2025.coling_main.22",
              "0_387_29295_0_82",
              "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
              "一种基于关系编码和层次注意力机制的图像段落描述方法_李睿凡",
              "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
              "3483207.3483233"
            ]
          },
          "research_content": {
            "domains": [
              "工业互联网数据安全，工业企业数字化转型",
              "多视图数据的分类问题",
              "计算机视觉",
              "机器学习",
              "Keystroke Dynamics-Based Authentication",
              "Quantum Security Communication",
              "Text-to-image generation, subject-driven image generation, training-free framework",
              "Personalized content creation",
              "Image synthesis",
              "Few-shot learning in NLP",
              "NLP tasks",
              "Chinese text classification",
              "textual entailment tasks",
              "Multi-modal learning, bimodal data representations, image-tags",
              "Predictive systems for word tags",
              "Aspect-Based Sentiment Analysis (ABSA), Aspect Sentiment Quadruplet Extraction (ASQE)",
              "Aspect-Based Sentiment Analysis",
              "Structured Sentiment Analysis",
              "Opinion analysis",
              "Sentiment classification",
              "Natural language processing",
              "Microblogging emotional classification based on User-Generated Content (UGC)",
              "User-Generated Content (UGC) analysis",
              "Microblog sentiment analysis",
              "Aspect-based Sentiment Analysis (ABSA), Graph Convolutional Network",
              "Sentiment analysis",
              "Natural language processing",
              "Computer vision tasks, image harmonization",
              "Digital editing",
              "Image generation and editing tasks",
              "Text-to-image synthesis with multi-concept customization",
              "Image generation",
              "AI art",
              "Virtual reality",
              "Visual feature learning for image retrieval in e-commerce",
              "E-commerce",
              "Content-based image retrieval",
              "Chinese Spelling Correction (CSC)",
              "Natural Language Processing",
              "图像检索",
              "图像数据库检索",
              "Japanese Idiom Education Support System",
              "Japanese language education",
              "信息评级，隐私保护",
              "评定隐私信息的安全等级",
              "在成本和技术受限条件下优先保护重要的隐私",
              "Educational data mining",
              "Educational data mining",
              "Phrase Grounding under weak supervision",
              "Multimedia and multimodal retrieval",
              "Image captioning",
              "Computer Vision",
              "Natural Language Processing",
              "Weakly supervised phrase grounding",
              "Image-text alignment",
              "Multimodal information processing",
              "在人工智能类课程中引入深度学习的初步内容和实施建议",
              "智能科学技术导论的教学目的、策略和改进思考",
              "Quantum information security, quantum direct communication",
              "Quantum secure direct communication (QSDC)",
              "Deep Learning for Multimedia and Emotional and Social Signals in Multimedia",
              "Image captioning",
              "Computer Vision",
              "Natural Language Processing",
              "文本挖掘领域",
              "实时网络话题检测",
              "文本特征提取",
              "Dimensionality reduction in text processing",
              "Pattern recognition",
              "Text processing",
              "Data visualization",
              "Multimodal sentiment classification",
              "Social network posts sentiment analysis",
              "移动通信、编码理论、人工智能",
              "移动通信系统",
              "Visual Question Answering (VQA)",
              "Human-computer interaction",
              "Image and text understanding",
              "Stock Trend Prediction in Artificial Intelligence",
              "Intelligent Investment",
              "Financial Time Series Analysis",
              "少样本文本分类",
              "文本分类任务",
              "区间灰数预测模型",
              "系统工程",
              "电子技术",
              "表情识别",
              "小样本图像分类",
              "Text-based power equipment fault recognition",
              "power equipment maintenance",
              "China’s State Grid",
              "Cross-modal retrieval",
              "Web image database",
              "Information retrieval",
              "Commonsense question answering, Knowledge Enhanced Graph Contrastive Learning, Graph reasoning",
              "Natural language understanding",
              "Question Answering",
              "基于文本生成图像",
              "Aspect-based sentiment analysis",
              "Natural language processing",
              "Sentiment analysis",
              "Aspect Sentiment Triplet Extraction, Machine Reading Comprehension, Fine-grained Aspect-based Sentiment Analysis",
              "Aspect-based Sentiment Analysis",
              "Natural Language Processing",
              "电力领域",
              "电力语料实体识别",
              "Distantly supervised relation extraction",
              "Natural Language Processing",
              "Cross-modal retrieval, deep learning, autoencoder",
              "Multimodal data retrieval",
              "Cross-lingual Learning, Pointwise Mutual Information (PMI), Hallucination, Large Language Models (LLMs)",
              "Cross-lingual transfer learning",
              "Language model pretraining",
              "小样本图像分类",
              "C-way K-shot分类任务",
              "智能科学与技术专业的C语言教学变革",
              "Multimodal data analysis using deep neural learning",
              "Image and text analysis",
              "Complex systems",
              "Financial time-series classification",
              "Investment management",
              "Chinese stock market",
              "人脸识别",
              "人脸识别",
              "深度学习中的卷积神经网络的教学工作",
              "方面情感分析任务",
              "Image paragraph captioning",
              "Computer vision tasks",
              "计算机游戏中的智能技术",
              "计算机游戏",
              "空时编码调制；星座映射；LDC码",
              "Aspect-Based Sentiment Analysis",
              "sentiment analysis",
              "recommendation",
              "advertisement computation",
              "Counterfactual Referring Expression Comprehension (C-REC)",
              "Vision-Language Tasks",
              "Image-Text Matching",
              "Multimodal Aspect-Based Sentiment Analysis (MABSA)",
              "Social media sentiment analysis",
              "Face recognition",
              "Face recognition",
              "Image paragraph captioning",
              "Beijing Academy of Artificial Intelligence",
              "Computer Vision",
              "Natural Language Processing",
              "图像段落描述",
              "斯坦福段落描述数据集",
              "Text-to-image synthesis, Generative Adversarial Networks, Multi-modal disentangled representation learning",
              "Interactive art",
              "Computer-aided drawing",
              "Electric equipment maintenance using reinforcement learning",
              "Electrical equipment maintenance",
              "Power grid management"
            ],
            "methods": [
              "包括数据采集模块、数据流转与分布监测模块、数据安全事件分析模块和数据安全事件溯源模块",
              "多视图有监督的LDA模型",
              "TOP10 detector",
              "FreeGraftor, a training-free framework that uses cross-image feature grafting",
              "Template selection mechanism using a masked language model",
              "Hierarchical representations of bimodal data using MPEG-7, gist descriptors, RBMs, and a quasi-Siamese auto-encoder",
              "Enhanced Machine Reading Comprehension (EMRC)",
              "USSA, a unified 2D table-filling scheme that utilizes 13 relation types and a bi-axial attention module",
              "A system that removes noise from microblogs, extracts features, and classifies emotions using Support Vector Machine (SVM), integrating dictionary and rule-based approaches for feature extraction and weight computation",
              "Enhanced Multi-Channel Graph Convolutional Network (EMC-GCN)",
              "Harmony-VAE, inverse harmonization diffusion model",
              "Concept Conductor framework with multipath sampling, layout alignment, and concept injection",
              "Utilizing product titles as supervised signals to learn image features, constructing an image classification dataset using n-grams from product titles, fine-tuning a pre-trained model, and extracting the basic max-pooling activation of convolutions (MAC) feature.",
              "NCO-Spell, multi-character masking strategy, iterative inference algorithm",
              "计算图像间的内点数和直接与间接相关度值，构建赋权邻接矩阵，进行衰减计算和迭代调整",
              "The system includes modules, a knowledge base, and a database. It incorporates functions like idiom retrieval, idiom teaching, and uses MS Agent for operational assistance.",
              "本发明提供一种基于采集搜索引擎数据的隐私信息评级方法，包括确定每个隐私信息的普遍性分值U，敏感性分值S，根据U×S计算结果确定隐私信息的安全等级",
              "Employ various classification algorithms, including KNN, SVD, and logistic regression, to combine their results for the final prediction",
              "Triple alignment strategies: Region-Text Alignment (RTA), Domain Alignment (DomA), and Category Alignment (CatA)",
              "Improved Transformer with IoU Position encoding model (TIP)",
              "Refinement-based approach using a detector-free phrase grounding model fine-tuned with a visual prompt from CLIP text-related representations",
              "Extended three-particle GHZ state, entropy theory",
              "Facial emotion recognition during speech; Correspondence autoencoders for cross-modal retrieval",
              "Topic-Oriented Multi-Sentence (TOMS) captioning model",
              "本发明提供了一种基于词性和位置的特征关键词提取方法，包括文本预处理，去除特定词性的候选关键词，计算加权词频，计算增量逆文档频率，计算权重，排序并选择权重最大的词作为关键词",
              "Locally Linear Embedding (LLE)",
              "Multi-level fusion classification (MFC) model",
              "使用TAST空时编码方案的星座映射设计优化问题",
              "Differential Networks (DN) and DN-based Fusion (DF)",
              "Multi-scale Two-way Deep Neural Network (MTDNN), using eXtreme Gradient Boosting and Recurrent Convolutional Neural Network",
              "EPL4FTC算法",
              "合成灰数灰度的定义及其性质分析，建立灰度序列的GM(1,1)模型实现灰度预测",
              "鲁棒局部保持投影算法",
              "引入注意力机制和自适应度量学习，构建面向小样本图像分类的任务相关度量学习模型",
              "EP-BERTGCN, combining pre-trained BERT and Graph Convolutional Network (GCN)",
              "Correspondence autoencoder (Corr-AE), Corr-Cross-AE, and Corr-Full-AE",
              "KE-GCL model, Graph Contrastive Learning, Adaptive sampling, Hard negative graph pairs",
              "通过模态解纠缠提取真实度参数的模型训练方法",
              "DualGCN model",
              "COntext-Masked MRC (COM-MRC) framework",
              "使用BERT电力实体识别模型，通过哈夫曼编码映射得到实体标签",
              "BERT-based Graph Convolutional network Model (BGM)",
              "Correspondence Autoencoder (Corr-AE), integrating representation learning and correlation learning into a single process",
              "InfoLoss, a novel loss function for continual pretraining",
              "为每个类别构建一个度量模块，通过对类内共性特征的学习，建立基于类内共性特征的度量",
              "教学内容和方法改变的具体措施，将课程分为语言学习和项目实践两大部分",
              "Bimodal Deep Architecture (BDA)",
              "Multi-Scale Temporal Dependent Recurrent Convolutional Neural Network (MSTD-RCNN)",
              "KFD—Isomap算法",
              "教学内容安排和教学内容之外的考虑",
              "使用BERT作为句子的编码器，通过掩码上下文进行方面词和意见词的抽取，以及情感分类",
              "Splitting to Tree Decoder (S2TD)",
              "讨论游戏设计元素与人工智能之间的关系，介绍A*算法、有限状态机、群体寻径等技术，讨论适应与学习等游戏开发者关注的问题",
              "基于最大化编码增益的高维星座映射设计优化问题；基于最大化欧式距离调和均值的一维星座映射设计优化问题",
              "DualGCN, integrating SynGCN and SemGCN through a mutual BiAffine module, with orthogonal and differential regularizers.",
              "A method to generate fine-grained counterfactual samples and a C-REC framework with dual-branch attentive fusion and contrastive learning.",
              "COnditional Relation based Sentiment Analysis framework (CORSA), including a conditional relation detector (CRD) and a visual object localizer (VOL)",
              "SKFD-Isomap, which uses class information to construct the neighborhood and kernel Fisher discriminant (KFD) for nonlinear embedding",
              "DualRel model",
              "关系编码过程和层次注意力解码过程，包括空间关系编码器和语义关系编码器，以及使用两个LSTM和一个层次注意力动态融合关系信息和物体区域信息",
              "Modality disentangled discriminator, AttnGAN, DM-GAN",
              "Reinforcement learning with dynamic programming, Markov hypothesis, and cut set of the power grid"
            ],
            "keywords": [
              "数据安全风险监测",
              "人工智能",
              "数据流转",
              "安全事件溯源",
              "多视图分类",
              "概率主题模型",
              "变分期望最大化",
              "Keystroke Dynamics",
              "Authentication",
              "Feature Extraction",
              "Probability",
              "Weight",
              "Text-to-image generation",
              "Subject-driven image generation",
              "Cross-image feature grafting",
              "Semantic matching",
              "Efficiency",
              "Multi-subject generation",
              "Few-shot learning",
              "textual entailment",
              "template selection",
              "MacBERT",
              "FewCLUE",
              "Multi-modal learning",
              "Bimodal representations",
              "Image-tags",
              "Word tags",
              "Restricted Boltzmann Machines",
              "Siamese network",
              "ABSA",
              "ASQE",
              "EMRC",
              "bi-directional attention",
              "hierarchical category classification",
              "Structured Sentiment Analysis",
              "USSA",
              "Bi-axial attention",
              "Overlap",
              "Discontinuity",
              "emotional classification",
              "feature extraction",
              "weight computing",
              "support vector machine",
              "Aspect Sentiment Triplet Extraction",
              "Graph Convolutional Network",
              "Linguistic Features",
              "Refining Strategy",
              "image harmonization",
              "latent diffusion model",
              "VAE",
              "data augmentation",
              "inverse harmonization",
              "stable diffusion",
              "Text-to-image synthesis",
              "Personalization",
              "Concept Conductor",
              "Attribute leakage",
              "Layout confusion",
              "Visual feature learning",
              "Bag of n-grams",
              "Image retrieval",
              "CNN",
              "MAC",
              "Chinese Spelling Correction",
              "BERT",
              "NCO-Spell",
              "noisy contexts",
              "iterative inference",
              "图像检索",
              "内点数",
              "相关度值",
              "赋权邻接矩阵",
              "间接相关度衰减因子",
              "Japanese idioms",
              "education support system",
              "language learners",
              "animation",
              "Super Function",
              "隐私信息评级",
              "搜索引擎数据",
              "普遍性分值",
              "敏感性分值",
              "安全等级",
              "data mining",
              "logistic regression",
              "k-nearest neighbor",
              "singular value decomposition",
              "classifiers combination",
              "Phrase Grounding",
              "weak supervision",
              "zero-shot learning",
              "alignment strategies",
              "CLIP",
              "Image Captioning",
              "Transformer",
              "IoU Position Encoding",
              "Intra-modal Attention",
              "Weakly supervised phrase grounding",
              "Visual prompt tuning",
              "CLIP",
              "Detector-free",
              "人工智能",
              "深度学习",
              "教学建议",
              "智能科学技术导论",
              "教学目标",
              "教学策略",
              "Quantum key distribution (QKD)",
              "Dense coding",
              "Extended three-particle GHZ state",
              "Eavesdropping detection",
              "Entropy",
              "Multimedia",
              "Deep Learning",
              "Emotion Recognition",
              "Speech",
              "Correspondence Autoencoders",
              "Cross-Modal Retrieval",
              "Image captioning",
              "Topic-Oriented Multi-Sentence",
              "Latent Dirichlet Allocation",
              "Fusion Gate Unit",
              "Instance Coverage",
              "词性和位置",
              "特征关键词",
              "提取方法",
              "加权词频",
              "增量逆文档频率",
              "Dimensionality reduction",
              "Locally Linear Embedding",
              "Text processing",
              "Manifold learning",
              "LSI",
              "Graph embedding",
              "multimodal fusion",
              "sentiment analysis",
              "deep learning",
              "空时编码调制",
              "星座映射",
              "TAST码",
              "Visual Question Answering",
              "Differential Networks",
              "DN-based Fusion",
              "attention distribution",
              "Stock Trend Prediction",
              "Multi-scale Analysis",
              "Deep Neural Network",
              "Wavelet Transform",
              "Downsampling",
              "XGBoost",
              "RCNN",
              "预训练语言模型",
              "少样本学习",
              "文本分类",
              "提示学习",
              "三元组损失",
              "区间灰数",
              "预测模型",
              "合成灰数",
              "灰度",
              "局部保持投影",
              "鲁棒性",
              "表情识别",
              "小样本图像分类",
              "任务相关度量学习",
              "注意力机制",
              "自适应度量学习",
              "power equipment fault recognition",
              "BERT",
              "GCN",
              "domain adaptation",
              "Cross-modal retrieval",
              "Correspondence autoencoder",
              "Representation learning",
              "Correlation learning",
              "Commonsense Question Answering",
              "Graph Contrastive Learning",
              "Knowledge Enhancement",
              "KE-GCL",
              "文本生成图像",
              "模态解纠缠",
              "损失函数",
              "模型训练",
              "aspect-based sentiment analysis",
              "dual graph convolutional networks",
              "dependency parsing",
              "semantic correlations",
              "regularizers",
              "Aspect Sentiment Triplet Extraction",
              "Machine Reading Comprehension",
              "Context Masking",
              "Sentiment Analysis",
              "多层神经网络",
              "电力实体识别",
              "BERT",
              "哈夫曼编码",
              "Distant supervision",
              "Relation extraction",
              "BERT",
              "Graph Convolutional Network",
              "Cross-entropy loss",
              "Cross-modal",
              "retrieval",
              "image and text",
              "deep learning",
              "autoencoder",
              "Cross-lingual Learning",
              "Pointwise Mutual Information (PMI)",
              "Hallucination",
              "Large Language Models (LLMs)",
              "类别自适应度量学习",
              "小样本图像分类",
              "度量模块",
              "嵌入模块",
              "均方误差损失函数",
              "C语言",
              "智能科学与技术",
              "教学",
              "Multimodal data",
              "Deep neural learning",
              "Similarity metric",
              "Bimodal deep architecture",
              "Financial time-series classification",
              "Multi-Scale",
              "Temporal Dependency",
              "Recurrent Convolutional Neural Network",
              "Simulated trading",
              "人脸识别",
              "流形",
              "Isomap",
              "核Fisher判别",
              "智能科学与技术",
              "深度学习",
              "卷积神经网络",
              "教学建议",
              "掩码上下文",
              "机器阅读理解",
              "方面情感三元组",
              "BERT",
              "数据增强",
              "image captioning",
              "paragraph generation",
              "tree-structured decoder",
              "vision and language",
              "人工智能",
              "有限状态机",
              "群体智能",
              "神经网络",
              "遗传算法",
              "空时编码调制",
              "星座映射",
              "LDC码",
              "Aspect-Based Sentiment Analysis",
              "DualGCN",
              "graph convolutional networks",
              "dependency parsing",
              "semantic information",
              "Counterfactual Referring Expression Comprehension",
              "Fine-grained Attributes",
              "Dual-branch Attentive Fusion",
              "Contrastive Learning",
              "Multimodal Aspect-Based Sentiment Analysis",
              "Conditional Relation",
              "CORSA framework",
              "Visual Object Localizer",
              "face recognition",
              "manifold",
              "Isomap",
              "KFD",
              "Image paragraph captioning",
              "Dual Relations",
              "Spatial Relation",
              "Semantic Relation",
              "Weakly Supervised",
              "Relation-aware Attention",
              "关系编码",
              "层次注意力",
              "图像段落描述",
              "语义关系编码",
              "空间关系编码",
              "text-to-image synthesis",
              "generative adversarial networks",
              "multi-modal disentangled representation learning",
              "Electric equipment maintenance",
              "Reinforcement learning",
              "Dynamic decision making"
            ],
            "innovations": [
              "数据识别特征库",
              "数据资产清单",
              "数据安全风险监测策略库",
              "实时安全事件溯源分析",
              "将集成学习思想引入主题模型中",
              "结合概率主题模型LDA模型和集成分类方法Softmax混合模型",
              "Filtering outlier data",
              "Calculating probabilities of each input key",
              "Selecting less informative features",
              "Assigning weights",
              "Training-free cross-image feature grafting",
              "Semantic matching",
              "Position-constrained attention fusion",
              "Noise initialization strategy for geometry priors",
              "Using a template selection mechanism to assess candidate templates",
              "Applying the method on FewCLUE shared tasks",
              "Three-level representations in three stages",
              "Bimodal auto-encoder for level-3 representations",
              "Data-specific strategy for choosing correct tag words",
              "A novel EMRC model",
              "Hierarchical category classification strategy",
              "Bi-directional attention mechanism",
              "Unified 2D table-filling scheme",
              "Bi-axial attention module",
              "Addressing overlap and discontinuity in SSA",
              "Integration of dictionary and rule-based methods for feature extraction and weight calculation",
              "Modified CHI algorithm (MCHI) for feature word selection",
              "Modified TF-IDF (MTF-IDF) algorithm for weight calculation",
              "Proposed EMC-GCN to exploit word relations",
              "Defined ten relation types",
              "Incorporated linguistic features",
              "Developed refining strategy for improved triplet extraction",
              "Harmony-VAE to enhance decoded image quality",
              "Inverse harmonization model for data augmentation",
              "Introduction of the Human Harmony dataset",
              "Training-free framework",
              "Isolates sampling processes",
              "Self-attention-based spatial guidance",
              "Shape-aware masks for concept injection",
              "Using product titles to guide the learning of visual features",
              "Conversion of product titles into discrete labels for supervised learning",
              "Bag of n-grams approach",
              "multi-character masking strategy",
              "dynamic confusion set",
              "iterative inference method",
              "引入间接相关度衰减因子αB",
              "赋权邻接矩阵A",
              "迭代计算间接相关度T",
              "Idiom retrieval and teaching functions",
              "Use of MS Agent",
              "Animation in teaching idioms",
              "Super Function for input sentence analysis",
              "使用搜索引擎采集数据确定隐私信息的普遍性和敏感性分值",
              "根据普遍性和敏感性分值计算隐私信息的安全等级",
              "Classifiers combination",
              "Feature engineering",
              "Regularized logistic regression model",
              "Triple alignment strategies for zero-shot Phrase Grounding under Weak Supervision",
              "CLIP-based heatmap generation",
              "Region-category relations consideration",
              "Intra-modal attention mechanism",
              "IoU spatial position encoding method",
              "Use of similarity tokens for spatial information capture",
              "Detector-free network fine-tuning",
              "改革教学模式",
              "课堂与实验室结合教学",
              "多样化、全方位的考评模式",
              "Improved eavesdropping detection strategy using extended three-particle GHZ state",
              "Latent Dirichlet Allocation (LDA) for topic mining",
              "Fusion Gate Unit (FGU) for sentence generation",
              "Multi-label logistic regression and softmax for training",
              "考虑词性和词位置计算加权词频",
              "计算文本中关键候选词的增量逆文档频率",
              "动态调整逆文档频率以适应动态变化的数据集",
              "Application of LLE to text processing",
              "Comparison of LLE with latent semantic indexing (LSI) within the graph embedding framework",
              "Propose a multi-level fusion classification model",
              "Uses CNNs and Bi-GRU for feature extraction and fusion",
              "Introduces a rectified conflict detection mechanism",
              "证明了在使用TAST码的BI-STCM-ID系统中，高维星座映射设计的最大化编码增益准则与一维星座设计的最大化欧氏距调和均值准则的等价性",
              "Propose Differential Networks (DN) module",
              "Introduce DN-based Fusion (DF) model for VQA",
              "Two-way end-to-end model",
              "Wavelet-based and downsampling-based scale information",
              "Enhancing stock trend prediction with multi-scale information",
              "提示学习",
              "三元组损失优化",
              "自然语言推理",
              "提出合成灰数灰度的定义及其性质",
              "建立灰度序列的GM(1,1)模型实现灰度预测",
              "提出了一种鲁棒的局部保持投影算法",
              "在样本数据集X上执行局部鲁棒主成分分析",
              "引入注意力机制",
              "任务自适应度量学习",
              "任务相关空间映射学习",
              "Introduction of the BERT module into C-TextGCN",
              "Domain adaption of the BERT model",
              "Integrates representation and correlation learning into a single process",
              "Proposes a novel loss function",
              "Extends the Corr-AE to two other correspondence models",
              "Incorporates entity contextual descriptions into KGs",
              "Adaptive graph augmentation",
              "Selects hard negatives from incorrect answers",
              "模态解纠缠方式提取真实度参数",
              "内容损失函数采用排序目标函数的三元组损失函数",
              "各损失函数线性组合构成总体损失函数",
              "DualGCN architecture",
              "SynGCN and SemGCN modules",
              "orthogonal and differential regularizers",
              "Context augmentation strategy",
              "Discriminative model",
              "Two-stage inference method",
              "使用BERT语言模型进行预训练",
              "构建电力实体标签的哈夫曼编码",
              "增加分类层构成BERT电力实体识别模型",
              "Combining a Pretrained Language Model (PLM) and a Graph Convolutional Network (GCN)",
              "Using GCN to directly learn bag representations over instances",
              "Proposed the Corr-AE",
              "Integrates representation and correlation learning",
              "Two correspondence models: Corr-Cross-AE and Corr-Full-AE",
              "Proposal of InfoLoss for continually pretraining LLMs",
              "Mitigation of hallucinations in cross-lingual transfer setting",
              "融入类别自适应度量学习",
              "引入机器智能前沿问题作为实践项目",
              "Three interconnected components",
              "Stacked restricted Boltzmann machines",
              "Variant autoencoder with predefined loss function",
              "A novel method combining both MS and TD properties in financial time-series",
              "MS feature extraction with convolutional units without predefined parameters",
              "Fusion of different scale features using a Recurrent Neural Network to capture temporal dependencies",
              "用核Fisher判别替代Isomap中的经典多维尺度分析",
              "基于掩码上下文机器阅读理解(COM-MRC)的方面情感三元组抽取方法",
              "上下文数据增强方法",
              "适应推理算法的有效模型架构",
              "Tree-structured decoder",
              "Top-down binary tree expansion",
              "Split module",
              "Score module",
              "Tree structure loss",
              "提出游戏AI设计的目标和实现方法",
              "探讨不同学习方法在游戏AI中的应用",
              "DualGCN model",
              "SynGCN",
              "SemGCN",
              "Mutual BiAffine module",
              "orthogonal and differential regularizers",
              "Dual-branch attentive fusion module",
              "Counterfactual sample generation method",
              "Contrastive learning for counterfactual perception",
              "Proposed CORSA framework",
              "Conditional Relation Detector (CRD)",
              "Visual Object Localizer (VOL)",
              "Utilizes class information in feature extraction",
              "Applies KFD to the matrix of shortest paths",
              "Captures spatial and semantic relations among objects",
              "Weakly supervised multi-label classifier",
              "Relation-aware attention",
              "Fusion Gates",
              "关系编码",
              "层次注意力机制",
              "关系门和视觉门的设计",
              "Proposed modality disentangled discriminator",
              "Separate classification of content and style",
              "Enhanced text-image correlation",
              "Style transfer",
              "Dynamic policy generator",
              "Equipment weight modeling",
              "State communication through cut sets"
            ],
            "technical_concepts": [
              "数据采集模块",
              "数据流转与分布监测模块",
              "数据安全事件分析模块",
              "数据安全事件溯源模块",
              "数据识别特征库",
              "数据资产清单",
              "风险传导预警模型",
              "LDA模型",
              "Softmax模型",
              "变分EM算法",
              "Keystroke eigenvalues",
              "Normal distribution",
              "CDF",
              "Probability vector",
              "Weight vector",
              "FreeGraftor",
              "Semantic-Aware Feature Grafting",
              "Structure-Consistent Initialization",
              "Multimodal-Diffusion Transformer",
              "U-Net",
              "Transformer",
              "Stable Diffusion",
              "FLUX.1",
              "Entailment-based Few-shot Learning (EFL)",
              "Masked Language Model (MLM)",
              "MacBERT",
              "PyTorch",
              "HuggingFace toolkit",
              "MPEG-7",
              "gist descriptors",
              "Restricted Boltzmann Machines",
              "Siamese network",
              "auto-encoder",
              "Contrastive Divergence",
              "Machine Reading Comprehension (MRC)",
              "BERT",
              "bi-directional attention mechanism",
              "category classification",
              "quadruplet extraction",
              "Structured Sentiment Analysis",
              "2D table-filling scheme",
              "Bi-axial attention module",
              "Bi-lexical dependency parsing",
              "Support Vector Machine (SVM)",
              "dictionary-based method",
              "rule-based method",
              "vector space model",
              "jieba segmentation tool",
              "Modified CHI algorithm (MCHI)",
              "Modified TF-IDF (MTF-IDF) algorithm",
              "EMC-GCN",
              "Aspect-based Sentiment Analysis",
              "Graph Convolutional Operations",
              "Linguistic Features",
              "Biaffine Attention Module",
              "Latent diffusion models",
              "Harmony-VAE",
              "Inverse harmonization model",
              "Denoising Diffusion Probabilistic Models (DDPMs)",
              "Stable Diffusion",
              "Text-to-image diffusion models",
              "Multipath sampling",
              "Layout alignment",
              "Concept injection",
              "VAE",
              "Image representations",
              "Visual content-based indexing and retrieval",
              "CNN",
              "MAC",
              "SPoC",
              "RMAC",
              "RAMAC",
              "MS-RMAC",
              "GRMAC",
              "BERT",
              "NCO-Spell",
              "masking strategy",
              "confusion sets",
              "iterative inference",
              "内点数计算",
              "相关度计算",
              "赋权邻接矩阵",
              "迭代计算",
              "Idiom retrieval module",
              "Idiom reverse resolution module",
              "Super Function Database",
              "MS Agent",
              "TVML",
              "普遍性分值U",
              "敏感性分值S",
              "隐私信息的安全等级",
              "KNN",
              "SVD",
              "Logistic Regression",
              "Classifiers combination",
              "Feature engineering",
              "Region-Text Alignment",
              "Domain Alignment",
              "Category Alignment",
              "Contrastive Language-Image Pre-Training (CLIP)",
              "heatmap generation",
              "CNNs",
              "RNNs",
              "Attention Mechanisms",
              "Transformer Structure",
              "IoU Spatial Position Encoding",
              "CLIP",
              "Grounding model",
              "Similarity tokens",
              "Cosine similarity",
              "VGG16",
              "深度学习",
              "层次训练的基本算法",
              "波尔兹曼机",
              "深层信念网络",
              "Bell states",
              "EPR pair block",
              "Quantum teleportation",
              "Quantum secret sharing",
              "TSET protocol",
              "TSE protocol",
              "Facial emotion recognition",
              "Correspondence autoencoders",
              "Cross-modal retrieval",
              "LSTM",
              "FGU",
              "LDA",
              "Multi-label logistic regression",
              "Softmax",
              "BELU",
              "METEOR",
              "ROUGE L",
              "CIDEr",
              "Instance Coverage",
              "TF-IDF",
              "词频",
              "逆文档频率",
              "加权词频",
              "增量逆文档频率",
              "Locally Linear Embedding (LLE)",
              "Manifold learning",
              "Latent Semantic Indexing (LSI)",
              "Graph embedding",
              "K-Nearest-Neighbor classification",
              "CNN",
              "RNN",
              "Bi-GRU",
              "feature fusion",
              "sentiment classification",
              "conflict detection",
              "BI-STCM-ID系统",
              "TAST码",
              "星座映射设计",
              "编码增益",
              "欧氏距调和均值",
              "Differential Networks (DN)",
              "DN-based Fusion (DF)",
              "pair-wise feature elements",
              "attention-based models",
              "Neural Networks",
              "Support Vector Machine",
              "Random Forest",
              "DWT",
              "CNN",
              "GRU",
              "Cross-entropy Loss",
              "自然语言推理",
              "掩码语言模型",
              "三元组损失",
              "度量优化",
              "GM(1,1)模型",
              "合成灰数",
              "灰度",
              "局部保持投影算法",
              "Laplace Beltrami算子",
              "鲁棒主成分分析",
              "Gabor小波特征提取",
              "最近邻分类器",
              "卷积块",
              "注意力模块",
              "余弦度量模块",
              "Adam优化算法",
              "BERT",
              "Graph Convolutional Network",
              "domain adaptation",
              "text classification",
              "PMI",
              "TF-IDF",
              "Autoencoders",
              "Correlation measure",
              "Loss function",
              "Deep architecture",
              "Canonical correlation analysis",
              "Knowledge Graphs",
              "Graph Attention Networks",
              "Contrastive Learning",
              "InfoNCE Loss",
              "图像生成模型",
              "真实度参数",
              "子损失函数",
              "总体损失函数",
              "内容损失函数",
              "风格损失函数",
              "生成器损失函数",
              "判别器损失函数",
              "Graph Convolutional Networks",
              "BiLSTM",
              "BERT",
              "dependency probability matrix",
              "self-attention mechanism",
              "COntext-Masked MRC",
              "Aspect Extraction",
              "Opinion Extraction",
              "Sentiment Classification",
              "Aspect Detection",
              "BERT电力实体识别模型",
              "哈夫曼树编码",
              "伪标注语料",
              "BERT-based PLM",
              "Graph Convolutional Network (GCN)",
              "Bag representation",
              "Cross-entropy loss",
              "Gradient descent optimization",
              "Autoencoders",
              "Correlation learning",
              "Multimodal reconstruction",
              "CCA",
              "InfoLoss",
              "Cross-entropy loss",
              "Pointwise Mutual Information (PMI)",
              "Hallucinations",
              "Large Language Models (LLMs)",
              "数据预处理模块",
              "网络模型构建模块",
              "模型参数训练模块",
              "模型性能测试模块",
              "嵌入模块fθ",
              "类相关自适应度量模块",
              "C语言程序设计",
              "机器智能前沿问题",
              "教学进程影响因素",
              "Multimodal data",
              "Deep neural networks",
              "Restricted Boltzmann machines",
              "Autoencoder",
              "Similarity measure",
              "Multi-Scale Temporal Dependent Recurrent Convolutional Neural Network (MSTD-RCNN)",
              "convolutional units",
              "Gated Recurrent Unit (GRU)",
              "backpropagation",
              "KFD-Isomap",
              "Isomap",
              "核Fisher判别",
              "多维尺度分析",
              "人工神经网络",
              "深度学习",
              "卷积神经网络",
              "机器学习",
              "Hopfield神经网络",
              "多层前向神经网络",
              "反向传播算法",
              "方面词推理算法",
              "上下文数据增强方法",
              "COM-MRC框架",
              "注意力矩阵",
              "损失函数",
              "Computer vision tasks",
              "Split module",
              "Score module",
              "Word-level RNN",
              "Tree structure loss",
              "Cosine similarity",
              "LSTM",
              "Highway Network",
              "Sentence-BERT",
              "A*算法",
              "有限状态机",
              "群体寻径",
              "N-gram模型",
              "遗传算法",
              "神经网络",
              "粒子群优化",
              "BI-STCM-ID系统",
              "LDC码",
              "空时编码器",
              "MIMO信道模型",
              "渐进性能",
              "星座设计",
              "调和均值准则",
              "graph neural networks",
              "GCNs",
              "GATs",
              "syntax structures",
              "semantic correlations",
              "dependency trees",
              "Counterfactual Sample Generation",
              "Dual-branch Attentive Fusion",
              "Contrastive Loss",
              "IoU",
              "Cross-Entropy Loss",
              "Multimodal Aspect Term Extraction",
              "Multimodal Aspect-oriented Sentiment Classification",
              "Joint Multimodal Aspects of Sentiment Analysis",
              "UNINEXT",
              "YOLOv8",
              "Isomap",
              "kernel Fisher discriminant (KFD)",
              "geodesic distances",
              "nearest neighbor classifier",
              "DualRel model",
              "Faster R-CNN",
              "Relation Embedding Module",
              "Relation-aware Interaction Module",
              "Self-critical sequence training",
              "LSTM",
              "层次注意力",
              "空间关系编码器",
              "语义关系编码器",
              "多标签分类",
              "Generative Adversarial Networks (GANs)",
              "AttnGAN",
              "DM-GAN",
              "Inception Score (IS)",
              "Fréchet Inception Distance (FID)",
              "Modality disentangled representation",
              "Markov hypothesis",
              "Dynamic programming",
              "Multi-Agent Reinforcement Learning",
              "Cut set"
            ]
          },
          "detailed_innovations": {
            "contributions_by_paper": {
              "多视图有监督的LDA模型": [
                "提出一个多视图有监督的LDA模型"
              ],
              "基于统计和加权的提高击键认证识别方法(英文)": [
                "proposes a methodology for improving the recognition accuracy of keystroke authentication"
              ],
              "2504.15958v2": [
                "training-free framework",
                "cross-image feature grafting",
                "structure-consistent initialization"
              ],
              "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning": [
                "introducing a template selection mechanism using a masked language model to assess candidate templates"
              ],
              "FAIA_372_FAIA230600": [
                "A novel EMRC model that effectively builds associations among sentimental subtasks.",
                "A hierarchical category classification strategy to improve the context representation's task-awareness.",
                "Extensive experimental results demonstrating EMRC's superiority over existing baselines."
              ],
              "2023.acl_long.802": [
                "a bi-lexical dependency parsing graph converted to a unified 2D table filling scheme (USSA)",
                "an effective model that collaborates with USSA, utilizing the bi-axial attention module",
                "extensive experimental validation on benchmark datasets"
              ],
              "2022.acl_long.212": [
                "a novel EMC-GCN model",
                "a comprehensive exploitation of linguistic features",
                "an effective refining strategy"
              ],
              "3664647.3681466": [
                "Introduction of the Human Harmony dataset",
                "Harmony-VAE's effectiveness in enhancing the VAE component in latent diffusion models for image harmonization"
              ],
              "2408.03632v3": [
                "We introduce Concept Conductor, a training-free framework that ensures visual fidelity and correct layout in multi-concept customization."
              ],
              "3394171.3416296": [
                "method for learning product visual representations",
                "deep CNNs pre-trained and fine-tuned on this dataset can enhance feature effectiveness for product image retrieval"
              ],
              "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction": [
                "propose NCO-Spell for CSC task in noisy contexts"
              ],
              "3664647.3680897": [
                "提出了一种零样本短语接地的新框架",
                "引入了三重对齐策略"
              ],
              "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding": [
                "提出了一种改进的Transformer模型TIP",
                "引入了IoU空间位置编码方法"
              ],
              "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding": [
                "use of similarity tokens for spatial information capture",
                "detector-free network fine-tuning"
              ],
              "0592": [
                "a novel topic-oriented captioning model",
                "the FGU design",
                "extensive experimental evaluation"
              ],
              "Dimensionality_reduction_for_text_using_LLE": [
                "introduces LLE for text dimensionality reduction",
                "presents experimental results comparing LLE with LSI"
              ],
              "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification": [
                "A multi-level fusion classification (MFC) model that fuses features from different levels by exploiting their dependencies.",
                "An R-MFC module to address sentiment conflicts in social network posts."
              ],
              "4930_Article_Text_7995_1_10_20190709": [
                "We propose a general DN module and a new DF model for the VQA task."
              ],
              "0628": [
                "Proposing a Multi-scale Two-way Deep Neural Network (MTDNN) for stock trend prediction"
              ],
              "Enhanced_Prompt_Learning_for_Few_shot_Text_Classification_Method": [
                "提出基于提示学习和三元组损失优化的少样本文本分类EPL4FTC算法"
              ],
              "鲁棒局部保持投影的表情识别": [
                "提出了一种鲁棒的局部保持投影算法"
              ],
              "3548636.3548646": [
                "引入BERT模块到C-TextGCN中形成EP-BERTGCN",
                "在收集的数据集上进行比较实验，展示方法性能的优越性",
                "对BERT模型进行领域适应，进一步增强了我们方法的能力"
              ],
              "2647868.2654902": [
                "We propose the correspondence autoencoder (Corr-AE) based on two basic unimodal autoencoders.",
                "The Corr-AE integrates representation and correlation learning into a single process.",
                "A novel loss function is designed, incorporating the losses of autoencoders for all modalities and the loss of correlation between modalities."
              ],
              "2022.findings_emnlp.6": [
                "提出了一种新的KE-GCL模型",
                "在CQA任务中整合了GCL",
                "通过自适应图增强和选择错误的答案来增强图表示学习"
              ],
              "2021.acl_long.494": [
                "提出了一种结合语法和语义特征的DualGCN模型",
                "设计了正交和微分正则化器以增强模型捕获语义关联的能力"
              ],
              "2022.emnlp_main.212": [
                "propose a COntext-Masked MRC (COM-MRC) framework for ASTE tasks",
                "alleviate interference in ASTE tasks",
                "the context augmentation strategy effectively expanding the training corpus"
              ],
              "2022.coling_1.234": [
                "提出了一种结合预训练语言模型和图卷积网络的简单模型"
              ],
              "2808205": [
                "We propose the correspondence autoencoder (Corr-AE), which integrates representation learning and correlation learning into a single process."
              ],
              "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining": [
                "the proposal of InfoLoss for continually pretraining LLMs",
                "the first attempt to mitigate hallucinations in a cross-lingual transfer setting",
                "extensive experiments on twelve benchmarks"
              ],
              "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture": [
                "proposes the BDA to measure similarity in multimodal systems",
                "combines feature extraction and deep neural networks",
                "demonstrates effectiveness in classifying image tags"
              ],
              "1911.09359v1": [
                "A novel method combining both MS and TD properties in financial time-series.",
                "MS feature extraction with convolutional units without predefined parameters.",
                "Fusion of different scale features using a Recurrent Neural Network to capture temporal dependencies."
              ],
              "基于KFD_Isomap的人脸识别": [
                "提出了一种用核Fisher判别函数改进的Isomap算法",
                "该方法用测地线距离矩阵的列向量作为特征，并用核Fisher判别替代多维尺度分析建立最佳投影方向"
              ],
              "使用LDC码的BI_STCM_ID系统中的星座映射分析": [
                "提出了一种基于LDC码的BI—STCM—ID系统星座映射分析方法",
                "简化了在使用LDC码的BI—STCM—ID系统中高位星座映射设计问题"
              ],
              "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis": [
                "proposal of DualGCN architecture",
                "integration of syntactic knowledge through SynGCN and semantic information through SemGCN",
                "orthogonal and differential regularizers"
              ],
              "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension": [
                "deep examination of fine-grained attributes in C-REC",
                "effective sample generation method",
                "robust C-REC framework"
              ],
              "2025.coling_main.22": [
                "propose the CORSA framework",
                "address the impact of irrelevant images",
                "localize condition-related visual regions"
              ],
              "Improving_Image_Paragraph_Captioning_with_Dual_Relations": [
                "提出DualRel模型，明确捕捉空间和语义关系以改进图像段落字幕生成"
              ],
              "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis": [
                "The proposed discriminator is the first to learn modality disentangled representation for text-to-image synthesis, enhancing discrimination of image-text correlation and facilitating image synthesis manipulation."
              ]
            },
            "novelty_claims_by_paper": {
              "多视图有监督的LDA模型": [
                "将集成学习思想引入主题模型中"
              ],
              "基于统计和加权的提高击键认证识别方法(英文)": [
                "uses typing biometrics to enhance login-password authentication"
              ],
              "2022.acl_long.212": [
                "Our proposed framework is EMC-GCN, which addresses Aspect and Opinion Term Co-Extraction (AOTE) and Aspect-Sentiment Pair Extraction (ASPE)",
                "We define ten types of relations between words for the ASTE task"
              ],
              "3394171.3416296": [
                "using product titles to guide the learning of visual features",
                "conversion of product titles into discrete labels for supervised learning"
              ],
              "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction": [
                "multi-character masking strategy",
                "dynamic confusion sets",
                "iterative inference method"
              ],
              "3664647.3680897": [
                "在弱监督下实现零样本接地",
                "使用CLIP进行区域文本对齐"
              ],
              "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding": [
                "TIP模型结合了模态内注意力机制和视觉与空间特征的融合"
              ],
              "0592": [
                "Our TOMS model differs by generating sentences from topics of interest, capturing linguistic distinctions in image descriptions.",
                "The Fusion Gate Unit (FGU) fuses three sources of representations: image, context, and topic."
              ],
              "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification": [
                "The architecture uses convolutional neural networks (CNNs) to extract features in image and text modalities and a bi-directional (Bi) recurrent neural network (RNN) to integrate features from different CNN layers."
              ],
              "4930_Article_Text_7995_1_10_20190709": [
                "原文明确声明的新颖性，如无则为空数组"
              ],
              "0628": [
                "Using wavelet-based and downsampling-based scale information",
                "Achieving state-of-the-art performance on FI-2010 and CSI-2016 datasets"
              ],
              "Enhanced_Prompt_Learning_for_Few_shot_Text_Classification_Method": [
                "将文本分类任务转换为基于自然语言推理的提示学习形式，并引入三元组损失"
              ],
              "鲁棒局部保持投影的表情识别": [
                "利用了带权值的主成分分析算法",
                "在样本数据集X上执行局部鲁棒主成分分析"
              ],
              "3548636.3548646": [
                "原文未明确声明"
              ],
              "2021.acl_long.494": [
                "DualGCN模型的结构设计",
                "正交和微分正则化器的应用"
              ],
              "2022.emnlp_main.212": [
                "COM-MRC’s components work collaboratively",
                "the two-stage inference method reduces interference from other aspects"
              ],
              "2022.coling_1.234": [
                "首次使用GCN直接学习实例之间的包表示"
              ],
              "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture": [
                "the BDA's three components are designed to extract features, stack RBMs, and use a variant autoencoder for discriminative learning"
              ],
              "基于KFD_Isomap的人脸识别": [
                "在人脸识别实验中，该方法性能优于实验采用的其它方法"
              ],
              "3469877.3490585": [
                "We propose Splitting to Tree Decoder (S2TD), a novel tree-structured decoder that models paragraph decoding as top-down binary tree expansion."
              ],
              "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis": [
                "DualGCN architecture",
                "orthogonal and differential regularizers"
              ],
              "Improving_Image_Paragraph_Captioning_with_Dual_Relations": [
                "DualRel模型考虑了特定的语义和空间关系，并使用关系感知交互"
              ]
            },
            "technical_relationships": {
              "多视图有监督的LDA模型": {
                "base_methods": [
                  {
                    "method_name": "LDA",
                    "relationship_type": "结合",
                    "evidence_text": "本文建立在概率主题模型LDA和集成分类方法Softmax混合模型上"
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "S-LDA",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "原文无此信息"
                  },
                  {
                    "method_name": "SBMLR",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "原文无此信息"
                  },
                  {
                    "method_name": "SVM-POL",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "原文无此信息"
                  },
                  {
                    "method_name": "SVM-RBF",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "原文无此信息"
                  },
                  {
                    "method_name": "Fu-L",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "原文无此信息"
                  },
                  {
                    "method_name": "MCa-sLDA",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "原文无此信息"
                  }
                ]
              },
              "基于统计和加权的提高击键认证识别方法(英文)": {
                "base_methods": [
                  {
                    "method_name": "Keystroke Dynamics-Based Authentication (KDA)",
                    "relationship_type": "改进",
                    "evidence_text": "This paper aims to enhance login-password recognition accuracy using biometric characteristics, which are unique and cannot be stolen, lost, or forgotten."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "TOP10 detector",
                    "comparison_result": "TOP10 showing the best improvement with an increase in the number of samples",
                    "evidence_text": "The performance of each detector is analyzed, with TOP10 showing the best improvement with an increase in the number of samples."
                  }
                ]
              },
              "2504.15958v2": {
                "base_methods": [
                  {
                    "method_name": "FLUX.1",
                    "relationship_type": "基于",
                    "evidence_text": "We build upon FLUX.1, where joint attention is computed as:"
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "FreeCustom",
                    "comparison_result": "FreeCustom suffers from severe attribute confusion",
                    "evidence_text": "FreeCustom suffers from severe attribute confusion, while MS-Diffusion and OmniGen lack visual faithfulness for small objects."
                  },
                  {
                    "method_name": "MS-Diffusion",
                    "comparison_result": "MS-Diffusion and OmniGen lack visual faithfulness for small objects",
                    "evidence_text": "FreeCustom suffers from severe attribute confusion, while MS-Diffusion and OmniGen lack visual faithfulness for small objects."
                  },
                  {
                    "method_name": "OmniGen",
                    "comparison_result": "OmniGen lacks visual faithfulness for small objects",
                    "evidence_text": "FreeCustom suffers from severe attribute confusion, while MS-Diffusion and OmniGen lack visual faithfulness for small objects."
                  }
                ]
              },
              "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning": {
                "base_methods": [
                  {
                    "method_name": "MacBERT",
                    "relationship_type": "基于",
                    "evidence_text": "We choose MacBERT, a pre-training model, as our backbone"
                  },
                  {
                    "method_name": "Entailment-based Few-shot Learning (EFL)",
                    "relationship_type": "结合",
                    "evidence_text": "We address this by introducing a template selection mechanism using a masked language model to assess candidate templates"
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "PET",
                    "comparison_result": "EFL is more effective on sentence-pair tasks, while PET is better for single sentence classification",
                    "evidence_text": "The results on the testing datasets of the nine NLP tasks show that the EFL method with automatic template selection outperforms other methods"
                  }
                ]
              },
              "FAIA_372_FAIA230600": {
                "base_methods": [
                  {
                    "method_name": "Machine Reading Comprehension (MRC)",
                    "relationship_type": "基于",
                    "evidence_text": "Our EMRC method is based on Machine Reading Comprehension (MRC)."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "Double-Propagation",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "Our EMRC model is compared with state-of-art baselines, including pipeline methods like Double-Propagation"
                  },
                  {
                    "method_name": "Extract-Classify",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "Our EMRC model is compared with state-of-art baselines, including pipeline methods like Extract-Classify"
                  },
                  {
                    "method_name": "TAS-BERT",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "Our EMRC model is compared with state-of-art baselines, including end-to-end methods like TAS-BERT"
                  },
                  {
                    "method_name": "JET",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "Our EMRC model is compared with state-of-art baselines, including end-to-end methods like JET"
                  },
                  {
                    "method_name": "BARTABSA",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "Generative methods include BARTABSA"
                  },
                  {
                    "method_name": "GAS",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "GAS approaches ABSA tasks in a unified generative framework"
                  },
                  {
                    "method_name": "Paraphrase detection",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "Paraphrase detection aims to jointly detect all sentiment elements in quads"
                  },
                  {
                    "method_name": "Opinion tree generation",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "Opinion tree generation detects all sentiment elements in a tree for a given review sentence"
                  }
                ]
              },
              "2023.acl_long.802": {
                "base_methods": [
                  {
                    "method_name": "bi-lexical dependency parsing",
                    "relationship_type": "改进",
                    "evidence_text": "We propose USSA, a unified 2D table-filling scheme that utilizes 13 relation types."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "RACL-BERT, Head-first, Head-final, Frozen PERIN, TGLS",
                    "comparison_result": "USSA generally outperforms other baselines",
                    "evidence_text": "Table 4 shows that USSA generally outperforms other baselines in terms of Span F1."
                  }
                ]
              },
              "2022.acl_long.212": {
                "base_methods": [
                  {
                    "method_name": "Graph Convolutional Network (GCN)",
                    "relationship_type": "扩展",
                    "evidence_text": "Motivated by CNN, GCN is an efficient variant operating on graphs (Kipf and Welling, 2017)."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "GTS-BERT",
                    "comparison_result": "Our EMC-GCN significantly surpasses GTS-BERT with an average of 1.96% and 2.61% F1-score on D1 and D2, respectively",
                    "evidence_text": "Under the F1 metric, our EMC-GCN model outperforms all pipeline, end-to-end, and MRC-based methods on two groups of datasets."
                  },
                  {
                    "method_name": "BMRC",
                    "comparison_result": "The term 'light' is challenging to identify by GTS-BERT and BMRC, yet 'easy' is predicted correctly by all methods",
                    "evidence_text": "The term 'light' is challenging to identify by GTS-BERT and BMRC, yet 'easy' is predicted correctly by all methods due to its closer proximity to 'transport' than 'light'."
                  }
                ]
              },
              "3664647.3681466": {
                "base_methods": [
                  {
                    "method_name": "Stable Diffusion",
                    "relationship_type": "基于",
                    "evidence_text": "Harmony-VAE is a method designed for image harmonization, which includes repairing the content of the foreground area while maintaining a harmonized appearance. It builds upon the Stable Diffusion model"
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "HDNet",
                    "comparison_result": "DiffHarmony++ shows superior performance as the foreground proportion increases.",
                    "evidence_text": "On the iHarmony4 dataset, HDNet512 outperforms DiffHarmony++ in samples with small foreground proportions (0% ∼5%), but DiffHarmony++ shows superior performance as the foreground proportion increases."
                  }
                ]
              },
              "2408.03632v3": {
                "base_methods": [
                  {
                    "method_name": "Stable Diffusion",
                    "relationship_type": "改进",
                    "evidence_text": "Our method is implemented on Stable Diffusion v1.5, utilizing layout references from SDXL."
                  },
                  {
                    "method_name": "ED-LoRA",
                    "relationship_type": "结合",
                    "evidence_text": "ED-LoRA, a method for single-concept customization, is used as our default single-concept model, involving learnable hierarchical text embeddings and low-rank adaptation (LoRA) applied to pre-trained weights."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "Custom Diffusion",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "We compare our method against three baselines: Custom Diffusion, Cones 2, and Mix-of-Show, using their official code implementations."
                  },
                  {
                    "method_name": "Cones 2",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "We compare our method against three baselines: Custom Diffusion, Cones 2, and Mix-of-Show, using their official code implementations."
                  },
                  {
                    "method_name": "Mix-of-Show",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "We compare our method against three baselines: Custom Diffusion, Cones 2, and Mix-of-Show, using their official code implementations."
                  }
                ]
              },
              "3394171.3416296": {
                "base_methods": [
                  {
                    "method_name": "Deep Convolutional Neural Networks (CNNs)",
                    "relationship_type": "基于",
                    "evidence_text": "Our method, detailed in Figure 2, addresses this by leveraging the titles of product images."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "SEResnet-152 and Densenet-201",
                    "comparison_result": "The deepest model, Densenet-201, shows the best performance among the ImageNet-1k trained models, while the shallowest model, ResNet-50, performs the worst",
                    "evidence_text": "Table 1 presents the results of five deep CNN models with four different pooling methods on the validation set."
                  }
                ]
              },
              "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction": {
                "base_methods": [
                  {
                    "method_name": "BERT",
                    "relationship_type": "基于",
                    "evidence_text": "BERT-based models, dominant in CSC research, face performance challenges with noisy contexts."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "BERT",
                    "comparison_result": "NCO-Spell outperforms compared baseline models.",
                    "evidence_text": "Main results in Table IV show improvements, especially for PLOME(cfs) and NCO-Spell."
                  },
                  {
                    "method_name": "PLOME",
                    "comparison_result": "NCO-Spell shows better performance in certain conditions.",
                    "evidence_text": "Main results in Table IV show improvements, especially for PLOME(cfs) and NCO-Spell."
                  }
                ]
              },
              "978_3_642_23223_7_60": {
                "base_methods": [
                  {
                    "method_name": "KNN",
                    "relationship_type": "结合",
                    "evidence_text": "K-Nearest Neighbors (KNN) and Singular Value Decomposition (SVD) are two classification methods used in collaborative filtering."
                  },
                  {
                    "method_name": "SVD",
                    "relationship_type": "结合",
                    "evidence_text": "K-Nearest Neighbors (KNN) and Singular Value Decomposition (SVD) are two classification methods used in collaborative filtering."
                  },
                  {
                    "method_name": "logistic regression",
                    "relationship_type": "结合",
                    "evidence_text": "We apply logistic regression to build a classifier using the generated feature vectors."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "KNN",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "原文无此信息"
                  },
                  {
                    "method_name": "SVD",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "原文无此信息"
                  },
                  {
                    "method_name": "logistic regression",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "原文无此信息"
                  },
                  {
                    "method_name": "combination",
                    "comparison_result": "The combined classifier outperforms single classifiers.",
                    "evidence_text": "The combined classifier outperforms single classifiers."
                  }
                ]
              },
              "3664647.3680897": {
                "base_methods": [
                  {
                    "method_name": "CLIP",
                    "relationship_type": "基于",
                    "evidence_text": "Our framework uses a novel PG framework using triple alignment strategies under weak supervision: 1) Region-Text Alignment (RTA) to associate region-level attributes based on Contrastive Language-Image Pre-Training (CLIP)."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "WWbl",
                    "comparison_result": "Our CLIP-based heatmap surpasses the pseudo label used by WWbl, explaining a 9% increase in bbox accuracy.",
                    "evidence_text": "Our CLIP-based heatmap surpasses the pseudo label used by WWbl, explaining a 9% increase in bbox accuracy."
                  },
                  {
                    "method_name": "ZSGNet",
                    "comparison_result": "Our approach exceeds previous methods and will explore interpretable solutions for grounding-related tasks.",
                    "evidence_text": "Our approach exceeds previous methods and will explore interpretable solutions for grounding-related tasks."
                  }
                ]
              },
              "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding": {
                "base_methods": [
                  {
                    "method_name": "Neural Image Captioning (NIC)",
                    "relationship_type": "基于",
                    "evidence_text": "The Neural Image Captioning (NIC) model uses a convolutional neural network to extract image features and an LSTM to translate these into sentences."
                  },
                  {
                    "method_name": "Transformer",
                    "relationship_type": "改进",
                    "evidence_text": "To address this, we adopt the transformer structure as the decoder."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "CoordNorm(hw), Coord(hw), Coord",
                    "comparison_result": "IoUc and IoU+ models further enhance performance",
                    "evidence_text": "Table III presents the performance of different position encoding methods. The CoordNorm(hw) model shows improvements over the Coord(hw) and Coord models, indicating the effectiveness of normalization."
                  }
                ]
              },
              "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding": {
                "base_methods": [
                  {
                    "method_name": "Weakly-Supervised Grounding (WSG)",
                    "relationship_type": "改进",
                    "evidence_text": "We propose a refinement-based approach using a detector-free phrase grounding model fine-tuned with a visual prompt from CLIP text-related representations."
                  },
                  {
                    "method_name": "CLIP",
                    "relationship_type": "结合",
                    "evidence_text": "Our approach leverages the relationship between CLIP and the grounding model, refining training through visual prompt tuning."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "WWbl",
                    "comparison_result": "Our method shows improvements in metrics on Flickr30K and ReferIt",
                    "evidence_text": "Our method is trained on COCO and VG train splits and evaluated on the test splits of Flickr30K, VG, and ReferIt. The performance of our method is compared with state-of-the-art DF-WSG methods quantitatively and qualitatively."
                  },
                  {
                    "method_name": "Gbs",
                    "comparison_result": "Our approach not only surpasses detector-free methods like Gbs and WWbl",
                    "evidence_text": "Our approach not only surpasses detector-free methods like Gbs and WWbl but also outperforms detector-based methods on almost all categories for Flickr30K Entities."
                  }
                ]
              },
              "0592": {
                "base_methods": [
                  {
                    "method_name": "Single-sentence (SS) captioning models",
                    "relationship_type": "基于",
                    "evidence_text": "Single-sentence (SS) captioning models have been developed using templates and neural network approaches, but they often provide incomplete descriptions."
                  },
                  {
                    "method_name": "MS captioning methods",
                    "relationship_type": "基于",
                    "evidence_text": "MS captioning methods generate multiple sentences for a complete depiction, with some focusing on regions of interest."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "NIC",
                    "comparison_result": "Our TOMS demonstrates improved performance, especially in terms of IC.",
                    "evidence_text": "Our proposed Topic-Oriented Multi-Sentence (TOMS) captioning model incorporates topic embedding to guide the generation process... Our TOMS demonstrates improved performance, especially in terms of IC."
                  },
                  {
                    "method_name": "ATT-FCN",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "We compare our TOMS model with NIC and ATT-FCN for sentence level MS captioning"
                  },
                  {
                    "method_name": "RTT-GAN",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "We compare our TOMS model with NIC and ATT-FCN for sentence level MS captioning and with RTT-GAN, Regions-Hierarchical, and Sentence-Concat for paragraph level MS captioning."
                  },
                  {
                    "method_name": "Regions-Hierarchical",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "We compare our TOMS model with NIC and ATT-FCN for sentence level MS captioning and with RTT-GAN, Regions-Hierarchical, and Sentence-Concat for paragraph level MS captioning."
                  },
                  {
                    "method_name": "Sentence-Concat",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "We compare our TOMS model with NIC and ATT-FCN for sentence level MS captioning and with RTT-GAN, Regions-Hierarchical, and Sentence-Concat for paragraph level MS captioning."
                  }
                ]
              },
              "Dimensionality_reduction_for_text_using_LLE": {
                "base_methods": [
                  {
                    "method_name": "Locally Linear Embedding (LLE)",
                    "relationship_type": "基于",
                    "evidence_text": "This paper introduces LLE, analyzes its advantages and limitations, discusses its relationship with latent semantic indexing (LSI) within the graph embedding framework, and presents experimental results using Reuters21578 and TDT2 datasets."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "LSI",
                    "comparison_result": "LLE significantly outperforms the other methods",
                    "evidence_text": "In terms of classification precision, LLE significantly outperforms the other methods."
                  }
                ]
              },
              "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification": {
                "base_methods": [
                  {
                    "method_name": "CNNs and RNNs",
                    "relationship_type": "基于",
                    "evidence_text": "Previous work utilized high-level features for fusion, such as the last outputs of different modal model layers."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "Image CNN",
                    "comparison_result": "R-MFC shows improved performance",
                    "evidence_text": "The MFC model shows improved performance over several baseline methods, with the ITIGNN method achieving the best results."
                  },
                  {
                    "method_name": "Text CNN",
                    "comparison_result": "R-MFC shows improved performance",
                    "evidence_text": "The MFC model shows improved performance over several baseline methods, with the ITIGNN method achieving the best results."
                  },
                  {
                    "method_name": "ITIGNN",
                    "comparison_result": "MFC method employing fewer parameters outperforms ITIGNN",
                    "evidence_text": "We propose that our MFC method, employing fewer parameters, outperforms ITIGNN, which uses a pretrained CNN combined with graph neural networks for textual and visual analysis."
                  }
                ]
              },
              "4930_Article_Text_7995_1_10_20190709": {
                "base_methods": [
                  {
                    "method_name": "Attention-based models",
                    "relationship_type": "基于",
                    "evidence_text": "We review current VQA models, focusing on attention-based models and fusion strategies."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "HighOrderAtt",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "HighOrderAtt (Schwartz, Schwing, and Hazan 2017) - - - - 69.4"
                  },
                  {
                    "method_name": "MLB(7)",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "MLB(7) (Kim et al. 2017) 66.77 84.54 39.21 57.81"
                  },
                  {
                    "method_name": "Mutan(5)",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "Mutan(5) (Ben-younes et al. 2017) 67.42 85.14 39.81 58.52"
                  },
                  {
                    "method_name": "DualMFA",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "DualMFA (Lu et al. 2018) 66.01 83.59 40.18 56.84 70.04"
                  },
                  {
                    "method_name": "ReasonNet",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "ReasonNet (Ilievski and Feng 2017) - - - - -"
                  }
                ]
              },
              "0628": {
                "base_methods": [
                  {
                    "method_name": "Support Vector Machine and Neural Networks",
                    "relationship_type": "基于",
                    "evidence_text": "STP is a classification task traditionally tackled by Support Vector Machine and Neural Networks."
                  },
                  {
                    "method_name": "Ensemble-based methods like Random Forest and deep learning models",
                    "relationship_type": "基于",
                    "evidence_text": "Ensemble-based methods like Random Forest and deep learning models have also been explored."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "XGBoost",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "The data is treated as a non-stationary discrete signal and decomposed using DWT to obtain transformed multi-scale components. These are concatenated and input to an XGBoost model to ensemble multi-scale information and output category scores."
                  },
                  {
                    "method_name": "CNN",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "A key operation concatenates these features, and a GRU unit temporally cascades the information to output categories."
                  },
                  {
                    "method_name": "RNN",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "A key operation concatenates these features, and a GRU unit temporally cascades the information to output categories."
                  },
                  {
                    "method_name": "RCNN",
                    "comparison_result": "RCNN shows the best performance on most indices",
                    "evidence_text": "In the multi-scale rows, variations are fed with DWT-based, downsampling-based, and CNN multi-kernel size scale-information. RCNN shows the best performance on most indices, while XGBoost is less effective."
                  }
                ]
              },
              "Enhanced_Prompt_Learning_for_Few_shot_Text_Classification_Method": {
                "base_methods": [
                  {
                    "method_name": "基于度量学习的方法和基于提示学习的方法",
                    "relationship_type": "基于",
                    "evidence_text": "本文提出的方法与基于度量学习的方法和基于提示学习的方法密切相关。"
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "对比基线方法",
                    "comparison_result": "EPL4FTC方法的准确度明显优于对比基线方法",
                    "evidence_text": "实验评估表明EPL4FTC方法的准确度明显优于对比基线方法。"
                  }
                ]
              },
              "考虑合成灰数灰度性质的改进区间灰数预测模型": {
                "builds_upon": [
                  "基于核和灰度的区间灰数预测模型"
                ],
                "extends": [
                  "扩展了原有模型，支持误差分析和精度检验"
                ],
                "relates_to": [
                  "与区间灰数序列预测相关"
                ]
              },
              "鲁棒局部保持投影的表情识别": {
                "base_methods": [
                  {
                    "method_name": "局部保持投影",
                    "relationship_type": "改进",
                    "evidence_text": "针对局部保持投影的流形学习算法对于噪声与异常值的敏感性，提出了一种鲁棒的局部保持投影算法。"
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "Gabor小波特征提取方法",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "以Gabor小波为基准测试了算法性能。"
                  }
                ]
              },
              "3548636.3548646": {
                "base_methods": [
                  {
                    "method_name": "C-TextGCN",
                    "relationship_type": "改进",
                    "evidence_text": "Our work spans text-based power equipment fault recognition, graph neural networks, pre-trained BERT, and cross-domain BERT adaption. Previous research in power equipment fault recognition has largely focused on image modalities, with recent inroads into text-based models for TPFR. Graph Convolutional Neural Network (GCN) has been applied to text classification by constructing graphs based on textual relationships. This approach is extended with the integration of pre-trained BERT and domain-specific adaption in our EP-BERTGCN method."
                  },
                  {
                    "method_name": "BERT",
                    "relationship_type": "结合",
                    "evidence_text": "EP-BERTGCN constructs a graph among documents and words using pre-trained BERT and combines softmax outputs from BERT and GCNs for classification."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "TextRNN",
                    "comparison_result": "EP-BERTGCN性能更优",
                    "evidence_text": "Table 1 presents Experimental Results on the CPTF Dataset: ... EP-BERTGCN 0.6700 ... compared to TextRNN 0.4950"
                  },
                  {
                    "method_name": "TextRNN_Att",
                    "comparison_result": "EP-BERTGCN性能更优",
                    "evidence_text": "Table 1 presents Experimental Results on the CPTF Dataset: ... EP-BERTGCN 0.6700 ... compared to TextRNN_Att 0.5538"
                  },
                  {
                    "method_name": "TextRCNN",
                    "comparison_result": "EP-BERTGCN性能更优",
                    "evidence_text": "Table 1 presents Experimental Results on the CPTF Dataset: ... EP-BERTGCN 0.6700 ... compared to TextRCNN 0.6126"
                  },
                  {
                    "method_name": "DPCNN",
                    "comparison_result": "EP-BERTGCN性能更优",
                    "evidence_text": "Table 1 presents Experimental Results on the CPTF Dataset: ... EP-BERTGCN 0.6700 ... compared to DPCNN 0.5753"
                  },
                  {
                    "method_name": "FastText",
                    "comparison_result": "EP-BERTGCN性能更优",
                    "evidence_text": "Table 1 presents Experimental Results on the CPTF Dataset: ... EP-BERTGCN 0.6700 ... compared to FastText 0.6098"
                  },
                  {
                    "method_name": "Transformer",
                    "comparison_result": "EP-BERTGCN性能更优",
                    "evidence_text": "Table 1 presents Experimental Results on the CPTF Dataset: ... EP-BERTGCN 0.6700 ... compared to Transformer 0.4570"
                  },
                  {
                    "method_name": "C-TextGCN",
                    "comparison_result": "EP-BERTGCN性能更优",
                    "evidence_text": "Table 1 presents Experimental Results on the CPTF Dataset: ... EP-BERTGCN 0.6700 ... compared to C-TextGCN 0.6607"
                  }
                ]
              },
              "2647868.2654902": {
                "base_methods": [
                  {
                    "method_name": "Correspondence LDA, deep autoencoders, deep belief networks, and deep Boltzmann Machines",
                    "relationship_type": "基于",
                    "evidence_text": "Shared layer models include topic models like Correspondence LDA and deep architectures such as deep autoencoders, deep belief networks, and deep Boltzmann Machines."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "CCA-based models and multi-modal models",
                    "comparison_result": "Our correspondence autoencoders significantly outperform other models",
                    "evidence_text": "Our correspondence autoencoders significantly outperform other models on both retrieval tasks across Wikipedia, Pascal, and NUS-WIDE-10k data sets."
                  }
                ]
              },
              "2022.findings_emnlp.6": {
                "base_methods": [
                  {
                    "method_name": "Graph Contrastive Learning",
                    "relationship_type": "扩展",
                    "evidence_text": "Graph contrastive learning extends contrastive learning to graph-structured data, with works focusing on unsupervised representation learning."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "RoBERTa-Large (w/o KG)",
                    "comparison_result": "KE-GCL consistently outperforms previous methods",
                    "evidence_text": "Our KE-GCL model consistently outperforms other baselines on CommonsenseQA and OpenBookQA datasets."
                  },
                  {
                    "method_name": "Relation Network (RN), RGCN, GconAttn, KagNet, MHGRN, QA-GNN",
                    "comparison_result": "KE-GCL consistently outperforms previous methods",
                    "evidence_text": "We compare our KE-GCL with state-of-the-art baselines: RoBERTa-Large (w/o KG), Relation Network (RN), RGCN, GconAttn, KagNet, MHGRN, and QA-GNN."
                  }
                ]
              },
              "2021.acl_long.494": {
                "base_methods": [
                  {
                    "method_name": "Graph Convolutional Network (GCN)",
                    "relationship_type": "改进",
                    "evidence_text": "We propose a GCN-based method that combines syntactic and semantic features."
                  },
                  {
                    "method_name": "Attention-based LSTM",
                    "relationship_type": "改进",
                    "evidence_text": "Attention-based neural networks, including those proposed by Wang et al. (2016) and others, have been introduced to implicitly model the semantic relation between aspects and their context."
                  },
                  {
                    "method_name": "Recursive neural network by Dong et al. (2014)",
                    "relationship_type": "改进",
                    "evidence_text": "Notable works include the recursive neural network by Dong et al. (2014)"
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "ATAE-LSTM, IAN, RAM, MGAN, TNet, ASGCN, CDT, BiGCN, kumaGCN, InterGCN, R-GAT, DGEDT, BERT, R-GAT+BERT, DGEDT+BERT",
                    "comparison_result": "Our DualGCN model consistently outperforms attention-based and syntax-based methods",
                    "evidence_text": "We compare DualGCN with state-of-the-art baselines, including ATAE-LSTM, IAN, RAM, MGAN, TNet, ASGCN, CDT, BiGCN, kumaGCN, InterGCN, R-GAT, DGEDT, BERT, R-GAT+BERT, and DGEDT+BERT."
                  }
                ]
              },
              "2022.emnlp_main.212": {
                "base_methods": [
                  {
                    "method_name": "Aspect Sentiment Triplet Extraction (ASTE)",
                    "relationship_type": "基于",
                    "evidence_text": "Aspect Sentiment Triplet Extraction (ASTE) extracts sentiment triplets from sentences and is formalized as an effective machine reading comprehension (MRC) framework."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "BMRC",
                    "comparison_result": "Our COM-MRC outperforms other baselines in terms of Precision, Recall, and F1 scores on both D1 and D2 datasets",
                    "evidence_text": "Our COM-MRC outperforms other baselines in terms of Precision, Recall, and F1 scores on both D1 and D2 datasets, as shown in Tables 3 and 4."
                  },
                  {
                    "method_name": "EMC-GCN",
                    "comparison_result": "Our COM-MRC outperforms EMC-GCN",
                    "evidence_text": "Our COM-MRC outperforms EMC-GCN on datasets D1 and D2."
                  }
                ]
              },
              "2022.coling_1.234": {
                "base_methods": [
                  {
                    "method_name": "BERT-based Graph Convolutional network Model (BGM)",
                    "relationship_type": "结合",
                    "evidence_text": "We introduce BGM, a model that combines a Pretrained Language Model (PLM) and a Graph Convolutional Network (GCN)"
                  },
                  {
                    "method_name": "PCNN-based methods",
                    "relationship_type": "基于",
                    "evidence_text": "DS-RE methods can be categorized into PCNN-based and PLMs-based approaches"
                  },
                  {
                    "method_name": "PLMs-based methods",
                    "relationship_type": "基于",
                    "evidence_text": "PCNN-based methods use various techniques to acquire effective bag representations, while PLMs-based methods have shown remarkable performance"
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "multiple baseline methods",
                    "comparison_result": "report better performance",
                    "evidence_text": "We compare our BGM with multiple baseline methods and report better performance"
                  },
                  {
                    "method_name": "BGM without GCN",
                    "comparison_result": "performance drop",
                    "evidence_text": "In the ablation study, we find that removing GCN results in a performance drop"
                  },
                  {
                    "method_name": "BGM without EntCon",
                    "comparison_result": "performance drop",
                    "evidence_text": "In the ablation study, we find that removing entity connections results in a performance drop"
                  }
                ]
              },
              "2808205": {
                "base_methods": [
                  {
                    "method_name": "autoencoder",
                    "relationship_type": "改进",
                    "evidence_text": "We propose several novel models based on different autoencoders, which correlate hidden representations of a pair of autoencoders."
                  },
                  {
                    "method_name": "Canonical Correlation Analysis (CCA)",
                    "relationship_type": "结合",
                    "evidence_text": "The models are trained using a novel optimal objective that minimizes a linear combination of representation learning errors and correlation learning error."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "CCA-based models",
                    "comparison_result": "Our models achieve substantial improvements in mAP scores.",
                    "evidence_text": "Compared to CCA-AE, our Corr-AE enhances the average mAP by 53.6%, 81.5%, and 48.3% on the respective datasets."
                  },
                  {
                    "method_name": "Bimodal AE, Bimodal DBN",
                    "comparison_result": "Our Corr-AE also achieves notable improvements over Bimodal AE and Bimodal DBN.",
                    "evidence_text": "Our Corr-AE also achieves notable improvements over Bimodal AE and Bimodal DBN."
                  }
                ]
              },
              "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining": {
                "base_methods": [
                  {
                    "method_name": "Cross-lingual Continual Pretraining",
                    "relationship_type": "扩展",
                    "evidence_text": "Cross-Lingual Continual Pretraining addresses the limitation of LLMs [1, 2, 3, 4] in languages other than English."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "traditional cross-entropy loss",
                    "comparison_result": "InfoLoss aims to avoid learning incorrect language distributions caused by noisy tokens",
                    "evidence_text": "We compare our method, InfoLoss, with the traditional cross-entropy loss for training Llama 2."
                  }
                ]
              },
              "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture": {
                "base_methods": [
                  {
                    "method_name": "deep neural learning",
                    "relationship_type": "基于",
                    "evidence_text": "Deep neural learning, inspired by biological propagation phenomena in the human brain, has seen rapid development since 2006 and has achieved success in tasks involving single modal data."
                  },
                  {
                    "method_name": "topic models, joint models, undirected Markov random fields",
                    "relationship_type": "扩展",
                    "evidence_text": "Various approaches have been proposed for learning from cross-modal data. These include extensions of topic models, joint models, and undirected Markov random fields, but these single-hidden-layer models struggle with the complexity of images and text."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "Multilayer Perceptrons (MLP)",
                    "comparison_result": "Our deep neural architecture outperforms an MLP-based system",
                    "evidence_text": "Our deep neural architecture outperforms an MLP-based system with two hidden layers and a CCA-based system with two RBMs, achieving an accuracy of 88.96%."
                  },
                  {
                    "method_name": "Canonical Correlation Analysis (CCA)",
                    "comparison_result": "The CCA-based system performs better than the MLP-based system",
                    "evidence_text": "The CCA-based system performs better than the MLP-based system."
                  }
                ]
              },
              "1911.09359v1": {
                "base_methods": [
                  {
                    "method_name": "Multi-Scale (MS) methods, TD-oriented methods",
                    "relationship_type": "结合",
                    "evidence_text": "Existing FTC research can be categorized into MS-oriented and TD-oriented methods. However, few studies effectively integrate both properties."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "Support Vector Machine (SVM), Random Forest (RF), Fuzzy Deep Neural Network (FDNN), TreNet, State-Frequency Memory Recurrent Neural Networks (SFM), Multi-Scale CNN (MS-CNN)",
                    "comparison_result": "Our MSTD-RCNN model is compared with six baseline models on three datasets. The results, listed in Table 5, show that our model achieves the best performance in accuracy and F1.",
                    "evidence_text": "Our MSTD-RCNN model is compared with six baseline models on three datasets."
                  }
                ]
              },
              "基于KFD_Isomap的人脸识别": {
                "base_methods": [
                  {
                    "method_name": "Isomap",
                    "relationship_type": "改进"
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "Isomap",
                    "comparison_result": "KFD-Isomap在识别率上有较大提高"
                  },
                  {
                    "method_name": "Ext.Isomap",
                    "comparison_result": "KFD-Isomap方法中判别分析的引入使得识别率提高"
                  },
                  {
                    "method_name": "特征脸",
                    "comparison_result": "具有类似的识别效果"
                  },
                  {
                    "method_name": "Fisher脸",
                    "comparison_result": "KFD-Isomap方法性能优于Fisher脸算法"
                  }
                ]
              },
              "3469877.3490585": {
                "base_methods": [
                  {
                    "method_name": "hierarchical and non-hierarchical decoders",
                    "relationship_type": "基于",
                    "evidence_text": "Existing approaches, such as hierarchical and non-hierarchical decoders, have limitations in managing visual observations and maintaining paragraph coherence."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "DAM-ATT, SCST, SCST+RP, CRL, OR-ATT, OR-ATT+RP",
                    "comparison_result": "Our S2TD achieves competitive performance, especially in terms of BLEU-1 and CIDEr.",
                    "evidence_text": "Performance evaluation compares S2TD with state-of-the-art sequential decoding methods, grouped into hierarchical and non-hierarchical methods."
                  }
                ]
              },
              "使用LDC码的BI_STCM_ID系统中的星座映射分析": {
                "base_methods": [
                  {
                    "method_name": "BI-STCM—ID系统",
                    "relationship_type": "基于"
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "Alamouti空时编码方案",
                    "comparison_result": "原文无此信息"
                  },
                  {
                    "method_name": "OSTBC码的BI—STCM—ID系统",
                    "comparison_result": "原文无此信息"
                  }
                ]
              },
              "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis": {
                "base_methods": [
                  {
                    "method_name": "GCN",
                    "relationship_type": "改进",
                    "evidence_text": "Incorporating syntactic dependency structure with graph convolutional networks (GCNs) has shown advantages, but performance depends on dependency parsers."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "ATAE-LSTM",
                    "comparison_result": "DualGCN model consistently outperforms attention-based and syntax-based methods",
                    "evidence_text": "The DualGCN model consistently outperforms attention-based and syntax-based methods on Restaurant14, Laptop14 and Twitter datasets."
                  },
                  {
                    "method_name": "GCAE",
                    "comparison_result": "DualGCN model outperforms",
                    "evidence_text": "DualGCN model outperforms SynGCN-head on the Restaurant14 and Laptop14 datasets"
                  },
                  {
                    "method_name": "R-GAT + BERT",
                    "comparison_result": "DualGCN + BERT and DualGCN + BERT-PT show improved performance over the basic DualGCN model and other BERT-based methods",
                    "evidence_text": "DualGCN + BERT and DualGCN + BERT-PT show improved performance over the basic DualGCN model and other BERT-based methods."
                  }
                ]
              },
              "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension": {
                "base_methods": [
                  {
                    "method_name": "ReferItGame",
                    "relationship_type": "基于",
                    "evidence_text": "Our C-REC samples are based on fine-grained attributes from referring expressions, inspired by the ReferItGame [18]."
                  },
                  {
                    "method_name": "BERT",
                    "relationship_type": "结合",
                    "evidence_text": "Candidate word prediction using BERT"
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "SCRE",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "Related work on counterfactual REC includes approaches like SCRE"
                  },
                  {
                    "method_name": "MTG",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "and MTG, which treat C-REC as a matching task based on logical rules."
                  },
                  {
                    "method_name": "SimREC",
                    "comparison_result": "our model outperforms one-stage REC models, particularly SimREC",
                    "evidence_text": "Table 4 shows that our model outperforms one-stage REC models, particularly SimREC [29]."
                  }
                ]
              },
              "2025.coling_main.22": {
                "base_methods": [
                  {
                    "method_name": "Traditional methods",
                    "relationship_type": "基于",
                    "evidence_text": "Traditional methods assume the image contains objects referred to by the aspects, which is not always true."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "ChatGPT 3.5, Llama 2, VisualGLM-6B, Llava 1.5, MMICL, mPLUG-Owl2, GPT4V",
                    "comparison_result": "our model outperforms",
                    "evidence_text": "Comparative experiments with large language models (LLMs) and multi-modal LLMs (MLLMs) on the JMASA and MASC tasks demonstrate that our model outperforms ChatGPT 3.5, Llama 2, VisualGLM-6B, Llava 1.5, MMICL, mPLUG-Owl2, and GPT4V, showcasing the advantage of multimodal input and the effectiveness of our approach."
                  }
                ]
              },
              "Improving_Image_Paragraph_Captioning_with_Dual_Relations": {
                "base_methods": [
                  {
                    "method_name": "Faster R-CNN",
                    "relationship_type": "基于",
                    "evidence_text": "We employ Faster R-CNN [19] to detect N objects in an image, represented as C={c1, · · · , cN}."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "Regions-Hierarchical",
                    "comparison_result": "DualRel outperforms SCST on all metrics (except for a tie in METEOR) and the recent IMAP method.",
                    "evidence_text": "Baselines: We compare DualRel with baselines including Regions-Hierarchical [1], RTT-GAN [5], DAM [7], SCST [8], DCPG-VAE [6], TMOS [27], CAE-LSTM [11], DHPV [9], CVAP [10], CRL [12], Dual-CNN [15], VREN [17], IMAP [14], S2TD [16], and OR-ATT [18]."
                  },
                  {
                    "method_name": "SCST",
                    "comparison_result": "DualRel outperforms SCST on all metrics (except for a tie in METEOR)",
                    "evidence_text": "Results: Table 1 shows our DualRel method achieves the best scores in B@{1-4} and CIDEr. DualRel outperforms SCST on all metrics (except for a tie in METEOR) and the recent IMAP method."
                  }
                ]
              },
              "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis": {
                "base_methods": [
                  {
                    "method_name": "AttnGAN",
                    "relationship_type": "改进",
                    "evidence_text": "The modality disentangled discriminator is integrated into two models: AttnGAN and DM-GAN."
                  },
                  {
                    "method_name": "DM-GAN",
                    "relationship_type": "改进",
                    "evidence_text": "The modality disentangled discriminator is integrated into two models: AttnGAN and DM-GAN."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "GAN-INT-CLS",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "In terms of style manipulation experiments on the CUB dataset, two types of experiments are conducted. The first involves style transfer, where the model uses modality-specific features for direct style transfer, achieving more accurate style reconstruction compared to GAN-INT-CLS."
                  }
                ]
              }
            }
          },
          "collaboration_raw": {
            "collaborators_by_paper": {
              "一种基于人工智能的数据安全风险监测追溯系统_黄永军": [
                "周春楠",
                "李睿凡",
                "孙健",
                "黄永军"
              ],
              "多视图有监督的LDA模型": [
                "冯方向",
                "王小捷",
                "李睿凡",
                "曹洁",
                "李晓旭"
              ],
              "基于统计和加权的提高击键认证识别方法(英文)": [
                "Li Jian",
                "Li Ruifan",
                "Guo Xiaojing",
                "Li Meiyun"
              ],
              "2504.15958v2": [
                "Fangxiang Feng",
                "Chen Wei",
                "Xiaojie Wang",
                "Zebin Yao",
                "Huixing Jiang",
                "Ruifan Li",
                "Lei Ren"
              ],
              "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning": [
                "Zhanyu Ma",
                "Lihui Zhang",
                "Zhiyu Wei",
                "Zeyuan Wang",
                "Ruifan Li"
              ],
              "1307.1275v1": [
                "Fangxiang Feng",
                "Xiaojie Wang",
                "Ruifan Li"
              ],
              "FAIA_372_FAIA230600": [
                "Ruifan Li",
                "Shuqin Ye",
                "Zepeng Zhang"
              ],
              "2023.acl_long.802": [
                "Zepeng Zhai",
                "Hao Chen",
                "Xiaojie Wang",
                "Ruifan Li"
              ],
              "3191835.3191989": [
                "Ding Yuan",
                "Peng Lu",
                "Yanquan Zhou",
                "Ruifan Li"
              ],
              "2022.acl_long.212": [
                "Fangxiang Feng",
                "Xiaojie Wang",
                "Zepeng Zhai",
                "Hao Chen",
                "Ruifan Li"
              ],
              "3664647.3681466": [
                "Fangxiang Feng",
                "Xiaojie Wang",
                "Ruifan Li",
                "Guang Liu",
                "Pengfei Zhou"
              ],
              "2408.03632v3": [
                "Fangxiang Feng",
                "Xiaojie Wang",
                "Zebin Yao",
                "Ruifan Li"
              ],
              "3394171.3416296": [
                "Fangxiang Feng",
                "Xiaojie Wang",
                "Tianrui Niu",
                "Huixing Jiang",
                "Ruifan Li"
              ],
              "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction": [
                "Guangwei Zhang",
                "Ruifan Li",
                "Yongping Xiong"
              ],
              "一种图像检索方法_鲁鹏": [
                "刘咏彬",
                "王小捷",
                "袁彩霞",
                "鲁鹏",
                "李睿凡"
              ],
              "Designing_a_Japanese_idiom_education_support_system_for_overseas_students": [
                "Ruifan LI",
                "KONISHI Yusuke",
                "Fuji REN"
              ],
              "基于采集搜索引擎数据的隐私信息评级方法_芦效峰": [
                "刘咏彬",
                "李晖",
                "李蕾",
                "袁彩霞",
                "芦效峰",
                "曲昭伟",
                "鲁鹏",
                "李睿凡"
              ],
              "978_3_642_23223_7_60": [
                "Li Ruifan",
                "He Chuan",
                "Zhong Yixin"
              ],
              "3664647.3680897": [
                "Fangxiang Feng",
                "Zhanyu Ma",
                "Xiaojie Wang",
                "Ruifei Zhang",
                "Zhihong Chen",
                "Guanbin Li",
                "Yuzhe Ji",
                "Zhihan Yu",
                "Yibing Song",
                "Pengyue Lin",
                "Ruifan Li",
                "Xiang Wan"
              ],
              "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding": [
                "Yazhou Li",
                "Yihui Shi",
                "Zhanyu Ma",
                "Yun Liu",
                "Ruifan Li"
              ],
              "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding": [
                "Fangxiang Feng",
                "Xiaojie Wang",
                "Mingcong Lu",
                "Zhihan Yu",
                "Pengyue Lin",
                "Ruifan Li"
              ],
              "引入深度学习的人工智能类课程": [
                "钟义信",
                "李睿凡",
                "王小捷"
              ],
              "智能科学技术导论教学目的及策略": [
                "焦晨晨",
                "周延泉",
                "李睿凡"
              ],
              "Improved+Eavesdropping+Detection+Strategy+Based+on+Extended+Three_particle+Greenberger_Horne_Zeilinger+State+in+Two_step+Quantum+Direct+Communication+Protocol": [
                "LU Xiaofeng",
                "LI Jian",
                "YE Xinxin",
                "LI Ruifan",
                "ZOU Yongzhong"
              ],
              "2820400": [
                "Fangxiang Feng",
                "George Toderici",
                "Xiaojie Wang",
                "Yelin Kim",
                "Emily Mower-Provost",
                "Hayley Hung",
                "Ruifan Li"
              ],
              "0592": [
                "Xiaojie Wang",
                "Ruifan Li",
                "Chang Zhou",
                "Yuzhao Mao"
              ],
              "基于词性和位置的特征关键词提取方法_芦效峰": [
                "王文婷",
                "芦效峰",
                "李睿凡"
              ],
              "Dimensionality_reduction_for_text_using_LLE": [
                "Yixin ZHONG",
                "Chuan HE",
                "Zhe DONG",
                "Ruifan LI"
              ],
              "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification": [
                "Zhao Bing",
                "Li Ruifan",
                "Zhang Guangwei"
              ],
              "使用TAST码的BI_STCM_ID系统中的星座映射分析": [
                "未找到该信息",
                "李睿凡",
                "赵传钢"
              ],
              "4930_Article_Text_7995_1_10_20190709": [
                "Chenfei Wu",
                "Xiaojie Wang",
                "Ruifan Li",
                "Jinlai Liu"
              ],
              "0628": [
                "Qi Sun",
                "Xiaojie Wang",
                "Hailong Huang",
                "JianPing Shen",
                "Weiguo Gao",
                "Yuzhao Mao",
                "Guang Liu",
                "Ruifan Li",
                "Xuan Li"
              ],
              "Enhanced_Prompt_Learning_for_Few_shot_Text_Classification_Method": [
                "魏志宇",
                "范元涛",
                "叶书勤",
                "张光卫",
                "李睿凡"
              ],
              "考虑合成灰数灰度性质的改进区间灰数预测模型": [
                "王大鹏",
                "汪秉文",
                "李睿凡"
              ],
              "鲁棒局部保持投影的表情识别": [
                "朱强生",
                "郭燕慧",
                "李睿凡",
                "刘海涛"
              ],
              "面向小样本图像分类的任务相关度量学习方法及装置_李晓旭": [
                "刘俊",
                "燕锦涛",
                "杨世丞",
                "张文斌",
                "安文娟",
                "马占宇",
                "陶剑",
                "李睿凡",
                "李晓旭"
              ],
              "3548636.3548646": [
                "Yusong Zhang",
                "Yongping Xiong",
                "Qu-An Zheng",
                "Zhenyuan Ma",
                "Liqing Liu",
                "Mingcong Lu",
                "Ruifan Li"
              ],
              "2647868.2654902": [
                "Fangxiang Feng",
                "Xiaojie Wang",
                "Ruifan Li"
              ],
              "2022.findings_emnlp.6": [
                "Ruifan Li",
                "Lihui Zhang"
              ],
              "基于文本生成图像的模型训练方法、设备和图像生成方法_冯方向": [
                "牛天睿",
                "冯方向",
                "王小捷",
                "袁彩霞",
                "李睿凡"
              ],
              "2021.acl_long.494": [
                "Fangxiang Feng",
                "Zhanyu Ma",
                "Xiaojie Wang",
                "Eduard Hovy",
                "Hao Chen",
                "Ruifan Li"
              ],
              "2022.emnlp_main.212": [
                "Fangxiang Feng",
                "Xiaojie Wang",
                "Zepeng Zhai",
                "Hao Chen",
                "Dongyan Zhao",
                "Feifan Fan",
                "Yansong Feng",
                "Ruifan Li"
              ],
              "基于多层神经网络的电力实体识别方法、存储介质和设备_刘子全": [
                "刘子全",
                "王泽元",
                "朱雪琼",
                "熊永平",
                "胡成博",
                "李睿凡"
              ],
              "2022.coling_1.234": [
                "Fangxiang Feng",
                "Xiaojie Wang",
                "Ruifan Li",
                "Ziqin Rao"
              ],
              "2808205": [
                "Fangxiang Feng",
                "XIAOJIE WANG",
                "RUIFAN LI",
                "Xiaojie Wang",
                "FANGXIANG FENG",
                "IBRAR AHMAD",
                "Ruifan Li"
              ],
              "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining": [
                "Xiaojie Wang",
                "Yuantao Fan",
                "Guangwei Zhang",
                "Chuan Shi",
                "Ruifan Li"
              ],
              "融入类别自适应度量学习的小样本图像分类方法及装置_李晓旭": [
                "武继杰",
                "刘俊",
                "李真",
                "马占宇",
                "曾俊瑀",
                "陶剑",
                "孙浩",
                "李睿凡",
                "李晓旭"
              ],
              "面向_智能科学与技术_专业的C语言教学探讨": [
                "李蕾",
                "李睿凡"
              ],
              "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture": [
                "Fangxiang Feng",
                "Bohan Li",
                "Xiaojie Wang",
                "Peng Lu",
                "Ruifan Li"
              ],
              "1911.09359v1": [
                "Wang Xiaojie",
                "Li Ruifan",
                "Liu Guang"
              ],
              "基于KFD_Isomap的人脸识别": [
                "王枞",
                "郝红卫",
                "李睿凡",
                "涂序彦"
              ],
              "深度学习中卷积神经网络的教学探讨": [
                "周延泉",
                "陈佳洁",
                "钟义信",
                "王小捷",
                "李睿凡"
              ],
              "基于掩码上下文机器阅读理解的方面情感三元组抽取方法_李睿凡": [
                "翟泽鹏",
                "冯方向",
                "张光卫",
                "王小捷",
                "李睿凡"
              ],
              "3469877.3490585": [
                "Fangxiang Feng",
                "Zhanyu Ma",
                "Yihui Shi",
                "Xiaojie Wang",
                "Yun Liu",
                "Ruifan Li"
              ],
              "计算机游戏中的智能技术": [
                "李睿凡",
                "左申正",
                "李卫"
              ],
              "使用LDC码的BI_STCM_ID系统中的星座映射分析": [
                "李睿凡",
                "赵传钢"
              ],
              "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis": [
                "Fangxiang Feng",
                "Zhanyu Ma",
                "Xiaojie Wang",
                "Eduard Hovy",
                "T. Qian",
                "M. Zhang",
                "Hao Chen",
                "Ruifan Li"
              ],
              "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension": [
                "Zhihan Yu",
                "Ruifan Li"
              ],
              "2025.coling_main.22": [
                "Shuqin Ye",
                "Xiaojie Wang",
                "Xinjing Liu",
                "Guangwei Zhang",
                "Ruifan Li"
              ],
              "0_387_29295_0_82": [
                "Cong Wang",
                "Ruifan Li",
                "Xuyan Tu"
              ],
              "Improving_Image_Paragraph_Captioning_with_Dual_Relations": [
                "Fangxiang Feng",
                "Zhanyu Ma",
                "Yihui Shi",
                "Xiaojie Wang",
                "Yun Liu",
                "Ruifan Li"
              ],
              "一种基于关系编码和层次注意力机制的图像段落描述方法_李睿凡": [
                "刘云",
                "冯方向",
                "石祎晖",
                "王小捷",
                "马占宇",
                "李睿凡"
              ],
              "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis": [
                "Fangxiang Feng",
                "Xiaojie Wang",
                "Tianrui Niu",
                "Member, IEEE",
                "Ruifan Li"
              ],
              "3483207.3483233": [
                "Yongping Xiong",
                "Zeyuan Wang",
                "Zepeng Zhai",
                "Yifan Du",
                "Ziqun Liu",
                "Ruifan Li"
              ]
            },
            "institutional_networks": [
              "School of Computer Sciences, Beijing University of Posts and Communications, Beijing 100876, China",
              "School of Information Engineering, Beijing University of Posts and Telecommunications, Beijing, China",
              "北京邮电大学人工智能学院",
              "Key Laboratory of Interactive Technology and Experience System, Ministry of Culture and Tourism, China",
              "School of Artificial Intelligence, Beijing University of Posts and Telecommunications, Beijing, China",
              "School of Artificial Intelligence, Beijing University of Posts and Communications, Beijing 100876, China",
              "交互技术与体验系统文化和旅游部重点实验室",
              "Beijing Natural Science Foundation",
              "Beijing University of Posts and Telecommunications",
              "National Nature Science Foundation of China",
              "Engineering Research Center of Information Networks, Ministry of Education",
              "School of Artiﬁcial Intelligence, Beijing University of Posts and Telecommunications, China",
              "Engineering Research Center of Information Networks, Ministry of Education, China",
              "中国民航大学通信工程系",
              "北京邮电大学计算机学院",
              "Engineering Research Center of Information Networks, Ministry of Education, Beijing 100876, China",
              "Li Auto Inc.",
              "School of Artificial Intelligence, Beijing University of Posts and Telecommunications",
              "教育部信息网络工程研究中心",
              "Beijing University of Posts and Telecommunications, Beijing, China",
              "School of Artificial Intelligence, Beijing University of Posts and Telecommunications, China",
              "北京科技大学信息工程学院",
              "School of Science, Yanshan University, Qinhuangdao 066004, China",
              "原文未明确提到",
              "兰州理工大学计算机与通信学院",
              "School of Artiﬁcial Intelligence, Beijing University of Posts and Telecommunications, Beijing, China",
              "北京林业大学信息学院",
              "School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China",
              "Beijing Academy of Artificial Intelligence",
              "School of Computer, Beijing University of Posts and Telecommunications, Beijing 100876, P. R. China",
              "北京邮电大学信息工程学院",
              "BUPT",
              "School of Computers, Beijing University of Posts and Telecommunications, Beijing 100876, China",
              "华中科技大学控制科学与工程系",
              "Center for Intelligence Science and Technology, School of Computer Science, Beijing University of Posts and Telecommunications",
              "Beijing University of Posts & Telecommunications",
              "Beijing Academy of Artificial Intelligence, Beijing, China",
              "Center for Intelligence Science and Technology, Beijing University of Posts and Telecommunications",
              "High Performance Computing Platform of BUPT"
            ]
          },
          "experimental_details": {
            "datasets_used": [
              {
                "paper": "多视图有监督的LDA模型",
                "dataset": "LabelMe",
                "description": "带有标注的场景分类数据集"
              },
              {
                "paper": "多视图有监督的LDA模型",
                "dataset": "UIUC-Sport",
                "description": "带有标注的事件分类数据集"
              },
              {
                "paper": "基于统计和加权的提高击键认证识别方法(英文)",
                "dataset": "benchmark dataset",
                "description": "from 51 subjects typing a strong 10-character password"
              },
              {
                "paper": "2504.15958v2",
                "dataset": "DreamBench",
                "description": "原文无此信息"
              },
              {
                "paper": "2504.15958v2",
                "dataset": "CustomConcept101",
                "description": "原文无此信息"
              },
              {
                "paper": "2504.15958v2",
                "dataset": "Mix-of-Show",
                "description": "原文无此信息"
              },
              {
                "paper": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning",
                "dataset": "Few-CLUE",
                "description": "encompassing sentiment analysis, short text classification, long text classification, natural language inference, sentence similarity, Chinese cloze, and co-reference resolution"
              },
              {
                "paper": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning",
                "dataset": "CMNLI",
                "description": "used for intermediate training"
              },
              {
                "paper": "FAIA_372_FAIA230600",
                "dataset": "Restaurant-ACOS",
                "description": "原文无此信息"
              },
              {
                "paper": "FAIA_372_FAIA230600",
                "dataset": "Laptop-ACOS",
                "description": "原文无此信息"
              },
              {
                "paper": "2023.acl_long.802",
                "dataset": "NoReCFine, MultiBEU, MultiBCA, MPQA, DSUnis",
                "description": "原文无此信息"
              },
              {
                "paper": "2022.acl_long.212",
                "dataset": "SemEval ABSA Challenges (Pontiki et al., 2014, 2015, 2016)",
                "description": "two ABSA datasets"
              },
              {
                "paper": "3664647.3681466",
                "dataset": "iHarmony4",
                "description": "Includes HCOCO, HFlickr, HAdobe5K, and Hday2night sub-datasets, consisting of 65,742 training and 7,404 testing pairs of composite and real images."
              },
              {
                "paper": "3664647.3681466",
                "dataset": "Human Harmony",
                "description": "Created by filtering the imaterialist-fashion-2020-fgvc7 dataset to exclude images with only products. The cleaned dataset contains 29,106 images."
              },
              {
                "paper": "2408.03632v3",
                "dataset": "Mix-of-show",
                "description": "原文无此信息"
              },
              {
                "paper": "2408.03632v3",
                "dataset": "DreamBooth",
                "description": "原文无此信息"
              },
              {
                "paper": "2408.03632v3",
                "dataset": "CustomConcept101",
                "description": "原文无此信息"
              },
              {
                "paper": "3394171.3416296",
                "dataset": "Perfect-500K",
                "description": "dataset from the 'AI Meets Beauty' challenge"
              },
              {
                "paper": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction",
                "dataset": "ChineseNlpCorpus12",
                "description": "used as the pre-training data"
              },
              {
                "paper": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction",
                "dataset": "SIGHAN",
                "description": "training data includes 10K manually annotated samples"
              },
              {
                "paper": "978_3_642_23223_7_60",
                "dataset": "algebra 2008-2009",
                "description": "student interaction logs from intelligent tutoring systems"
              },
              {
                "paper": "978_3_642_23223_7_60",
                "dataset": "bridge to algebra 2008-2009",
                "description": "student interaction logs from intelligent tutoring systems"
              },
              {
                "paper": "3664647.3680897",
                "dataset": "Flickr-Split-S0",
                "description": "零样本PG设置下的数据集"
              },
              {
                "paper": "3664647.3680897",
                "dataset": "Flickr-Split-S1",
                "description": "零样本PG设置下的数据集"
              },
              {
                "paper": "3664647.3680897",
                "dataset": "VG-Split-S2",
                "description": "零样本PG设置下的数据集"
              },
              {
                "paper": "3664647.3680897",
                "dataset": "VG-Split-S3",
                "description": "零样本PG设置下的数据集"
              },
              {
                "paper": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding",
                "dataset": "MS-COCO",
                "description": "包括 82,783 训练、40,504 验证和 40,775 测试图像"
              },
              {
                "paper": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
                "dataset": "Flickr30K Entities",
                "description": "原文无此信息"
              },
              {
                "paper": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
                "dataset": "COCO",
                "description": "原文无此信息"
              },
              {
                "paper": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
                "dataset": "Visual Genome",
                "description": "原文无此信息"
              },
              {
                "paper": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
                "dataset": "ReferIt",
                "description": "原文无此信息"
              },
              {
                "paper": "0592",
                "dataset": "Flickr8k",
                "description": "原文无此信息"
              },
              {
                "paper": "0592",
                "dataset": "Flickr30k",
                "description": "原文无此信息"
              },
              {
                "paper": "0592",
                "dataset": "COCO",
                "description": "原文无此信息"
              },
              {
                "paper": "0592",
                "dataset": "paragraph dataset from Krause et al. (2017)",
                "description": "原文无此信息"
              },
              {
                "paper": "Dimensionality_reduction_for_text_using_LLE",
                "dataset": "Reuters21578",
                "description": "document subsets were vectorized after preprocessing, and the number of topics and documents varied"
              },
              {
                "paper": "Dimensionality_reduction_for_text_using_LLE",
                "dataset": "TDT2",
                "description": "contains 9394 documents across 30 topics"
              },
              {
                "paper": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification",
                "dataset": "Flickr dataset",
                "description": "原文无此信息"
              },
              {
                "paper": "4930_Article_Text_7995_1_10_20190709",
                "dataset": "VQA 1.0",
                "description": "原文无此信息"
              },
              {
                "paper": "4930_Article_Text_7995_1_10_20190709",
                "dataset": "VQA 2.0",
                "description": "原文无此信息"
              },
              {
                "paper": "4930_Article_Text_7995_1_10_20190709",
                "dataset": "COCO-QA",
                "description": "原文无此信息"
              },
              {
                "paper": "4930_Article_Text_7995_1_10_20190709",
                "dataset": "TDIUC",
                "description": "原文无此信息"
              },
              {
                "paper": "0628",
                "dataset": "FI-2010",
                "description": "The first publicly available benchmark dataset of high-frequency Limit Order Book (LOB) data"
              },
              {
                "paper": "0628",
                "dataset": "CSI-2016",
                "description": "A dataset we collected from three one-minute stock index data"
              },
              {
                "paper": "Enhanced_Prompt_Learning_for_Few_shot_Text_Classification_Method",
                "dataset": "中文数据集和英文数据集",
                "description": "原文无此信息"
              },
              {
                "paper": "鲁棒局部保持投影的表情识别",
                "dataset": "JAFFE数据库",
                "description": "原文无此信息"
              },
              {
                "paper": "3548636.3548646",
                "dataset": "CPTF dataset",
                "description": "包含1484个实例，跨越12个类别"
              },
              {
                "paper": "2647868.2654902",
                "dataset": "Wikipedia, Pascal, NUS-WIDE-10k",
                "description": "These datasets vary in text modality, size, and category numbers."
              },
              {
                "paper": "2022.findings_emnlp.6",
                "dataset": "CommonsenseQA",
                "description": "原文无此信息"
              },
              {
                "paper": "2022.findings_emnlp.6",
                "dataset": "OpenbookQA",
                "description": "原文无此信息"
              },
              {
                "paper": "2021.acl_long.494",
                "dataset": "Restaurant, Laptop, Twitter",
                "description": "三个公开数据集"
              },
              {
                "paper": "2022.emnlp_main.212",
                "dataset": "D1 (Wu et al., 2020a)",
                "description": "benchmark datasets from SemEval Challenges"
              },
              {
                "paper": "2022.emnlp_main.212",
                "dataset": "D2 (Xu et al., 2020)",
                "description": "benchmark datasets from SemEval Challenges"
              },
              {
                "paper": "2022.coling_1.234",
                "dataset": "NYT10",
                "description": "原文无此信息"
              },
              {
                "paper": "2022.coling_1.234",
                "dataset": "GDS",
                "description": "原文无此信息"
              },
              {
                "paper": "2808205",
                "dataset": "Wikipedia dataset",
                "description": "2,866 image/text pairs from ten semantic categories."
              },
              {
                "paper": "2808205",
                "dataset": "Pascal dataset",
                "description": "1,000 image/text pairs from twenty categories."
              },
              {
                "paper": "2808205",
                "dataset": "NUS-WIDE-10k dataset",
                "description": "1,000 image/text pairs per category from ten categories."
              },
              {
                "paper": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining",
                "dataset": "RedPajama-V2 and MAP-CC",
                "description": "We sample an equal proportion of 15 billion English and Chinese tokens from RedPajama-V2 and MAP-CC, respectively."
              },
              {
                "paper": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
                "dataset": "Small ESP Game dataset",
                "description": "contains 100,000 labeled images with corresponding tags"
              },
              {
                "paper": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
                "dataset": "MLC-2013 dataset",
                "description": "has 1,000 manually labeled images with two labels per image"
              },
              {
                "paper": "1911.09359v1",
                "dataset": "SH000001, SZ399005, SZ399006",
                "description": "Data spans from January 1, 2016, to December 30, 2016, with a total of 58,000 data points."
              },
              {
                "paper": "基于KFD_Isomap的人脸识别",
                "dataset": "ORL人脸数据库",
                "description": "包含36位男性和4位女性，每人10幅图像，共400幅面部图像，每幅图像为112×92象素，256灰度级"
              },
              {
                "paper": "基于KFD_Isomap的人脸识别",
                "dataset": "Yale人脸数据库",
                "description": "包括15个人，大小为320×243的165幅人脸图像"
              },
              {
                "paper": "3469877.3490585",
                "dataset": "Stanford image paragraph benchmark dataset",
                "description": "原文无此信息"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "dataset": "Restaurant14",
                "description": "All datasets have three sentimental polarities: positive, neutral, and negative"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "dataset": "Laptop14",
                "description": "All datasets have three sentimental polarities: positive, neutral, and negative"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "dataset": "Twitter",
                "description": "All datasets have three sentimental polarities: positive, neutral, and negative"
              },
              {
                "paper": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
                "dataset": "RefCOCO",
                "description": "based on MS-COCO images and differ in the types of descriptions allowed"
              },
              {
                "paper": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
                "dataset": "C-RefCOCO",
                "description": "generated using the CSG method to balance normal and counterfactual samples"
              },
              {
                "paper": "2025.coling_main.22",
                "dataset": "TWITTER-15 and TWITTER-17",
                "description": "benchmark datasets for MABSA tasks, specifically C-MABSA with conditional relations"
              },
              {
                "paper": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
                "dataset": "Stanford benchmark dataset",
                "description": "includes 14575/2487/2489 pairs for training/validation/test. The dataset comprises an average of 67.5 words per paragraph and 5.7 sentences."
              },
              {
                "paper": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
                "dataset": "CUB",
                "description": "原文无此信息"
              },
              {
                "paper": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
                "dataset": "Oxford-102",
                "description": "原文无此信息"
              },
              {
                "paper": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
                "dataset": "COCO",
                "description": "原文无此信息"
              }
            ],
            "evaluation_metrics": [
              {
                "paper": "基于统计和加权的提高击键认证识别方法(英文)",
                "metric": "FRR"
              },
              {
                "paper": "基于统计和加权的提高击键认证识别方法(英文)",
                "metric": "FAR"
              },
              {
                "paper": "基于统计和加权的提高击键认证识别方法(英文)",
                "metric": "EER"
              },
              {
                "paper": "基于统计和加权的提高击键认证识别方法(英文)",
                "metric": "FAR + FRR"
              },
              {
                "paper": "2504.15958v2",
                "metric": "CLIP"
              },
              {
                "paper": "2504.15958v2",
                "metric": "DINOv2"
              },
              {
                "paper": "FAIA_372_FAIA230600",
                "metric": "Precision"
              },
              {
                "paper": "FAIA_372_FAIA230600",
                "metric": "Recall"
              },
              {
                "paper": "FAIA_372_FAIA230600",
                "metric": "F1-score"
              },
              {
                "paper": "2023.acl_long.802",
                "metric": "Sentiment Graph F1 (SF1)"
              },
              {
                "paper": "2023.acl_long.802",
                "metric": "Holder F1"
              },
              {
                "paper": "2023.acl_long.802",
                "metric": "Target F1"
              },
              {
                "paper": "2023.acl_long.802",
                "metric": "Exp. F1"
              },
              {
                "paper": "2023.acl_long.802",
                "metric": "Nonpolarity Sentiment Graph F1 (NSF1)"
              },
              {
                "paper": "2022.acl_long.212",
                "metric": "F1"
              },
              {
                "paper": "3664647.3681466",
                "metric": "PSNR"
              },
              {
                "paper": "3664647.3681466",
                "metric": "MSE"
              },
              {
                "paper": "3664647.3681466",
                "metric": "fMSE"
              },
              {
                "paper": "2408.03632v3",
                "metric": "CLIP"
              },
              {
                "paper": "2408.03632v3",
                "metric": "ImageReward"
              },
              {
                "paper": "2408.03632v3",
                "metric": "Segmentation Similarity (SegSim)"
              },
              {
                "paper": "3394171.3416296",
                "metric": "Mean Average Precision (MAP)"
              },
              {
                "paper": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction",
                "metric": "Precision"
              },
              {
                "paper": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction",
                "metric": "recall"
              },
              {
                "paper": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction",
                "metric": "F1 scores"
              },
              {
                "paper": "978_3_642_23223_7_60",
                "metric": "root mean squared error (RMSE)"
              },
              {
                "paper": "3664647.3680897",
                "metric": "IoU"
              },
              {
                "paper": "3664647.3680897",
                "metric": "bbox accuracy"
              },
              {
                "paper": "3664647.3680897",
                "metric": "recognition accuracy"
              },
              {
                "paper": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding",
                "metric": "CIDEr"
              },
              {
                "paper": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding",
                "metric": "BLEU"
              },
              {
                "paper": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding",
                "metric": "METEOR"
              },
              {
                "paper": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding",
                "metric": "ROUGE"
              },
              {
                "paper": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding",
                "metric": "SPICE"
              },
              {
                "paper": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
                "metric": "pointing game accuracy"
              },
              {
                "paper": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
                "metric": "bounding box accuracy"
              },
              {
                "paper": "0592",
                "metric": "BELU"
              },
              {
                "paper": "0592",
                "metric": "METEOR"
              },
              {
                "paper": "0592",
                "metric": "ROUGE L"
              },
              {
                "paper": "0592",
                "metric": "CIDEr"
              },
              {
                "paper": "0592",
                "metric": "Instance Coverage (IC)"
              },
              {
                "paper": "Dimensionality_reduction_for_text_using_LLE",
                "metric": "precision"
              },
              {
                "paper": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification",
                "metric": "Accuracy"
              },
              {
                "paper": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification",
                "metric": "Recall"
              },
              {
                "paper": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification",
                "metric": "F1 score"
              },
              {
                "paper": "4930_Article_Text_7995_1_10_20190709",
                "metric": "Acc(ans)"
              },
              {
                "paper": "0628",
                "metric": "F1 score"
              },
              {
                "paper": "0628",
                "metric": "accuracy"
              },
              {
                "paper": "3548636.3548646",
                "metric": "Acc"
              },
              {
                "paper": "3548636.3548646",
                "metric": "Macro-F1"
              },
              {
                "paper": "3548636.3548646",
                "metric": "Weighted Macro-F1"
              },
              {
                "paper": "2647868.2654902",
                "metric": "mean average precision (mAP)"
              },
              {
                "paper": "2647868.2654902",
                "metric": "top 20% percentage"
              },
              {
                "paper": "2022.findings_emnlp.6",
                "metric": "Accuracy (Acc)"
              },
              {
                "paper": "2021.acl_long.494",
                "metric": "accuracy"
              },
              {
                "paper": "2021.acl_long.494",
                "metric": "macro-averaged F1-score"
              },
              {
                "paper": "2022.emnlp_main.212",
                "metric": "Precision"
              },
              {
                "paper": "2022.emnlp_main.212",
                "metric": "Recall"
              },
              {
                "paper": "2022.emnlp_main.212",
                "metric": "F1 scores"
              },
              {
                "paper": "2022.coling_1.234",
                "metric": "P@N"
              },
              {
                "paper": "2022.coling_1.234",
                "metric": "AUC"
              },
              {
                "paper": "2022.coling_1.234",
                "metric": "Micro-F1 score"
              },
              {
                "paper": "2808205",
                "metric": "mean average precision (mAP)"
              },
              {
                "paper": "2808205",
                "metric": "top 20% percentage"
              },
              {
                "paper": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining",
                "metric": "average accuracy"
              },
              {
                "paper": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
                "metric": "accuracy"
              },
              {
                "paper": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
                "metric": "area under the ROC curve"
              },
              {
                "paper": "1911.09359v1",
                "metric": "accuracy"
              },
              {
                "paper": "1911.09359v1",
                "metric": "F-score (F1)"
              },
              {
                "paper": "1911.09359v1",
                "metric": "Confusion Matrix (CM)"
              },
              {
                "paper": "1911.09359v1",
                "metric": "accumulated profit"
              },
              {
                "paper": "基于KFD_Isomap的人脸识别",
                "metric": "错误率(％)"
              },
              {
                "paper": "3469877.3490585",
                "metric": "BLEU-1"
              },
              {
                "paper": "3469877.3490585",
                "metric": "BLEU-2"
              },
              {
                "paper": "3469877.3490585",
                "metric": "BLEU-3"
              },
              {
                "paper": "3469877.3490585",
                "metric": "BLEU-4"
              },
              {
                "paper": "3469877.3490585",
                "metric": "METEOR"
              },
              {
                "paper": "3469877.3490585",
                "metric": "CIDEr"
              },
              {
                "paper": "使用LDC码的BI_STCM_ID系统中的星座映射分析",
                "metric": "渐进误比特率性能"
              },
              {
                "paper": "使用LDC码的BI_STCM_ID系统中的星座映射分析",
                "metric": "分集增益"
              },
              {
                "paper": "使用LDC码的BI_STCM_ID系统中的星座映射分析",
                "metric": "编码增益"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "metric": "LAL-Parser"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "metric": "Glove vectors"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "metric": "BiLSTM"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "metric": "dropout"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "metric": "Adam optimizer"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "metric": "learning rate"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "metric": "epochs"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "metric": "batch size"
              },
              {
                "paper": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
                "metric": "Acc-Box (IoU@0.5)"
              },
              {
                "paper": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
                "metric": "Acc-Cls"
              },
              {
                "paper": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
                "metric": "Acc-Cf"
              },
              {
                "paper": "2025.coling_main.22",
                "metric": "Accuracy"
              },
              {
                "paper": "2025.coling_main.22",
                "metric": "F1 score"
              },
              {
                "paper": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
                "metric": "BLEU@{1, 2, 3, 4}"
              },
              {
                "paper": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
                "metric": "METEOR"
              },
              {
                "paper": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
                "metric": "CIDEr"
              },
              {
                "paper": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
                "metric": "BERTScore F metrics"
              },
              {
                "paper": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
                "metric": "Inception Score (IS)"
              },
              {
                "paper": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
                "metric": "Fréchet Inception Distance (FID)"
              },
              {
                "paper": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
                "metric": "R-Precision"
              }
            ],
            "performance_data": []
          }
        },
        "ai_analysis": {
          "academic_identity": {
            "primary_research_identity": "人工智能与自然语言处理",
            "research_signature": [
              "数据安全",
              "多模态学习",
              "文本生成图像",
              "情感分析"
            ],
            "expertise_domains": [
              "数据安全",
              "多模态学习",
              "文本生成图像",
              "情感分析"
            ],
            "academic_stage": "成熟期",
            "unique_strengths": [
              "跨领域研究",
              "技术创新",
              "丰富的合作网络"
            ]
          },
          "innovation_assessment": {
            "innovation_level": 8,
            "creativity_patterns": "注重技术创新，善于跨领域研究",
            "technical_depth": "深厚",
            "methodological_innovation": "较强",
            "breakthrough_potential": "高",
            "innovation_consistency": "持续"
          },
          "research_trajectory": {
            "development_pattern": "稳步上升",
            "topic_evolution": "从数据安全到多模态学习、文本生成图像、情感分析",
            "productivity_trend": "持续增长",
            "collaboration_evolution": "合作范围不断扩大",
            "impact_growth": "持续提升"
          },
          "technical_capabilities": {
            "experimental_skills": "强",
            "dataset_expertise": "丰富",
            "evaluation_proficiency": "熟练",
            "technical_breadth": "广泛",
            "implementation_ability": "强"
          },
          "collaboration_analysis": {
            "network_position": "核心节点",
            "collaboration_quality": "高",
            "institutional_connections": "广泛的机构合作网络",
            "leadership_potential": "高",
            "mentoring_ability": "强"
          },
          "development_recommendations": {
            "strategic_directions": [
              "持续技术创新",
              "拓展跨领域合作",
              "加强学术影响力"
            ],
            "skill_enhancement": [
              "强化数据安全研究",
              "深化多模态学习",
              "提升文本生成图像技术"
            ],
            "collaboration_targets": [
              "数据安全领域专家",
              "多模态学习领域专家",
              "文本生成图像领域专家"
            ],
            "research_priorities": [
              "数据安全",
              "多模态学习",
              "文本生成图像",
              "情感分析"
            ],
            "career_milestones": [
              "获得更多学术荣誉",
              "发表更多高影响力论文",
              "建立更广泛的合作网络"
            ],
            "risk_mitigation": [
              "关注技术更新",
              "防范学术不端",
              "维护合作网络"
            ]
          },
          "comparative_analysis": {
            "peer_comparison": "处于领先地位",
            "competitive_advantages": [
              "跨领域研究",
              "技术创新",
              "丰富的合作网络"
            ],
            "improvement_areas": [
              "数据安全研究可加强"
            ],
            "market_position": "领先"
          }
        }
      },
      "Wang Xiaojie": {
        "raw_data": {
          "basic_info": {
            "standardized_name": "Wang Xiaojie",
            "name_variations": [
              "Xiaojie Wang"
            ],
            "total_papers": 25,
            "document_types": {
              "academic_paper": 25
            },
            "institutions": [
              "Center for Intelligence Science and Technology, School of Computer Science, Beijing University of Posts and Telecommunications",
              "Beijing University of Posts & Telecommunications",
              "Engineering Research Center of Information Networks, Ministry of Education, Beijing 100876, China",
              "Li Auto Inc.",
              "School of Artificial Intelligence, Beijing University of Posts and Telecommunications",
              "Beijing Academy of Artificial Intelligence, Beijing, China",
              "Beijing Natural Science Foundation",
              "Beijing University of Posts and Telecommunications, Beijing, China",
              "Beijing Academy of Artificial Intelligence",
              "Beijing University of Posts and Telecommunications",
              "High Performance Computing Platform of BUPT",
              "National Nature Science Foundation of China",
              "Center for Intelligence Science and Technology, Beijing University of Posts and Telecommunications",
              "School of Artificial Intelligence, Beijing University of Posts and Telecommunications, China",
              "Engineering Research Center of Information Networks, Ministry of Education, China",
              "BUPT",
              "School of Computers, Beijing University of Posts and Telecommunications, Beijing 100876, China"
            ],
            "papers": [
              "2504.15958v2",
              "1307.1275v1",
              "2023.acl_long.802",
              "2022.acl_long.212",
              "3664647.3681466",
              "2408.03632v3",
              "3394171.3416296",
              "3664647.3680897",
              "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
              "2820400",
              "0592",
              "4930_Article_Text_7995_1_10_20190709",
              "0628",
              "2647868.2654902",
              "2021.acl_long.494",
              "2022.emnlp_main.212",
              "2022.coling_1.234",
              "2808205",
              "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining",
              "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
              "3469877.3490585",
              "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
              "2025.coling_main.22",
              "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
              "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis"
            ]
          },
          "research_content": {
            "domains": [
              "Text-to-image generation, subject-driven image generation, training-free framework",
              "Personalized content creation",
              "Image synthesis",
              "Multi-modal learning, bimodal data representations, image-tags",
              "Predictive systems for word tags",
              "Structured Sentiment Analysis",
              "Opinion analysis",
              "Sentiment classification",
              "Natural language processing",
              "Aspect-based Sentiment Analysis (ABSA), Graph Convolutional Network",
              "Sentiment analysis",
              "Natural language processing",
              "Computer vision tasks, image harmonization",
              "Digital editing",
              "Image generation and editing tasks",
              "Text-to-image synthesis with multi-concept customization",
              "Image generation",
              "AI art",
              "Virtual reality",
              "Visual feature learning for image retrieval in e-commerce",
              "E-commerce",
              "Content-based image retrieval",
              "Phrase Grounding under weak supervision",
              "Multimedia and multimodal retrieval",
              "Weakly supervised phrase grounding",
              "Image-text alignment",
              "Multimodal information processing",
              "Deep Learning for Multimedia and Emotional and Social Signals in Multimedia",
              "Image captioning",
              "Computer Vision",
              "Natural Language Processing",
              "Visual Question Answering (VQA)",
              "Human-computer interaction",
              "Image and text understanding",
              "Stock Trend Prediction in Artificial Intelligence",
              "Intelligent Investment",
              "Financial Time Series Analysis",
              "Cross-modal retrieval",
              "Web image database",
              "Information retrieval",
              "Aspect-based sentiment analysis",
              "Natural language processing",
              "Sentiment analysis",
              "Aspect Sentiment Triplet Extraction, Machine Reading Comprehension, Fine-grained Aspect-based Sentiment Analysis",
              "Aspect-based Sentiment Analysis",
              "Natural Language Processing",
              "Distantly supervised relation extraction",
              "Natural Language Processing",
              "Cross-modal retrieval, deep learning, autoencoder",
              "Multimodal data retrieval",
              "Cross-lingual Learning, Pointwise Mutual Information (PMI), Hallucination, Large Language Models (LLMs)",
              "Cross-lingual transfer learning",
              "Language model pretraining",
              "Multimodal data analysis using deep neural learning",
              "Image and text analysis",
              "Complex systems",
              "Image paragraph captioning",
              "Computer vision tasks",
              "Aspect-Based Sentiment Analysis",
              "sentiment analysis",
              "recommendation",
              "advertisement computation",
              "Multimodal Aspect-Based Sentiment Analysis (MABSA)",
              "Social media sentiment analysis",
              "Image paragraph captioning",
              "Beijing Academy of Artificial Intelligence",
              "Computer Vision",
              "Natural Language Processing",
              "Text-to-image synthesis, Generative Adversarial Networks, Multi-modal disentangled representation learning",
              "Interactive art",
              "Computer-aided drawing"
            ],
            "methods": [
              "FreeGraftor, a training-free framework that uses cross-image feature grafting",
              "Hierarchical representations of bimodal data using MPEG-7, gist descriptors, RBMs, and a quasi-Siamese auto-encoder",
              "USSA, a unified 2D table-filling scheme that utilizes 13 relation types and a bi-axial attention module",
              "Enhanced Multi-Channel Graph Convolutional Network (EMC-GCN)",
              "Harmony-VAE, inverse harmonization diffusion model",
              "Concept Conductor framework with multipath sampling, layout alignment, and concept injection",
              "Utilizing product titles as supervised signals to learn image features, constructing an image classification dataset using n-grams from product titles, fine-tuning a pre-trained model, and extracting the basic max-pooling activation of convolutions (MAC) feature.",
              "Triple alignment strategies: Region-Text Alignment (RTA), Domain Alignment (DomA), and Category Alignment (CatA)",
              "Refinement-based approach using a detector-free phrase grounding model fine-tuned with a visual prompt from CLIP text-related representations",
              "Facial emotion recognition during speech; Correspondence autoencoders for cross-modal retrieval",
              "Topic-Oriented Multi-Sentence (TOMS) captioning model",
              "Differential Networks (DN) and DN-based Fusion (DF)",
              "Multi-scale Two-way Deep Neural Network (MTDNN), using eXtreme Gradient Boosting and Recurrent Convolutional Neural Network",
              "Correspondence autoencoder (Corr-AE), Corr-Cross-AE, and Corr-Full-AE",
              "DualGCN model",
              "COntext-Masked MRC (COM-MRC) framework",
              "BERT-based Graph Convolutional network Model (BGM)",
              "Correspondence Autoencoder (Corr-AE), integrating representation learning and correlation learning into a single process",
              "InfoLoss, a novel loss function for continual pretraining",
              "Bimodal Deep Architecture (BDA)",
              "Splitting to Tree Decoder (S2TD)",
              "DualGCN, integrating SynGCN and SemGCN through a mutual BiAffine module, with orthogonal and differential regularizers.",
              "COnditional Relation based Sentiment Analysis framework (CORSA), including a conditional relation detector (CRD) and a visual object localizer (VOL)",
              "DualRel model",
              "Modality disentangled discriminator, AttnGAN, DM-GAN"
            ],
            "keywords": [
              "Text-to-image generation",
              "Subject-driven image generation",
              "Cross-image feature grafting",
              "Semantic matching",
              "Efficiency",
              "Multi-subject generation",
              "Multi-modal learning",
              "Bimodal representations",
              "Image-tags",
              "Word tags",
              "Restricted Boltzmann Machines",
              "Siamese network",
              "Structured Sentiment Analysis",
              "USSA",
              "Bi-axial attention",
              "Overlap",
              "Discontinuity",
              "Aspect Sentiment Triplet Extraction",
              "Graph Convolutional Network",
              "Linguistic Features",
              "Refining Strategy",
              "image harmonization",
              "latent diffusion model",
              "VAE",
              "data augmentation",
              "inverse harmonization",
              "stable diffusion",
              "Text-to-image synthesis",
              "Personalization",
              "Concept Conductor",
              "Attribute leakage",
              "Layout confusion",
              "Visual feature learning",
              "Bag of n-grams",
              "Image retrieval",
              "CNN",
              "MAC",
              "Phrase Grounding",
              "weak supervision",
              "zero-shot learning",
              "alignment strategies",
              "CLIP",
              "Weakly supervised phrase grounding",
              "Visual prompt tuning",
              "CLIP",
              "Detector-free",
              "Multimedia",
              "Deep Learning",
              "Emotion Recognition",
              "Speech",
              "Correspondence Autoencoders",
              "Cross-Modal Retrieval",
              "Image captioning",
              "Topic-Oriented Multi-Sentence",
              "Latent Dirichlet Allocation",
              "Fusion Gate Unit",
              "Instance Coverage",
              "Visual Question Answering",
              "Differential Networks",
              "DN-based Fusion",
              "attention distribution",
              "Stock Trend Prediction",
              "Multi-scale Analysis",
              "Deep Neural Network",
              "Wavelet Transform",
              "Downsampling",
              "XGBoost",
              "RCNN",
              "Cross-modal retrieval",
              "Correspondence autoencoder",
              "Representation learning",
              "Correlation learning",
              "aspect-based sentiment analysis",
              "dual graph convolutional networks",
              "dependency parsing",
              "semantic correlations",
              "regularizers",
              "Aspect Sentiment Triplet Extraction",
              "Machine Reading Comprehension",
              "Context Masking",
              "Sentiment Analysis",
              "Distant supervision",
              "Relation extraction",
              "BERT",
              "Graph Convolutional Network",
              "Cross-entropy loss",
              "Cross-modal",
              "retrieval",
              "image and text",
              "deep learning",
              "autoencoder",
              "Cross-lingual Learning",
              "Pointwise Mutual Information (PMI)",
              "Hallucination",
              "Large Language Models (LLMs)",
              "Multimodal data",
              "Deep neural learning",
              "Similarity metric",
              "Bimodal deep architecture",
              "image captioning",
              "paragraph generation",
              "tree-structured decoder",
              "vision and language",
              "Aspect-Based Sentiment Analysis",
              "DualGCN",
              "graph convolutional networks",
              "dependency parsing",
              "semantic information",
              "Multimodal Aspect-Based Sentiment Analysis",
              "Conditional Relation",
              "CORSA framework",
              "Visual Object Localizer",
              "Image paragraph captioning",
              "Dual Relations",
              "Spatial Relation",
              "Semantic Relation",
              "Weakly Supervised",
              "Relation-aware Attention",
              "text-to-image synthesis",
              "generative adversarial networks",
              "multi-modal disentangled representation learning"
            ],
            "innovations": [
              "Training-free cross-image feature grafting",
              "Semantic matching",
              "Position-constrained attention fusion",
              "Noise initialization strategy for geometry priors",
              "Three-level representations in three stages",
              "Bimodal auto-encoder for level-3 representations",
              "Data-specific strategy for choosing correct tag words",
              "Unified 2D table-filling scheme",
              "Bi-axial attention module",
              "Addressing overlap and discontinuity in SSA",
              "Proposed EMC-GCN to exploit word relations",
              "Defined ten relation types",
              "Incorporated linguistic features",
              "Developed refining strategy for improved triplet extraction",
              "Harmony-VAE to enhance decoded image quality",
              "Inverse harmonization model for data augmentation",
              "Introduction of the Human Harmony dataset",
              "Training-free framework",
              "Isolates sampling processes",
              "Self-attention-based spatial guidance",
              "Shape-aware masks for concept injection",
              "Using product titles to guide the learning of visual features",
              "Conversion of product titles into discrete labels for supervised learning",
              "Bag of n-grams approach",
              "Triple alignment strategies for zero-shot Phrase Grounding under Weak Supervision",
              "CLIP-based heatmap generation",
              "Region-category relations consideration",
              "Use of similarity tokens for spatial information capture",
              "Detector-free network fine-tuning",
              "Latent Dirichlet Allocation (LDA) for topic mining",
              "Fusion Gate Unit (FGU) for sentence generation",
              "Multi-label logistic regression and softmax for training",
              "Propose Differential Networks (DN) module",
              "Introduce DN-based Fusion (DF) model for VQA",
              "Two-way end-to-end model",
              "Wavelet-based and downsampling-based scale information",
              "Enhancing stock trend prediction with multi-scale information",
              "Integrates representation and correlation learning into a single process",
              "Proposes a novel loss function",
              "Extends the Corr-AE to two other correspondence models",
              "DualGCN architecture",
              "SynGCN and SemGCN modules",
              "orthogonal and differential regularizers",
              "Context augmentation strategy",
              "Discriminative model",
              "Two-stage inference method",
              "Combining a Pretrained Language Model (PLM) and a Graph Convolutional Network (GCN)",
              "Using GCN to directly learn bag representations over instances",
              "Proposed the Corr-AE",
              "Integrates representation and correlation learning",
              "Two correspondence models: Corr-Cross-AE and Corr-Full-AE",
              "Proposal of InfoLoss for continually pretraining LLMs",
              "Mitigation of hallucinations in cross-lingual transfer setting",
              "Three interconnected components",
              "Stacked restricted Boltzmann machines",
              "Variant autoencoder with predefined loss function",
              "Tree-structured decoder",
              "Top-down binary tree expansion",
              "Split module",
              "Score module",
              "Tree structure loss",
              "DualGCN model",
              "SynGCN",
              "SemGCN",
              "Mutual BiAffine module",
              "orthogonal and differential regularizers",
              "Proposed CORSA framework",
              "Conditional Relation Detector (CRD)",
              "Visual Object Localizer (VOL)",
              "Captures spatial and semantic relations among objects",
              "Weakly supervised multi-label classifier",
              "Relation-aware attention",
              "Fusion Gates",
              "Proposed modality disentangled discriminator",
              "Separate classification of content and style",
              "Enhanced text-image correlation",
              "Style transfer"
            ],
            "technical_concepts": [
              "FreeGraftor",
              "Semantic-Aware Feature Grafting",
              "Structure-Consistent Initialization",
              "Multimodal-Diffusion Transformer",
              "U-Net",
              "Transformer",
              "Stable Diffusion",
              "FLUX.1",
              "MPEG-7",
              "gist descriptors",
              "Restricted Boltzmann Machines",
              "Siamese network",
              "auto-encoder",
              "Contrastive Divergence",
              "Structured Sentiment Analysis",
              "2D table-filling scheme",
              "Bi-axial attention module",
              "Bi-lexical dependency parsing",
              "EMC-GCN",
              "Aspect-based Sentiment Analysis",
              "Graph Convolutional Operations",
              "Linguistic Features",
              "Biaffine Attention Module",
              "Latent diffusion models",
              "Harmony-VAE",
              "Inverse harmonization model",
              "Denoising Diffusion Probabilistic Models (DDPMs)",
              "Stable Diffusion",
              "Text-to-image diffusion models",
              "Multipath sampling",
              "Layout alignment",
              "Concept injection",
              "VAE",
              "Image representations",
              "Visual content-based indexing and retrieval",
              "CNN",
              "MAC",
              "SPoC",
              "RMAC",
              "RAMAC",
              "MS-RMAC",
              "GRMAC",
              "Region-Text Alignment",
              "Domain Alignment",
              "Category Alignment",
              "Contrastive Language-Image Pre-Training (CLIP)",
              "heatmap generation",
              "CLIP",
              "Grounding model",
              "Similarity tokens",
              "Cosine similarity",
              "VGG16",
              "Facial emotion recognition",
              "Correspondence autoencoders",
              "Cross-modal retrieval",
              "LSTM",
              "FGU",
              "LDA",
              "Multi-label logistic regression",
              "Softmax",
              "BELU",
              "METEOR",
              "ROUGE L",
              "CIDEr",
              "Instance Coverage",
              "Differential Networks (DN)",
              "DN-based Fusion (DF)",
              "pair-wise feature elements",
              "attention-based models",
              "Neural Networks",
              "Support Vector Machine",
              "Random Forest",
              "DWT",
              "CNN",
              "GRU",
              "Cross-entropy Loss",
              "Autoencoders",
              "Correlation measure",
              "Loss function",
              "Deep architecture",
              "Canonical correlation analysis",
              "Graph Convolutional Networks",
              "BiLSTM",
              "BERT",
              "dependency probability matrix",
              "self-attention mechanism",
              "COntext-Masked MRC",
              "Aspect Extraction",
              "Opinion Extraction",
              "Sentiment Classification",
              "Aspect Detection",
              "BERT-based PLM",
              "Graph Convolutional Network (GCN)",
              "Bag representation",
              "Cross-entropy loss",
              "Gradient descent optimization",
              "Autoencoders",
              "Correlation learning",
              "Multimodal reconstruction",
              "CCA",
              "InfoLoss",
              "Cross-entropy loss",
              "Pointwise Mutual Information (PMI)",
              "Hallucinations",
              "Large Language Models (LLMs)",
              "Multimodal data",
              "Deep neural networks",
              "Restricted Boltzmann machines",
              "Autoencoder",
              "Similarity measure",
              "Computer vision tasks",
              "Split module",
              "Score module",
              "Word-level RNN",
              "Tree structure loss",
              "Cosine similarity",
              "LSTM",
              "Highway Network",
              "Sentence-BERT",
              "graph neural networks",
              "GCNs",
              "GATs",
              "syntax structures",
              "semantic correlations",
              "dependency trees",
              "Multimodal Aspect Term Extraction",
              "Multimodal Aspect-oriented Sentiment Classification",
              "Joint Multimodal Aspects of Sentiment Analysis",
              "UNINEXT",
              "YOLOv8",
              "DualRel model",
              "Faster R-CNN",
              "Relation Embedding Module",
              "Relation-aware Interaction Module",
              "Self-critical sequence training",
              "Generative Adversarial Networks (GANs)",
              "AttnGAN",
              "DM-GAN",
              "Inception Score (IS)",
              "Fréchet Inception Distance (FID)",
              "Modality disentangled representation"
            ]
          },
          "detailed_innovations": {
            "contributions_by_paper": {
              "2504.15958v2": [
                "training-free framework",
                "cross-image feature grafting",
                "structure-consistent initialization"
              ],
              "2023.acl_long.802": [
                "a bi-lexical dependency parsing graph converted to a unified 2D table filling scheme (USSA)",
                "an effective model that collaborates with USSA, utilizing the bi-axial attention module",
                "extensive experimental validation on benchmark datasets"
              ],
              "2022.acl_long.212": [
                "a novel EMC-GCN model",
                "a comprehensive exploitation of linguistic features",
                "an effective refining strategy"
              ],
              "3664647.3681466": [
                "Introduction of the Human Harmony dataset",
                "Harmony-VAE's effectiveness in enhancing the VAE component in latent diffusion models for image harmonization"
              ],
              "2408.03632v3": [
                "We introduce Concept Conductor, a training-free framework that ensures visual fidelity and correct layout in multi-concept customization."
              ],
              "3394171.3416296": [
                "method for learning product visual representations",
                "deep CNNs pre-trained and fine-tuned on this dataset can enhance feature effectiveness for product image retrieval"
              ],
              "3664647.3680897": [
                "提出了一种零样本短语接地的新框架",
                "引入了三重对齐策略"
              ],
              "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding": [
                "use of similarity tokens for spatial information capture",
                "detector-free network fine-tuning"
              ],
              "0592": [
                "a novel topic-oriented captioning model",
                "the FGU design",
                "extensive experimental evaluation"
              ],
              "4930_Article_Text_7995_1_10_20190709": [
                "We propose a general DN module and a new DF model for the VQA task."
              ],
              "0628": [
                "Proposing a Multi-scale Two-way Deep Neural Network (MTDNN) for stock trend prediction"
              ],
              "2647868.2654902": [
                "We propose the correspondence autoencoder (Corr-AE) based on two basic unimodal autoencoders.",
                "The Corr-AE integrates representation and correlation learning into a single process.",
                "A novel loss function is designed, incorporating the losses of autoencoders for all modalities and the loss of correlation between modalities."
              ],
              "2021.acl_long.494": [
                "提出了一种结合语法和语义特征的DualGCN模型",
                "设计了正交和微分正则化器以增强模型捕获语义关联的能力"
              ],
              "2022.emnlp_main.212": [
                "propose a COntext-Masked MRC (COM-MRC) framework for ASTE tasks",
                "alleviate interference in ASTE tasks",
                "the context augmentation strategy effectively expanding the training corpus"
              ],
              "2022.coling_1.234": [
                "提出了一种结合预训练语言模型和图卷积网络的简单模型"
              ],
              "2808205": [
                "We propose the correspondence autoencoder (Corr-AE), which integrates representation learning and correlation learning into a single process."
              ],
              "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining": [
                "the proposal of InfoLoss for continually pretraining LLMs",
                "the first attempt to mitigate hallucinations in a cross-lingual transfer setting",
                "extensive experiments on twelve benchmarks"
              ],
              "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture": [
                "proposes the BDA to measure similarity in multimodal systems",
                "combines feature extraction and deep neural networks",
                "demonstrates effectiveness in classifying image tags"
              ],
              "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis": [
                "proposal of DualGCN architecture",
                "integration of syntactic knowledge through SynGCN and semantic information through SemGCN",
                "orthogonal and differential regularizers"
              ],
              "2025.coling_main.22": [
                "propose the CORSA framework",
                "address the impact of irrelevant images",
                "localize condition-related visual regions"
              ],
              "Improving_Image_Paragraph_Captioning_with_Dual_Relations": [
                "提出DualRel模型，明确捕捉空间和语义关系以改进图像段落字幕生成"
              ],
              "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis": [
                "The proposed discriminator is the first to learn modality disentangled representation for text-to-image synthesis, enhancing discrimination of image-text correlation and facilitating image synthesis manipulation."
              ]
            },
            "novelty_claims_by_paper": {
              "2022.acl_long.212": [
                "Our proposed framework is EMC-GCN, which addresses Aspect and Opinion Term Co-Extraction (AOTE) and Aspect-Sentiment Pair Extraction (ASPE)",
                "We define ten types of relations between words for the ASTE task"
              ],
              "3394171.3416296": [
                "using product titles to guide the learning of visual features",
                "conversion of product titles into discrete labels for supervised learning"
              ],
              "3664647.3680897": [
                "在弱监督下实现零样本接地",
                "使用CLIP进行区域文本对齐"
              ],
              "0592": [
                "Our TOMS model differs by generating sentences from topics of interest, capturing linguistic distinctions in image descriptions.",
                "The Fusion Gate Unit (FGU) fuses three sources of representations: image, context, and topic."
              ],
              "4930_Article_Text_7995_1_10_20190709": [
                "原文明确声明的新颖性，如无则为空数组"
              ],
              "0628": [
                "Using wavelet-based and downsampling-based scale information",
                "Achieving state-of-the-art performance on FI-2010 and CSI-2016 datasets"
              ],
              "2021.acl_long.494": [
                "DualGCN模型的结构设计",
                "正交和微分正则化器的应用"
              ],
              "2022.emnlp_main.212": [
                "COM-MRC’s components work collaboratively",
                "the two-stage inference method reduces interference from other aspects"
              ],
              "2022.coling_1.234": [
                "首次使用GCN直接学习实例之间的包表示"
              ],
              "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture": [
                "the BDA's three components are designed to extract features, stack RBMs, and use a variant autoencoder for discriminative learning"
              ],
              "3469877.3490585": [
                "We propose Splitting to Tree Decoder (S2TD), a novel tree-structured decoder that models paragraph decoding as top-down binary tree expansion."
              ],
              "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis": [
                "DualGCN architecture",
                "orthogonal and differential regularizers"
              ],
              "Improving_Image_Paragraph_Captioning_with_Dual_Relations": [
                "DualRel模型考虑了特定的语义和空间关系，并使用关系感知交互"
              ]
            },
            "technical_relationships": {
              "2504.15958v2": {
                "base_methods": [
                  {
                    "method_name": "FLUX.1",
                    "relationship_type": "基于",
                    "evidence_text": "We build upon FLUX.1, where joint attention is computed as:"
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "FreeCustom",
                    "comparison_result": "FreeCustom suffers from severe attribute confusion",
                    "evidence_text": "FreeCustom suffers from severe attribute confusion, while MS-Diffusion and OmniGen lack visual faithfulness for small objects."
                  },
                  {
                    "method_name": "MS-Diffusion",
                    "comparison_result": "MS-Diffusion and OmniGen lack visual faithfulness for small objects",
                    "evidence_text": "FreeCustom suffers from severe attribute confusion, while MS-Diffusion and OmniGen lack visual faithfulness for small objects."
                  },
                  {
                    "method_name": "OmniGen",
                    "comparison_result": "OmniGen lacks visual faithfulness for small objects",
                    "evidence_text": "FreeCustom suffers from severe attribute confusion, while MS-Diffusion and OmniGen lack visual faithfulness for small objects."
                  }
                ]
              },
              "2023.acl_long.802": {
                "base_methods": [
                  {
                    "method_name": "bi-lexical dependency parsing",
                    "relationship_type": "改进",
                    "evidence_text": "We propose USSA, a unified 2D table-filling scheme that utilizes 13 relation types."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "RACL-BERT, Head-first, Head-final, Frozen PERIN, TGLS",
                    "comparison_result": "USSA generally outperforms other baselines",
                    "evidence_text": "Table 4 shows that USSA generally outperforms other baselines in terms of Span F1."
                  }
                ]
              },
              "2022.acl_long.212": {
                "base_methods": [
                  {
                    "method_name": "Graph Convolutional Network (GCN)",
                    "relationship_type": "扩展",
                    "evidence_text": "Motivated by CNN, GCN is an efficient variant operating on graphs (Kipf and Welling, 2017)."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "GTS-BERT",
                    "comparison_result": "Our EMC-GCN significantly surpasses GTS-BERT with an average of 1.96% and 2.61% F1-score on D1 and D2, respectively",
                    "evidence_text": "Under the F1 metric, our EMC-GCN model outperforms all pipeline, end-to-end, and MRC-based methods on two groups of datasets."
                  },
                  {
                    "method_name": "BMRC",
                    "comparison_result": "The term 'light' is challenging to identify by GTS-BERT and BMRC, yet 'easy' is predicted correctly by all methods",
                    "evidence_text": "The term 'light' is challenging to identify by GTS-BERT and BMRC, yet 'easy' is predicted correctly by all methods due to its closer proximity to 'transport' than 'light'."
                  }
                ]
              },
              "3664647.3681466": {
                "base_methods": [
                  {
                    "method_name": "Stable Diffusion",
                    "relationship_type": "基于",
                    "evidence_text": "Harmony-VAE is a method designed for image harmonization, which includes repairing the content of the foreground area while maintaining a harmonized appearance. It builds upon the Stable Diffusion model"
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "HDNet",
                    "comparison_result": "DiffHarmony++ shows superior performance as the foreground proportion increases.",
                    "evidence_text": "On the iHarmony4 dataset, HDNet512 outperforms DiffHarmony++ in samples with small foreground proportions (0% ∼5%), but DiffHarmony++ shows superior performance as the foreground proportion increases."
                  }
                ]
              },
              "2408.03632v3": {
                "base_methods": [
                  {
                    "method_name": "Stable Diffusion",
                    "relationship_type": "改进",
                    "evidence_text": "Our method is implemented on Stable Diffusion v1.5, utilizing layout references from SDXL."
                  },
                  {
                    "method_name": "ED-LoRA",
                    "relationship_type": "结合",
                    "evidence_text": "ED-LoRA, a method for single-concept customization, is used as our default single-concept model, involving learnable hierarchical text embeddings and low-rank adaptation (LoRA) applied to pre-trained weights."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "Custom Diffusion",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "We compare our method against three baselines: Custom Diffusion, Cones 2, and Mix-of-Show, using their official code implementations."
                  },
                  {
                    "method_name": "Cones 2",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "We compare our method against three baselines: Custom Diffusion, Cones 2, and Mix-of-Show, using their official code implementations."
                  },
                  {
                    "method_name": "Mix-of-Show",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "We compare our method against three baselines: Custom Diffusion, Cones 2, and Mix-of-Show, using their official code implementations."
                  }
                ]
              },
              "3394171.3416296": {
                "base_methods": [
                  {
                    "method_name": "Deep Convolutional Neural Networks (CNNs)",
                    "relationship_type": "基于",
                    "evidence_text": "Our method, detailed in Figure 2, addresses this by leveraging the titles of product images."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "SEResnet-152 and Densenet-201",
                    "comparison_result": "The deepest model, Densenet-201, shows the best performance among the ImageNet-1k trained models, while the shallowest model, ResNet-50, performs the worst",
                    "evidence_text": "Table 1 presents the results of five deep CNN models with four different pooling methods on the validation set."
                  }
                ]
              },
              "3664647.3680897": {
                "base_methods": [
                  {
                    "method_name": "CLIP",
                    "relationship_type": "基于",
                    "evidence_text": "Our framework uses a novel PG framework using triple alignment strategies under weak supervision: 1) Region-Text Alignment (RTA) to associate region-level attributes based on Contrastive Language-Image Pre-Training (CLIP)."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "WWbl",
                    "comparison_result": "Our CLIP-based heatmap surpasses the pseudo label used by WWbl, explaining a 9% increase in bbox accuracy.",
                    "evidence_text": "Our CLIP-based heatmap surpasses the pseudo label used by WWbl, explaining a 9% increase in bbox accuracy."
                  },
                  {
                    "method_name": "ZSGNet",
                    "comparison_result": "Our approach exceeds previous methods and will explore interpretable solutions for grounding-related tasks.",
                    "evidence_text": "Our approach exceeds previous methods and will explore interpretable solutions for grounding-related tasks."
                  }
                ]
              },
              "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding": {
                "base_methods": [
                  {
                    "method_name": "Weakly-Supervised Grounding (WSG)",
                    "relationship_type": "改进",
                    "evidence_text": "We propose a refinement-based approach using a detector-free phrase grounding model fine-tuned with a visual prompt from CLIP text-related representations."
                  },
                  {
                    "method_name": "CLIP",
                    "relationship_type": "结合",
                    "evidence_text": "Our approach leverages the relationship between CLIP and the grounding model, refining training through visual prompt tuning."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "WWbl",
                    "comparison_result": "Our method shows improvements in metrics on Flickr30K and ReferIt",
                    "evidence_text": "Our method is trained on COCO and VG train splits and evaluated on the test splits of Flickr30K, VG, and ReferIt. The performance of our method is compared with state-of-the-art DF-WSG methods quantitatively and qualitatively."
                  },
                  {
                    "method_name": "Gbs",
                    "comparison_result": "Our approach not only surpasses detector-free methods like Gbs and WWbl",
                    "evidence_text": "Our approach not only surpasses detector-free methods like Gbs and WWbl but also outperforms detector-based methods on almost all categories for Flickr30K Entities."
                  }
                ]
              },
              "0592": {
                "base_methods": [
                  {
                    "method_name": "Single-sentence (SS) captioning models",
                    "relationship_type": "基于",
                    "evidence_text": "Single-sentence (SS) captioning models have been developed using templates and neural network approaches, but they often provide incomplete descriptions."
                  },
                  {
                    "method_name": "MS captioning methods",
                    "relationship_type": "基于",
                    "evidence_text": "MS captioning methods generate multiple sentences for a complete depiction, with some focusing on regions of interest."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "NIC",
                    "comparison_result": "Our TOMS demonstrates improved performance, especially in terms of IC.",
                    "evidence_text": "Our proposed Topic-Oriented Multi-Sentence (TOMS) captioning model incorporates topic embedding to guide the generation process... Our TOMS demonstrates improved performance, especially in terms of IC."
                  },
                  {
                    "method_name": "ATT-FCN",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "We compare our TOMS model with NIC and ATT-FCN for sentence level MS captioning"
                  },
                  {
                    "method_name": "RTT-GAN",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "We compare our TOMS model with NIC and ATT-FCN for sentence level MS captioning and with RTT-GAN, Regions-Hierarchical, and Sentence-Concat for paragraph level MS captioning."
                  },
                  {
                    "method_name": "Regions-Hierarchical",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "We compare our TOMS model with NIC and ATT-FCN for sentence level MS captioning and with RTT-GAN, Regions-Hierarchical, and Sentence-Concat for paragraph level MS captioning."
                  },
                  {
                    "method_name": "Sentence-Concat",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "We compare our TOMS model with NIC and ATT-FCN for sentence level MS captioning and with RTT-GAN, Regions-Hierarchical, and Sentence-Concat for paragraph level MS captioning."
                  }
                ]
              },
              "4930_Article_Text_7995_1_10_20190709": {
                "base_methods": [
                  {
                    "method_name": "Attention-based models",
                    "relationship_type": "基于",
                    "evidence_text": "We review current VQA models, focusing on attention-based models and fusion strategies."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "HighOrderAtt",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "HighOrderAtt (Schwartz, Schwing, and Hazan 2017) - - - - 69.4"
                  },
                  {
                    "method_name": "MLB(7)",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "MLB(7) (Kim et al. 2017) 66.77 84.54 39.21 57.81"
                  },
                  {
                    "method_name": "Mutan(5)",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "Mutan(5) (Ben-younes et al. 2017) 67.42 85.14 39.81 58.52"
                  },
                  {
                    "method_name": "DualMFA",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "DualMFA (Lu et al. 2018) 66.01 83.59 40.18 56.84 70.04"
                  },
                  {
                    "method_name": "ReasonNet",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "ReasonNet (Ilievski and Feng 2017) - - - - -"
                  }
                ]
              },
              "0628": {
                "base_methods": [
                  {
                    "method_name": "Support Vector Machine and Neural Networks",
                    "relationship_type": "基于",
                    "evidence_text": "STP is a classification task traditionally tackled by Support Vector Machine and Neural Networks."
                  },
                  {
                    "method_name": "Ensemble-based methods like Random Forest and deep learning models",
                    "relationship_type": "基于",
                    "evidence_text": "Ensemble-based methods like Random Forest and deep learning models have also been explored."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "XGBoost",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "The data is treated as a non-stationary discrete signal and decomposed using DWT to obtain transformed multi-scale components. These are concatenated and input to an XGBoost model to ensemble multi-scale information and output category scores."
                  },
                  {
                    "method_name": "CNN",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "A key operation concatenates these features, and a GRU unit temporally cascades the information to output categories."
                  },
                  {
                    "method_name": "RNN",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "A key operation concatenates these features, and a GRU unit temporally cascades the information to output categories."
                  },
                  {
                    "method_name": "RCNN",
                    "comparison_result": "RCNN shows the best performance on most indices",
                    "evidence_text": "In the multi-scale rows, variations are fed with DWT-based, downsampling-based, and CNN multi-kernel size scale-information. RCNN shows the best performance on most indices, while XGBoost is less effective."
                  }
                ]
              },
              "2647868.2654902": {
                "base_methods": [
                  {
                    "method_name": "Correspondence LDA, deep autoencoders, deep belief networks, and deep Boltzmann Machines",
                    "relationship_type": "基于",
                    "evidence_text": "Shared layer models include topic models like Correspondence LDA and deep architectures such as deep autoencoders, deep belief networks, and deep Boltzmann Machines."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "CCA-based models and multi-modal models",
                    "comparison_result": "Our correspondence autoencoders significantly outperform other models",
                    "evidence_text": "Our correspondence autoencoders significantly outperform other models on both retrieval tasks across Wikipedia, Pascal, and NUS-WIDE-10k data sets."
                  }
                ]
              },
              "2021.acl_long.494": {
                "base_methods": [
                  {
                    "method_name": "Graph Convolutional Network (GCN)",
                    "relationship_type": "改进",
                    "evidence_text": "We propose a GCN-based method that combines syntactic and semantic features."
                  },
                  {
                    "method_name": "Attention-based LSTM",
                    "relationship_type": "改进",
                    "evidence_text": "Attention-based neural networks, including those proposed by Wang et al. (2016) and others, have been introduced to implicitly model the semantic relation between aspects and their context."
                  },
                  {
                    "method_name": "Recursive neural network by Dong et al. (2014)",
                    "relationship_type": "改进",
                    "evidence_text": "Notable works include the recursive neural network by Dong et al. (2014)"
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "ATAE-LSTM, IAN, RAM, MGAN, TNet, ASGCN, CDT, BiGCN, kumaGCN, InterGCN, R-GAT, DGEDT, BERT, R-GAT+BERT, DGEDT+BERT",
                    "comparison_result": "Our DualGCN model consistently outperforms attention-based and syntax-based methods",
                    "evidence_text": "We compare DualGCN with state-of-the-art baselines, including ATAE-LSTM, IAN, RAM, MGAN, TNet, ASGCN, CDT, BiGCN, kumaGCN, InterGCN, R-GAT, DGEDT, BERT, R-GAT+BERT, and DGEDT+BERT."
                  }
                ]
              },
              "2022.emnlp_main.212": {
                "base_methods": [
                  {
                    "method_name": "Aspect Sentiment Triplet Extraction (ASTE)",
                    "relationship_type": "基于",
                    "evidence_text": "Aspect Sentiment Triplet Extraction (ASTE) extracts sentiment triplets from sentences and is formalized as an effective machine reading comprehension (MRC) framework."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "BMRC",
                    "comparison_result": "Our COM-MRC outperforms other baselines in terms of Precision, Recall, and F1 scores on both D1 and D2 datasets",
                    "evidence_text": "Our COM-MRC outperforms other baselines in terms of Precision, Recall, and F1 scores on both D1 and D2 datasets, as shown in Tables 3 and 4."
                  },
                  {
                    "method_name": "EMC-GCN",
                    "comparison_result": "Our COM-MRC outperforms EMC-GCN",
                    "evidence_text": "Our COM-MRC outperforms EMC-GCN on datasets D1 and D2."
                  }
                ]
              },
              "2022.coling_1.234": {
                "base_methods": [
                  {
                    "method_name": "BERT-based Graph Convolutional network Model (BGM)",
                    "relationship_type": "结合",
                    "evidence_text": "We introduce BGM, a model that combines a Pretrained Language Model (PLM) and a Graph Convolutional Network (GCN)"
                  },
                  {
                    "method_name": "PCNN-based methods",
                    "relationship_type": "基于",
                    "evidence_text": "DS-RE methods can be categorized into PCNN-based and PLMs-based approaches"
                  },
                  {
                    "method_name": "PLMs-based methods",
                    "relationship_type": "基于",
                    "evidence_text": "PCNN-based methods use various techniques to acquire effective bag representations, while PLMs-based methods have shown remarkable performance"
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "multiple baseline methods",
                    "comparison_result": "report better performance",
                    "evidence_text": "We compare our BGM with multiple baseline methods and report better performance"
                  },
                  {
                    "method_name": "BGM without GCN",
                    "comparison_result": "performance drop",
                    "evidence_text": "In the ablation study, we find that removing GCN results in a performance drop"
                  },
                  {
                    "method_name": "BGM without EntCon",
                    "comparison_result": "performance drop",
                    "evidence_text": "In the ablation study, we find that removing entity connections results in a performance drop"
                  }
                ]
              },
              "2808205": {
                "base_methods": [
                  {
                    "method_name": "autoencoder",
                    "relationship_type": "改进",
                    "evidence_text": "We propose several novel models based on different autoencoders, which correlate hidden representations of a pair of autoencoders."
                  },
                  {
                    "method_name": "Canonical Correlation Analysis (CCA)",
                    "relationship_type": "结合",
                    "evidence_text": "The models are trained using a novel optimal objective that minimizes a linear combination of representation learning errors and correlation learning error."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "CCA-based models",
                    "comparison_result": "Our models achieve substantial improvements in mAP scores.",
                    "evidence_text": "Compared to CCA-AE, our Corr-AE enhances the average mAP by 53.6%, 81.5%, and 48.3% on the respective datasets."
                  },
                  {
                    "method_name": "Bimodal AE, Bimodal DBN",
                    "comparison_result": "Our Corr-AE also achieves notable improvements over Bimodal AE and Bimodal DBN.",
                    "evidence_text": "Our Corr-AE also achieves notable improvements over Bimodal AE and Bimodal DBN."
                  }
                ]
              },
              "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining": {
                "base_methods": [
                  {
                    "method_name": "Cross-lingual Continual Pretraining",
                    "relationship_type": "扩展",
                    "evidence_text": "Cross-Lingual Continual Pretraining addresses the limitation of LLMs [1, 2, 3, 4] in languages other than English."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "traditional cross-entropy loss",
                    "comparison_result": "InfoLoss aims to avoid learning incorrect language distributions caused by noisy tokens",
                    "evidence_text": "We compare our method, InfoLoss, with the traditional cross-entropy loss for training Llama 2."
                  }
                ]
              },
              "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture": {
                "base_methods": [
                  {
                    "method_name": "deep neural learning",
                    "relationship_type": "基于",
                    "evidence_text": "Deep neural learning, inspired by biological propagation phenomena in the human brain, has seen rapid development since 2006 and has achieved success in tasks involving single modal data."
                  },
                  {
                    "method_name": "topic models, joint models, undirected Markov random fields",
                    "relationship_type": "扩展",
                    "evidence_text": "Various approaches have been proposed for learning from cross-modal data. These include extensions of topic models, joint models, and undirected Markov random fields, but these single-hidden-layer models struggle with the complexity of images and text."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "Multilayer Perceptrons (MLP)",
                    "comparison_result": "Our deep neural architecture outperforms an MLP-based system",
                    "evidence_text": "Our deep neural architecture outperforms an MLP-based system with two hidden layers and a CCA-based system with two RBMs, achieving an accuracy of 88.96%."
                  },
                  {
                    "method_name": "Canonical Correlation Analysis (CCA)",
                    "comparison_result": "The CCA-based system performs better than the MLP-based system",
                    "evidence_text": "The CCA-based system performs better than the MLP-based system."
                  }
                ]
              },
              "3469877.3490585": {
                "base_methods": [
                  {
                    "method_name": "hierarchical and non-hierarchical decoders",
                    "relationship_type": "基于",
                    "evidence_text": "Existing approaches, such as hierarchical and non-hierarchical decoders, have limitations in managing visual observations and maintaining paragraph coherence."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "DAM-ATT, SCST, SCST+RP, CRL, OR-ATT, OR-ATT+RP",
                    "comparison_result": "Our S2TD achieves competitive performance, especially in terms of BLEU-1 and CIDEr.",
                    "evidence_text": "Performance evaluation compares S2TD with state-of-the-art sequential decoding methods, grouped into hierarchical and non-hierarchical methods."
                  }
                ]
              },
              "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis": {
                "base_methods": [
                  {
                    "method_name": "GCN",
                    "relationship_type": "改进",
                    "evidence_text": "Incorporating syntactic dependency structure with graph convolutional networks (GCNs) has shown advantages, but performance depends on dependency parsers."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "ATAE-LSTM",
                    "comparison_result": "DualGCN model consistently outperforms attention-based and syntax-based methods",
                    "evidence_text": "The DualGCN model consistently outperforms attention-based and syntax-based methods on Restaurant14, Laptop14 and Twitter datasets."
                  },
                  {
                    "method_name": "GCAE",
                    "comparison_result": "DualGCN model outperforms",
                    "evidence_text": "DualGCN model outperforms SynGCN-head on the Restaurant14 and Laptop14 datasets"
                  },
                  {
                    "method_name": "R-GAT + BERT",
                    "comparison_result": "DualGCN + BERT and DualGCN + BERT-PT show improved performance over the basic DualGCN model and other BERT-based methods",
                    "evidence_text": "DualGCN + BERT and DualGCN + BERT-PT show improved performance over the basic DualGCN model and other BERT-based methods."
                  }
                ]
              },
              "2025.coling_main.22": {
                "base_methods": [
                  {
                    "method_name": "Traditional methods",
                    "relationship_type": "基于",
                    "evidence_text": "Traditional methods assume the image contains objects referred to by the aspects, which is not always true."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "ChatGPT 3.5, Llama 2, VisualGLM-6B, Llava 1.5, MMICL, mPLUG-Owl2, GPT4V",
                    "comparison_result": "our model outperforms",
                    "evidence_text": "Comparative experiments with large language models (LLMs) and multi-modal LLMs (MLLMs) on the JMASA and MASC tasks demonstrate that our model outperforms ChatGPT 3.5, Llama 2, VisualGLM-6B, Llava 1.5, MMICL, mPLUG-Owl2, and GPT4V, showcasing the advantage of multimodal input and the effectiveness of our approach."
                  }
                ]
              },
              "Improving_Image_Paragraph_Captioning_with_Dual_Relations": {
                "base_methods": [
                  {
                    "method_name": "Faster R-CNN",
                    "relationship_type": "基于",
                    "evidence_text": "We employ Faster R-CNN [19] to detect N objects in an image, represented as C={c1, · · · , cN}."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "Regions-Hierarchical",
                    "comparison_result": "DualRel outperforms SCST on all metrics (except for a tie in METEOR) and the recent IMAP method.",
                    "evidence_text": "Baselines: We compare DualRel with baselines including Regions-Hierarchical [1], RTT-GAN [5], DAM [7], SCST [8], DCPG-VAE [6], TMOS [27], CAE-LSTM [11], DHPV [9], CVAP [10], CRL [12], Dual-CNN [15], VREN [17], IMAP [14], S2TD [16], and OR-ATT [18]."
                  },
                  {
                    "method_name": "SCST",
                    "comparison_result": "DualRel outperforms SCST on all metrics (except for a tie in METEOR)",
                    "evidence_text": "Results: Table 1 shows our DualRel method achieves the best scores in B@{1-4} and CIDEr. DualRel outperforms SCST on all metrics (except for a tie in METEOR) and the recent IMAP method."
                  }
                ]
              },
              "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis": {
                "base_methods": [
                  {
                    "method_name": "AttnGAN",
                    "relationship_type": "改进",
                    "evidence_text": "The modality disentangled discriminator is integrated into two models: AttnGAN and DM-GAN."
                  },
                  {
                    "method_name": "DM-GAN",
                    "relationship_type": "改进",
                    "evidence_text": "The modality disentangled discriminator is integrated into two models: AttnGAN and DM-GAN."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "GAN-INT-CLS",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "In terms of style manipulation experiments on the CUB dataset, two types of experiments are conducted. The first involves style transfer, where the model uses modality-specific features for direct style transfer, achieving more accurate style reconstruction compared to GAN-INT-CLS."
                  }
                ]
              }
            }
          },
          "collaboration_raw": {
            "collaborators_by_paper": {
              "2504.15958v2": [
                "Fangxiang Feng",
                "Chen Wei",
                "Xiaojie Wang",
                "Zebin Yao",
                "Huixing Jiang",
                "Ruifan Li",
                "Lei Ren"
              ],
              "1307.1275v1": [
                "Fangxiang Feng",
                "Xiaojie Wang",
                "Ruifan Li"
              ],
              "2023.acl_long.802": [
                "Zepeng Zhai",
                "Hao Chen",
                "Xiaojie Wang",
                "Ruifan Li"
              ],
              "2022.acl_long.212": [
                "Fangxiang Feng",
                "Xiaojie Wang",
                "Zepeng Zhai",
                "Hao Chen",
                "Ruifan Li"
              ],
              "3664647.3681466": [
                "Fangxiang Feng",
                "Xiaojie Wang",
                "Ruifan Li",
                "Guang Liu",
                "Pengfei Zhou"
              ],
              "2408.03632v3": [
                "Fangxiang Feng",
                "Xiaojie Wang",
                "Zebin Yao",
                "Ruifan Li"
              ],
              "3394171.3416296": [
                "Fangxiang Feng",
                "Xiaojie Wang",
                "Tianrui Niu",
                "Huixing Jiang",
                "Ruifan Li"
              ],
              "3664647.3680897": [
                "Fangxiang Feng",
                "Zhanyu Ma",
                "Xiaojie Wang",
                "Ruifei Zhang",
                "Zhihong Chen",
                "Guanbin Li",
                "Yuzhe Ji",
                "Zhihan Yu",
                "Yibing Song",
                "Pengyue Lin",
                "Ruifan Li",
                "Xiang Wan"
              ],
              "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding": [
                "Fangxiang Feng",
                "Xiaojie Wang",
                "Mingcong Lu",
                "Zhihan Yu",
                "Pengyue Lin",
                "Ruifan Li"
              ],
              "2820400": [
                "Fangxiang Feng",
                "George Toderici",
                "Xiaojie Wang",
                "Yelin Kim",
                "Emily Mower-Provost",
                "Hayley Hung",
                "Ruifan Li"
              ],
              "0592": [
                "Xiaojie Wang",
                "Ruifan Li",
                "Chang Zhou",
                "Yuzhao Mao"
              ],
              "4930_Article_Text_7995_1_10_20190709": [
                "Chenfei Wu",
                "Xiaojie Wang",
                "Ruifan Li",
                "Jinlai Liu"
              ],
              "0628": [
                "Qi Sun",
                "Xiaojie Wang",
                "Hailong Huang",
                "JianPing Shen",
                "Weiguo Gao",
                "Yuzhao Mao",
                "Guang Liu",
                "Ruifan Li",
                "Xuan Li"
              ],
              "2647868.2654902": [
                "Fangxiang Feng",
                "Xiaojie Wang",
                "Ruifan Li"
              ],
              "2021.acl_long.494": [
                "Fangxiang Feng",
                "Zhanyu Ma",
                "Xiaojie Wang",
                "Eduard Hovy",
                "Hao Chen",
                "Ruifan Li"
              ],
              "2022.emnlp_main.212": [
                "Fangxiang Feng",
                "Xiaojie Wang",
                "Zepeng Zhai",
                "Hao Chen",
                "Dongyan Zhao",
                "Feifan Fan",
                "Yansong Feng",
                "Ruifan Li"
              ],
              "2022.coling_1.234": [
                "Fangxiang Feng",
                "Xiaojie Wang",
                "Ruifan Li",
                "Ziqin Rao"
              ],
              "2808205": [
                "Fangxiang Feng",
                "XIAOJIE WANG",
                "RUIFAN LI",
                "Xiaojie Wang",
                "FANGXIANG FENG",
                "IBRAR AHMAD",
                "Ruifan Li"
              ],
              "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining": [
                "Xiaojie Wang",
                "Yuantao Fan",
                "Guangwei Zhang",
                "Chuan Shi",
                "Ruifan Li"
              ],
              "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture": [
                "Fangxiang Feng",
                "Bohan Li",
                "Xiaojie Wang",
                "Peng Lu",
                "Ruifan Li"
              ],
              "3469877.3490585": [
                "Fangxiang Feng",
                "Zhanyu Ma",
                "Yihui Shi",
                "Xiaojie Wang",
                "Yun Liu",
                "Ruifan Li"
              ],
              "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis": [
                "Fangxiang Feng",
                "Zhanyu Ma",
                "Xiaojie Wang",
                "Eduard Hovy",
                "T. Qian",
                "M. Zhang",
                "Hao Chen",
                "Ruifan Li"
              ],
              "2025.coling_main.22": [
                "Shuqin Ye",
                "Xiaojie Wang",
                "Xinjing Liu",
                "Guangwei Zhang",
                "Ruifan Li"
              ],
              "Improving_Image_Paragraph_Captioning_with_Dual_Relations": [
                "Fangxiang Feng",
                "Zhanyu Ma",
                "Yihui Shi",
                "Xiaojie Wang",
                "Yun Liu",
                "Ruifan Li"
              ],
              "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis": [
                "Fangxiang Feng",
                "Xiaojie Wang",
                "Tianrui Niu",
                "Member, IEEE",
                "Ruifan Li"
              ]
            },
            "institutional_networks": [
              "Center for Intelligence Science and Technology, School of Computer Science, Beijing University of Posts and Telecommunications",
              "Beijing University of Posts & Telecommunications",
              "Engineering Research Center of Information Networks, Ministry of Education, Beijing 100876, China",
              "Li Auto Inc.",
              "School of Artificial Intelligence, Beijing University of Posts and Telecommunications",
              "Beijing Academy of Artificial Intelligence, Beijing, China",
              "Beijing Natural Science Foundation",
              "Beijing University of Posts and Telecommunications, Beijing, China",
              "Beijing Academy of Artificial Intelligence",
              "Beijing University of Posts and Telecommunications",
              "High Performance Computing Platform of BUPT",
              "National Nature Science Foundation of China",
              "Center for Intelligence Science and Technology, Beijing University of Posts and Telecommunications",
              "School of Artificial Intelligence, Beijing University of Posts and Telecommunications, China",
              "Engineering Research Center of Information Networks, Ministry of Education, China",
              "BUPT",
              "School of Computers, Beijing University of Posts and Telecommunications, Beijing 100876, China"
            ]
          },
          "experimental_details": {
            "datasets_used": [
              {
                "paper": "2504.15958v2",
                "dataset": "DreamBench",
                "description": "原文无此信息"
              },
              {
                "paper": "2504.15958v2",
                "dataset": "CustomConcept101",
                "description": "原文无此信息"
              },
              {
                "paper": "2504.15958v2",
                "dataset": "Mix-of-Show",
                "description": "原文无此信息"
              },
              {
                "paper": "2023.acl_long.802",
                "dataset": "NoReCFine, MultiBEU, MultiBCA, MPQA, DSUnis",
                "description": "原文无此信息"
              },
              {
                "paper": "2022.acl_long.212",
                "dataset": "SemEval ABSA Challenges (Pontiki et al., 2014, 2015, 2016)",
                "description": "two ABSA datasets"
              },
              {
                "paper": "3664647.3681466",
                "dataset": "iHarmony4",
                "description": "Includes HCOCO, HFlickr, HAdobe5K, and Hday2night sub-datasets, consisting of 65,742 training and 7,404 testing pairs of composite and real images."
              },
              {
                "paper": "3664647.3681466",
                "dataset": "Human Harmony",
                "description": "Created by filtering the imaterialist-fashion-2020-fgvc7 dataset to exclude images with only products. The cleaned dataset contains 29,106 images."
              },
              {
                "paper": "2408.03632v3",
                "dataset": "Mix-of-show",
                "description": "原文无此信息"
              },
              {
                "paper": "2408.03632v3",
                "dataset": "DreamBooth",
                "description": "原文无此信息"
              },
              {
                "paper": "2408.03632v3",
                "dataset": "CustomConcept101",
                "description": "原文无此信息"
              },
              {
                "paper": "3394171.3416296",
                "dataset": "Perfect-500K",
                "description": "dataset from the 'AI Meets Beauty' challenge"
              },
              {
                "paper": "3664647.3680897",
                "dataset": "Flickr-Split-S0",
                "description": "零样本PG设置下的数据集"
              },
              {
                "paper": "3664647.3680897",
                "dataset": "Flickr-Split-S1",
                "description": "零样本PG设置下的数据集"
              },
              {
                "paper": "3664647.3680897",
                "dataset": "VG-Split-S2",
                "description": "零样本PG设置下的数据集"
              },
              {
                "paper": "3664647.3680897",
                "dataset": "VG-Split-S3",
                "description": "零样本PG设置下的数据集"
              },
              {
                "paper": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
                "dataset": "Flickr30K Entities",
                "description": "原文无此信息"
              },
              {
                "paper": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
                "dataset": "COCO",
                "description": "原文无此信息"
              },
              {
                "paper": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
                "dataset": "Visual Genome",
                "description": "原文无此信息"
              },
              {
                "paper": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
                "dataset": "ReferIt",
                "description": "原文无此信息"
              },
              {
                "paper": "0592",
                "dataset": "Flickr8k",
                "description": "原文无此信息"
              },
              {
                "paper": "0592",
                "dataset": "Flickr30k",
                "description": "原文无此信息"
              },
              {
                "paper": "0592",
                "dataset": "COCO",
                "description": "原文无此信息"
              },
              {
                "paper": "0592",
                "dataset": "paragraph dataset from Krause et al. (2017)",
                "description": "原文无此信息"
              },
              {
                "paper": "4930_Article_Text_7995_1_10_20190709",
                "dataset": "VQA 1.0",
                "description": "原文无此信息"
              },
              {
                "paper": "4930_Article_Text_7995_1_10_20190709",
                "dataset": "VQA 2.0",
                "description": "原文无此信息"
              },
              {
                "paper": "4930_Article_Text_7995_1_10_20190709",
                "dataset": "COCO-QA",
                "description": "原文无此信息"
              },
              {
                "paper": "4930_Article_Text_7995_1_10_20190709",
                "dataset": "TDIUC",
                "description": "原文无此信息"
              },
              {
                "paper": "0628",
                "dataset": "FI-2010",
                "description": "The first publicly available benchmark dataset of high-frequency Limit Order Book (LOB) data"
              },
              {
                "paper": "0628",
                "dataset": "CSI-2016",
                "description": "A dataset we collected from three one-minute stock index data"
              },
              {
                "paper": "2647868.2654902",
                "dataset": "Wikipedia, Pascal, NUS-WIDE-10k",
                "description": "These datasets vary in text modality, size, and category numbers."
              },
              {
                "paper": "2021.acl_long.494",
                "dataset": "Restaurant, Laptop, Twitter",
                "description": "三个公开数据集"
              },
              {
                "paper": "2022.emnlp_main.212",
                "dataset": "D1 (Wu et al., 2020a)",
                "description": "benchmark datasets from SemEval Challenges"
              },
              {
                "paper": "2022.emnlp_main.212",
                "dataset": "D2 (Xu et al., 2020)",
                "description": "benchmark datasets from SemEval Challenges"
              },
              {
                "paper": "2022.coling_1.234",
                "dataset": "NYT10",
                "description": "原文无此信息"
              },
              {
                "paper": "2022.coling_1.234",
                "dataset": "GDS",
                "description": "原文无此信息"
              },
              {
                "paper": "2808205",
                "dataset": "Wikipedia dataset",
                "description": "2,866 image/text pairs from ten semantic categories."
              },
              {
                "paper": "2808205",
                "dataset": "Pascal dataset",
                "description": "1,000 image/text pairs from twenty categories."
              },
              {
                "paper": "2808205",
                "dataset": "NUS-WIDE-10k dataset",
                "description": "1,000 image/text pairs per category from ten categories."
              },
              {
                "paper": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining",
                "dataset": "RedPajama-V2 and MAP-CC",
                "description": "We sample an equal proportion of 15 billion English and Chinese tokens from RedPajama-V2 and MAP-CC, respectively."
              },
              {
                "paper": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
                "dataset": "Small ESP Game dataset",
                "description": "contains 100,000 labeled images with corresponding tags"
              },
              {
                "paper": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
                "dataset": "MLC-2013 dataset",
                "description": "has 1,000 manually labeled images with two labels per image"
              },
              {
                "paper": "3469877.3490585",
                "dataset": "Stanford image paragraph benchmark dataset",
                "description": "原文无此信息"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "dataset": "Restaurant14",
                "description": "All datasets have three sentimental polarities: positive, neutral, and negative"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "dataset": "Laptop14",
                "description": "All datasets have three sentimental polarities: positive, neutral, and negative"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "dataset": "Twitter",
                "description": "All datasets have three sentimental polarities: positive, neutral, and negative"
              },
              {
                "paper": "2025.coling_main.22",
                "dataset": "TWITTER-15 and TWITTER-17",
                "description": "benchmark datasets for MABSA tasks, specifically C-MABSA with conditional relations"
              },
              {
                "paper": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
                "dataset": "Stanford benchmark dataset",
                "description": "includes 14575/2487/2489 pairs for training/validation/test. The dataset comprises an average of 67.5 words per paragraph and 5.7 sentences."
              },
              {
                "paper": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
                "dataset": "CUB",
                "description": "原文无此信息"
              },
              {
                "paper": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
                "dataset": "Oxford-102",
                "description": "原文无此信息"
              },
              {
                "paper": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
                "dataset": "COCO",
                "description": "原文无此信息"
              }
            ],
            "evaluation_metrics": [
              {
                "paper": "2504.15958v2",
                "metric": "CLIP"
              },
              {
                "paper": "2504.15958v2",
                "metric": "DINOv2"
              },
              {
                "paper": "2023.acl_long.802",
                "metric": "Sentiment Graph F1 (SF1)"
              },
              {
                "paper": "2023.acl_long.802",
                "metric": "Holder F1"
              },
              {
                "paper": "2023.acl_long.802",
                "metric": "Target F1"
              },
              {
                "paper": "2023.acl_long.802",
                "metric": "Exp. F1"
              },
              {
                "paper": "2023.acl_long.802",
                "metric": "Nonpolarity Sentiment Graph F1 (NSF1)"
              },
              {
                "paper": "2022.acl_long.212",
                "metric": "F1"
              },
              {
                "paper": "3664647.3681466",
                "metric": "PSNR"
              },
              {
                "paper": "3664647.3681466",
                "metric": "MSE"
              },
              {
                "paper": "3664647.3681466",
                "metric": "fMSE"
              },
              {
                "paper": "2408.03632v3",
                "metric": "CLIP"
              },
              {
                "paper": "2408.03632v3",
                "metric": "ImageReward"
              },
              {
                "paper": "2408.03632v3",
                "metric": "Segmentation Similarity (SegSim)"
              },
              {
                "paper": "3394171.3416296",
                "metric": "Mean Average Precision (MAP)"
              },
              {
                "paper": "3664647.3680897",
                "metric": "IoU"
              },
              {
                "paper": "3664647.3680897",
                "metric": "bbox accuracy"
              },
              {
                "paper": "3664647.3680897",
                "metric": "recognition accuracy"
              },
              {
                "paper": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
                "metric": "pointing game accuracy"
              },
              {
                "paper": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
                "metric": "bounding box accuracy"
              },
              {
                "paper": "0592",
                "metric": "BELU"
              },
              {
                "paper": "0592",
                "metric": "METEOR"
              },
              {
                "paper": "0592",
                "metric": "ROUGE L"
              },
              {
                "paper": "0592",
                "metric": "CIDEr"
              },
              {
                "paper": "0592",
                "metric": "Instance Coverage (IC)"
              },
              {
                "paper": "4930_Article_Text_7995_1_10_20190709",
                "metric": "Acc(ans)"
              },
              {
                "paper": "0628",
                "metric": "F1 score"
              },
              {
                "paper": "0628",
                "metric": "accuracy"
              },
              {
                "paper": "2647868.2654902",
                "metric": "mean average precision (mAP)"
              },
              {
                "paper": "2647868.2654902",
                "metric": "top 20% percentage"
              },
              {
                "paper": "2021.acl_long.494",
                "metric": "accuracy"
              },
              {
                "paper": "2021.acl_long.494",
                "metric": "macro-averaged F1-score"
              },
              {
                "paper": "2022.emnlp_main.212",
                "metric": "Precision"
              },
              {
                "paper": "2022.emnlp_main.212",
                "metric": "Recall"
              },
              {
                "paper": "2022.emnlp_main.212",
                "metric": "F1 scores"
              },
              {
                "paper": "2022.coling_1.234",
                "metric": "P@N"
              },
              {
                "paper": "2022.coling_1.234",
                "metric": "AUC"
              },
              {
                "paper": "2022.coling_1.234",
                "metric": "Micro-F1 score"
              },
              {
                "paper": "2808205",
                "metric": "mean average precision (mAP)"
              },
              {
                "paper": "2808205",
                "metric": "top 20% percentage"
              },
              {
                "paper": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining",
                "metric": "average accuracy"
              },
              {
                "paper": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
                "metric": "accuracy"
              },
              {
                "paper": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
                "metric": "area under the ROC curve"
              },
              {
                "paper": "3469877.3490585",
                "metric": "BLEU-1"
              },
              {
                "paper": "3469877.3490585",
                "metric": "BLEU-2"
              },
              {
                "paper": "3469877.3490585",
                "metric": "BLEU-3"
              },
              {
                "paper": "3469877.3490585",
                "metric": "BLEU-4"
              },
              {
                "paper": "3469877.3490585",
                "metric": "METEOR"
              },
              {
                "paper": "3469877.3490585",
                "metric": "CIDEr"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "metric": "LAL-Parser"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "metric": "Glove vectors"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "metric": "BiLSTM"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "metric": "dropout"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "metric": "Adam optimizer"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "metric": "learning rate"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "metric": "epochs"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "metric": "batch size"
              },
              {
                "paper": "2025.coling_main.22",
                "metric": "Accuracy"
              },
              {
                "paper": "2025.coling_main.22",
                "metric": "F1 score"
              },
              {
                "paper": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
                "metric": "BLEU@{1, 2, 3, 4}"
              },
              {
                "paper": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
                "metric": "METEOR"
              },
              {
                "paper": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
                "metric": "CIDEr"
              },
              {
                "paper": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
                "metric": "BERTScore F metrics"
              },
              {
                "paper": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
                "metric": "Inception Score (IS)"
              },
              {
                "paper": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
                "metric": "Fréchet Inception Distance (FID)"
              },
              {
                "paper": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
                "metric": "R-Precision"
              }
            ],
            "performance_data": []
          }
        },
        "ai_analysis": {
          "academic_identity": {
            "primary_research_identity": "计算机视觉与自然语言处理",
            "research_signature": [
              "多模态学习",
              "图像生成",
              "情感分析"
            ],
            "expertise_domains": [
              "计算机视觉",
              "自然语言处理",
              "机器学习"
            ],
            "academic_stage": "成熟期",
            "unique_strengths": [
              "多模态学习",
              "图像生成",
              "情感分析"
            ]
          },
          "innovation_assessment": {
            "innovation_level": 8,
            "creativity_patterns": "持续创新，注重跨领域融合",
            "technical_depth": "技术深度较高，掌握多种深度学习模型",
            "methodological_innovation": "在多模态学习、图像生成等领域有创新方法",
            "breakthrough_potential": "具有突破性研究潜力",
            "innovation_consistency": "创新持续性较好"
          },
          "research_trajectory": {
            "development_pattern": "稳步发展，注重跨领域研究",
            "topic_evolution": "从计算机视觉到多模态学习、图像生成等",
            "productivity_trend": "产出稳定增长",
            "collaboration_evolution": "合作范围不断扩大",
            "impact_growth": "影响力持续提升"
          },
          "technical_capabilities": {
            "experimental_skills": "实验技能较强",
            "dataset_expertise": "熟练使用多种公开数据集",
            "evaluation_proficiency": "熟练运用多种评价指标",
            "technical_breadth": "技术广度较广",
            "implementation_ability": "技术实现能力较强"
          },
          "collaboration_analysis": {
            "network_position": "处于合作网络的中心位置",
            "collaboration_quality": "合作质量较高",
            "institutional_connections": "与多个机构保持良好合作关系",
            "leadership_potential": "具有学术领导潜力",
            "mentoring_ability": "指导能力较强"
          },
          "development_recommendations": {
            "strategic_directions": [
              "继续深耕多模态学习",
              "拓展图像生成新方向"
            ],
            "skill_enhancement": [
              "学习最新深度学习模型",
              "加强跨领域合作"
            ],
            "collaboration_targets": [
              "与顶级机构合作",
              "拓展国际合作"
            ],
            "research_priorities": [
              "多模态学习",
              "图像生成"
            ],
            "career_milestones": [
              "获得重要奖项",
              "发表顶级期刊论文"
            ],
            "risk_mitigation": [
              "关注最新研究动态",
              "加强团队建设"
            ]
          },
          "comparative_analysis": {
            "peer_comparison": "处于同领域研究者前列",
            "competitive_advantages": [
              "多模态学习",
              "图像生成",
              "情感分析"
            ],
            "improvement_areas": [
              "加强理论创新",
              "拓展应用场景"
            ],
            "market_position": "处于学术市场高端位置"
          }
        }
      },
      "Feng Fangxiang": {
        "raw_data": {
          "basic_info": {
            "standardized_name": "Feng Fangxiang",
            "name_variations": [
              "Fangxiang Feng"
            ],
            "total_papers": 19,
            "document_types": {
              "academic_paper": 19
            },
            "institutions": [
              "Beijing University of Posts & Telecommunications",
              "Engineering Research Center of Information Networks, Ministry of Education, Beijing 100876, China",
              "Li Auto Inc.",
              "School of Artificial Intelligence, Beijing University of Posts and Telecommunications",
              "Beijing Academy of Artificial Intelligence, Beijing, China",
              "Beijing Natural Science Foundation",
              "Beijing University of Posts and Telecommunications, Beijing, China",
              "Beijing Academy of Artificial Intelligence",
              "Beijing University of Posts and Telecommunications",
              "High Performance Computing Platform of BUPT",
              "National Nature Science Foundation of China",
              "Engineering Research Center of Information Networks, Ministry of Education, China",
              "School of Artificial Intelligence, Beijing University of Posts and Telecommunications, China",
              "BUPT",
              "School of Computers, Beijing University of Posts and Telecommunications, Beijing 100876, China"
            ],
            "papers": [
              "2504.15958v2",
              "1307.1275v1",
              "2022.acl_long.212",
              "3664647.3681466",
              "2408.03632v3",
              "3394171.3416296",
              "3664647.3680897",
              "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
              "2820400",
              "2647868.2654902",
              "2021.acl_long.494",
              "2022.emnlp_main.212",
              "2022.coling_1.234",
              "2808205",
              "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
              "3469877.3490585",
              "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
              "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
              "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis"
            ]
          },
          "research_content": {
            "domains": [
              "Text-to-image generation, subject-driven image generation, training-free framework",
              "Personalized content creation",
              "Image synthesis",
              "Multi-modal learning, bimodal data representations, image-tags",
              "Predictive systems for word tags",
              "Aspect-based Sentiment Analysis (ABSA), Graph Convolutional Network",
              "Sentiment analysis",
              "Natural language processing",
              "Computer vision tasks, image harmonization",
              "Digital editing",
              "Image generation and editing tasks",
              "Text-to-image synthesis with multi-concept customization",
              "Image generation",
              "AI art",
              "Virtual reality",
              "Visual feature learning for image retrieval in e-commerce",
              "E-commerce",
              "Content-based image retrieval",
              "Phrase Grounding under weak supervision",
              "Multimedia and multimodal retrieval",
              "Weakly supervised phrase grounding",
              "Image-text alignment",
              "Multimodal information processing",
              "Deep Learning for Multimedia and Emotional and Social Signals in Multimedia",
              "Cross-modal retrieval",
              "Web image database",
              "Information retrieval",
              "Aspect-based sentiment analysis",
              "Natural language processing",
              "Sentiment analysis",
              "Aspect Sentiment Triplet Extraction, Machine Reading Comprehension, Fine-grained Aspect-based Sentiment Analysis",
              "Aspect-based Sentiment Analysis",
              "Natural Language Processing",
              "Distantly supervised relation extraction",
              "Natural Language Processing",
              "Cross-modal retrieval, deep learning, autoencoder",
              "Multimodal data retrieval",
              "Multimodal data analysis using deep neural learning",
              "Image and text analysis",
              "Complex systems",
              "Image paragraph captioning",
              "Computer vision tasks",
              "Aspect-Based Sentiment Analysis",
              "sentiment analysis",
              "recommendation",
              "advertisement computation",
              "Image paragraph captioning",
              "Beijing Academy of Artificial Intelligence",
              "Computer Vision",
              "Natural Language Processing",
              "Text-to-image synthesis, Generative Adversarial Networks, Multi-modal disentangled representation learning",
              "Interactive art",
              "Computer-aided drawing"
            ],
            "methods": [
              "FreeGraftor, a training-free framework that uses cross-image feature grafting",
              "Hierarchical representations of bimodal data using MPEG-7, gist descriptors, RBMs, and a quasi-Siamese auto-encoder",
              "Enhanced Multi-Channel Graph Convolutional Network (EMC-GCN)",
              "Harmony-VAE, inverse harmonization diffusion model",
              "Concept Conductor framework with multipath sampling, layout alignment, and concept injection",
              "Utilizing product titles as supervised signals to learn image features, constructing an image classification dataset using n-grams from product titles, fine-tuning a pre-trained model, and extracting the basic max-pooling activation of convolutions (MAC) feature.",
              "Triple alignment strategies: Region-Text Alignment (RTA), Domain Alignment (DomA), and Category Alignment (CatA)",
              "Refinement-based approach using a detector-free phrase grounding model fine-tuned with a visual prompt from CLIP text-related representations",
              "Facial emotion recognition during speech; Correspondence autoencoders for cross-modal retrieval",
              "Correspondence autoencoder (Corr-AE), Corr-Cross-AE, and Corr-Full-AE",
              "DualGCN model",
              "COntext-Masked MRC (COM-MRC) framework",
              "BERT-based Graph Convolutional network Model (BGM)",
              "Correspondence Autoencoder (Corr-AE), integrating representation learning and correlation learning into a single process",
              "Bimodal Deep Architecture (BDA)",
              "Splitting to Tree Decoder (S2TD)",
              "DualGCN, integrating SynGCN and SemGCN through a mutual BiAffine module, with orthogonal and differential regularizers.",
              "DualRel model",
              "Modality disentangled discriminator, AttnGAN, DM-GAN"
            ],
            "keywords": [
              "Text-to-image generation",
              "Subject-driven image generation",
              "Cross-image feature grafting",
              "Semantic matching",
              "Efficiency",
              "Multi-subject generation",
              "Multi-modal learning",
              "Bimodal representations",
              "Image-tags",
              "Word tags",
              "Restricted Boltzmann Machines",
              "Siamese network",
              "Aspect Sentiment Triplet Extraction",
              "Graph Convolutional Network",
              "Linguistic Features",
              "Refining Strategy",
              "image harmonization",
              "latent diffusion model",
              "VAE",
              "data augmentation",
              "inverse harmonization",
              "stable diffusion",
              "Text-to-image synthesis",
              "Personalization",
              "Concept Conductor",
              "Attribute leakage",
              "Layout confusion",
              "Visual feature learning",
              "Bag of n-grams",
              "Image retrieval",
              "CNN",
              "MAC",
              "Phrase Grounding",
              "weak supervision",
              "zero-shot learning",
              "alignment strategies",
              "CLIP",
              "Weakly supervised phrase grounding",
              "Visual prompt tuning",
              "CLIP",
              "Detector-free",
              "Multimedia",
              "Deep Learning",
              "Emotion Recognition",
              "Speech",
              "Correspondence Autoencoders",
              "Cross-Modal Retrieval",
              "Cross-modal retrieval",
              "Correspondence autoencoder",
              "Representation learning",
              "Correlation learning",
              "aspect-based sentiment analysis",
              "dual graph convolutional networks",
              "dependency parsing",
              "semantic correlations",
              "regularizers",
              "Aspect Sentiment Triplet Extraction",
              "Machine Reading Comprehension",
              "Context Masking",
              "Sentiment Analysis",
              "Distant supervision",
              "Relation extraction",
              "BERT",
              "Graph Convolutional Network",
              "Cross-entropy loss",
              "Cross-modal",
              "retrieval",
              "image and text",
              "deep learning",
              "autoencoder",
              "Multimodal data",
              "Deep neural learning",
              "Similarity metric",
              "Bimodal deep architecture",
              "image captioning",
              "paragraph generation",
              "tree-structured decoder",
              "vision and language",
              "Aspect-Based Sentiment Analysis",
              "DualGCN",
              "graph convolutional networks",
              "dependency parsing",
              "semantic information",
              "Image paragraph captioning",
              "Dual Relations",
              "Spatial Relation",
              "Semantic Relation",
              "Weakly Supervised",
              "Relation-aware Attention",
              "text-to-image synthesis",
              "generative adversarial networks",
              "multi-modal disentangled representation learning"
            ],
            "innovations": [
              "Training-free cross-image feature grafting",
              "Semantic matching",
              "Position-constrained attention fusion",
              "Noise initialization strategy for geometry priors",
              "Three-level representations in three stages",
              "Bimodal auto-encoder for level-3 representations",
              "Data-specific strategy for choosing correct tag words",
              "Proposed EMC-GCN to exploit word relations",
              "Defined ten relation types",
              "Incorporated linguistic features",
              "Developed refining strategy for improved triplet extraction",
              "Harmony-VAE to enhance decoded image quality",
              "Inverse harmonization model for data augmentation",
              "Introduction of the Human Harmony dataset",
              "Training-free framework",
              "Isolates sampling processes",
              "Self-attention-based spatial guidance",
              "Shape-aware masks for concept injection",
              "Using product titles to guide the learning of visual features",
              "Conversion of product titles into discrete labels for supervised learning",
              "Bag of n-grams approach",
              "Triple alignment strategies for zero-shot Phrase Grounding under Weak Supervision",
              "CLIP-based heatmap generation",
              "Region-category relations consideration",
              "Use of similarity tokens for spatial information capture",
              "Detector-free network fine-tuning",
              "Integrates representation and correlation learning into a single process",
              "Proposes a novel loss function",
              "Extends the Corr-AE to two other correspondence models",
              "DualGCN architecture",
              "SynGCN and SemGCN modules",
              "orthogonal and differential regularizers",
              "Context augmentation strategy",
              "Discriminative model",
              "Two-stage inference method",
              "Combining a Pretrained Language Model (PLM) and a Graph Convolutional Network (GCN)",
              "Using GCN to directly learn bag representations over instances",
              "Proposed the Corr-AE",
              "Integrates representation and correlation learning",
              "Two correspondence models: Corr-Cross-AE and Corr-Full-AE",
              "Three interconnected components",
              "Stacked restricted Boltzmann machines",
              "Variant autoencoder with predefined loss function",
              "Tree-structured decoder",
              "Top-down binary tree expansion",
              "Split module",
              "Score module",
              "Tree structure loss",
              "DualGCN model",
              "SynGCN",
              "SemGCN",
              "Mutual BiAffine module",
              "orthogonal and differential regularizers",
              "Captures spatial and semantic relations among objects",
              "Weakly supervised multi-label classifier",
              "Relation-aware attention",
              "Fusion Gates",
              "Proposed modality disentangled discriminator",
              "Separate classification of content and style",
              "Enhanced text-image correlation",
              "Style transfer"
            ],
            "technical_concepts": [
              "FreeGraftor",
              "Semantic-Aware Feature Grafting",
              "Structure-Consistent Initialization",
              "Multimodal-Diffusion Transformer",
              "U-Net",
              "Transformer",
              "Stable Diffusion",
              "FLUX.1",
              "MPEG-7",
              "gist descriptors",
              "Restricted Boltzmann Machines",
              "Siamese network",
              "auto-encoder",
              "Contrastive Divergence",
              "EMC-GCN",
              "Aspect-based Sentiment Analysis",
              "Graph Convolutional Operations",
              "Linguistic Features",
              "Biaffine Attention Module",
              "Latent diffusion models",
              "Harmony-VAE",
              "Inverse harmonization model",
              "Denoising Diffusion Probabilistic Models (DDPMs)",
              "Stable Diffusion",
              "Text-to-image diffusion models",
              "Multipath sampling",
              "Layout alignment",
              "Concept injection",
              "VAE",
              "Image representations",
              "Visual content-based indexing and retrieval",
              "CNN",
              "MAC",
              "SPoC",
              "RMAC",
              "RAMAC",
              "MS-RMAC",
              "GRMAC",
              "Region-Text Alignment",
              "Domain Alignment",
              "Category Alignment",
              "Contrastive Language-Image Pre-Training (CLIP)",
              "heatmap generation",
              "CLIP",
              "Grounding model",
              "Similarity tokens",
              "Cosine similarity",
              "VGG16",
              "Facial emotion recognition",
              "Correspondence autoencoders",
              "Cross-modal retrieval",
              "Autoencoders",
              "Correlation measure",
              "Loss function",
              "Deep architecture",
              "Canonical correlation analysis",
              "Graph Convolutional Networks",
              "BiLSTM",
              "BERT",
              "dependency probability matrix",
              "self-attention mechanism",
              "COntext-Masked MRC",
              "Aspect Extraction",
              "Opinion Extraction",
              "Sentiment Classification",
              "Aspect Detection",
              "BERT-based PLM",
              "Graph Convolutional Network (GCN)",
              "Bag representation",
              "Cross-entropy loss",
              "Gradient descent optimization",
              "Autoencoders",
              "Correlation learning",
              "Multimodal reconstruction",
              "CCA",
              "Multimodal data",
              "Deep neural networks",
              "Restricted Boltzmann machines",
              "Autoencoder",
              "Similarity measure",
              "Computer vision tasks",
              "Split module",
              "Score module",
              "Word-level RNN",
              "Tree structure loss",
              "Cosine similarity",
              "LSTM",
              "Highway Network",
              "Sentence-BERT",
              "graph neural networks",
              "GCNs",
              "GATs",
              "syntax structures",
              "semantic correlations",
              "dependency trees",
              "DualRel model",
              "Faster R-CNN",
              "Relation Embedding Module",
              "Relation-aware Interaction Module",
              "Self-critical sequence training",
              "Generative Adversarial Networks (GANs)",
              "AttnGAN",
              "DM-GAN",
              "Inception Score (IS)",
              "Fréchet Inception Distance (FID)",
              "Modality disentangled representation"
            ]
          },
          "detailed_innovations": {
            "contributions_by_paper": {
              "2504.15958v2": [
                "training-free framework",
                "cross-image feature grafting",
                "structure-consistent initialization"
              ],
              "2022.acl_long.212": [
                "a novel EMC-GCN model",
                "a comprehensive exploitation of linguistic features",
                "an effective refining strategy"
              ],
              "3664647.3681466": [
                "Introduction of the Human Harmony dataset",
                "Harmony-VAE's effectiveness in enhancing the VAE component in latent diffusion models for image harmonization"
              ],
              "2408.03632v3": [
                "We introduce Concept Conductor, a training-free framework that ensures visual fidelity and correct layout in multi-concept customization."
              ],
              "3394171.3416296": [
                "method for learning product visual representations",
                "deep CNNs pre-trained and fine-tuned on this dataset can enhance feature effectiveness for product image retrieval"
              ],
              "3664647.3680897": [
                "提出了一种零样本短语接地的新框架",
                "引入了三重对齐策略"
              ],
              "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding": [
                "use of similarity tokens for spatial information capture",
                "detector-free network fine-tuning"
              ],
              "2647868.2654902": [
                "We propose the correspondence autoencoder (Corr-AE) based on two basic unimodal autoencoders.",
                "The Corr-AE integrates representation and correlation learning into a single process.",
                "A novel loss function is designed, incorporating the losses of autoencoders for all modalities and the loss of correlation between modalities."
              ],
              "2021.acl_long.494": [
                "提出了一种结合语法和语义特征的DualGCN模型",
                "设计了正交和微分正则化器以增强模型捕获语义关联的能力"
              ],
              "2022.emnlp_main.212": [
                "propose a COntext-Masked MRC (COM-MRC) framework for ASTE tasks",
                "alleviate interference in ASTE tasks",
                "the context augmentation strategy effectively expanding the training corpus"
              ],
              "2022.coling_1.234": [
                "提出了一种结合预训练语言模型和图卷积网络的简单模型"
              ],
              "2808205": [
                "We propose the correspondence autoencoder (Corr-AE), which integrates representation learning and correlation learning into a single process."
              ],
              "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture": [
                "proposes the BDA to measure similarity in multimodal systems",
                "combines feature extraction and deep neural networks",
                "demonstrates effectiveness in classifying image tags"
              ],
              "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis": [
                "proposal of DualGCN architecture",
                "integration of syntactic knowledge through SynGCN and semantic information through SemGCN",
                "orthogonal and differential regularizers"
              ],
              "Improving_Image_Paragraph_Captioning_with_Dual_Relations": [
                "提出DualRel模型，明确捕捉空间和语义关系以改进图像段落字幕生成"
              ],
              "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis": [
                "The proposed discriminator is the first to learn modality disentangled representation for text-to-image synthesis, enhancing discrimination of image-text correlation and facilitating image synthesis manipulation."
              ]
            },
            "novelty_claims_by_paper": {
              "2022.acl_long.212": [
                "Our proposed framework is EMC-GCN, which addresses Aspect and Opinion Term Co-Extraction (AOTE) and Aspect-Sentiment Pair Extraction (ASPE)",
                "We define ten types of relations between words for the ASTE task"
              ],
              "3394171.3416296": [
                "using product titles to guide the learning of visual features",
                "conversion of product titles into discrete labels for supervised learning"
              ],
              "3664647.3680897": [
                "在弱监督下实现零样本接地",
                "使用CLIP进行区域文本对齐"
              ],
              "2021.acl_long.494": [
                "DualGCN模型的结构设计",
                "正交和微分正则化器的应用"
              ],
              "2022.emnlp_main.212": [
                "COM-MRC’s components work collaboratively",
                "the two-stage inference method reduces interference from other aspects"
              ],
              "2022.coling_1.234": [
                "首次使用GCN直接学习实例之间的包表示"
              ],
              "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture": [
                "the BDA's three components are designed to extract features, stack RBMs, and use a variant autoencoder for discriminative learning"
              ],
              "3469877.3490585": [
                "We propose Splitting to Tree Decoder (S2TD), a novel tree-structured decoder that models paragraph decoding as top-down binary tree expansion."
              ],
              "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis": [
                "DualGCN architecture",
                "orthogonal and differential regularizers"
              ],
              "Improving_Image_Paragraph_Captioning_with_Dual_Relations": [
                "DualRel模型考虑了特定的语义和空间关系，并使用关系感知交互"
              ]
            },
            "technical_relationships": {
              "2504.15958v2": {
                "base_methods": [
                  {
                    "method_name": "FLUX.1",
                    "relationship_type": "基于",
                    "evidence_text": "We build upon FLUX.1, where joint attention is computed as:"
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "FreeCustom",
                    "comparison_result": "FreeCustom suffers from severe attribute confusion",
                    "evidence_text": "FreeCustom suffers from severe attribute confusion, while MS-Diffusion and OmniGen lack visual faithfulness for small objects."
                  },
                  {
                    "method_name": "MS-Diffusion",
                    "comparison_result": "MS-Diffusion and OmniGen lack visual faithfulness for small objects",
                    "evidence_text": "FreeCustom suffers from severe attribute confusion, while MS-Diffusion and OmniGen lack visual faithfulness for small objects."
                  },
                  {
                    "method_name": "OmniGen",
                    "comparison_result": "OmniGen lacks visual faithfulness for small objects",
                    "evidence_text": "FreeCustom suffers from severe attribute confusion, while MS-Diffusion and OmniGen lack visual faithfulness for small objects."
                  }
                ]
              },
              "2022.acl_long.212": {
                "base_methods": [
                  {
                    "method_name": "Graph Convolutional Network (GCN)",
                    "relationship_type": "扩展",
                    "evidence_text": "Motivated by CNN, GCN is an efficient variant operating on graphs (Kipf and Welling, 2017)."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "GTS-BERT",
                    "comparison_result": "Our EMC-GCN significantly surpasses GTS-BERT with an average of 1.96% and 2.61% F1-score on D1 and D2, respectively",
                    "evidence_text": "Under the F1 metric, our EMC-GCN model outperforms all pipeline, end-to-end, and MRC-based methods on two groups of datasets."
                  },
                  {
                    "method_name": "BMRC",
                    "comparison_result": "The term 'light' is challenging to identify by GTS-BERT and BMRC, yet 'easy' is predicted correctly by all methods",
                    "evidence_text": "The term 'light' is challenging to identify by GTS-BERT and BMRC, yet 'easy' is predicted correctly by all methods due to its closer proximity to 'transport' than 'light'."
                  }
                ]
              },
              "3664647.3681466": {
                "base_methods": [
                  {
                    "method_name": "Stable Diffusion",
                    "relationship_type": "基于",
                    "evidence_text": "Harmony-VAE is a method designed for image harmonization, which includes repairing the content of the foreground area while maintaining a harmonized appearance. It builds upon the Stable Diffusion model"
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "HDNet",
                    "comparison_result": "DiffHarmony++ shows superior performance as the foreground proportion increases.",
                    "evidence_text": "On the iHarmony4 dataset, HDNet512 outperforms DiffHarmony++ in samples with small foreground proportions (0% ∼5%), but DiffHarmony++ shows superior performance as the foreground proportion increases."
                  }
                ]
              },
              "2408.03632v3": {
                "base_methods": [
                  {
                    "method_name": "Stable Diffusion",
                    "relationship_type": "改进",
                    "evidence_text": "Our method is implemented on Stable Diffusion v1.5, utilizing layout references from SDXL."
                  },
                  {
                    "method_name": "ED-LoRA",
                    "relationship_type": "结合",
                    "evidence_text": "ED-LoRA, a method for single-concept customization, is used as our default single-concept model, involving learnable hierarchical text embeddings and low-rank adaptation (LoRA) applied to pre-trained weights."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "Custom Diffusion",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "We compare our method against three baselines: Custom Diffusion, Cones 2, and Mix-of-Show, using their official code implementations."
                  },
                  {
                    "method_name": "Cones 2",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "We compare our method against three baselines: Custom Diffusion, Cones 2, and Mix-of-Show, using their official code implementations."
                  },
                  {
                    "method_name": "Mix-of-Show",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "We compare our method against three baselines: Custom Diffusion, Cones 2, and Mix-of-Show, using their official code implementations."
                  }
                ]
              },
              "3394171.3416296": {
                "base_methods": [
                  {
                    "method_name": "Deep Convolutional Neural Networks (CNNs)",
                    "relationship_type": "基于",
                    "evidence_text": "Our method, detailed in Figure 2, addresses this by leveraging the titles of product images."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "SEResnet-152 and Densenet-201",
                    "comparison_result": "The deepest model, Densenet-201, shows the best performance among the ImageNet-1k trained models, while the shallowest model, ResNet-50, performs the worst",
                    "evidence_text": "Table 1 presents the results of five deep CNN models with four different pooling methods on the validation set."
                  }
                ]
              },
              "3664647.3680897": {
                "base_methods": [
                  {
                    "method_name": "CLIP",
                    "relationship_type": "基于",
                    "evidence_text": "Our framework uses a novel PG framework using triple alignment strategies under weak supervision: 1) Region-Text Alignment (RTA) to associate region-level attributes based on Contrastive Language-Image Pre-Training (CLIP)."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "WWbl",
                    "comparison_result": "Our CLIP-based heatmap surpasses the pseudo label used by WWbl, explaining a 9% increase in bbox accuracy.",
                    "evidence_text": "Our CLIP-based heatmap surpasses the pseudo label used by WWbl, explaining a 9% increase in bbox accuracy."
                  },
                  {
                    "method_name": "ZSGNet",
                    "comparison_result": "Our approach exceeds previous methods and will explore interpretable solutions for grounding-related tasks.",
                    "evidence_text": "Our approach exceeds previous methods and will explore interpretable solutions for grounding-related tasks."
                  }
                ]
              },
              "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding": {
                "base_methods": [
                  {
                    "method_name": "Weakly-Supervised Grounding (WSG)",
                    "relationship_type": "改进",
                    "evidence_text": "We propose a refinement-based approach using a detector-free phrase grounding model fine-tuned with a visual prompt from CLIP text-related representations."
                  },
                  {
                    "method_name": "CLIP",
                    "relationship_type": "结合",
                    "evidence_text": "Our approach leverages the relationship between CLIP and the grounding model, refining training through visual prompt tuning."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "WWbl",
                    "comparison_result": "Our method shows improvements in metrics on Flickr30K and ReferIt",
                    "evidence_text": "Our method is trained on COCO and VG train splits and evaluated on the test splits of Flickr30K, VG, and ReferIt. The performance of our method is compared with state-of-the-art DF-WSG methods quantitatively and qualitatively."
                  },
                  {
                    "method_name": "Gbs",
                    "comparison_result": "Our approach not only surpasses detector-free methods like Gbs and WWbl",
                    "evidence_text": "Our approach not only surpasses detector-free methods like Gbs and WWbl but also outperforms detector-based methods on almost all categories for Flickr30K Entities."
                  }
                ]
              },
              "2647868.2654902": {
                "base_methods": [
                  {
                    "method_name": "Correspondence LDA, deep autoencoders, deep belief networks, and deep Boltzmann Machines",
                    "relationship_type": "基于",
                    "evidence_text": "Shared layer models include topic models like Correspondence LDA and deep architectures such as deep autoencoders, deep belief networks, and deep Boltzmann Machines."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "CCA-based models and multi-modal models",
                    "comparison_result": "Our correspondence autoencoders significantly outperform other models",
                    "evidence_text": "Our correspondence autoencoders significantly outperform other models on both retrieval tasks across Wikipedia, Pascal, and NUS-WIDE-10k data sets."
                  }
                ]
              },
              "2021.acl_long.494": {
                "base_methods": [
                  {
                    "method_name": "Graph Convolutional Network (GCN)",
                    "relationship_type": "改进",
                    "evidence_text": "We propose a GCN-based method that combines syntactic and semantic features."
                  },
                  {
                    "method_name": "Attention-based LSTM",
                    "relationship_type": "改进",
                    "evidence_text": "Attention-based neural networks, including those proposed by Wang et al. (2016) and others, have been introduced to implicitly model the semantic relation between aspects and their context."
                  },
                  {
                    "method_name": "Recursive neural network by Dong et al. (2014)",
                    "relationship_type": "改进",
                    "evidence_text": "Notable works include the recursive neural network by Dong et al. (2014)"
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "ATAE-LSTM, IAN, RAM, MGAN, TNet, ASGCN, CDT, BiGCN, kumaGCN, InterGCN, R-GAT, DGEDT, BERT, R-GAT+BERT, DGEDT+BERT",
                    "comparison_result": "Our DualGCN model consistently outperforms attention-based and syntax-based methods",
                    "evidence_text": "We compare DualGCN with state-of-the-art baselines, including ATAE-LSTM, IAN, RAM, MGAN, TNet, ASGCN, CDT, BiGCN, kumaGCN, InterGCN, R-GAT, DGEDT, BERT, R-GAT+BERT, and DGEDT+BERT."
                  }
                ]
              },
              "2022.emnlp_main.212": {
                "base_methods": [
                  {
                    "method_name": "Aspect Sentiment Triplet Extraction (ASTE)",
                    "relationship_type": "基于",
                    "evidence_text": "Aspect Sentiment Triplet Extraction (ASTE) extracts sentiment triplets from sentences and is formalized as an effective machine reading comprehension (MRC) framework."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "BMRC",
                    "comparison_result": "Our COM-MRC outperforms other baselines in terms of Precision, Recall, and F1 scores on both D1 and D2 datasets",
                    "evidence_text": "Our COM-MRC outperforms other baselines in terms of Precision, Recall, and F1 scores on both D1 and D2 datasets, as shown in Tables 3 and 4."
                  },
                  {
                    "method_name": "EMC-GCN",
                    "comparison_result": "Our COM-MRC outperforms EMC-GCN",
                    "evidence_text": "Our COM-MRC outperforms EMC-GCN on datasets D1 and D2."
                  }
                ]
              },
              "2022.coling_1.234": {
                "base_methods": [
                  {
                    "method_name": "BERT-based Graph Convolutional network Model (BGM)",
                    "relationship_type": "结合",
                    "evidence_text": "We introduce BGM, a model that combines a Pretrained Language Model (PLM) and a Graph Convolutional Network (GCN)"
                  },
                  {
                    "method_name": "PCNN-based methods",
                    "relationship_type": "基于",
                    "evidence_text": "DS-RE methods can be categorized into PCNN-based and PLMs-based approaches"
                  },
                  {
                    "method_name": "PLMs-based methods",
                    "relationship_type": "基于",
                    "evidence_text": "PCNN-based methods use various techniques to acquire effective bag representations, while PLMs-based methods have shown remarkable performance"
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "multiple baseline methods",
                    "comparison_result": "report better performance",
                    "evidence_text": "We compare our BGM with multiple baseline methods and report better performance"
                  },
                  {
                    "method_name": "BGM without GCN",
                    "comparison_result": "performance drop",
                    "evidence_text": "In the ablation study, we find that removing GCN results in a performance drop"
                  },
                  {
                    "method_name": "BGM without EntCon",
                    "comparison_result": "performance drop",
                    "evidence_text": "In the ablation study, we find that removing entity connections results in a performance drop"
                  }
                ]
              },
              "2808205": {
                "base_methods": [
                  {
                    "method_name": "autoencoder",
                    "relationship_type": "改进",
                    "evidence_text": "We propose several novel models based on different autoencoders, which correlate hidden representations of a pair of autoencoders."
                  },
                  {
                    "method_name": "Canonical Correlation Analysis (CCA)",
                    "relationship_type": "结合",
                    "evidence_text": "The models are trained using a novel optimal objective that minimizes a linear combination of representation learning errors and correlation learning error."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "CCA-based models",
                    "comparison_result": "Our models achieve substantial improvements in mAP scores.",
                    "evidence_text": "Compared to CCA-AE, our Corr-AE enhances the average mAP by 53.6%, 81.5%, and 48.3% on the respective datasets."
                  },
                  {
                    "method_name": "Bimodal AE, Bimodal DBN",
                    "comparison_result": "Our Corr-AE also achieves notable improvements over Bimodal AE and Bimodal DBN.",
                    "evidence_text": "Our Corr-AE also achieves notable improvements over Bimodal AE and Bimodal DBN."
                  }
                ]
              },
              "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture": {
                "base_methods": [
                  {
                    "method_name": "deep neural learning",
                    "relationship_type": "基于",
                    "evidence_text": "Deep neural learning, inspired by biological propagation phenomena in the human brain, has seen rapid development since 2006 and has achieved success in tasks involving single modal data."
                  },
                  {
                    "method_name": "topic models, joint models, undirected Markov random fields",
                    "relationship_type": "扩展",
                    "evidence_text": "Various approaches have been proposed for learning from cross-modal data. These include extensions of topic models, joint models, and undirected Markov random fields, but these single-hidden-layer models struggle with the complexity of images and text."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "Multilayer Perceptrons (MLP)",
                    "comparison_result": "Our deep neural architecture outperforms an MLP-based system",
                    "evidence_text": "Our deep neural architecture outperforms an MLP-based system with two hidden layers and a CCA-based system with two RBMs, achieving an accuracy of 88.96%."
                  },
                  {
                    "method_name": "Canonical Correlation Analysis (CCA)",
                    "comparison_result": "The CCA-based system performs better than the MLP-based system",
                    "evidence_text": "The CCA-based system performs better than the MLP-based system."
                  }
                ]
              },
              "3469877.3490585": {
                "base_methods": [
                  {
                    "method_name": "hierarchical and non-hierarchical decoders",
                    "relationship_type": "基于",
                    "evidence_text": "Existing approaches, such as hierarchical and non-hierarchical decoders, have limitations in managing visual observations and maintaining paragraph coherence."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "DAM-ATT, SCST, SCST+RP, CRL, OR-ATT, OR-ATT+RP",
                    "comparison_result": "Our S2TD achieves competitive performance, especially in terms of BLEU-1 and CIDEr.",
                    "evidence_text": "Performance evaluation compares S2TD with state-of-the-art sequential decoding methods, grouped into hierarchical and non-hierarchical methods."
                  }
                ]
              },
              "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis": {
                "base_methods": [
                  {
                    "method_name": "GCN",
                    "relationship_type": "改进",
                    "evidence_text": "Incorporating syntactic dependency structure with graph convolutional networks (GCNs) has shown advantages, but performance depends on dependency parsers."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "ATAE-LSTM",
                    "comparison_result": "DualGCN model consistently outperforms attention-based and syntax-based methods",
                    "evidence_text": "The DualGCN model consistently outperforms attention-based and syntax-based methods on Restaurant14, Laptop14 and Twitter datasets."
                  },
                  {
                    "method_name": "GCAE",
                    "comparison_result": "DualGCN model outperforms",
                    "evidence_text": "DualGCN model outperforms SynGCN-head on the Restaurant14 and Laptop14 datasets"
                  },
                  {
                    "method_name": "R-GAT + BERT",
                    "comparison_result": "DualGCN + BERT and DualGCN + BERT-PT show improved performance over the basic DualGCN model and other BERT-based methods",
                    "evidence_text": "DualGCN + BERT and DualGCN + BERT-PT show improved performance over the basic DualGCN model and other BERT-based methods."
                  }
                ]
              },
              "Improving_Image_Paragraph_Captioning_with_Dual_Relations": {
                "base_methods": [
                  {
                    "method_name": "Faster R-CNN",
                    "relationship_type": "基于",
                    "evidence_text": "We employ Faster R-CNN [19] to detect N objects in an image, represented as C={c1, · · · , cN}."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "Regions-Hierarchical",
                    "comparison_result": "DualRel outperforms SCST on all metrics (except for a tie in METEOR) and the recent IMAP method.",
                    "evidence_text": "Baselines: We compare DualRel with baselines including Regions-Hierarchical [1], RTT-GAN [5], DAM [7], SCST [8], DCPG-VAE [6], TMOS [27], CAE-LSTM [11], DHPV [9], CVAP [10], CRL [12], Dual-CNN [15], VREN [17], IMAP [14], S2TD [16], and OR-ATT [18]."
                  },
                  {
                    "method_name": "SCST",
                    "comparison_result": "DualRel outperforms SCST on all metrics (except for a tie in METEOR)",
                    "evidence_text": "Results: Table 1 shows our DualRel method achieves the best scores in B@{1-4} and CIDEr. DualRel outperforms SCST on all metrics (except for a tie in METEOR) and the recent IMAP method."
                  }
                ]
              },
              "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis": {
                "base_methods": [
                  {
                    "method_name": "AttnGAN",
                    "relationship_type": "改进",
                    "evidence_text": "The modality disentangled discriminator is integrated into two models: AttnGAN and DM-GAN."
                  },
                  {
                    "method_name": "DM-GAN",
                    "relationship_type": "改进",
                    "evidence_text": "The modality disentangled discriminator is integrated into two models: AttnGAN and DM-GAN."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "GAN-INT-CLS",
                    "comparison_result": "原文无此信息",
                    "evidence_text": "In terms of style manipulation experiments on the CUB dataset, two types of experiments are conducted. The first involves style transfer, where the model uses modality-specific features for direct style transfer, achieving more accurate style reconstruction compared to GAN-INT-CLS."
                  }
                ]
              }
            }
          },
          "collaboration_raw": {
            "collaborators_by_paper": {
              "2504.15958v2": [
                "Fangxiang Feng",
                "Chen Wei",
                "Xiaojie Wang",
                "Zebin Yao",
                "Huixing Jiang",
                "Ruifan Li",
                "Lei Ren"
              ],
              "1307.1275v1": [
                "Fangxiang Feng",
                "Xiaojie Wang",
                "Ruifan Li"
              ],
              "2022.acl_long.212": [
                "Fangxiang Feng",
                "Xiaojie Wang",
                "Zepeng Zhai",
                "Hao Chen",
                "Ruifan Li"
              ],
              "3664647.3681466": [
                "Fangxiang Feng",
                "Xiaojie Wang",
                "Ruifan Li",
                "Guang Liu",
                "Pengfei Zhou"
              ],
              "2408.03632v3": [
                "Fangxiang Feng",
                "Xiaojie Wang",
                "Zebin Yao",
                "Ruifan Li"
              ],
              "3394171.3416296": [
                "Fangxiang Feng",
                "Xiaojie Wang",
                "Tianrui Niu",
                "Huixing Jiang",
                "Ruifan Li"
              ],
              "3664647.3680897": [
                "Fangxiang Feng",
                "Zhanyu Ma",
                "Xiaojie Wang",
                "Ruifei Zhang",
                "Zhihong Chen",
                "Guanbin Li",
                "Yuzhe Ji",
                "Zhihan Yu",
                "Yibing Song",
                "Pengyue Lin",
                "Ruifan Li",
                "Xiang Wan"
              ],
              "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding": [
                "Fangxiang Feng",
                "Xiaojie Wang",
                "Mingcong Lu",
                "Zhihan Yu",
                "Pengyue Lin",
                "Ruifan Li"
              ],
              "2820400": [
                "Fangxiang Feng",
                "George Toderici",
                "Xiaojie Wang",
                "Yelin Kim",
                "Emily Mower-Provost",
                "Hayley Hung",
                "Ruifan Li"
              ],
              "2647868.2654902": [
                "Fangxiang Feng",
                "Xiaojie Wang",
                "Ruifan Li"
              ],
              "2021.acl_long.494": [
                "Fangxiang Feng",
                "Zhanyu Ma",
                "Xiaojie Wang",
                "Eduard Hovy",
                "Hao Chen",
                "Ruifan Li"
              ],
              "2022.emnlp_main.212": [
                "Fangxiang Feng",
                "Xiaojie Wang",
                "Zepeng Zhai",
                "Hao Chen",
                "Dongyan Zhao",
                "Feifan Fan",
                "Yansong Feng",
                "Ruifan Li"
              ],
              "2022.coling_1.234": [
                "Fangxiang Feng",
                "Xiaojie Wang",
                "Ruifan Li",
                "Ziqin Rao"
              ],
              "2808205": [
                "Fangxiang Feng",
                "XIAOJIE WANG",
                "RUIFAN LI",
                "Xiaojie Wang",
                "FANGXIANG FENG",
                "IBRAR AHMAD",
                "Ruifan Li"
              ],
              "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture": [
                "Fangxiang Feng",
                "Bohan Li",
                "Xiaojie Wang",
                "Peng Lu",
                "Ruifan Li"
              ],
              "3469877.3490585": [
                "Fangxiang Feng",
                "Zhanyu Ma",
                "Yihui Shi",
                "Xiaojie Wang",
                "Yun Liu",
                "Ruifan Li"
              ],
              "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis": [
                "Fangxiang Feng",
                "Zhanyu Ma",
                "Xiaojie Wang",
                "Eduard Hovy",
                "T. Qian",
                "M. Zhang",
                "Hao Chen",
                "Ruifan Li"
              ],
              "Improving_Image_Paragraph_Captioning_with_Dual_Relations": [
                "Fangxiang Feng",
                "Zhanyu Ma",
                "Yihui Shi",
                "Xiaojie Wang",
                "Yun Liu",
                "Ruifan Li"
              ],
              "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis": [
                "Fangxiang Feng",
                "Xiaojie Wang",
                "Tianrui Niu",
                "Member, IEEE",
                "Ruifan Li"
              ]
            },
            "institutional_networks": [
              "Beijing University of Posts & Telecommunications",
              "Engineering Research Center of Information Networks, Ministry of Education, Beijing 100876, China",
              "Li Auto Inc.",
              "School of Artificial Intelligence, Beijing University of Posts and Telecommunications",
              "Beijing Academy of Artificial Intelligence, Beijing, China",
              "Beijing Natural Science Foundation",
              "Beijing University of Posts and Telecommunications, Beijing, China",
              "Beijing Academy of Artificial Intelligence",
              "Beijing University of Posts and Telecommunications",
              "High Performance Computing Platform of BUPT",
              "National Nature Science Foundation of China",
              "Engineering Research Center of Information Networks, Ministry of Education, China",
              "School of Artificial Intelligence, Beijing University of Posts and Telecommunications, China",
              "BUPT",
              "School of Computers, Beijing University of Posts and Telecommunications, Beijing 100876, China"
            ]
          },
          "experimental_details": {
            "datasets_used": [
              {
                "paper": "2504.15958v2",
                "dataset": "DreamBench",
                "description": "原文无此信息"
              },
              {
                "paper": "2504.15958v2",
                "dataset": "CustomConcept101",
                "description": "原文无此信息"
              },
              {
                "paper": "2504.15958v2",
                "dataset": "Mix-of-Show",
                "description": "原文无此信息"
              },
              {
                "paper": "2022.acl_long.212",
                "dataset": "SemEval ABSA Challenges (Pontiki et al., 2014, 2015, 2016)",
                "description": "two ABSA datasets"
              },
              {
                "paper": "3664647.3681466",
                "dataset": "iHarmony4",
                "description": "Includes HCOCO, HFlickr, HAdobe5K, and Hday2night sub-datasets, consisting of 65,742 training and 7,404 testing pairs of composite and real images."
              },
              {
                "paper": "3664647.3681466",
                "dataset": "Human Harmony",
                "description": "Created by filtering the imaterialist-fashion-2020-fgvc7 dataset to exclude images with only products. The cleaned dataset contains 29,106 images."
              },
              {
                "paper": "2408.03632v3",
                "dataset": "Mix-of-show",
                "description": "原文无此信息"
              },
              {
                "paper": "2408.03632v3",
                "dataset": "DreamBooth",
                "description": "原文无此信息"
              },
              {
                "paper": "2408.03632v3",
                "dataset": "CustomConcept101",
                "description": "原文无此信息"
              },
              {
                "paper": "3394171.3416296",
                "dataset": "Perfect-500K",
                "description": "dataset from the 'AI Meets Beauty' challenge"
              },
              {
                "paper": "3664647.3680897",
                "dataset": "Flickr-Split-S0",
                "description": "零样本PG设置下的数据集"
              },
              {
                "paper": "3664647.3680897",
                "dataset": "Flickr-Split-S1",
                "description": "零样本PG设置下的数据集"
              },
              {
                "paper": "3664647.3680897",
                "dataset": "VG-Split-S2",
                "description": "零样本PG设置下的数据集"
              },
              {
                "paper": "3664647.3680897",
                "dataset": "VG-Split-S3",
                "description": "零样本PG设置下的数据集"
              },
              {
                "paper": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
                "dataset": "Flickr30K Entities",
                "description": "原文无此信息"
              },
              {
                "paper": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
                "dataset": "COCO",
                "description": "原文无此信息"
              },
              {
                "paper": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
                "dataset": "Visual Genome",
                "description": "原文无此信息"
              },
              {
                "paper": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
                "dataset": "ReferIt",
                "description": "原文无此信息"
              },
              {
                "paper": "2647868.2654902",
                "dataset": "Wikipedia, Pascal, NUS-WIDE-10k",
                "description": "These datasets vary in text modality, size, and category numbers."
              },
              {
                "paper": "2021.acl_long.494",
                "dataset": "Restaurant, Laptop, Twitter",
                "description": "三个公开数据集"
              },
              {
                "paper": "2022.emnlp_main.212",
                "dataset": "D1 (Wu et al., 2020a)",
                "description": "benchmark datasets from SemEval Challenges"
              },
              {
                "paper": "2022.emnlp_main.212",
                "dataset": "D2 (Xu et al., 2020)",
                "description": "benchmark datasets from SemEval Challenges"
              },
              {
                "paper": "2022.coling_1.234",
                "dataset": "NYT10",
                "description": "原文无此信息"
              },
              {
                "paper": "2022.coling_1.234",
                "dataset": "GDS",
                "description": "原文无此信息"
              },
              {
                "paper": "2808205",
                "dataset": "Wikipedia dataset",
                "description": "2,866 image/text pairs from ten semantic categories."
              },
              {
                "paper": "2808205",
                "dataset": "Pascal dataset",
                "description": "1,000 image/text pairs from twenty categories."
              },
              {
                "paper": "2808205",
                "dataset": "NUS-WIDE-10k dataset",
                "description": "1,000 image/text pairs per category from ten categories."
              },
              {
                "paper": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
                "dataset": "Small ESP Game dataset",
                "description": "contains 100,000 labeled images with corresponding tags"
              },
              {
                "paper": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
                "dataset": "MLC-2013 dataset",
                "description": "has 1,000 manually labeled images with two labels per image"
              },
              {
                "paper": "3469877.3490585",
                "dataset": "Stanford image paragraph benchmark dataset",
                "description": "原文无此信息"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "dataset": "Restaurant14",
                "description": "All datasets have three sentimental polarities: positive, neutral, and negative"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "dataset": "Laptop14",
                "description": "All datasets have three sentimental polarities: positive, neutral, and negative"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "dataset": "Twitter",
                "description": "All datasets have three sentimental polarities: positive, neutral, and negative"
              },
              {
                "paper": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
                "dataset": "Stanford benchmark dataset",
                "description": "includes 14575/2487/2489 pairs for training/validation/test. The dataset comprises an average of 67.5 words per paragraph and 5.7 sentences."
              },
              {
                "paper": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
                "dataset": "CUB",
                "description": "原文无此信息"
              },
              {
                "paper": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
                "dataset": "Oxford-102",
                "description": "原文无此信息"
              },
              {
                "paper": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
                "dataset": "COCO",
                "description": "原文无此信息"
              }
            ],
            "evaluation_metrics": [
              {
                "paper": "2504.15958v2",
                "metric": "CLIP"
              },
              {
                "paper": "2504.15958v2",
                "metric": "DINOv2"
              },
              {
                "paper": "2022.acl_long.212",
                "metric": "F1"
              },
              {
                "paper": "3664647.3681466",
                "metric": "PSNR"
              },
              {
                "paper": "3664647.3681466",
                "metric": "MSE"
              },
              {
                "paper": "3664647.3681466",
                "metric": "fMSE"
              },
              {
                "paper": "2408.03632v3",
                "metric": "CLIP"
              },
              {
                "paper": "2408.03632v3",
                "metric": "ImageReward"
              },
              {
                "paper": "2408.03632v3",
                "metric": "Segmentation Similarity (SegSim)"
              },
              {
                "paper": "3394171.3416296",
                "metric": "Mean Average Precision (MAP)"
              },
              {
                "paper": "3664647.3680897",
                "metric": "IoU"
              },
              {
                "paper": "3664647.3680897",
                "metric": "bbox accuracy"
              },
              {
                "paper": "3664647.3680897",
                "metric": "recognition accuracy"
              },
              {
                "paper": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
                "metric": "pointing game accuracy"
              },
              {
                "paper": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
                "metric": "bounding box accuracy"
              },
              {
                "paper": "2647868.2654902",
                "metric": "mean average precision (mAP)"
              },
              {
                "paper": "2647868.2654902",
                "metric": "top 20% percentage"
              },
              {
                "paper": "2021.acl_long.494",
                "metric": "accuracy"
              },
              {
                "paper": "2021.acl_long.494",
                "metric": "macro-averaged F1-score"
              },
              {
                "paper": "2022.emnlp_main.212",
                "metric": "Precision"
              },
              {
                "paper": "2022.emnlp_main.212",
                "metric": "Recall"
              },
              {
                "paper": "2022.emnlp_main.212",
                "metric": "F1 scores"
              },
              {
                "paper": "2022.coling_1.234",
                "metric": "P@N"
              },
              {
                "paper": "2022.coling_1.234",
                "metric": "AUC"
              },
              {
                "paper": "2022.coling_1.234",
                "metric": "Micro-F1 score"
              },
              {
                "paper": "2808205",
                "metric": "mean average precision (mAP)"
              },
              {
                "paper": "2808205",
                "metric": "top 20% percentage"
              },
              {
                "paper": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
                "metric": "accuracy"
              },
              {
                "paper": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
                "metric": "area under the ROC curve"
              },
              {
                "paper": "3469877.3490585",
                "metric": "BLEU-1"
              },
              {
                "paper": "3469877.3490585",
                "metric": "BLEU-2"
              },
              {
                "paper": "3469877.3490585",
                "metric": "BLEU-3"
              },
              {
                "paper": "3469877.3490585",
                "metric": "BLEU-4"
              },
              {
                "paper": "3469877.3490585",
                "metric": "METEOR"
              },
              {
                "paper": "3469877.3490585",
                "metric": "CIDEr"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "metric": "LAL-Parser"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "metric": "Glove vectors"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "metric": "BiLSTM"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "metric": "dropout"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "metric": "Adam optimizer"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "metric": "learning rate"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "metric": "epochs"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "metric": "batch size"
              },
              {
                "paper": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
                "metric": "BLEU@{1, 2, 3, 4}"
              },
              {
                "paper": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
                "metric": "METEOR"
              },
              {
                "paper": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
                "metric": "CIDEr"
              },
              {
                "paper": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
                "metric": "BERTScore F metrics"
              },
              {
                "paper": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
                "metric": "Inception Score (IS)"
              },
              {
                "paper": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
                "metric": "Fréchet Inception Distance (FID)"
              },
              {
                "paper": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
                "metric": "R-Precision"
              }
            ],
            "performance_data": []
          }
        },
        "ai_analysis": {
          "academic_identity": {
            "primary_research_identity": "计算机视觉与自然语言处理",
            "research_signature": [
              "文本到图像生成",
              "多模态学习",
              "图像字幕生成"
            ],
            "expertise_domains": [
              "计算机视觉",
              "自然语言处理",
              "机器学习"
            ],
            "academic_stage": "成熟期",
            "unique_strengths": [
              "跨模态学习",
              "图像生成",
              "文本理解"
            ]
          },
          "innovation_assessment": {
            "innovation_level": 8,
            "creativity_patterns": "持续创新，注重方法创新",
            "technical_depth": "深厚",
            "methodological_innovation": "强",
            "breakthrough_potential": "高",
            "innovation_consistency": "持续"
          },
          "research_trajectory": {
            "development_pattern": "稳步上升",
            "topic_evolution": "从文本到图像生成到多模态学习",
            "productivity_trend": "持续增长",
            "collaboration_evolution": "合作范围扩大",
            "impact_growth": "影响力持续提升"
          },
          "technical_capabilities": {
            "experimental_skills": "强",
            "dataset_expertise": "丰富",
            "evaluation_proficiency": "熟练",
            "technical_breadth": "广",
            "implementation_ability": "强"
          },
          "collaboration_analysis": {
            "network_position": "核心",
            "collaboration_quality": "高",
            "institutional_connections": "广泛",
            "leadership_potential": "强",
            "mentoring_ability": "强"
          },
          "development_recommendations": {
            "strategic_directions": [
              "跨模态学习",
              "图像生成",
              "文本理解"
            ],
            "skill_enhancement": [
              "强化深度学习",
              "拓展合作网络"
            ],
            "collaboration_targets": [
              "跨学科合作"
            ],
            "research_priorities": [
              "多模态学习"
            ],
            "career_milestones": [
              "获得重要奖项"
            ],
            "risk_mitigation": [
              "避免技术过时"
            ]
          },
          "comparative_analysis": {
            "peer_comparison": "领先",
            "competitive_advantages": [
              "跨模态学习",
              "图像生成",
              "文本理解"
            ],
            "improvement_areas": [
              "强化深度学习"
            ],
            "market_position": "领先"
          }
        }
      },
      "Ma Zhanyu": {
        "raw_data": {
          "basic_info": {
            "standardized_name": "Ma Zhanyu",
            "name_variations": [
              "Zhanyu Ma"
            ],
            "total_papers": 7,
            "document_types": {
              "academic_paper": 7
            },
            "institutions": [
              "School of Artiﬁcial Intelligence, Beijing University of Posts and Telecommunications, Beijing, China",
              "School of Artificial Intelligence, Beijing University of Posts and Telecommunications, Beijing, China",
              "Beijing Academy of Artificial Intelligence, Beijing, China",
              "Beijing Natural Science Foundation",
              "High Performance Computing Platform of BUPT",
              "National Nature Science Foundation of China",
              "School of Artificial Intelligence, Beijing University of Posts and Telecommunications, China",
              "BUPT"
            ],
            "papers": [
              "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning",
              "3664647.3680897",
              "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding",
              "2021.acl_long.494",
              "3469877.3490585",
              "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
              "Improving_Image_Paragraph_Captioning_with_Dual_Relations"
            ]
          },
          "research_content": {
            "domains": [
              "Few-shot learning in NLP",
              "NLP tasks",
              "Chinese text classification",
              "textual entailment tasks",
              "Phrase Grounding under weak supervision",
              "Multimedia and multimodal retrieval",
              "Image captioning",
              "Computer Vision",
              "Natural Language Processing",
              "Aspect-based sentiment analysis",
              "Natural language processing",
              "Sentiment analysis",
              "Image paragraph captioning",
              "Computer vision tasks",
              "Aspect-Based Sentiment Analysis",
              "sentiment analysis",
              "recommendation",
              "advertisement computation",
              "Image paragraph captioning",
              "Beijing Academy of Artificial Intelligence",
              "Computer Vision",
              "Natural Language Processing"
            ],
            "methods": [
              "Template selection mechanism using a masked language model",
              "Triple alignment strategies: Region-Text Alignment (RTA), Domain Alignment (DomA), and Category Alignment (CatA)",
              "Improved Transformer with IoU Position encoding model (TIP)",
              "DualGCN model",
              "Splitting to Tree Decoder (S2TD)",
              "DualGCN, integrating SynGCN and SemGCN through a mutual BiAffine module, with orthogonal and differential regularizers.",
              "DualRel model"
            ],
            "keywords": [
              "Few-shot learning",
              "textual entailment",
              "template selection",
              "MacBERT",
              "FewCLUE",
              "Phrase Grounding",
              "weak supervision",
              "zero-shot learning",
              "alignment strategies",
              "CLIP",
              "Image Captioning",
              "Transformer",
              "IoU Position Encoding",
              "Intra-modal Attention",
              "aspect-based sentiment analysis",
              "dual graph convolutional networks",
              "dependency parsing",
              "semantic correlations",
              "regularizers",
              "image captioning",
              "paragraph generation",
              "tree-structured decoder",
              "vision and language",
              "Aspect-Based Sentiment Analysis",
              "DualGCN",
              "graph convolutional networks",
              "dependency parsing",
              "semantic information",
              "Image paragraph captioning",
              "Dual Relations",
              "Spatial Relation",
              "Semantic Relation",
              "Weakly Supervised",
              "Relation-aware Attention"
            ],
            "innovations": [
              "Using a template selection mechanism to assess candidate templates",
              "Applying the method on FewCLUE shared tasks",
              "Triple alignment strategies for zero-shot Phrase Grounding under Weak Supervision",
              "CLIP-based heatmap generation",
              "Region-category relations consideration",
              "Intra-modal attention mechanism",
              "IoU spatial position encoding method",
              "DualGCN architecture",
              "SynGCN and SemGCN modules",
              "orthogonal and differential regularizers",
              "Tree-structured decoder",
              "Top-down binary tree expansion",
              "Split module",
              "Score module",
              "Tree structure loss",
              "DualGCN model",
              "SynGCN",
              "SemGCN",
              "Mutual BiAffine module",
              "orthogonal and differential regularizers",
              "Captures spatial and semantic relations among objects",
              "Weakly supervised multi-label classifier",
              "Relation-aware attention",
              "Fusion Gates"
            ],
            "technical_concepts": [
              "Entailment-based Few-shot Learning (EFL)",
              "Masked Language Model (MLM)",
              "MacBERT",
              "PyTorch",
              "HuggingFace toolkit",
              "Region-Text Alignment",
              "Domain Alignment",
              "Category Alignment",
              "Contrastive Language-Image Pre-Training (CLIP)",
              "heatmap generation",
              "CNNs",
              "RNNs",
              "Attention Mechanisms",
              "Transformer Structure",
              "IoU Spatial Position Encoding",
              "Graph Convolutional Networks",
              "BiLSTM",
              "BERT",
              "dependency probability matrix",
              "self-attention mechanism",
              "Computer vision tasks",
              "Split module",
              "Score module",
              "Word-level RNN",
              "Tree structure loss",
              "Cosine similarity",
              "LSTM",
              "Highway Network",
              "Sentence-BERT",
              "graph neural networks",
              "GCNs",
              "GATs",
              "syntax structures",
              "semantic correlations",
              "dependency trees",
              "DualRel model",
              "Faster R-CNN",
              "Relation Embedding Module",
              "Relation-aware Interaction Module",
              "Self-critical sequence training"
            ]
          },
          "detailed_innovations": {
            "contributions_by_paper": {
              "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning": [
                "introducing a template selection mechanism using a masked language model to assess candidate templates"
              ],
              "3664647.3680897": [
                "提出了一种零样本短语接地的新框架",
                "引入了三重对齐策略"
              ],
              "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding": [
                "提出了一种改进的Transformer模型TIP",
                "引入了IoU空间位置编码方法"
              ],
              "2021.acl_long.494": [
                "提出了一种结合语法和语义特征的DualGCN模型",
                "设计了正交和微分正则化器以增强模型捕获语义关联的能力"
              ],
              "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis": [
                "proposal of DualGCN architecture",
                "integration of syntactic knowledge through SynGCN and semantic information through SemGCN",
                "orthogonal and differential regularizers"
              ],
              "Improving_Image_Paragraph_Captioning_with_Dual_Relations": [
                "提出DualRel模型，明确捕捉空间和语义关系以改进图像段落字幕生成"
              ]
            },
            "novelty_claims_by_paper": {
              "3664647.3680897": [
                "在弱监督下实现零样本接地",
                "使用CLIP进行区域文本对齐"
              ],
              "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding": [
                "TIP模型结合了模态内注意力机制和视觉与空间特征的融合"
              ],
              "2021.acl_long.494": [
                "DualGCN模型的结构设计",
                "正交和微分正则化器的应用"
              ],
              "3469877.3490585": [
                "We propose Splitting to Tree Decoder (S2TD), a novel tree-structured decoder that models paragraph decoding as top-down binary tree expansion."
              ],
              "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis": [
                "DualGCN architecture",
                "orthogonal and differential regularizers"
              ],
              "Improving_Image_Paragraph_Captioning_with_Dual_Relations": [
                "DualRel模型考虑了特定的语义和空间关系，并使用关系感知交互"
              ]
            },
            "technical_relationships": {
              "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning": {
                "base_methods": [
                  {
                    "method_name": "MacBERT",
                    "relationship_type": "基于",
                    "evidence_text": "We choose MacBERT, a pre-training model, as our backbone"
                  },
                  {
                    "method_name": "Entailment-based Few-shot Learning (EFL)",
                    "relationship_type": "结合",
                    "evidence_text": "We address this by introducing a template selection mechanism using a masked language model to assess candidate templates"
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "PET",
                    "comparison_result": "EFL is more effective on sentence-pair tasks, while PET is better for single sentence classification",
                    "evidence_text": "The results on the testing datasets of the nine NLP tasks show that the EFL method with automatic template selection outperforms other methods"
                  }
                ]
              },
              "3664647.3680897": {
                "base_methods": [
                  {
                    "method_name": "CLIP",
                    "relationship_type": "基于",
                    "evidence_text": "Our framework uses a novel PG framework using triple alignment strategies under weak supervision: 1) Region-Text Alignment (RTA) to associate region-level attributes based on Contrastive Language-Image Pre-Training (CLIP)."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "WWbl",
                    "comparison_result": "Our CLIP-based heatmap surpasses the pseudo label used by WWbl, explaining a 9% increase in bbox accuracy.",
                    "evidence_text": "Our CLIP-based heatmap surpasses the pseudo label used by WWbl, explaining a 9% increase in bbox accuracy."
                  },
                  {
                    "method_name": "ZSGNet",
                    "comparison_result": "Our approach exceeds previous methods and will explore interpretable solutions for grounding-related tasks.",
                    "evidence_text": "Our approach exceeds previous methods and will explore interpretable solutions for grounding-related tasks."
                  }
                ]
              },
              "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding": {
                "base_methods": [
                  {
                    "method_name": "Neural Image Captioning (NIC)",
                    "relationship_type": "基于",
                    "evidence_text": "The Neural Image Captioning (NIC) model uses a convolutional neural network to extract image features and an LSTM to translate these into sentences."
                  },
                  {
                    "method_name": "Transformer",
                    "relationship_type": "改进",
                    "evidence_text": "To address this, we adopt the transformer structure as the decoder."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "CoordNorm(hw), Coord(hw), Coord",
                    "comparison_result": "IoUc and IoU+ models further enhance performance",
                    "evidence_text": "Table III presents the performance of different position encoding methods. The CoordNorm(hw) model shows improvements over the Coord(hw) and Coord models, indicating the effectiveness of normalization."
                  }
                ]
              },
              "2021.acl_long.494": {
                "base_methods": [
                  {
                    "method_name": "Graph Convolutional Network (GCN)",
                    "relationship_type": "改进",
                    "evidence_text": "We propose a GCN-based method that combines syntactic and semantic features."
                  },
                  {
                    "method_name": "Attention-based LSTM",
                    "relationship_type": "改进",
                    "evidence_text": "Attention-based neural networks, including those proposed by Wang et al. (2016) and others, have been introduced to implicitly model the semantic relation between aspects and their context."
                  },
                  {
                    "method_name": "Recursive neural network by Dong et al. (2014)",
                    "relationship_type": "改进",
                    "evidence_text": "Notable works include the recursive neural network by Dong et al. (2014)"
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "ATAE-LSTM, IAN, RAM, MGAN, TNet, ASGCN, CDT, BiGCN, kumaGCN, InterGCN, R-GAT, DGEDT, BERT, R-GAT+BERT, DGEDT+BERT",
                    "comparison_result": "Our DualGCN model consistently outperforms attention-based and syntax-based methods",
                    "evidence_text": "We compare DualGCN with state-of-the-art baselines, including ATAE-LSTM, IAN, RAM, MGAN, TNet, ASGCN, CDT, BiGCN, kumaGCN, InterGCN, R-GAT, DGEDT, BERT, R-GAT+BERT, and DGEDT+BERT."
                  }
                ]
              },
              "3469877.3490585": {
                "base_methods": [
                  {
                    "method_name": "hierarchical and non-hierarchical decoders",
                    "relationship_type": "基于",
                    "evidence_text": "Existing approaches, such as hierarchical and non-hierarchical decoders, have limitations in managing visual observations and maintaining paragraph coherence."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "DAM-ATT, SCST, SCST+RP, CRL, OR-ATT, OR-ATT+RP",
                    "comparison_result": "Our S2TD achieves competitive performance, especially in terms of BLEU-1 and CIDEr.",
                    "evidence_text": "Performance evaluation compares S2TD with state-of-the-art sequential decoding methods, grouped into hierarchical and non-hierarchical methods."
                  }
                ]
              },
              "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis": {
                "base_methods": [
                  {
                    "method_name": "GCN",
                    "relationship_type": "改进",
                    "evidence_text": "Incorporating syntactic dependency structure with graph convolutional networks (GCNs) has shown advantages, but performance depends on dependency parsers."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "ATAE-LSTM",
                    "comparison_result": "DualGCN model consistently outperforms attention-based and syntax-based methods",
                    "evidence_text": "The DualGCN model consistently outperforms attention-based and syntax-based methods on Restaurant14, Laptop14 and Twitter datasets."
                  },
                  {
                    "method_name": "GCAE",
                    "comparison_result": "DualGCN model outperforms",
                    "evidence_text": "DualGCN model outperforms SynGCN-head on the Restaurant14 and Laptop14 datasets"
                  },
                  {
                    "method_name": "R-GAT + BERT",
                    "comparison_result": "DualGCN + BERT and DualGCN + BERT-PT show improved performance over the basic DualGCN model and other BERT-based methods",
                    "evidence_text": "DualGCN + BERT and DualGCN + BERT-PT show improved performance over the basic DualGCN model and other BERT-based methods."
                  }
                ]
              },
              "Improving_Image_Paragraph_Captioning_with_Dual_Relations": {
                "base_methods": [
                  {
                    "method_name": "Faster R-CNN",
                    "relationship_type": "基于",
                    "evidence_text": "We employ Faster R-CNN [19] to detect N objects in an image, represented as C={c1, · · · , cN}."
                  }
                ],
                "compared_methods": [
                  {
                    "method_name": "Regions-Hierarchical",
                    "comparison_result": "DualRel outperforms SCST on all metrics (except for a tie in METEOR) and the recent IMAP method.",
                    "evidence_text": "Baselines: We compare DualRel with baselines including Regions-Hierarchical [1], RTT-GAN [5], DAM [7], SCST [8], DCPG-VAE [6], TMOS [27], CAE-LSTM [11], DHPV [9], CVAP [10], CRL [12], Dual-CNN [15], VREN [17], IMAP [14], S2TD [16], and OR-ATT [18]."
                  },
                  {
                    "method_name": "SCST",
                    "comparison_result": "DualRel outperforms SCST on all metrics (except for a tie in METEOR)",
                    "evidence_text": "Results: Table 1 shows our DualRel method achieves the best scores in B@{1-4} and CIDEr. DualRel outperforms SCST on all metrics (except for a tie in METEOR) and the recent IMAP method."
                  }
                ]
              }
            }
          },
          "collaboration_raw": {
            "collaborators_by_paper": {
              "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning": [
                "Zhanyu Ma",
                "Lihui Zhang",
                "Zhiyu Wei",
                "Zeyuan Wang",
                "Ruifan Li"
              ],
              "3664647.3680897": [
                "Fangxiang Feng",
                "Zhanyu Ma",
                "Xiaojie Wang",
                "Ruifei Zhang",
                "Zhihong Chen",
                "Guanbin Li",
                "Yuzhe Ji",
                "Zhihan Yu",
                "Yibing Song",
                "Pengyue Lin",
                "Ruifan Li",
                "Xiang Wan"
              ],
              "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding": [
                "Yazhou Li",
                "Yihui Shi",
                "Zhanyu Ma",
                "Yun Liu",
                "Ruifan Li"
              ],
              "2021.acl_long.494": [
                "Fangxiang Feng",
                "Zhanyu Ma",
                "Xiaojie Wang",
                "Eduard Hovy",
                "Hao Chen",
                "Ruifan Li"
              ],
              "3469877.3490585": [
                "Fangxiang Feng",
                "Zhanyu Ma",
                "Yihui Shi",
                "Xiaojie Wang",
                "Yun Liu",
                "Ruifan Li"
              ],
              "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis": [
                "Fangxiang Feng",
                "Zhanyu Ma",
                "Xiaojie Wang",
                "Eduard Hovy",
                "T. Qian",
                "M. Zhang",
                "Hao Chen",
                "Ruifan Li"
              ],
              "Improving_Image_Paragraph_Captioning_with_Dual_Relations": [
                "Fangxiang Feng",
                "Zhanyu Ma",
                "Yihui Shi",
                "Xiaojie Wang",
                "Yun Liu",
                "Ruifan Li"
              ]
            },
            "institutional_networks": [
              "School of Artiﬁcial Intelligence, Beijing University of Posts and Telecommunications, Beijing, China",
              "School of Artificial Intelligence, Beijing University of Posts and Telecommunications, Beijing, China",
              "Beijing Academy of Artificial Intelligence, Beijing, China",
              "Beijing Natural Science Foundation",
              "High Performance Computing Platform of BUPT",
              "National Nature Science Foundation of China",
              "School of Artificial Intelligence, Beijing University of Posts and Telecommunications, China",
              "BUPT"
            ]
          },
          "experimental_details": {
            "datasets_used": [
              {
                "paper": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning",
                "dataset": "Few-CLUE",
                "description": "encompassing sentiment analysis, short text classification, long text classification, natural language inference, sentence similarity, Chinese cloze, and co-reference resolution"
              },
              {
                "paper": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning",
                "dataset": "CMNLI",
                "description": "used for intermediate training"
              },
              {
                "paper": "3664647.3680897",
                "dataset": "Flickr-Split-S0",
                "description": "零样本PG设置下的数据集"
              },
              {
                "paper": "3664647.3680897",
                "dataset": "Flickr-Split-S1",
                "description": "零样本PG设置下的数据集"
              },
              {
                "paper": "3664647.3680897",
                "dataset": "VG-Split-S2",
                "description": "零样本PG设置下的数据集"
              },
              {
                "paper": "3664647.3680897",
                "dataset": "VG-Split-S3",
                "description": "零样本PG设置下的数据集"
              },
              {
                "paper": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding",
                "dataset": "MS-COCO",
                "description": "包括 82,783 训练、40,504 验证和 40,775 测试图像"
              },
              {
                "paper": "2021.acl_long.494",
                "dataset": "Restaurant, Laptop, Twitter",
                "description": "三个公开数据集"
              },
              {
                "paper": "3469877.3490585",
                "dataset": "Stanford image paragraph benchmark dataset",
                "description": "原文无此信息"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "dataset": "Restaurant14",
                "description": "All datasets have three sentimental polarities: positive, neutral, and negative"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "dataset": "Laptop14",
                "description": "All datasets have three sentimental polarities: positive, neutral, and negative"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "dataset": "Twitter",
                "description": "All datasets have three sentimental polarities: positive, neutral, and negative"
              },
              {
                "paper": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
                "dataset": "Stanford benchmark dataset",
                "description": "includes 14575/2487/2489 pairs for training/validation/test. The dataset comprises an average of 67.5 words per paragraph and 5.7 sentences."
              }
            ],
            "evaluation_metrics": [
              {
                "paper": "3664647.3680897",
                "metric": "IoU"
              },
              {
                "paper": "3664647.3680897",
                "metric": "bbox accuracy"
              },
              {
                "paper": "3664647.3680897",
                "metric": "recognition accuracy"
              },
              {
                "paper": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding",
                "metric": "CIDEr"
              },
              {
                "paper": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding",
                "metric": "BLEU"
              },
              {
                "paper": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding",
                "metric": "METEOR"
              },
              {
                "paper": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding",
                "metric": "ROUGE"
              },
              {
                "paper": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding",
                "metric": "SPICE"
              },
              {
                "paper": "2021.acl_long.494",
                "metric": "accuracy"
              },
              {
                "paper": "2021.acl_long.494",
                "metric": "macro-averaged F1-score"
              },
              {
                "paper": "3469877.3490585",
                "metric": "BLEU-1"
              },
              {
                "paper": "3469877.3490585",
                "metric": "BLEU-2"
              },
              {
                "paper": "3469877.3490585",
                "metric": "BLEU-3"
              },
              {
                "paper": "3469877.3490585",
                "metric": "BLEU-4"
              },
              {
                "paper": "3469877.3490585",
                "metric": "METEOR"
              },
              {
                "paper": "3469877.3490585",
                "metric": "CIDEr"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "metric": "LAL-Parser"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "metric": "Glove vectors"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "metric": "BiLSTM"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "metric": "dropout"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "metric": "Adam optimizer"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "metric": "learning rate"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "metric": "epochs"
              },
              {
                "paper": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
                "metric": "batch size"
              },
              {
                "paper": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
                "metric": "BLEU@{1, 2, 3, 4}"
              },
              {
                "paper": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
                "metric": "METEOR"
              },
              {
                "paper": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
                "metric": "CIDEr"
              },
              {
                "paper": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
                "metric": "BERTScore F metrics"
              }
            ],
            "performance_data": []
          }
        },
        "ai_analysis": {
          "academic_identity": {
            "primary_research_identity": "人工智能领域的研究者，专注于自然语言处理和计算机视觉",
            "research_signature": [
              "模板选择机制",
              "图卷积网络",
              "多模态学习"
            ],
            "expertise_domains": [
              "自然语言处理",
              "计算机视觉",
              "多模态学习"
            ],
            "academic_stage": "处于学术成长期，具有较大的发展潜力",
            "unique_strengths": [
              "跨领域研究能力",
              "创新方法应用",
              "多学科交叉"
            ]
          },
          "innovation_assessment": {
            "innovation_level": 8,
            "creativity_patterns": "注重方法创新，善于跨领域融合",
            "technical_depth": "技术深度较好，掌握前沿算法",
            "methodological_innovation": "在模板选择、图卷积网络等方面有创新",
            "breakthrough_potential": "具有突破性研究潜力",
            "innovation_consistency": "创新持续性较好"
          },
          "research_trajectory": {
            "development_pattern": "从自然语言处理扩展到计算机视觉和多模态学习",
            "topic_evolution": "关注模板选择、图卷积网络、多模态学习等主题",
            "productivity_trend": "论文产出呈增长趋势",
            "collaboration_evolution": "合作网络不断扩大",
            "impact_growth": "学术影响力持续提升"
          },
          "technical_capabilities": {
            "experimental_skills": "实验能力较强，掌握多种评估方法",
            "dataset_expertise": "熟练使用公开数据集",
            "evaluation_proficiency": "评估方法应用熟练",
            "technical_breadth": "技术广度较好，跨多个领域",
            "implementation_ability": "技术实现能力强"
          },
          "collaboration_analysis": {
            "network_position": "处于合作网络的中心位置",
            "collaboration_quality": "合作质量较高，与多位知名学者合作",
            "institutional_connections": "与多个机构有合作关系",
            "leadership_potential": "具有学术领导潜力",
            "mentoring_ability": "指导能力较强"
          },
          "development_recommendations": {
            "strategic_directions": [
              "继续跨领域研究",
              "加强技术创新",
              "扩大合作网络"
            ],
            "skill_enhancement": [
              "提升数据集构建能力",
              "学习新的评估方法",
              "加强算法实现能力"
            ],
            "collaboration_targets": [
              "与计算机视觉领域顶尖学者合作",
              "与自然语言处理领域知名团队合作"
            ],
            "research_priorities": [
              "关注多模态学习前沿",
              "探索新的图卷积网络应用",
              "研究模板选择机制优化"
            ],
            "career_milestones": [
              "获得重要学术奖项",
              "发表高影响力论文",
              "建立自己的研究团队"
            ],
            "risk_mitigation": [
              "避免过度依赖特定合作者",
              "关注研究热点变化",
              "加强跨领域交流"
            ]
          },
          "comparative_analysis": {
            "peer_comparison": "与同领域研究者相比，具有跨领域研究优势",
            "competitive_advantages": [
              "跨领域研究能力",
              "创新方法应用",
              "多学科交叉"
            ],
            "improvement_areas": [
              "数据集构建能力",
              "评估方法应用",
              "算法实现能力"
            ],
            "market_position": "处于人工智能领域的前沿位置，具有较大发展潜力"
          }
        }
      }
    }
  }
}