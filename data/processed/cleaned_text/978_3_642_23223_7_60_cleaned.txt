---

He Chuan, Li Ruifan, and Zhong Yixin

School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China

Abstract. Educational data mining is a critical application of machine learning. This paper presents our approach to the KDD Cup 2010 Challenge, a supervised learning problem in educational data from computer-aided tutoring. We employ various classification algorithms, including KNN, SVD, and logistic regression, to process the data and combine their results for the final prediction. Our method achieves results comparable to the top-ranked entries in the KDD Cup 2010.

Keywords: data mining, logistic regression, k-nearest neighbor, singular value decomposition, classifiers combination.

1 Introduction

The KDD Cup 2010 task focused on predicting student performance in algebraic problem-solving based on historical data. This prediction task is technically challenging and has practical significance for optimizing the learning process. Participants were provided with student interaction logs from intelligent tutoring systems, with two datasets: algebra 2008-2009 and bridge to algebra 2008-2009. The datasets include various interaction details, with some fields available only in the training set, such as correctness on the first attempt and hint requests. The competition used the root mean squared error (RMSE) as the evaluation criterion.

2 Our Method

A. Validation Set Generation

We generate validation sets due to the lack of ground truth labels for the test data.

B. Feature Engineering

Our feature engineering process includes basic features, combining features, temporal features, and other features. We categorize and combine features to capture relevant student and problem characteristics.

C. Logistic Regression

We apply logistic regression to build a classifier using the generated feature vectors. The logistic regression model predicts the probability of an event's occurrence and is essential for our classification task.

---

We employ a regularized logistic regression model defined as:

\[ \min_{w} \frac{1}{2} \log(1 + e^{-y_w x}) + \lambda \lVert w \rVert_2^2 \]
where \( w \) is the weight vector, \( x \) is the feature vector of a sample, \( y \) is the label (CFA in this context), and \( \lambda \) is the regularization parameter. Our models are built and tested using the liblinear toolkit, which is efficient for large-scale problems.

K-Nearest Neighbors (KNN) and Singular Value Decomposition (SVD) are two classification methods used in collaborative filtering. KNN classifies samples based on the closest training examples, while SVD decomposes a matrix into a product of three matrices and is effective on sparse data.

For combining classifiers, individual predictions are merged to enhance accuracy. This ensemble approach performs better than individual classifiers, as evidenced in our experiments.

**Experiment Results:**
- KNN: RMSE values on Algebra 2008-2009 dataset is 0.3257 with meta-parameters K β.
- SVD: RMSE values on Algebra 2008-2009 dataset is 0.446277 with meta-parameters N, η, λ.
- Logistic Regression: RMSE on Algebra 2008-2009 dataset is 0.2895.
- Combination: The combined classifier achieves an RMSE of 0.2820 on Algebra 2008-2009 dataset.

**Discussion:**
The combined classifier outperforms single classifiers. Logistic regression shows the best performance due to its exploitation of detailed feature vectors. Exploring additional information unique to educational data mining, such as KC components, can provide more discriminative information. Other approaches, like dynamic Bayesian networks, also show promise.

---

 Acknowledgements and references have been omitted as per the cleaning instructions.