This special issue invites the best papers from ACM Multimedia 2014 to extend their work into journal articles. In 2015, the conference took place in Orlando, FL, USA, introducing several new areas. The selected articles in this issue are from the Deep Learning for Multimedia and Emotional and Social Signals in Multimedia areas. They underwent a rigorous review process, including a two-day technical program committee meeting. The debate surrounded the importance of multimodal and innovative perspectives in best paper candidates. The extended papers, "Emotion Recognition During Speech Using Dynamics of Multiple Regions of Face" by Yelin Kim and Emily Mower-Provost, and "Correspondence Autoencoders for Cross-Modal Retrieval" by Fangxiang Feng, Xiaojie Wang, and Ruifan Li, significantly extend their conference versions. The first article improves facial emotion recognition during speech, while the second enhances cross-modal retrieval using correspondence autoencoders. Experimental results demonstrate improvements over existing literature. The authors have made their code publicly available, facilitating further research. These articles may potentially start new trends in future conferences.

Hayley Hung, Intelligent Systems Department, Delft University of Technology, The Netherlands
George Toderici, Google Research, Mountain View, USA
Guest Editors