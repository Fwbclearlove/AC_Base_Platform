Revisiting Counterfactual Problems in Referring Expression Comprehension

Zhihan Yu and Ruifan Li
School of Artificial Intelligence, Beijing University of Posts and Telecommunications, Beijing, China
{yzh0, rfli}@bupt.edu.cn

Abstract
Traditional referring expression comprehension (REC) tasks involve locating target referents in images based on text queries. We address the counterfactual REC (C-REC) problem by focusing on fine-grained attributes. We introduce a method to generate counterfactual samples and a C-REC framework that includes dual-branch attentive fusion for cross-modal feature learning. Our approach incorporates contrastive learning with generated counterfactual samples and demonstrates promising performance on public REC datasets and our constructed C-REC datasets.

1. Introduction
Referring expression comprehension (REC) links image regions with natural language, aiding vision-language tasks. While most REC models assume target referents are present in images, the counterfactual scenario, where the referent is absent, is overlooked. We define this as Counterfactual Referring Expression Comprehension (C-REC). Existing C-REC methods mainly focus on overall image-text or specific attribute mismatches, neglecting fine-grained attributes. We propose a method to generate fine-grained counterfactual samples and a framework to address C-REC by learning joint cross-modal features. Our approach enhances counterfactual perception through contrastive learning and achieves promising results. Major contributions include a deep examination of fine-grained attributes in C-REC, an effective sample generation method, and a robust C-REC framework.

---

2. Methodology

The counterfactual referring expression comprehension (C-REC) problem is addressed as a multi-task framework involving binary classification and coordinate regression. Given an image I and a text query T, the C-REC task aims to predict a counterfactual label C ∈ {0, 1} and simultaneously locate the target referent with a bounding box B. Here, C = 1 signifies a matched pair, while C = 0 indicates a counterfactual query. The bounding box B is defined by (x, y, w, h), marking the center point and dimensions.

2.1. Counterfactual Sample Generation (CSG)

Our C-REC samples are based on fine-grained attributes from referring expressions, inspired by the ReferItGame [18]. We define seven types of attributes, including head noun, color, size, absolute and relative location relations, relative location object, and generic attribute. We generate negative texts based on existing REC datasets, using a language model to modify attribute words and preserve query context. The CSG method comprises four steps: attribute word extraction, candidate word prediction using BERT, re-ranking based on correlation scores, and selection of counterfactual words.

2.2. Counterfactual REC Model

Our C-REC model is a one-stage structure with three encoders, a dual-branch attentive fusion (DAF) module, a regression head, and a counterfactual detection head. It incorporates contrastive learning and overall loss. Encoders extract features from images, text queries, and attribute words. The DAF module uses attention mechanisms to fuse visual and textual features. The fusion process involves projecting features to identical dimensions, calculating attention maps, and summing attention features into the visual features.

---

Figure 3. Our C-REC model employs three encoders to extract image, text, and attribute features, which are fused in the dual-branch attentive fusion (DAF) module to produce a fusion feature. A regression head predicts the bounding box, while a counterfactual head predicts the counterfactual label. We enhance cross-modal fusion with a contrastive loss using negative samples generated by the CSG method.

The image-text fusion feature Fvt is obtained with a residual connection:
Fvt = Fv + ad * f_att. (4)

Similarly, the visual feature Fv and attribute feature Fa are fused to get the image-attribute feature Fva, which is then averaged with Fvt to produce the final fusion feature Ff for prediction:
Ff = α * Fvt + (1 - α) * Fva. (5)

Prediction heads include a convolutional regression layer for bounding box and confidence score prediction, and a binary classifier for counterfactual prediction. The localization loss is a combination of IoU and cross-entropy loss, while the counterfactual classification loss is based on cross-entropy.

Contrastive learning is applied to improve the model's counterfactual perceptual ability, using the InfoNCE loss to minimize the distance between an image and its positive text query and maximize it with negative queries.

The overall loss is a weighted sum of the localization, counterfactual, and contrastive losses:
L = Lloc + γcf * Lcf + γcl * Lcl. (9)

3. Experiments

3.1. Datasets
We evaluate our C-REC framework on three REC benchmark datasets (RefCOCO, RefCOCO+, RefCOCOg) and our C-REC datasets (C-RefCOCO/+/g). RefCOCO/+/g are based on MS-COCO images and differ in the types of descriptions allowed. C-REC datasets are generated using the CSG method to balance normal and counterfactual samples.

3.2. Metrics
We use Acc-Box (IoU@0.5) for localization performance, Acc-Cls for counterfactual detection, and a new metric Acc-Cf for overall C-REC model performance.

3.3. Implementation Details
Our methods are implemented on NVIDIA RTX A6000 GPUs, using Adam as the optimizer with a batch size of 32 and an initial learning rate of 0.0001. The image encoder is pre-trained on MS-COCO without the validation and test images. The model is trained on RefCOCO/+/g for 40 epochs and fine-tuned on C-RefCOCO/+/g for 20 epochs. The temperature parameter τ in the contrastive loss is set to 0.2.

---

Table 4. Acc-Box (%) comparison of our model with baseline models on RefCOCO/+/g. Best and sub-optimal results are bolded and underlined, respectively.

Model Visual Pretrained RefCOCO RefCOCO+ RefCOCOg Encoder Images val testA testB val testA testB val-u test-u

Vision-Language Pretrain
MDETR[17] ResNet-101 200K ...
OFA[43] ResNet-152 20M ...
m-PLUG[21] ViT-L 14M ...

One-stage REC
FAOA[46] DarkNet-53 ... 
ReSC[47] DarkNet-53 ... 
MCN[28] DarkNet-53 ... 
RealGIN[51] DarkNet-53 ... 
LG-FPN[39] DarkNet-53 ... 
PFOS[38] DarkNet-53 ... 
VGTR[9] ResNet-101 ... 
TransVG[7] ResNet-101 ... 
SimREC[29] CSPDarkNet-53 ... 
Ours CSPDarkNet-53 ...

Table 5. Acc-Cls (%) on C-RefCOCO/+/g. Our model's performance against random choice, various confidence scores, and a binary classifier.

Method C-RefCOCO C-RefCOCO+ C-RefCOCOg val testA testB val testA testB val test
Random ... 
Conf. score (0.01) ... 
Conf. score (0.1) ... 
Conf. score (0.5) ... 
Binary classifier ...

Parameters γcf and γcl are set to 2.0. Parameter α in DAF module is set to 0.25 for the counterfactual head and 1.0 for the regression head.

3.4. Baselines
We compare our model with vision-language pretrained models and one-stage REC baselines.

3.5. Main Results
Table 4 shows that our model outperforms one-stage REC models, particularly SimREC [29]. Our model achieves strong performance on the traditional REC task without large-scale vision-language dataset pretraining. Dual task training, i.e., localization and counterfactual classification, appears to enhance each other.

Table 6. Acc-Cf (%) of our model on C-RefCOCO/+/g.

Model C-RefCOCO C-RefCOCO+ C-RefCOCOg val testA testB val testA testB val test
Ours ...

Table 5 presents Acc-Cls on C-RefCOCO/+/g. Using a random choice and confidence scores as baselines, we set thresholds at 0.01, 0.1, and 0.5. The binary classifier achieves up to 90% accuracy, surpassing other methods by up to 15.73%. Table 6 reports Acc-Cf, indicating our model's strong overall performance on C-REC.

3.6. Ablation Study
Table 7 shows the impact of attribute features (Fa), contrastive loss (Lcl), and counterfactual training (C-Train) on model performance.

Table 7. Performance (%) with different components on C-RefCOCO testB.

Fa Lcl C-Train Acc-Box Acc-Cls Acc-Cf
... 

Table 8. Acc-Cf (%) using different cross-modal fusion methods on C-RefCOCO.

Method val testA testB
Baseline (Fvt) ... 
Serial fusion (Fa →Fvt) ... 
Serial fusion (Ft →Fva) ... 
Parallel fusion (Fva + Fvt) ...

Figure 5 and Table 9 present the effects of hyper-parameter α in DAF module and temperature parameter τ in contrastive loss on Acc-Cf.

Figure 5. Acc-Cf (%) with different α settings in DAF module on C-RefCOCO.

Table 9. Acc-Cf (%) with different τ settings in contrastive loss on C-RefCOCO.

τ val testA testB
0.05 ... 
0.1 ... 
0.2 ... 
0.5 ... 

---

---

We investigate the importance of global context information for localizing visual target referents and vary the temperature parameter τ in our model's training. Experimental results, shown in Table 9, indicate that our model performs best with τ equal to 0.2, suggesting that higher or lower values could weaken feature fusion in the latent feature space.

In qualitative analysis, we visualize examples in Figure 6 to demonstrate our model's performance. The first row illustrates the localization capabilities of our model and SimREC on RefCOCO. Our model accurately predicts counterfactual labels and provides precise localization, even outperforming SimREC in terms of IoU in failure cases. The following rows present counterfactual samples across seven pre-defined attributes. Our model successfully identifies most mismatched attributes but encounters difficulties with complex queries, such as less attention to the size attribute "small" and an overemphasis on absolute location "center".

Related work on counterfactual REC includes approaches like SCRE and MTG, which treat C-REC as a matching task based on logical rules. Other methods extend existing models with binary classifiers. However, these do not address fine-grained counterfactual queries on various attributes.

In vision-language tasks, generating counterfactual image-text pairs is crucial for evaluating resilient models. Methods range from random matching to using GANs and manual modifications. In contrast, we propose a labor-free method based on language models.

Our paper concludes by highlighting the CSG method for constructing fine-grained C-REC datasets and the C-REC framework for detecting counterfactual polarity and target localization. The inclusion of a DAF module and contrastive learning shows promising results. Future work could explore generating counterfactual images to enhance dataset diversity.

---

Acknowledgement:
This work was supported by the National Natural Science Foundation of China under Grant 6207603 and the High-Performance Computing Platform of BUPT.

References:
(Not included as per the instruction to only clean the current fragment and not to generate a complete paper structure.)

Michal Adamkiewicz et al. present vision-only robot navigation in a neural radiance world (2022). Vedika Agarwal and colleagues work on causal VQA, addressing spurious correlations through invariant and covariant semantic editing (2020). Peter Anderson's team investigates bottom-up and top-down attention for image captioning and visual question answering (2018). Long Chen and associates propose counterfactual samples synthesizing for robust visual question answering (2020) and Ref-NMS to address proposal bottlenecks in referring expression grounding (2021). Enjie Cui's research focuses on selective comprehension for referring expression using prebuilt entity dictionaries with modular networks (2018).

Jiajun Deng's group introduces TransVG, an end-to-end visual grounding method with transformers (2021). Jacob Devlin and others present BERT, a pre-trained deep bidirectional transformer for language understanding (2018). Ye Du and team explore visual grounding with transformers (2022). Zhiyuan Fang's work involves modularized textual grounding for counterfactual resilience (2019).

Akira Fukui and colleagues contribute to multimodal compact bilinear pooling for VQA and visual grounding (2016). Ross Girshick presents Fast R-CNN for object detection (2015). Tanmay Gupta's study involves contrastive learning for weakly supervised phrase grounding (2020). Kaiming He's research introduces momentum contrast for unsupervised visual representation learning (2020).

Sepp Hochreiter and Jürgen Schmidhuber propose the Long Short-Term Memory network (1997). Justin Johnson et al. introduce DenseCap for dense captioning (2016). Aishwarya Kamath and team present MDETR for modulated detection in end-to-end multi-modal understanding (2021). Sahar Kazemzadeh's ReferItGame refers to objects in photographs of natural scenes (2014).

Yongmin Kim's work focuses on flexible visual grounding (2022). Diederik P. Kingma and Jimmy Ba introduce the Adam optimization method (2014). Chenliang Li's research involves vision-language learning by cross-modal skip-connections in mPlug (2022). Menghao Li and colleagues propose iterative robust visual grounding with masked reference-based centerpoint supervision (2023).

Tsung-Yi Lin et al. introduce Microsoft COCO, a dataset of common objects in context (2014). Chang Liu's GRes aims at generalized referring expression segmentation (2023).

---

Daqing Liu, Hanwang Zhang, Feng Wu, and Zheng-Jun Zha. Learning to assemble neural module tree networks for visual grounding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4673–4682, 2019.

Xihui Liu, Zihao Wang, Jing Shao, Xiaogang Wang, and Hongsheng Li. Improving referring expression grounding with cross-modal attention-guided erasing. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1950–1959, 2019.

Mingcong Lu, Ruifan Li, Fangxiang Feng, Zhanyu Ma, and Xiaojie Wang. Lgr-net: Language guided reasoning network for referring expression comprehension. IEEE Transactions on Circuits and Systems for Video Technology, 2024.

Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Liujuan Cao, Chenglin Wu, Cheng Deng, and Rongrong Ji. Multi-task collaborative network for joint referring expression comprehension and segmentation. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 10034–10043, 2020.

Gen Luo, Yiyi Zhou, Jiamu Sun, Shubin Huang, Xiaoshuai Sun, Qixiang Ye, Yongjian Wu, and Rongrong Ji. What goes beyond multi-modal fusion in one-stage referring expression comprehension: An empirical study. arXiv preprint arXiv:2204.07913, 2022.

Christopher D Manning, Mihai Surdeanu, John Bauer, Jenny Rose Finkel, Steven Bethard, and David McClosky. The stanford corenlp natural language processing toolkit. In Proceedings of 52nd annual meeting of the association for computational linguistics: system demonstrations, pages 55–60, 2014.

Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 11–20, 2016.

Varun K Nagaraja, Vlad I Morariu, and Larry S Davis. Modeling context between objects for referring expression understanding. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part IV 14, pages 792–807. Springer, 2016.

Jingjing Pan, Yash Goyal, and Stefan Lee. Question-conditioned counterfactual image generation for vqa. arXiv preprint arXiv:1911.06352, 2019.

Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532–1543, 2014.

Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28, 2015.

Mohit Shridhar, Dixant Mittal, and David Hsu. Ingress: Interactive visual grounding of referring expressions. The International Journal of Robotics Research, 39(2-3):217–232, 2020.

Sanjay Subramanian, William Merrill, Trevor Darrell, Matt Gardner, Sameer Singh, and Anna Rohrbach. Reclip: A strong zero-shot baseline for referring expression comprehension. arXiv preprint arXiv:2204.05991, 2022.

Mengyang Sun, Wei Suo, Peng Wang, Yanning Zhang, and Qi Wu. A proposal-free one-stage framework for referring expression comprehension and generation via dense cross-attention. IEEE Transactions on Multimedia, 2022.

Wei Suo, Mengyang Sun, Peng Wang, Yanning Zhang, and Qi Wu. Rethinking and improving feature pyramids for one-stage referring expression comprehension. IEEE Transactions on Image Processing, 32:854–864, 2022.

Chien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu, Ping-Yang Chen, Jun-Wei Hsieh, and I-Hau Yeh. Cspnet: A new backbone that can enhance learning capability of cnn. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pages 390–391, 2020.

Peng Wang, Qi Wu, Chunhua Shen, Anthony Dick, and Anton Van Den Hengel. Fvqa: Fact-based visual question answering. IEEE transactions on pattern analysis and machine intelligence, 40(10):2413–2427, 2017.

Peng Wang, Qi Wu, Jiewei Cao, Chunhua Shen, Lianli Gao, and Anton van den Hengel. Neighbourhood watch: Referring expression comprehension via language-guided graph attention networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1960–1968, 2019.

Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In International Conference on Machine Learning, pages 23318–23340. PMLR, 2022.

Nevan Wichers, Dilek Hakkani-T¨ur, and Jindong Chen. Resolving referring expressions in images with labeled elements. In 2018 IEEE Spoken Language Technology Work- shop (SLT), pages 800–806. IEEE, 2018.

Qi Wu, Chunhua Shen, Anton Van Den Hengel, Peng Wang, and Anthony Dick. Image captioning and visual question answering based on attributes and their related external knowledge. arXiv preprint arXiv:1603.02814, 2016.

Zhengyuan Yang, Boqing Gong, Liwei Wang, Wenbing Huang, Dong Yu, and Jiebo Luo. A fast and accurate one-stage approach to visual grounding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4683–4693, 2019.

Zhengyuan Yang, Tianlang Chen, Liwei Wang, and Jiebo Luo. Improving one-stage visual grounding by recursive sub-query construction. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XIV 16, pages 387–404. Springer, 2020.

Jiabo Ye, Junfeng Tian, Ming Yan, Xiaoshan Yang, Xuwu Wang, Ji Zhang, Liang He, and Xin Lin. Shifting more attention to visual backbone: Query-modulated refinement networks for end-to-end visual grounding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15502–15512, 2022.

Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and Jiebo Luo. Image captioning with semantic attention. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4651–4659, 2016.

Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, and Tamara L Berg. Mattnet: Modular attention network for referring expression comprehension. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1307–1315, 2018.

Yiyi Zhou, Rongrong Ji, Gen Luo, Xiaoshuai Sun, Jinsong Su, Xinghao Ding, Chia-Wen Lin, and Qi Tian. A real-time global inference network for one-stage referring expression comprehension. IEEE Transactions on Neural Networks and Learning Systems, 2021.

---