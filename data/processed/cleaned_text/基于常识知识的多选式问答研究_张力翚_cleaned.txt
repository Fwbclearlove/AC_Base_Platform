随着移动互联网的飞速发展与普及，问答系统已在多个工业领域成功落地并取得了良好的经济收益与社会价值。常识知识作为海量认知信息中的研究重点，在问答系统中的作用也在不断凸显，具有极大的研究前景。常识问答任务（Commonsense Question Answering，CQA）正是研究如何获取相关常识知识，并通过问题解析与知识推理，进而获取精准答案。本文重点聚焦于CQA任务在有监督和无监督场景下的研究。

目前CQA任务存在以下问题亟待解决：在有监督场景下，当前的研究工作集中于优化和改进模型的知识推理策略，而忽视了知识覆盖面不足及知识噪声等问题，这会严重折损模型在知识推理阶段的性能而致使预测偏差。在无监督场景下，当前的多数研究方法着重于设计特定任务的手工规则以提升知识的生成质量，因而导致知识类型受限并且模型框架的自适应迁移能力较弱。

故本文针对以上挑战进行了探索与研究，并将具体工作总结如下：

1. 针对有监督CQA任务中的知识覆盖不足以及噪声问题，本文提出了一种基于知识增强的图对比学习模型（Knowledge Enhanced Graph Contrastive Learning，KE-GCL）。首先，该模型将问答对的实体上下文描述集成到当前知识子图以实现多源知识融合；随后，模型提出一个自适应的带权采样策略以生成当前子图的增强视图，并同时完成正负图例的构建；最后，该模型通过关系边的散射与实体节点的信息聚集以完成知识图谱的更新与推理。

2. 针对无监督CQA任务中的迁移及自适应能力弱的问题，本文提出了一种基于通用提示模版的知识生成模型（Prompt-based Knowledge Generation Network，PKGN）。首先，该模型通过Dropout增强策略进行无监督的对比学习，以捕获问题之间的细微差异并学到更好的问题表征；接着，PKGN模型通过带指令的模版提示以生成问题相关的知识描述；最后，该模型利用文本匹配模型完成知识推理与答案预测。

本文在CommonsenseQA、OpenbookQA、SociallQA等三个常识问答数据集上开展了大量的实验。通过定量分析和定性对比，验证了KE-GCL和PKGN模型在有监督和无监督方向的可行性与有效性。实验结果表明，本文所提出的常识问答模型在有监督和无监督两个方向均优于当前的基线方法，且具备较好的鲁棒性与泛化能力。

基于知识的自动问答（KBQA）旨在回答用户提出的各类自然语言问题，在具备海量数据的开源互联系统中，知识作为行为与认知的重点，在问答理解与推理上作用不断凸显。谷歌于2012年首次提出了知识图谱的概念，知识图谱通过模拟人类理解客观世界的方式来动态构建知识，同时赋予了机器建模知识与理解语义的可能，高质量的数据由此开始以大规模的开源知识库方式出现，其中蕴涵了大量实体属性关系与拓扑结构信息。由此，基于知识的自动问答（KBQA）应运而生，并迅速成为了问答任务的一个重要分支。图1-1展示了KBQA任务的具体定义，即给定一个自然语言问题，机器需要对问题进行语法解析和语义匹配，并结合相关的外部知识库进行查询和推理，进而获得精确的预测答案。根据知识来源的不同，KBQA任务可主要划分为开放领域（OpenDomain）知识问答，如百科知识、常识的问答；以及特定领域（CertainDomain）知识问答，如金融、医学领域的问题问答。

从工业落地的角度来看，基于知识的问答系统在现阶段也得到了广泛的应用，典型的落地服务场景包括：智能语音助手、智能客服、搜索引擎、情感类聊天等。在语音助手领域，广为人知的微软小冰、苹果Siri、小米小爱等智能助手产品正是依托了高性能KBQA系统的支撑，才能为用户提供日常化的精准化定制服务。在搜索引擎场景下，百度推出的百度知道，搜狗推出的搜狗问问等引擎服务，正是将用户的自然语言问题作为搜索查询词进行知识查询与检索排序，从而直接为用户提供准确简洁的答案，而不是返回一个和问题最相关的网页供用户自行查找，这大大降低了用户获取信息的成本。在智能客服领域，阿里小蜜通过知识图谱技术，自动解答用户关于支付宝使用时遇到的问题，这不仅降低了使用人工客服带来的人力成本，同时也降低了用户获得服务的响应时间，并提升用户体验。除此之外，基于知识库的自动问答在电信运营商、金融保险、税务管理等行业领域也有着广泛的应用。

在KBQA任务中，常识作为人类社会对同一事物普遍存在的日常共识，是一种重要表现形式。常识问答任务（Commonsense Question Answering，CQA）考验的正是模型的常识推理能力，这类任务一般聚焦于多项选择式问答形式，需要模型针对给定的问题进行推理，从而选择最为贴切的答案，目前常见的相关数据集有CommonsenseQA、OpenbookQA、SocialIQA、CosmosQA以及NinersenseW等。区别于机器阅读理解和事实性抽取等传统问答任务，常识问答任务通常只给出题干而没有给出相关的背景知识。在CQA任务中，模型不仅要理解问题和选项的语义，还需要具备相关的常识背景知识才能进行合理的推断及预测。在图1-2所示的CQA样例中，模型只有在知道“老板需要员工向他”的知识前提下，才能推断出正确选项B（进行信息传递）。

如何有效获取高质量的常识知识，并执行有效地知识推理，这也正是常识问答任务的难点所在。目前常见的大型知识库，例如YAGO、OpenIE、NELLMDBpedia、Freebase、WebChild以及ConceptNet，这些结构型数据库以机器可读的方式存储“实体-关系-实体”的三元组知识，并可根据资源描述框架规范对知识进行描述。这些知识库组成了常识信息的大型语义网络，具有极好的知识表达能力和灵活的建模空间，因而能够辅助模型进行结构化的常识知识推理，并输出具备高置信度的答案决策。

本文的工作聚焦于基于知识的自动问答领域中的常识问答任务，并在有监督和无监督的两个方向展开详细研究。通过建立端到端的深度学习网络，并运用知识建模及推理的相关技术解决CQA任务中现存的挑战和困难。

本文针对有监督和无监督常识问答任务，结合当前研究的问题要点，阐述了主要研究内容与工作贡献。针对有监督常识问答模型存在的知识覆盖不完备及知识噪声问题，本文设计了基于知识增强型图对比学习的常识问答算法，通过自适应采样原始图的边和节点特征得到增强视图，并采用多源的知识增强，将问答对的实体描述性文本作为补充节点信息插入到当前子图中，以充分完成知识的融合与对齐。针对无监督常识问答模型缺乏易迁移的自适应方法问题，本文设计了基于提示型知识生成的常识问答算法，通过无监督对比学习策略对问题语义进行继续预训练，并采用具备指令及示例的提示模版产生一系列知识描述的集合。在常识问答的有监督和无监督两个方向分别对比了目前主要的基线方法，并通过大量的定性与定量实验充分证明了本文所提出模型的有效性。

在向量空间中，词向量表示方法Word2Vec通过浅层神经网络学习词向量，包括CBOW和Skip-Gram两种模型。CBOW利用上下文预测目标词，而Skip-Gram则相反，利用目标词预测上下文。知识表示方法包括神经网络模型和张量模型，其中TransE模型基于平移不变性假设，将关系视为头尾实体向量之间的平移。深度学习技术中，注意力机制通过为输入向量分配权重，帮助模型关注关键信息。

其中，＜7表示非线性层的激活函数，％ ，撕2为权重矩阵，为偏置项。

（2）权重归一化：此阶段负责将每个输入向量％ eh对应的相似度得分归一化的权重af，一般采用Softmax函数实现，具体公式如下：

gSimi = Softmaxi(simi) = e^simi / Σe^simi (2-12)

（3）带权向量计算：利用权重对所有输入向量进行加权求和，即将输入序列中的每个向量与它对应的权重项相乘，并将结果进行相加以得到最终向量＾；，其公式表示如下：

＾ = Σaifiei (2-13)

2.3.2预训练语言模型

预训练语言模型通过在大规模文本语料上进行无监督的先行训练，使得模型能掌握丰富的自然语言的语义和语法规律，并可应用到各类下游的文本任务中以提升性能效果。其中，最具代表性预训练语言模型便是由Open AI于2018年发布的大规模生成式的预训练语言模型GPT[73]（Generative Pretrained Transformer）和Google团队于同年发布的双向文本编码模型BERT[74]（Bidirectional Encoder Representations from Transformers），它们都是基于Transformer网络架构，分别在自回归和自编码方向上对模型的训练范式进行了突破性创新，在机器翻译、文本分类、序列标注、问答系统等多个领域上均表现优异并得到了广泛的应用。下面将针对上述模型展开详细介绍。

（1）Transformer架构

Transformer的模型结构如图2-8所示，主要由两个部分组成，即编码器（Encoder）和解码器（Decoder），其中编码器负责将输入序列编码为隐藏状态序列，解码器则负责从隐藏状态序列中生成输出序列，而它们均是由多层组块（Block）堆叠而成，每个组块内部也包含了不同的层次结构[75]。以编码器为例，每个组块内部由三种类型的结构组成，即多头自注意力层、残差及规范化层、全连接前馈层。

Output

Probabilities

1

Softmax 1

1

Linear 1

--I

Add & Norm

F^d

Forward

>—^

Add & Norm

Attention

Forward

I

*Nx

>

.

j

Add & Norm

j

j

?

.

.

“.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

第6部分内容：

1. GCN模型通过Laplace矩阵进行谱分解，采用傅里叶变换进行特征分解，并定义了在谱域上的卷积核运算。GCN模型利用每个节点及其邻居节点的特征来更新当前节点的信息表示，并在不同层之间共享权重。

2. GAT模型在GCN的基础上引入了注意力机制，能够自适应地学习不同节点之间的关系权重，从而更精细地捕获节点之间的相互作用。

3. GraphSAGE模型通过随机采样邻居节点并聚合邻居节点的特征向量来完成信息传递和特征更新。相比GCN和GAT，GraphSAGE考虑了所有邻居节点信息，并通过随机采样方式极大减少了模型的计算量。

4. 提示学习通过引入具备指向性的提示模版信息，控制和影响模型生成的文本，并使得输出的可解释性和可控性更强。提示学习可以结合大规模的预训练语言模型，在模版中构建领域相关的先验知识来弥补数据稀缺问题，从而在一定程度上提高模型的性能表现和泛化能力。

从ConceptNet结构化知识库中可以检索得到针对当前问答对(g, q)的知识子图仏，这是与g和q中的主题实体相关的两跳范围内节点与边所构成的子图。对于节点嵌入，使用Feng等人提供的实体嵌入参数进行初始化，它将预训练语言模型应用于ConceptNet中的所有三元组，同时为每个实体获得一个池化表示。经过初始化后的节点嵌入表示如下：＝Ki * vi，2，－＞vin]。

对于边的嵌入表示，首先将当前边的类型rst以及边所连接的两个节点的类型us，构成的三元组编码为一个独热向量[us？rst十叫]；然后采用两层MLP结构将独热向量映射为了当前边的初始化嵌入表示。经过初始化后的m条有向边的嵌入表示如下：i = {eu，ei，2，…，e_m}。

因此，当前知识子图对应的图谱嵌入可表示为& = {1，财]，具备n个节点和m条有向边。

知识融合部分通过节点插入和注意力机制进行多源的知识融合。具体来说，上下文的语义特征向量被视为一个新节点插入到当前的知识子图中，插入后的节点嵌入更新为K = {1，新插入的上下文节点定义如下，其中M表示个两层的MLP。i = {1}。

对于相关边的构建，这里将与当前问答对(q, q)中的实体有直接拓扑关系的实体与新插入的上下文节点建立连接，由此图谱边数增加到话。边的嵌入表示更新为{eu，ei，2，…，e_m}。

模块采用注意力机制来进一步融合文本知识，上下文表示4被视作查询(Query)作用于所有图谱节点。对于每个节点融合后的表示定义如下，其中映射Q表示MLP模块，%为节点的嵌入维度。i，q = softmax？vUq。

因此，多源知识的融合图可表示为￥ = {1，M]，具备n+1个节点和衍+1条边。

自适应图增强模块针对多源知识的融合图￥，通过自适应地进行节点特征掩码和边丢弃以构建对应的增强视图。首先定义子图中的每个节点eR的影响为如下公式所示：Pv_i，q = /r(＾i，a)]。

这里r(＾)和丸(Y)分别代表拓扑连通性和上下文相关性。拓扑连通性是通过PageRank算法进行计算，该算法会从拓扑结构的角度对那些具有更多入度的节点进行加权；而上下文相关性则通过当前节点与上下文语义表示z_f之间的语义相似性衡量，公式定义如下，其中0(Y)表示两个向量之间的佘弦相似度。

一个直观性假设，即那些经常在有影响力的节点中有高频表达的维度应该是更重要的。因此，对于芡中的任何节点，维度d的重要性权重计算公式定义如下，并将权重数值归一化后作为是否对维度进行掩码的概率。

对于S；中的每条边e，其重要性取决于当前边所指向的尾节点珥的重要性权重，如下公式所示。类似节点的操作，权重I在归一化后便作为边e的丢弃概率。

因此，该模块基于归一化的概率进行伯努利采样，并获得￥对应的增强视图￥ = {1}。

知识图推理模块中，融合知识子图S；及其增强视图￥会并行执行相同的前向操作，故此处以前者S；为例，通过关联边的散射和基于注意力的节点聚合进行知识推理。具体来说，为了利用图谱中带有关系类型的边信息，对于每个节点圬eK，通过对指向节点珥的那些边进行散射操作可获得对应节点的初始隐藏表示。

之后，当前模块使用GAT网络用以传播和聚合节点的知识信息。对于每层图神经网络，节点的隐藏表示的更新公式如下，其中t/是注意力头的数量，M/u是对应的线性投影矩阵，| |是多个注意力头的拼接操作。

在经过L层的图推理之后，模块选择将上下文节点的隐藏状态作为整个知识子图的池化表示，即：zf = Pool(hL)。

答案预测模块对于答案选项q，使用相应的上下文语义特征表示z_c和知识子图表示z_f来计算其成为正确答案的概率，计算公式如下：P_ic_i|q) = gQ[z_c，z_f]。

其中，门控仍用以控制上下文文本特征和图谱特征的重要性权重。对于答案预测，在得到所有候选项的预测概率后，模型将具有最高分数的选项视为最合理的答案。

正负图例构造与目标函数中，损失目标函数KE-GCL采用端到端的有监督训练，其总目标损失Xi定义如下，其中和分别表示答案预测分类损失和图对比学习损失，而入则是用于调节图对比学习影响的超参数，同时也反映了对预测分类目标的惩罚程度。

答案预测分类损失使用标准的交叉熵以优化正确答案q的预测概率，其公式定义如下，而图对比损失则将结合正负图例的构造在下文中详细展开。

正负图例的构建及图对比学习损失中，图对比学习损失定义如下公式所示，该损失是基于InfoNCE函数构建，并在此基础上增设了难负例项以增强对比学习的效果。直观来说，对于一个给定的问题，候选选项相关的知识子图及其增强视图通常会具备较多相同的节点和边，这种相似性会导致模型难以分辨。因此，当前子图的难负例存在两个来源，一个是那些带有错误选项的问答对相关的知识子图；另一个则是它们相应的增强视图。

由此，正例、难负例、普通难负例对应的贡献项可表示如下：TP = exp？z_f/r，TN = i exp？z_f/r，T_C = z exp？M)/r。

在OpenBookQA数据集上，基于Huggingface框架部署实现了AristoBERTa。检索知识子图的限制跳数设置为2。文本编码器的上下文特征维度设置为1024，最大序列长度为128；图推理模块中节点与边的嵌入维度设置为200，GAT的推理层数设置为3。为了控制图对比学习中的变量约束影响，将超参数I、P和t分别设置为0.1、2和0.2。文本编码器的学习率设置为1e-5，其他模型组件的学习率为1e-3，dropout数值为0.2。模型在训练过程中采用Adam作为优化器，将Epoch设置为30并采用早停策略进行端到端的有监督训练。一个完整的训练周期大约需要13小时。此外，采用Mini-batchSize=2的梯度累积策略以实现BatchSize=128的等效训练效果。最终的主实验结果是使用不同的随机种子进行五次运行后的平均指标。

本章在有监督常识问答方向开展了基于知识增强型的图对比学习模型（KE-GCL）的研究工作。首先，本章从缓解知识噪声问题和增加知识覆盖面的角度给出了KE-GCL模型的设计动机与解决思路，并概述了模型端到端的处理流程。接着，本章详细介绍了KE-GCL的各个组件模块，该框架将上下文描述集成到当前的知识子图中，形成知识增强图；接着，本章提出了一种自适应采样策略来生成当前知识子图所对应的增强视图；随后，本章通过边散射和节点聚合的方式对图谱的信息进行更新与推理；此外，为了进一步增强图对比学习的训练效果，模型将当前样例中错误问答对相关的知识子图及其增强视图视作为难负例。最后，本章在CommonsenseQA和OpenbookQA两个基准数据集上进行了大量的定量与定性实验分析，从模型性能对比、消融实验、案例分析、可视化分析、难负例影响等方面充分验证了所提出的KE-GCL方法的可行性与有效性。

知识生成的提示模版中包含多个常识问题及其相关背景知识作为输入，构成不同的常识模版提示。然后根据问题和提示的对应关系，生成相应的知识描述作为输出。这种基于提示学习的方式可以有效地利用预训练语言模型中的先验知识，并能以生成的方式提高知识的质量和多样性。

在每个问题得到其对应的一系列相关知识描述后，知识推理与答案预测模块负责将当前的知识进行有效整合，推理模型将其结合当前的所有候选答案进行置信度相关的可靠预测。具体而言，对于每个问题g生成的S个知识陈述知识推理模型会将知识陈述单独添加到问题前，形成S+1个带有知识描述的完整问题表达。推理模型会对每个候选答案进行匹配与评分操作，即采用最支持当前候选项的知识描述来确定一个置信度分数。因此，推理模型会从知识描述中选择一个给出最高置信度预测的候选答案作为最终输出。

无监督常识问答任务的实验在CommonsenseQA和OpenbookQA数据集的基础上，增设了SociallQA基准数据集。数据集总共包含37,588个问答对，训练集、测试集、验证集的数量分布如表4-4所示。无监督常识问答任务仍然采用答案预测准确率作为模型性能的评估指标。

本节选取了近年在无监督常识问答任务上的多种研究方法作为对比基线，并对各自的研究工作进行简要地概括说明。为了进行合理地对比分析，后续所有的在三个基准数据集上的实验报道均是基于相关验证集开展。值得注意的是，模型在训练过程中严格控制标签不可见，标签仅用于最终的性能评估。

本章提出的PKGN模型采用开源深度学习框架Pytorch进行网络搭建与模型训练，并在3个常识问答基准数据集上进行了定性和定量实验。实验环境为搭载了2张48GB NVIDIA RTX A6000显卡的Ubuntu 20.04服务器。基于问题语义的对比学习模块和提示型知识生成模块以生成式预训练语言模型GPT-2作为基础骨架。在知识推理与答案预测模块中，与SEQA方法保持相同的结构设置，即用SROBERTa-Large作为基础架构。

表4-5展示了PKGN模型的无监督实验结果。在这三个基准常识问答数据集上，所提出的PKGN模型的性能均优于当前所有的对比基线。具体来说，以GPT-2-Medium的配置为例，与之前的最优模型SEQA相比，PKGN模型在CommonsenseQA和OpenbookQA数据集上的表现取得了显著的性能提升，分别高出了2.6%和9.2%。而在SociallQA数据集上，PKGN模型相比SEQA的方法则高出1.5%。随着GPT-2架构的参数规模逐渐上升的同时，PKGN模型在这三个基准数据集上的实验性能也在稳步提升。

为了研究PKGN模型中每个模块的有效性，本小节基于CommonsenseQA和OpenbookQA两个基准数据集，在GPT-2-Medium的配置下，对PKGN模型中的各个组件进行了消融实验分析。实验结果如表4-6所示。可以看到，当移除基于问题语义的对比学习模块后，在不进行任何针对问题本身的学习训练后，模型性能在两个数据集上均显著下降了2.6%和3.7%。这也证实了继续预训练对于后续知识生成的重要性。在移除提示型知识生成模块后，PKGN模型退化为了基于SROBERTa架构的自编码器，并直接以问题与候选答案的组合拼接为输入进行打分预测。因此，模型性能在两个数据集上迎来了最大幅度的下降，分别为4.1%和9.5%。这也反映了常识知识的关键性。在移除知识推理与答案预测模块的设置中，PKGN模型直接拼接问题和首条生成知识，并连同候选答案输入到GPT-2中进行预测打分，模型性能在两个数据集上的下降幅度分别为0.9%和1.7%。这证明了该模块采用的问题知识拼接策略和文本匹配模型能较好地理解并筛选对答案预测有助益的有效知识。

本小节对PKGN模型中的预定义模版设置多种扰动策略，从而测试模型在不同攻击力度下的鲁棒性与泛化能力。扰动方式可分为四种类型，分别是：改述模版指令、调换模版指令与示例顺序、同域随机替换模版示例、跨域随机替换模版示例。为避免偶然性与数据偏差，在每个扰动设置下，PKGN基于GPT-2-Medium的配置在CommonsenseQA数据集上进行3次重复实验并取得平均值进行报道，实验结果如表4-7所示。

在本研究中，我们提出了一种基于提示型知识生成的无监督常识问答算法框架（PKGN）。该模型首先利用基于Dropout增强的对比学习策略对常识问题进行继续预训练，以捕捉更精细的问题语义表示。随后，模型利用设计的提示模版生成一系列问题相关的知识描述。最终，通过文本匹配模型对带有知识的问题和候选项之间进行置信度评分与答案预测。实验结果显示，PKGN模型在三个基准常识数据集上的性能指标均优于当前基线方法，并通过组件消融、模版敏感度分析以及案例分析等角度验证了模型的有效性与泛化能力。

清洗后的内容如下：

12. Tandon N, De Melo G, Suchanek F, et al. WebChild: Harvesting and organizing commonsense knowledge from the web. In Proceedings of the 7th ACM international conference on Web search and data mining. 2014: 523-532.

13. Liu H, Singh P. ConceptNet - A practical commonsense reasoning toolkit. In BT Technology Journal, 2004, 22(4): 211-226.

14. Lan Y, He G, Jiang J, et al. A survey on complex knowledge base question answering: Methods, challenges and solutions. arXiv preprint arXiv:2105.11644, 2021.

15. Ducharme B. Learning SPARQL: Querying and updating with SPARQL 1.1. Really Media, Inc., 2013.

16. Reddy S, Lapa M, Steedman M. Large-scale semantic parsing without question-answer pairs. Transactions of the Association for Computational Linguistics, 2014, 2: 377-392.

17. Santoro A, Raposo D, Barrett D, et al. A simple neural network module for relational reasoning. In Advances in neural information processing systems, 2017, 30.

18. Schlichtkrull M, Kipf TN, Bloem P, et al. Modeling relational data with graph convolutional networks. In 15th International Conference on Extended Semantic Web Conference, ESWC 2018. SpringerA/erlag, 2018: 593-607.

19. Lin B, Chen X, Chen J, et al. KagNet: Knowledge-aware Graph Networks for Commonsense Reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019: 2829-2839.

20. Wang X, Kapanipathi P, Musa R, et al. Improving natural language inference using external knowledge in science questions domain. In Proceedings of the AAAI Conference on Artificial Intelligence. 2019, 33(01): 7208-7215.

21. Feng Y, Chen X, Lin B, et al. Scalable Multi-Hop Relational Reasoning for Knowledge-aware Question Answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020: 1295-1309.

22. Yasunaga M, Ren H, Bosselut A, et al. QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021: 535-546.

23. Gao T, Yao X, Chen D. SimCSE: Simple Contrastive Learning of Sentence Embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021: 6894-6910.

24. Unger C, Biemann L, Lehmann J, et al. Template-based question answering over RDF data. In Proceedings of the 21st international conference on World Wide Web. 2012: 639-648.

25. S W, Chang M W, He X, et al. Semantic parsing via staged query graph generation: Question answering with knowledge base. In Proceedings of the Joint Conference of the 53rd Annual Meeting of the ACL and the 7th International Joint Conference on Natural Language Processing of the AFNLP. 2015.

26. Burgers C. From RankNet to LambdaRank to LambdaMART: An overview. In Journal of Machine Learning, 2010.

27. Hu S, Zou L, Zhang X. A state-transition framework to answer complex questions over knowledge base. In Proceedings of the 2018 conference on empirical methods in natural language processing. 2018: 2098-2108.

28. Dong L, Lapata M. Language to logical form with neural attention. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2016: 33-43.

29. Xu K, Wu L, Wang Z, et al. Exploiting rich syntactic information for semantic parsing with graph-to-sequence model. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 2018: 918-924.

30. Cui W, Xiao Y, Wang H, et al. KBQA: Learning question answering over knowledge base with multi-hop attention. In Proceedings of the VLDB Endowment, 2017, 10(5): 565-576.

31. Liao C, Berant J, Le Q, et al. Neural symbolic machines: Learning semantic parsers on freebase with weak supervision. In 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017. Association for Computational Linguistics (ACL), 2017: 23-33.

32. McCarty J. History of Lisp. In History of programming languages, 1978: 173-185.

33. 陈子睿，王鑫，王林等. 开放领域知识图谱问答研究综述. 计算机科学与探索，2021，15(10): 1843-1869.

34. 郑泳智，朱定局，吴惠粦，彭小荣. 知识图谱问答领域综述. 计算机系统应用，2022，31(04): 1-13.

35. Yao X, Van Durme B. Information extraction over structured data: Question answering with jBases. In Proceedings of the 52nd annual meeting of the association for computational linguistics (volume 1: Long papers). 2014: 956-966.

36. Bordes A, Chopra S, Weston J. Question answering with subgraph embeddings. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2014: 615-620.

37. Dong L, Wei F, Zhou M, et al. Question answering over freebase with multi-column convolutional neural networks. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2015: 260-269.

38. Bordes A, Usunier N, Chopra S, et al. Large-scale simple question answering with memory networks. arXiv preprint arXiv:1506.02075, 2015.

39. Sukhbaatar S, Szlam A, Weston J, et al. Weakly Supervised Memory Networks. In Journal, 2015.

40. Chen Y, Wu L, Zak M J. Bidirectional Attention Memory Networks for Question Answering over Knowledge Bases. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019: 2913-2923.

41. Saxe A, Tripathi A, Talukdar P. Improving multi-hop question answering over knowledge graphs using knowledge base embeddings. In Proceedings of the 58th annual meeting of the association for computational linguistics. 2020: 4498-4507.

42. Lv S, Guo D, Xu J, et al. Graph-based reasoning over heterogeneous external knowledge for commonsense question answering. In Proceedings of the AAAI Conference on Artificial Intelligence. 2020, 34(05): 8449-8456.

43. 赵思洋. 基于联合训练和无监督方法的中文知识图谱问答研究. 哈尔滨工业大学，2020. DOI: 10.27061/d.cnki.ghgdu.2020.000642.

44. Lewis P, Denoyer L, Riedel S. Unsupervised question answering by cloze translation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Florence, Italy: Association for Computational Linguistics, 2019: 4896-4910.

45. Reimers N, Gurvich I. Sentence-BERT: Sentence Embeddings using Siamese Networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019: 3982-3992.

46. Bosselut A, Rashkin H, Sap M, et al. COMET: Commonsense Transformers for Knowledge Graph Construction. In Association for Computational Linguistics, 2019.

清洗后的内容：

47. Raffel C 5, Shazeer N, Roberts A, et al. Exploring the limits of transfer learning with a unified text-to-text transformer [J]. The Journal of Machine Learning Research, 2020, 21(1): 5485-5551.

48. Liu P, Yuan W 5, Fu J? et al. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing [J]. ACM Computing Surveys, 2023, 55(9): 1-35.

49. Khashabi D, Min S? Khot T, et al. UNIFIEDQA: Crossing Format Boundaries with a Single QA System [C]//Findings of the Association for Computational Linguistics: EMNLP 2020. 2020: 1896-1907.

50. Brown T, Mann B? Ryder N 5, et al. Language models are few-shot learners [J]. Advances in Neural Information Processing Systems, 2020, 33: 1877-190L.

51. Li XL, Liang P. Prefix-Tuning: Optimizing Continuous Prompts for Generation [C]//Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021: 4582-4597.

52. Sun Y, Zhang Y, Qi L, et al. TSGP: Two-Stage Generative Prompting for Unsupervised Commonsense Question Answering [J]. arXiv preprint arXiv:2211.3515 [cs] (2022).

53. Chren WA. One-hot residue coding for low dday_power product CMOS design [J]. IEEE Transactions on Circuits and Systems II: Analog and Digital Signal Processing, 1998, 45(3): 303-313.

54. Koppel M. The course of dimensionality [C]//5th online world conference on soft computing in industrial applications (WSC5). 2000: 1-8.

55. Beng io Y, Ducharme R, Vincent P. A neural probabilistic language model [J]. Advances in Neural Information Processing Systems, 2000, 13.

56. Vidaurri D? Bielza C, Larrañaga P. A survey of LI regression [J]. International Statistical Review, 2013, 81(3): 361-387.

57. Cortes C, Mohri M, Rostamizadeh A. L 2 regularization for learning kernels [C]//Proceedings of the Twentieth Conference on Uncertainty in Artificial Intelligence. 2009: 109-116.

58. Mikolov T, Chen K, Corrado G, et al. Efficient estimation of word representations in vector space [M]. 2013.

59. Shershtinsky A. Fundamentals of recurrent neural network (RNN) and long short-term memory (LSTM) network [J]. Physics D: Nonlinear Phenomena, 2020, 404: 132306.

60. Mikolov T, Sutskever I? Chen K 3, et al. Distributed representations of words and phrases and their compositionality [J]. Advances in Neural Information Processing Systems, 2013, 26.

61. Huffman D A. A method for the construction of minimum-redundancy codes [J]. Proceedings of the IRE, 1952, 40(9): 1098-1101.

62. Kleinbaum D G, Dietz K, Gail M, et al. Logistic regression [M]. New York: Springer-Verlag, 2002.

63. Pennington J, Socher R, Manning C D. GloVe: Global vectors for word representation [A]//Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) [C]. 2014: 1532-1543.

64. Socher R, Chen D, Manning C D, et al. Reasoning with neural tensor networks for knowledge base completion [J]. Advances in Neural Information Processing Systems, 2013, 26.

65. Liu Z 5, Li J 5, Shen Z 5, et al. Learning efficient convolutional networks through network slimming [C]//Proceedings of the IEEE international conference on computer vision. 2017: 2736-2744.

66. Bordes A, Usunier N 5, Garcia-Duran A? et al. Translating embeddings for modeling multi-relational data [J]. Advances in Neural Information Processing Systems, 2013, 26.

67. Wang Z 5, Zhang J 5, Feng J? et al. Knowledge graph embedding by translating on hyperplanes [C]//Proceedings of the AAAI conference on artificial intelligence. 2014, 28(1).

68. Lin Y, Liu Z, Sun M, et al. Learning entity and relation embeddings for knowledge graph completion [C]//Proceedings of the AAAI conference on artificial intelligence. 2015, 29(1).

69. Ji G 5, He S 5, Xu L, et al. Knowledge graph embedding via dynamic mapping matrix [C]//Proceedings of the 53rd annual meeting of the association for computational linguistics and the 7th international joint conference on natural language processing (volume 1: Long papers). 2015: 687-696.

70. Wang F, Jiang M, Qian C, et al. Residual attention network for image classification [C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 3156-3164.

71. Oliveira A, Torralba A, Castelhano M S? et al. Top-down control of visual attention in object detection [C]//2003 IEEE International Conference on Image Processing (Cat. No.03CH37429). 2003, 1: 1-253.

72. Bahdanau D? Cho K, Beng io Y. Neural machine translation by jointly learning to align and translate [J]. arXiv preprint arXiv:1409.0473 [cs] (2014).

73. Radford A, Narasimhan K, Salimans T, et al. Improving language understanding by generative pre-training [J]. 2018.

74. Devlin J, Chang M-W, Lee K, et al. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [A]//Minneapolis, Minnesota: Association for Computational Linguistics, 2019: 4171-4186.

75. Vaswani A, Shazeer N 5, Parmar N, et al. Attention is all you need [A]//Proceedings of the 31st International Conference on Neural Information Processing Systems. 2017: 6000-6010.

76. Glorot X 9, Bordes A, Beng io Y. Deep sparse rectifier neural networks [C]//Fourteenth International Conference on Artificial Intelligence and Statistics. 2011: -323.

77. Hochreiter S 5, Schmidhuber J. Long short-term memory [J]. Neural Computation, 1997, 9(8): 1735-1780.

78. Cho K, Merrienboer B, Gulcehre C, et al. Learning phrase representations using RNN Encoder-Decoder for Statistical Machine Translation [C]//EMNLP. 2014.

79. Radford A, Wu J? Child R 5, et al. Language models are unsupervised multitask learners [J]. OpenAI blog, 2019, 1(8): 9.

80. Liu Y, Ott M, Goyal N 5, et al. RoBERTa: A robustly optimized BERT pretraining approach [J]. arXiv preprint arXiv:1907.11692 [cs] (2019).

81. Lan Z, Chen M, Goodman S? et al. ALBERT: A lite BERT for self-supervised learning of language representations [J]. arXiv preprint arXiv:1909.11942 [cs]; 2019.

82. Wu Z, Pan S, Chen F? et al. A Comprehensive Survey on Graph Neural Networks [J]. IEEE Transactions on Neural Networks and Learning Systems, 2021, 32(1): 4-24.

83. Kipf TN, Wellinger M. Semi-supervised classification with graph convolutional networks [J]. arXiv preprint arXiv:1609029907 [cs]; 2016.

84. Velickovic P, Cucurull G, Casanova A, et al. Graph attention networks [J]. arXiv preprint arXiv:1710.10903 [cs]; 2017.

85. Hamilton WL, Ying R, Leskovec J. Inductive representation learning on large graphs [A]//Proceedings of the 31st International Conference on Neural Information Processing Systems. 2017.

86.

参考文献

[87] Zhou D, Zheng L, Han J, et al. A data-driven graph generative model for temporal interaction networks [A]. // Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining [C], 2020: 401-411.

[88] Kipf TN, Wellinger M. Variational graph auto-encoders [J]. arXiv preprint arXiv:161107308, 2016.

[89] Liu P, Yuan W, Fu J, et al. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing [J]. ACM Computing Surveys, 2023, 55(9): 1-35.

[90] Petroni F, Rocktaschel T, Riedel S, et al. Language Models as Knowledge Bases? [A]. // Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing [C], 2019: 2463-2473.

[91] Schick T, Schütze H. It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners [C]. // Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies [C], 2021: 2339-2352.

[92] Zhong Z, Friedman D, Chen D. Factual Probing Is [MASK]: Learning vs. Recall [C]. // Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies [C], 2021: 5017-5033.

[93] Liu X, Zheng Y, Du Z, et al. GPT understands, too [J]. arXiv preprint arXiv:2103.10385, 2021.

[94] Xu Y, Zhu C, Xu R, et al. Fusing Context into Knowledge Graph for Commonsense Question Answering [C]. // Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021 [C], 2021: 1201-1207.

[95] Shwartz V, West P, Le Bras R, et al. Unsupervised Commonsense Question Answering with Self-Talk [C]. // Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing [C], 2020: 4615-4629.

[96] Bosselut A, Le Bras R, Choi Y. Dynamic neuro-symbolic knowledge graph construction for zero-shot Commonsense question answering [C]. // Proceedings of the AAAI conference on Artificial Intelligence [C], 2021: 4923-4931.

[97] Niu Y, Huang F, Long J, et al. A Semantic-based Method for Unsupervised Commonsense Question Answering [C]. // Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) [C], 2021: 3037-3049.

[98] Liu J, Liu A, Lu X, et al. Generated Knowledge Prompting for Commonsense Reasoning [C]. // Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) [C], 2022: 3154-3169.

[99] Oord A, Li Y, Vinyals O. Representation learning with contrastive predictive coding [J]. arXiv preprint arXiv:1807.03748, 2018.