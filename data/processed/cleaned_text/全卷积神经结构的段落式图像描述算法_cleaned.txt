段落式图像描述算法的目标是为给定图像生成描述性自然语言段落。当前，段落式图像描述算法主要采用编码器与解码器组合的端到端结构。编码器基于卷积神经网络(CNN)将图像表示为较低维的视觉向量，解码器基于循环神经网络(RNN)将视觉向量解码为自然语言段落。然而，RNN解码器在长时记忆和梯度消失问题上存在局限，导致生成段落的连贯性不佳。

为提升连贯性，提出了一种全卷积神经结构的段落式图像描述算法。该算法的解码器由句子CNN解码器和词CNN解码器组成，句子CNN解码器捕捉段落内句子关系，词CNN解码器生成段落内单词。通过门控机制增强解码器的长时记忆能力。实验结果表明，该算法相比基于RNN的传统方法，能生成更连贯的段落式文本描述。

段落生成算法包括两个主要过程：利用基于卷积网络的目标检测器对图像进行编码，通过卷积解码器对图像特征进行层次性解码得到描述性段落。解码生成句子时，可采取最大概率采样或集束搜索方法。实验采用斯坦福大学最新建立的图像段落描述公开数据集进行验证，结果表明所提方法在CIDEr等评价指标上优于基线方法，验证了其有效性。

本文提出的基于全卷积神经网络结构的段落式图像描述生成模型，在层次性结构有效性方面优于Hierarchical-RNN方法，提升了17.8%的解码性能。所提方法在CIDEr指标上，相比Sentence-Concat、Image-Flat和Hierarchical-RNN方法，取得了更优的评测结果，有效提高了生成段落的品质，弥补了传统方法在描述能力上的不足。

束大小是影响算法性能的关键参数。实验评估了不同束大小对指标的影响。结果表明，当束大小为2时，评测结果最优。过小的束大小会导致解码信息丢失，而过大则会使段落间句子重复度增加，降低多样性，并显著增加解码时间复杂度。

进一步的研究中，通过观察迭代过程中各指标的变化，验证了结果的一致性。指标随迭代轮次的变化趋势基本保持一致，在第5至15个轮次间上升，在第15个轮次左右达到最优性能，之后出现过拟合现象。

主观评价部分，通过对比所提方法和Hierarchical-RNN方法生成的描述段落，显示了所提方法在上下文连贯性和语言逻辑性方面的优势。所提方法减少了信息重复，生成的段落更具连贯性，与人类认知系统更为贴近。

综上，所提模型通过卷积网络获取图像表示，构建层次性深度卷积解码器，引入门控机制，生成更具连贯性的段落式图像描述，实验证明在评测指标上取得了较好结果。

参考文献：
[1] Vinyals O, Toshev A, Bengio S, et al. Show and tell: a neural image caption generator. CVPR 2015.
[2] Lu Jiasen, Xiong Caiming, Parikh D, et al. Knowing when to look: adaptive attention via a visual sentinel for image captioning. CVPR 2017.
[3] Mao Yuzhao, Zhou Chang, Wang Xiaojie, et al. Show and tell more: topic-oriented multi-sentence image captioning. IJCAI 2018.
[4] Xu K, Ba J, Kiros R, et al. Show, attend and tell: neural image caption generation with visual attention. ICML 2015.
[5] You Quanzeng, Jin Hailin, Wang Zhaowen, et al. Image captioning with semantic attention. CVPR 2016.
[6] Karpathy A, Li Feifei. Deep visual-semantic alignments for generating image descriptions. CVPR 2015.
[7] Anderson P, He Xiaodong, Buehler C, et al. Bottom-up and top-down attention for image captioning and visual question answering. CVPR 2018.
[8] Krause J, Johnson J, Krishna R, et al. A hierarchical approach for generating descriptive image paragraphs. CVPR 2017.
[9] Liang Xiaodan, Hu Zhiting, Zhang Hao, et al. Recurrent topic-transition GAN for visual paragraph generation. ICCV 2017.
[10] Goodfellow I, Pouget-Abadie J, Mirza M, et al. Generative adversarial nets. NIPS 2014.
[11] Chatterjee M, Schwing A G. Diverse and coherent paragraph generation from images. ECCV 2018.
[12] Wang Z, Luo Y, Li Y, et al. Look deeper see richer: depth-aware image paragraph captioning. ACM Multimedia 2018.
[13] Che Wenbin, Fan Xiaopeng, Xiong Ruiqin, et al. Paragraph generation network with visual relationship detection. ACM Multimedia 2018.
[14] Dauphin Y N, Fan A, Auli M, et al. Language modeling with gated convolutional networks. ICML 2017.
[15] Krishna R, Zhu Yuke, Groth O, et al. Visual genome: connecting language and vision using crowdsourced dense image annotations. IJCV 2017.
[16] Chen X, Fang H, Lin T Y, et al. Microsoft COCO captions: data collection and evaluation server. arXiv 2015.
[17] Papineni K, Roukos S, Ward T, et al. BLEU: a method for automatic evaluation of machine translation. ACL 2002.
[18] Vedantam R, Zitnick C L, Parikh D. CIDEr: consensus-based image description evaluation. CVPR 2015.