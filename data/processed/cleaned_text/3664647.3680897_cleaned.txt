Triple Alignment Strategies for Zero-shot Phrase Grounding under Weak Supervision

Pengyue Lin, Ruifan Li, Yuzhe Ji, Zhihan Yu, Fangxiang Feng, Zhanyu Ma, Xiaojie Wang

Abstract
Phrase Grounding (PG) aims to locate objects referred by noun phrases. We propose a framework for zero-shot PG under weak supervision, based on triple alignment strategies. These include region-text alignment (RTA) to associate region-level attributes via CLIP, domain alignment (DomA) to minimize distribution differences between training and pre-training, and category alignment (CatA) considering category semantics and region-category relations. Our PG framework outperforms previous zero-shot and weakly-supervised methods.

CCS Concepts
â€¢ Information systems â†’Multimedia and multimodal retrieval.

1 Introduction
PG is crucial for various downstream tasks. PG under weak supervision reduces annotation costs but struggles with limited seen categories during training. Zero-shot PG addresses this by leveraging semantic information across categories. We introduce a framework that addresses challenges in both weak supervision and zero-shot settings, focusing on attribute association, knowledge transfer, and category similarity and difference measurement.

Phrases can be utilized to categorize referred objects, with correct visual-textual relations aiding in distinguishing categories. Our approach employs a novel PG framework using triple alignment strategies under weak supervision: 1) Region-Text Alignment (RTA) to associate region-level attributes based on Contrastive Language-Image Pre-Training (CLIP). 2) Domain Alignment (DomA) transfers knowledge from seen classes by aligning grounding-related features in a domain-invariant space. 3) Category Alignment (CatA) distinguishes grounding-region categories, inspired by class activation methods.

Related works in weakly-supervised PG include detector-based methods and auxiliary task designs. Zero-shot PG methods, on the other hand, aim to predict bounding boxes for unseen categories using limited seen category data. They struggle with unseen categories due to limited training data.

Our methodology involves a CLIP-based module, pre-trained on a large dataset, which extracts region-level image semantics and fuses them with text embeddings to generate heatmaps. The grounding module consists of bi-modal encoders and a grounding decoder, utilizing the RTA, DomA, and CatA strategies for improved grounding performance in zero-shot settings under weak supervision.

The grounding module consists of an image encoder Eğ‘–ğ‘šğ‘”(Â·), a text encoder Eğ‘¡ğ‘¥ğ‘¡(Â·), and a grounding decoder Dğ‘”ğ‘›ğ‘‘(Â·), returning a heatmap ğ» as follows:

ğ» = Dğ‘”ğ‘›ğ‘‘(Eğ‘–ğ‘šğ‘”(ğ¼), Eğ‘¡ğ‘¥ğ‘¡(ğ‘‡))

The multimodal feature fusion calculates the similarity of textual and visual features, ğ´ğ‘€ = Eğ¼ğ‘šğ‘”(ğ¼) âŠ— Eğ‘‡ğ‘¥ğ‘¡(ğ‘‡), with attention ğ‘…ğ‘€ = Eğ‘–ğ‘šğ‘”(ğ¼) âŠ™ ğ´ğ‘€. The decoder converts high-dimensional fusion features into grounding heatmaps ğ» using two up-sampling layers. We propose triple alignment strategies: Region-text Alignment (RTA), Domain Alignment (DomA), and Category Alignment (CatA).

RTA strategy employs a fixed CLIP to generate region-level attribute associations. We process mask tokens in the CLIP image encoder to transfer semantic information to patch tokens, enhancing semantic transfer in the last layer. The CLIP-based heatmap ğ´ğ¶ is computed by the inner product of the text embedding and the final embedding vectors. Heatmap refinement involves patch-to-patch attention and gradient map computation.

DomA strategy aims to minimize the difference in feature distributions between training and pretraining phases. We use contrastive learning and alignment loss to select positive and negative samples based on the CLIP-based heatmap ğ´âˆ— ğ¶.

CatA strategy is not detailed in the provided content but is mentioned as part of the triple alignment strategies.

æˆ‘ä»¬å°†çŸ­è¯­æ¥åœ°ï¼ˆPGï¼‰è§†ä¸ºçŸ­è¯­-åŒºåŸŸå¯¹é½é—®é¢˜ã€‚å—ç±»æ¿€æ´»æ–¹æ³•å¯å‘ï¼Œæˆ‘ä»¬ä½¿ç”¨çŸ­è¯­åµŒå…¥Eğ‘¡ğ‘¥ğ‘¡(ğ‘‡)æ¥åŒºåˆ†æ¥åœ°ç›¸å…³çš„ç‰¹å¾ç±»åˆ«ï¼Œå¹¶å°†å…¶æ ‡å‡†åŒ–ä¸ºç±»åˆ«æ ‡ç­¾ğ‘¦ã€‚ä¸ºäº†æ„å»ºåŒºåŸŸ-ç±»åˆ«å…³ç³»ï¼Œæˆ‘ä»¬åŸºäºçŸ­è¯­åµŒå…¥å’Œæ¥åœ°åŒºåŸŸåµŒå…¥è®¡ç®—CLIPåŒ¹é…åˆ†æ•°ã€‚ä¸ºç¡®ä¿æ¯ä¸ªè®­ç»ƒç±»åˆ«èƒ½è¢«é¢„è®­ç»ƒçš„CLIPçœ‹åˆ°ï¼Œæˆ‘ä»¬æ²¿ç€è¾¹ç•Œæ¡†è£å‰ªå‡ºæ¥åœ°åŒºåŸŸã€‚æˆ‘ä»¬é€šè¿‡CLIPåˆ©ç”¨ä¸çŸ­è¯­ç›¸å…³çš„å¯¹è±¡ä¿¡æ¯ï¼Œå°†åŒºåŸŸğµ(ğ»)æ¥åœ°ï¼Œç„¶åå°†å…¶é‡å¡‘ä¸ºğ»ï¼Œå¹¶é€šè¿‡CLIPæ˜ å°„åˆ°å›¾åƒè¡¨ç¤ºï¼Œå³ğ‘£ğ‘‚= Eğ‘–ğ‘šğ‘”(ğµ(ğ»))ã€‚å¯¹è±¡è¡¨ç¤ºğ‘£ğ‘‚å’ŒçŸ­è¯­è¡¨ç¤ºEğ‘¡ğ‘¥ğ‘¡(ğ‘‡)ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼æ€§ç”¨äºæŸå¤±è®¡ç®—ï¼š

â„“ğ‘‚ğ‘ƒ= âˆ’ âˆ‘ï¸ğ‘› ğ‘¦ğ‘›logğ‘ ğ‘›ï¼Œ

å…¶ä¸­ğ‘ ğ‘›æ˜¯ä½™å¼¦ç›¸ä¼¼æ€§ã€‚å› æ­¤ï¼Œåœ¨â„“ğ‘‚ğ‘ƒçš„ç›‘ç£ä¸‹ï¼Œçƒ­å›¾ğ»é€æ¸æ¥è¿‘ä¸çŸ­è¯­ç›¸å…³çš„å¯¹è±¡ã€‚

ä¸ºäº†å¢åŠ ä¸çŸ­è¯­æ— å…³åŒºåŸŸä¸çŸ­è¯­è¡¨ç¤ºä¹‹é—´çš„è·ç¦»ï¼Œæˆ‘ä»¬é€šè¿‡è£å‰ªéšæœºç”Ÿæˆçš„ğ»å¤§å°çš„æ¡†ğµğ‘…æ¥æ¥åœ°éå¯¹è±¡åŒºåŸŸã€‚ç”Ÿæˆçš„æ¡†åº”ä¸è¾¹ç•Œæ¡†ğµ(ğ»)çš„äº¤å å°½é‡å°‘ã€‚è¯¥æ¡†ç”±CLIPå›¾åƒç¼–ç å™¨è¡¨ç¤ºï¼Œå³ğ‘£ğ‘…= Eğ‘–ğ‘šğ‘”(ğµğ‘…)ã€‚æ¡†è§†è§‰è¡¨ç¤ºğ‘£ğ‘…å’ŒçŸ­è¯­è¡¨ç¤ºEğ‘¡ğ‘¥ğ‘¡(ğ‘‡)ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼æ€§ç”¨äºæŸå¤±è®¡ç®—ï¼š

â„“ğ‘…ğ‘ƒ= âˆ’ âˆ‘ï¸ğ‘› ğ‘¦ğ‘›log(1 âˆ’ğ‘ âˆ— ğ‘›)ï¼Œ

å…¶ä¸­ğ‘ âˆ—ğ‘›è¡¨ç¤ºä½™å¼¦ç›¸ä¼¼æ€§ã€‚å› æ­¤ï¼Œçƒ­å›¾ğ»ä¿ç•™äº†è¾ƒå°‘çš„çŸ­è¯­æ— å…³åŒºåŸŸçš„å¯¹è±¡éƒ¨åˆ†ã€‚

ä¸ºäº†è¿›ä¸€æ­¥æ’é™¤çƒ­å›¾ä¸­çš„ä¸ç›¸å…³èƒŒæ™¯ï¼Œæˆ‘ä»¬é€šè¿‡ä»¥ä¸‹æŸå¤±å‡½æ•°é™åˆ¶çƒ­å›¾åŒºåŸŸå¤§å°ï¼š

â„“ğ‘…ğ¸= 1/ğ‘› âˆ‘ğ»ğ‘›ï¼Œ

æˆ‘ä»¬çš„æ¨¡å‹çš„æ€»æŸå¤±ä¸ºï¼š

â„“ğ‘‡= â„“1 + ğœ†1â„“ğ‘‚ğ‘ƒ+ ğœ†2â„“ğ‘…ğ‘ƒ+ ğœ†3â„“ğ‘…ğ¸+ ğœ†4â„“ğ‘ğ‘œğ‘›ã€‚

åŸºäºæ¥åœ°æ¨¡å—ï¼Œæˆ‘ä»¬æŒ‰ä»¥ä¸‹æ–¹å¼ç”Ÿæˆè¾¹ç•Œæ¡†ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬å°†ä½å€¼åƒç´ è®¾ç½®ä¸º0ï¼Œé˜ˆå€¼ä¸º0.5ã€‚ç„¶åæœç´¢è½®å»“å¹¶æå–åˆé€‚çš„è¾¹ç•Œæ¡†ã€‚æˆ‘ä»¬æ ¹æ®çƒ­å›¾ğ»çš„é¢ç§¯ç™¾åˆ†æ¯”è®¡ç®—è¾¹ç•Œæ¡†çš„åˆ†æ•°ã€‚æœ€åï¼Œåº”ç”¨éæå¤§å€¼æŠ‘åˆ¶ï¼Œğ¼ğ‘œğ‘ˆ= 0.3ï¼Œè¿‡æ»¤æ‰å¾—åˆ†ä½äºæœ€å¤§å¾—åˆ†50%çš„æ¡†ï¼Œå®Œæˆå®šä½ã€‚

4.1 æ•°æ®é›†

æˆ‘ä»¬åœ¨é›¶æ ·æœ¬PGè®¾ç½®ä¸‹ä½¿ç”¨Flickr-Split-S0ã€Flickr-Split-S1ã€VG-Split-S2å’ŒVG-Split-S3è¯„ä¼°æˆ‘ä»¬çš„æ¡†æ¶ã€‚æ­¤å¤–ï¼Œä¸ºäº†ä¸å…ˆå‰çš„å¼±ç›‘ç£æ¥åœ°æ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬ä½¿ç”¨äº†åœ¨MGä¸­é‡‡ç”¨çš„è®¾ç½®ï¼Œè¯¥è®¾ç½®åˆ†åˆ«ä½¿ç”¨MS-COCOæˆ–VGè®­ç»ƒåˆ’åˆ†çš„å¤šç§å·¥ä½œã€‚åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œæ¨¡å‹åœ¨Flickr30Kã€VGå’ŒReferItçš„æµ‹è¯•åˆ’åˆ†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚

4.2 åŸºçº¿ä¸è¯„ä»·æŒ‡æ ‡

æˆ‘ä»¬å°†æˆ‘ä»¬çš„æ¡†æ¶ä¸å„ç§SoTAåŸºçº¿è¿›è¡Œäº†æ¯”è¾ƒï¼Œè¿™äº›åŸºçº¿å¯ä»¥åˆ†ä¸ºä»¥ä¸‹ä¸‰ç±»ï¼š1) ç›‘ç£é›¶æ ·æœ¬åŸºçº¿ï¼Œå³ZSGNetã€‚2) é›¶æ ·æœ¬åŸºçº¿ï¼ŒåŒ…æ‹¬æ£€æµ‹å™¨+CLIPã€GAEã€Grad-CAMã€AdaptingCLIPå’ŒMaskCLIPã€‚3) å¼±ç›‘ç£åŸºçº¿ï¼ŒåŒ…æ‹¬MGã€GbSã€WWblã€SMSTã€BBRå’ŒVPTã€‚æˆ‘ä»¬ä½¿ç”¨äº†â€œæŒ‡ç‚¹æ¸¸æˆâ€å‡†ç¡®æ€§ã€è¾¹ç•Œæ¡†å‡†ç¡®æ€§å’Œè¯†åˆ«å‡†ç¡®æ€§ä¸‰ç§è¯„ä»·æŒ‡æ ‡ã€‚

4.3 å®ç°ç»†èŠ‚

åœ¨æˆ‘ä»¬çš„æ¡†æ¶ä¸­ï¼Œä½¿ç”¨äº†VGG-16å’ŒCLIP RN50ä½œä¸ºè§†è§‰ç¼–ç å™¨ã€‚æ¨¡å‹æ¥å—224Ã—224çš„å›¾åƒå¤§å°ï¼Œè¿™æ˜¯CLIPè§†è§‰åˆ†æ”¯VIT-B/32çš„è¾“å…¥å¤§å°ï¼Œå¹¶ç”Ÿæˆç›¸åŒå¤§å°çš„çƒ­å›¾ğ»ã€‚æˆ‘ä»¬ä½¿ç”¨SGDä¼˜åŒ–å™¨ï¼ˆæ‰¹é‡å¤§å°ä¸º64ï¼Œåˆå§‹å­¦ä¹ ç‡ä¸º0.0003ï¼‰è®­ç»ƒ150ä¸ªå‘¨æœŸï¼Œå…¶ä¸­ä¼˜åŒ–å™¨åŠ¨é‡ä¸º0.9ï¼Œæƒé‡è¡°å‡ä¸º0.0001ã€‚æ­¤å¤–ï¼Œå±‚ğ¿è®¾ç½®ä¸º11ã€‚å½“ğ‘™<11æ—¶ï¼Œå‚æ•°ğ›¼å’Œğ›½è®¾ç½®ä¸º0.01å’Œ1ï¼›å½“ğ‘™=ğ¿æ—¶ï¼Œå®ƒä»¬è®¾ç½®ä¸º1å’Œ10ã€‚æ‰€æœ‰æ–¹æ³•éƒ½åœ¨NVIDIA RTX A6000ä¸Šå®ç°ã€‚åœ¨æˆ‘ä»¬æ‰€æœ‰çš„å®éªŒä¸­ï¼Œæ ¹æ®æ–¹ç¨‹å¼(11)ä¸­çš„æŸå¤±æƒé‡è®¾ç½®ä¸ºğœ†1 = 0.25ï¼Œğœ†2 = 0.125ï¼Œğœ†3 = 0.25ï¼Œå’Œğœ†4 = 1ã€‚

4.4 ä¸»è¦ç»“æœ
---

We present zero-shot evaluation results on unseen phrase classes using Flickr-Split and VG-Split test splits, with IoU thresholds set at 0.3 and 0.5 due to VG-Splitâ€™s textual noise. Our approach outperforms other methods at IoU threshold 0.5, indicating better coverage of unseen categories. While there is a gap compared to supervised methods, our approach significantly improves upon weakly supervised methods. The WWbl model's heatmap masking and external knowledge usage for similarity measurement introduce an incorrect accumulation of category judgment. Our results are close to supervised grounding SoTA on Flickr-Split-1, showcasing strong generalization on unseen phrase classes.

In weakly supervised evaluation on seen classes, our PG framework demonstrates competitive performance against other methods on Flickr30K, VG, and ReferIt datasets. Our CLIP-based heatmap surpasses the pseudo label used by WWbl, explaining a 9% increase in bbox accuracy.

Our ablation study shows the effects of alignment strategies on Flickr30K Entities, with the best performance achieved using all alignment strategies. Different loss functions were analyzed, with the addition of â„“ğ‘ğ‘œğ‘›, â„“ğ‘‚ğ‘ƒ, â„“ğ‘…ğ‘ƒ, and â„“ğ‘…ğ¸ improving IoU performance. We also compared our domain alignment strategy with a baseline using CLIP (RN50) as the image encoder, with our strategy achieving better domain adaptation between seen and unseen classes.

Qualitative analysis illustrates the effectiveness of our alignment strategies, with our method successfully grounding referred objects and capturing less phrase-related background regions compared to VLP-based and weakly-supervised PG methods.

---

Table 5: Bounding box accuracy across unseen splits. For Flickr-Split-0&1, IoU threshold of 0.5 is used. For VG-Split-2&3, accuracy is reported with IoU thresholds of 0.3 and 0.5. "B" and "UB" indicate balanced and unbalanced sets in VG-Split.

Phrase: clarinetto
RTA + CatA

Phrase: purple shirt
Phrase: a beautiful bride
Phrase: a handsome groom

GT & Ours Image + DomA
GT & Ours RTA + CatA + DomA

Figure 7: Qualitative results are presented. Input Images are in the leftmost column. Columns #2-4 show generated heatmaps using RTA, RTA+DomA, and triple alignments, respectively. Columns #6-8 present results for another phrase. White boxes represent ground truth, and red indicates our results.

Method Overall People Animals Vehicles Scene Other

MaskCLIP 34.26 37.46 40.93 52.25 48.40 25.87
AdaptingCLIP 29.47 29.23 40.15 45.00 41.86 24.92
GAE 25.56 26.76 39.72 38.12 33.72 22.22
CH 43.75 56.33 62.31 58.60 52.78 32.26
Ours w/ GAE 36.35 43.58 48.22 52.72 55.94 26.44
Ours w/ CH 45.46 56.44 59.95 57.68 70.04 32.53

Table 6: Category-wise bounding box accuracy on Flickr30K.

AdaptingCLIP GAE MaskCLIP Ours

Caption: A man on a black mountain bike
Caption: A guy wearing a white shirt
Caption: A woman pointing to the subway station
Caption: A man in a plaid shirt and blue jeans

WWbl Ground Truth Image

Figure 8: Qualitative results comparison.

Our framework grounds more complete and compact regions, avoiding false grounding between the region of blue jeans and the woman or the mountain bike and the subway station.

Caption: A new crew and a reporter in a blue coat make a film in the rain.
Ground Truth
Caption: The kid wears glasses and two kids are smiling.

Figure 9: Failure cases of our proposed method.

Failure cases are categorized into similar dense objects and in-context entities-related objects. Our framework highlights connected regions for dense objects, with imprecise determination of the number of bounding boxes. Additionally, it extracts only noun phrases without considering in-context phrases, leading to inaccurate location evaluation of referred objects.

5 Conclusion and Future Work

We propose a PG framework that designs alignment strategies to address three problems of zero-shot grounding under weak supervision. Our approach exceeds previous methods and will explore interpretable solutions for grounding-related tasks. Future work includes using multi-modal large language models to enhance zero-shot PG under weak supervision.

Acknowledgment

This work was supported by the Beijing Natural Science Foundation Project No. Z200002, the National Nature Science Foundation of China (Grants 62076032, 62225601, U23B2052), the Youth Innovative Research Team of BUPT No. 2023YQTD02, BUPT Excellent Ph.D. Students Foundation CX2023113, and High Performance Computing Platform of BUPT.

References

---

---

[1] Hassan Akbari et al. 2019. Multi-level multimodal common semantic space for image-phrase grounding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 12476â€“12486.

[2] Assaf Arbelle et al. 2021. Detector-free weakly supervised grounding by separation. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 1801â€“1812.

[3] Kai Uwe Barthel et al. 2019. Real-time visual navigation in huge image sets using similarity graphs. In Proceedings of the 27th ACM International Conference on Multimedia. 2202â€“2204.

[4] Hila Chefer et al. 2021. Transformer interpretability beyond attention visualization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 782â€“791.

[5] Kan Chen et al. 2017. Query-guided regression network with context policy for phrase grounding. In Proceedings of the IEEE International Conference on Computer Vision. 824â€“832.

[6] Kang Chen et al. 2023. VTQA2023: ACM Multimedia 2023 Visual Text Question Answering Challenge. In Proceedings of the 31st ACM International Conference on Multimedia. 9646â€“9650.

[7] Nenglun Chen et al. 2021. Distributed attention for grounded image captioning. In Proceedings of the 29th ACM International Conference on Multimedia. 1966â€“1975.

[8] Samyak Datta et al. 2019. Align2ground: Weakly supervised phrase grounding guided by image-caption alignment. In Proceedings of the IEEE/CVF international conference on computer vision. 2601â€“2610.

[9] Thomas Eiter et al. 2023. A Logic-based Approach to Contrastive Explainability for Neurosymbolic Visual Question Answering. In IJCAI. 3668â€“3676.

[10] Hao Fang et al. 2015. From captions to visual concepts and back. In Proceedings of the IEEE conference on computer vision and pattern recognition. 1473â€“1482.

[11] Eyal Gomel et al. 2023. Box-based Refinement for Weakly Supervised and Unsupervised Localization Tasks. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 16044â€“16054.

[12] Michael Grubinger et al. 2006. The iapr tc-12 benchmark: A new evaluation resource for visual information systems. In International workshop ontoImage. 13â€“23.

[13] Tanmay Gupta et al. 2020. Contrastive learning for weakly supervised phrase grounding. In European Conference on Computer Vision. 752â€“768.

[14] Kaiming He et al. 2020. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 9729â€“9738.

[15] Yicong Hong et al. 2023. Learning navigational visual representations with semantic map supervision. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 3055â€“3067.

[16] Syed Ashar Javed et al. 2018. Learning unsupervised visual grounding through semantic self-supervision. CoRR abs/1803.06506.

[17] Haojun Jiang et al. 2022. Pseudo-q: Generating pseudo language queries for visual grounding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 15513â€“15523.

[18] Huiju Kim et al. 2023. Examining Consistency of Visual Commonsense Reasoning based on Person Grounding. In Proceedings of the 13th International Joint Conference on Natural Language Processing.

---

Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L-J., Shamma, D.A., et al. (2017). Visual genome: Connecting language and vision using crowdsourced dense image annotations. International Journal of Computer Vision, 123, 32â€“73.

Li, J., Selvaraju, R., Gotmare, A., Joty, S., Xiong, C., & Hoi, S.C.H. (2021). Align before fuse: Vision and language representation learning with momentum distillation. Advances in Neural Information Processing Systems, 34, 9694â€“9705.

Li, J., Shakhnarovich, G., & Yeh, R.A. (2022). Adapting CLIP for phrase localization without further training. CoRR, arXiv:2204.03647.

Li, M., Wang, Z., Tuytelaars, T., & Moens, M.-F. (2023). Layout-aware Dreamer for embodied visual referring expression grounding. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 37, 1386â€“1395.

Li, W., Song, X., Bai, Y., Zhang, S., & Jiang, S. (2021). Ion: Instance-level object navigation. In Proceedings of the 29th ACM International Conference on Multimedia, 4343â€“4352.

Lin, P., Yu, Z., Lu, M., Feng, F., Li, R., & Wang, X. (2024). Visual Prompt Tuning for Weakly Supervised Phrase Grounding. In ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 7895â€“7899.

Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., DollÃ¡r, P., & Zitnick, C.L. (2014). Microsoft COCO: Common objects in context. In Computer Visionâ€“ECCV 2014: 13th European Conference, 740â€“755.

Lin, Y., Chen, M., Wang, W., Wu, B., Li, K., Lin, B., Liu, H., & He, X. (2023). Clip is also an efficient segmenter: A text-driven approach for weakly supervised semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 15305â€“15314.

Liu, X., Li, L., Wang, S., Zha, Z.-J., Li, Z., Tian, Q., & Huang, Q. (2022). Entity-enhanced adaptive reconstruction network for weakly supervised referring expression grounding. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(3), 3003â€“3018.

Liu, X., Li, L., Wang, S., Zha, Z.-J., Meng, D., & Huang, Q. (2019). Adaptive reconstruction network for weakly supervised referring expression grounding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2611â€“2620.

Liu, X., Li, L., Wang, S., Zha, Z.-J., Su, L., & Huang, Q. (2019). Knowledge-guided pairwise reconstruction network for weakly supervised referring expression grounding. In Proceedings of the 27th ACM International Conference on Multimedia, 539â€“547.

Liu, Y., Shi, Y., Feng, F., Li, R., Ma, Z., & Wang, X. (2022). Improving Image Paragraph Captioning with Dual Relations. In 2022 IEEE International Conference on Multimedia and Expo (ICME), 1â€“6.

Liu, Y., Wan, B., Ma, L., & He, X. (2021). Relation-aware instance refinement for weakly supervised visual grounding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 5612â€“5621.

Liu, Y., Zhang, J., Chen, Q., & Peng, Y. (2023). Confidence-aware Pseudo-label Learning for Weakly Supervised Visual Grounding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2828â€“2838.

Lu, J., Goswami, V., Rohrbach, M., Parikh, D., & Lee, S. (2020). 12-in-1: Multi-task vision and language representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 10437â€“10446.

Lu, M., Li, R., Feng, F., Ma, Z., & Wang, X. (2024). LGR-NET: Language Guided Reasoning Network for Referring Expression Comprehension. IEEE Transactions on Circuits and Systems for Video Technology, 1â€“1. https://doi.org/10.1109/TCSVT.2024.3374786

Plummer, B.A., Wang, L., Cervantes, C.M., Caicedo, J.C., Hockenmaier, J., & Lazebnik, S. (2015). Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In Proceedings of the IEEE international conference on computer vision, 2641â€“2649.

Qin, L., Chen, Q., Zhou, Y., Chen, Z., Li, Y., Liao, L., Li, M., Che, W., & Yu, P.S. (2024). Multilingual Large Language Model: A Survey of Resources, Taxonomy and Frontiers. arXiv:2404.04925 [cs.CL].

Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. (2021). Learning transferable visual models from natural language supervision. In International conference on machine learning, 8748â€“8763.

Raghu, M., Unterthiner, T., Kornblith, S., Zhang, C., & Dosovitskiy, A. (2021). Do vision transformers see like convolutional neural networks? Advances in Neural Information Processing Systems, 34, 12116â€“12128.

---

Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. Advances in Neural Information Processing Systems, Vol. 28.

Arka Sadhu, Kan Chen, and Ram Nevatia. 2019. Zero-shot grounding of objects from natural language queries. Proceedings of the IEEE/CVF International Conference on Computer Vision, 4694â€“4703.

Tal Shaharabany, Yoad Tewel, and Lior Wolf. 2022. What is where by looking: Weakly-supervised open-world phrase-grounding without text inputs. Advances in Neural Information Processing Systems 35, 28222â€“28237.

Tal Shaharabany and Lior Wolf. 2023. Similarity Maps for Self-Training Weakly-Supervised Phrase Grounding. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 6925â€“6934.

Haozhan Shen, Tiancheng Zhao, Mingwei Zhu, and Jianwei Yin. 2024. Ground-VLP: Harnessing Zero-Shot Visual Grounding from Vision-Language Pre-training and Open-Vocabulary Object Detection. AAAI Conference on Artificial Intelligence, Vol. 38, 4766â€“4775.

Yibing Song, Ruifei Zhang, Zhihong Chen, Xiang Wan, and Guanbin Li. 2023. Advancing visual grounding with scene knowledge: Benchmark and method. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 15039â€“15049.

Sanjay Subramanian, William Merrill, Trevor Darrell, Matt Gardner, Sameer Singh, and Anna Rohrbach. 2022. ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension. 60th Annual Meeting of the Association for Computational Linguistics, 5198â€“5215.

Satoshi Suzuki et al. 1985. Topological structural analysis of digitized binary images by border following. Computer vision, graphics, and image processing 30, 1, 32â€“46.

Feng Wang, Jieru Mei, and Alan Yuille. 2024. SCLIP: Rethinking Self-Attention for Dense Vision-Language Inference. arXiv:2312.01597 [cs.CV]

Mingzhe Wang, Mahmoud Azab, Noriyuki Kojima, Rada Mihalcea, and Jia Deng. 2016. Structured matching for phrase localization. Computer Visionâ€“ECCV 2016, 696â€“711.

Qinxin Wang, Hao Tan, Sheng Shen, Michael Mahoney, and Zhewei Yao. 2020. MAF: Multimodal Alignment Framework for Weakly-Supervised Phrase Grounding. Conference on Empirical Methods in Natural Language Processing (EMNLP), 2030â€“2038.

Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin. 2023. Towards explainable in-the-wild video quality assessment: a database and a language-prompted approach. 31st ACM International Conference on Multimedia, 1045â€“1054.

Siying Wu, Xueyang Fu, Feng Wu, and Zheng-Jun Zha. 2022. Cross-modal semantic alignment pre-training for vision-and-language navigation. 30th ACM International Conference on Multimedia, 4233â€“4241.

Yuechen Wu, Zhenhuan Rao, Wei Zhang, Shijian Lu, Weizhi Lu, and Zheng-Jun Zha. 2019. Exploring the Task Cooperation in Multi-goal Visual Navigation. IJCAI, 609â€“615.

Lian Xu, Wanli Ouyang, Mohammed Bennamoun, Farid Boussaid, and Dan Xu. 2022. Multi-class token transformer for weakly supervised semantic segmentation. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 4310â€“4319.

Dizhan Xue, Shengsheng Qian, and Changsheng Xu. 2023. Variational Causal Inference Network for Explanatory Visual Question Answering. IEEE/CVF International Conference on Computer Vision, 2515â€“2525.

Zhihan Yu and Ruifan Li. 2024. Revisiting Counterfactual Problems in Referring Expression Comprehension. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 13438â€“13448.

Jianming Zhang, Sarah Adel Bargal, Zhe Lin, Jonathan Brandt, Xiaohui Shen, and Stan Sclaroff. 2018. Top-down neural attention by excitation backprop. International Journal of Computer Vision, 126, 10, 1084â€“1102.

Wenqiao Zhang, Xin Eric Wang, Siliang Tang, Haizhou Shi, Haochen Shi, Jun Xiao, Yueting Zhuang, and William Yang Wang. 2020. Relational graph learning for grounded video description generation. 28th ACM International Conference on Multimedia, 3807â€“3828.

Zicheng Zhang, Bonan Li, Xuecheng Nie, Congying Han, Tiande Guo, and Luoqi Liu. 2023. Towards Consistent Video Editing with Text-to-Image Diffusion Models. Advances in Neural Information Processing Systems, Vol. 36.

Chong Zhou, Chen Change Loy, and Bo Dai. 2022. Extract free dense labels from clip. European Conference on Computer Vision, Springer, 696â€“712.

---