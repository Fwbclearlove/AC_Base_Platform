[
  {
    "chunk_id": "0_387_29295_0_82_s0_c0",
    "source_id": "0_387_29295_0_82",
    "text": "---\n\nSKFD-ISOMAP FOR FACE RECOGNITION\n\nRuifan Li, Cong Wang, and Xuyan Tu\nBeijing University of Posts and Telecommunications, Beijing, China\nUniversity of Science and Technology Beijing, Beijing, China\nAbstract:",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 3,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 211,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0_387_29295_0_82_s0_c1",
    "source_id": "0_387_29295_0_82",
    "text": "Recently, neuroscientists emphasized the manifold ways of perception and proposed Isomap for manifold learning. Isomap has achieved favorable results in data description and visualization. However, the unsupervised Isomap, developed based on multidimensional scaling (MDS) without using class-specific information, may not be optimal for pattern classification. We propose an improved version of Isomap, SKFD-Isomap, which uses class information to construct the neighborhood and kernel Fisher discriminant (KFD) for nonlinear embedding. A nearest neighbor classifier is applied in the subspace for",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0_387_29295_0_82_s0_c2",
    "source_id": "0_387_29295_0_82",
    "text": "nlinear embedding. A nearest neighbor classifier is applied in the subspace for classification, with experimental results demonstrating the effectiveness of our approach.\nKey words: face recognition, manifold, Isomap, KFD",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 221,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0_387_29295_0_82_s1_c0",
    "source_id": "0_387_29295_0_82",
    "text": "Subspace methods have gained interest for face recognition. Facial images, high-dimensional pixel arrays, often belong to a low-dimensional subspace. Eigenfaces and Fisherfaces are two such examples. Eigenfaces, based on PCA, is unsupervised and effective for data visualization but not optimal for classification. Fisherfaces, based on FLD, optimizes between-class distances but confronts the singularity issue. Both search for a linear subspace and fail to address nonlinear changes in face images. Kernel methods like kernel Eigenfaces and Fisherfaces consider nonlinear structures but do not expl",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 3,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 9,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 600,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0_387_29295_0_82_s1_c1",
    "source_id": "0_387_29295_0_82",
    "text": "kernel Eigenfaces and Fisherfaces consider nonlinear structures but do not explicitly account for the manifold structure. We propose SKFD-Isomap, which utilizes class information and KFD for optimal projection direction using geodesic distances.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 4,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 245,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0_387_29295_0_82_s1_c2",
    "source_id": "0_387_29295_0_82",
    "text": "2. ISOMAP\nIsomap estimates the geodesic distance between data points by preserving the intrinsic geometry of the data. Given a set of samples, Isomap constructs a neighborhood graph, computes shortest paths, and applies MDS to the matrix of graph distances for a lower-dimensional embedding. For classification, Isomap serves as a feature extraction process but lacks effective discriminant information. Modifications are necessary for pattern classification.\n\n3. SKFD-ISOMAP\n\n---",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 5,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 480,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0_387_29295_0_82_s1_c3",
    "source_id": "0_387_29295_0_82",
    "text": "3. SKFD-ISOMAP\n\n---\n\nConsider a set of c disjoint subsets, each with /?, samples. SKFD-Isomap modifies the Euclidean distance between data points by incorporating class information with a constant factor A (0 < Ä < 1) if the class labels are the same. This modified distance matrix is used to determine neighborhood relationships represented in a weighted graph G. Geodesic distances are estimated, with neighboring points approximated by input space distance and faraway points by shortest paths on G.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 6,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 502,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0_387_29295_0_82_s1_c4",
    "source_id": "0_387_29295_0_82",
    "text": "The feature vectors of data points are created using geodesic distances and used in KFD to find an optimal projection direction for classification. KFD projects vectors to a high-dimensional feature space G and formulates the FLD problem using dot products. The between-class and within-class scatter matrices are defined, and FLD is applied in the kernel space to find eigenvalues and eigenvectors.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 7,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 399,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0_387_29295_0_82_s1_c5",
    "source_id": "0_387_29295_0_82",
    "text": "The kernel matrix K is defined, and the KFD problem is expressed as KKα = KZKα. Test sample feature vectors are projected onto the eigenvectors to learn a subspace for classification. SKFD-Isomap involves five steps: computing original distances, scaling with class information, computing shortest paths, applying KFD to the matrix of shortest paths, and classifying test samples.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 8,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 380,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0_387_29295_0_82_s1_c6",
    "source_id": "0_387_29295_0_82",
    "text": "In experiments, SKFD-Isomap is compared with kernel Fisherfaces using the Yale face database. Images are represented as raster scan vectors, normalized, and randomly split into training and test sets. The performance of SKFD-Isomap with different values of A and polynomial kernel functions is tested. SKFD-Isomap shows better performance than kernel Fisherfaces and Ext-Isomap, likely due to the use of class information in feature extraction.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 9,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 444,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0_387_29295_0_82_s1_c7",
    "source_id": "0_387_29295_0_82",
    "text": "Face recognition research has seen significant contributions from various techniques. Zhao et al. (2003) provided a comprehensive literature survey on the topic. Turk and Pentland (1991) introduced eigenfaces for recognition, while Belhumeur et al. (1997) compared eigenfaces with Fisherfaces. Martinez and Kak (2001) discussed PEA versus Ida.\n\nLocality preserving subspace learning was explored by He et al. (2003) for visual recognition. Yang (2002) compared kernel eigenfaces with kernel Fisherfaces. Liu et al. (2002) proposed kernel-based nonlinear discriminant analysis for face recognition.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 10,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 597,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0_387_29295_0_82_s1_c8",
    "source_id": "0_387_29295_0_82",
    "text": "Manifold learning approaches were investigated by Seung and Lee (2000) and Silva et al. (2000), with Roweis and Saul (2000) introducing locally linear embedding. Belkin and Niyogi (2001) discussed Laplacian eigenmaps, while Yang (2002) extended isomap for face recognition.\n\nFisher discriminant analysis with kernels was presented by Mika et al. (1999), and Scholkopf et al. (1998) addressed nonlinear component analysis as a kernel eigenvalue problem. Finally, Specht (1991) introduced the general regression neural network.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 11,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 525,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0592_s0_c0",
    "source_id": "0592",
    "text": "Show and Tell More: Topic-Oriented Multi-Sentence Image Captioning\n\nYuzhao Mao, Chang Zhou, Xiaojie Wang, Ruifan Li\nCenter for Intelligence Science and Technology, School of Computer Science, Beijing University of Posts and Telecommunications\n{maoyuzhao,elani,xjwang,rﬂi}@bupt.edu.cn",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 5,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 283,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0592_s1_c0",
    "source_id": "0592",
    "text": "Image captioning generates textual descriptions for images. We propose a Topic-Oriented Multi-Sentence (TOMS) captioning model that generates multiple topic-oriented sentences. Our model uses latent Dirichlet allocation to reflect hidden thematic structures and integrates topics with a Fusion Gate Unit (FGU) for sentence generation. TOMS provides a complete image description and demonstrates effec",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 5,
    "chunk_index": 1,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 2,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 400,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9900000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0592_s1_c1",
    "source_id": "0592",
    "text": "complete image description and demonstrates effectiveness in terms of topical consistency and descriptive completeness.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 5,
    "chunk_index": 2,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 2,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 119,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9450000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0592_s2_c0",
    "source_id": "0592",
    "text": "Image captioning typically generates a single textual description but is often insufficient for a complete view of an image. We present a TOMS model for multi-sentence captioning, focusing on different topics to provide a semantically rich description. Our approach uses Latent Dirichlet Allocation (LDA) to mine topics from textual descriptions, avoiding the need for additional annotations. The TOMS model integrates topic embeddings with LSTM using a Fusion Gate Unit (FGU) for topical consistency. Our contributions include a novel topic-oriented captioning model, the FGU design, and extensive e",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 3,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 2,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 600,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0592_s2_c1",
    "source_id": "0592",
    "text": "include a novel topic-oriented captioning model, the FGU design, and extensive experimental evaluation.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 4,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 2,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 103,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0592_s3_c0",
    "source_id": "0592",
    "text": "Single-sentence (SS) captioning models have been developed using templates and neural network approaches, but they often provide incomplete descriptions. MS captioning methods generate multiple sentences for a complete depiction, with some focusing on regions of interest. Our TOMS model differs by generating sentences from topics of interest, capturing linguistic distinctions in image descriptions.\n\n---",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 5,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 7,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 406,
    "chunk_method": "hierarchical",
    "importance_weight": 0.66,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0592_s3_c1",
    "source_id": "0592",
    "text": "---\n\nThe term \"topic\" has been used in various studies, such as Liang et al. (2017), where it refers to regions of interest in an image, distinct from our TOMS definition. Dai et al. (2017) introduced a random vector for sentence diversity control using generative adversarial nets, while Wang et al. (2017) employed conditional variational auto-encoder. However, these methods lack directionality and descriptive completeness, unlike our TOMS, which generates directional sentences guided by explicit topic embeddings.\n\n3 Model",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 6,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 7,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 528,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0592_s3_c2",
    "source_id": "0592",
    "text": "3 Model\n\n3.1 Formulation\nGiven an image I and a sentence S = {w0, ..., wT} with T + 1 words, traditional image captioning aims to maximize the log likelihood of the sentence given the image:\n\nlog p(S|I) = Σ_t=1 log p(wt|wt−1, ..., w0, I)\n\nWe introduce the topic variable z ∈{z1, ..., zK} to learn distinctions from reference sentences. The objective function is modified to represent the joint distribution p(S, z|I), which can be unfolded into two terms:\n\nlog p(S, z|I) = log p(S|z, I) + log p(z|I) (1)",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 7,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 7,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 503,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0592_s3_c3",
    "source_id": "0592",
    "text": "log p(S, z|I) = log p(S|z, I) + log p(z|I) (1)\n\n3.2 LSTM\nOur model uses LSTM to encode image-sentence pairs. The LSTM processes the image feature initially and then the sentence word by word:\n\nht = LSTM(x0) (t = 0)\nht = LSTM(ht−1, xt) (t > 0) (2)\n\n3.3 Topic Embedding\nLDA is applied to reference sentences to generate topic embeddings. Each topic embedding is a weighted sum of the top N word embeddings:\n\np(w|zk) = Σ_n=1 φn,k * ⃗wn (4)",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 8,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 7,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 436,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0592_s3_c4",
    "source_id": "0592",
    "text": "p(w|zk) = Σ_n=1 φn,k * ⃗wn (4)\n\n3.4 FGU\nThe Fusion Gate Unit (FGU) fuses three sources of representations: image, context, and topic. It uses Hadamard product and concatenation to emphasize relevant information and maintain sentence fluency.\n\n3.5 Training\nOur TOMS outputs a topic classifier and a topic-oriented language model. The training involves a multi-label logistic regression for the classifier and a softmax for the language model. Topic selection for training is a sampling process based on LDA-inferred probabilities.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 9,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 7,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 529,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0592_s3_c5",
    "source_id": "0592",
    "text": "The training loss is a sum of two cross-entropy terms: the sentence loss on p(xt+1|xt, ..., x0, ˆz) and the topic loss on p(zi|x0):\n\nLoss = Σ_t=1 ℓsent(p(xt+1|xt, ..., x0, ˆz), yt+1) + Σ_i=1 ℓtopic(p(zi|x0), 1k)\n\n---\n\n---",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 10,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 7,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 221,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0592_s3_c6",
    "source_id": "0592",
    "text": "3.6 Inference MS is generated based on topics observed from a given image, with one topic per sentence. The observed topics are obtained by normalizing Eq.(9) to ensure the probabilities sum to one, ranking the topics, summing the probabilities of the top n topics, and choosing the n topics that exceed a threshold. Each sentence is sampled word by word using p(xt+1|xt,.,0, zk) until the end of the sentence token. Our method observes topics from a topic model trained on both images and sentences, without access to the reference sentences. We use p(z|x0) to approximate p(z|d) for generation.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 11,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 7,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 596,
    "chunk_method": "hierarchical",
    "importance_weight": 0.63,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0592_s4_c0",
    "source_id": "0592",
    "text": "4.1 Datasets and Metrics We evaluate our model on standard datasets like Flickr8k, Flickr30k, COCO for sentence level MS captioning, and a paragraph dataset from Krause et al. (2017) for paragraph level MS captioning. We use coco-caption for evaluation with metrics including BELU, METEOR, ROUGE L, and CIDEr. We also introduce Instance Coverage (IC) to measure descriptive completeness.",
    "section_title": "Experiment",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 12,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 13,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 387,
    "chunk_method": "hierarchical",
    "importance_weight": 0.935,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0592_s4_c1",
    "source_id": "0592",
    "text": "4.2 Implementation Our code is based on PyTorch and uses pre-trained CNN models for image features. We employ two-layer LSTM with a hidden dimension of 512 and set topic and word embedding size to 256. FGU uses 512-dimensional vectors for topics and images, and 1024 for context representations. Dropout is applied in input and output layers.",
    "section_title": "Experiment",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 13,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 342,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0592_s4_c2",
    "source_id": "0592",
    "text": "4.3 MS Captioning Experiment We compare our TOMS model with NIC and ATT-FCN for sentence level MS captioning and with RTT-GAN, Regions-Hierarchical, and Sentence-Concat for paragraph level MS captioning. Our TOMS demonstrates improved performance, especially in terms of IC.",
    "section_title": "Experiment",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 14,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 274,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0592_s4_c3",
    "source_id": "0592",
    "text": "4.4 Topical Consistency We evaluate our TOMS on topical consistency by annotating reference sentences with topic labels and clustering them into topic groups. Our model generates descriptions based on these topic labels, and the results show improved performance over baselines.",
    "section_title": "Experiment",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 15,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 278,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0592_s4_c4",
    "source_id": "0592",
    "text": "Our proposed Topic-Oriented Multi-Sentence (TOMS) captioning model incorporates topic embedding to guide the generation process. The Fixed Gate Unit (FGU) is designed to integrate topic, image, and context information, enhancing the model's performance compared to NIC, which lacks topic information. NIC-MS, a variant of NIC, demonstrates the effectiveness of considering topics by training sub-models on different image-sentence pairs. The use of gLSTM and SCN-RNN for comparison further validates",
    "section_title": "Experiment",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 16,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 499,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0592_s4_c5",
    "source_id": "0592",
    "text": "he use of gLSTM and SCN-RNN for comparison further validates the superiority of FGU in capturing common information and ensuring topical consistency.",
    "section_title": "Experiment",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 17,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 149,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0592_s4_c6",
    "source_id": "0592",
    "text": "Qualitative results show that TOMS generates descriptions with a clear theme, highlighting topic words and capturing different perspectives of the same scene. In contrast, NIC generates diverse descriptions that may not align with the observed topic. Topic exploration using LDA on COCO reveals diverse topics, each describing the same scene from different viewpoints.",
    "section_title": "Experiment",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 18,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 368,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0592_s4_c7",
    "source_id": "0592",
    "text": "This work's contributions are reflected in the robustness, effectiveness, and interpretability of the TOMS model, which arranges multi-sentence generation using topics. Experimental evaluations confirm that topic-oriented multi-sentence captions capture image details better than single-sentence captions. The research is supported by various funding sources, including NSFC and NSSFC.",
    "section_title": "Experiment",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 19,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 385,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0592_s4_c8",
    "source_id": "0592",
    "text": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft COCO: Common Objects in Context. 2014.\n\nLin. ROUGE: A Package for Automatic Evaluation of Summaries. 2010.\n\nFeng Liu, Tao Xiang, Timothy M Hospedales, Wankou Yang, and Changyin Sun. Semantic Regularisation for Recurrent Image Annotation. CVPR, 2017.",
    "section_title": "Experiment",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 20,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 383,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0592_s4_c9",
    "source_id": "0592",
    "text": "Junhua Mao, Huang Jonathan, Alexander Toshev, Oana Camburu, Alan Yuille, and Kevin Murphy. Generation and Comprehension of Unambiguous Object Descriptions. CVPR, 2016.\n\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: a Method for Automatic Evaluation of Machine Translation. In ACL, 2002.\n\nRamakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. CIDER: Consensus-based Image Description Evaluation. 2014.",
    "section_title": "Experiment",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 21,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 427,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0592_s4_c10",
    "source_id": "0592",
    "text": "Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and Tell: A Neural Image Caption Generator. In CVPR, 2015.\n\nLiwei Wang, Alexander Schwing, and Svetlana Lazebnik. Diverse and Accurate Image Description using a Variational Auto-Encoder with an Additive Gaussian Encoding Space. In NIPS, 2017.",
    "section_title": "Experiment",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 22,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 312,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0592_s4_c11",
    "source_id": "0592",
    "text": "Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. In International Conference on Machine Learning, 2015.\n\nQuanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and Jiebo Luo. Image Captioning with Semantic Attention. CVPR, 2016.",
    "section_title": "Experiment",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 23,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 370,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0592_s4_c12",
    "source_id": "0592",
    "text": "Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From Image Descriptions to Visual Denotations: New Similarity Metrics for Semantic Inference over Event Descriptions. Transactions of the Association for Computational Linguistics, 2014.\n\nHaonan Yu, Jiang Wang, Zhiheng Huang, Yi Yang, and Wei Xu. Video Paragraph Captioning using Hierarchical Recurrent Neural Networks. In CVPR, 2016.",
    "section_title": "Experiment",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 24,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 395,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8925,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0628_s0_c0",
    "source_id": "0628",
    "text": "Multi-scale Two-way Deep Neural Network for Stock Trend Prediction\n\nGuang Liu, Yuzhao Mao, Qi Sun, Hailong Huang, Weiguo Gao, Xuan Li, JianPing Shen, Ruifan Li, and Xiaojie Wang",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 177,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0628_s1_c0",
    "source_id": "0628",
    "text": "Stock Trend Prediction (STP) is crucial for investors and has garnered significant attention in Artificial Intelligence. Most studies focus on single-scale analysis, neglecting the multi-scale perspective essential for intelligent investment decisions. We propose a Multi-scale Two-way Deep Neural Network (MTDNN) that learns from wavelet-based and downsampling-based scale information using eXtreme",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 1,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 2,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 399,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9900000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0628_s1_c1",
    "source_id": "0628",
    "text": "ownsampling-based scale information using eXtreme Gradient Boosting and Recurrent Convolutional Neural Network. MTDNN achieves state-of-the-art performance on FI-2010 and CSI-2016, a long-range stock dataset we published for STP research.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 2,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 2,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 238,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9450000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0628_s2_c0",
    "source_id": "0628",
    "text": "STP automatically predicts stock price movement and is challenging due to the non-stationary and chaotic nature of stock data. Previous studies smooth data at a single scale, ignoring the multi-scale behavior. We argue that multi-scale information is critical for capturing stock price behavior and propose MTDNN to explore two types of scale-information for STP.\n\n2 Related Works",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 3,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 17,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 380,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0628_s2_c1",
    "source_id": "0628",
    "text": "2 Related Works\n\n2.1 Multi-scale for Time Series\nMany studies extract multi-scale patterns from time-series for precise description. Financial time-series have been extensively investigated, with methods using multi-scale information outperforming single-scale methods.\n\n2.2 Stock Trend Prediction\nSTP is a classification task traditionally tackled by Support Vector Machine and Neural Networks. Ensemble-based methods like Random Forest and deep learning models have also been explored. However, there is a lack of benchmark datasets and a focus on single-scale models.\n\n3 Task Formulation",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 4,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 590,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0628_s2_c2",
    "source_id": "0628",
    "text": "3 Task Formulation\n\nSTP predicts stock price trends using stock data. A stock event time-series of length T is denoted as x = {xt}T, where xt is a stock event at time t with d dimensions. The dataset D consists of paired data {(xn, yn)}N, where yn represents the trend category. The trend direction is judged by a threshold α and the percentage change ∆pT of the future mid-price.\n\n---\n\n∆pT = mT (k) − pT",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 5,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 404,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0628_s2_c3",
    "source_id": "0628",
    "text": "---\n\n∆pT = mT (k) − pT\n\nwhere mT (k) = 1/k Σ pT+i, and k is the prediction horizon. STP involves constructing a nonlinear function f(xn; θ) to map input stock data xn to a category yn. The objective is to learn parameters θ that best fit the function to map xn to the correct yn.\n\n4 Model\n\n4.1 Overview\nThe MTDNN architecture is a two-way end-to-end model, consisting of a wavelet-based way and a downsampling-based way. These convey discriminative information, with multi-scale information being key to enhancing stock trend prediction.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 6,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 537,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0628_s2_c4",
    "source_id": "0628",
    "text": "4.2 Wavelet-based Way\nThis approach processes stock data from a signal processing perspective to explore multi-scale behavior. The data is treated as a non-stationary discrete signal and decomposed using DWT to obtain transformed multi-scale components. These are concatenated and input to an XGBoost model to ensemble multi-scale information and output category scores.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 7,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 370,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0628_s2_c5",
    "source_id": "0628",
    "text": "4.3 Downsampling-based Way\nHere, a novel strategy uses a RCNN structure to temporally cascade a sequence of increasing multi-scale information. Stock data is downscaled to multi-scale formations and processed by CNNs to extract spatial features. A key operation concatenates these features, and a GRU unit temporally cascades the information to output categories.\n\n4.4 Output and Objective\nA network with two fully connected layers fuses category scores from both ways to output the category prediction results.\n\n---\n\n---",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 8,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 521,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0628_s2_c6",
    "source_id": "0628",
    "text": "---\n\n---\n\nIn our model, the output score is denoted by ˆy, with the output layer represented by flogit(·). The loss function used is cross-entropy, which measures the discrepancy between the predicted classification distribution ˆyn and the actual distribution yn:\n\nJ = −Σ ynlog(ˆyn)\n\nOur experiments are conducted on two datasets: FI-2010 and CSI-2016. The statistics of these datasets are as follows:\n\nFI-2010:\n- Train: 32.03% of 352,300 samples\n- Test: 31.18% of 31,837 samples\n\nCSI-2016:\n- Train: 38.34% of 143,262 samples\n- Test: 25.99% of 30,000 samples",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 9,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 559,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0628_s2_c7",
    "source_id": "0628",
    "text": "CSI-2016:\n- Train: 38.34% of 143,262 samples\n- Test: 25.99% of 30,000 samples\n\nFI-2010 is the first publicly available benchmark dataset of high-frequency Limit Order Book (LOB) data, while CSI-2016 is a dataset we collected from three one-minute stock index data.\n\nFor FI-2010, the experimental settings are:\n- Label threshold α = 0.002\n- Prediction horizon k = 50\n- Input window size T = 100\n- Features used: First 40 z-score normalized dimensions",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 10,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 449,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0628_s2_c8",
    "source_id": "0628",
    "text": "For CSI-2016, the experimental settings are:\n- Label threshold α = 0.01\n- Prediction horizon k = 5\n- Input window size T = 100\n- Feature dimension d = 6\n- Features normalized by z-score\n\nWe evaluate our model's performance using classical methods and advanced models. The results on FI-2010 show that our two-way model achieves state-of-the-art (SOTA) performance with 81.05% F1 score and 81.12% accuracy. On CSI-2016, our MTDNN model achieves the highest accuracy of 63.07% and F1 score of 61.65%.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 11,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 498,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0628_s2_c9",
    "source_id": "0628",
    "text": "The ablation study further investigates the multi-scale behavior in stock data, with variations of our model tested in both single- and multi-scale environments. The results demonstrate the effectiveness of different feature extractors and model structures.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 12,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 257,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0628_s2_c10",
    "source_id": "0628",
    "text": "In this section, we analyze the performance of single-scale and multi-scale variations in the STP task. Single-scale rows demonstrate the capability of each model, with XGBoost using raw data, CNN employing convolutional feature extraction, RNN considering temporal information, and RCNN capturing temporal information from CNN receptive fields. RCNN shows the best performance on most indices, while XGBoost is less effective. We observe that CNN features significantly improve RNN predictions, and raw data is challenging to predict trends.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 13,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 542,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0628_s2_c11",
    "source_id": "0628",
    "text": "In the multi-scale rows, variations are fed with DWT-based, downsampling-based, and CNN multi-kernel size scale-information. XGBoost shows a significant performance increase with DWT features, while the other variations yield mixed results. We attribute this to the similarity between DWT and CNN operations, making it difficult for CNN to extract additional information from DWT-processed features, and the temporal structure breakdown affecting RNN performance.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 14,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 463,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0628_s2_c12",
    "source_id": "0628",
    "text": "Comparing RCNN with different multi-scale approaches, our key operation demonstrates superiority in utilizing multi-scale information. We find that downsampling can weaken nonstationarity in raw data, potentially aiding CNN in feature extraction.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 15,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 246,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0628_s2_c13",
    "source_id": "0628",
    "text": "Our conclusion is that the MTDNN model effectively utilizes multi-scale information in stock data, achieving state-of-the-art performance on the FI-2010 dataset. We also provide the CSI-2016 dataset for further study. The results highlight the value of multi-scale information and the superiority of our model in utilizing it. Future work will involve incorporating an attention mechanism to dynamically select the most relevant scale of information.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 16,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 450,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0628_s2_c14",
    "source_id": "0628",
    "text": "Li et al. (2016) conducted an empirical analysis on stock market prediction using extreme learning machine. Lin et al. (2017) proposed hybrid neural networks for learning trends in time series. Luo and Yu (2019) introduced recurrent highway networks with grouped auxiliary memory. Ntakaris et al. (2018) provided a benchmark dataset for mid-price forecasting of limit order book data using machine learning methods. Papadimitriou and Yu (2006) discussed optimal multi-scale patterns in time series streams. Patel et al. (2015) predicted stock movement using trend deterministic data preparation and",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 17,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0628_s2_c15",
    "source_id": "0628",
    "text": "(2015) predicted stock movement using trend deterministic data preparation and machine learning techniques. Tran et al. (2018) presented a temporal attention-augmented bilinear network for financial time series data analysis. Tsai and Hsiao (2010) combined multiple feature selection methods for stock prediction. Tsantekidis et al. (2017a, 2017b, 2018) explored various deep learning approaches for stock price forecasting and change detection in financial markets. Zhang et al. (2019) introduced DeepLOB, a deep convolutional neural network for limit order books.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 18,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 565,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "0628_s2_c16",
    "source_id": "0628",
    "text": "This research was presented at the Twenty-Ninth International Joint Conference on Artificial Intelligence (IJCAI-20) Special Track on AI in FinTech.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 19,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 148,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "10.5560_zna.2012_0029_s0_c0",
    "source_id": "10.5560_zna.2012_0029",
    "text": "An improved quantum secure direct communication protocol based on a four-particle Green–Horne–Zeilinger (GHZ) state is presented to enhance the efficiency of eavesdropping detection. The protocol utilizes the four-particle GHZ state for eavesdropper detection and quantum dense coding for message encoding. Entropy theory is employed in the security analysis, quantitatively comparing two detection strategies by examining the relationship between the information obtainable by eavesdroppers and the introduced interference. The detection rate of the proposed protocol is 87%, compared to 50% for the",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 19,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 600,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "10.5560_zna.2012_0029_s0_c1",
    "source_id": "10.5560_zna.2012_0029",
    "text": "nce. The detection rate of the proposed protocol is 87%, compared to 50% for the quantum secure direct communication using an Einstein–Podolsky–Rosen (EPR) pair block. The security analysis demonstrates that the proposed protocol offers higher security than others.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 265,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "10.5560_zna.2012_0029_s0_c2",
    "source_id": "10.5560_zna.2012_0029",
    "text": "In the field of cryptography, the goal is to ensure that only legitimate users, such as Alice and Bob, have access to secret messages. Quantum cryptography, particularly quantum key distribution (QKD), has been shown to be effective in addressing key distribution challenges. Quantum secure direct communication (QSDC) protocols have been proposed, with various detection strategies available to guarantee the security of shared quantum states. A more effective detection strategy can enhance the security of quantum communication protocols. The improved protocol, referred to as SDPP, increases the",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "10.5560_zna.2012_0029_s0_c3",
    "source_id": "10.5560_zna.2012_0029",
    "text": "mmunication protocols. The improved protocol, referred to as SDPP, increases the efficiency of eavesdropping detection compared to the previous TSPP protocol. The analysis results indicate that the proposed protocol is more secure.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 231,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "10.5560_zna.2012_0029_s0_c4",
    "source_id": "10.5560_zna.2012_0029",
    "text": "The TSPP protocol is reviewed, where an EPR pair can be one of the four Bell states, and each Bell state can carry two bits of classical information. Alice and Bob encode the states |ψ−⟩, |ψ+⟩, |φ−⟩, and |φ+⟩ as 00, 01, 10, and 11, respectively.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 245,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "10.5560_zna.2012_0029_s0_c5",
    "source_id": "10.5560_zna.2012_0029",
    "text": "Alice extracts particles in the Bell states to form sequences A1 (travel qubits) for message transmission and A2 (home qubits). She sends A1 to Bob, and they perform a check for eavesdropping by measuring photons in A1 and A2. If no eavesdropping is detected, Alice encodes her message on A1 using dense coding with unitary operations U0, U1, U2, and U3, then broadcasts A1. Bob performs Bell-basis measurements to decode the message.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 5,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 434,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "10.5560_zna.2012_0029_s0_c6",
    "source_id": "10.5560_zna.2012_0029",
    "text": "The SDPP protocol involves Bob preparing Bell states and GHZ states, creating a sequence D by inserting decoy photons. Alice extracts and measures these to detect eavesdropping. If secure, Alice encodes her message on particles E using unitary operations and broadcasts the encoded sequence E'. Bob decodes the message with Bell measurements.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 6,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 342,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "10.5560_zna.2012_0029_s0_c7",
    "source_id": "10.5560_zna.2012_0029",
    "text": "The security analysis of the SDPP protocol considers the efficiency of eavesdropping detection compared to TSPP, with an expression for the information Eve can eavesdrop and the probability of her detection. Eve's attack operation ˆE results in states |ϕ'0⟩ and |ϕ'1⟩, with probabilities p and q, and the system's state is described by ρ'. After unitary operations, the state ρ'' is calculated for eavesdropping detection analysis.\n\nWith the orthogonal basis {|0,χ0⟩, |1,χ1⟩, |0,χ1⟩, |1,χ0⟩}, the state ρ′′ can be rewritten as",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 7,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 526,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "10.5560_zna.2012_0029_s0_c8",
    "source_id": "10.5560_zna.2012_0029",
    "text": "(p0 + p3)|α|2 (p0 − p3)αβ∗ 0 0\n(p0 − p3)α∗β (p0 + p3)|β|2 0 0\n0 0 (p1 + p2)|α|2 (p1 − p2)αβ∗\n0 0 (p1 − p2)α∗β (p1 + p2)|β|2\n\nwhere p0 + p1 + p2 + p3 = 1. The information I0 that Eve can obtain is given by the Von Neumann entropy,\n\nI0 = −3∑i=0 λi logλi,\n\nwith eigenvalues λi (i = 0,1,2,3) as\n\nλ0,1 = 1/2(p0 + p3) [ (p0 + p3)2 − 16p0p3 |α|2 |β|2 ]−1/2,\nλ2,3 = 1/2(p1 + p2) [ (p1 + p2)2 − 16p1p2 |α|2 |β|2 ]−1/2.\n\nThe state of the composed system after Eve's attack is\n\n|ψ⟩Eve = ...\n\nThe probability without an eavesdropper is\n\np(|ψ⟩) = 1/2 [ |α4|2 + |β4|2 + |m4|2 + |n4|2 ].",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 8,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 572,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "10.5560_zna.2012_0029_s0_c9",
    "source_id": "10.5560_zna.2012_0029",
    "text": "p(|ψ⟩) = 1/2 [ |α4|2 + |β4|2 + |m4|2 + |n4|2 ].\n\nThe lower bound of the detection probability is\n\ndlFG = 1 − p(|ψ⟩).\n\nThe analysis of the maximum information Eve can gain without a control mode yields\n\ndSD = 1 − p(|ψ⟩) / [ α4 2 + β4 2 + m4 2 + n4 2 ].\n\nIn the case p0 = p1 = p2 = p3 = 1/4, the expressions simplify to\n\nλ0,1 = λ2,3 = 1/s [ a−1 2 ].\n\nThe maximal amount of information equals the Shannon entropy of a binary channel,\n\nI0 = H(a), I1 = H(t).\n\nThe maximum information that Eve can obtain is\n\nI = 1/2 [ H(a) + H(t) ].\n\nThe detection probability dSD is given by",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 9,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 570,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "10.5560_zna.2012_0029_s0_c10",
    "source_id": "10.5560_zna.2012_0029",
    "text": "I = 1/2 [ H(a) + H(t) ].\n\nThe detection probability dSD is given by\n\ndSD = −2a4 + 4a3 − 6a2 + 4a.\n\nThe maximum I is\n\nI(dSD) = H [ −3 + √(16 − 8dSD) ].\n\nThe analysis shows that the function I(dTS) and I(dSD) have similar algebraic properties. If Eve gains full information (I = 2), the probability of eavesdropping detection is dTS(I = 2) = 0.5 in the TSPP protocol, while in the SDPP protocol, dTS(I = 2) = 0.87.\n\nThe probability for Eve to successfully eavesdrop I = nI(d) bits is\n\ns(I,c,d) = [ 1−c / (1−c(1−d)) ] I/I(d).",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 10,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 522,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "10.5560_zna.2012_0029_s0_c11",
    "source_id": "10.5560_zna.2012_0029",
    "text": "s(I,c,d) = [ 1−c / (1−c(1−d)) ] I/I(d).\n\nIn the limit when I → ∞, s → 0, so the protocol is asymptotically secure. If the security of the quantum channel is ensured, the protocol is completely secure.\n\nThe SDPP protocol is secure and has the following differences compared to the TSPP protocol:\n\n(i) Similar eavesdropping detection method using the four-particle GHZ state.\n(ii) Bell states are prepared by Bob, ensuring no information leak.\n(iii) Based on the four-particle GHZ state, reducing detection times.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 11,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 511,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "10.5560_zna.2012_0029_s0_c12",
    "source_id": "10.5560_zna.2012_0029",
    "text": "An improved QSDC scheme based on the four-particle GHZ state has been introduced, with two eavesdropping detection strategies compared quantitatively. The SDPP protocol shows higher efficiency in eavesdropping detection, ensuring greater security at the cost of sending more particles. Further work includes researching other QSDC protocols' security and improvements.\n\nThis work is supported by the National Natural Science Foundation of China (Grant No. 61100205).",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 12,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 466,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "10.5560_zna.2012_0029_s0_c13",
    "source_id": "10.5560_zna.2012_0029",
    "text": "References:\n[1] G. S. Vernam, J. Am. Inst. Electr. Eng. 55, 109 (1926).\n[2] C. E. Shannon, Bell Syst. Tech. J. 28, 656 (1949).\n[3] C. H. Bennett and G. Brassard, in: Proc. IEEE Int. Conf. Computers, Systems and Signal Processing, Bangalore 1984, p. 175–179.\n[4] A. K. Ekert, Phys. Rev. Lett. 67, 661 (1991).\n[5] C. H. Bennett, Phys. Rev. Lett. 68, 3121 (1992).\n[6] N. Gisin, G. Ribordy, W. Tittel, and H. Zbinden, Rev. Mod. Phys. 74, 145 (2002).\n[7] K. Boström and T. Felbinger, Phys. Rev. Lett. 89, 187902 (2002).\n[8] G. L. Long and X. S. Liu, Phys. Rev. A 65, 032302 (2002).",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 13,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 576,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "10.5560_zna.2012_0029_s0_c14",
    "source_id": "10.5560_zna.2012_0029",
    "text": "[8] G. L. Long and X. S. Liu, Phys. Rev. A 65, 032302 (2002).\n[9] Q. Y. Cai and B. W. Li, Chin. Phys. Lett. 21, 601 (2004).\n[10] F. G. Deng, G. L. Long, and G. L. Long, Phys. Rev. A 69, 052319 (2004).\n[11] M. Lucamarini and S. Mancini, Phys. Rev. Lett. 94, 140501 (2005).\n[12] F. G. Deng, G. L. Long, and X. S. Liu, Phys. Rev. A 68, 042317 (2003).\n[13] Q. Y. Cai and B. W. Li, Phys. Rev. A 69, 054301 (2004).\n[14] T. Gao, F. L. Yan, and Z. X. Wang, Chin. Phys. Lett. 22, 2473 (2005).\n[15] C. Wang, F. G. Deng, Y. S. Li, X. S. Liu, and G. L. Long, Phys. Rev. A 71, 044305 (2005).",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 14,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 578,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "10.5560_zna.2012_0029_s0_c15",
    "source_id": "10.5560_zna.2012_0029",
    "text": "[16] C. Wang, F. G. Deng, and G. L. Long, Opt. Commun. 253, 15 (2005).\n[17] X. H. Li, F. G. Deng, and H. Y. Zhou, Phys. Rev. A 74, 054302 (2006).\n[18] X. H. Li, C. Y. Li, F. G. Deng, P. Zhou, Y. J. Liang, and H. Y. Zhou, Chin. Phys. Lett. 16, 2149 (2007).\n[19] B. A. Nguyen, Phys. Lett. A 328, 6 (2004).\n[20] Z. X. Man, Z. J. Zhang, and Y. Li, Chin. Phys. Lett. 22, 22 (2005).\n[21] X. Ji and S. Zhang, Chin. Phys. Lett. 15, 1418 (2006).\n[22] Z. X. Man, Y. J. Xia, and B. A. Nguyen, J. Phys. B, At. Mol. Opt. Phys. 39, 3855 (2006).\n[23] Z. X. Man and Y. J. Xia, Chin. Phys. Lett. 23, 1680 (2006).",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 15,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 595,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "10.5560_zna.2012_0029_s0_c16",
    "source_id": "10.5560_zna.2012_0029",
    "text": "[23] Z. X. Man and Y. J. Xia, Chin. Phys. Lett. 23, 1680 (2006).\n[24] Y. Xia, C. B. Fu, S. Zhang, S. K. Hong, K. H. Yeon, and C. I. Um, J. Korean Phys. Soc. 48, 24 (2006).\n[25] X. R. Jin, X. Ji, Y. Q. Zhang, S. Zhang, S.-K. Hong, K.-H. Yeon, and C.-I. Um, Phys. Lett. A 354, 67 (2006).\n[26] Z. X. Man and Y. J. Xia, Chin. Phys. Lett. 24, 15 (2007).\n[27] Y. Chen, Z. X. Man, and Y. J. Xia, Chin. Phys. Lett. 24, 19 (2007).\n[28] Y. G. Yang, and Q. Y. Wen, Sci. China Ser. G, Phys. Mech. Astron. 50, 558 (2007).\n[29] A. Wojcik, Phys. Rev. Lett. 90, 157901 (2003).",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 16,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 560,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "10.5560_zna.2012_0029_s0_c17",
    "source_id": "10.5560_zna.2012_0029",
    "text": "[29] A. Wojcik, Phys. Rev. Lett. 90, 157901 (2003).\n[30] F. G. Deng, X. H. Li, C. Y. Li, P. Zhou, and H. Y. Zhou, Chin. Phys. Lett. 16, 277 (2007).\n[31] Q. Y. Cai, Phys. Rev. Lett. 91, 109801 (2003).\n[32] Z. J. Zhang and Z. X. Man, Int. J. Quantum Inf. 2, 521 (2004).\n[33] H. Hoffmann, K. Boström, and T. Felbinger, Phys. Rev. A 72, 016301 (2005).\n[34] F. G. Deng and G. L. Long, Phys. Rev. A 72, 016302 (2005).\n[35] F. G. Deng, X. H. Li, C. Y. Li, P. Zhou, and H. Y. Zhou, Phys. Lett. A 359, 359 (2006).\n[36] A. Beige, B. G. Englert, C. Kurtsiefer, and H. Weinfurter, Acta Phys. A 101, 357 (2002).",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 17,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "10.5560_zna.2012_0029_s0_c18",
    "source_id": "10.5560_zna.2012_0029",
    "text": "[37] M. Hillery, V. Buzek, and A. Berthiaume, Phys. Rev. A 59, 1829 (1999).\n[38] R. Cleve, D. Gottesman, and H. K. Lo, Phys. Rev. Lett. 83, 648 (1999).\n[39] C. H. Bennett, G. Brassard, and N. D. Mermin, Phys. Rev. Lett. 68, 557 (1992).\n[40] F. Gao, F. Z. Guo, Q. Y. Wen, and F. C. Zhu, Phys. Lett. A 349, 53 (2006).\n[41] C. H Bennet and S. J. Wiesner, Phys. Rev. Lett. 69, 2881 (1992).\n[42] C. Y. Li, H. Y. Zhou, Y. Wang, and F. G. Deng, Chin. Phys. Lett. 22, 1049 (2005).\n[43] C. Y. Li, X. H. Li, F. G. Deng, P. Zhou, Y. J. Liang, and H. Y. Zhou, Chin. Phys. Lett. 23, 2896 (2006).",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 18,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 582,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1307.0414v1_s0_c0",
    "source_id": "1307.0414v1",
    "text": "Challenges in Representation Learning: A report on three machine learning contests\n\nIan J. Goodfellow et al.\n\nAbstract. The ICML 2013 Workshop on Challenges in Representation Learning focused on three challenges: the black box learning challenge, the facial expression recognition challenge, and the multimodal learning challenge. We describe the datasets created for these challenges and summarize the competition results. We offer suggestions for future challenges and insights from machine learning competitions.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 515,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1307.0414v1_s1_c0",
    "source_id": "1307.0414v1",
    "text": "This paper presents three machine learning contests from the ICML workshop “Challenges in Representation Learning.” The workshop, organized by Ian Goodfellow, Dumitru Erhan, and Yoshua Bengio, aimed to advance representation learning by testing current algorithms and fostering new developments through these contests. Ben Hamner and Will Cukierski managed Kaggle hosting, ensuring smooth contest operations. Google provided prizes for the winners. We summarize the solutions and discuss learnings from the diverse group of participants.\n\n2 The black box learning challenge",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 1,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 16,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 573,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1307.0414v1_s1_c1",
    "source_id": "1307.0414v1",
    "text": "2 The black box learning challenge\n\nThe black box learning challenge had two objectives: to obfuscate data to limit human-in-the-loop techniques and to test algorithms on limited labeled data with additional unsupervised data. The BBL-2013 dataset, created by Dumitru Erhan, was an obfuscated subset of the Street View House Numbers dataset. With only 1,000 labeled examples for training, 5,000 for the public leaderboard, and 130,000 unlabeled examples, the contest highlighted semi-supervised learning.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 2,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 504,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1307.0414v1_s1_c2",
    "source_id": "1307.0414v1",
    "text": "218 teams submitted 1963 entries, with 75 beating the baseline. David Thaler won with an accuracy of 70.22% using a blend of models. Other top performers used methods such as sparse filtering, entropy regularization, and ensemble voting techniques with denoising autoencoders and maxout networks.\n\n3 The facial expression recognition challenge\n---",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 3,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 347,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1307.0414v1_s1_c3",
    "source_id": "1307.0414v1",
    "text": "In the facial expression recognition challenge, we invited participants to create systems for recognizing emotions expressed in facial images. The contest utilized the Facial Expression Recognition 2013 (FER-2013) dataset, compiled by Pierre Luc Carrier and Aaron Courville. This dataset, searchable via the Google image search API, consists of nearly 36,000 images categorized into seven emotions. Human performance on a similar dataset was estimated at 68±5%, while the best \"null\" model, a convolutional network with no feature learning, achieved an accuracy of 60%. The top-performing teams used",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 4,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1307.0414v1_s1_c4",
    "source_id": "1307.0414v1",
    "text": "no feature learning, achieved an accuracy of 60%. The top-performing teams used convolutional neural networks, with the winner employing an SVM primal objective as the loss function. The contest also included a multimodal learning challenge, intended to encourage algorithms that integrate image and text modalities. However, the matching task proved too easy, resulting in perfect accuracy scores for the top three teams. Recommendations for future contests include enhancing the difficulty of the classification task and providing more test images to prevent the reliance on matching algorithms.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 5,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 597,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1307.0414v1_s1_c5",
    "source_id": "1307.0414v1",
    "text": "---\n\nOrganizing a contest involves considerable effort from all parties. Here are some suggestions for a successful contest:\n\n**Allocation of Time:**\n- Before the contest: Create datasets, verify state-of-the-art algorithms, prepare baseline solutions, and design contest rules.\n- During the contest: Address questions, resolve baseline portability issues.\n- After the contest: Verify winners' submissions, distribute private test data, prepare presentations and papers.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 6,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 470,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1307.0414v1_s1_c6",
    "source_id": "1307.0414v1",
    "text": "**Designing Rules:**\n- Consider allowing \"transductive\" methods, prohibiting labeling of public leaderboard data, training with external data, and web scraping.\n- Enforce rules by requiring participants to upload trained models before the test set release.\n- Verify winning submissions' predictions with the uploaded models to deter cheating.\n\n**Difficulty and Participation Rate:**\n- Make the contest challenging enough to be interesting.\n- Hosting on platforms like Kaggle can increase participation.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 7,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 502,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1307.0414v1_s1_c7",
    "source_id": "1307.0414v1",
    "text": "**Organize Multiple Contests:**\n- Running additional contests has a low marginal cost and ensures interesting results.\n\n**Provide Baselines and a Leaderboard:**\n- Baselines encourage participation by reducing the need for boilerplate code.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 8,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 239,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1307.0414v1_s1_c8",
    "source_id": "1307.0414v1",
    "text": "**Discussion and Conclusion:**\n- Contests offer a different perspective on machine learning algorithms than research papers.\n- Practitioners use various methods to win, regardless of novelty or inventorship.\n- Contests provide a realistic evaluation of generalization error.\n- This year's contest highlighted SVM loss functions, sparse filtering, and entropy regularization.\n- Future contest organizers can use these insights to plan effective contests.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 9,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 453,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1307.0414v1_s1_c9",
    "source_id": "1307.0414v1",
    "text": "**Bibliography:**\n- [1] Bengio, Courville, and Vincent. Unsupervised feature learning and deep learning: A review and new perspectives. 2012.\n- [2] Guyon et al. Unsupervised and transfer learning challenge. 2011.\n- [3] Netzer et al. Reading digits in natural images with unsupervised feature learning. 2011.\n- [4] Ngiam et al. Sparse filtering. 2011.\n- [5] Breiman. Random forests. 2001.\n\n---\n\n---\n\n[6] Cortes, C., & Vapnik, V. (1995). Support vector networks. Machine Learning, 20(3), 273–297.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 10,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 494,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1307.0414v1_s1_c10",
    "source_id": "1307.0414v1",
    "text": "[7] Romaszko, L. (2013). A deep learning approach with an ensemble-based neural network classifier for black box icml 2013 contest. Workshop on Challenges in Representation Learning, ICML.\n\n[8] Lee, D.-H. (2013). Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. Workshop on Challenges in Representation Learning, ICML.\n\n[9] Grandvalet, Y., & Bengio, Y. (2005). Semi-supervised Learning by Entropy Minimization. In NIPS’04 (pp. XX-XX). Cambridge, MA: MIT Press.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 11,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 508,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1307.0414v1_s1_c11",
    "source_id": "1307.0414v1",
    "text": "[10] Vincent, P., Larochelle, H., Bengio, Y., & Manzagol, P.-A. (2008). Extracting and composing robust features with denoising autoencoders. In ICML 2008.\n\n[11] Goodfellow, I. J., Warde-Farley, D., Mirza, M., Courville, A., & Bengio, Y. (2013). Maxout networks. ICML. URL: http://icml.cc/2013/\n\n[12] Susskind, J., Anderson, A., & Hinton, G. E. (2010). The Toronto face dataset. Technical Report UTML TR 2010-001, U. Toronto.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 12,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 425,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1307.0414v1_s1_c12",
    "source_id": "1307.0414v1",
    "text": "[13] Bergstra, J., & Cox, D. D. (2013). Hyperparameter optimization and boosting for classifying facial expressions: How good can a “null” model be? Workshop on Challenges in Representation Learning, ICML.\n\n[14] Fukushima, K. (1980). Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological Cybernetics, 36(4), 193-202.\n\n[15] Lowe, D. (1999). Object recognition from local scale invariant features. ICCV’99.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 13,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 486,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1307.0414v1_s1_c13",
    "source_id": "1307.0414v1",
    "text": "[16] Tang, Y. (2013). Deep learning using linear support vector machines. Workshop on Challenges in Representation Learning, ICML.\n\n[17] Ionescu, R. T., Popescu, M., & Grozea, C. (2013). Local learning to improve bag of visual words model for facial expression recognition. Workshop on Challenges in Representation Learning, ICML.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 14,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 330,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1307.0414v1_s1_c14",
    "source_id": "1307.0414v1",
    "text": "[18] von Ahn, L., & Dabbish, L. (2004). Labeling images with a computer game. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’04) (pp. 319–326). New York, NY, USA: ACM. ISBN: 1-58113-702-8. doi: 10.1145/985692.985733. URL: http://doi.acm.org/10.1145/985692.985733\n\n[19] Feng, F., Li, R., & Wang, X. (2013). Constructing hierarchical image-tags bimodal representations for word tags alternative choice. Workshop on Challenges in Representation Learning, ICML.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 15,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 494,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1307.0414v1_s1_c15",
    "source_id": "1307.0414v1",
    "text": "[20] Le, Q. V., Ranzato, M., Salakhutdinov, R., Ng, A., & Tenenbaum, J. (2011). NIPS Workshop on Challenges in Learning Hierarchical Models: Transfer Learning and Optimization. URL: https://sites.google.com/site/nips2011workshop\n\n[21] Goodfellow, I., Courville, A., & Bengio, Y. (2012). Large-scale feature learning with spike-and-slab sparse coding. ICML. URL: http://icml.cc/discuss/2012/590.html\n\n---",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 16,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 403,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1307.1275v1_s0_c0",
    "source_id": "1307.1275v1",
    "text": "---\n\nConstructing Hierarchical Image-tags Bimodal Representations for Word Tags Alternative Choice\n\nFangxiang Feng, Ruifan Li, Xiaojie Wang\nEngineering Research Center of Information Networks, Ministry of Education, School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, 100876 China",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 317,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1307.1275v1_s1_c0",
    "source_id": "1307.1275v1",
    "text": "This paper presents a solution to the multi-modal learning challenge of ICML, involving the construction of three-level representations in three stages and a data-specific strategy for choosing correct tag words. Level-1 representations are obtained using MPEG-7, gist descriptors, and additional features provided by the contest organizers for images, and a bag-of-words model with a 4000-word dicti",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 1,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 2,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 400,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9900000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1307.1275v1_s1_c1",
    "source_id": "1307.1275v1",
    "text": "s, and a bag-of-words model with a 4000-word dictionary for word tags. Level-2 representations are learned using stacked RBMs for each modality, while a bimodal auto-encoder is proposed for level-3 representations. Our approach achieves a final average accuracy of 100% on the private test set.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 2,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 2,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 294,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9450000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1307.1275v1_s2_c0",
    "source_id": "1307.1275v1",
    "text": "The multi-modal learning challenge of ICML 2013 focuses on developing a predictive system for word tags using bimodal data: images and texts. The data consists of the Small ESP Game Dataset and a manually labeled dataset by Ian Goodfellow, referred to as ESP and GF, respectively. Our approach models similar representations between image and tag word pairs.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 3,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 16,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 358,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1307.1275v1_s2_c1",
    "source_id": "1307.1275v1",
    "text": "2. System Architecture\nOur solution aims to construct hierarchical representations of bimodal data. The training phase involves three stages: obtaining low-level representations, distilling level-1 representations using RBMs, and learning level-3 similar representations with a quasi-Siamese auto-encoder. The test phase uses a data-specific strategy to choose the correct tag words based on similarity/dissimilarity.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 4,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 417,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1307.1275v1_s2_c2",
    "source_id": "1307.1275v1",
    "text": "3. Obtaining Level 1 Representations\nFor image representation, we adopt features from the contest organizer, MPEG-7, and gist descriptors. The organizer's features are reduced from 816 to 408 dimensions. MPEG-7 descriptors include Color Layout, Color Structure, Edge Histogram, and Scalable Color, resulting in a 784-dimensional feature set. Gist descriptors capture the scene's dominant spatial structure, adding 512 dimensions. Each image is represented as a 1704-dimensional vector.\n\n---\n\n---",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 5,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 495,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1307.1275v1_s2_c3",
    "source_id": "1307.1275v1",
    "text": "---\n\n---\n\nFor tags representation, we employ a bag-of-words model. We construct a dictionary of 4000 high-frequency words from all the tag words of ESP. Each word in an image tag is represented as a multinomial variable using a 1-of-4000 coding scheme. Thus, each tag is represented as a vector with 4000 binary elements, indicating the presence or absence of the tag word in the dictionary.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 6,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 391,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1307.1275v1_s2_c4",
    "source_id": "1307.1275v1",
    "text": "In the second stage, we use Restricted Boltzmann Machines (RBM) to build level-2 representations. Given that the level-1 representations of images and tag words have distinct properties, we utilize Gaussian-Bernoulli RBM for real-valued image data and Replicated Softmax for the count data of tag words. The learning process can be efficiently performed using the Contrastive Divergence approximation.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 7,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 401,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1307.1275v1_s2_c5",
    "source_id": "1307.1275v1",
    "text": "For level-3 representations, we propose a quasi-Siamese auto-encoder for bimodal representations. This architecture is based on the Siamese network, originally designed for signature verification. The quasi-Siamese network consists of two sub-networks with the same architecture but different parameters, connected by a compatibility measure. The loss function is defined to learn similar representations for the two modalities, and the learning process is carried out using the back-propagation algorithm.\n\n---",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 8,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 511,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1307.1275v1_s2_c6",
    "source_id": "1307.1275v1",
    "text": "---\n\n*Note: All unnecessary formatting, page numbers, and repetitive information have been removed to ensure clarity and focus on the core academic content.*",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 9,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 157,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1307.1275v1_s2_c7",
    "source_id": "1307.1275v1",
    "text": "By obtaining hierarchical three-level representations, the model is ready to choose alternatives. We employ two strategies: a general strategy and a data-specific strategy. The general strategy involves calculating the compatibility LC(pi, qi) between an image pi and its tag word qi, then comparing it with the compatibility LC(pi, eqi) of the image with another tag word eqi. The tag word with higher compatibility is chosen as the correct tag for the image. The data-specific strategy considers the characteristics of the data, particularly the loops among tag words of some images, which can be",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 10,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1307.1275v1_s2_c8",
    "source_id": "1307.1275v1",
    "text": "f the data, particularly the loops among tag words of some images, which can be resolved by finding the image with the maximum discrepancy of compatibility between its two tag words.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 11,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 182,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1307.1275v1_s2_c9",
    "source_id": "1307.1275v1",
    "text": "In our experiments, we used the datasets ESP and GF. For the ESP dataset, we generated an incorrect counterpart for each image's tag words by randomly choosing from the correct tag words of the remaining images. We extracted level-1, level-2, and level-3 representations successively. For learning level-2 representations, we constructed two stacked RBMs with neuron configurations for images and tag words. For level-3, we used a quasi-Siamese auto-encoder with specific neuron configurations and parameters α = 0.5 and λ = 0.2. We encouraged sparsity in the representations at all layers.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 12,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 590,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1307.1275v1_s2_c10",
    "source_id": "1307.1275v1",
    "text": "We used both strategies for computation of AUC. The probability P(pi) of dissimilarity between an image and one of its tag words was expressed as the ratio of the squares of the compatibility scores. The general strategy achieved an AUC of 0.87533, while the data-specific strategy achieved 100%. The public and private leaderboard scores confirmed the effectiveness of our approach.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 13,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 383,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1307.1275v1_s2_c11",
    "source_id": "1307.1275v1",
    "text": "Our results suggest that the strategy for choosing alternatives is crucial and that moderate representations are sufficient for accurate choices. We constructed a hierarchically bimodal representation and data-specific strategy for word tag alternative choice. The three-stage extraction process includes typical methods, RBMs, and a quasi-Siamese auto-encoder. Our approach finds the maximum discrepancy among a link of images based on data characteristics.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 14,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 458,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1307.1275v1_s2_c12",
    "source_id": "1307.1275v1",
    "text": "Acknowledgments:\nWe thank the organizers and Nitish Srivastava for his DeepNet library. Part of this work was supported by the National Sciences Foundation of China and the Fundamental Research Funds for the Central Universities.\n\nSalakhutdinov, R., and Hinton, G. (2007). Learning a non-linear embedding by preserving class neighbourhood structure. In Proceedings of the 11th International Conference on Artificial Intelligence and Statistics, San Juan, PR, pp. 412–419.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 15,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 471,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1307.1275v1_s2_c13",
    "source_id": "1307.1275v1",
    "text": "Salakhutdinov, R., and Hinton, G. (2009). Replicated softmax: an undirected topic model. In Advances in Neural Information Processing Systems 22, Vancouver, Morgan Kaufmann, pp. 1607–1614.\n\nSmolensky, P. (1986). Information processing in dynamical systems: foundations of harmony theory. In Parallel distributed processing: explorations in the microstructure of cognition, vol. 1, MIT Press, Cambridge, MA, USA, pp. 194–281.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 16,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 424,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1307.1275v1_s2_c14",
    "source_id": "1307.1275v1",
    "text": "Srivastava, N., and Salakhutdinov, R. (2012). Multimodal learning with deep Boltzmann machines. In Advances in Neural Information Processing Systems 25, Lake Tahoe, NV, Morgan Kaufmann, pp. 2231–2239.\n\nvon Ahn, L., and Dabbish, L. (2004). Labeling images with a computer game. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, Vienna, Austria, ACM, pp. 319–326.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 17,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 390,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1307.1275v1_s2_c15",
    "source_id": "1307.1275v1",
    "text": "Welling, M., Rosen-Zvi, M., and Hinton, G. (2004). Exponential family harmoniums with an application to information retrieval. In Advances in Neural Information Processing Systems 17, Vancouver, Morgan Kaufmann, pp. 501–508.\n\nLeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., and Huang, F.-J. (2006). A Tutorial on Energy-Based Learning. In Predicting structured data, MIT Press, pp. 1–59.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 18,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 388,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s0_c0",
    "source_id": "1911.09359v1",
    "text": "Multi-Scale RCNN Model for Financial Time-series Classification\n\nLiu Guang · Wang Xiaojie · Li Ruifan",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 3,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 101,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s0_c1",
    "source_id": "1911.09359v1",
    "text": "Abstract: Financial time-series classification (FTC) is crucial for investment management and has received significant attention from various research areas, particularly Artificial Intelligence. Existing studies often focus on the Multi-Scale (MS) property or Temporal Dependency (TD) within financial time-series but rarely combine them effectively. We introduce a Multi-Scale Temporal Dependent Recurrent Convolutional Neural Network (MSTD-RCNN) to address this issue. Our method extracts MS features using convolutional units and captures TD and scale complementarity with a Recurrent Neural Net",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s0_c2",
    "source_id": "1911.09359v1",
    "text": "onal units and captures TD and scale complementarity with a Recurrent Neural Network. The MSTD-RCNN demonstrates state-of-the-art performance in trend classification and simulated trading on Chinese stock market datasets.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 221,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c0",
    "source_id": "1911.09359v1",
    "text": "Financial time-series classification (FTC) is vital for investors and has attracted interest from a broad range of research fields, including Artificial Intelligence. The Effective Market Hypothesis suggests that all market information impacts security prices, leading to extensive research on the use of historical financial data. Due to the large volume of financial data, automated processing technologies are essential. Existing FTC research can be categorized into MS-oriented and TD-oriented methods. MS-oriented methods extract features across multiple scales, while TD-oriented methods focus",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 3,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 42,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c1",
    "source_id": "1911.09359v1",
    "text": "ethods extract features across multiple scales, while TD-oriented methods focus on the temporal dynamics. However, few studies effectively integrate both properties. We propose the MSTD-RCNN to address this gap, which learns parameters end-to-end and contributes the following:",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 4,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 42,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 277,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c2",
    "source_id": "1911.09359v1",
    "text": "- A novel method combining both MS and TD properties in financial time-series.\n- MS feature extraction with convolutional units without predefined parameters.\n- Fusion of different scale features using a Recurrent Neural Network to capture temporal dependencies.\n\nOur model is evaluated on three minute-level index price datasets from the Chinese stock market, showing superior performance compared to classical and state-of-the-art models in classification and simulated trading.\n\n2 Related works\n\n2.1 Financial Time-series Prediction",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 5,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 42,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 535,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c3",
    "source_id": "1911.09359v1",
    "text": "Financial time-series prediction is crucial for effective trading strategies in the financial market. It has received significant attention from researchers, particularly from the Artificial Intelligence community. Research primarily focuses on specific markets such as the stock market, foreign exchange market, and futures market. The challenges arise from the irregular and noisy nature of financial data. Existing research can be categorized into regression and classification approaches. Regression approaches aim to predict future financial time-series values, while classification approaches",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 6,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 42,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c4",
    "source_id": "1911.09359v1",
    "text": "to predict future financial time-series values, while classification approaches focus on financial time-series classification (FTC) and often achieve higher profits.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 7,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 42,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 165,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c5",
    "source_id": "1911.09359v1",
    "text": "The Multi-Scale (MS) property of time-series has been widely studied, akin to its use in Computer Vision tasks. Time-series, like images, possess MS-property, providing detailed information for similarity analysis. This paper concentrates on predicting financial time-series movement direction using the MS-property.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 8,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 42,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 316,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c6",
    "source_id": "1911.09359v1",
    "text": "Temporal Dependency (TD) in financial time-series has been explored in previous research. Methods can be classified into feature-oriented, model-oriented, and integrated approaches. Feature-oriented methods extract effective features, while model-oriented methods focus on improving model fitting. Integrated methods combine various techniques for classification. Deep learning models have shown effectiveness in FTC, inspired by their success in Computer Vision and Natural Language Processing.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 9,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 42,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 495,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c7",
    "source_id": "1911.09359v1",
    "text": "This paper defines the financial time-series classification problem and introduces the proposed MSTD-RCNN model. The model addresses the challenges of incorporating MS and TD features for accurate classification. The architecture of MSTD-RCNN consists of three layers: transform, feature, and fusion, each serving specific functions in processing financial time-series data.\n\n---",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 10,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 42,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 379,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c8",
    "source_id": "1911.09359v1",
    "text": "1 The transform layer converts the input sequence into multi-scale (MS) sequences using downsampling transformations in the time domain. 2 The feature layer employs different convolutional units to extract features from each scale independently. The resulting feature maps are padded to the same length and concatenated. 3 The fusion layer processes the concatenated feature maps with a Gated Recurrent Unit (GRU), followed by fully connected layers and a softmax layer to generate the final output. Our MSTD-RCNN model is an end-to-end system, with all parameters trained jointly via backpropagatio\nis an end-to-end system, with all parameters trained jointly via backpropagation.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 11,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 42,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 681,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c9",
    "source_id": "1911.09359v1",
    "text": "3.2.1 Transform Layer\nThis layer creates multiple sequences with different scales from a single-scale input sequence. Downsampling generates financial data sketches at various scales, potentially enhancing prediction quality. Given an input sequence x = {x1, x2, ..., xT} and a downsampling rate d, the new sequence xd = {xd, x2d, ..., xmd} is formed, with m = T/d. This process generates multiple sequences with different downsampling rates, denoted as X = {x1, x2, ..., xd}.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 12,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 42,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 476,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c10",
    "source_id": "1911.09359v1",
    "text": "3.2.2 Feature Layer\nThis layer processes MS sequences and outputs concatenated features. It includes convolutional units and a concatenation operation. 1D CNNs are used to extract feature maps from sequences of varying scales, sharing filter size and number. The convolution operation captures features with different receptive fields. Feature maps are padded and concatenated into a feature matrix E.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 13,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 42,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 401,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c11",
    "source_id": "1911.09359v1",
    "text": "3.2.3 Fusion Layer\nThe fusion layer integrates multi-scale features for prediction. It uses a GRU to handle the temporal dependency and variety in the feature maps. The GRU adaptively captures dependencies of different time scales and is applied to the feature matrix. The GRU output passes through fully connected layers and a softmax activation function to produce a probability distribution over classes.\n\n---\n\n4 Experimental Settings\n4.1 Datasets",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 14,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 42,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 450,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c12",
    "source_id": "1911.09359v1",
    "text": "We describe the datasets from the Chinese stock market: SH000001, SZ399005, and SZ399006. Data spans from January 1, 2016, to December 30, 2016, with a total of 58,000 data points. The dataset is divided into training, verification, and testing sets. Categorical values are defined based on price changes relative to a threshold δ. The threshold is selected to equally distribute categories in the development set: 0.3 for SH000001, 0.2 for SZ399006, and 0.8 for SZ399005. The window size T = 30 is chosen based on Random Forest performance. Pearson Correlation Coefficient (PCC) analysis shows no s",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 15,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 42,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c13",
    "source_id": "1911.09359v1",
    "text": "om Forest performance. Pearson Correlation Coefficient (PCC) analysis shows no strong correlations between datasets.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 16,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 42,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 116,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c14",
    "source_id": "1911.09359v1",
    "text": "4.2 Baselines\nSix baseline models are used: Support Vector Machine (SVM), Random Forest (RF), Fuzzy Deep Neural Network (FDNN), TreNet, State-Frequency Memory Recurrent Neural Networks (SFM), and Multi-Scale CNN (MS-CNN). Model parameters are selected based on validation set performance, with a maximum epoch of 100, trained using the Adam optimization algorithm with a learning rate of 0.0005, and a batch size of 32.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 17,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 42,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 419,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c15",
    "source_id": "1911.09359v1",
    "text": "4.3 Evaluation Metrics\nEvaluation metrics include accuracy, F-score (F1), Confusion Matrix (CM), and accumulated profit. Accuracy and F1 are calculated based on the Confusion Matrix. The accumulated profit measures profitability.\n\nThe recall (R) and precision (P) are calculated as follows:\n\nP = TP / (TP + FP), (17)\n\nR = TP / (TP + FN). (18)",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 18,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 42,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 342,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c16",
    "source_id": "1911.09359v1",
    "text": "P = TP / (TP + FP), (17)\n\nR = TP / (TP + FN). (18)\n\nThe simulated trading algorithm is based on the predicted result c't, the real trend ct, and the index change value ∆xt. We execute a buy-in or sell-out for each trading signal. For correct predictions in the upward and downward categories, we make a profit; otherwise, we incur losses. For the still category, ∆xt = 0. The transaction cost is set to zero, and the accumulated profit P. is calculated by\n\nP. = Σ (I(c't, ct) × ∆xt) . (19)",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 19,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 42,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 489,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c17",
    "source_id": "1911.09359v1",
    "text": "P. = Σ (I(c't, ct) × ∆xt) . (19)\n\nHere, P. represents the profit from the change points, and I(c't, ct) is an indicator function that equals 1 when c't = ct, otherwise 0.\n\n5 Results and Analysis\n\nThe model's performance is compared with baseline models on three datasets. The effects of the feature layer in extracting Multi-Scale (MS) features and the fusion layer in capturing Temporal Dependent (TD) are analyzed. The profitability of the models is evaluated through simulated trading, and the improvement in profitability is analyzed using the confusion matrix.\n\n5.1 Comprehensive evaluation",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 20,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 42,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 595,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c18",
    "source_id": "1911.09359v1",
    "text": "5.1 Comprehensive evaluation\n\nOur MSTD-RCNN model is compared with six baseline models on three datasets. The results, listed in Table 5, show that our model achieves the best performance in accuracy and F1. It improves by 3.07%, 3.00%, and 2.13% on SH000001, SZ399005, and SZ399006, respectively. The t-test results in Table 6 confirm the significance of these improvements. Our model effectively extracts MS features and captures Temporal Dependency (TD) within financial time-series.\n\n5.2 Effects of multi-scale features",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 21,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 42,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 523,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c19",
    "source_id": "1911.09359v1",
    "text": "5.2 Effects of multi-scale features\n\nMSTD-RCNN is evaluated under different scale settings to illustrate its use of MS property. The model's performance increases with the number of scales, as shown in Table 7, indicating the complementary effects of MS features.\n\n5.3 Effects of temporal dependency\n\nThe classification performance of MS-CNN and MSTD-RCNN under different scale settings is compared to show the effects of TD. MSTD-RCNN's fusion layer, using a GRU, is more efficient in capturing TD than MS-CNN's fully connected layers.\n\n5.4 Simulated trading\n---",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 22,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 42,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 563,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c20",
    "source_id": "1911.09359v1",
    "text": "The goal of financial time-series classification is to generate profit. We evaluate the profitability of models using a simulated trading algorithm based on their predictions on testing sets. Table 8 presents the simulated trading results on three datasets, comparing the profitability of models with the baseline strategy, Buy & Hold (B&H). MSTD-RCNN achieves the highest profit on all three datasets, significantly outperforming the most profitable baseline model. Despite market downturns that lead to B&H losses, all models manage to make a profit. The results indicate that our model is not onl",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 23,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 42,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c21",
    "source_id": "1911.09359v1",
    "text": "l models manage to make a profit. The results indicate that our model is not only more accurate in classification but also more profitable than baseline models. We analyze the confusion matrix of our model to understand the source of improved profitability.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 24,
    "chunk_index_in_section": 21,
    "total_chunks_in_section": 42,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 257,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c22",
    "source_id": "1911.09359v1",
    "text": "The confusion matrix analysis of MSTD-RCNN and MS-CNN on three datasets reveals that MSTD-RCNN has fewer errors in classifying \"upward\" to \"downward\" and \"downward\" to \"upward\" categories. MSTD-RCNN also exhibits higher precision in classifying \"upward\" and \"downward\" categories, contributing to its higher profitability in simulated trading.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 25,
    "chunk_index_in_section": 22,
    "total_chunks_in_section": 42,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 343,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c23",
    "source_id": "1911.09359v1",
    "text": "In conclusion, this paper introduces MSTD-RCNN, a Multi-Scale Recurrent Convolutional Neural Network for financial time-series classification. MSTD-RCNN effectively combines Multi-Scale and Temporal Dependency features, resulting in a powerful end-to-end classifier. The profitability of our model is confirmed through a simulated trading algorithm, with extensive experimental results demonstrating state-of-the-art performance. Future work will explore different feature extractors, attention mechanisms, and multi-source information integration to further enhance MSTD-RCNN.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 26,
    "chunk_index_in_section": 23,
    "total_chunks_in_section": 42,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 577,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c24",
    "source_id": "1911.09359v1",
    "text": "This research was funded by the National Social Science Fund of China and the Discipline Building Plan in 111 Base. We acknowledge NVIDIA's GPU donations and appreciate the feedback from editors and reviewers.\n\n---",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 27,
    "chunk_index_in_section": 24,
    "total_chunks_in_section": 42,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 214,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c25",
    "source_id": "1911.09359v1",
    "text": "8. Cho et al.: Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078 (2014)\n9. Choudhry and Garg: A hybrid machine learning system for stock market forecasting. World Academy of Science, Engineering and Technology 39(3), 315–318 (2008)\n10. Cui et al.: Multi-scale convolutional neural networks for time series classification. arXiv preprint arXiv:1603.06995 (2016)\n11. Dacorogna et al.: Changing time scale for short-term forecasting in financial markets. Journal of Forecasting 15(3), 203–227 (1996)",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 28,
    "chunk_index_in_section": 25,
    "total_chunks_in_section": 42,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 572,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c26",
    "source_id": "1911.09359v1",
    "text": "12. Das et al.: A hybridized ELM-Jaya forecasting model for currency exchange prediction. Journal of King Saud University-Computer and Information Sciences (2017)\n13. De Fortuny et al.: Evaluating and understanding text-based stock price prediction models. Information Processing & Management 50(2), 426–441 (2014)\n14. Deng et al.: A hierarchical fused fuzzy deep neural network for data classification. IEEE Transactions on Fuzzy Systems 25(4), 1006–1012 (2017)",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 29,
    "chunk_index_in_section": 26,
    "total_chunks_in_section": 42,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 462,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c27",
    "source_id": "1911.09359v1",
    "text": "15. Devlin et al.: Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018)\n16. Eigen and Fergus: Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. In: Proceedings of the IEEE International Conference on Computer Vision, pp. 2650–2658 (2015)\n17. Fernández et al.: A meta extreme learning machine method for forecasting financial time series. Applied Intelligence 49(2), 532–554 (2019)",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 30,
    "chunk_index_in_section": 27,
    "total_chunks_in_section": 42,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 506,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c28",
    "source_id": "1911.09359v1",
    "text": "18. Frankel and Froot: Chartists, fundamentalists, and trading in the foreign exchange market. The American Economic Review 80(2), 181–185 (1990)\n19. Geva: Scalenet-multiscale neural-network architecture for time series prediction. IEEE Transactions on neural networks 9(6), 1471–1482 (1998)\n20. Guan et al.: A novel stock forecasting model based on high-order-fuzzy-fluctuation trends and back propagation neural network. PloS one 13(2), e0192366 (2018)\n21. Hochreiter and Schmidhuber: Long short-term memory. Neural computation 9(8), 1735–1780 (1997)",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 31,
    "chunk_index_in_section": 28,
    "total_chunks_in_section": 42,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 552,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c29",
    "source_id": "1911.09359v1",
    "text": "22. Howard et al.: MobileNets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861 (2017)\n23. Hsieh et al.: Forecasting stock markets using wavelet transforms and recurrent neural networks: An integrated system based on artificial bee colony algorithm. Applied soft computing 11(2), 2510–2525 (2011)\n24. Hu and Qi: State-frequency memory recurrent neural networks. In: International Conference on Machine Learning, pp. 1568–1577 (2017)",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 32,
    "chunk_index_in_section": 29,
    "total_chunks_in_section": 42,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 487,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c30",
    "source_id": "1911.09359v1",
    "text": "25. Huang et al.: Application of wrapper approach and composite classifier to the stock trend prediction. Expert Systems with Applications 34(4), 2870–2878 (2008)\n26. Kara et al.: Predicting direction of stock price index movement using artificial neural networks and support vector machines: The sample of the Istanbul Stock Exchange. Expert systems with Applications 38(5), 5311–5319 (2011)\n27. Kim: Financial time series forecasting using support vector machines. Neurocomputing 55(1-2), 307–319 (2003)",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 33,
    "chunk_index_in_section": 30,
    "total_chunks_in_section": 42,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 505,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c31",
    "source_id": "1911.09359v1",
    "text": "28. Kim and Han: Genetic algorithms approach to feature discretization in artificial neural networks for the prediction of stock price index. Expert systems with Applications 19(2), 125–132 (2000)\n29. Kim: Convolutional neural networks for sentence classification. In: Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1746–1751 (2014)\n30. Kim et al.: An intelligent hybrid trading system for discovering trading rules for the futures market using rough sets and genetic algorithms. Applied Soft Computing 55, 127–140 (2017)",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 34,
    "chunk_index_in_section": 31,
    "total_chunks_in_section": 42,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 574,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c32",
    "source_id": "1911.09359v1",
    "text": "31. Krizhevsky et al.: Imagenet classification with deep convolutional neural networks. In: Advances in neural information processing systems, pp. 1097–1105 (2012)\n32. Lee and Ready: Inferring trade direction from intraday data. The Journal of Finance 46(2), 733–746 (1991)\n33. Lee: Using support vector machine with a hybrid feature selection method to the stock trend prediction. Expert Systems with Applications 36(8), 10896–10904 (2009)",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 35,
    "chunk_index_in_section": 32,
    "total_chunks_in_section": 42,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 440,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c33",
    "source_id": "1911.09359v1",
    "text": "34. Leung et al.: Forecasting stock indices: a comparison of classification and level estimation models. International Journal of Forecasting 16(2), 173–190 (2000)\n35. Li et al.: Empirical analysis: stock market prediction via extreme learning machine. Neural Computing and Applications 27(1), 67–78 (2016)\n36. Lin et al.: Hybrid neural networks for learning the trend in time series. In: Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17, pp. 2273–2279 (2017)",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 36,
    "chunk_index_in_section": 33,
    "total_chunks_in_section": 42,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 510,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c34",
    "source_id": "1911.09359v1",
    "text": "37. Lin et al.: An SVM-based approach for stock market trend prediction. In: Neural Networks (IJCNN), The 2013 International Joint Conference on, pp. 1–7. IEEE (2013)\n38. Liu et al.: Foreign exchange rates forecasting with convolutional neural network. Neural Processing Letters 46(3), 1095–1119 (2017)\n39. Liu and Wang: A numerical-based attention method for stock market prediction with dual information. IEEE Access 7, 7357–7367 (2019)\n40. Malkiel and Fama: Efficient capital markets: A review of theory and empirical work. The journal of Finance 25(2), 383–417 (1970)",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 37,
    "chunk_index_in_section": 34,
    "total_chunks_in_section": 42,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 571,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c35",
    "source_id": "1911.09359v1",
    "text": "41. Mozer: A focused backpropagation algorithm for temporal pattern recognition. Complex Systems 3, 349–381 (1989)\n42. O’Connor and Madden: A neural network approach to predicting stock exchange movements using external factors. Knowledge-Based Systems 19(5), 371–378 (2006)\n43. Papadimitriou and Yu: Optimal multi-scale patterns in time series streams. In: Proceedings of the 2006 ACM",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 38,
    "chunk_index_in_section": 35,
    "total_chunks_in_section": 42,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 385,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c36",
    "source_id": "1911.09359v1",
    "text": "---\n\n---\n\n44. Patel, J., Shah, S., Thakkar, P., Kotecha, K.: Predicting stock and stock price index movement using trend deterministic data preparation and machine learning techniques. Expert Systems with Applications 42(1), 259–268 (2015)\n\n45. Peng, C.K., Hausdorff, J., Havlin, S., Mietus, J., Stanley, H., Goldberger, A.: Multiple-time scales analysis of physiological time series under neural control. Physica A: Statistical Mechanics and its Applications 249(1-4), 491–500 (1998)",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 39,
    "chunk_index_in_section": 36,
    "total_chunks_in_section": 42,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 484,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c37",
    "source_id": "1911.09359v1",
    "text": "46. Saad, E.W., Prokhorov, D.V., Wunsch, D.C.: Comparative study of stock trend prediction using time delay, recurrent and probabilistic neural networks. IEEE Transactions on neural networks 9(6), 1456–1470 (1998)\n\n47. Schumaker, R.P., Chen, H.: Textual analysis of stock market prediction using breaking financial news: The azfin text system. ACM Transactions on Information Systems (TOIS) 27(2), 12 (2009)",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 40,
    "chunk_index_in_section": 37,
    "total_chunks_in_section": 42,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 407,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c38",
    "source_id": "1911.09359v1",
    "text": "48. Shynkevich, Y., McGinnity, T., Coleman, S., Belatreche, A.: Predicting stock price movements based on different categories of news articles. In: Computational Intelligence, 2015 IEEE Symposium Series on, pp. 703–710. IEEE (2015)\n\n49. Song, Y., Lee, J.W., Lee, J.: A study on novel filtering and relationship between input-features and target-vectors in a deep learning model for stock price prediction. Applied Intelligence pp. 1–15 (2018)",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 41,
    "chunk_index_in_section": 38,
    "total_chunks_in_section": 42,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 443,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c39",
    "source_id": "1911.09359v1",
    "text": "50. Stopar, L., Skraba, P., Grobelnik, M., Mladenic, D.: Streamstory: Exploring multivariate time series on multiple scales. IEEE Transactions on Visualization and Computer Graphics (2018)\n\n51. Sutskever, I., Vinyals, O., Le, Q.V.: Sequence to sequence learning with neural networks. In: Advances in neural information processing systems, pp. 3104–3112 (2014)\n\n52. Teixeira, L.A., De Oliveira, A.L.I.: A method for automatic stock trading combining technical analysis and nearest neighbor classification. Expert systems with applications 37(10), 6885–6890 (2010)",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 42,
    "chunk_index_in_section": 39,
    "total_chunks_in_section": 42,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 562,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c40",
    "source_id": "1911.09359v1",
    "text": "53. Tsai, C.F., Hsiao, Y.C.: Combining multiple feature selection methods for stock prediction: Union, intersection, and multi-intersection approaches. Decision Support Systems 50(1), 258–269 (2010)\n\n54. Wang, Y., Choi, I.C.: Market index and stock price direction prediction using machine learning techniques: an empirical study on the Kospi and HSI. arXiv preprint arXiv:1309.7119 (2013)",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 43,
    "chunk_index_in_section": 40,
    "total_chunks_in_section": 42,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 389,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "1911.09359v1_s1_c41",
    "source_id": "1911.09359v1",
    "text": "55. Wang, Z., Yan, W., Oates, T.: Time series classification from scratch with deep neural networks: A strong baseline. In: Neural Networks (IJCNN), 2017 International Joint Conference on, pp. 1578–1585. IEEE (2017)\n\n56. Yang, J., Nguyen, M.N., San, P.P., Li, X., Krishnaswamy, S.: Deep convolutional neural networks on multichannel time series for human activity recognition. In: IJCAI, vol. 15, pp. 3995–4001 (2015)\n\n57. Zirilli, J.S.: Financial prediction using neural networks. International Thomson Computer Press (1996)\n\n---",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 44,
    "chunk_index_in_section": 41,
    "total_chunks_in_section": 42,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 530,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2021.acl_long.494_s0_c0",
    "source_id": "2021.acl_long.494",
    "text": "Dual Graph Convolutional Networks for Aspect-based Sentiment Analysis\n\nRuifan Li, Hao Chen, Fangxiang Feng, Zhanyu Ma, Xiaojie Wang, and Eduard Hovy",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 21,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 148,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2021.acl_long.494_s0_c1",
    "source_id": "2021.acl_long.494",
    "text": "Abstract:\nAspect-based sentiment analysis is a fine-grained classification task. We propose a dual graph convolutional networks (DualGCN) model that considers both syntactic structures and semantic correlations. The SynGCN module incorporates syntactic knowledge, while the SemGCN module uses a self-attention mechanism. We introduce orthogonal and differential regularizers to enhance the model's ability to capture semantic correlations. Experimental results on three datasets show the effectiveness of our DualGCN model.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 523,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2021.acl_long.494_s0_c2",
    "source_id": "2021.acl_long.494",
    "text": "1 Introduction:\nSentiment analysis is a key topic in natural language processing. Aspect-based sentiment analysis (ABSA) focuses on determining sentiment polarities of aspects within sentences. Traditional methods struggle with dependency parsing errors and the complexity of online reviews. Our DualGCN model addresses these challenges by simultaneously considering syntactic structures and semantic correlations. We design regularizers to improve the model's performance and provide experimental validation on SemEval 2014 and Twitter datasets.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 546,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2021.acl_long.494_s0_c3",
    "source_id": "2021.acl_long.494",
    "text": "Figure 1: Example sentence with dependency tree illustrating two aspects with opposite sentiment polarities.\n\nFigure 2: DualGCN architecture for ABSA task.\n\nWe propose a DualGCN model that integrates SynGCN and SemGCN through a BiAffine module. The orthogonal regularizer encourages the SemGCN to learn non-overlapping semantic attentions, while the differential regularizer ensures the SemGCN captures distinct semantic features from syntactic ones.\n\n2 Related Work: \n[Content not provided in the given fragment.]\n\n---",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 519,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2021.acl_long.494_s0_c4",
    "source_id": "2021.acl_long.494",
    "text": "Traditional sentiment analysis tasks focus on the sentence or document level, whereas Aspect-Based Sentiment Analysis (ABSA) is a more fine-grained task, centered on the entity level. Early methods, such as those by Titov and McDonald (2008) and others, relied on handcrafted features and did not effectively model the dependency between aspects and their context. Attention-based neural networks, including those proposed by Wang et al. (2016) and others, have been introduced to implicitly model the semantic relation between aspects and their context. Pre-trained language models like BERT (Devli",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2021.acl_long.494_s0_c5",
    "source_id": "2021.acl_long.494",
    "text": "between aspects and their context. Pre-trained language models like BERT (Devlin et al., 2019) have also shown remarkable performance in ABSA tasks.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 5,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 148,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2021.acl_long.494_s0_c6",
    "source_id": "2021.acl_long.494",
    "text": "Methods that explicitly leverage syntactic knowledge have been another trend. These methods help establish connections between aspects and other words in a sentence, leading to syntax-aware feature representations. Notable works include the recursive neural network by Dong et al. (2014) and the attention model with syntactic information by He et al. (2018).",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 6,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 359,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2021.acl_long.494_s0_c7",
    "source_id": "2021.acl_long.494",
    "text": "Building on this, several works have extended GCN and GAT models using syntactical dependency trees, such as those by Zhang et al. (2019) and others. These models exploit syntactic structure information to learn node representations and alleviate the problem of long-range dependency.\n\nRecent works have explored combining different types of graphs for ABSA tasks. For example, Chen et al. (2020) combined a dependency graph with a latent graph, while Zhang and Qian (2020) designed hierarchical syntactic and lexical graphs.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 7,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 525,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2021.acl_long.494_s0_c8",
    "source_id": "2021.acl_long.494",
    "text": "In this paper, we propose a GCN-based method that combines syntactic and semantic features. We use a dependency probability matrix with richer syntactic information and design orthogonal and differential regularizers to enhance the capture of semantic associations.\n\nThe Graph Convolutional Network (GCN) is an efficient CNN variant that operates directly on graphs. It can apply convolution operations on directly connected nodes and encode local information. Through message passing in multilayer GCNs, nodes learn more global information.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 8,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 541,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2021.acl_long.494_s0_c9",
    "source_id": "2021.acl_long.494",
    "text": "Our proposed DualGCN model, as shown in Figure 2, processes a sentence-aspect pair (s, a) using BiLSTM or BERT as the sentence encoder. The SynGCN module takes the syntactic encoding as input, utilizing the probability matrix of all dependency arcs from a dependency parser. This captures rich structural information compared to the discrete output of a dependency parser.\n\n---\n\n---",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 9,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 382,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2021.acl_long.494_s0_c10",
    "source_id": "2021.acl_long.494",
    "text": "---\n\n---\n\nThe dependency probability matrix is employed to mitigate dependency parsing errors, utilizing the state-of-the-art LAL-Parser (Mrini et al., 2019). The SynGCN module incorporates the syntactic encoding of an adjacency matrix Asyn ∈ Rn×n, taking the hidden state vectors H from BiLSTM as initial node representations in the syntactic graph. The syntactic graph representation Hsyn is obtained from the SynGCN module using Eq. (1). For aspect nodes, we denote their hidden representations as {hsyn a1, hsyn a2, ..., hsyn am}.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 10,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 534,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2021.acl_long.494_s0_c11",
    "source_id": "2021.acl_long.494",
    "text": "SemGCN, in contrast to SynGCN, uses an attention matrix derived from a self-attention mechanism as the adjacency matrix. Self-attention captures semantically related terms and is more flexible than syntactic structure. The attention score matrix Asem ∈ Rn×n is computed using a self-attention layer and serves as the SemGCN module's adjacency matrix.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 11,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 350,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2021.acl_long.494_s0_c12",
    "source_id": "2021.acl_long.494",
    "text": "To facilitate feature exchange between the SynGCN and SemGCN modules, a mutual BiAffine transformation is adopted. The final feature representation for the ABSA task is obtained through average pooling and concatenation operations on the aspect nodes of both modules.\n\nTwo regularizers are proposed for the SemGCN module: orthogonal and differential regularizers. The orthogonal regularizer encourages orthogonality among attention score vectors, while the differential regularizer ensures distinct information representation between the SynGCN and SemGCN modules.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 12,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 564,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2021.acl_long.494_s0_c13",
    "source_id": "2021.acl_long.494",
    "text": "The loss function is defined as the total objective function, which includes the cross-entropy loss and regularization terms. Experiments are conducted on three public datasets: Restaurant, Laptop, and Twitter. The model is implemented with pretrained Glove vectors, BiLSTM, and specified hyperparameters. Baseline methods are also considered for comparison.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 13,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 358,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2021.acl_long.494_s0_c14",
    "source_id": "2021.acl_long.494",
    "text": "We compare DualGCN with state-of-the-art baselines, including ATAE-LSTM, IAN, RAM, MGAN, TNet, ASGCN, CDT, BiGCN, kumaGCN, InterGCN, R-GAT, DGEDT, BERT, R-GAT+BERT, and DGEDT+BERT. Our DualGCN model consistently outperforms attention-based and syntax-based methods on the Restaurant, Laptop, and Twitter datasets. It effectively integrates syntactic knowledge and semantic information, fitting various review styles.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 14,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 416,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2021.acl_long.494_s0_c15",
    "source_id": "2021.acl_long.494",
    "text": "The evaluation metrics are accuracy and macro-averaged F1-score. The DualGCN model's performance is superior, especially when incorporating BERT, achieving the best results in our DualGCN+BERT variant.\n\nAblation studies reveal the importance of each module in DualGCN. SynGCN and SemGCN demonstrate the value of syntactic and semantic knowledge, respectively. Removing the BiAffine module or regularizers significantly degrades performance, confirming their contributions.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 15,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 472,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2021.acl_long.494_s0_c16",
    "source_id": "2021.acl_long.494",
    "text": "In a case study, DualGCN shows improved handling of complex and informal sentences compared to attention-based and syntax-based models. Attention visualization further illustrates the effectiveness of our regularizers in capturing semantic correlations.\n\nThe experimental results are summarized in Table 2, showcasing the performance of various models across different datasets.\n\n---\n\nTable 2: Experimental results comparison on three publicly available datasets.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 16,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 463,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2021.acl_long.494_s0_c17",
    "source_id": "2021.acl_long.494",
    "text": "Models | Restaurant | Laptop | Twitter | Accuracy | Macro-F1 | Accuracy | Macro-F1 | Accuracy | Macro-F1\n--- | --- | --- | --- | --- | --- | --- | --- | --- | ---\nSynGCN-head | 82.93 | 75.29 | 76.27 | 72.39 | 75.04 | 73.85\nSynGCN | 83.74 | 76.97 | 76.58 | 73.17 | 74.59 | 72.86\nSemGCN | 83.29 | 76.30 | 76.90 | 73.72 | 75.18 | 73.86\nDualGCN w/o BiAffine | 82.84 | 75.31 | 76.90 | 73.23 | 75.33 | 73.92\nDualGCN w/o RO&RD | 82.93 | 75.79 | 76.58 | 72.03 | 74.59 | 73.20\nDualGCN w/o RO | 83.56 | 77.43 | 76.58 | 72.78 | 75.18 | 73.55\nDualGCN w/o RD | 83.65 | 76.34 | 77.53 | 73.72 | 74.45 | 72.82",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 17,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 593,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2021.acl_long.494_s0_c18",
    "source_id": "2021.acl_long.494",
    "text": "DualGCN w/o RD | 83.65 | 76.34 | 77.53 | 73.72 | 74.45 | 72.82\nDualGCN | 84.27 | 78.08 | 78.48 | 74.74 | 75.92 | 74.29",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 18,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 118,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2021.acl_long.494_s0_c19",
    "source_id": "2021.acl_long.494",
    "text": "Table 3: Experimental results of ablation study.\n\nThe attention score matrix of DualGCN w/o RO&RD indicates redundant attention to non-essential information, failing to focus on the keyword \"quick\". In contrast, the attention score matrix produced by our DualGCN is relatively sparse, with \"safari\" and \"browser\" attending to semantically related terms, including \"quick\". The attention scores tend to be distinct and precise due to semantic constraints, allowing our DualGCN model to predict sentiment polarity accurately.\n\n5.8 Impact of the DualGCN Layer Number",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 19,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 563,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2021.acl_long.494_s0_c20",
    "source_id": "2021.acl_long.494",
    "text": "5.8 Impact of the DualGCN Layer Number\n\nEvaluating our DualGCN model with one to eight layers on the Restaurant and Laptop datasets reveals that two DualGCN layers perform the best. Insufficient layers limit node representation propagation, while too many layers lead to model instability due to vanishing gradients and information redundancy.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 20,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 343,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2021.acl_long.494_s1_c0",
    "source_id": "2021.acl_long.494",
    "text": "We propose a DualGCN architecture to overcome the limitations of attention-based and dependency-based methods for ABSA tasks. The DualGCN model integrates syntactic knowledge and semantic information through SynGCN and SemGCN modules. The orthogonal and differential regularizers in the SemGCN module effectively capture semantic correlations with less overlap and distinct feature representations. Experiments on benchmark datasets demonstrate the superiority of our DualGCN model.",
    "section_title": "Conclusion",
    "section_type": "conclusion",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 21,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 2,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 482,
    "chunk_method": "hierarchical",
    "importance_weight": 0.935,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2021.acl_long.494_s1_c1",
    "source_id": "2021.acl_long.494",
    "text": "Figure 4: The impact of the number of DualGCN layers on performance.\n\nAcknowledgments\n\nThis work was supported by various grants from the National Key R&D Program of China, the National Natural Science Foundation of China, the 111 Project, and the Fundamental Research Funds for the Central Universities.",
    "section_title": "Conclusion",
    "section_type": "conclusion",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 22,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 2,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 304,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8925,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2021.acl_long.494_s2_c0",
    "source_id": "2021.acl_long.494",
    "text": "[References listed here]\n\n---\n\nLishuang Li, Yang Liu, and AnQiao Zhou. (2018a). Hierarchical attention based position-aware network for aspect-level sentiment analysis. In Proceedings of the 22nd Conference on Computational Natural Language Learning, 181–189.\n\nXin Li, Lidong Bing, Wai Lam, and Bei Shi. (2018b). Transformation networks for target-oriented sentiment classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 946–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 23,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 13,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 503,
    "chunk_method": "hierarchical",
    "importance_weight": 0.44000000000000006,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2021.acl_long.494_s2_c1",
    "source_id": "2021.acl_long.494",
    "text": "956.\n\nBin Liang, Rongdi Yin, Lin Gui, Jiachen Du, and Ruifeng Xu. (2020). Jointly learning aspect-focused and inter-aspect relations with graph convolutional networks for aspect sentiment analysis. In Proceedings of the 28th International Conference on Computational Linguistics, 150–161.\n\nBing Liu. (2012). Sentiment analysis and opinion mining. Synthesis lectures on human language technologies, 5(1):1–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 24,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 405,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2021.acl_long.494_s2_c2",
    "source_id": "2021.acl_long.494",
    "text": "167.\n\nDehong Ma, Sujian Li, Xiaodong Zhang, and Houfeng Wang. (2017). Interactive attention networks for aspect-level sentiment classification. In Proceedings of the 26th International Joint Conference on Artificial Intelligence, 4068–4074.\n\nDiego Marcheggiani and Ivan Titov. (2017). Encoding sentences with graph convolutional networks for semantic role labeling. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, 1506–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 25,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 462,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2021.acl_long.494_s2_c3",
    "source_id": "2021.acl_long.494",
    "text": "1515.\n\nKhalil Mrini, Franck Dernoncourt, Trung Bui, Walter Chang, and Ndapa Nakashole. (2019). Rethinking self-attention: An interpretable self-attentive encoder-decoder parser. arXiv preprint arXiv:1911.03875.\n\nJeffrey Pennington, Richard Socher, and Christopher Manning. (2014). GloVe: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, 1532–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 26,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 424,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2021.acl_long.494_s2_c4",
    "source_id": "2021.acl_long.494",
    "text": "1543.\n\nMinh Hieu Phan and Philip O. Ogunbona. (2020). Modelling context and syntactical features for aspect-based sentiment analysis. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 3211–3220.\n\nMaria Pontiki, Dimitris Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. (2014). SemEval-2014 task 4: Aspect based sentiment analysis. In Proceedings of the 8th International Workshop on Semantic Evaluation, 27–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 27,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 491,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2021.acl_long.494_s2_c5",
    "source_id": "2021.acl_long.494",
    "text": "35.\n\nChi Sun, Luyao Huang, and Xipeng Qiu. (2019a). Utilizing BERT for aspect-based sentiment analysis via constructing auxiliary sentence. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 380–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 28,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 291,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2021.acl_long.494_s2_c6",
    "source_id": "2021.acl_long.494",
    "text": "385.\n\nKai Sun, Richong Zhang, Samuel Mensah, Yongyi Mao, and Xudong Liu. (2019b). Aspect-level sentiment analysis via convolution over dependency tree. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, 5679–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 29,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 322,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2021.acl_long.494_s2_c7",
    "source_id": "2021.acl_long.494",
    "text": "5688.\n\nXingwei Tan, Yi Cai, and Changxi Zhu. (2019). Recognizing conflict opinions in aspect-level sentiment classification with dual attention networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, 3426–3431.\n\nDuyu Tang, Bing Qin, and Ting Liu. (2016a). Aspect level sentiment classification with deep memory network. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, 214–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 30,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 534,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2021.acl_long.494_s2_c8",
    "source_id": "2021.acl_long.494",
    "text": "224.\n\nHao Tang, Donghong Ji, Chenliang Li, and Qiji Zhou. (2020). Dependency graph enhanced dual-transformer structure for aspect-based sentiment classification. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 6578–6588.\n\nIvan Titov and Ryan McDonald. (2008). Modeling online reviews with multi-grain topic models. In Proceedings of the 17th International Conference on World Wide Web, 111–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 31,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 434,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2021.acl_long.494_s2_c9",
    "source_id": "2021.acl_long.494",
    "text": "120.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. (2017). Attention is all you need. In Advances in Neural Information Processing Systems 30, 5998–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 32,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 228,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2021.acl_long.494_s2_c10",
    "source_id": "2021.acl_long.494",
    "text": "6008.\n\nDuy-Tin Vo and Yue Zhang. (2015). Deep learning for event-driven stock prediction. In Proceedings of IJCAI, BueNos Aires, Argentina.\n\nKai Wang, Weizhou Shen, Yunyi Yang, Xiaojun Quan, and Rui Wang. (2020). Relational graph attention network for aspect-based sentiment analysis. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 3229–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 33,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 382,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2021.acl_long.494_s2_c11",
    "source_id": "2021.acl_long.494",
    "text": "3238.\n\nYequan Wang, Minlie Huang, Xiaoyan Zhu, and Li Zhao. (2016). Attention-based LSTM for aspect-level sentiment classification. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, 606–615.\n\nHu Xu, Bing Liu, Lei Shu, and Philip Yu. (2019). BERT post-training for review reading comprehension and aspect-based sentiment analysis. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2324–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 34,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 523,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2021.acl_long.494_s2_c12",
    "source_id": "2021.acl_long.494",
    "text": "2335.\n\nChen Zhang, Qiuchi Li, and Dawei Song. (2019). Aspect-based sentiment classification with aspect-specific graph convolutional networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, 4568–4578.\n\nMi Zhang and Tieyun Qian. (2020). Convolution over hierarchical syntactic and lexical graphs for aspect level sentiment analysis. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, 3540–3549.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 35,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 551,
    "chunk_method": "hierarchical",
    "importance_weight": 0.42000000000000004,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s0_c0",
    "source_id": "2022.acl_long.212",
    "text": "Enhanced Multi-Channel Graph Convolutional Network for Aspect Sentiment Triplet Extraction\n\nHao Chen, Zepeng Zhai, Fangxiang Feng, Ruifan Li, Xiaojie Wang\nSchool of Artificial Intelligence, Beijing University of Posts and Telecommunications, China\nAbstract:",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 5,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 257,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s0_c1",
    "source_id": "2022.acl_long.212",
    "text": "Aspect Sentiment Triplet Extraction (ASTE) is a sentiment analysis task. Existing studies often focus on new tagging schemes for end-to-end extraction, neglecting word relations. We propose an Enhanced Multi-Channel Graph Convolutional Network (EMC-GCN) to exploit these relations. Our model defines ten relation types, utilizes a biaffine attention module for relation embedding, and treats words and relations as nodes and edges in a multi-channel graph. EMC-GCN incorporates linguistic features and employs a refining strategy that considers implicit aspect and opinion extraction results. Experi",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 5,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s0_c2",
    "source_id": "2022.acl_long.212",
    "text": "g strategy that considers implicit aspect and opinion extraction results. Experimental results demonstrate the effectiveness and robustness of our model.\n1 Introduction:",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 5,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 169,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s0_c3",
    "source_id": "2022.acl_long.212",
    "text": "ASTE is a new variant of Aspect-based Sentiment Analysis (ABSA), aiming to extract triplets of aspect, opinion, and sentiment. Previous approaches include pipeline methods, multi-turn machine reading comprehension, and end-to-end frameworks. However, challenges remain in utilizing word relations and linguistic features. We introduce EMC-GCN to address these challenges, enhancing node representations with relation awareness and incorporating syntactic and lexical features. An effective refining strategy for word-pair representation is also proposed. Our contributions include a novel EMC-GCN mo",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 5,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s0_c4",
    "source_id": "2022.acl_long.212",
    "text": "ir representation is also proposed. Our contributions include a novel EMC-GCN model, a comprehensive exploitation of linguistic features, and an effective refining strategy, supported by extensive experimental validation.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 5,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 221,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s1_c0",
    "source_id": "2022.acl_long.212",
    "text": "---\n\nAspect-based Sentiment Analysis (ABSA) is a fine-grained sentiment analysis task, focusing on aspects or entities. It consists of three basic subtasks: Aspect Term Extraction (ATE), Aspect Sentiment Classification (ASC), and Opinion Term Extraction (OTE). Previous studies often addressed these tasks separately, ignoring their interdependencies. Efforts have been made to couple these subtasks, proposing models for joint extraction of aspect-based pairs.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 5,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 9,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 461,
    "chunk_method": "hierarchical",
    "importance_weight": 0.66,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s1_c1",
    "source_id": "2022.acl_long.212",
    "text": "Our proposed framework is EMC-GCN, which addresses Aspect and Opinion Term Co-Extraction (AOTE) and Aspect-Sentiment Pair Extraction (ASPE). We define ten types of relations between words for the ASTE task, enhancing the model's ability to extract triplets. The relations not only identify aspect and opinion terms but also determine sentiment polarities.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 6,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 355,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s1_c2",
    "source_id": "2022.acl_long.212",
    "text": "Given a sentence X = {w1, w2, ..., wn}, our model aims to output a set of triplets T = {(a, o, s)m}. The sentiment label set S = {POS, NEU, NEG} includes positive, neutral, and negative polarities. We use a table filling method to construct a relation table for each sentence and decode triplets with an algorithm that considers the predicted relations of word pairs.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 7,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 367,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s1_c3",
    "source_id": "2022.acl_long.212",
    "text": "The EMC-GCN model consists of an Input and Encoding Layer using BERT for contextual representations and a Biaffine Attention Module to capture relation probability distributions for word pairs. This module uses multi-layer perceptrons and biaffine functions to model the relations between words, forming an adjacency tensor that represents the relations between words in the sentence.\n\n---\n3.4.3 Multi-Channel GCN",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 8,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 413,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s1_c4",
    "source_id": "2022.acl_long.212",
    "text": "Motivated by CNN, GCN is an efficient variant operating on graphs (Kipf and Welling, 2017). A graph's nodes and edges allow GCN to apply convolution operations on connected nodes, aggregating relevant information. For a sentence with n words, an adjacency matrix A ∈ Rn×n is constructed using the syntactic dependency tree (Zhang et al., 2019; Sun et al., 2019). The element Aij represents the edge between nodes wi and wj. Soft edges can be constructed by attention mechanisms (Guo et al., 2019; Chen et al., 2020a; Li et al., 2021). Our EMC-GCN extends the vanilla GCN with a multi-channel adjacen",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 9,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s1_c5",
    "source_id": "2022.acl_long.212",
    "text": "et al., 2021). Our EMC-GCN extends the vanilla GCN with a multi-channel adjacency tensor Rba ∈ Rn×n×m, using the biaffine attention module. Each channel models a word relation, and a GCN aggregates information along each channel for each node:",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 10,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 243,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s1_c6",
    "source_id": "2022.acl_long.212",
    "text": "eHba k = σ(Rba(:,:,k)HWk + bk) (6)\n\nˆHba = f(eHba 1, eHba 2, ..., eHba m) (7)\n\n3.4.4 Linguistic Features\nTo enhance EMC-GCN, we introduce four types of linguistic features for each word pair: part-of-speech combination, syntactic dependency type, tree-based distance, and relative position distance. We initialize four adjacency tensors based on these features: Rpsc, Rdep, Rtbd, and Rrpd. The graph convolution operation is repeated using these tensors to obtain node representations ˆHpsc, ˆHdep, ˆHtbd, and ˆHrpd. These are combined with average pooling and concatenation:",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 11,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 575,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s1_c7",
    "source_id": "2022.acl_long.212",
    "text": "H = f(ˆHba, ˆHpsc, ˆHdep, ˆHtbd, ˆHrpd) (8)\n\nR = Rba ⊕ Rpsc ⊕ Rdep ⊕ Rtbd ⊕ Rrpd (9)\n\n3.4.5 Relation Constraint\nWe impose a constraint on the adjacency tensor obtained from the biaffine module to capture word relations precisely. The constraint costs are Lba, Lpsc, Ldep, Ltbd, and Lrpd.\n\n3.4.6 Refining Strategy and Prediction Layer\nFor label prediction, we concatenate node representations hi, hj and edge representation rij. We refine this using a strategy that considers the implicit results of aspect and opinion extraction. The refined representation sij is:",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 12,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 564,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s1_c8",
    "source_id": "2022.acl_long.212",
    "text": "sij = hi ⊕ hj ⊕ rij ⊕ rii ⊕ rjj (11)\n\nThe word pair representation sij is fed into a linear layer followed by a softmax function to produce a label probability distribution pij.\n\n3.5 Loss Function\nOur objective function is:\n\nL = Lp + αLba + β(Lpsc + Ldep + Ltbd + Lrpd) (13)\n\nwhere Lp is the standard cross-entropy loss.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 13,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 320,
    "chunk_method": "hierarchical",
    "importance_weight": 0.63,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s2_c0",
    "source_id": "2022.acl_long.212",
    "text": "4.1 Datasets\nWe evaluate our method on two ABSA datasets from the SemEval ABSA Challenges (Pontiki et al., 2014, 2015, 2016).\n\n4.2 Baselines\nWe compare EMC-GCN with state-of-the-art baselines, grouped into pipeline methods, end-to-end methods, and MRC-based methods.\n\n4.3 Implementation Details\nWe use BERT-base-uncased as the sentence encoder and train EMC-GCN with AdamW optimizer. Hyperparameters and training details are specified.\n\n---\n\n---",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 14,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 32,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 445,
    "chunk_method": "hierarchical",
    "importance_weight": 0.935,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s2_c1",
    "source_id": "2022.acl_long.212",
    "text": "---\n\nModel | 14res | 14lap | 15res | 16res\n--- | --- | --- | --- | ---\nCMLA+♮ | 39.18 | 47.13 | 42.79 | 33.16\nRINANTE+♮ | 31.42 | 34.95 | 29.88 | 23.87\nLi-unified-R♮ | 41.04 | 51.00 | 44.72 | 44.31\nPeng-two-stage♮ | 43.24 | 51.46 | 48.07 | 52.32\nOTE-MTL† | 62.00 | 58.71 | 56.37 | 43.42\nJET-BERT♮ | 70.56 | 62.40 | 64.45 | 57.53\nGTS-BERT† | 68.09 | 68.81 | 59.28 | 55.42\nBMRC† | 75.61 | 67.99 | 68.51 | 57.82\nBART-ABSA† | 65.52 | 65.25 | 59.14 | 59.26\nOur EMC-GCN | 71.21 | 71.78 | 61.54 | 68.33",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 15,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 32,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 495,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s2_c2",
    "source_id": "2022.acl_long.212",
    "text": "Table 4: Experimental results on D2 (Xu et al., 2020). The “♮” denotes results from Xu et al. (2020). The “†” indicates reproduced models using released code with original parameters.\n\nThe reported results are the average of five runs with different random seeds.\n\n4.4 Main Results",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 16,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 32,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 281,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s2_c3",
    "source_id": "2022.acl_long.212",
    "text": "4.4 Main Results\n\nUnder the F1 metric, our EMC-GCN model outperforms all pipeline, end-to-end, and MRC-based methods on two groups of datasets. End-to-end and MRC-based methods show more significant improvements than pipeline methods by jointly training multiple subtasks. Our EMC-GCN significantly surpasses GTS-BERT with an average of 1.96% and 2.61% F1-score on D1 and D2, respectively, due to leveraging word relations and linguistic knowledge.\n\n4.5 Model Analysis\n\n4.5.1 Ablation Study",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 17,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 32,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 490,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s2_c4",
    "source_id": "2022.acl_long.212",
    "text": "4.5 Model Analysis\n\n4.5.1 Ablation Study\n\nTable 5 presents the F1 scores of the ablation study on D2, showing the effectiveness of different modules in EMC-GCN.\n\nModel | 14res | 14lap | 15res | 16res\n--- | --- | --- | --- | ---\nEMC-GCN | 71.78 | 58.81 | 61.93 | 68.33\nw/o Ten Relations | 70.68 | 57.71 | 59.85 | 66.48\nw/o Linguistic Features | 71.22 | 58.38 | 60.62 | 67.15\nw/o Relation Constraint | 70.59 | 57.28 | 59.83 | 67.89\nw/o Refining Strategy | 70.62 | 56.72 | 60.23 | 67.31",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 18,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 32,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 483,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s2_c5",
    "source_id": "2022.acl_long.212",
    "text": "Table 5: F1 scores of ablation study on D2.\n\n4.5.2 Effect of Refining Strategy\n\nTable 6 shows the F1 scores of three sentiment relations on 14rest and 14lap of D2, verifying the effectiveness of the refining strategy.\n\nModel | POS | NEU | NEG\n--- | --- | --- | ---\nEMC-GCN | 74.69 | 62.43 | 67.74\nw/o Refining Strategy | 74.98 | 59.87 | 67.31\n\nTable 6: F1 scores of three sentiment relations on D2.\n\n4.5.3 Channel Visualization",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 19,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 32,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 427,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s2_c6",
    "source_id": "2022.acl_long.212",
    "text": "4.5.3 Channel Visualization\n\nVisualization of the adjacency tensor Rba's POS and NEG relation channels demonstrates the relation between words.\n\n4.5.4 Linguistic Feature Visualization\n\nVisualizations of adjacency tensors for four linguistic features show their contributions to the ASTE task.\n\n4.5.5 Case Study\n\n---\n\nFigure and table references have been retained for context, assuming they are correctly referenced within the full paper.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 20,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 32,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 438,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s2_c7",
    "source_id": "2022.acl_long.212",
    "text": "A case study is presented in Figure 7, where aspect terms are highlighted in blue and opinion terms in yellow. The red line signifies a match between an aspect and opinion term, forming a triplet with a positive sentiment. The term \"light\" is challenging to identify by GTS-BERT and BMRC, yet \"easy\" is predicted correctly by all methods due to its closer proximity to \"transport\" than \"light\". Consequently, the triplet (\"transport\", \"light\", positive) is ignored by these methods, while our EMC-GC",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 21,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 32,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 499,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s2_c8",
    "source_id": "2022.acl_long.212",
    "text": "ht\", positive) is ignored by these methods, while our EMC-GCN can accurately extract it. We attribute this to the significant connections established between \"light\" and \"transport\" through sentiment relations and linguistic features.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 22,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 32,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 234,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s2_c9",
    "source_id": "2022.acl_long.212",
    "text": "In this paper, we introduce the EMC-GCN architecture for the ASTE task. We design a multi-channel graph structure to model various relation types between word pairs and employ graph convolution operations across all channels to learn relation-aware node representations. Moreover, we incorporate linguistic features to enhance the GCN-based model and develop an effective refining strategy for improved triplet extraction. Our EMC-GCN model consistently outperforms all baseline methods on benchmark",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 23,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 32,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 499,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s2_c10",
    "source_id": "2022.acl_long.212",
    "text": "l consistently outperforms all baseline methods on benchmark datasets. Future work will involve analyzing the roles of linguistic features and their combinations.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 24,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 32,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 162,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s2_c11",
    "source_id": "2022.acl_long.212",
    "text": "This work was supported by the National Key R&D Program of China, the National Natural Science Foundation of China, the 111 Project, and the Fundamental Research Funds for the Central Universities.\n\nReferences:\n[References listed here]\n\nLyu, Chenyang, Jennifer Foster, and Yvette Graham. 2020. Improving document-level sentiment analysis with user and product context. In Proceedings of the 28th International Conference on Computational Linguistics, 6724–6729.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 25,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 32,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 461,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s2_c12",
    "source_id": "2022.acl_long.212",
    "text": "Ma, Dehong, Sujian Li, and Houfeng Wang. 2018. Joint learning for targeted sentiment analysis. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 4737–4742.\n\nMa, Dehong, Sujian Li, Fangzhao Wu, Xing Xie, and Houfeng Wang. 2019. Exploring sequence-to-sequence learning in aspect term extraction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 3538–3547.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 26,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 32,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 437,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s2_c13",
    "source_id": "2022.acl_long.212",
    "text": "Ma, Dehong, Sujian Li, Xiaodong Zhang, and Houfeng Wang. 2017. Interactive attention networks for aspect-level sentiment classification. In IJCAI’17, 4068–4074.\n\nMao, Yue, Yi Shen, Chao Yu, and Longjun Cai. 2021. A joint training dual-mrc framework for aspect based sentiment analysis. Proceedings of the AAAI Conference on Artificial Intelligence, 35(15):13543–13551.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 27,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 32,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 368,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s2_c14",
    "source_id": "2022.acl_long.212",
    "text": "Miwa, Makoto, and Yutaka Sasaki. 2014. Modeling joint entity and relation extraction with table representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 1858–1869.\n\nPeng, Haiyun, Lu Xu, Lidong Bing, Fei Huang, Wei Lu, and Luo Si. 2020. Knowing what, how and why: A near complete solution for aspect-based sentiment analysis. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):8600–8607.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 28,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 32,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 463,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s2_c15",
    "source_id": "2022.acl_long.212",
    "text": "Phan, Minh Hieu, and Philip O. Ogunbona. 2020. Modelling context and syntactical features for aspect-based sentiment analysis. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 3211–3220.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 29,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 32,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 229,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s2_c16",
    "source_id": "2022.acl_long.212",
    "text": "Pontiki, Maria, Dimitris Galanis, Haris Papageorgiou, Ion Androutsopoulos, Suresh Manandhar, Mohammad AL-Smadi, Mahmoud Al-Ayyoub, Yanyan Zhao, Bing Qin, Orphée De Clercq, Véronique Hoste, Marianna Apidianaki, Xavier Tannier, Natalia Loukachevitch, Evgeniy Kotelnikov, Nuria Bel, Salud María Jiménez-Zafra, and Gül¸sen Eryi˘git. 2016. SemEval-2016 task 5: Aspect based sentiment analysis. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), 19–30.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 30,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 32,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 484,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s2_c17",
    "source_id": "2022.acl_long.212",
    "text": "Pontiki, Maria, Dimitris Galanis, Haris Papageorgiou, Suresh Manandhar, and Ion Androutsopoulos. 2015. SemEval-2015 task 12: Aspect based sentiment analysis. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), 486–495.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 31,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 32,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 254,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s2_c18",
    "source_id": "2022.acl_long.212",
    "text": "Pontiki, Maria, Dimitris Galanis, John Pavlopoulos, Haris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. 2014. SemEval-2014 task 4: Aspect based sentiment analysis. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), 27–35.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 32,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 32,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 269,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s2_c19",
    "source_id": "2022.acl_long.212",
    "text": "Qi, Peng, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and Christopher D. Manning. 2020. Stanza: A python natural language processing toolkit for many human languages. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, 101–108.\n\nRead, Jesse, Bernhard Pfahringer, Geoff Holmes, and Eibe Frank. 2011. Classifier chains for multi-label classification. Machine learning, 85(3):333–359.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 33,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 32,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 440,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s2_c20",
    "source_id": "2022.acl_long.212",
    "text": "Severyn, Aliaksei, and Alessandro Moschitti. 2015. Twitter sentiment analysis with deep convolutional neural networks. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, 959–962.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 34,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 32,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 243,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s2_c21",
    "source_id": "2022.acl_long.212",
    "text": "Sun, Kai, Richong Zhang, Samuel Mensah, Yongyi Mao, and Xudong Liu. 2019. Aspect-level sentiment analysis via convolution over dependency tree. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 5679–5688.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 35,
    "chunk_index_in_section": 21,
    "total_chunks_in_section": 32,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 334,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s2_c22",
    "source_id": "2022.acl_long.212",
    "text": "Tang, Duyu, Bing Qin, and Ting Liu. 2016. Aspect level sentiment classification with deep memory network. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, 214–224.\n\nWang, Kai, Weizhou Shen, Yunyi Yang, Xiaojun Quan, and Rui Wang. 2020. Relational graph attention network for aspect-based sentiment analysis. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 3229–3238.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 36,
    "chunk_index_in_section": 22,
    "total_chunks_in_section": 32,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 452,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s2_c23",
    "source_id": "2022.acl_long.212",
    "text": "Wang, Wenya, and Sinno Jialin Pan. 2019. Transferable interactive memory network for domain adaptation in fine-grained opinion extraction. Proceedings of the AAAI Conference on Artificial Intelligence, 33(01):7192–7199.\n\nWang, Wenya, Sinno Jialin Pan, Daniel Dahlmeier, and Xiaokui Xiao. 2016. Recursive neural conditional random fields for aspect-based sentiment analysis. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, 616–626.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 37,
    "chunk_index_in_section": 23,
    "total_chunks_in_section": 32,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 473,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s2_c24",
    "source_id": "2022.acl_long.212",
    "text": "Wang, Wenya, Sinno Jialin Pan, Daniel Dahlmeier, and Xiaokui Xiao. 2017. Coupled multi-layer attentions for co-extraction of aspect and opinion terms. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, 3316–3322.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 38,
    "chunk_index_in_section": 24,
    "total_chunks_in_section": 32,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 240,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s2_c25",
    "source_id": "2022.acl_long.212",
    "text": "Wei, Zhenkai, Yu Hong, Bowei Zou, Meng Cheng, and Jianmin Yao. 2020. Don’t eclipse your arts due to small discrepancies: Boundary repositioning with a pointer network for aspect extraction. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 3678–3684.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 39,
    "chunk_index_in_section": 25,
    "total_chunks_in_section": 32,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 292,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s2_c26",
    "source_id": "2022.acl_long.212",
    "text": "Wu, Zhen, Chengcan Ying, Fei Zhao, Zhifang Fan, Xinyu Dai, and Rui Xia. 2020a. Grid tagging scheme for aspect-oriented fine-grained opinion extraction. In Findings of the Association for Computational Linguistics: EMNLP 2020, 2576–2585.\n\nWu, Zhen, Fei Zhao, Xin-Yu Dai, Shujian Huang, and Jiajun Chen. 2020b. Latent opinions transfer network for target-oriented opinion words extraction. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):9298–9305.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 40,
    "chunk_index_in_section": 26,
    "total_chunks_in_section": 32,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 468,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s2_c27",
    "source_id": "2022.acl_long.212",
    "text": "Xu, Hu, Bing Liu, Lei Shu, and Philip S. Yu. 2018. Double embeddings and CNN-based sequence labeling for aspect extraction. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), 592–598.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 41,
    "chunk_index_in_section": 27,
    "total_chunks_in_section": 32,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 249,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s2_c28",
    "source_id": "2022.acl_long.212",
    "text": "Xu, Lu, Yew Ken Chia, and Lidong Bing. 2021. Learning span-level interactions for aspect sentiment triplet extraction. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 4755–4766.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 42,
    "chunk_index_in_section": 28,
    "total_chunks_in_section": 32,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 320,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s2_c29",
    "source_id": "2022.acl_long.212",
    "text": "Xu, Lu, Hao Li, Wei Lu, and Lidong Bing. 2020. Position-aware tagging for aspect sentiment triplet extraction. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2339–2349.\n\nHang Yan, Junqi Dai, Tuo Ji, Xipeng Qiu, and Zheng Zhang. 2021. A unified generative framework for aspect-based sentiment analysis.\n\nBishan Yang and Claire Cardie. 2012. Extracting opinion expressions with semi-Markov conditional random fields.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 43,
    "chunk_index_in_section": 29,
    "total_chunks_in_section": 32,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 466,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s2_c30",
    "source_id": "2022.acl_long.212",
    "text": "Bishan Yang and Claire Cardie. 2013. Joint inference for fine-grained opinion extraction.\n\nBishan Yang and Claire Cardie. 2014. Context-aware learning for sentence-level sentiment analysis with posterior regularization.\n\nYichun Yin, Furu Wei, Li Dong, Kaimeng Xu, Ming Zhang, and Ming Zhou. 2016. Unsupervised word and dependency path embeddings for aspect term extraction.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 44,
    "chunk_index_in_section": 30,
    "total_chunks_in_section": 32,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 373,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.acl_long.212_s2_c31",
    "source_id": "2022.acl_long.212",
    "text": "Chen Zhang, Qiuchi Li, and Dawei Song. 2019. Aspect-based sentiment classification with aspect-specific graph convolutional networks.\n\nChen Zhang, Qiuchi Li, Dawei Song, and Benyou Wang. 2020. A multi-task learning framework for opinion triplet extraction.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 45,
    "chunk_index_in_section": 31,
    "total_chunks_in_section": 32,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 256,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8925,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.coling_1.234_s0_c0",
    "source_id": "2022.coling_1.234",
    "text": "A Simple Model for Distantly Supervised Relation Extraction\n\nZiqin Rao, Fangxiang Feng, Ruifan Li, Xiaojie Wang\nSchool of Artificial Intelligence, Beijing University of Posts and Telecommunications, China",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 5,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 204,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.coling_1.234_s1_c0",
    "source_id": "2022.coling_1.234",
    "text": "Distantly supervised relation extraction is challenging due to data noise. We propose BERT-based Graph Convolutional network Model (BGM), which includes an instance embedding module and a bag representation module. BGM achieves significant improvements on benchmark datasets NYT10 and GDS1.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 5,
    "chunk_index": 1,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 290,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9900000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.coling_1.234_s2_c0",
    "source_id": "2022.coling_1.234",
    "text": "In the distant supervision relation extraction (DS-RE) setting, noisy training data poses a significant challenge. Existing methods address this by using deep neural networks and complex de-noising schemes. We introduce BGM, a model that combines a Pretrained Language Model (PLM) and a Graph Convolutional Network (GCN) to learn instance correlations for bag representations.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 2,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 376,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.coling_1.234_s3_c0",
    "source_id": "2022.coling_1.234",
    "text": "DS-RE methods can be categorized into PCNN-based and PLMs-based approaches. PCNN-based methods use various techniques to acquire effective bag representations, while PLMs-based methods have shown remarkable performance by incorporating linguistic and semantic information. BGM is the first to use GCN to directly learn bag representations over instances.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 3,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 354,
    "chunk_method": "hierarchical",
    "importance_weight": 0.66,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.coling_1.234_s4_c0",
    "source_id": "2022.coling_1.234",
    "text": "Our BGM model consists of an embedding layer and a bag representation layer. The embedding layer uses a BERT-based PLM to represent instances, with special tokens for entity representation. The bag representation layer constructs a bag graph and applies convolutional operations to obtain the bag representation.",
    "section_title": "Method",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 4,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 20,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 312,
    "chunk_method": "hierarchical",
    "importance_weight": 0.935,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.coling_1.234_s4_c1",
    "source_id": "2022.coling_1.234",
    "text": "3.1 Embedding Layer\nEach instance is represented by a token sequence including [CLS], entities, and [SEP], processed by the BERT encoder to obtain hidden states for the instance and entities.\n\n3.2 Bag Representation Layer\n---",
    "section_title": "Method",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 5,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 20,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 225,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.coling_1.234_s4_c2",
    "source_id": "2022.coling_1.234",
    "text": "3.2 Bag Representation Layer\n---\n\nWe construct the bag graph G = {V, A} using the embedding layer, where nodes V are initialized by concatenating the representations of instance and two entities, resulting in a node dimension of 3d. The adjacency matrix A is generated using a softmax function and trainable parameters WQ and WK. Our Graph Convolutional Network (GCN) is updated classically, with an added self-connection to maintain the instance’s original semantics.",
    "section_title": "Method",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 6,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 20,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 468,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.coling_1.234_s4_c3",
    "source_id": "2022.coling_1.234",
    "text": "For relation prediction, we apply average pooling to the bag representation and use a linear layer followed by a softmax layer. The model is trained with a cross-entropy loss using gradient descent optimization.",
    "section_title": "Method",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 7,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 20,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 211,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.coling_1.234_s4_c4",
    "source_id": "2022.coling_1.234",
    "text": "Our experiments utilize the BERT-base-uncased model, with a maximum input sequence length of 120 and a hidden size of 768. The GCN has two layers, and we apply dropout rates of 0.3 to GCN and 0.5 to all linear layers. We compare our BGM with multiple baseline methods and report better performance on the NYT10 and GDS datasets in terms of P@N, AUC, and Micro-F1 score.",
    "section_title": "Method",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 8,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 20,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 369,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.coling_1.234_s4_c5",
    "source_id": "2022.coling_1.234",
    "text": "In the ablation study, we find that removing GCN or entity connections results in a performance drop, indicating their importance. We also explore the impact of various PLMs in our BGM and find that different PLMs lead to competitive performance, with Roberta-large achieving the best results.",
    "section_title": "Method",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 9,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 20,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 293,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.coling_1.234_s4_c6",
    "source_id": "2022.coling_1.234",
    "text": "We conducted a case study using two bags from NYT10, as shown in Table 5, applying three methods: BGM, BGM without GCN, and BGM without EntCon. For the first bag, BGM correctly identifies the relation /location/country/administrative_divisions, while the other variants incorrectly predict /location/location/contains. This is because BGM without EntCon cannot utilize the key phrase \"in the state of\" in S3, and BGM without GCN fails to effectively share captured information across instances. In t",
    "section_title": "Method",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 10,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 20,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 499,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.coling_1.234_s4_c7",
    "source_id": "2022.coling_1.234",
    "text": "ffectively share captured information across instances. In the second bag, the relation is /people/person/place_of_birth. BGM without EntCon误 predicts /people/person/place_lived, whereas BGM and BGM without GCN correctly identify the golden truth by capturing the contextual information of entities with \"was born in\".",
    "section_title": "Method",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 11,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 20,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 318,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.coling_1.234_s4_c8",
    "source_id": "2022.coling_1.234",
    "text": "In our approach, BGM operates on the entire graph, using a self-attention mechanism within the GCN layer. Unlike other attention-based methods, BGM does not selectively attend to the bag's target relation. Instead, it captures correlations among instances as graph nodes. This combination of GCN with self-attention proves effective for DS-RE.",
    "section_title": "Method",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 12,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 20,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 343,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.coling_1.234_s4_c9",
    "source_id": "2022.coling_1.234",
    "text": "Our conclusion is that the proposed BGM model, based on PLMs and GCN, is simple yet effective for DS-RE. It represents each instance with a BERT-based pre-trained language model and uses GCN to capture correlations within a bag. A cross-entropy loss is applied for relation prediction. The model demonstrates superior performance on two benchmark datasets. Future work will delve into the underlying theory for better explainability and consider extending BGM to handle single-instance bags.",
    "section_title": "Method",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 13,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 20,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 491,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.coling_1.234_s4_c10",
    "source_id": "2022.coling_1.234",
    "text": "This work was supported by the National Key Research and Development Program of China under Grants 2019YFF0303300 and 2019YFF0303302, and the National Natural Science Foundation of China under Grant 62076032. The authors appreciate the valuable comments from the editor and anonymous reviewers.\n\n(References section remains unchanged.)",
    "section_title": "Method",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 14,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 20,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 335,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.coling_1.234_s4_c11",
    "source_id": "2022.coling_1.234",
    "text": "(References section remains unchanged.)\n\nLiu, T., Wang, K., Chang, B., & Sui, Z. (2017). A soft-label method for noise-tolerant distantly supervised relation extraction. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, 1790–1795.",
    "section_title": "Method",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 15,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 20,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 271,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.coling_1.234_s4_c12",
    "source_id": "2022.coling_1.234",
    "text": "Mintz, M., Bills, S., Snow, R., & Jurafsky, D. (2009). Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, 1003–1011.",
    "section_title": "Method",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 16,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 20,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 296,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.coling_1.234_s4_c13",
    "source_id": "2022.coling_1.234",
    "text": "Qin, P., Xu, W., & Wang, W. Y. (2018). DSGAN: Generative adversarial training for distant supervision relation extraction. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 496–505.\n\nRiedel, S., Yao, L., & McCallum, A. (2010). Modeling relations and their mentions without labeled text. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 148–163.",
    "section_title": "Method",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 17,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 20,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 448,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.coling_1.234_s4_c14",
    "source_id": "2022.coling_1.234",
    "text": "Shang, Y.-M., Huang, H., Sun, X., Wei, W., & Mao, X.-L. (2022). A pattern-aware self-attention network for distant supervised relation extraction. Information Sciences, 584, 269–279.\n\nSurdeanu, M., Tibshirani, J., Nallapati, R., & Manning, C. D. (2012). Multi-instance multi-label learning for relation extraction. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, 455–465.",
    "section_title": "Method",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 18,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 20,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 464,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.coling_1.234_s4_c15",
    "source_id": "2022.coling_1.234",
    "text": "Tang, H., Sun, X., Jin, B., Wang, J., Zhang, F., & Wu, W. (2021). Improving document representations by generating pseudo query embeddings for dense retrieval. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, 5054–5064.",
    "section_title": "Method",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 19,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 20,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 337,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.coling_1.234_s4_c16",
    "source_id": "2022.coling_1.234",
    "text": "Vashishth, S., Joshi, R., Prayaga, S. S., Bhattacharyya, C., & Talukdar, P. (2018). RESIDE: Improving distantly-supervised neural relation extraction using side information. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 1257–1266.\n\nWang, Y., Sun, C., Wu, Y., Zhou, H., Li, L., & Yan, J. (2021). UniRE: A unified label space for entity relation extraction. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics.",
    "section_title": "Method",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 20,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 20,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 492,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.coling_1.234_s4_c17",
    "source_id": "2022.coling_1.234",
    "text": "Ye, Z.-X., & Ling, Z.-H. (2019). Distant supervision relation extraction with intra-bag and inter-bag attentions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2810–2819.",
    "section_title": "Method",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 21,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 20,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 271,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.coling_1.234_s4_c18",
    "source_id": "2022.coling_1.234",
    "text": "Yu, E., Han, W., Tian, Y., & Chang, Y. (2020). ToHRE: A top-down classification strategy with hierarchical bag representation for distantly supervised relation extraction. In Proceedings of the 28th International Conference on Computational Linguistics, 1665–1676.",
    "section_title": "Method",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 22,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 20,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 264,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.coling_1.234_s4_c19",
    "source_id": "2022.coling_1.234",
    "text": "Zeng, D., Liu, K., Chen, Y., & Zhao, J. (2015). Distant supervision for relation extraction via piecewise convolutional neural networks. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 1753–1762.",
    "section_title": "Method",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 23,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 20,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 238,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8925,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s0_c0",
    "source_id": "2022.emnlp_main.212",
    "text": "COM-MRC: A Context-Masked Machine Reading Comprehension Framework for Aspect Sentiment Triplet Extraction\n\nZepeng Zhai, Hao Chen, Fangxiang Feng, Ruifan Li*, Xiaojie Wang\n\nSchool of Artificial Intelligence, Beijing University of Posts and Telecommunications, China\nEngineering Research Center of Information Networks, Ministry of Education, China\n{zepeng, ccchenhao997, fxfeng, rfli, xjwang}@bupt.edu.cn\nAbstract:",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 23,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 413,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s0_c1",
    "source_id": "2022.emnlp_main.212",
    "text": "Aspect Sentiment Triplet Extraction (ASTE) extracts sentiment triplets from sentences and is formalized as an effective machine reading comprehension (MRC) framework. MRC-based methods may fail due to interference from multiple aspect terms. We propose a COntext-Masked MRC (COM-MRC) framework for ASTE, which includes a context augmentation strategy, a discriminative model, and an inference method. The context augmentation strategy generates masked contexts for each aspect term. The discriminative model consists of aspect and opinion extraction modules, sentiment classification, and aspect det",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 23,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s0_c2",
    "source_id": "2022.emnlp_main.212",
    "text": "aspect and opinion extraction modules, sentiment classification, and aspect detection modules. The two-stage inference method extracts aspects first and then identifies opinions and sentiment through iterative masking. Experimental results on benchmark datasets demonstrate the effectiveness of our COM-MRC framework.\n1 Introduction:",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 23,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 333,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s0_c3",
    "source_id": "2022.emnlp_main.212",
    "text": "ASTE, a variant of fine-grained Aspect-based Sentiment Analysis (ABSA), extracts sentiment triplets of aspect, opinion, and sentiment polarity. Early methods used a two-stage pipeline framework, which may ignore interactions and lead to error propagation. Recent studies extract sentiment triplets end-to-end, mainly by designing tagging schemes. MRC-based methods formalize ASTE using a multi-turn QA framework but may suffer from interference in sentences with multiple aspects. We introduce masking aspects to alleviate interference and propose the COM-MRC framework. It includes a context augmen",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 23,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s0_c4",
    "source_id": "2022.emnlp_main.212",
    "text": "ate interference and propose the COM-MRC framework. It includes a context augmentation strategy, a discriminative model, and an inference method to better identify information from different aspects and expand the training corpus.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 23,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 230,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s0_c5",
    "source_id": "2022.emnlp_main.212",
    "text": "---\n\nOur COM-MRC framework for Aspect-based Sentiment Analysis (ASTE) task consists of three components: a context augmentation strategy, a discriminative model, and an inference method. The framework collaboratively employs four modules: aspect extraction, opinion extraction, sentiment classification, and aspect detection.\n\n2.1 Problem Formulation\nGiven a sentence S = {w1, w2, ..., wn}, the ASTE task aims to extract all sentiment triplets T = {(a, o, s)}. Each sentiment triplet consists of an aspect term a, an opinion term o, and a sentiment polarity s ∈ {POS, NEU, NEG}.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 5,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 23,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 578,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s0_c6",
    "source_id": "2022.emnlp_main.212",
    "text": "2.2 Context Augmentation Strategy\nOur model uses a fixed query to prompt the BERT sentence encoder for adapting to the ASTE task. We augment the training corpus by masking aspect terms, creating 2t instances from each sentence with t aspect terms.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 6,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 23,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 247,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s0_c7",
    "source_id": "2022.emnlp_main.212",
    "text": "2.3 Discriminative Model\nOur discriminative model includes:\n- Aspect Extraction Module: Predicts the starting and ending positions of the first unmasked aspect term.\n- Opinion Extraction Module: Similar to the aspect module, it predicts opinion terms for the first unmasked aspect.\n- Sentiment Classification Module: Uses multi-head attention to fuse semantic information from the masked context, aspect, and opinion terms.\n- Aspect Detection Module: Detects the existence of aspect terms in the masked context.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 7,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 23,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 511,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s0_c8",
    "source_id": "2022.emnlp_main.212",
    "text": "The inference method involves two stages: Aspect Inference (AI) and Aspect Accessory Inference (AAI), where aspects are extracted iteratively from left to right.\n\n---\n\nFigure 2 illustrates the overview of our COM-MRC framework.\n\n---\n\nAlgorithm 1 outlines the inference algorithm.\n\nInput: Sentence S and query q.\nOutput: Triplets T = {(a, o, s)}N.\n\n---\n\n---",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 8,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 23,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 356,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s0_c9",
    "source_id": "2022.emnlp_main.212",
    "text": "Input: Sentence S and query q.\nOutput: Triplets T = {(a, o, s)}N.\n\n---\n\n---\n\n1: Initialize T, A = {}, {}\n2: e, a ← GetAI(q, S)\n3: while e = True do\n4: A ← A ∪ {a}\n5: e, a ← GetAI(q, S.Mask(A))\n6: end while\n7: for ai ∈ A do\n8: O, s ← GetAAI(q, S.Mask(A − {ai}))\n9: for oj ∈ O do\n10: T ← T ∪ {(ai, oj, s)}\n11: end for\n12: end for\n13: return T\n\nLoss Function:\nLT = αLA + βLO + γLS + δLE (14)",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 9,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 23,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 388,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s0_c10",
    "source_id": "2022.emnlp_main.212",
    "text": "Loss Function:\nLT = αLA + βLO + γLS + δLE (14)\n\nInference Method:\nOur method involves two stages: AI and AAI. AI extracts aspects; AAI identifies opinions and sentiment polarities. We illustrate this with an example in Figure 2. During AI, we obtain aspect detection flag e and aspect term a using our trained model. If e is True, we add a to A and mask it in S. We repeat this until e is False. For AAI, we mask all aspects except a, and with the query, we obtain opinion set O and sentiment s. We append triplets to T based on O.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 10,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 23,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 531,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s0_c11",
    "source_id": "2022.emnlp_main.212",
    "text": "Dataset Statistics:\nTable 2: Statistics for experimental datasets D1 and D2, including #S (sentences), #MA-S (sentences with multiple aspect terms), #T (triplets), and #MA-T (triplets in sentences with multiple aspects).\n\nExperiments:\n3.1 Datasets:\nWe use two groups of benchmark datasets from SemEval Challenges (Pontiki et al., 2014, 2015, 2016): D1 (Wu et al., 2020a) and D2 (Xu et al., 2020).\n\n3.2 Baseline Methods:\nWe compare our COM-MRC with state-of-the-art baselines, categorized into pipeline, end-to-end, and MRC-based methods.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 11,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 23,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 537,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s0_c12",
    "source_id": "2022.emnlp_main.212",
    "text": "3.3 Implementation Details:\nWe use Bert-Base-Uncased as our base encoder. Our model is trained for 100 epochs with a learning rate of 9 × 10−5. We use an AdamW optimizer with a batch size of 15 and a dropout rate of 0.1. Hyper-parameters α, β, γ, and δ are set to 8.0, 3.2, 1.0, and 1.0, respectively.\n\n3.4 Main Results:\nOur COM-MRC outperforms other baselines in terms of Precision, Recall, and F1 scores on both D1 and D2 datasets, as shown in Tables 3 and 4.\n\n---\n\n---",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 12,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 23,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 471,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s0_c13",
    "source_id": "2022.emnlp_main.212",
    "text": "CMLA+†: 39.18, 47.13, 42.79, 30.09, 36.92, 33.16, 34.56, 39.84, 37.01, 41.34, 42.10, 41.72\nRINANTE+†: 31.42, 39.38, 34.95, 21.71, 18.66, 20.07, 29.88, 30.06, 29.97, 25.68, 22.30, 23.87\nLi-unified-R†: 41.04, 67.35, 51.00, 40.56, 44.28, 42.34, 44.72, 51.39, 47.82, 37.33, 54.51, 44.31\nPeng-two-stage†: 43.24, 63.66, 51.46, 37.38, 50.38, 42.87, 48.07, 57.51, 52.32, 46.96, 64.24, 54.21\nOTE-MTL∗: 62.00, 55.97, 58.71, 49.53, 39.22, 43.42, 56.37, 40.94, 47.13, 62.88, 52.10, 59.96\nJET-BERT†: 70.56, 55.94, 62.40, 55.39, 47.33, 51.04, 64.45, 51.96, 57.53, 70.42, 58.37, 63.83",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 13,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 23,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 569,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s0_c14",
    "source_id": "2022.emnlp_main.212",
    "text": "GTS-BERT∗: 68.09, 69.54, 68.81, 59.40, 51.94, 55.42, 59.28, 57.93, 58.60, 68.32, 66.86, 67.58\nUnified: 65.52, 64.99, 65.25, 61.41, 56.19, 58.69, 59.14, 59.38, 59.26, 66.60, 68.68, 67.62\nBMRC∗: 75.61, 61.77, 67.99, 70.55, 48.98, 57.82, 68.51, 53.40, 60.02, 71.20, 61.08, 65.75\nSPAN-ASTE: 72.89, 70.89, 71.85, 63.44, 55.84, 59.38, 62.18, 64.45, 63.27, 69.45, 71.17, 70.26\nEMC-GCN: 71.21, 72.39, 71.78, 61.70, 56.26, 58.81, 61.54, 62.47, 61.93, 65.62, 71.30, 68.33",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 14,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 23,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 461,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s0_c15",
    "source_id": "2022.emnlp_main.212",
    "text": "Our COM-MRC: 75.46, 68.91, 72.01, 62.35, 58.16, 60.17, 68.35, 61.24, 64.53, 71.55, 71.59, 71.57\n\nTable 4: Results on the benchmark D2 (Xu et al., 2020). The symbol † indicates results are from Xu et al. (2020), and ∗ from Chen et al. (2022).\n\nTo ensure the significance of our experimental results, we conducted pairwise t-tests on F1 scores comparing our COM-MRC with BMRC and EMC-GCN on datasets D1 and D2. All produced p-values were less than 0.05.\n\n4 Analysis",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 15,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 23,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 463,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s0_c16",
    "source_id": "2022.emnlp_main.212",
    "text": "4 Analysis\n\n4.1 On Context Augmentation Strategy\nWe compared our exponential strategy with linear and NOP strategies. The exponential strategy obtained 2t samples per sentence, while the linear strategy produced 2t samples and the NOP strategy used original sentences without augmentation. Our exponential strategy achieved significantly better performance and increased performance with the number of training samples.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 16,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 23,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 419,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s0_c17",
    "source_id": "2022.emnlp_main.212",
    "text": "4.2 On Discriminative Model\nAblation experiments on D2 were conducted by removing aspect representation, opinion representation, existence concatenation, and sentiment attention. The sentiment attention had the largest impact, resulting in a 1.50% decrement in performance. This indicates our attention mechanism effectively fuses semantic information within aspects and opinions.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 17,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 23,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 380,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s0_c18",
    "source_id": "2022.emnlp_main.212",
    "text": "4.3 On Inference Method\nTwo versions of the inference method were compared: AAI 1, which masks aspects one by one, and AAI 2, used in our COM-MRC, which masks all aspects but the current one. AAI 2 significantly outperformed AAI 1 in the multi-aspect setting.\n\n4.4 On Query\nExperiments were conducted with regular, improper (removing the keyword \"first\"), and null queries. The regular query performed best, indicating the effectiveness of our query design.\n\n---",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 18,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 23,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 462,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s0_c19",
    "source_id": "2022.emnlp_main.212",
    "text": "---\n\n*Note: Some formatting and structural elements such as tables and figures have been maintained as per the original content, while noise and redundant information have been removed.*\n\n---\n\nTable 9: F1 scores of different queries on D2.\n\nThe performance of the improper query decreases by a mean 1.26%, compared with a null query, which experiences a mean decrement of 2.60%. This highlights the effectiveness of our query.\n\n4.5 Attention Visualization",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 19,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 23,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 455,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s0_c20",
    "source_id": "2022.emnlp_main.212",
    "text": "We visualize attention matrices to demonstrate the effective treatment of interference problems. Consider the sentence “good food, bad decor, great customer service, bad manager”. Figure 4(a) shows that for identifying the opinion term “food”, both subfigures focus on the golden opinion “good”. However, the left subfigure indicates non-negligible attention on incorrect opinions, particularly “bad”. In contrast, masking other aspect terms reduces attention on incorrect opinions, as shown on the right. Similarly, Figure 4(b) illustrates reduced attention on incorrect opinions like “great” when",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 20,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 23,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s0_c21",
    "source_id": "2022.emnlp_main.212",
    "text": "gure 4(b) illustrates reduced attention on incorrect opinions like “great” when other aspects are masked. The span “corresponding opinion terms” in our query receives high attention scores with golden opinions. Masking other aspects effectively aids in identifying the current aspect information.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 21,
    "chunk_index_in_section": 21,
    "total_chunks_in_section": 23,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 296,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s0_c22",
    "source_id": "2022.emnlp_main.212",
    "text": "4.6 Case Study\n\nTable 10 compares our COM-MRC with BMRC on cases with multiple aspects. In the first example, both methods correctly extract the aspect terms “ambience” and “place” with their opinion terms “Nice” and “overrated”, respectively. BMRC, however, fails to identify the correct sentiment polarity of “place”. The second example shows BMRC extracting an incorrect triplet, where “price” and “shipping” do not share the opinion term “great”.\n\nFigure 4: Visualization of attention matrices.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 22,
    "chunk_index_in_section": 22,
    "total_chunks_in_section": 23,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 498,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s1_c0",
    "source_id": "2022.emnlp_main.212",
    "text": "ABSA includes subtasks like Aspect Term Extraction (ATE), Aspect Sentiment Classification (ASC), and Opinion Term Extraction (OTE). Studies have ignored correlations between these subtasks, with some later works coupling two subtasks, such as Aspect and Opinion Term Co-Extraction (AOTE) and Aspect-Sentiment Pair Extraction (ASPE). ASTE, a new ABSA variant, has gained attention recently. End-to-end approaches and MRC-based methods have been proposed, though they are susceptible to interference from multiple aspect terms.\n\n6 Conclusion and Future Work",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 23,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 3,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 555,
    "chunk_method": "hierarchical",
    "importance_weight": 0.66,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s1_c1",
    "source_id": "2022.emnlp_main.212",
    "text": "6 Conclusion and Future Work\n\nWe propose a COntext-Masked MRC (COM-MRC) framework to alleviate interference in ASTE tasks. COM-MRC’s components work collaboratively, with the context augmentation strategy effectively expanding the training corpus. Our inference method, involving two stages, reduces interference from other aspects. Experiments on benchmark datasets demonstrate COM-MRC’s effectiveness. Future work includes a one-stage method for faster inference.\n\nLimitations",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 24,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 478,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s1_c2",
    "source_id": "2022.emnlp_main.212",
    "text": "Limitations\n\nOur context augmentation strategy may increase training time, preventing COM-MRC from being applied to large-scale data scenarios.\n\nAcknowledgements\n\nThis work was supported by the National Key R&D Program of China under Grant 2019YFF0303302 and the National Natural Science Foundation of China under Grant 62076032.\n\n---",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 25,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 334,
    "chunk_method": "hierarchical",
    "importance_weight": 0.63,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s2_c0",
    "source_id": "2022.emnlp_main.212",
    "text": "[References section remains unchanged, as per the instructions to only clean the provided content, not adding or removing any sections like References.]\n\n---\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186.\n\nFeifan Fan, Yansong Feng, and Dongyan Zhao.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 26,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 25,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 555,
    "chunk_method": "hierarchical",
    "importance_weight": 0.44000000000000006,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s2_c1",
    "source_id": "2022.emnlp_main.212",
    "text": "4186.\n\nFeifan Fan, Yansong Feng, and Dongyan Zhao. 2018. Multi-grained attention network for aspect-level sentiment classification. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3433–3442.\n\nZhifang Fan, Zhen Wu, Xin-Yu Dai, Shujian Huang, and Jiajun Chen. 2019. Target-oriented opinion words extraction with target-fused neural sequence labeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2509–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 27,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 590,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s2_c2",
    "source_id": "2022.emnlp_main.212",
    "text": "2518.\n\nRuidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel Dahlmeier. 2019. An interactive multi-task learning network for end-to-end aspect-based sentiment analysis. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 504–515.\n\nMinghao Hu, Yuxing Peng, Zhen Huang, Dongsheng Li, and Yiwei Lv. 2019. Open-domain targeted sentiment analysis via span-based extraction and classification. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 537–546.\n\nMinqing Hu and Bing Liu.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 28,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 560,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s2_c3",
    "source_id": "2022.emnlp_main.212",
    "text": "546.\n\nMinqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 168–177.\n\nRuifan Li, Hao Chen, Fangxiang Feng, Zhanyu Ma, Xijao Wang, and Eduard Hovy. 2021. Dual graph convolutional networks for aspect-based sentiment analysis. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6319–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 29,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 554,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s2_c4",
    "source_id": "2022.emnlp_main.212",
    "text": "6329.\n\nXin Li, Lidong Bing, Wai Lam, and Bei Shi. 2018a. Transformation networks for target-oriented sentiment classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 946–956.\n\nXin Li, Lidong Bing, Piji Li, and Wai Lam. 2019a. A unified model for opinion target extraction and target sentiment prediction. Proceedings of the AAAI Conference on Artificial Intelligence, 33(01):6714–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 30,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 463,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s2_c5",
    "source_id": "2022.emnlp_main.212",
    "text": "6721.\n\nXin Li, Lidong Bing, Piji Li, Wai Lam, and Zhimou Yang. 2018b. Aspect term extraction with history attention and selective transformation. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18, pages 4194–4200.\n\nXin Li, Lidong Bing, Wenxuan Zhang, and Wai Lam. 2019b. Exploiting BERT for end-to-end aspect-based sentiment analysis. In Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019), pages 34–41.\n\nIlya Loshchilov and Frank Hutter.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 31,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 515,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s2_c6",
    "source_id": "2022.emnlp_main.212",
    "text": "41.\n\nIlya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In International Conference on Learning Representations.\n\nDehong Ma, Sujian Li, and Houfeng Wang. 2018. Joint learning for targeted sentiment analysis. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4737–4742.\n\nDehong Ma, Sujian Li, Fangzhao Wu, Xing Xie, and Houfeng Wang. 2019. Exploring sequence-to-sequence learning in aspect term extraction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3538–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 32,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 584,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s2_c7",
    "source_id": "2022.emnlp_main.212",
    "text": "3547.\n\nDehong Ma, Sujian Li, Xiaodong Zhang, and Houfeng Wang. 2017. Interactive attention networks for aspect-level sentiment classification. In Proceedings of the 26th International Joint Conference on Artificial Intelligence, IJCAI’17, pages 4068–4074.\n\nYue Mao, Yi Shen, Chao Yu, and Longjun Cai. 2021. A joint training dual-mrc framework for aspect based sentiment analysis. Proceedings of the AAAI Conference on Artificial Intelligence, 35(15):13543–13551.\n\nHaiyun Peng, Lu Xu, Lidong Bing, Fei Huang, Wei Lu, and Luo Si.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 33,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 527,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s2_c8",
    "source_id": "2022.emnlp_main.212",
    "text": "13551.\n\nHaiyun Peng, Lu Xu, Lidong Bing, Fei Huang, Wei Lu, and Luo Si. 2020. Knowing what, how and why: A near complete solution for aspect-based sentiment analysis. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):8600–8607.\n\nMaria Pontiki, Dimitris Galanis, Haris Papageorgiou, Ion Androutsopoulos, Suresh Manandhar, Mohammad AL-Smadi, Mahmoud Al-Ayyoub, Yanyan Zhao, Bing Qin, Orphée De Clercq, Véronique Hoste, Marianna Apidianaki, Xavier Tannier, Natalia Loukachevitch, Evgeniy Kotelnikov, Nuria Bel, Salud María Jiménez-Zafra, and Gülşen Eryiğit.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 34,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 574,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s2_c9",
    "source_id": "2022.emnlp_main.212",
    "text": "2016. SemEval-2016 task 5: Aspect based sentiment analysis. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016).\n\nMaria Pontiki, Dimitris Galanis, Haris Papageorgiou, Suresh Manandhar, and Ion Androutsopoulos. 2015. SemEval-2015 task 12: Aspect based sentiment analysis. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015).\n\nMaria Pontiki, Dimitris Galanis, John Pavlopoulos, Haris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 35,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 509,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s2_c10",
    "source_id": "2022.emnlp_main.212",
    "text": "2014. SemEval-2014 task 4: Aspect based sentiment analysis. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014).\n\nKai Sun, Richong Zhang, Samuel Mensah, Yongyi Mao, and Xudong Liu. 2019. Aspect-level sentiment analysis via convolution over dependency tree. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP).\n\nDuyu Tang, Bing Qin, and Ting Liu.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 36,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 507,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s2_c11",
    "source_id": "2022.emnlp_main.212",
    "text": "2016. Aspect level sentiment classification with deep memory network. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30.\n\nKai Wang, Weizhou Shen, Yunyi Yang, Xiaojun Quan, and Rui Wang.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 37,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 449,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s2_c12",
    "source_id": "2022.emnlp_main.212",
    "text": "30.\n\nKai Wang, Weizhou Shen, Yunyi Yang, Xiaojun Quan, and Rui Wang. 2020. Relational graph attention network for aspect-based sentiment analysis. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.\n\nWenya Wang and Sinno Jialin Pan.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 38,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 272,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s2_c13",
    "source_id": "2022.emnlp_main.212",
    "text": "2019. Transferable interactive memory network for domain adaptation in fine-grained opinion extraction. Proceedings of the AAAI Conference on Artificial Intelligence, 33(01).\n\nWenya Wang, Sinno Jialin Pan, Daniel Dahlmeier, and Xiaokui Xiao. 2016a. Recursive neural conditional random fields for aspect-based sentiment analysis. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 616– 626, Austin, Texas. Association for Computational Linguistics.\n\nWenya Wang, Sinno Jialin Pan, Daniel Dahlmeier, and Xiaokui Xiao.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 39,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 560,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s2_c14",
    "source_id": "2022.emnlp_main.212",
    "text": "2017. Coupled multi-layer attentions for co-extraction of aspect and opinion terms. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI’17, page 3316–3322. AAAI Press.\n\nYequan Wang, Minlie Huang, Xiaoyan Zhu, and Li Zhao. 2016b. Attention-based LSTM for aspect- level sentiment classification. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 606–615, Austin, Texas. Association for Computational Linguistics.\n\nZhenkai Wei, Yu Hong, Bowei Zou, Meng Cheng, and Jianmin Yao.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 40,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 552,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s2_c15",
    "source_id": "2022.emnlp_main.212",
    "text": "2020. Don’t eclipse your arts due to small discrepancies: Boundary repositioning with a pointer network for aspect extraction. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3678–3684, Online. Association for Computational Linguistics.\n\nZhen Wu, Chengcan Ying, Fei Zhao, Zhifang Fan, Xinyu Dai, and Rui Xia. 2020a. Grid tagging scheme for aspect-oriented fine-grained opinion extraction. In Findings of the Association for Computational Lin- guistics: EMNLP 2020, pages 2576–2585, Online. Association for Computational Linguistics.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 41,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 582,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s2_c16",
    "source_id": "2022.emnlp_main.212",
    "text": "Zhen Wu, Fei Zhao, Xin-Yu Dai, Shujian Huang, and Jiajun Chen. 2020b. Latent opinions transfer net- work for target-oriented opinion words extraction. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):9298–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 42,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 226,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s2_c17",
    "source_id": "2022.emnlp_main.212",
    "text": "9305.\n\nHu Xu, Bing Liu, Lei Shu, and Philip S. Yu. 2018. Dou- ble embeddings and CNN-based sequence labeling for aspect extraction. In Proceedings of the 56th An- nual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 592–598, Melbourne, Australia. Association for Computational Linguistics.\n\nLu Xu, Yew Ken Chia, and Lidong Bing.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 43,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 369,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s2_c18",
    "source_id": "2022.emnlp_main.212",
    "text": "2021. Learn- ing span-level interactions for aspect sentiment triplet extraction. In Proceedings of the 59th Annual Meet- ing of the Association for Computational Linguistics and the 11th International Joint Conference on Natu- ral Language Processing (Volume 1: Long Papers), pages 4755–4766, Online. Association for Computa- tional Linguistics.\n\nLu Xu, Hao Li, Wei Lu, and Lidong Bing.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 44,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 387,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s2_c19",
    "source_id": "2022.emnlp_main.212",
    "text": "2020.\n\nPosition-aware tagging for aspect sentiment triplet extraction. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2339–2349, Online. Association for Computational Linguistics.\n\nHang Yan, Junqi Dai, Tuo Ji, Xipeng Qiu, and Zheng Zhang.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 45,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 296,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s2_c20",
    "source_id": "2022.emnlp_main.212",
    "text": "2021. A unified generative framework for aspect-based sentiment analysis. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 2416–2429, Online. Association for Computational Linguistics.\n\nBishan Yang and Claire Cardie.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 46,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 364,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s2_c21",
    "source_id": "2022.emnlp_main.212",
    "text": "2012. Extracting opin- ion expressions with semi-Markov conditional ran- dom fields. In Proceedings of the 2012 Joint Con- ference on Empirical Methods in Natural Language\n\nProcessing and Computational Natural Language Learning, pages 1335–1345, Jeju Island, Korea. As- sociation for Computational Linguistics.\n\nBishan Yang and Claire Cardie.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 47,
    "chunk_index_in_section": 21,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 342,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s2_c22",
    "source_id": "2022.emnlp_main.212",
    "text": "2013. Joint inference for fine-grained opinion extraction. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1640–1649, Sofia, Bulgaria. Association for Computational Linguistics.\n\nYichun Yin, Furu Wei, Li Dong, Kaimeng Xu, Ming Zhang, and Ming Zhou. 2016. Unsupervised word and dependency path embeddings for aspect term extraction. In Proceedings of the Twenty-Fifth Inter- national Joint Conference on Artificial Intelligence, IJCAI’16, page 2979–2985. AAAI Press.\n\nChen Zhang, Qiuchi Li, and Dawei Song.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 48,
    "chunk_index_in_section": 22,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 579,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s2_c23",
    "source_id": "2022.emnlp_main.212",
    "text": "2985. AAAI Press.\n\nChen Zhang, Qiuchi Li, and Dawei Song. 2019. Aspect- based sentiment classification with aspect-specific graph convolutional networks. In Proceedings of the 2019 Conference on Empirical Methods in Natu- ral Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4568–4578, Hong Kong, China. Association for Computational Linguistics.\n\nChen Zhang, Qiuchi Li, Dawei Song, and Benyou Wang.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 49,
    "chunk_index_in_section": 23,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 466,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.emnlp_main.212_s2_c24",
    "source_id": "2022.emnlp_main.212",
    "text": "2020. A multi-task learning framework for opinion triplet extraction. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 819–828, Online. Association for Computational Lin- guistics.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 50,
    "chunk_index_in_section": 24,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 211,
    "chunk_method": "hierarchical",
    "importance_weight": 0.42000000000000004,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.findings_emnlp.6_s0_c0",
    "source_id": "2022.findings_emnlp.6",
    "text": "KE-GCL: Knowledge Enhanced Graph Contrastive Learning for Commonsense Question Answering\n\nLihui Zhang1 and Ruifan Li1,2∗\n\n1School of Artificial Intelligence, Beijing University of Posts and Telecommunications, China\n2Engineering Research Center of Information Networks, Ministry of Education, China\n{elliot_zlh, rfli}@bupt.edu.cn",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 329,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.findings_emnlp.6_s1_c0",
    "source_id": "2022.findings_emnlp.6",
    "text": "Commonsense question answering (CQA) aims to select correct answers for commonsense questions. Existing works primarily focus on extracting and reasoning over external knowledge graphs (KG), but noise within KGs hinders effective representation learning. We propose KE-GCL, a Knowledge Enhanced Graph Contrastive Learning model, which incorporates entity contextual descriptions and employs a graph c",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 1,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 3,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 400,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9900000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.findings_emnlp.6_s1_c1",
    "source_id": "2022.findings_emnlp.6",
    "text": "tity contextual descriptions and employs a graph contrastive learning scheme. For QA pairs, we integrate knowledge from KGs and contextual descriptions, inserting context nodes into KGs to form knowledge-enhanced graphs. We design a contrastive learning method on graphs, using an adaptive sampling strategy to create augmented views. Graph reasoning updates representations by scattering edges and a",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 2,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 400,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.findings_emnlp.6_s1_c2",
    "source_id": "2022.findings_emnlp.6",
    "text": "updates representations by scattering edges and aggregating nodes. Hard graph negatives are chosen based on incorrect answers. KE-GCL consistently outperforms previous methods on two benchmark datasets.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 3,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 202,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9450000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.findings_emnlp.6_s2_c0",
    "source_id": "2022.findings_emnlp.6",
    "text": "Commonsense question answering (CQA) evaluates machine language understanding by choosing answers to natural language questions about commonsense. External knowledge graphs (KGs) are used for reasoning, but noise in KGs can be problematic. To mitigate this, we enhance KGs with textual descriptions and employ graph contrastive learning (GCL). KE-GCL is an end-to-end model that concatenates QA pairs with Wiktionary descriptions, extracts subgraphs from ConceptNet, and inserts contextual representations into graphs. GCL is integrated into KE-GCL with adaptive graph augmentation. A graph attention",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 4,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 35,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 600,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.findings_emnlp.6_s2_c1",
    "source_id": "2022.findings_emnlp.6",
    "text": "CL is integrated into KE-GCL with adaptive graph augmentation. A graph attention network (GAT) based reasoning module scatters edges and aggregates nodes for efficient message propagation. Training is enhanced by selecting hard graph negatives.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 5,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 35,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 244,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.findings_emnlp.6_s2_c2",
    "source_id": "2022.findings_emnlp.6",
    "text": "We compute graph contrastive loss using positive and negative pairs. Positive pairs are generated from the graph augmented view of the correct answer, while negative pairs consist of the graph and its augmented counterparts from incorrect choices. We also consider other graphs in the mini-batch as common negatives. Our KE-GCL model is trained on a combination of answer prediction loss and graph contrastive loss. Major contributions include: 1) a novel KE-GCL model with a GCL scheme for CQA, using adaptive sampling for graph augmentation and selecting hard negatives from incorrect answers; 2)",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 6,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 35,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.findings_emnlp.6_s2_c3",
    "source_id": "2022.findings_emnlp.6",
    "text": "for graph augmentation and selecting hard negatives from incorrect answers; 2) enhancing the Knowledge Graph (KG) with contextual descriptions of entities, building a knowledge-enhanced graph, and updating graph representations via edge scattering and node aggregation; and 3) extensive experiments on two benchmark datasets showing consistent performance improvements over strong baselines.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 7,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 35,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 391,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.findings_emnlp.6_s2_c4",
    "source_id": "2022.findings_emnlp.6",
    "text": "In related works, KG-aware methods for CQA have focused on knowledge utilization and graph reasoning. Early works retrieved reasoning paths in KGs, while recent studies used GNNs for graph encoding and message aggregation. However, they often ignored the noise in KGs. Graph contrastive learning extends contrastive learning to graph-structured data, with works focusing on unsupervised representation learning. Our KE-GCL model incorporates GCL into the CQA task to enhance graph representations and training signals.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 8,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 35,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 518,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.findings_emnlp.6_s2_c5",
    "source_id": "2022.findings_emnlp.6",
    "text": "Our model framework involves: 1) representing the QA pair with its Wiktionary descriptions and retrieving the corresponding subgraph from ConceptNet; 2) inserting the context node and performing attentive knowledge fusion; 3) generating the graph view through adaptive augmentation; 4) edge-scattered reasoning over graphs to obtain representations; and 5) answer prediction and loss computation in a mini-batch.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 9,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 35,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 412,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.findings_emnlp.6_s2_c6",
    "source_id": "2022.findings_emnlp.6",
    "text": "In the KE-GCL model, knowledge enhancement is achieved through knowledge representation and graph-oriented knowledge fusion. We use PLMs as context encoders and retrieve knowledge graphs from ConceptNet. Node and edge embeddings are initialized and updated, with attention mechanisms applied for improved knowledge fusion.\n\n---\n\nwhere the mapping 𝑓𝑄 is a MLP and 𝐷𝑔 is the dimension of node embedding. We obtain the knowledge-enhanced graph ˜𝐺𝑖 = (˜𝑉𝑖, ˜𝐸𝑖), 𝑖 ∈ [1, 𝑀] with 𝑛 + 1 nodes and ˜𝑚 edges.\n\n3.3 Graph Contrastive Learning",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 10,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 35,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 532,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.findings_emnlp.6_s2_c7",
    "source_id": "2022.findings_emnlp.6",
    "text": "3.3 Graph Contrastive Learning\n\n3.3.1 Adaptive Graph Augmentation\nBased on the knowledge-enhanced graph ˜𝐺𝑖, we construct an augmented view ˆ𝐺𝑖 for GCL through node-feature masking and edge dropping. The influence of each node ˜𝑣𝑖,𝑞 ∈ ˜𝑉𝑖 is defined as,\n\n𝜌˜𝑣𝑖,𝑞 = 𝑓𝑇(˜𝑣𝑖,𝑞) + 𝑓𝑅(˜𝑣𝑖,𝑞, z𝐶𝑖) \n\nwhere 𝑓𝑇(·) and 𝑓𝑅(·, ·) represent the topological connectivity and contextual relevance, respectively. The importance weight of dimension 𝑑 for any node in ˜𝑉𝑖 is calculated as,\n\n𝛾𝑖,𝑑 = log ∑˜𝑣 ∈ ˜𝑉𝑖 |˜𝑣[𝑑]| · 𝜌˜𝑣",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 11,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 35,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 507,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.findings_emnlp.6_s2_c8",
    "source_id": "2022.findings_emnlp.6",
    "text": "𝛾𝑖,𝑑 = log ∑˜𝑣 ∈ ˜𝑉𝑖 |˜𝑣[𝑑]| · 𝜌˜𝑣 \n\nFor each edge 𝑒 in ˜𝐸𝑖, its importance depends on the importance weight of the tail node ˜𝑣𝑡 which the edge points to. We obtain the augmented view ˆ𝐺𝑖 = (ˆ𝑉𝑖, ˆ𝐸𝑖) of ˜𝐺𝑖 through sampling with these normalized probabilities.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 12,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 35,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 262,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.findings_emnlp.6_s2_c9",
    "source_id": "2022.findings_emnlp.6",
    "text": "3.3.2 Graph Reasoning\nBoth the knowledge-enhanced graph ˜𝐺𝑖 and its augmented view ˆ𝐺𝑖 are performed the same reasoning. Taking ˜𝐺𝑖 = {˜𝑉𝑖, ˜𝐸𝑖} as an example, we reason over the graph via edge scattering and attention-based node aggregating. The hidden states of the context node are chosen as the pooling of the entire knowledge graph, i.e.,\n\nz ˜𝐺𝑖 = Pool(ℎ(𝐿)0, ℎ(𝐿)1, ..., ℎ(𝐿)𝑛) = ℎ(𝐿)0.\n\n3.4 Answer Prediction\nThe probability of choice 𝑐𝑖 being the correct answer is calculated using contextual representation z𝐶𝑖 and graph representation z ˜𝐺𝑖,\n\n𝑃(𝑐𝑖|𝑞) = 𝑔𝑖 ⊙ (z𝐶𝑖𝑊𝐶, z ˜𝐺𝑖𝑊˜𝐺)",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 13,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 35,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 585,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.findings_emnlp.6_s2_c10",
    "source_id": "2022.findings_emnlp.6",
    "text": "𝑃(𝑐𝑖|𝑞) = 𝑔𝑖 ⊙ (z𝐶𝑖𝑊𝐶, z ˜𝐺𝑖𝑊˜𝐺)\n\nwhere the gate 𝑔𝑖 is given as,\n\n𝑔𝑖 = softmax(MLP([z𝐶𝑖, z ˜𝐺𝑖]))\n\n3.5 Training Objective\nWe train our KE-GCL model by minimizing the total loss L𝑇 as follows,\n\nL𝑇 = L𝐶𝐸 + 𝜆L𝐶𝐿 \n\nwhere L𝐶𝐸 and L𝐶𝐿 denote the answer prediction loss and the graph contrastive loss, respectively. The answer prediction loss is a standard cross-entropy loss. The graph contrastive loss is based on the InfoNCE and incorporates hard negatives.\n\n3.5.1 Graph Contrastive Loss\nThe graph contrastive loss is designed as,\n\nL𝐶𝐿 = −log(𝑇P / (𝑇P + 𝛽𝑇N𝐻 + 𝑇N𝐶))",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 14,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 35,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 562,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.findings_emnlp.6_s2_c11",
    "source_id": "2022.findings_emnlp.6",
    "text": "L𝐶𝐿 = −log(𝑇P / (𝑇P + 𝛽𝑇N𝐻 + 𝑇N𝐶))\n\nwith the positive contribution term,\n\n𝑇P = exp(𝜃(z ˜𝐺𝑖, z ˆ𝐺𝑖) / 𝜏)\n\nthe hard negatives contribution term, and the common negatives contribution term.\n\n4 Experiments and Results\n\n4.1 Datasets and Metric\nWe evaluate our model on CommonsenseQA and OpenbookQA datasets using Accuracy (Acc) as the metric.\n\n4.2 Baselines\n\n---",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 15,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 35,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 357,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.findings_emnlp.6_s2_c12",
    "source_id": "2022.findings_emnlp.6",
    "text": "4.2 Baselines\n\n---\n\nWe compare our KE-GCL with state-of-the-art baselines: RoBERTa-Large (w/o KG), Relation Network (RN), RGCN, GconAttn, KagNet, MHGRN, and QA-GNN. For fair comparison, we use the same backbones and implement our model with Huggingface. The hop size of retrieved subgraphs is set to 2, and the context dimension to 1024. The graph dimension is set to 200 with 3 GAT layers. Hyper-parameters for graph contrastive learning are set as 𝜆=0.1, 𝛽=2, and 𝜏=0.2. We train the model for 30 epochs with RAdam optimizer and apply gradient accumulation.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 16,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 35,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 559,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.findings_emnlp.6_s2_c13",
    "source_id": "2022.findings_emnlp.6",
    "text": "Our KE-GCL model consistently outperforms other baselines on CommonsenseQA and OpenBookQA datasets. It achieves an average accuracy improvement of 1.08% on CommonsenseQA and 0.83% and 0.64% on OpenBookQA. The effectiveness of incorporating external knowledge is confirmed, with knowledge-aware models showing performance gains over vanilla PLMs.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 17,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 35,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 345,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.findings_emnlp.6_s2_c14",
    "source_id": "2022.findings_emnlp.6",
    "text": "In the ablation study on CommonsenseQA IHdev set, we find that removing KGs from ConceptNet decreases performance by 3.41%, while removing both ConceptNet and Wiktionary drops performance by 7.86%. This highlights the importance of enhancing KGs with contextual descriptions. Graph augmentation and reasoning components also contribute to the model's performance.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 18,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 35,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 363,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.findings_emnlp.6_s2_c15",
    "source_id": "2022.findings_emnlp.6",
    "text": "On OpenBookQA, switching the PLM to AristoRoBERTa significantly improves performance, indicating our model's effectiveness in integrating additional science facts. The ablation study demonstrates the benefits of each component in KE-GCL.\n\n---\n\nTable 5 presents a case study of the predicted choices and scores from three distinct models, with correct choices underlined.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 19,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 35,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 370,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.findings_emnlp.6_s2_c16",
    "source_id": "2022.findings_emnlp.6",
    "text": "Our adaptive sampling strategy is effective; removing it (\"w/o Either\") leads to a 1.11% decline. Ablation studies on the graph reasoning module show a slight performance decrease when removing either edge scatter or GAT components. The removal of the entire reasoning module (\"w/o Either\") results in a more significant 1.93% decline, indicating the module's importance in learning graph representations. Graph Contrastive Learning (GCL) ablations demonstrate a heavy performance drop of 2.23% without the contrastive learning objective, confirming GCL's role in differentiating correct answers fro",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 20,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 35,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.findings_emnlp.6_s2_c17",
    "source_id": "2022.findings_emnlp.6",
    "text": "learning objective, confirming GCL's role in differentiating correct answers from distractors. We observe a 0.77% drop when not using hard negative graph pairs, highlighting their importance for GCL.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 21,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 35,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 199,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.findings_emnlp.6_s2_c18",
    "source_id": "2022.findings_emnlp.6",
    "text": "In 4.6 Attention Visualization, we illustrate the effectiveness of our graph augmentation strategy by visualizing attention weights for a CommonsenseQA dataset case. The attention weights show that the model focuses on the correct answer after augmentation.\n\nSection 4.7 presents a case study where we randomly select four cases from CommonsenseQA. We analyze correct and incorrect predictions, showing the nuances captured by our KE-GCL model and areas for improvement.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 22,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 35,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 470,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.findings_emnlp.6_s2_c19",
    "source_id": "2022.findings_emnlp.6",
    "text": "4.8 Investigates the impact of hard negatives in GCL by evaluating our KE-GCL model with different values of the weighing factor 𝛽. The best performance is achieved at 𝛽= 2.0, indicating the benefits of appropriate hard negatives.\n\nIn the conclusion, we summarize our KE-GCL model's contributions to reducing KG noise in CQA tasks and discuss future work, including applying GCL in few-shot or unsupervised scenarios. Limitations and acknowledgments are also noted, along with references to related work.\n\n---",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 23,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 35,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 509,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.findings_emnlp.6_s2_c20",
    "source_id": "2022.findings_emnlp.6",
    "text": "---\n\nTing Chen et al. (2020) introduced a simple framework for contrastive learning of visual representations. Sumit Chopra et al. (2005) proposed a discriminative approach to learning a similarity metric, applied to face verification. Peter Clark et al. (2020) presented an overview of the ARISTO project, which aims to improve performance on science exams. Yanlin Feng et al. (2020) developed a scalable multi-hop relational reasoning method for knowledge-aware question answering.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 24,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 35,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 483,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.findings_emnlp.6_s2_c21",
    "source_id": "2022.findings_emnlp.6",
    "text": "Tianyu Gao et al. (2021) proposed SimCSE, a simple contrastive learning approach for sentence embeddings. Justin Gilmer et al. (2017) applied neural message passing to quantum chemistry. Raia Hadsell et al. (2006) explored dimensionality reduction by learning an invariant mapping. Kaveh Hassani and Amir Hosein Khasahmadi (2020) introduced contrastive multi-view representation learning on graphs.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 25,
    "chunk_index_in_section": 21,
    "total_chunks_in_section": 35,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 398,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.findings_emnlp.6_s2_c22",
    "source_id": "2022.findings_emnlp.6",
    "text": "Kaiming He et al. (2020) presented Momentum Contrast for unsupervised visual representation learning. Olivier Henaff (2020) investigated data-efficient image recognition with contrastive predictive coding. R Devon Hjelm et al. (2018) focused on learning deep representations through mutual information estimation and maximization. Thomas N. Kipf and Max Welling (2017) introduced graph convolutional networks for semi-supervised classification.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 26,
    "chunk_index_in_section": 22,
    "total_chunks_in_section": 35,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 444,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.findings_emnlp.6_s2_c23",
    "source_id": "2022.findings_emnlp.6",
    "text": "Zhenzhong Lan et al. (2019) proposed ALBERT, a lite BERT for self-supervised learning. Yujia Li et al. (2016) worked on gated graph sequence neural networks. Bill Yuchen Lin et al. (2019) introduced Kagnet, a knowledge-aware graph network for commonsense reasoning. Liyuan Liu et al. (2019a) discussed the variance of the adaptive learning rate. Yinhan Liu et al. (2019b) presented RoBERTa, a robustly optimized BERT pretraining approach.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 27,
    "chunk_index_in_section": 23,
    "total_chunks_in_section": 35,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 438,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.findings_emnlp.6_s2_c24",
    "source_id": "2022.findings_emnlp.6",
    "text": "Shangwen Lv et al. (2020) used graph-based reasoning over heterogeneous external knowledge for commonsense question answering. Yu Meng et al. (2021) proposed Coco-LM for correcting and contrasting text sequences in language model pretraining. Todor Mihaylov et al. (2018) introduced a dataset for open book question answering and explored enhancing cloze-style reading comprehension with external commonsense knowledge.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 28,
    "chunk_index_in_section": 24,
    "total_chunks_in_section": 35,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 419,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.findings_emnlp.6_s2_c25",
    "source_id": "2022.findings_emnlp.6",
    "text": "Ishan Misra and Laurens van der Maaten (2020) focused on self-supervised learning of pretext-invariant representations. Andriy Mnih and Koray Kavukcuoglu (2013) efficiently learned word embeddings with noise-contrastive estimation. Zhen Peng et al. (2020) worked on graph representation learning via graphical mutual information maximization.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 29,
    "chunk_index_in_section": 25,
    "total_chunks_in_section": 35,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 342,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.findings_emnlp.6_s2_c26",
    "source_id": "2022.findings_emnlp.6",
    "text": "Jiezhong Qiu et al. (2020) proposed Graph Contrastive Coding for graph neural network pre-training. Lin Qiu et al. (2019) developed a dynamically fused graph network for multi-hop reasoning. Adam Santoro et al. (2017) introduced a neural network module for relational reasoning. Michael Schlichtkrull et al. (2018) modeled relational data with graph convolutional networks.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 30,
    "chunk_index_in_section": 26,
    "total_chunks_in_section": 35,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 373,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.findings_emnlp.6_s2_c27",
    "source_id": "2022.findings_emnlp.6",
    "text": "Robyn Speer et al. (2017) presented ConceptNet 5.5, an open multilingual graph of general knowledge. Alon Talmor et al. (2019) introduced CommonsenseQA, a question answering challenge targeting commonsense knowledge. Yonglong Tian et al. (2020) proposed Contrastive Multiview Coding. Aaron Van den Oord et al. (2018) worked on representation learning with contrastive predictive coding.\n\nPetar Veličković et al. (2018) introduced Graph Attention Networks. Petar Velickovic et al. (2019) proposed Deep Graph Infomax for ICLR.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 31,
    "chunk_index_in_section": 27,
    "total_chunks_in_section": 35,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 524,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.findings_emnlp.6_s2_c28",
    "source_id": "2022.findings_emnlp.6",
    "text": "Wang, X., Kapanipathi, P., Musa, R., Yu, M., Talamadupula, K., Abdelaziz, I., Chang, M., Fokoue, A., Makni, B., Mattei, N., et al. (2019). Improving natural language inference using external knowledge in the science questions domain. AAAI.\n\nWeissenborn, D., Kočisk`y, T., & Dyer, C. (2017). Dynamic integration of background knowledge in neural NLU systems. arXiv preprint arXiv:1706.02596.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 32,
    "chunk_index_in_section": 28,
    "total_chunks_in_section": 35,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 390,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.findings_emnlp.6_s2_c29",
    "source_id": "2022.findings_emnlp.6",
    "text": "Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., et al. (2020). Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations, 38–45.\n\nXiong, W., Yu, M., Chang, S., Guo, X., & Wang, W. Y. (2019). Improving question answering over incomplete kbs with knowledge-aware reader. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 4258–4264.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 33,
    "chunk_index_in_section": 29,
    "total_chunks_in_section": 35,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 554,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.findings_emnlp.6_s2_c30",
    "source_id": "2022.findings_emnlp.6",
    "text": "Xu, K., Hu, W., Leskovec, J., & Jegelka, S. (2019). How powerful are graph neural networks? In International Conference on Learning Representations.\n\nXu, Y., Zhu, C., Xu, R., Liu, Y., Zeng, M., & Huang, X. (2021). Fusing context into knowledge graph for commonsense question answering. In Findings of the Association for Computational Linguistics: ACL-ĲCNLP 2021, 1201–1207.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 34,
    "chunk_index_in_section": 30,
    "total_chunks_in_section": 35,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 374,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.findings_emnlp.6_s2_c31",
    "source_id": "2022.findings_emnlp.6",
    "text": "Yan, Y., Li, R., Wang, S., Zhang, F., Wu, W., & Xu, W. (2021). Consert: A contrastive framework for self-supervised sentence representation transfer. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 5065–5075.\n\nYang, H., Chen, H., Pan, S., Li, L., Yu, P. S., & Xu, G. (2022). Dual space graph contrastive learning. In Proceedings of the ACM Web Conference 2022, 1238–1247.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 35,
    "chunk_index_in_section": 31,
    "total_chunks_in_section": 35,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 514,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.findings_emnlp.6_s2_c32",
    "source_id": "2022.findings_emnlp.6",
    "text": "Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R. R., & Le, Q. V. (2019). Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems, 32.\n\nYasunaga, M., Ren, H., Bosselut, A., Liang, P., & Leskovec, J. (2021). Qa-gnn: Reasoning with language models and knowledge graphs for question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 535–546.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 36,
    "chunk_index_in_section": 32,
    "total_chunks_in_section": 35,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 526,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.findings_emnlp.6_s2_c33",
    "source_id": "2022.findings_emnlp.6",
    "text": "You, Y., Chen, T., Sui, Y., Chen, T., Wang, Z., & Shen, Y. (2020). Graph contrastive learning with augmentations. Advances in Neural Information Processing Systems, 33:5812–5823.\n\nZhang, D., Li, S., Xiao, W., Zhu, H., Nallapati, R., Arnold, A. O., & Xiang, B. (2021). Pairwise supervised contrastive learning of sentence representations. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 5786–5798.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 37,
    "chunk_index_in_section": 33,
    "total_chunks_in_section": 35,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 439,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2022.findings_emnlp.6_s2_c34",
    "source_id": "2022.findings_emnlp.6",
    "text": "Zhu, Y., Xu, Y., Liu, Q., & Wu, S. (2021a). An empirical study of graph contrastive learning. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2).\n\nZhu, Y., Xu, Y., Yu, F., Liu, Q., Wu, S., & Wang, L. (2021b). Graph contrastive learning with adaptive augmentation. In Proceedings of the Web Conference 2021, 2069–2080.\n\nZhuang, C., Zhai, A. L., & Yamins, D. (2019). Local aggregation for unsupervised learning of visual embeddings. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 6002–6012.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 38,
    "chunk_index_in_section": 34,
    "total_chunks_in_section": 35,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 574,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2023.acl_long.802_s0_c0",
    "source_id": "2023.acl_long.802",
    "text": "USSA: A Unified Table Filling Scheme for Structured Sentiment Analysis\n\nZepeng Zhai, Hao Chen, Ruifan Li, Xiaojie Wang",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 5,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 118,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2023.acl_long.802_s1_c0",
    "source_id": "2023.acl_long.802",
    "text": "Most studies on Structured Sentiment Analysis (SSA) treat it as a bi-lexical dependency parsing problem, failing to address overlap and discontinuity. We propose USSA, a unified 2D table-filling scheme that utilizes 13 relation types. Our approach incorporates a bi-axial attention module to capture correlations within the table. Experimental results demonstrate the effectiveness of our framework,",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 5,
    "chunk_index": 1,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 2,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 399,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9900000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2023.acl_long.802_s1_c1",
    "source_id": "2023.acl_long.802",
    "text": "s demonstrate the effectiveness of our framework, consistently outperforming state-of-the-art methods.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 5,
    "chunk_index": 2,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 2,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 102,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9450000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2023.acl_long.802_s2_c0",
    "source_id": "2023.acl_long.802",
    "text": "SSA identifies opinion tuples (h, t, e, p) within sentences. It is more challenging than related tasks due to the need to identify all four elements, which may overlap or be discontinuous. Existing methods, such as bi-lexical dependency parsing, are lossy and unable to handle these issues effectively. We construct a novel bi-lexical dependency parsing graph that addresses overlap and discontinuity, converting it into the USSA scheme. This scheme resolves the core challenges of SSA by distinguishing between overlapping entities and identifying discontinuous ones.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 3,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 14,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 568,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2023.acl_long.802_s2_c1",
    "source_id": "2023.acl_long.802",
    "text": "Dataset Overlap Discontinuity # % # %\nNoReCFine 2178 19.6 1080 9.7\nMultiBEU 0 0 164 7.1\nMultiBCA 3 0.1 113 4.1\nMPQA 403 1.4 0 0\nDSUnis 18 1.7 102 9.9",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 4,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 149,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2023.acl_long.802_s2_c2",
    "source_id": "2023.acl_long.802",
    "text": "Our method comprises Relation Prediction (RP) and Token Extraction (TE) edges. RP handles entity boundary identification and relation prediction, solving the overlap problem. TE identifies tokens within boundaries, resolving discontinuity. The dependency parsing graph is converted to a 2D table, with x and y-coordinates representing the start and end positions of edges, and the edge type as the relationship label. The table is divided into lower and upper triangular regions for RP and TE, respectively.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 5,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 507,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2023.acl_long.802_s2_c3",
    "source_id": "2023.acl_long.802",
    "text": "We propose a model for Structured Sentiment Analysis (SSA) that utilizes multilingual BERT and bi-directional LSTM (BiLSTM) to provide contextualized word representations. These representations are used to construct a 2-Dimensional (2D) table for word pairs, capturing strong correlations in both axes. A bi-axial attention module is introduced to effectively capture these correlations, followed by a predictor to determine the relations between word pairs. Our model demonstrates state-of-the-art performance on five benchmark datasets, including NoReCFine, MultiBEU, MultiBCA, MPQA, and DSUnis. O",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 6,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2023.acl_long.802_s2_c4",
    "source_id": "2023.acl_long.802",
    "text": "benchmark datasets, including NoReCFine, MultiBEU, MultiBCA, MPQA, and DSUnis. Our contributions include: a bi-lexical dependency parsing graph converted to a unified 2D table filling scheme (USSA), addressing overlap and discontinuity in SSA; an effective model that collaborates with USSA, utilizing the bi-axial attention module; and extensive experimental validation on benchmark datasets.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 7,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 393,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2023.acl_long.802_s2_c5",
    "source_id": "2023.acl_long.802",
    "text": "In related work, SSA is divided into sub-tasks such as entity extraction, relationship determination, and sentiment assignment. Previous studies have explored various methods, including BiLSTM-CRF, BERT-based models, transition-based approaches, and span-based models, yet often ignore sentiment polarity classification. In Aspect-Based Sentiment Analysis (ABSA), multiple subtasks have been unified, with methods categorized into pipeline, end-to-end, and MRC-based approaches. However, these methods primarily focus on flat entities and overlook holder extraction. To overcome these limitations, w",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 8,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2023.acl_long.802_s2_c6",
    "source_id": "2023.acl_long.802",
    "text": "n flat entities and overlook holder extraction. To overcome these limitations, we propose a novel dependency parsing method that can handle overlapping and discontinuous entities, converting the parsing graph to a 2D table filling scheme.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 9,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 238,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2023.acl_long.802_s2_c7",
    "source_id": "2023.acl_long.802",
    "text": "The USSA scheme uses 13 types of relations for word pairs, with the table divided into lower and upper triangular regions for relation prediction and token extraction, respectively. This approach aims to extract opinion tuples from USSA tagging results, addressing the challenges of discontinuous entities, overlapping counterparts, and null holders or targets. The decoding algorithm identifies entity boundaries in the lower triangle and specific tokens in the upper triangle, ensuring accurate opinion tuple extraction.\n\n---",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 10,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 527,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2023.acl_long.802_s2_c8",
    "source_id": "2023.acl_long.802",
    "text": "---\n\nWe identify boundary words of holders and targets corresponding to sentiment expressions using {H-S, H-E, H-SE} and {T-S, T-E, T-SE} respectively. The specific tokens of holder, target, and expression are extracted according to {E-NW, H-NW, T-NW} and entity boundaries, forming sentiment tuples (h,t,e,p). Figure 3 illustrates decoding cases from simple to complex, including flat, overlapping, discontinuous, and complex cases, demonstrating the effectiveness of relation types in handling various issues.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 11,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 511,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2023.acl_long.802_s2_c9",
    "source_id": "2023.acl_long.802",
    "text": "Our model, as depicted in Figure 4, is designed to integrate the USSA scheme and comprises four components: the encoder layer, word-pair representation layer, refining strategy, and prediction layer.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 12,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 199,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2023.acl_long.802_s2_c10",
    "source_id": "2023.acl_long.802",
    "text": "The encoder layer uses BiLSTM to output hidden representation sequence H, enhanced with pretrained contextualized embeddings from multilingual BERT. The word-pair representation layer models asymmetric relations using Conditional Layer Normalization (CLN). The refining strategy employs a bi-axial attention module to capture the correlation of relations and includes a distance feature to improve representation. The prediction layer uses FFN and biaffine predictors to obtain label probability distribution. The loss function is a cross-entropy loss for minimizing errors.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 13,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 574,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2023.acl_long.802_s2_c11",
    "source_id": "2023.acl_long.802",
    "text": "Table 3 presents dataset statistics, including the number of sentences, holders, targets, and expressions, along with entity overlaps and discontinuities and polarity distribution.\n\n---\n\n---\n\nIn this section, we present the experimental setup and results for our proposed USSA method.\n\n5.1 Datasets and Configuration\nWe conduct experiments on five benchmark datasets across four languages. The datasets include NoReCFine, MultiBEU, MultiBCA, MPQA, and DSUnis, with statistics detailed in Table 3.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 14,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 496,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2023.acl_long.802_s2_c12",
    "source_id": "2023.acl_long.802",
    "text": "5.2 Baseline Methods\nOur method is compared with five state-of-the-art baselines: RACL-BERT, Head-first, Head-final, Frozen PERIN, and TGLS.\n\n5.3 Evaluation Metrics\nWe primarily use Sentiment Graph F1 (SF1) as our evaluation metric, along with Holder F1, Target F1, Exp. F1, and Nonpolarity Sentiment Graph F1 (NSF1).",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 15,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 317,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2023.acl_long.802_s2_c13",
    "source_id": "2023.acl_long.802",
    "text": "5.4 Main Results\nTable 4 shows that USSA generally outperforms other baselines in terms of Span F1. It achieves significant improvements, including a 7.2% F1 score increase for target extraction on MPQA and a 5.4% F1 score increase for holder extraction on NoReCFine. USSA consistently surpasses other methods in NSF1 and SF1 metrics, with average improvements of 3.48 NSF1 and 3.14% SF1 over TGLS.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 16,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 398,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2023.acl_long.802_s3_c0",
    "source_id": "2023.acl_long.802",
    "text": "6.1 Model Component Validity\nAblation experiments in Table 5 reveal that the bi-axial attention module is crucial, with its removal leading to a performance decline. The FFN predictor and biaffine predictor are also found to be effective, with the ∗-NW relations being significant, particularly for datasets with a higher proportion of discontinuous entities.",
    "section_title": "Discussion",
    "section_type": "discussion",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 17,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 6,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 359,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8250000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2023.acl_long.802_s3_c1",
    "source_id": "2023.acl_long.802",
    "text": "6.2 Effectiveness of Bi-axial Attention Module\nThe bi-axial attention module is shown to be effective, as evidenced by the performance drop when replaced with a CNN, as illustrated in Figure 5. Figure 6 visualizes the bi-axial attention scores applied to the E-POS cell.\n\n---",
    "section_title": "Discussion",
    "section_type": "discussion",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 18,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 275,
    "chunk_method": "hierarchical",
    "importance_weight": 0.75,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2023.acl_long.802_s3_c2",
    "source_id": "2023.acl_long.802",
    "text": "Previous research has demonstrated the effectiveness of convolutional neural networks (CNNs) in table filling methods (Li et al., 2022; Yan et al., 2022). However, when the table is large, CNNs may struggle to capture global information quickly (Peng et al., 2021). Our direct comparison with the CNN method of (Li et al., 2022), as shown in Figure 5, indicates that CNN performance decreases across all five datasets. This is likely due to the prevalence of long sentences in SSA tasks. We also visualize bi-axial attention scores applied to the E-POS cell in Figure 6, which shows attention on rel",
    "section_title": "Discussion",
    "section_type": "discussion",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 19,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.75,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2023.acl_long.802_s3_c3",
    "source_id": "2023.acl_long.802",
    "text": "ntion scores applied to the E-POS cell in Figure 6, which shows attention on related relations such as T-S and T-E. In summary, the bi-axial attention mechanism effectively identifies relations in the table.",
    "section_title": "Discussion",
    "section_type": "discussion",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 20,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 207,
    "chunk_method": "hierarchical",
    "importance_weight": 0.75,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2023.acl_long.802_s3_c4",
    "source_id": "2023.acl_long.802",
    "text": "In this paper, we propose a novel bi-lexical dependency parsing graph and convert it into a unified 2D table-filling scheme, USSA, to address overlapping and discontinuous issues simultaneously. Our model includes a bi-axial attention module to refine word-pair representations. This framework may inspire other tasks involving tuple extraction with overlap and discontinuity challenges.",
    "section_title": "Discussion",
    "section_type": "discussion",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 21,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 387,
    "chunk_method": "hierarchical",
    "importance_weight": 0.75,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2023.acl_long.802_s3_c5",
    "source_id": "2023.acl_long.802",
    "text": "Limitations include the increased training time and memory usage associated with the table filling method, due to the 2D table representation of word-pair relations. In comparison, a sequence representation could be more efficient. Our approach also faces computational challenges.\n\nThis work was supported in part by the National Natural Science Foundation of China under Grant 62076032. We appreciate feedback from anonymous reviewers.",
    "section_title": "Discussion",
    "section_type": "discussion",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 22,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 437,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7875000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2023.acl_long.802_s4_c0",
    "source_id": "2023.acl_long.802",
    "text": "[References section remains unchanged]",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 23,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 12,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 38,
    "chunk_method": "hierarchical",
    "importance_weight": 0.44000000000000006,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2023.acl_long.802_s4_c1",
    "source_id": "2023.acl_long.802",
    "text": "Huang et al. (2019) introduced Ccnet for semantic segmentation. Katiyar and Cardie (2016) investigated LSTMs for joint extraction of opinion entities and relations. Li et al. (2022) proposed a unified named entity recognition approach as word-word relation classification. Li et al. (2021a) presented MRN, a network for document-level relation extraction. Li et al. (2021b) employed dual graph convolutional networks for aspect-based sentiment analysis. Li et al. (2019a) and (2019b) explored models for opinion target extraction and target sentiment prediction, as well as the use of BERT for aspec",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 24,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2023.acl_long.802_s4_c2",
    "source_id": "2023.acl_long.802",
    "text": "extraction and target sentiment prediction, as well as the use of BERT for aspect-based sentiment analysis. Ma et al. (2018) conducted joint learning for targeted sentiment analysis.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 25,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 182,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2023.acl_long.802_s4_c3",
    "source_id": "2023.acl_long.802",
    "text": "Mao et al. (2021) introduced a joint training dual-MRC framework for aspect-based sentiment analysis. Øvrelid et al. (2020) presented a fine-grained sentiment dataset for Norwegian. Peng et al. (2020) provided a comprehensive solution for aspect-based sentiment analysis. Peng et al. (2021) proposed Conformer, a method coupling local features with global representations for visual recognition. Pontiki et al. (2016, 2015, 2014) organized SemEval tasks on aspect-based sentiment analysis. Quan et al. (2019) performed end-to-end joint opinion role labeling with BERT.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 26,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 568,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2023.acl_long.802_s4_c4",
    "source_id": "2023.acl_long.802",
    "text": "Samuel et al. (2022) worked on direct parsing to sentiment graphs, and Samuel and Straka (2020) focused on permutation-invariant semantic parsing. Shi et al. (2022) developed an effective token graph modeling strategy for structured sentiment analysis. Toprak et al. (2010) annotated opinions at the sentence and expression level in user-generated discourse. Wang et al. (2020a) introduced Axial-Deeplab for panoptic segmentation. Wang and Pan (2019) proposed a transferable interactive memory network for domain adaptation in fine-grained opinion extraction. Wang et al. (2016, 2017) employed recur",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 27,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2023.acl_long.802_s4_c5",
    "source_id": "2023.acl_long.802",
    "text": "tion in fine-grained opinion extraction. Wang et al. (2016, 2017) employed recursive neural conditional random fields and coupled multi-layer attentions for aspect-based sentiment analysis. Wang et al. (2020b) presented TPLinker, a method for joint extraction of entities and relations through token pair linking.\n---\n\nYucheng Wang, Bowen Yu, Hongsong Zhu, Tingwen Liu, Nan Yu, and Limin Sun.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 28,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 392,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2023.acl_long.802_s4_c6",
    "source_id": "2023.acl_long.802",
    "text": "2021. Discontinuous named entity recognition as maximal clique discovery.\n\nJanyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language.\n\nZhen Wu, Chengcan Ying, Fei Zhao, Zhifang Fan, Xinyu Dai, and Rui Xia. 2020. Grid tagging scheme for aspect-oriented fine-grained opinion extraction.\n\nQingrong Xia, Bo Zhang, Rui Wang, Zhenghua Li, Yue Zhang, Fei Huang, Luo Si, and Min Zhang. 2021. A unified span-based approach for opinion mining with syntactic constituents.\n\nLu Xu, Yew Ken Chia, and Lidong Bing.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 29,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 557,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2023.acl_long.802_s4_c7",
    "source_id": "2023.acl_long.802",
    "text": "2021. Learning span-level interactions for aspect sentiment triplet extraction.\n\nLu Xu, Hao Li, Wei Lu, and Lidong Bing. 2020. Position-aware tagging for aspect sentiment triplet extraction.\n\nHang Yan, Junqi Dai, Tuo Ji, Xipeng Qiu, and Zheng Zhang. 2021. A unified generative framework for aspect-based sentiment analysis.\n\nHang Yan, Yu Sun, Xiaonan Li, and Xipeng Qiu. 2022. An embarrassingly easy but strong baseline for nested named entity recognition.\n\nBowen Yu, Zhenyu Zhang, Jiawei Sheng, Tingwen Liu, Yubin Wang, Yucheng Wang, and Bin Wang.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 30,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 548,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2023.acl_long.802_s4_c8",
    "source_id": "2023.acl_long.802",
    "text": "2021. Semi-open information extraction.\n\nZepeng Zhai, Hao Chen, Fangxiang Feng, Ruifan Li, and Xiaojie Wang. 2022. COM-MRC: A COntext-masked machine reading comprehension framework for aspect sentiment triplet extraction.\n\nBo Zhang, Yue Zhang, Rui Wang, Zhenghua Li, and Min Zhang. 2020. Syntax-aware opinion role labeling with dependency graph convolutional networks.\n\nChen Zhang, Qiuchi Li, Dawei Song, and Benyou Wang.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 31,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 421,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2023.acl_long.802_s4_c9",
    "source_id": "2023.acl_long.802",
    "text": "2020. A multi-task learning framework for opinion triplet extraction.\n\nHyperparameter Settings\n\nGlobal Hyper-parameter Settings\n- Contextualized Embedding: mBERT Embeddings\n- Trainable: False\n- Num of Epochs: 60\n- Batch Size: 16\n- Hidden LSTM Dim: 768\n- Distance Feature Dim: 100\n- Gradient Accumulation Step: 2\n\nLocal Hyper-parameter Settings\n- Dataset: MaxTokenLen, LearningRate α\n  - NoReCFine: 150, 2e-3, 0.650\n  - MultiBEU: 150, 2e-3, 0.500\n  - MultiBCA: 386, 1e-3, 0.650\n  - MPQA: 210, 2e-3, 0.725\n  - DSUnis: 386, 1e-3,",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 32,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 526,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2023.acl_long.802_s4_c10",
    "source_id": "2023.acl_long.802",
    "text": "0.650\n  - MPQA: 210, 2e-3, 0.725\n  - DSUnis: 386, 1e-3, 0.650\n\nACL 2023 Responsible NLP Checklist\n\nA. For every submission:\n- A1: Limitations described on Page 9\n- A2: Not applicable\n- A3: Abstract and introduction summarize main claims\n- A4: Left blank\n\nB. Use or creation of scientific artifacts\n- B1: Cite artifact creators in Section 5.1\n- B2: Not applicable\n- B3: Not applicable\n- B4: Not applicable\n- B5: Not applicable\n- B6: Report relevant statistics in Section 5.1\n\nC. Computational experiments\n- C1: Report model parameters, computational budget, and infrastructure in Section",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 33,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 586,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2023.acl_long.802_s4_c11",
    "source_id": "2023.acl_long.802",
    "text": "5.1\n- C2: Discuss experimental setup and hyperparameter values in Section 5.1 and Appendix A\n- C3: Report descriptive statistics of results in Section 5.1\n- C4: Left blank\n\nD. Use of human annotators or participants\n- D1: Not applicable\n- D2: Not applicable\n- D3: Not applicable\n\n---\n\nThe data collection protocol was not applicable and thus not submitted for ethics review. Similarly, reporting demographic and geographic characteristics of the annotator population was not applicable and left blank.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 34,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 501,
    "chunk_method": "hierarchical",
    "importance_weight": 0.42000000000000004,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2025.coling_main.22_s0_c0",
    "source_id": "2025.coling_main.22",
    "text": "Multimodal Aspect-Based Sentiment Analysis under Conditional Relation\n\nXinjing Liu, Ruifan Li, Shuqin Ye, Guangwei Zhang, Xiaojie Wang",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 4,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 134,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2025.coling_main.22_s1_c0",
    "source_id": "2025.coling_main.22",
    "text": "Multimodal Aspect-Based Sentiment Analysis (MABSA) extracts aspect terms from text-image pairs and identifies their sentiments. Traditional methods assume the image contains objects referred to by the aspects, which is not always true. We propose the COnditional Relation based Sentiment Analysis framework (CORSA), including a conditional relation detector (CRD) and a visual object localizer (VOL).",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 4,
    "chunk_index": 1,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 2,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 400,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9900000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2025.coling_main.22_s1_c1",
    "source_id": "2025.coling_main.22",
    "text": "etector (CRD) and a visual object localizer (VOL). CRD mitigates the impact of unrelated images, while VOL locates condition-related visual regions. We conduct two types of annotations for effective learning: conditional relation and bounding boxes of visual objects. Experiments on our C-MABSA dataset demonstrate CORSA's superior performance.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 4,
    "chunk_index": 2,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 2,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 344,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9450000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2025.coling_main.22_s2_c0",
    "source_id": "2025.coling_main.22",
    "text": "Fine-grained MABSA has gained attention for its application in social media sentiment analysis. It involves three subtasks: Multimodal Aspect Term Extraction (MATE), Multimodal Aspect-oriented Sentiment Classification (MASC), and Joint Multimodal Aspects of Sentiment Analysis (JMASA). Existing methods assume images always contain referred objects, but this is not the case, especially in social media. We introduce CORSA to address this issue by filtering irrelevant visual information and precisely locating condition-related regions.\n\n2 Problem Formulation",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 3,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 2,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 560,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2025.coling_main.22_s2_c1",
    "source_id": "2025.coling_main.22",
    "text": "2 Problem Formulation\n\nWe frame MABSA as a multi-task framework, aiming to extract aspects and sentiment polarities from a tweet containing an image V and a sentence S. We determine conditional relations and detect visual objects, generating bounding boxes and categories.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 4,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 2,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 272,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2025.coling_main.22_s3_c0",
    "source_id": "2025.coling_main.22",
    "text": "3.1 Data Generation for C-MABSA\n---\n\nWe construct datasets for MABSA tasks, specifically C-MABSA with conditional relations. We automatically annotate two popular datasets, TWITTER-15 and TWITTER-17, using UNINEXT for conditional relation detection and YOLOv8 for visual object annotation. UNINEXT generates the probability of an image containing aspects, with thresholds τ1 set for relevance annotation. YOLOv8 detects visual objects, categorizing them as person, object, or background.",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 5,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 29,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 487,
    "chunk_method": "hierarchical",
    "importance_weight": 0.935,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2025.coling_main.22_s3_c1",
    "source_id": "2025.coling_main.22",
    "text": "Our proposed CORSA framework mitigates the impact of irrelevant conditions using a Conditional Relation Detector (CRD), locates condition-related regions with a Visual Object Localizer (VOL), and extracts aspect-sentiment pairs using a multi-modal BART-based sentiment analyzer. The textual encoder uses BART, while the visual encoder employs YOLOv8's backbone for multi-scale feature extraction.",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 6,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 29,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 396,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2025.coling_main.22_s3_c2",
    "source_id": "2025.coling_main.22",
    "text": "CRD applies self-attention and cross-modal attention to detect relevance and filters condition-related visual features. VOL enhances CRD by localizing objects and aligning them with aspects using cross-model attention. We use a gating mechanism to concatenate visual features and optimize with losses for regression and classification.\n\nFigure 2 illustrates the CORSA framework, detailing the process from unimodal feature extraction to the final sentiment analysis.",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 7,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 29,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 466,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2025.coling_main.22_s3_c3",
    "source_id": "2025.coling_main.22",
    "text": "We obtain the condition-aligned visual features ˆHVi = { ˆh1, ..., ˆhj, ..., ˆhm}, i ∈{1, 2, 3}. The visual feature ˆHVi is relevant to aspects and contains the accurate aspect’s visual information. Our Multi-modal Sentiment Analyzer (MSA) encodes multimodal inputs to predict aspects and sentiment. We concatenate three scales of visual features, i.e., ˆH′ = ˆHV1 ⊕WV2 ˆHV2 ⊕WV3 ˆHV3, and use a linear layer to map the concatenated feature to 49-dimensions. The multimodal BART encoder-decoder pred",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 8,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 29,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 499,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2025.coling_main.22_s3_c4",
    "source_id": "2025.coling_main.22",
    "text": "e to 49-dimensions. The multimodal BART encoder-decoder predicts the token probability distribution.",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 9,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 29,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 100,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2025.coling_main.22_s3_c5",
    "source_id": "2025.coling_main.22",
    "text": "For the JMASA task, our CORSA model outperforms state-of-the-art multimodal methods on all metrics on Twitter-15 and Twitter-17 datasets. On MATE, our model achieves the best performance in Twitter-15 and is second to CMMT in Twitter-17. For MASC, our model achieves the best results on both datasets in terms of Accuracy and F1 score.",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 10,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 29,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 335,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2025.coling_main.22_s3_c6",
    "source_id": "2025.coling_main.22",
    "text": "In the ablation study, removing either the Conditional Relation Detection (CRD) or the Visual Object Localization (VOL) module results in a decline in F1 scores. The full CORSA model performs the best, indicating the effectiveness of both modules. Ablation results on different scales of the visual encoder also demonstrate the importance of each scale for the model's performance.",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 11,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 29,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 381,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2025.coling_main.22_s3_c7",
    "source_id": "2025.coling_main.22",
    "text": "We evaluate the impact of conditional relation detection (CRD) and visual object localization (VOL) on model performance. The removal of CRD results in a significant decrease in F1-scores on Twitter-15 and Twitter-17 datasets, indicating the importance of identifying conditional image regions. The decline in performance is more pronounced when removing VOL, suggesting the sequence of processing is crucial—first eliminating unmet conditional image information, then localizing condition-related r",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 12,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 29,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 499,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2025.coling_main.22_s3_c8",
    "source_id": "2025.coling_main.22",
    "text": "ional image information, then localizing condition-related regions. The ablation study on multi-scale features demonstrates their effectiveness, with small-scale features proving particularly beneficial. Our experiments with hyper-parameters λD and λL show that our CORSA model is optimal when λD equals",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 13,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 29,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 303,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2025.coling_main.22_s3_c9",
    "source_id": "2025.coling_main.22",
    "text": "1.0 and λL equals 0.5. The choice of thresholds for annotation data generation is also critical, with different thresholds τ1 and τ2 selected for CRD and VOL, respectively.",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 14,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 29,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 172,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2025.coling_main.22_s3_c10",
    "source_id": "2025.coling_main.22",
    "text": "Comparative experiments with large language models (LLMs) and multi-modal LLMs (MLLMs) on the JMASA and MASC tasks demonstrate that our model outperforms ChatGPT 3.5, Llama 2, VisualGLM-6B, Llava 1.5, MMICL, mPLUG-Owl2, and GPT4V, showcasing the advantage of multimodal input and the effectiveness of our approach. A case study further illustrates the importance of filtering unmet conditional information and the precise localization of condition-related regions for accurate sentiment predictions.",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 15,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 29,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 499,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2025.coling_main.22_s3_c11",
    "source_id": "2025.coling_main.22",
    "text": "MABSA encompasses three tasks: MATE, MASC, and JMASA. Early methods for MATE, such as those by Moon et al. (2018), Arshad et al. (2019), and Wu et al. (2020a), employed cross-modal attention mechanisms, which were insufficient for effectively learning multimodal information. Subsequent methods by Yu et al. (2020), Liu et al. (2022), and Zheng et al. (2023) utilized pre-trained language models and modality translation-based approaches. Yu et al.",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 16,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 29,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 448,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2025.coling_main.22_s3_c12",
    "source_id": "2025.coling_main.22",
    "text": "(2023) extended Multimodal NER to MATE using a generative framework. For MASC, existing methods typically rely on attention mechanisms and graph convolutional networks (GCNs). Notably, Zhang et al. (2021) introduced an attention network with a discriminative mechanism, Xiao et al. (2023) proposed a cross-modal fine-grained alignment and fusion network, and Zhao and Yang (2023) presented a fusion model combining GCN and SE-ResNeXt networks. Wang et al.",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 17,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 29,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 455,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2025.coling_main.22_s3_c13",
    "source_id": "2025.coling_main.22",
    "text": "(2023) addressed the issue of irrelevant aspects in images with an aspect-oriented filtration module.",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 18,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 29,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 101,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2025.coling_main.22_s3_c14",
    "source_id": "2025.coling_main.22",
    "text": "On JMASA, pipeline framework-based methods such as those by Ju et al. (2021) and Yang et al. (2022b) have been proposed, as well as generative models like those by Ling et al. (2022) and Zhou et al.",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 19,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 29,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 198,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2025.coling_main.22_s3_c15",
    "source_id": "2025.coling_main.22",
    "text": "(2022) and Zhou et al. (2023). Liu et al. (2024b) introduced a framework that implicitly calculates the similarity between sentences and images to reduce multi-level modality noise and multi-grained semantic gaps. However, these methods often overlook the conditional multimodal relations between images and texts, and the assumption that images contain objects referred to by text aspects is not always met. In response, we propose the CORSA framework, which explicitly considers this issue.",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 20,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 29,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 492,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2025.coling_main.22_s3_c16",
    "source_id": "2025.coling_main.22",
    "text": "In this paper, we introduce the CORSA framework for MABSA, which includes two key modules: CRD and VO",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 21,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 29,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 101,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2025.coling_main.22_s3_c17",
    "source_id": "2025.coling_main.22",
    "text": "L. CRD is designed to address the impact of the conditional image issue, while VOL aims to locate condition-related visual regions with aspects. We perform two types of annotations on benchmark datasets for training and demonstrate the effectiveness of our CORSA model. Despite achieving significant performance, there is room for improvement, and we plan to explore annotation methods using MLLMs to enhance annotation accuracy.",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 22,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 29,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 429,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2025.coling_main.22_s3_c18",
    "source_id": "2025.coling_main.22",
    "text": "Our method has limitations, including the use of a pre-trained model (UNINEXT) for automatic data annotation, which can introduce inaccuracies. This results in a lack of ground truth for conditional relations and affects the CORSA model's performance, presenting challenges for further research.",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 23,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 29,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 295,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2025.coling_main.22_s3_c19",
    "source_id": "2025.coling_main.22",
    "text": "This work was supported by the National Natural Science Foundation of China under Grant 62076032 and the CCF-Zhipu Large Model Innovation Fund (NO. CCF-Zhipu202407). We thank the anonymous reviewers for their constructive feedback.\n\nYan Ling, Jianfei Yu, and Rui Xia. 2022. Vision-language pre-training for multimodal aspect-based sentiment analysis.\n\nHaotian Liu et al. 2024a. Improved baselines with visual instruction tuning.",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 24,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 29,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 428,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2025.coling_main.22_s3_c20",
    "source_id": "2025.coling_main.22",
    "text": "Luping Liu et al. 2022. Uamner: uncertainty-aware multimodal named entity recognition in social media posts.\n\nYaxin Liu et al. 2024b. Rng: Reducing multi-level noise and multi-grained semantic gap for joint multimodal aspect-sentiment analysis.\n\nIlya Loshchilov and Frank Hutter. 2017. Fixing weight decay regularization in adam.\n\nSeungwhan Moon, Leonardo Neves, and Vitor Carvalho. 2018. Multimodal named entity recognition for short social media posts.",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 25,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 29,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 454,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2025.coling_main.22_s3_c21",
    "source_id": "2025.coling_main.22",
    "text": "Jie Mu et al. 2023. Mocolnet: A momentum contrastive learning network for multimodal aspect-level sentiment analysis.\n\nOpenAI. 2023. ChatGPT: A large language model.\n\nOpenAI. 2024. Gpt-4 technical report.\n\nLin Sun et al. 2021. Rpbert: a text-image relation propagation-based bert model for multimodal ner.\n\nHugo Touvron et al. 2023. Llama 2: Open foundation and fine-tuned chat models.",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 26,
    "chunk_index_in_section": 21,
    "total_chunks_in_section": 29,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 385,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2025.coling_main.22_s3_c22",
    "source_id": "2025.coling_main.22",
    "text": "Qianlong Wang et al. 2023. Image-to-text conversion and aspect-oriented filtration for multimodal aspect-based sentiment analysis.\n\nHanqian Wu et al. 2020a. Multimodal aspect extraction with region-aware alignment network.\n\nZhiwei Wu et al. 2020b. Multimodal representation with embedded visual guiding objects for named entity recognition in social media posts.",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 27,
    "chunk_index_in_section": 22,
    "total_chunks_in_section": 29,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 362,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2025.coling_main.22_s3_c23",
    "source_id": "2025.coling_main.22",
    "text": "Luwei Xiao et al. 2024. Atlantis: Aesthetic-oriented multiple granularities fusion network for joint multimodal aspect-based sentiment analysis.\n\nLuwei Xiao et al. 2023. Cross-modal fine-grained alignment and fusion network for multimodal aspect-based sentiment analysis.\n\nBin Yan et al. 2023. Universal instance perception as object discovery and retrieval.",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 28,
    "chunk_index_in_section": 23,
    "total_chunks_in_section": 29,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 358,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2025.coling_main.22_s3_c24",
    "source_id": "2025.coling_main.22",
    "text": "Bin Yang and Jinlong Li. 2023. Visual elements mining as prompts for instruction learning for target-oriented multimodal sentiment classification.\n\nHao Yang et al. 2022a. Face-sensitive image-to-emotional-text cross-modal translation for multimodal aspect-based sentiment analysis.\n\nJuan Yang et al. 2024. Amifn: Aspect-guided multi-view interactions and fusion network for multimodal aspect-based sentiment analysis.",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 29,
    "chunk_index_in_section": 24,
    "total_chunks_in_section": 29,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 417,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2025.coling_main.22_s3_c25",
    "source_id": "2025.coling_main.22",
    "text": "Li Yang et al. 2022b. Cross-modal multitask transformer for end-to-end multimodal aspect-based sentiment analysis.\n\nQinghao Ye et al. 2024. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration.\n\nJianfei Yu and Jing Jiang. 2019. Adapting bert for target-oriented multimodal sentiment classification.\n\nJianfei Yu et al. 2020. Improving multimodal named entity recognition via entity span detection with unified multimodal transformer.",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 30,
    "chunk_index_in_section": 25,
    "total_chunks_in_section": 29,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 468,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2025.coling_main.22_s3_c26",
    "source_id": "2025.coling_main.22",
    "text": "Jianfei Yu et al. 2023. Grounded multimodal named entity recognition on social media.\n\nZhewen Yu et al. 2022. Dual-encoder transformers with cross-modal alignment for multimodal aspect-based sentiment analysis.\n\nJing Zhang et al. 2024. Mcpl: Multi-model co-guided progressive learning for multimodal aspect-based sentiment analysis.\n\nZhe Zhang et al. 2021. Modalnet: an aspect-level sentiment classification model by exploring multimodal data with fusion discriminant attentional network.",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 31,
    "chunk_index_in_section": 26,
    "total_chunks_in_section": 29,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 488,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2025.coling_main.22_s3_c27",
    "source_id": "2025.coling_main.22",
    "text": "Fei Zhao et al. 2023. M2df: Multi-grained multi-curriculum denoising framework for multimodal aspect-based sentiment analysis.\n\nHaozhe Zhao et al. 2024a. Mmicl: Empowering vision-language model with multi-modal in-context learning.\n\nHua Zhao et al. 2024b. A survey on multimodal aspect-based sentiment analysis.\n\nJun Zhao and Fuping Yang. 2023. Fusion with gcn and se-resnext network for aspect based multimodal sentiment analysis.",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 32,
    "chunk_index_in_section": 27,
    "total_chunks_in_section": 29,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 431,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2025.coling_main.22_s3_c28",
    "source_id": "2025.coling_main.22",
    "text": "Changmeng Zheng et al. 2023. Rethinking multimodal entity and relation extraction from a translation point of view.\n\nRu Zhou, Wenya Guo, Xumeng Liu, Shenglong Yu, Ying Zhang, and Xiaojie Yuan. 2023. Aom: Detecting aspect-oriented information for multimodal aspect-based sentiment analysis. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 33,
    "chunk_index_in_section": 28,
    "total_chunks_in_section": 29,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 402,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8925,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2408.03632v3_s0_c0",
    "source_id": "2408.03632v3",
    "text": "Concept Conductor: Orchestrating Multiple Personalized Concepts in Text-to-Image Synthesis\n\nZebin Yao, Fangxiang Feng, Ruifan Li, Xiaojie Wang\n\nBeijing University of Posts and Telecommunications",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 5,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 194,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2408.03632v3_s1_c0",
    "source_id": "2408.03632v3",
    "text": "The customization of text-to-image models has seen significant advancements, yet generating multiple personalized concepts remains challenging. Current methods face issues of attribute leakage and layout confusion, reducing concept fidelity and semantic consistency. We introduce Concept Conductor, a training-free framework that ensures visual fidelity and correct layout in multi-concept customizat",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 5,
    "chunk_index": 1,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 3,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 400,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9900000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2408.03632v3_s1_c1",
    "source_id": "2408.03632v3",
    "text": "ity and correct layout in multi-concept customization. It isolates sampling processes to prevent attribute leakage and uses self-attention-based spatial guidance for layout correction. A concept injection technique with shape-aware masks specifies generation areas and ensures harmony through feature fusion. Experiments demonstrate that Concept Conductor generates composite images with accurate lay",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 5,
    "chunk_index": 2,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 400,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2408.03632v3_s1_c2",
    "source_id": "2408.03632v3",
    "text": "uctor generates composite images with accurate layouts and preserved visual details, outperforming existing baselines.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 5,
    "chunk_index": 3,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 118,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9450000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2408.03632v3_s2_c0",
    "source_id": "2408.03632v3",
    "text": "Text-to-image diffusion models have achieved remarkable success, with personalization techniques also advancing. Single-concept customization methods have been proposed, but handling multiple concepts is challenging, with issues like attribute leakage and layout confusion. Our Concept Conductor framework integrates multiple personalized concepts into a single image based on given text prompts, comprising multipath sampling, layout alignment, and concept injection. It prevents interference between concepts, ensures correct layouts, and maintains visual harmony. We evaluate our method with a new",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 4,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 2,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 600,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2408.03632v3_s2_c1",
    "source_id": "2408.03632v3",
    "text": "correct layouts, and maintains visual harmony. We evaluate our method with a new dataset and a fine-grained metric, showing improved concept fidelity and alignment with textual semantics.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 5,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 2,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 187,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2408.03632v3_s3_c0",
    "source_id": "2408.03632v3",
    "text": "Text-to-Image Diffusion Models\n\nText-to-image diffusion models, such as GLIDE, DALL-E 2, Imagen, and Stable Diffusion, have become the主流 approach in generating realistic images. However, these models struggle with understanding the relationships between multiple concepts, particularly when dealing with visually similar concepts. We apply our method to the publicly available Stable Diffusion, which operates in the latent space of a Variational Autoencoder (VAE).",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 6,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 18,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 465,
    "chunk_method": "hierarchical",
    "importance_weight": 0.66,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2408.03632v3_s3_c1",
    "source_id": "2408.03632v3",
    "text": "Customization in text-to-image (T2I) diffusion models has been explored through fine-tuning and optimizing text embeddings, such as DreamBooth, Textual Inversion, P+, and NeTI. Frameworks combining multiple single-concept models and introducing manually defined layouts in attention maps have also been proposed. However, these methods may still result in interference between concepts and layout control failures.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 7,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 18,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 414,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2408.03632v3_s3_c2",
    "source_id": "2408.03632v3",
    "text": "For precise spatial control in T2I diffusion models, additional modules have been trained to generate controllable images using layout conditions, such as bounding boxes, keypoints, and depth maps. Attention layer manipulation and gradient-based methods align the generated image layout but may lead to detail loss and quality degradation.\n\nOur method proposes the use of layout information from a reference image as a supervisory signal for stable layout control while preserving the diversity of generated subjects' poses.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 8,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 18,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 524,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2408.03632v3_s3_c3",
    "source_id": "2408.03632v3",
    "text": "In the LDM architecture, self-attention controls image structure, while cross-attention integrates textual information. Our proposed Concept Conductor corrects input latent vectors using the Layout Alignment module, then applies Concept Injection for denoising, utilizing the Multipath Sampling structure.\n\nED-LoRA, a method for single-concept customization, is used as our default single-concept model, involving learnable hierarchical text embeddings and low-rank adaptation (LoRA) applied to pre-trained weights.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 9,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 18,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 515,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2408.03632v3_s3_c4",
    "source_id": "2408.03632v3",
    "text": "Our method consists of three components: multipath sampling, layout alignment, and concept injection. Multipath sampling is designed to utilize multiple existing single-concept models for composite generation without attribute leakage. It incorporates a base model and multiple custom models, each maintaining independent denoising processes.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 10,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 18,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 342,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2408.03632v3_s3_c5",
    "source_id": "2408.03632v3",
    "text": "Layout alignment addresses layout confusion by using a reference image to correct the layout during generation. We propose a gradient-guided approach where DDIM inversion is performed on the reference image to obtain its latent space representation, and an optimization objective encourages the generated image's layout to align with the given layout.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 11,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 18,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 351,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2408.03632v3_s3_c6",
    "source_id": "2408.03632v3",
    "text": "Concept injection follows layout alignment. The corrected latents are used in multipath sampling to generate the next latents. We introduce an attention-based concept injection technique, which includes feature fusion and mask refinement. This ensures the fidelity of the target concepts and avoids disharmonious images by fusing output feature maps of attention layers with refined masks.\n\nThe key processes can be summarized as follows:",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 12,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 18,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 438,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2408.03632v3_s3_c7",
    "source_id": "2408.03632v3",
    "text": "The key processes can be summarized as follows:\n\n- Multipath Sampling: Independent denoising processes for base and custom models with edited prompts to focus on single concepts.\n- Layout Alignment: Gradient-guided approach using DDIM inversion and self-attention features to align the layout of the generated image with the reference image.\n- Concept Injection: Feature fusion and mask refinement to harmoniously combine the output of base and custom models for composite image generation.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 13,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 18,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 490,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2408.03632v3_s3_c8",
    "source_id": "2408.03632v3",
    "text": "In our approach, the fused feature ht is derived from the attention layers of both the base model and custom models, where M base t = 1 − NS and M Vi t represents the binary mask for concept Vi at timestep t. This fusion addresses potential inaccuracies in predefined masks due to uncertain poses of generated subjects. We employ mask refinement, inspired by local-prompt-mixing, using self-attention-based semantic segmentation to adjust masks according to the shapes and poses of target subjects.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 14,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 18,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 498,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2408.03632v3_s3_c9",
    "source_id": "2408.03632v3",
    "text": "Our dataset encompasses 30 personalized concepts across various categories, with images sourced from the Mix-of-show, DreamBooth, and CustomConcept101 datasets. For quantitative evaluation, we generate 400 images per method using Chat-GPT and select 10 visually similar concept pairs.\n\nOur method is implemented on Stable Diffusion v1.5, utilizing layout references from SDXL. We use the U-Net decoder's sixth self-attention layer for mask refinement and set the weighting coefficient α to 1 and the gradient descent step size λ to 10.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 15,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 18,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 535,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2408.03632v3_s3_c10",
    "source_id": "2408.03632v3",
    "text": "We compare our method against three baselines: Custom Diffusion, Cones 2, and Mix-of-Show, using their official code implementations. Evaluation metrics include text alignment using CLIP and ImageReward, and a new metric called Segmentation Similarity (SegSim) for image alignment.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 16,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 18,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 281,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2408.03632v3_s3_c11",
    "source_id": "2408.03632v3",
    "text": "Quantitative comparisons in challenging scenarios show that our method outperforms Mix-of-Show and others in retaining visual details and handling complex layouts, with lower attribute leakage and layout confusion. Table 1 presents a comparison of these multi-concept customization methods, demonstrating our approach's superior performance.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 17,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 18,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 341,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2408.03632v3_s3_c12",
    "source_id": "2408.03632v3",
    "text": "Our Concept Conductor generates all target concepts with high fidelity without leakage through multipath sampling and concept injection, ensuring correct layout via layout alignment. It maintains stable performance across different concept combinations, even with similar targets such as two teddy bears. We compare our method with Mix-of-Show in more challenging scenarios, showing that Mix-of-Show experiences attribute leakage and concept omission, and struggles with dense layouts.\n\nQuantitative Comparison:",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 18,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 18,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 511,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2408.03632v3_s3_c13",
    "source_id": "2408.03632v3",
    "text": "Quantitative Comparison:\n\nConcept Conductor significantly outperforms previous methods in both image alignment and text alignment, as shown in Table 1. This is primarily due to our multipath sampling framework and attention-based concept injection. The significant reduction in omission and redundancy rates supports this.\n\nAblation Study:",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 19,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 18,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 339,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2408.03632v3_s3_c14",
    "source_id": "2408.03632v3",
    "text": "Ablation Study:\n\nWe conduct qualitative and quantitative comparisons to verify the effectiveness of the proposed components (Figure 8 and Table 2). Removing layout alignment, self-attention, cross-attention, or mask refinement leads to a decline in performance, indicating the importance of each component for preserving visual details and achieving correct layouts.\n\nConclusion:",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 20,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 18,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 379,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2408.03632v3_s3_c15",
    "source_id": "2408.03632v3",
    "text": "Conclusion:\n\nConcept Conductor addresses attribute leakage and layout confusion in multi-concept personalization using multipath sampling, layout alignment, and concept injection. Experimental results demonstrate its consistent generation of composite images with correct layouts and fully preserved concept attributes.\n\n---",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 21,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 18,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 324,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2408.03632v3_s3_c16",
    "source_id": "2408.03632v3",
    "text": "---\n\nIn this section, we reference key works on text-guided image generation and editing, with a focus on stable diffusion models and attention mechanisms. Notably, studies by Liu et al. (2024), Ma et al. (2024), and Mou et al. (2024) contribute to the understanding and control of object placement and controllable abilities in text-to-image diffusion models. The works of Nichol et al. (2021), OpenAI (2023), and Oquab et al. (2023) introduce significant advancements in unsupervised learning and robust visual feature extraction.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 22,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 18,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 532,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2408.03632v3_s3_c17",
    "source_id": "2408.03632v3",
    "text": "Our experimental settings involve the selection of 30 personalized concepts, including real humans, anime humans, animals, buildings, and common objects. We utilize ChatGPT to generate diverse text prompts for quantitative evaluation. The chosen scenes for evaluation cover a wide range of indoor and outdoor settings.\n\n---",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 23,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 18,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 323,
    "chunk_method": "hierarchical",
    "importance_weight": 0.63,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2408.03632v3_s4_c0",
    "source_id": "2408.03632v3",
    "text": "The supplementary material provides detailed experimental procedures and analyses. Appendix A describes the datasets, implementation details, and the proposed evaluation metric SegSim. Additional experimental results are presented in Appendix B, with limitations analyzed in Appendix C. Finally, Appendix D discusses the potential societal impacts of our approach.\n\nA Experimental Settings\nA.1 Datasets",
    "section_title": "Appendix",
    "section_type": "default",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 24,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 17,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 402,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2408.03632v3_s4_c1",
    "source_id": "2408.03632v3",
    "text": "A Experimental Settings\nA.1 Datasets\n\nThirty personalized concepts are chosen, and 10 pairs of visually similar concepts are selected for quantitative evaluation. ChatGPT is used to generate text prompts for each concept pair, incorporating various scenes.\n\n---\n\nFigure 9 illustrates the diverse scenes used in the prompts, encompassing both indoor and outdoor environments.\n\n---",
    "section_title": "Appendix",
    "section_type": "default",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 25,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 379,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2408.03632v3_s4_c2",
    "source_id": "2408.03632v3",
    "text": "---\n\nIn both qualitative and quantitative comparisons, the original prompts are adapted for different methods. For Custom Diffusion, concepts are represented as \"modifier+class\" (e.g., \"<monster> toy\"). Prompts for Cones 2 use two-word phrases (e.g., \"monster toy\"), while Mix-of-Show uses two tokens (e.g., \"<monster toy 1> <monster toy 2>\"). Our approach, Concept Concept, follows Mix-of-Show but includes a base prompt and two variants.",
    "section_title": "Appendix",
    "section_type": "default",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 26,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 439,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2408.03632v3_s4_c3",
    "source_id": "2408.03632v3",
    "text": "We use Stable Diffusion v1.5 as the base model with community-trained weights. Chilloutmix1 and Anything-v42 are used for real-world and anime concept images, respectively. Experiments are conducted using 200-step DDIM sampling, with a guidance scale of 7.5. For evaluation, we generate 8 images per prompt with fixed random seeds.\n\nED-LoRA is applied to both U-Net and text encoder attention layers, using Extended Textual Inversion for layer-wise embeddings. Training hyperparameters match the original paper, and during inference, LoRA weights are integrated with a coefficient of 0.7.",
    "section_title": "Appendix",
    "section_type": "default",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 27,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 588,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2408.03632v3_s4_c4",
    "source_id": "2408.03632v3",
    "text": "SDXL generates layout reference images, which undergo 1000 steps of DDIM inversion for self-attention keys as supervision. Layout alignment is limited to steps 0 to 60 to preserve concept structure.\n\nMask refinement uses self-attention maps to dynamically adjust masks for feature fusion. Clustering and segmentation matching degree computations are performed to select the best segmentation for each concept. Overlapping regions are resolved, as detailed in Algorithm 1. Mask refinement occurs every 5 steps from 50 to 80.",
    "section_title": "Appendix",
    "section_type": "default",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 28,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 523,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2408.03632v3_s4_c5",
    "source_id": "2408.03632v3",
    "text": "We propose Segmentation Similarity (SegSim) as a metric to evaluate visual consistency between generated images and personalized concepts. Brief prompts are used with Grounded-SAM to extract segments from the generated and reference images. Similarity calculations are performed on these segments, as shown in Algorithm 2.\n\n---",
    "section_title": "Appendix",
    "section_type": "default",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 29,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 327,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2408.03632v3_s4_c6",
    "source_id": "2408.03632v3",
    "text": "---\n\nThe similarity between the generated image and each reference image for a concept is calculated, with the maximum value serving as the similarity score for that concept. In cases with multiple reference images, an average score is determined. The final image alignment score is obtained by averaging the similarities across all target concepts. This process is illustrated in Figure 11 and detailed in Algorithm 2.\n\n**B. Additional Experiments**\n\n**B.1 Visualizations**",
    "section_title": "Appendix",
    "section_type": "default",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 30,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 474,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2408.03632v3_s4_c7",
    "source_id": "2408.03632v3",
    "text": "**B. Additional Experiments**\n\n**B.1 Visualizations**\n\n*Visualization of Layout Alignment*\nThe attention probabilities during sampling are visualized to illustrate layout alignment. Self-attention regions refine over time, capturing the image's general layout initially and structural details later. Layout alignment is limited to the first 60 steps for learning the correct layout while preserving structural features. Cross-attention visualization reveals that layout alignment prevents concept omission or merging by encouraging attention activation across multiple locations.",
    "section_title": "Appendix",
    "section_type": "default",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 31,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 579,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2408.03632v3_s4_c8",
    "source_id": "2408.03632v3",
    "text": "*Visualization of Mask Refinement*\nMasks for feature fusion are initialized from reference image segmentations and refined between steps 50 and 80. Mask refinement ensures that each subject's area is dynamically located on the attention map, facilitating the injection of target concept visual features into the generated image.\n\n**B.2 Applications**\n\n*Collage-to-Image Generation*\nOur method allows users to create a collage as a layout reference, enabling the generation of images with custom and complex layouts.",
    "section_title": "Appendix",
    "section_type": "default",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 32,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 515,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2408.03632v3_s4_c9",
    "source_id": "2408.03632v3",
    "text": "*Object Placement*\nCombined with inpainting techniques, our method can replace or add objects in a given image, seamlessly integrating custom concepts into the scene.\n\n**B.3 User Study**\n\nA user study is conducted to assess preferences for generated images based on text and image alignment. Our method receives the highest ratings for both aspects, indicating superior performance compared to baselines.\n\n**B.4 More Qualitative Comparisons**\n\n---",
    "section_title": "Appendix",
    "section_type": "default",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 33,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 447,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2408.03632v3_s4_c10",
    "source_id": "2408.03632v3",
    "text": "**B.4 More Qualitative Comparisons**\n\n---\n\n[Please note that the figures and algorithms mentioned in the text are not included as per the instruction to only clean the text content and not generate a complete paper structure.]",
    "section_title": "Appendix",
    "section_type": "default",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 34,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 226,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2408.03632v3_s4_c11",
    "source_id": "2408.03632v3",
    "text": "Figure 16 exhibits further qualitative comparisons between our method and the baselines. Our approach maintains correct attributes and layouts across diverse scenarios, whereas the baselines experience significant attribute leakage and layout confusion. We also compare the performance of Mix-of-Show (Gu et al. 2024) and our method in handling more than two concepts, as illustrated in Figure 17. Mix-of-Show struggles with missing or mixed subjects as the number of concepts increases. In contrast, our method generates all concepts accurately and without confusion, showcasing its effectiveness i",
    "section_title": "Appendix",
    "section_type": "default",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 35,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2408.03632v3_s4_c12",
    "source_id": "2408.03632v3",
    "text": "es all concepts accurately and without confusion, showcasing its effectiveness in attribute preservation and layout control.\nLimitations",
    "section_title": "Appendix",
    "section_type": "default",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 36,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 136,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2408.03632v3_s4_c13",
    "source_id": "2408.03632v3",
    "text": "Our method experiences quality issues when generating small subjects, such as distorted and deformed small faces. This issue, also present in Mix-of-Show (Gu et al. 2024), primarily stems from the VAE's loss of visual details during image compression. Upgrading the base model, increasing image resolution, or altering the layout may mitigate this problem. Furthermore, our method introduces considerable computational overhead. To manage high memory usage from parallel sampling of multiple custom models, we load different concepts’ ED-LoRA weights alternately at each timestep, which diminishes i",
    "section_title": "Appendix",
    "section_type": "default",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 37,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2408.03632v3_s4_c14",
    "source_id": "2408.03632v3",
    "text": "erent concepts’ ED-LoRA weights alternately at each timestep, which diminishes inference efficiency. Sampling with back-propagation to update latent representations further adds to the latency.\nSocial Impacts",
    "section_title": "Appendix",
    "section_type": "default",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 38,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 208,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2408.03632v3_s4_c15",
    "source_id": "2408.03632v3",
    "text": "Our Concept Conductor represents a significant innovation in text-to-image generation, especially in multi-concept customization. It generates images with correct layouts, incorporating all target concepts while preserving their original characteristics and visual features, thus avoiding layout confusion and attribute leakage. This technology can offer users efficient creative tools, fostering artistic exploration and innovation, and potentially influencing fields like advertising, entertainment, and education. However, the powerful image generation capability may also be misused for unethica",
    "section_title": "Appendix",
    "section_type": "default",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 39,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2408.03632v3_s4_c16",
    "source_id": "2408.03632v3",
    "text": "wever, the powerful image generation capability may also be misused for unethical activities, including image forgery and privacy invasion. It is crucial to implement ethical reviews and safeguards to prevent misuse and protect the public. Ongoing research should address these ethical and security concerns to ensure the technology's responsible application.",
    "section_title": "Appendix",
    "section_type": "default",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 40,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 359,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s0_c0",
    "source_id": "2504.15958v2",
    "text": "FreeGraftor: Training-Free Cross-Image Feature Grafting for Subject-Driven Text-to-Image Generation\n\nZebin Yao, Lei Ren, Huixing Jiang, Chen Wei, Xiaojie Wang, Ruifan Li, Fangxiang Feng\n\nBeijing University of Posts and Telecommunications, Li Auto Inc.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 6,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 3,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 251,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s0_c1",
    "source_id": "2504.15958v2",
    "text": "Subject-driven image generation synthesizes novel scenes that preserve subject identity from reference images under textual guidance. Existing methods face a trade-off between fidelity and efficiency. We propose FreeGraftor, a training-free framework that uses cross-image feature grafting. FreeGraftor employs semantic matching and position-constrained attention fusion to transfer visual details and includes a noise initialization strategy for geometry priors. Our method demonstrates precise subject identity transfer and text-aligned scene synthesis without fine-tuning or additional training,",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 6,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s0_c2",
    "source_id": "2504.15958v2",
    "text": "er and text-aligned scene synthesis without fine-tuning or additional training, outperforming zero-shot and training-free approaches. It also extends to multi-subject generation.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 6,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 178,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s1_c0",
    "source_id": "2504.15958v2",
    "text": "Text-to-image generation has sparked interest in personalized content creation. Diffusion models can generate diverse imagery but struggle with preserving subject identity without costly optimization. Current solutions face a dilemma between fidelity and efficiency. Tuning-based methods are resource-intensive, while zero-shot methods lack fine visual details. Pre-trained text-to-image models have strong visual representation capabilities that are underutilized. We propose FreeGraftor, a training-free framework that taps into the base model's feature space for cross-image knowledge transfer. Fr",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 6,
    "chunk_index": 3,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 3,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 600,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s1_c1",
    "source_id": "2504.15958v2",
    "text": "taps into the base model's feature space for cross-image knowledge transfer. FreeGraftor resolves the fidelity-efficiency dilemma in subject-driven generation and maintains competitive efficiency. It preserves individual identities in multi-subject compositions without conflicts or confusion.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 6,
    "chunk_index": 4,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 293,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s1_c2",
    "source_id": "2504.15958v2",
    "text": "---\n\nWe present FreeGraftor, a framework for subject-driven image generation that maintains pixel-level detail and allows for flexible text guidance without fine-tuning or training. Our cross-image feature grafting technique transfers visual features from reference subjects to corresponding regions in the generated images through semantic matching and position-constrained attention fusion. We also introduce a noise initialization strategy that preserves the reference geometry for extending our method to multi-subject scenarios.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 6,
    "chunk_index": 5,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 533,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s2_c0",
    "source_id": "2504.15958v2",
    "text": "2.1 Multimodal-Diffusion Transformer\n\nText-to-image diffusion models, such as DALL-E 2, Imagen, GLIDE, and eDiff-I, have shown remarkable success. Open-source models like Stable Diffusion and SDXL use U-Net architecture with cross-attention. Diffusion Transformer (DiT) replaces U-Net with a Transformer, demonstrating better scalability and performance. Advanced models like Stable Diffusion 3 and FLUX.1 project text and image tokens into a unified space for joint attention, forming the Multimodal-Diffusion Transformer (MM-DiT). We build upon FLUX.1, where joint attention is computed as:",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 2,
    "total_sections": 6,
    "chunk_index": 6,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 4,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 592,
    "chunk_method": "hierarchical",
    "importance_weight": 0.66,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s2_c1",
    "source_id": "2504.15958v2",
    "text": "𝑄= h 𝑄txt;𝑄imgi , 𝐾= h 𝐾txt;𝐾imgi , 𝑉= h 𝑉txt;𝑉imgi \n\n˜𝑄= 𝑃𝐸⊙𝑄, ˜𝐾= 𝑃𝐸⊙𝐾 \n\n𝐴= softmax \u0012 ˜𝑄˜𝐾⊤\n\n\u0013 , 𝐻out = 𝐴𝑉 \n\n2.2 Subject-Driven Generation",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 2,
    "total_sections": 6,
    "chunk_index": 7,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 4,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 140,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s2_c2",
    "source_id": "2504.15958v2",
    "text": "𝐴= softmax \u0012 ˜𝑄˜𝐾⊤\n\n\u0013 , 𝐻out = 𝐴𝑉 \n\n2.2 Subject-Driven Generation\n\nSubject-driven generation methods can be categorized into tuning-based and zero-shot. Tuning-based methods require per-subject fine-tuning or iterative learning of unique embeddings. Zero-shot methods introduce additional modules for integrating reference subject information but may sacrifice fine details. Training-free approaches like FreeCustom and DiptychPrompting aim for plug-and-play generation but have limitations such as visual distortion and memory overhead.\n\n2.3 Appearance Transfer",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 2,
    "total_sections": 6,
    "chunk_index": 8,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 4,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 562,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s2_c3",
    "source_id": "2504.15958v2",
    "text": "2.3 Appearance Transfer\n\nAppearance transfer methods combine the appearance of one image with the structure of another. Techniques like Cross-Image, Dragon-diffusion, and DiffEditor inject key-value pairs, while DIFT and Eye-for-an-Eye identify semantic correspondences for feature transfer. However, these are primarily style transfer methods and may not preserve the geometric structure needed for subject-driven generation.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 2,
    "total_sections": 6,
    "chunk_index": 9,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 4,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 426,
    "chunk_method": "hierarchical",
    "importance_weight": 0.63,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s3_c0",
    "source_id": "2504.15958v2",
    "text": "3.1 Overview\n\nOur framework consists of three stages: collage construction, inversion, and generation. We construct a collage, invert it to obtain initialization noise, and extract reference features, which are then injected into the generation process.\n\n3.2 Semantic-Aware Feature Grafting\n\nWe propose the Semantic-Aware Feature Grafting (SAFG) module, which establishes semantic correspondences and fuses features through position-constrained attention without additional training costs.\n\n---\n3.2.1 Semantic Matching",
    "section_title": "Method",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 6,
    "chunk_index": 10,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 17,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 518,
    "chunk_method": "hierarchical",
    "importance_weight": 0.935,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s3_c1",
    "source_id": "2504.15958v2",
    "text": "Early appearance transfer methods directly concatenate keys and values of reference and generated images for cross-image attention. However, this does not guarantee semantic correspondences and may lead to structural distortions. We extend the DIFT approach, which uses cosine similarity-based semantic matching in U-Net-based diffusion models, to MM-DiT-based diffusion models. Given a reference image 𝐼ref and a binary mask 𝑀ref, we extract attention features using rectified flow inversion and ma",
    "section_title": "Method",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 6,
    "chunk_index": 11,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 499,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s3_c2",
    "source_id": "2504.15958v2",
    "text": "act attention features using rectified flow inversion and match reference 𝐻ref and generated 𝐻gen features. We compute cosine similarity between reference patch 𝑖 and all generated patches 𝑗:",
    "section_title": "Method",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 6,
    "chunk_index": 12,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 191,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s3_c3",
    "source_id": "2504.15958v2",
    "text": "𝑠𝑖𝑗= 𝐻img,ref 𝑖 · 𝐻img,gen 𝑗 / ∥𝐻img,ref 𝑖 ∥2∥𝐻img,gen 𝑗 ∥2\n\nErase Segment & Paste\n...\nFigure 3: Overview of FreeGraftor...\n\nWe apply two filtering strategies: Similarity Threshold Filtering and Cycle Consistency Filtering. This yields the final reference image mask:\n\n𝑀ref = 𝑀ref, pre ⊙𝑀ref, sim ⊙𝑀ref, consi",
    "section_title": "Method",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 6,
    "chunk_index": 13,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 309,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s3_c4",
    "source_id": "2504.15958v2",
    "text": "𝑀ref = 𝑀ref, pre ⊙𝑀ref, sim ⊙𝑀ref, consi\n\n3.2.2 Position-Constrained Attention Fusion\nAfter obtaining the filtered reference mask 𝑀ref, we transfer features by aligning position embeddings. We concatenate keys and values of the reference patch with those of the generated patch, along with the position embeddings:\n\n𝐾cat = [𝐾txt;𝐾img,gen;𝐾img,ref 𝑀 ],\n𝑉cat = [𝑉txt;𝑉img,gen;𝑉img,ref 𝑀 ]\n\nFigure 4: Illustration of the Proposed Semantic-aware Feature Grafting (SAFG) Module...",
    "section_title": "Method",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 6,
    "chunk_index": 14,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 475,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s3_c5",
    "source_id": "2504.15958v2",
    "text": "For each retained reference patch, we form a new position embedding:\n\n𝑃𝐸cat = [𝑃𝐸txt; 𝑃𝐸img,gen; {𝑃𝐸img,gen 𝑚(𝑖) | 𝑀ref 𝑖 = 1}]\n\nThe revised attention computation is:\n\n˜𝑄= 𝑃𝐸⊙𝑄, ˜𝐾cat = 𝑃𝐸cat ⊙𝐾cat\n\n𝐴+ = softmax \u0012 ˜𝑄( ˜𝐾cat)⊤\n\u0013 , 𝐻+ out = 𝐴+𝑉cat\n\nOur Semantic-Aware Feature Grafting module binds features between matching patches, facilitating cross-image feature grafting.",
    "section_title": "Method",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 6,
    "chunk_index": 15,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 373,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s3_c6",
    "source_id": "2504.15958v2",
    "text": "3.3 Structure-Consistent Initialization\nTo maintain structural integrity and enhance the robustness of semantic matching, we propose a structure-consistent approach.\n\n---",
    "section_title": "Method",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 6,
    "chunk_index": 16,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 170,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s3_c7",
    "source_id": "2504.15958v2",
    "text": "Our initialization strategy creates a collage of the reference subject and employs an inversion technique to derive the initial noise for the generation phase. We use a base text-to-image model to generate a template image that represents the scene described in the text prompt. The reference subject is preserved by replacing the corresponding subject in the template image. This involves localizing the subject with Grounding DINO, segmenting it with SAM, and inpainting with LaMa. The collage bec",
    "section_title": "Method",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 6,
    "chunk_index": 17,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 499,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s3_c8",
    "source_id": "2504.15958v2",
    "text": "nting it with SAM, and inpainting with LaMa. The collage becomes the new reference, and we invert it to obtain initial noise, recording the diffusion trajectory for feature grafting.",
    "section_title": "Method",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 6,
    "chunk_index": 18,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 182,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s3_c9",
    "source_id": "2504.15958v2",
    "text": "To prevent overfitting to the reference subject's pose, we introduce a dynamic feature dropout strategy. We reduce feature injection in early diffusion steps while retaining late-step features, using a dropout probability 𝑝= 𝜔𝑡, with 𝜔 controlling the intensity.",
    "section_title": "Method",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 6,
    "chunk_index": 19,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 262,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s3_c10",
    "source_id": "2504.15958v2",
    "text": "Our method is implemented on FLUX.1-dev and compared with previous subject-driven generation approaches. We use datasets from DreamBench, CustomConcept101, and Mix-of-Show for qualitative evaluation and generate 3,000 images for quantitative evaluation using CLIP and DINOv2 metrics.",
    "section_title": "Method",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 6,
    "chunk_index": 20,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 283,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s3_c11",
    "source_id": "2504.15958v2",
    "text": "FreeGraftor, our method, demonstrates superior image and text alignment compared to prior methods. It effectively preserves the identity of reference subjects through structure-consistent initialization and feature grafting. The gentle attention fusion strategy minimizes perturbations to the attention computation.",
    "section_title": "Method",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 6,
    "chunk_index": 21,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 315,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s3_c12",
    "source_id": "2504.15958v2",
    "text": "Quantitative results show that FreeGraftor outperforms other methods in both image and text alignment. Qualitative results for single-subject generation reveal pixel-accurate detail preservation, and ablation variants highlight the importance of each component of our method.\n\n---\n\n---",
    "section_title": "Method",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 6,
    "chunk_index": 22,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 285,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s3_c13",
    "source_id": "2504.15958v2",
    "text": "---\n\n---\n\n4.3.2 Multi-Subject Generation\nWe compare FreeGraftor with FreeCustom, MS-Diffusion, and OmniGen, as IP-Adapter and DiptychPrompting do not support multi-subject generation. FreeCustom suffers from severe attribute confusion, while MS-Diffusion and OmniGen lack visual faithfulness for small objects. FreeGraftor maintains all reference details and allows flexible text guidance.\n4.4 Ablation Study\n4.4.1 Component Ablation",
    "section_title": "Method",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 6,
    "chunk_index": 23,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 433,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s3_c14",
    "source_id": "2504.15958v2",
    "text": "We validate the effectiveness of our method's components through several ablation variants. Removing structure-consistent initialization leads to appearance inheritance without structural consistency. The absence of feature grafting results in the lack of identifiable correspondence with the reference subjects. Omitting semantic matching causes rigid poses and artifacts. Replacing position-constrained attention fusion with direct feature replacement introduces incoherence and distortions. Table",
    "section_title": "Method",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 6,
    "chunk_index": 24,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 499,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s3_c15",
    "source_id": "2504.15958v2",
    "text": "re replacement introduces incoherence and distortions. Table 2 quantitatively confirms the significance of structure-consistent initialization and feature grafting for image alignment.",
    "section_title": "Method",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 6,
    "chunk_index": 25,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 184,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s3_c16",
    "source_id": "2504.15958v2",
    "text": "4.4.2 Hyperparameter Analysis\nAdditional ablation experiments analyze the impact of similarity threshold (𝜏) and cycle consistency threshold (𝛿) on semantic matching. Proper thresholds alleviate mismatches, while overly strict ones discard critical details. Moderate dropout enhances pose flexibility while preserving subject identity, but excessive dropout intensities erode identity retention.",
    "section_title": "Method",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 6,
    "chunk_index": 26,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 395,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8925,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s4_c0",
    "source_id": "2504.15958v2",
    "text": "FreeGraftor is a training-free framework for subject-driven generation, utilizing Semantic-Aware Feature Grafting and Structure-Consistent Initialization. It achieves pixel-level detail preservation and flexible text guidance without training or test-time optimization, and extends to multi-subject generation, preserving all visual details.",
    "section_title": "Conclusion",
    "section_type": "conclusion",
    "section_index": 4,
    "total_sections": 6,
    "chunk_index": 27,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 341,
    "chunk_method": "hierarchical",
    "importance_weight": 0.935,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s5_c0",
    "source_id": "2504.15958v2",
    "text": "---\n\n---\n\nYuval Alaluf et al. (2024) introduced cross-image attention for zero-shot appearance transfer. Alaluf et al. (2023) also presented a neural space-time representation for text-to-image personalization. Moab Arar et al. (2023) proposed a domain-agnostic tuning-encoder for fast personalization of text-to-image models. Yogesh Balaji et al. (2022) presented ediff-i, a text-to-image diffusion model with an ensemble of expert denoisers. Yingying Deng et al. (2024) introduced FireFlow for fast inversion of rectified flow for image semantic editing.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 28,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 30,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 556,
    "chunk_method": "hierarchical",
    "importance_weight": 0.44000000000000006,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s5_c1",
    "source_id": "2504.15958v2",
    "text": "Ganggui Ding et al. (2024) proposed Freecustom, a tuning-free customized image generation method for multi-concept composition. Ziyi Dong et al. (2022) presented Dreamartist, towards controllable one-shot text-to-image generation via positive-negative prompt-tuning. Dave Epstein et al. (2023) introduced diffusion self-guidance for controllable image generation. Patrick Esser et al. (2024) scaled rectified flow transformers for high-resolution image synthesis.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 29,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 463,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s5_c2",
    "source_id": "2504.15958v2",
    "text": "Rinon Gal et al. (2022) personalized text-to-image generation using textual inversion, and Gal et al. (2023) presented encoder-based domain tuning for fast personalization of text-to-image models. Sooyeon Go et al. (2024) proposed appearance transfer with semantic correspondence in diffusion models. Yuchao Gu et al. (2023) introduced Mix-of-show for decentralized low-rank adaptation for multi-concept customization of diffusion models.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 30,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 438,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s5_c3",
    "source_id": "2504.15958v2",
    "text": "Miao Hua et al. (2023) presented Dream-Tuner, where a single image is enough for subject-driven generation. Xuhui Jia et al. (2023) proposed taming encoder for zero fine-tuning image customization with text-to-image diffusion models. Alexander Kirillov et al. (2023) introduced Segment Anything for segmenting objects in images.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 31,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 328,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s5_c4",
    "source_id": "2504.15958v2",
    "text": "Nupur Kumari et al. (2023) presented multi-concept customization of text-to-image diffusion. Black Forest Labs (2024) released FLUX, a relevant software tool. Dongxu Li et al. (2023) introduced Blip-Diffusion, a pre-trained subject representation for controllable text-to-image generation and editing. Shilong Liu et al. (2024) grounded DINO for open-set object detection.\n\nZhiheng Liu et al. (2023) presented Cones 2 for customizable image synthesis with multiple subjects. Chong Mou et al. (2023) introduced Dragondiffusion for enabling drag-style manipulation on diffusion models.\n\n---\n\n---",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 32,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 593,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s5_c5",
    "source_id": "2504.15958v2",
    "text": "[23] Mou, Chong; Wang, Xintao; Song, Jiechong; Shan, Ying; Zhang, Jian. 2024. Diffeditor: Boosting accuracy and flexibility on diffusion-based image editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 8488–8497.\n\n[24] Nam, Jisu; Kim, Heesu; Lee, DongJae; Jin, Siyoon; Kim, Seungryong; Chang, Seung-gyu. 2024. Dreammatcher: Appearance matching self-attention for semantically-consistent text-to-image personalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 8100–8110.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 33,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 556,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s5_c6",
    "source_id": "2504.15958v2",
    "text": "[25] Nichol, Alex; Dhariwal, Prafulla; Ramesh, Aditya; Shyam, Pranav; Mishkin, Pamela; McGrew, Bob; Sutskever, Ilya; Chen, Mark. 2021. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741.\n\n[26] Oquab, Maxime; Darcet, Timothée; Moutakanni, Théo; Vo, Huy; Szafraniec, Marc; Khalidov, Vasil; Fernandez, Pierre; Haziza, Daniel; Massa, Francisco; El-Nouby, Alaaeldin et al. 2023. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 34,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 542,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s5_c7",
    "source_id": "2504.15958v2",
    "text": "[27] Peebles, William; Xie, Saining. 2023. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision. 4195–4205.\n\n[28] Podell, Dustin; English, Zion; Lacey, Kyle; Blattmann, Andreas; Dockhorn, Tim; Müller, Jonas; Penna, Joe; Rombach, Robin. 2023. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 35,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 418,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s5_c8",
    "source_id": "2504.15958v2",
    "text": "[29] Radford, Alec; Kim, Jong Wook; Hallacy, Chris; Ramesh, Aditya; Goh, Gabriel; Agarwal, Sandhini; Sastry, Girish; Askell, Amanda; Mishkin, Pamela; Clark, Jack et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning. PMLR, 8748–8763.\n\n[30] Ramesh, Aditya; Dhariwal, Prafulla; Nichol, Alex; Chu, Casey; Chen, Mark. 2022. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 36,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 496,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s5_c9",
    "source_id": "2504.15958v2",
    "text": "[31] Ren, Tianhe; Liu, Shilong; Zeng, Ailing; Lin, Jing; Li, Kunchang; Cao, He; Chen, Jiayu; Huang, Xinyu; Chen, Yukang; Yan, Feng et al. 2024. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159.\n\n[32] Rombach, Robin; Blattmann, Andreas; Lorenz, Dominik; Esser, Patrick; Ommer, Björn. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 10684–10695.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 37,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 501,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s5_c10",
    "source_id": "2504.15958v2",
    "text": "[33] Ronneberger, Olaf; Fischer, Philipp; Brox, Thomas. 2015. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted intervention–MICCAI 2015. Springer, 234–241.\n\n[34] Rout, Litu; Chen, Yujia; Ruiz, Nataniel; Caramanis, Constantine; Shakkottai, Sanjay; Chu, Wen-Sheng. 2024. Semantic image inversion and editing using rectified stochastic differential equations. arXiv preprint arXiv:2410.10792.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 38,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 454,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s5_c11",
    "source_id": "2504.15958v2",
    "text": "[35] Ruiz, Nataniel; Li, Yuanzhen; Jampani, Varun; Pritch, Yael; Rubinstein, Michael; Aberman, Kfir. 2023. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 22500–22510.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 39,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 291,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s5_c12",
    "source_id": "2504.15958v2",
    "text": "[36] Ruiz, Nataniel; Li, Yuanzhen; Jampani, Varun; Wei, Wei; Hou, Tingbo; Pritch, Yael; Wadhwa, Neal; Rubinstein, Michael; Aberman, Kfir. 2024. Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 6527–6536.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 40,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 321,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s5_c13",
    "source_id": "2504.15958v2",
    "text": "[37] Saharia, Chitwan; Chan, William; Saxena, Saurabh; Li, Lala; Whang, Jay; Denton, Emily L; Ghasemipour, Kamyar; Lopes, Raphael Gontijo; Ayan, Burcu Karagol; Salimans, Tim et al. 2022. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems 35, 36479–36494.\n\n[38] Shi, Jing; Xiong, Wei; Lin, Zhe; Jung, Hyun Joon. 2024. Instantbooth: Personalized text-to-image generation without test-time finetuning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 8543–8552.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 41,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 574,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s5_c14",
    "source_id": "2504.15958v2",
    "text": "[39] Shin, Chaehun; Choi, Jooyoung; Kim, Heeseung; Yoon, Sungroh. 2024. Large-Scale Text-to-Image Model with Inpainting is a Zero-Shot Subject-Driven Image Generator. arXiv preprint arXiv:2411.15466.\n\n[40] Suvorov, Roman; Logacheva, Elizaveta; Mashikhin, Anton; Remizova, Anastasia; Ashukha, Arsenii; Silvestrov, Aleksei; Kong, Naejin; Goka, Harshith; Park, Kiwoong; Lempitsky, Victor. 2022. Resolution-robust large mask inpainting with fourier convolutions. In Proceedings of the IEEE/CVF winter conference on applications of computer vision. 2149–2159.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 42,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 554,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s5_c15",
    "source_id": "2504.15958v2",
    "text": "[41] Tang, Luming; Jia, Menglin; Wang, Qianqian; Phoo, Cheng Perng; Hariharan, Bharath. 2023. Emergent correspondence from image diffusion. Advances in Neural Information Processing Systems 36, 1363–1389.\n\n[42] Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N; Kaiser, Łukasz; Polosukhin, Illia. 2017. Attention is all you need. Advances in neural information processing systems 30.\n\n[43] Voynov, Andrey; Chu, Qinghao; Cohen-Or, Daniel; Aberman, Kfir. 2023. p+: Extended textual conditioning in text-to-image generation. arXiv preprint arXiv:2303.09522.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 43,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 597,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s5_c16",
    "source_id": "2504.15958v2",
    "text": "[44] Wang, Jiangshan; Pu, Junfu; Qi, Zhongang; Guo, Jiayi; Ma, Yue; Huang, Nisha; Chen, Yuxin; Li, Xiu; Shan, Ying. 2024. Taming rectified flow for inversion and editing. arXiv preprint arXiv:2411.04746.\n\n[45] Wang, Xierui; Fu, Siming; Huang, Qihan; He, Wanggui; Jiang, Hao. 2024. Ms-diffusion: Multi-subject zero-shot image personalization with layout guidance. arXiv preprint arXiv:2406.07209.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 44,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 395,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s5_c17",
    "source_id": "2504.15958v2",
    "text": "[46] Wei, Yuxiang; Zhang, Yabo; Ji, Zhilong; Bai, Jinfeng; Zhang, Lei; Zuo, Wangmeng. 2023. Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 15943–15953.\n\n[47] Xiao, Shitao; Wang, Yueze; Zhou, Junjie; Yuan, Huaying; Xing, Xingrun; Yan, Ruiran; Li, Chaofan; Wang, Shuting; Huang, Tiejun; Liu, Zheng. 2024. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 45,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 496,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s5_c18",
    "source_id": "2504.15958v2",
    "text": "[48] Xu, Jiazheng; Liu, Xiao; Wu, Yuchen; Tong, Yuxuan; Li, Qinkai; Ding, Ming; Tang, Jie; Dong, Yuxiao. 2024. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems 36.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 46,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 249,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s5_c19",
    "source_id": "2504.15958v2",
    "text": "[49] Ye, Hu; Zhang, Jun; Liu, Sibo; Han, Xiao; Yang, Wei. 2023. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721.\n\nAlgorithm 1: FreeGraftor Pipeline\n\nInput: Reference image 𝐼ref, text prompt 𝑃\nOutput: Generated image 𝐼gen\n\nStage 1: Collage Construction\n1. Template Generation 𝐼tmp ← T2I(𝑃)\n2. Template Subject Processing 𝐵tmp ← GroundingDINO(𝐼tmp, 𝑃), 𝑀tmp ← SAM(𝐼tmp, 𝐵tmp), 𝐼erase ← LaMa(𝐼tmp, 𝑀tmp)\n3. Reference Subject Extraction 𝐵ref ← GroundingDINO(𝐼ref, \"subject\"), 𝑀ref ← SAM(𝐼ref, 𝐵ref), 𝐼crop ← CropWithMask(𝐼ref, 𝑀ref)",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 47,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 597,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s5_c20",
    "source_id": "2504.15958v2",
    "text": "4. Collage Assembly 𝐼collage ← ResizeAndPaste(𝐼erase, 𝐼crop, 𝐵tmp)\n\nStage 2: Inversion\n5. 𝜖init, Fref ← FireFlow(𝐼collage)\n\nStage 3: Generation",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 48,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 143,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s5_c21",
    "source_id": "2504.15958v2",
    "text": "6. 𝐼gen ← SAFG(𝜖init, Fref, 𝑃)\n\nB Baselines\nIn both qualitative and quantitative comparisons, we only compared our FreeGraftor with training-free approaches and publicly available zero-shot models.\n\n---\n\n---",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 49,
    "chunk_index_in_section": 21,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 207,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s5_c22",
    "source_id": "2504.15958v2",
    "text": "IP-Adapter introduces a lightweight module that projects visual features from an image encoder into the cross-attention layers of a pre-trained text-to-image diffusion model, integrating image prompts with textual conditions without fine-tuning. MS-Diffusion addresses multi-subject personalization by using layout guidance and grounding tokens to preserve each subject's details in defined regions, minimizing subject conflicts with a specialized cross-attention mechanism. OmniGen unifies diverse image generation tasks within a single diffusion framework based on VAE and transformer, transferrin",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 50,
    "chunk_index_in_section": 22,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s5_c23",
    "source_id": "2504.15958v2",
    "text": "ks within a single diffusion framework based on VAE and transformer, transferring knowledge across tasks and domains without additional encoders. FreeCustom provides a tuning-free approach for generating multi-concept images using a multi-reference self-attention mechanism.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 51,
    "chunk_index_in_section": 23,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 274,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s5_c24",
    "source_id": "2504.15958v2",
    "text": "Algorithm 2 outlines the Semantic-Aware Feature Grafting process, which involves semantic matching, filtering, concatenating keys/values, binding positional embeddings, and computing revised attention for modified attention features.\n\nIn efficiency analysis, FreeGraftor shows significant improvements over DiptychPrompting in both time and memory efficiency, with a balance between generation quality and computational efficiency. MS-Diffusion achieves the fastest inference speed while OmniGen has the lowest memory footprint among evaluated methods.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 52,
    "chunk_index_in_section": 24,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 552,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s5_c25",
    "source_id": "2504.15958v2",
    "text": "The user study reveals that our method is preferred for its higher ratings in text and image alignment. Additional qualitative results demonstrate the method's effectiveness in generating text-aligned novel scenes while maintaining subject fidelity and preserving pixel-level details.\n\nLimitations of FreeGraftor include dependency on external models for reliable results and the significant GPU memory requirement of the FLUX.1 base model, which could be addressed by adapting to lighter models or exploring model distillation and quantization techniques.\n\n---",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 53,
    "chunk_index_in_section": 25,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 561,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s5_c26",
    "source_id": "2504.15958v2",
    "text": "The FreeGraftor framework has the potential to democratize personalized content creation by enabling users to synthesize high-fidelity images of specific subjects without extensive training. This could empower creators in digital art, advertising, and education to rapidly prototype visual concepts while maintaining intricate details. However, the ease of transferring visual identities raises concerns about misuse, such as creating deceptive imagery for disinformation or unauthorized replication of copyrighted subjects. Despite avoiding the computational costs of per-subject fine-tuning, the u",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 54,
    "chunk_index_in_section": 26,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s5_c27",
    "source_id": "2504.15958v2",
    "text": "ects. Despite avoiding the computational costs of per-subject fine-tuning, the use of large pre-trained diffusion models still requires significant energy during inference, necessitating further research into efficient deployment strategies. We propose ethical guidelines to balance creative freedom with safeguards against malicious applications.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 55,
    "chunk_index_in_section": 27,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 347,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s5_c28",
    "source_id": "2504.15958v2",
    "text": "Figure 10: Qualitative comparisons\n\n- A backpack on a cobblestone street.\n- A bowl on top of a dirt road.\n- A sneaker on top of a wooden floor.\n- A dog on top of pink fabric.\n- A robot toy in the jungle.\n- A sloth plushie with a tree and autumn leaves in the background.\n\nFigure 11: Single-subject generation results",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 56,
    "chunk_index_in_section": 28,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 316,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2504.15958v2_s5_c29",
    "source_id": "2504.15958v2",
    "text": "Figure 11: Single-subject generation results\n\n- A dog with angel wings.\n- A dog playing with a ball, eating a banana, wearing a birthday hat.\n- A dog in a box, getting a haircut, wearing headphones, pink glasses.\n- A purple dog on the floor, in the rain, in a doghouse, wearing a rainbow scarf.\n- A dog riding a skateboard, in a bucket, wearing sunglasses, a yellow shirt.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 57,
    "chunk_index_in_section": 29,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 372,
    "chunk_method": "hierarchical",
    "importance_weight": 0.42000000000000004,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s0_c0",
    "source_id": "2647868.2654902",
    "text": "---\n\nCross-modal Retrieval with Correspondence Autoencoder\n\nFangxiang Feng, Xiaojie Wang, Ruifan Li\nBeijing University of Posts and Telecommunications, Beijing, China",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 4,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 166,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s1_c0",
    "source_id": "2647868.2654902",
    "text": "This paper addresses the problem of cross-modal retrieval, such as using a text query to search for images and vice versa. We propose a novel model called correspondence autoencoder (Corr-AE) that correlates hidden representations of two uni-modal autoencoders. The model is trained by minimizing a linear combination of representation learning errors for each modality and correlation learning error",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 4,
    "chunk_index": 1,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 3,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 400,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9900000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s1_c1",
    "source_id": "2647868.2654902",
    "text": "s for each modality and correlation learning error between hidden representations of two modalities. A parameter α balances the representation learning error and the correlation learning error. We extend the Corr-AE to two other correspondence models: Corr-Cross-AE and Corr-Full-AE. The proposed models are evaluated on three publicly available datasets from real scenes and demonstrate significantl",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 4,
    "chunk_index": 2,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 400,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s1_c2",
    "source_id": "2647868.2654902",
    "text": "sets from real scenes and demonstrate significantly better performance than canonical correlation analysis-based models and popular multi-modal deep models on cross-modal retrieval tasks.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 4,
    "chunk_index": 3,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 187,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9450000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s2_c0",
    "source_id": "2647868.2654902",
    "text": "The inclusion of multi-modal data on webpages has become common, leading to a rise in cross-modal retrieval requirements. Unlike traditional information retrieval tasks in a single modality, cross-modal retrieval focuses on mining correlations between data from different modalities.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 4,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 14,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 283,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s2_c1",
    "source_id": "2647868.2654902",
    "text": "1.1 Previous Work\nTwo main strategies for modeling cross-modal correlations have been employed: shared layer modeling and two-stage frameworks. Shared layer models include topic models like Correspondence LDA and deep architectures such as deep autoencoders, deep belief networks, and deep Boltzmann Machines. Two-stage frameworks involve learning separate features for each modality followed by canonical correlation analysis (CCA) to build a common representation space.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 5,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 472,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s2_c2",
    "source_id": "2647868.2654902",
    "text": "1.2 Motivation\nPerceived data from different modalities for the same object contain common information and modality-specific information. Common information is crucial for cross-modal retrieval, while modality-specific information is not necessary and may even be detrimental.\n\nFigure 1: An image and its tags illustrate common and modality-specific information.\n\n---\n\n---",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 6,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 372,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s2_c3",
    "source_id": "2647868.2654902",
    "text": "The shared representation learns both common and modality-specific information, which is not ideally suited for cross-modal retrieval. A representation focusing solely on common information across modalities is more suitable, forming the first motivation for our model. The second strategy involves separating correlation learning from representation learning, which is insufficient for capturing complex correlations, particularly when autoencoders are used. Autoencoders can learn different levels of representation, but determining the optimal level for building correlations between different mo",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 7,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s2_c4",
    "source_id": "2647868.2654902",
    "text": "but determining the optimal level for building correlations between different modalities is challenging. Therefore, correlation learning should be integrated with representation learning, which is the second motivation for our model.\n1.3 Contribution",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 8,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 250,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s2_c5",
    "source_id": "2647868.2654902",
    "text": "We propose the correspondence autoencoder (Corr-AE) based on two basic unimodal autoencoders. The Corr-AE integrates representation and correlation learning into a single process, unlike two-stage methods. A novel loss function is designed, incorporating the losses of autoencoders for all modalities and the loss of correlation between modalities. Our model is evaluated on three public datasets and shows effectiveness compared to other multi-modal deep learning models. We extend the Corr-AE to two correspondence models, Corr-Cross-AE and Corr-Full-AE, and experimental results confirm the super",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 9,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s2_c6",
    "source_id": "2647868.2654902",
    "text": "dels, Corr-Cross-AE and Corr-Full-AE, and experimental results confirm the superiority of combining representation and correlation learning.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 10,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 140,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s2_c7",
    "source_id": "2647868.2654902",
    "text": "2. LEARNING ARCHITECTURE\nThis section details the architecture of the basic Corr-AE, proposes a loss function for learning similar representations, introduces extensions of the Corr-AE, and describes the deep architecture with training algorithms.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 11,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 247,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s2_c8",
    "source_id": "2647868.2654902",
    "text": "2.1 Correspondence Autoencoder\nThe Corr-AE architecture consists of two subnetworks, each a basic autoencoder, connected by a predefined similarity measure on the code layer. Each subnetwork is responsible for a modality. The mapping from inputs to the code layers is denoted as f(p; Wf) and g(q; Wg), with f and g as logistic activation functions. The similarity measure is defined as:\n\nC(p(i), q(i); Wf, Wg) = ||f(p(i); Wf) - g(q(i); Wg)||^2\n\nThe loss function for learning similar representations is:\n\nL(p(i), q(i); Θ) = (1 − α) * (LI(p(i); Θ) + LT(q(i); Θ)) + α * LC(p(i), q(i); Θ)",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 12,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 585,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s2_c9",
    "source_id": "2647868.2654902",
    "text": "where LI and LT are the reconstruction error losses, LC is the correlation loss, and α balances the trade-off between correlation and reconstruction losses.\n\n2.2 Correspondence Cross-modal Autoencoder\nThe Corr-Cross-AE replaces basic autoencoders with cross-modal autoencoders, which reconstruct input from different modalities. The loss function is:\n\nL(p(i), q(i); Θ) = (1 − α) * (LI(p(i), q(i); Θ) + LT(p(i), q(i); Θ)) + α * LC(p(i), q(i); Θ)",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 13,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 444,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s2_c10",
    "source_id": "2647868.2654902",
    "text": "The representation learning in the cross-modal autoencoder considers information from the other modality, capturing correlations in the reconstruction loss.\n\n---\n\n---\n\n2.3 Correspondence Full-modal Autoencoder\nThe full-modal autoencoder combines a basic autoencoder and cross-modal autoencoder to model audio and video data. The basic Corr-AE can be extended to the Corr-Full-AE, which reconstructs inputs from different modalities within a shared representation space. The loss function for the Corr-Full-AE is defined as:",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 14,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 523,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s2_c11",
    "source_id": "2647868.2654902",
    "text": "L(p(i), q(i); Θ) =(1 −α) LI(p(i), q(i); Θ) + LT (p(i), q(i); Θ) + αLC(p(i), q(i); Θ)\n\nHere, LI, LT, and LC represent the image, text, and correspondence losses, respectively.\n2.4 Deep Architecture",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 15,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 196,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s2_c12",
    "source_id": "2647868.2654902",
    "text": "To address the diverse statistical properties of data from different modalities, a deep architecture is proposed. It uses stacked modality-friendly models to learn higher-level representations and Corr-AE to learn similar representations. The deep architecture consists of three components: the first two are restricted Boltzmann machines (RBMs), and the third is a correspondence autoencoder. Gaussian RBM and replicated softmax RBM are used for the first layer to model image and text data, respectively. The second component uses basic RBMs to learn higher-level features. The third component emp",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 16,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s2_c13",
    "source_id": "2647868.2654902",
    "text": "omponent uses basic RBMs to learn higher-level features. The third component employs one of the three correspondence autoencoders.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 17,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 130,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s3_c0",
    "source_id": "2647868.2654902",
    "text": "We evaluate our models on three real-world datasets: Wikipedia, Pascal, and NUS-WIDE-10k. These datasets vary in text modality, size, and category numbers. For image representation, features such as Pyramid Histogram of Words (PHOW), Gist, and MPEG-7 descriptors are extracted. Text representation uses a bag of words model. The datasets are split into training, validation, and testing subsets.\n\n---\n\n---\n3.2 Evaluation Metric",
    "section_title": "EXPERIMENTS",
    "section_type": "experiment",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 18,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 25,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 427,
    "chunk_method": "hierarchical",
    "importance_weight": 0.935,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s3_c1",
    "source_id": "2647868.2654902",
    "text": "We evaluate two cross-modal retrieval tasks: text retrieval from an image query and image retrieval from a text query. We use two metrics, mean average precision (mAP) and top 20% percentage, following [30]. mAP represents the ability to learn discriminative cross-modal mapping functions, while the top 20% percentage reveals the ability to learn corresponding latent concepts. We report mAP@50 in all experiments. Semantic labels are only used for evaluation\n; our models do not require them for training.",
    "section_title": "EXPERIMENTS",
    "section_type": "experiment",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 19,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 507,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s3_c2",
    "source_id": "2647868.2654902",
    "text": "3.3 Baseline\nOur models and baseline methods use features from the first two components of the deep architecture described in section 2.4. The only difference lies in the third component. Cosine distance measures similarity in all experiments. We compare our models with three CCA-based models and two multi-modal models.",
    "section_title": "EXPERIMENTS",
    "section_type": "experiment",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 20,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 321,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s3_c3",
    "source_id": "2647868.2654902",
    "text": "3.4 Model Architecture\nWe perform grid search for the number of hidden units and restrict all hidden layers to the same number. Validation sets determine the best hyperparameters. For our correspondence models, we set the parameter α to 0.8 for Corr-AE and Corr-Full-AE, and to 0.2 for Corr-Cross-AE. CCA is applied to learn correlations with different numbers of units. Bimodal AE training uses three copies of all training data. We do not weight reconstruction errors from different modalities.",
    "section_title": "EXPERIMENTS",
    "section_type": "experiment",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 21,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 496,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s3_c4",
    "source_id": "2647868.2654902",
    "text": "3.5 Results\nOur correspondence autoencoders significantly outperform other models on both retrieval tasks across Wikipedia, Pascal, and NUS-WIDE-10k data sets. Our models improve mAP scores and top 20% performance, demonstrating the effectiveness of combining representation and correlation learning. The advantage of our correspondence models is their integration of these processes, making two-stage methods suboptimal in comparison.",
    "section_title": "EXPERIMENTS",
    "section_type": "experiment",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 22,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 435,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s3_c5",
    "source_id": "2647868.2654902",
    "text": "Table 1: Results of two retrieval protocols: mAP scores and top 20% on three data sets.\n\n---\n\n(Note: The table has been left intact as it contains core academic content.)\n\n---\n\nThe table below presents the mean Average Precision (mAP) for the top 20% of image and text queries for various models on two data sets, NUS-WIDE-10k and Wikipedia:",
    "section_title": "EXPERIMENTS",
    "section_type": "experiment",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 23,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 341,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s3_c6",
    "source_id": "2647868.2654902",
    "text": "**NUS-WIDE-10k**\n- CCA-AE: Image Query 0.161, Text Query 0.153, Average 0.157\n- CCA-Cross-AE: Image Query 0.137, Text Query 0.182, Average 0.159\n- CCA-Full-AE: Image Query 0.148, Text Query 0.177, Average 0.163\n- Bimodal AE: Image Query 0.250, Text Query 0.270, Average 0.260\n- Bimodal DBN: Image Query 0.219, Text Query 0.219, Average 0.219\n- Corr-AE: Image Query 0.290, Text Query 0.279, Average 0.285\n- Corr-Cross-AE: Image Query 0.271, Text Query 0.280, Average 0.276\n- Corr-Full-AE: Image Query 0.281, Text Query 0.276, Average 0.279",
    "section_title": "EXPERIMENTS",
    "section_type": "experiment",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 24,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 538,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s3_c7",
    "source_id": "2647868.2654902",
    "text": "**Wikipedia**\n- CCA-AE: Image Query 0.199, Text Query 0.268, Average 0.234\n- CCA-Cross-AE: Image Query 0.199, Text Query 0.344, Average 0.272\n- CCA-Full-AE: Image Query 0.241, Text Query 0.242, Average 0.242\n- Bimodal AE: Image Query 0.250, Text Query 0.297, Average 0.274\n- Bimodal DBN: Image Query 0.173, Text Query 0.203, Average 0.188\n- Corr-AE: Image Query 0.319, Text Query 0.375, Average 0.347\n- Corr-Cross-AE: Image Query 0.349, Text Query 0.348, Average 0.349\n- Corr-Full-AE: Image Query 0.331, Text Query 0.379, Average 0.355",
    "section_title": "EXPERIMENTS",
    "section_type": "experiment",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 25,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 535,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s3_c8",
    "source_id": "2647868.2654902",
    "text": "Our models, Corr-AE, Corr-Cross-AE, and Corr-Full-AE, which incorporate representation learning and correlation learning, significantly outperform other baseline models in cross-modal retrieval tasks on all data sets. The analysis of the parameter α shows that both too small and too large values result in poor performance, indicating the importance of balancing \"individuality\" and \"correlation\" in the data.",
    "section_title": "EXPERIMENTS",
    "section_type": "experiment",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 26,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 410,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s3_c9",
    "source_id": "2647868.2654902",
    "text": "Figures 7, 8, 9, 10, and 11 provide visual examples and analysis of the performance of our models, particularly the impact of varying α on the representation of image and text in the NUS-WIDE-10k test data set.\n\n---\n\n---",
    "section_title": "EXPERIMENTS",
    "section_type": "experiment",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 27,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 220,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s3_c10",
    "source_id": "2647868.2654902",
    "text": "[1] M. Bastan, H. Cam, U. Gülüz, and Z. Ulusoy. Bilvideo-7: an MPEG-7-compatible video indexing and retrieval system. IEEE MultiMedia, 17(3):62–73, 2010.\n[2] Y. Bengio. Learning deep architectures for AI. Foundations and Trends in Machine Learning, 2(1):1–127, 2009.\n[3] E. L. Bird and S. Klein. Natural Language Processing with Python. O’Reilly Media Inc., 2009.\n[4] D. M. Blei and M. I. Jordan. Modeling annotated data. ACM SIGIR, pages 127–134, 2003.",
    "section_title": "EXPERIMENTS",
    "section_type": "experiment",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 28,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 453,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s3_c11",
    "source_id": "2647868.2654902",
    "text": "[5] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet allocation. JMLR, 3:993–1022, 2003.\n[6] A. Bosch, A. Zisserman, and X. Muñoz. Image classification using random forests and ferns. In ICCV, pages 1–8. IEEE, 2007.\n[7] T.-S. Chua, J. Tang, R. Hong, H. Li, Z. Luo, and Y.-T. Zheng. NUS-WIDE: A real-world web image database from National University of Singapore. In CIVR, Santorini, Greece., 2009.",
    "section_title": "EXPERIMENTS",
    "section_type": "experiment",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 29,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 405,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s3_c12",
    "source_id": "2647868.2654902",
    "text": "[8] A. Farhadi, S. M. M. Hejrati, M. A. Sadeghi, P. Young, C. Rashtchian, J. Hockenmaier, and D. A. Forsyth. Every picture tells a story: Generating sentences from images. In ECCV (4), volume 6314 of Lecture Notes in Computer Science, pages 15–29. Springer, 2010.\n[9] A. Frome, G. Corrado, J. Shlens, S. Bengio, J. Dean, M. Ranzato, and T. Mikolov. Devise: A deep visual-semantic embedding model. In NIPS, pages 2121–2129, 2013.",
    "section_title": "EXPERIMENTS",
    "section_type": "experiment",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 30,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 428,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s3_c13",
    "source_id": "2647868.2654902",
    "text": "[10] D. R. Hardoon, S. Szedmák, and J. Shawe-Taylor. Canonical correlation analysis; an overview with application to learning methods. Neural Computation, 16:2639–2664, 2004.\n[11] G. Hinton and R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504–507, 2006.\n[12] G. E. Hinton. Training products of experts by minimizing contrastive divergence. Neural Computation, 14(8):1771–1800, 2002.",
    "section_title": "EXPERIMENTS",
    "section_type": "experiment",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 31,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 432,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s3_c14",
    "source_id": "2647868.2654902",
    "text": "[13] G. E. Hinton, S. Osindero, and Y. Teh. A fast learning algorithm for deep belief nets. Neural Computation, 18(7):1527–1554, 2006.\n[14] Y. Jia, M. Salzmann, and T. Darrell. Learning cross-modality similarity for multinomial data. ICCV, pages 2407–2414, 2011.",
    "section_title": "EXPERIMENTS",
    "section_type": "experiment",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 32,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 262,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s3_c15",
    "source_id": "2647868.2654902",
    "text": "[15] J. Kim, J. Nam, and I. Gurevych. Learning semantics with deep belief network for cross-language information retrieval. COLING, pages 579–588, 2012.\n[16] B. S. Manjunath, J. R. Ohm, V. V. Vinod, and A. Yamada. Color and texture descriptors. IEEE Trans. Circuits and Systems for Video Technology, Special Issue on MPEG-7, 11(6):703–715, June 2001.\n[17] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng. Multimodal deep learning. ICML, pages 689–696, 2011.",
    "section_title": "EXPERIMENTS",
    "section_type": "experiment",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 33,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 467,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s3_c16",
    "source_id": "2647868.2654902",
    "text": "[18] A. Oliva and A. Torralba. Modeling the shape of the scene: A holistic representation of the spatial envelope. IJCV, 42(3):145–175, 2001.\n[19] N. Rasiwasia, J. C. Pereira, E. Coviello, G. Doyle, G. R. G. Lanckriet, R. Levy, and N. Vasconcelos. A new approach to cross-modal multimedia retrieval. ACM MM, pages 251–260, 2010.\n[20] R. Salakhutdinov and G. Hinton. Replicated softmax: an undirected topic model. NIPS, pages 1607–1614, 2009.",
    "section_title": "EXPERIMENTS",
    "section_type": "experiment",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 34,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 441,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s3_c17",
    "source_id": "2647868.2654902",
    "text": "[21] R. R. Salakhutdinov and G. G. Hinton. An efficient learning procedure for deep Boltzmann machines. Neural computation, 24(8):1967–2006, 2012.\n[22] P. Smolensky. Parallel distributed processing: explorations in the microstructure of cognition, vol. 1. chapter Information processing in dynamical systems: foundations of harmony theory, pages 194–281. MIT Press, Cambridge, MA, USA, 1986.",
    "section_title": "EXPERIMENTS",
    "section_type": "experiment",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 35,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 391,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s3_c18",
    "source_id": "2647868.2654902",
    "text": "[23] R. Socher, M. Ganjoo, C. D. Manning, and A. Ng. Zero-shot learning through cross-modal transfer. In NIPS, pages 935–943, 2013.\n[24] N. Srivastava and R. Salakhutdinov. Learning representations for multimodal data with deep belief nets. ICML Representation Learning Workshop, 2012.\n[25] N. Srivastava and R. Salakhutdinov. Multimodal learning with deep Boltzmann machines. NIPS, pages 2231–2239, 2012.",
    "section_title": "EXPERIMENTS",
    "section_type": "experiment",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 36,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 405,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s3_c19",
    "source_id": "2647868.2654902",
    "text": "[26] L. van der Maaten and G. Hinton. Visualizing high-dimensional data using t-SNE. JMLR, 2008.\n[27] A. Vedaldi and B. Fulkerson. VLFeat: an open and portable library of computer vision algorithms. In ACM MM, pages 1469–1472, 2010.\n[28] M. Welling, M. Rosen-Zvi, and G. Hinton. Exponential family harmoniums with an application to information retrieval. In NIPS, pages 501–508, Vancouver, 2004. Morgan Kaufmann.",
    "section_title": "EXPERIMENTS",
    "section_type": "experiment",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 37,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 412,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s3_c20",
    "source_id": "2647868.2654902",
    "text": "[29] J. Weston, S. Bengio, and N. Usunier. Large scale image annotation: Learning to rank with joint word-image embeddings. In ECML, pages 21–35, 2010.\n[30] Y. Zhuang, Y. F. Wang, F. Wu, Y. Zhang, and W. Lu. Supervised coupled dictionary learning with group structures for multi-modal retrieval. In AAAI, 2013.",
    "section_title": "EXPERIMENTS",
    "section_type": "experiment",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 38,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 310,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s3_c21",
    "source_id": "2647868.2654902",
    "text": "Figure 7: Three examples of text-based cross-modal retrieval using our Corr-Full-AE and best baseline method. In each example, the query text and its corresponding image are shown on the top; retrieved images of our Corr-Full-AE are presented in the middle",
    "section_title": "EXPERIMENTS",
    "section_type": "experiment",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 39,
    "chunk_index_in_section": 21,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 256,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s3_c22",
    "source_id": "2647868.2654902",
    "text": "; retrieved images of the best baseline model are presented at the bottom. Relevant matches are shown with green bounding box. Irrelevant matches are shown with red bounding box. The text queries come from sport, aeroplane, and grass category respectively.",
    "section_title": "EXPERIMENTS",
    "section_type": "experiment",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 40,
    "chunk_index_in_section": 22,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 256,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s3_c23",
    "source_id": "2647868.2654902",
    "text": "---\n\n---\n\nSeveral individuals are depicted waiting to cross a road, including two bicyclists and a pedestrian. In another scene, two women with bicycles are observing the traffic to find a safe moment to cross. A group of people, including four women and two men, are posing while wearing furry hats with earflaps. Additionally, there is a mention of teenagers being silly and visitors wearing fur hats.\n\n(b) Pascal\n\nKeywords: nature, green, quality, spider, flowers, pink, balcony",
    "section_title": "EXPERIMENTS",
    "section_type": "experiment",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 41,
    "chunk_index_in_section": 23,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 481,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2647868.2654902_s3_c24",
    "source_id": "2647868.2654902",
    "text": "(c) NUS-WIDE-10k\n\nFigure 9: Image-text pairs from three datasets are presented. Each pair shows the closest images or texts to the query, which are the ground truth. Figure (a) presents pairs from categories of warfare, biology, and history. Figure (b) includes pairs from bicycle, cow, and person categories. Figure (c) displays pairs from animal, flower, and food categories, demonstrating effective retrieval of various images and texts.\n\n---",
    "section_title": "EXPERIMENTS",
    "section_type": "experiment",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 42,
    "chunk_index_in_section": 24,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 445,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8925,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s0_c0",
    "source_id": "2808205",
    "text": "Correspondence Autoencoders for Cross-Modal Retrieval\n\nFANGXIANG FENG, XIAOJIE WANG, RUIFAN LI, Beijing University of Posts and Telecommunications, and IBRAR AHMAD, Beijing University of Posts and Telecommunications and University of Peshawar",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 36,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 242,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s0_c1",
    "source_id": "2808205",
    "text": "This article addresses the cross-modal retrieval problem, where a text query can retrieve images, and vice versa. We propose several novel models based on different autoencoders, which correlate hidden representations of a pair of autoencoders. The models are trained using a novel optimal objective that minimizes a linear combination of representation learning errors and correlation learning error. This approach balances the learning of common information across modalities and the quality of reconstruction for each modality. Our models are categorized into two groups: multimodal reconstructio",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s0_c2",
    "source_id": "2808205",
    "text": "h modality. Our models are categorized into two groups: multimodal reconstruction correspondence autoencoder, which reconstructs both modalities, and unimodal reconstruction correspondence autoencoder, which reconstructs a single modality. Evaluation on three public datasets shows significant performance improvements over canonical correlation analysis models and popular multimodal deep learning models.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 406,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s0_c3",
    "source_id": "2808205",
    "text": "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval Models; I.2.6 [Artificial Intelligence]: Learning\nGeneral Terms: Algorithms, Design, Experimentation\nAdditional Key Words and Phrases: Cross-modal, retrieval, image and text, deep learning, autoencoder\n\nThe work was partially supported by the National Natural Science Foundation of China, the National High Technology Research and Development Program of China, and other organizations.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 473,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s0_c4",
    "source_id": "2808205",
    "text": "Authors' addresses: F. Feng, X. Wang, R. Li, School of Computer Sciences, Beijing University of Posts and Telecommunications, Beijing, 100876, China; I. Ahmad, School of Computer Sciences, Beijing University of Posts and Telecommunications, Beijing, 100876, China, and Department of Computer Science, University of Peshawar, Pakistan.\n\nThe increasing inclusion of multimodal data on the web has led to a rise in cross-modal retrieval tasks. Unlike traditional single-modality information retrieval, cross-modal retrieval focuses on exploiting the correlations between different modalities.\n\n---\n1.1. Previous Work",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 613,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s0_c5",
    "source_id": "2808205",
    "text": "Many approaches have been proposed for cross-modal retrieval tasks. Two main strategies for modeling cross-modal correlations have been defined. The first strategy involves a shared code layer for correlating different modalities, with some approaches using topic models such as Correspondence Latent Dirichlet Allocation (Corr-LDA). LDA-based models can be seen as a two-layer architecture with one hidden layer. Recently, deep architectures like deep autoencoders (DAE), deep belief networks (DBN), and deep Boltzmann Machines (DBM) have been extended to model multi-modal data. These models have",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 5,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s0_c6",
    "source_id": "2808205",
    "text": "Machines (DBM) have been extended to model multi-modal data. These models have not been used for cross-modal retrieval tasks. The second strategy is a two-stage framework that learns features from each modality and uses Canonical Correlation Analysis (CCA) to build a common representation space. CCA finds maximum correlations between matrices and is used for projecting input pairs into a lower-dimensional space for retrieval.\n1.2. Motivation",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 6,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 445,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s0_c7",
    "source_id": "2808205",
    "text": "The shared layer learned jointly from different modalities may not fit the needs of cross-modal retrieval. Perceived data from different modalities usually contain common and modality-specific information. For instance, \"sky\" and \"blue\" are common to both image and text modalities, while \"nikon\" and \"nikkor\" are text-specific, and \"flowers\" and \"clouds\" are image-specific. Common information is crucial for cross-modal retrieval, and a shared representation that learns both common and modality-specific information may not be suitable. The separation of correlation learning from representation",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 7,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s0_c8",
    "source_id": "2808205",
    "text": "may not be suitable. The separation of correlation learning from representation learning is insufficient, especially when autoencoders are used, as modalities may be correlated at different abstract levels of representations.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 8,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 225,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s0_c9",
    "source_id": "2808205",
    "text": "1.3. Contribution\nTo overcome the disadvantages of these strategies, we have proposed the correspondence autoencoder (Corr-AE), which integrates representation learning and correlation learning into a single process, in contrast to two-stage methods that separate these procedures.\n\n---\n\nFig. 1. An image and its tags illustrate common and modality-specific information.\n\nFig. 2. The difference between two-stage methods and our Corr-AE is shown, with the latter integrating representation and correlation learning.\n\n---",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 9,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 520,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s0_c10",
    "source_id": "2808205",
    "text": "The difference between the two-stage method and our Corr-AE is illustrated in Figure 2. Corr-AE integrates representation and correlation learning into a single process, with a loss function comprising two parts: the autoencoder loss for both modalities and the correlation loss between modalities. Our model is evaluated using three publicly available datasets and demonstrates increased effectiveness compared to several other multimodal deep learning models. We extend Corr-AE to two correspondence models: Corr-Cross-AE and Corr-Full-AE. Experimental results show that the combination of represe",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 10,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s0_c11",
    "source_id": "2808205",
    "text": "s-AE and Corr-Full-AE. Experimental results show that the combination of representation and correlation learning is more effective than the two-stage method.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 11,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 157,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s0_c12",
    "source_id": "2808205",
    "text": "This article is an extension of our preliminary studies, introducing two novel correspondence autoencoders: Corr-Image-AE and Corr-Text-AE, which reconstruct a single modality. These unimodal reconstruction Corr-AEs provide additional choices for implementing cross-modal retrieval tasks and offer insights into how all Corr-AEs function.\n\nSection 2 details the learning architecture, introducing the three multimodal reconstruction Corr-AEs, the two unimodal reconstruction Corr-AEs, and the deep architecture based on Corr-AEs, along with the training procedure.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 12,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 564,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s0_c13",
    "source_id": "2808205",
    "text": "The Corr-AE architecture consists of two subnetworks, each a basic autoencoder, connected by a similarity measure on the code layers. The loss function for training the Corr-AE is proposed to learn similar representations of different modalities. The learning can be performed using the standard back-propagation algorithm.\n\nWe also propose the Corr-Cross-AE, which replaces basic autoencoders with cross-modal autoencoders. The loss function for the Corr-Cross-AE is defined to reconstruct input from different modalities.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 13,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 523,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s0_c14",
    "source_id": "2808205",
    "text": "Here, ˆq(k) I and ˆp(k) T represent the reconstruction data from the image and text subnets, respectively. The representation learning in the cross-modal autoencoder considers information from the opposite modality, capturing correlations in the reconstruction loss.\n\nThe full-modal autoencoder, a combination of a basic autoencoder and cross-modal autoencoder, is applied to audio and video data. The loss function for the Corr-Full-AE is defined as:\n\nL(p(k), q(k); \u0002) = (1 −α) [LI(p(k), q(k); \u0002) + LT (p(k), q(k); \u0002)] + αLC(p(k), q(k); \u0002)",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 14,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 540,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s0_c15",
    "source_id": "2808205",
    "text": "with LI, LT, and LC representing the image, text, and correspondence reconstruction losses, respectively.\n\nWe propose the Corr-Image-AE and Corr-Text-AE, which reconstruct image and text in their respective modalities. The loss functions for these autoencoders are similar, adjusted for the specific modality.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 15,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 309,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s0_c16",
    "source_id": "2808205",
    "text": "For a deep architecture, to handle different statistical properties of modalities, we first use stacked modality-friendly models to learn higher-level representations. Then, the Corr-AE is applied to learn similar representations. This architecture includes RBMs and the Corr-AE, trained using contrastive divergence and back-propagation.\n\nIn our experiments, we evaluate the models on three real-world datasets, introduce evaluation protocols, report performance, and analyze the impact of the parameter α.\n\n3.1. Datasets and Feature Extraction",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 16,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 545,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s0_c17",
    "source_id": "2808205",
    "text": "3.1. Datasets and Feature Extraction\n\nThe Wikipedia dataset [Rasiwasia et al. 2010] consists of 2,866 image/text pairs from ten semantic categories. It is divided into training, validation, and test sets with 2,173, 231, and 462 cases, respectively. Image features are extracted using Pyramid Histogram of Words (PHOW) [Bosch et al. 2007], Gist [Oliva and Torralba 2001], and MPEG-7 descriptors [Manjunath et al. 2001]. Each image is represented by a 2,296-dimensional feature vector. Text is represented using a bag-of-words model with a dictionary of 3,000 high-frequency words.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 17,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 580,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s0_c18",
    "source_id": "2808205",
    "text": "The Pascal dataset [Farhadi et al. 2010] contains 1,000 image/text pairs from twenty categories. The images are from the 2008 PASCAL development kit and are labeled with five sentences. The dataset is split into training, validation, and test sets with 800, 100, and 100 cases, respectively. The feature extraction methods are the same as for the Wikipedia dataset, with a 1,000-dimensional text representation.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 18,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 411,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s0_c19",
    "source_id": "2808205",
    "text": "The NUS-WIDE-10k dataset is a subset of NUS-WIDE [Chua et al. 2009], with 1,000 image/text pairs per category from ten categories. The dataset is randomly split into training, validation, and test sets with 8,000, 1,000, and 1,000 cases, respectively. Images are represented by six descriptors, and text is represented by a 1,000-dimensional bag-of-words.\n\n3.2. Evaluation Metric",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 19,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 379,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s0_c20",
    "source_id": "2808205",
    "text": "3.2. Evaluation Metric\n\nTwo cross-modal retrieval tasks are considered: text retrieval from an image query and image retrieval from a text query. The retrieval performance is evaluated using mean average precision (mAP) and top 20% percentage. mAP@50 is used for evaluation.\n\n3.3. Baseline",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 20,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 289,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s0_c21",
    "source_id": "2808205",
    "text": "3.3. Baseline\n\nInputs to our models and baseline methods are features learned by the first two components of the deep architecture described in Section 2.3. Cosine distance is used to measure similarity. We compare our models with three CCA-based models and two multimodal models: CCA-AE, CCA-Cross-AE, CCA-Full-AE, Bimodal AE, and Bimodal DBN.\n\n---\n3.4. Model Architecture",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 21,
    "chunk_index_in_section": 21,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 373,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s0_c22",
    "source_id": "2808205",
    "text": "To determine the architecture of all models, we conduct a grid search for the number of hidden neurons per layer, considering settings of 32, 64, 128, 256, 512, and 1,024. We maintain identical numbers of units across all hidden layers in the deep architecture to reduce the search space. Validation sets are employed to identify the optimal number of hidden units and the dimensionality of the CCA latent space, as well as the iteration number for bimodal autoencoders in our correspondence models. For our five models, we select an appropriate value for the parameter α. Our findings indicate that",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 22,
    "chunk_index_in_section": 22,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s0_c23",
    "source_id": "2808205",
    "text": ", we select an appropriate value for the parameter α. Our findings indicate that α is not sensitive to datasets. Thus, for all datasets, we set α to 0.8 for Corr-AE and Corr-Full-AE, to 0.2 for Corr-Cross-AE, and to 0.3 and 0.7 for Corr-Image-AE and Corr-Text-AE, respectively. A detailed analysis of α is provided later in this section. For models involving CCA, we apply CCA to learn the correlation between the image and text hidden layers with varying numbers of units to achieve optimal performance. We use three copies of all training data for training bimodal AE: the first includes both modal",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 23,
    "chunk_index_in_section": 23,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 600,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s0_c24",
    "source_id": "2808205",
    "text": "pies of all training data for training bimodal AE: the first includes both modalities, the second contains only image data with text set to zeros, and the third contains only text data with images set to zeros. We do not weight reconstruction errors between different modalities. For the bimodal DBN, we adopt the variational mean field approach with ten steps to generate the unknown modality given a known modality. Gibbs sampling did not demonstrate improvement in our experiments. The code with parameter specifications for our models and baseline methods is available online.\n3.5. Results\n3.5.1. Multimodal Reconstruction: Corr-AEs vs. Baselines",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 24,
    "chunk_index_in_section": 24,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 650,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s0_c25",
    "source_id": "2808205",
    "text": "Table I summarizes the mAP scores and top 20% results for the two cross-modal retrieval tasks on Wikipedia, Pascal, and NUS-WIDE-10k datasets. Our three multimodal reconstruction Corr-AEs significantly outperform other models on both text and image retrieval tasks across all datasets. Compared to the best baseline results, our models achieve substantial improvements in mAP scores. For instance, Corr-Full-AE improves mAP scores by 12.3% and 16.6% for text-by-image and image-by-text retrieval on Wikipedia, by 12.4% and 2.2% on Pascal, and by 32.4% and 10.2% on NUS-WIDE-10k. Compared to CCA-AE,",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 25,
    "chunk_index_in_section": 25,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s0_c26",
    "source_id": "2808205",
    "text": "and 2.2% on Pascal, and by 32.4% and 10.2% on NUS-WIDE-10k. Compared to CCA-AE, our Corr-AE enhances the average mAP by 53.6%, 81.5%, and 48.3% on the respective datasets. Similarly, our correspondence models show significant improvement over two-stage methods. The three CCA-AEs first learn image and text representations using unimodal autoencoders and then employ CCA to capture the correlation between modalities. The advantage of our correspondence models is their integration of representation and correlation learning into a single process. The distinction within our correspondence autoencode",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 26,
    "chunk_index_in_section": 26,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 600,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s0_c27",
    "source_id": "2808205",
    "text": "ning into a single process. The distinction within our correspondence autoencoders is smaller than the differences among the three CCA-AEs, highlighting the effectiveness of combining representation and correlation learning. Our Corr-AE also achieves notable improvements over Bimodal AE and Bimodal DBN. Figures 9, 10, and 11 present retrieval examples and further details.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 27,
    "chunk_index_in_section": 27,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 374,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s0_c28",
    "source_id": "2808205",
    "text": "---\n\n[Table I contents and figure references have been omitted as per the instructions to only clean the provided text content and not to generate a complete structure.]",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 28,
    "chunk_index_in_section": 28,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 169,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s0_c29",
    "source_id": "2808205",
    "text": "We compare the performances of two unimodal reconstruction Corr-AEs and three multimodal reconstruction Corr-AEs in cross-modal retrieval tasks. The multimodal reconstruction Corr-AEs exhibit consistent performance with similar mAP scores for both image and text queries. In contrast, the unimodal Corr-AEs show uneven performance, with Corr-Text-AE excelling in text query tasks and Corr-Image-AE in image query tasks. tSNE visualization reveals that representations learned by Corr-Full-AE are well mixed semantically, while those in Corr-Text-AE and Corr-Image-AE show more distinguishable text a",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 29,
    "chunk_index_in_section": 29,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s0_c30",
    "source_id": "2808205",
    "text": ", while those in Corr-Text-AE and Corr-Image-AE show more distinguishable text and image representations, respectively.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 30,
    "chunk_index_in_section": 30,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 119,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s0_c31",
    "source_id": "2808205",
    "text": "Our analysis indicates that the distinct query is crucial for text query image tasks, where Corr-Text-AE achieves the best performance. The diversity in improvement across datasets is linked to the sparsity of text, with Wikipedia benefiting the most due to a higher number of words per example. Consequently, Corr-Text-AE is a better choice for text query-only cross-modal retrieval tasks, while multimodal reconstruction Corr-AEs are suitable for both directions. Figure 12 illustrates the visualization of image and text representations learned by different Corr-AEs on the NUS-WIDE-10k test data",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 31,
    "chunk_index_in_section": 31,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s0_c32",
    "source_id": "2808205",
    "text": "text representations learned by different Corr-AEs on the NUS-WIDE-10k test dataset. Figure 13 shows the mAP values of the correspondence autoencoders with varying α values across all datasets.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 32,
    "chunk_index_in_section": 32,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 193,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s0_c33",
    "source_id": "2808205",
    "text": "3.6. Analysis of the Parameter α\n\nIn this section, we analyze the impact of the parameter α using the three multimodal reconstruction Corr-AEs as an example. The mAP values for these models with varying α values across all datasets are presented in Figure 13. Both excessively small and large α values result in poorer performance, reflecting the nature of α in our models. Small α values overemphasize data “individuality” at the cost of correlations, while large α values do the opposite.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 33,
    "chunk_index_in_section": 33,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 490,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s0_c34",
    "source_id": "2808205",
    "text": "To support our hypothesis, tSNE visualization of image and text representations learned by our Corr-Full-AE is used. Figure 14 shows that at α = 0.01, the representation spaces are nearly disjoint, indicating strong “individuality” with no correlations. Conversely, at α = 0.99, the “individuality” is lost due to a confused representation space. At α = 0.2, bimodal data of the same category are clustered together. At α = 0.8, the representation space is effective for cross-modal retrieval, clustering a large number of image-text pairs with the same semantic labels. The yellow line in Figure 13",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 34,
    "chunk_index_in_section": 34,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s0_c35",
    "source_id": "2808205",
    "text": "of image-text pairs with the same semantic labels. The yellow line in Figure 13 represents the mAP scores of the best baseline model, which the multimodal reconstruction Corr-AEs consistently outperform across a wide range of α values.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 35,
    "chunk_index_in_section": 35,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 235,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s1_c0",
    "source_id": "2808205",
    "text": "This work introduces two groups of cross-modal learning models that integrate representation and correlation learning. One group, the multimodal reconstruction Corr-AEs, reconstructs all modalities, learning effective representations for each. The other group, the unimodal reconstruction Corr-AEs, reconstructs only one modality, learning effective representation for the targeted modality. These models are compared against state-of-the-art CCA-based and multimodal deep learning models on three pu",
    "section_title": "CONCLUSION",
    "section_type": "conclusion",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 36,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 17,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 500,
    "chunk_method": "hierarchical",
    "importance_weight": 0.935,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s1_c1",
    "source_id": "2808205",
    "text": "rt CCA-based and multimodal deep learning models on three public datasets, demonstrating their effectiveness in cross-modal retrieval tasks.",
    "section_title": "CONCLUSION",
    "section_type": "conclusion",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 37,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 140,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s1_c2",
    "source_id": "2808205",
    "text": "Muhammet Bastan, Hayati Cam, Ugur Gdkbay, and Ozgur Ulusoy. 2010. Bilvideo-7: An MPEG-7-compatible video indexing and retrieval system. IEEE MultiMedia 17, 3, 62–73.\nYoshua Bengio. 2009. Learning deep architectures for AI. Found. Trends Machine Learn. 2, 1, 1–127.\nSteven Bird. 2006. NLTK: the natural language toolkit. In Proceedings of the COLING/ACL on Interactive Presentation Sessions (COLING-ACL’06). 69–72.",
    "section_title": "CONCLUSION",
    "section_type": "conclusion",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 38,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 413,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s1_c3",
    "source_id": "2808205",
    "text": "David M. Blei and Michael I. Jordan. 2003. Modeling Annotated Data. In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’03). 127–134.\nDavid M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. J. Machine Learn. Res. 3, 993–1022.",
    "section_title": "CONCLUSION",
    "section_type": "conclusion",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 39,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 333,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s1_c4",
    "source_id": "2808205",
    "text": "Anna Bosch, Andrew Zisserman, and Xavier Munoz. 2007. Image Classification using Random Forests and Ferns. In Proceedings of the International Conference on Computer Vision (ICCV’07). 1–8.\nTat-Seng Chua, Jinhui Tang, Richang Hong, Haojie Li, Zhiping Luo, and Yan-Tao Zheng. 2009. NUS-WIDE: A real-world web image database from National University of Singapore. In Proceedings of ACM Conference on Image and Video Retrieval (CIVR’09). 1–9.",
    "section_title": "CONCLUSION",
    "section_type": "conclusion",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 40,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 438,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s1_c5",
    "source_id": "2808205",
    "text": "Ali Farhadi, Seyyed Mohammad Mohsen Hejrati, Mohammad Amin Sadeghi, Peter Young, Cyrus Rashtchian, Julia Hockenmaier, and David A. Forsyth. 2010. Every picture tells a story: Generating sentences from images. In Proceedings of the European Conference on Computer Vision (ECCV’10). 15–29.\nFangxiang Feng, Xiaojie Wang, and Ruifan Li. 2014. Cross-modal retrieval with correspondence autoencoder. In Proceedings of the International Conference on Multimedia (MM’14). 7–16.",
    "section_title": "CONCLUSION",
    "section_type": "conclusion",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 41,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 469,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s1_c6",
    "source_id": "2808205",
    "text": "Andrea Frome, Greg Corrado, Jon Shlens, Samy Bengio, Jeffrey Dean, Marc’Aurelio Ranzato, and Tomas Mikolov. 2013. DeViSE: A deep visual-semantic embedding model. In Neural Information Processing Systems (NIPS’13), 2121–2129.\nDavid R. Hardoon, Sandor Szedmak, and John Shawe-Taylor. 2004. Canonical correlation analysis; An overview with application to learning methods. Neural Comput. 16, 2639–2664.",
    "section_title": "CONCLUSION",
    "section_type": "conclusion",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 42,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 399,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s1_c7",
    "source_id": "2808205",
    "text": "G. Hinton and R. Salakhutdinov. 2006. Reducing the dimensionality of data with neural networks. Science 313, 5786, 504–507.\nG. E. Hinton. 2002. Training products of experts by minimizing contrastive divergence. Neural Comput. 14, 8, 1771–1800.\nG. E. Hinton, S. Osindero, and Y. Teh. 2006. A fast learning algorithm for deep belief nets. Neural Comput. 18, 7, 1527–1554.",
    "section_title": "CONCLUSION",
    "section_type": "conclusion",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 43,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 369,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s1_c8",
    "source_id": "2808205",
    "text": "Yangqing Jia, Mathieu Salzmann, and Trevor Darrell. 2011. Learning cross-modality similarity for multinomial data. In Proceedings of the International Conference on Computer Vision (ICCV’11). 2407–2414.\nJungi Kim, Jinseok Nam, and Iryna Gurevych. 2012. Learning semantics with deep belief network for cross-language information retrieval. In Proceedings of the 25th International Conference on Computational Linguistics (COLING’12). 579–588.",
    "section_title": "CONCLUSION",
    "section_type": "conclusion",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 44,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 441,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s1_c9",
    "source_id": "2808205",
    "text": "B. S. Manjunath, J. R. Ohm, V. V. Vinod, and A. Yamada. 2001. Color and texture descriptors. IEEE Trans. Circuits Syst. Video Technol. 11, 6, 703–715.\nJ. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng. 2011. Multimodal deep learning. In Proceedings of the 28th International Conference on Machine Learning (ICML’11). 689–696.\nA. Oliva and A. Torralba. 2001. Modeling the shape of the scene: A holistic representation of the spatial envelope. Int. J. Comput. Vision 42, 3, 145–175.",
    "section_title": "CONCLUSION",
    "section_type": "conclusion",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 45,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 488,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s1_c10",
    "source_id": "2808205",
    "text": "Nikhil Rasiwasia, Jose Costa Pereira, Emanuele Coviello, Gabriel Doyle, Gert R. G. Lanckriet, Roger Levy, and Nuno Vasconcelos. 2010. A new approach to cross-modal multimedia retrieval. In Proceedings of the International Conference on Multimedia (MM’10). 251–260.\nR. Salakhutdinov and G. Hinton. 2009. Replicated Softmax: an Undirected Topic Model. In Neural Information Processing Systems (NIPS’09), 1607–1614.",
    "section_title": "CONCLUSION",
    "section_type": "conclusion",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 46,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 412,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s1_c11",
    "source_id": "2808205",
    "text": "Ruslan R. Salakhutdinov and Geoffrey G. Hinton. 2012. An efficient learning procedure for deep Boltzmann machines. Neural Comput. 24, 8, 1967–2006.\nCarina Silberer and Mirella Lapata. 2014. Learning grounded meaning representations with autoencoders. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 721–732.",
    "section_title": "CONCLUSION",
    "section_type": "conclusion",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 47,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 375,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s1_c12",
    "source_id": "2808205",
    "text": "P. Smolensky. 1986. Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1. MIT Press, Cambridge, MA, Chapter Information processing in dynamical systems: foundations of harmony theory, 194–281.",
    "section_title": "CONCLUSION",
    "section_type": "conclusion",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 48,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 228,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s1_c13",
    "source_id": "2808205",
    "text": "Richard Socher, Milind Ganjoo, Christopher D. Manning, and Andrew Ng. 2013. Zero-shot learning through cross-modal transfer. In Neural Information Processing Systems (NIPS’13), 935–943.\nN. Srivastava and R. Salakhutdinov. 2012a. Learning representations for multimodal data with deep belief nets. In Proceedings of the International Conference on Machine Learning Representation Learning Workshop.",
    "section_title": "CONCLUSION",
    "section_type": "conclusion",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 49,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 397,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s1_c14",
    "source_id": "2808205",
    "text": "N. Srivastava and R. Salakhutdinov. 2012b. Multimodal learning with deep Boltzmann machines. In Neural Information Processing Systems (NIPS’12), 2231–2239.\nL. J. P. van der Maaten and G. E. Hinton. 2008. Visualizing High-Dimensional Data Using t-SNE. J. Machine Learn. Res. 9, 2579–2605.\nAndrea Vedaldi and Brian Fulkerson. 2010. Vlfeat: An open and portable library of computer vision algorithms. In Proceedings of the International Conference on Multimedia (MM’10). 1469–1472.",
    "section_title": "CONCLUSION",
    "section_type": "conclusion",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 50,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 478,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s1_c15",
    "source_id": "2808205",
    "text": "M. Welling, M. Rosen-Zvi, and G. Hinton. 2004. Exponential family harmoniums with an application to information retrieval. In Neural Information Processing Systems (NIPS’04), 501–508.\nJason Weston, Samy Bengio, and Nicolas Usunier. 2010. Large scale image annotation: Learning to rank with joint word-image embeddings. In Proceedings of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD’10). 21–35.",
    "section_title": "CONCLUSION",
    "section_type": "conclusion",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 51,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 467,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2808205_s1_c16",
    "source_id": "2808205",
    "text": "Yueting Zhuang, Yan Fei Wang, Fei Wu, Yin Zhang, and Weiming Lu. 2013. Supervised coupled dictionary learning with group structures for multi-modal retrieval. In Proceedings of the 27th AAAI Conference on Artificial Intelligence (AAAI’13). 1070–1076.",
    "section_title": "CONCLUSION",
    "section_type": "conclusion",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 52,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 17,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 250,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8925,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2820400_s0_c0",
    "source_id": "2820400",
    "text": "This special issue invites the best papers from ACM Multimedia 2014 to extend their work into journal articles. In 2015, the conference took place in Orlando, FL, USA, introducing several new areas. The selected articles in this issue are from the Deep Learning for Multimedia and Emotional and Social Signals in Multimedia areas. They underwent a rigorous review process, including a two-day technical program committee meeting. The debate surrounded the importance of multimodal and innovative perspectives in best paper candidates. The extended papers, \"Emotion Recognition During Speech Using Dyn",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 4,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 600,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2820400_s0_c1",
    "source_id": "2820400",
    "text": "er candidates. The extended papers, \"Emotion Recognition During Speech Using Dynamics of Multiple Regions of Face\" by Yelin Kim and Emily Mower-Provost, and \"Correspondence Autoencoders for Cross-Modal Retrieval\" by Fangxiang Feng, Xiaojie Wang, and Ruifan Li, significantly extend their conference versions. The first article improves facial emotion recognition during speech, while the second enhances cross-modal retrieval using correspondence autoencoders. Experimental results demonstrate improvements over existing literature. The authors have made their code publicly available, facilitating f",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 4,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 600,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2820400_s0_c2",
    "source_id": "2820400",
    "text": "literature. The authors have made their code publicly available, facilitating further research. These articles may potentially start new trends in future conferences.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 4,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 166,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "2820400_s0_c3",
    "source_id": "2820400",
    "text": "Hayley Hung, Intelligent Systems Department, Delft University of Technology, The Netherlands\nGeorge Toderici, Google Research, Mountain View, USA\nGuest Editors",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 4,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 159,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3191835.3191989_s0_c0",
    "source_id": "3191835.3191989",
    "text": "---\n\nSentiment Analysis of Microblog Combining Dictionary and Rules\n\nDing Yuan, Yanquan Zhou, Ruifan Li, Peng Lu\nSchool of Computers, Beijing University of Posts and Telecommunications\nEngineering Research Center of Information Networks, Ministry of Education, Beijing, China\n{yuanding_bupt, zhouyanquan, rfli, lupeng}@bupt.edu.cn",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 19,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 330,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3191835.3191989_s0_c1",
    "source_id": "3191835.3191989",
    "text": "Abstract—Microblogging emotional classification is a key research area based on User-Generated Content (UGC). This paper focuses on distinguishing between positive and negative emotional tendencies in microblogs. We propose a system that removes noise from microblogs, extracts features, and classifies emotions using Support Vector Machine (SVM). We enhance feature extraction and weight computation algorithms by integrating dictionary and rule-based approaches. Experimental results demonstrate the effectiveness of our method.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 530,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3191835.3191989_s0_c2",
    "source_id": "3191835.3191989",
    "text": "Keywords—emotional classification; feature extraction; weight computing; support vector machine\n\nI. INTRODUCTION",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 112,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3191835.3191989_s0_c3",
    "source_id": "3191835.3191989",
    "text": "The Internet's rapid development and the widespread use of microblogs have generated a vast amount of User-Generated Content (UGC). Analyzing user intent based on UGC, particularly through sentiment analysis, is crucial for various applications. Microblog sentiment analysis, which involves detecting emotional tendencies in text, is a significant challenge due to the brief and diverse nature of microblog content. This paper approaches sentiment analysis for Chinese microblogs as a binary classification problem and introduces improved algorithms for feature extraction and weight calculation, ut",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3191835.3191989_s0_c4",
    "source_id": "3191835.3191989",
    "text": "introduces improved algorithms for feature extraction and weight calculation, utilizing SVM for classification.\nII. RELATED WORK",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 128,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3191835.3191989_s0_c5",
    "source_id": "3191835.3191989",
    "text": "Sentiment analysis extracts subjective information to identify the author's attitude. Affective Computing, introduced by Picard in 1995, integrates computer science, psychology, and cognitive science to enable machines to recognize and respond to human emotions. Microblog sentiment analysis is generally treated as a two-polarity classification problem, where traditional machine learning methods like Naive Bayes, Maximum Entropy, K Nearest Neighbor, and SVM are used to build emotion classifiers. Pang and Lee demonstrated the effectiveness of supervised learning for sentiment classification, wi",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 5,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3191835.3191989_s0_c6",
    "source_id": "3191835.3191989",
    "text": "trated the effectiveness of supervised learning for sentiment classification, with SVM showing higher accuracy compared to other methods. Chinese text processing has also seen improvements with the application of machine learning techniques and various weight calculation algorithms, such as TF-IDF, in conjunction with SVM.\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 6,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 328,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3191835.3191989_s0_c7",
    "source_id": "3191835.3191989",
    "text": "The sentiment analysis method based on dictionary relies on the judgment of reference words from expert dictionaries. English emotional vocabulary primarily comes from WordNet, which organizes nouns, verbs, adjectives, and adverbs into cognitive synonym sets. Each syn-set represents a semantic concept and connects with others through various relations, forming a synonym network. For Chinese text, HowNet serves as the dictionary source. This method compares semantic similarity between unknown words and reference words, sets thresholds to judge sentiment, and accumulates scores from positive an",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 7,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3191835.3191989_s0_c8",
    "source_id": "3191835.3191989",
    "text": "rds, sets thresholds to judge sentiment, and accumulates scores from positive and negative emotional words to determine text sentiment.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 8,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 135,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3191835.3191989_s0_c9",
    "source_id": "3191835.3191989",
    "text": "In our experiment, we integrated dictionary-based and rule-based methods with machine learning. Feature word extraction through dictionaries and rules aids in more attentive weight calculation, making results more reasonable.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 9,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 225,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3191835.3191989_s0_c10",
    "source_id": "3191835.3191989",
    "text": "Microblog sentiment analysis is treated as a classification problem in this paper. We utilized the vector space model to represent microblogs, abstracted their features, mapped them into N-dimensional feature vectors, and used SVM for classification. The main steps include data preprocessing, feature extraction, weight calculation, and classification. We improved feature extraction and weight calculation by integrating dictionaries and rules.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 10,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 446,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3191835.3191989_s0_c11",
    "source_id": "3191835.3191989",
    "text": "The emotional dictionary from the Intelligence Science and Technology Center, Beijing University of Posts and Telecommunications, was used. It contains over 8500 commendatory and 7200 derogatory terms, and more than 200 degree adverbs or dynamic emotion words.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 11,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 260,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3191835.3191989_s0_c12",
    "source_id": "3191835.3191989",
    "text": "Data preprocessing for microblogs includes cleaning, word segmentation, and stop words removal. We used the jieba segmentation tool and further screened the stop word dictionary. Feature extraction involved a modified CHI algorithm (MCHI) that uses emotional information to select feature words. The MCHI score calculation formula adjusts the standard CHI score based on whether a word is emotional.\n\nThe standard CHI score between word w and class c is calculated as follows:\n\n2 2 ( ) , ) ( )( ) ( AD BC w c A B C D λ − = + +",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 12,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 526,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3191835.3191989_s0_c13",
    "source_id": "3191835.3191989",
    "text": "2 2 ( ) , ) ( )( ) ( AD BC w c A B C D λ − = + +\n\nThis helps emotional words to be more easily selected as feature words.\n\nThis paper presents a sentiment classification method for Chinese microblogs, focusing on feature selection and weight calculation. The number of features is crucial, with an optimal number around 4700 for highest accuracy. A Modified TF-IDF (MTF-IDF) algorithm is proposed, incorporating emotional word power and the influence of adverbs of degree and special punctuation.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 13,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 496,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3191835.3191989_s0_c14",
    "source_id": "3191835.3191989",
    "text": "The experiments conducted include a baseline comparison, efficiency on different corpuses, and a three-classification problem. Results show the proposed method's superiority, especially on simpler corpuses with a single topic. However, the method's performance on complex, multi-topic corpuses and three-classification problems indicates room for improvement.\n\nThe conclusion highlights the effectiveness of the method for emotional polarity analysis of Chinese microblogs and suggests future research directions, including incorporating semantic features and adapting to multi-topic models.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 14,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 591,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3191835.3191989_s0_c15",
    "source_id": "3191835.3191989",
    "text": "Pang B, Lee L, Vaithyanathan S. Sentiment classification using machine learning techniques. EMNLP '02. 2002: 79-86.\nLiu Zhiming, Liu Lu. Empirical study of sentiment classification for Chinese microblog based on machine learning. Computer Engineering and Applications, 2012, 48(1): 1-4.\nZhu Yanlan, Min Jin, Zhou Yaqian, Huang Xuanjin, Wu Lide. Semantic Orientation Computing Based on HowNet. Journal of Chinese Information Processing, 2006, 20(1): 14-20.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 15,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 455,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3191835.3191989_s0_c16",
    "source_id": "3191835.3191989",
    "text": "Yao Tianfang, Lou Decheng. Research on Semantic Orientation Distinction for Chinese Sentiment Words. The Seventh International Conference on Chinese Computing. Wuhang: ICCC 2007, 2007.\nHu M, Liu B. Mining and summarizing customer reviews. KDD '04. 2004: 168-177.\nTurney P D. Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews. ACL '02. 2002: 417-424.\nLiu Xiaojuan, You Bin, Zhang Aiyun. Review on the Data Used in Researches of Microblogs. Journal of Intelligence, 2013, 32(9): 39-45.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 16,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 530,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3191835.3191989_s0_c17",
    "source_id": "3191835.3191989",
    "text": "Zhu Lina. Research of Feature Selection for Chinese Web Page Categorization. Beijing: China University of Petroleum, 2009.\nMa Wenwen, Deng Yigui. New feature weight calculation method for short text. Journal of Computer Applications, 2013, 33(8): 2280-2282, 2292.\nMa Ting, Geng Guohua, Zhou Mingquan. An effective approach to calculate the feature weights. J. of Zhengzhou Univ. (Nat. Sci. Ed.), 2008, 40(4): 48-51.\nQin Shian, Li Fayun. Improved TF-IDF Method in Text Classification. New Technology of Library and Information Service, 2013, 238(10): 27-30.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 17,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 556,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3191835.3191989_s0_c18",
    "source_id": "3191835.3191989",
    "text": "2014 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2014)",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 18,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 103,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3394171.3416296_s0_c0",
    "source_id": "3394171.3416296",
    "text": "---\n\nLearning Visual Features from Product Title for Image Retrieval\n\nFangxiang Feng, Tianrui Niu, Ruifan Li, Xiaojie Wang, Huixing Jiang",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 4,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 137,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3394171.3416296_s1_c0",
    "source_id": "3394171.3416296",
    "text": "The market demand for searching products by images on e-commerce sites is significant. Visual features are crucial in content-based image retrieval. Existing methods primarily use pre-trained models on large-scale datasets like ImageNet, which may not effectively extract features for product images. We address this by utilizing product titles as supervised signals to learn image features. We const",
    "section_title": "ABSTRACT",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 4,
    "chunk_index": 1,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 4,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 400,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9900000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3394171.3416296_s1_c1",
    "source_id": "3394171.3416296",
    "text": "pervised signals to learn image features. We construct an image classification dataset using n-grams from product titles, fine-tune a pre-trained model on this dataset, and extract the basic max-pooling activation of convolutions (MAC) feature. Our method achieved the fourth position in the 2020 ACM Multimedia AI Meets Beauty Grand Challenge using a single ResNet-50 model without human annotations",
    "section_title": "ABSTRACT",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 4,
    "chunk_index": 2,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 4,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 400,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3394171.3416296_s1_c2",
    "source_id": "3394171.3416296",
    "text": "a single ResNet-50 model without human annotations or additional processing. Code is available at: https://github.com/FangxiangFeng/AI-Meets-Beauty-2020.",
    "section_title": "ABSTRACT",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 4,
    "chunk_index": 3,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 4,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 153,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3394171.3416296_s1_c3",
    "source_id": "3394171.3416296",
    "text": "CCS CONCEPTS\n\n• Computing methodologies →Image representations; Visual content-based indexing and retrieval\n• Information systems →Retrieval models and ranking\n\nKEYWORDS\n\nVisual feature learning; Bag of n-grams; Image retrieval; CNN; MAC",
    "section_title": "ABSTRACT",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 4,
    "chunk_index": 4,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 4,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 237,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9450000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3394171.3416296_s2_c0",
    "source_id": "3394171.3416296",
    "text": "Online shopping has gained popularity, with image-based product search becoming a convenient method for users. Content-based image retrieval (CBIR) is key, with feature extraction being its core. While deep neural networks have shown better performance, they require extensive annotated data. We propose using product titles to guide the learning of visual features. Our pipeline involves constructing a dataset for image classification using n-grams from titles, fine-tuning a pre-trained model, and extracting the MAC feature for retrieval based on cosine similarity.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 5,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 569,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3394171.3416296_s3_c0",
    "source_id": "3394171.3416296",
    "text": "CBIR systems rely on automatically indexing images by their visual content, with image features vital to retrieval performance. Traditional CBIR methods used hand-crafted low-level features, while deep CNNs now demonstrate superior performance on various computer vision tasks.\n\n---\n\n---",
    "section_title": "RELATED WORK",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 6,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 13,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 287,
    "chunk_method": "hierarchical",
    "importance_weight": 0.66,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3394171.3416296_s3_c1",
    "source_id": "3394171.3416296",
    "text": "Deep Convolutional Neural Networks (CNNs) have shown remarkable performance in various visual tasks, including image classification, object detection, and semantic segmentation. Their success has led to their application in image retrieval, where the activations of the penultimate fully connected layer from pre-trained CNNs are commonly used as image features. Advanced pooling methods like SPoC, RMAC, RAMAC, MS-RMAC, and GRMAC have been proposed to enhance retrieval performance by incorporating spatial information. The effectiveness of CNN features is dataset-dependent, with pre-trained model",
    "section_title": "RELATED WORK",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 7,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3394171.3416296_s3_c2",
    "source_id": "3394171.3416296",
    "text": ". The effectiveness of CNN features is dataset-dependent, with pre-trained models on target datasets generally performing better. However, most image retrieval datasets are small or lack annotations, necessitating methods that utilize pre-trained CNNs on large, annotated datasets, followed by retraining on the target dataset, though this is not feasible when the target dataset lacks labels.",
    "section_title": "RELATED WORK",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 8,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 393,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3394171.3416296_s3_c3",
    "source_id": "3394171.3416296",
    "text": "Our method, detailed in Figure 2, addresses this by leveraging the titles of product images. It consists of two stages: an offline stage, where we construct an image classification dataset from product titles, fine-tune a pre-trained CNN (ResNet-50), and extract MAC features; and an online stage, where we compute similarity scores for ranking given an image query. The key innovation is the conversion of product titles into discrete labels for supervised learning. We use \"Bag of n-grams\" instead of \"Bag of Words\" to address the overlap issue and multi-label fine-tuning of the ResNet-50 model.",
    "section_title": "RELATED WORK",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 9,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3394171.3416296_s3_c4",
    "source_id": "3394171.3416296",
    "text": "For evaluation, we use the Perfect-500K dataset from the \"AI Meets Beauty\" challenge, with a private test set provided for the competition. Retrieval performance is measured by Mean Average Precision (MAP), with a focus on MAP@7 in our experiments. We implement our approach using the BiT toolkit, fine-tuning the model on ImageNet-21k, and creating a vocabulary from the product titles.\n\nOur experimental results demonstrate...\n\n---",
    "section_title": "RELATED WORK",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 10,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 433,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3394171.3416296_s3_c5",
    "source_id": "3394171.3416296",
    "text": "Table 1 presents the results of five deep CNN models with four different pooling methods on the validation set. The first two models, SEResnet-152 and Densenet-201, are trained on the ImageNet-1k dataset, and were used by the previous year's challenge champion. The third and fourth models are ResNet-50 trained on the ImageNet-1k and ImageNet-21k datasets, respectively. The last model is a ResNet-50 pre-trained on ImageNet-21k and fine-tuned on the Perfect-500K dataset. Observations include: the deepest model, Densenet-201, shows the best performance among the ImageNet-1k trained models, while",
    "section_title": "RELATED WORK",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 11,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3394171.3416296_s3_c6",
    "source_id": "3394171.3416296",
    "text": "enet-201, shows the best performance among the ImageNet-1k trained models, while the shallowest model, ResNet-50, performs the worst, indicating that model complexity correlates with feature effectiveness. The model trained on ImageNet-21k outperforms all ImageNet-1k models, suggesting that dataset scale is a significant factor in feature extraction. Fine-tuning the ResNet-50 with the Perfect-500k dataset further significantly improves retrieval performance, demonstrating the dataset's impact on feature effectiveness. These conclusions hold across all pooling methods.",
    "section_title": "RELATED WORK",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 12,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 574,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3394171.3416296_s3_c7",
    "source_id": "3394171.3416296",
    "text": "Table 2 shows the performance on the private test set. Our team, using the fine-tuned ResNet-50 with the simplest MAC feature, ranks fourth with a MAP@7 of 0.402402, only slightly behind the third-place score.\n\nFigure 3 visualizes the retrieval results for two types of queries: real object images and advertisement images, showcasing our method's effectiveness in retrieving relevant products from these application scenarios.",
    "section_title": "RELATED WORK",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 13,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 427,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3394171.3416296_s3_c8",
    "source_id": "3394171.3416296",
    "text": "In conclusion, we propose a method for learning product visual representations by building an image classification dataset based on product images and title information. Empirical results show that deep CNNs pre-trained and fine-tuned on this dataset can enhance feature effectiveness for product image retrieval. For Chinese titled products, our method can create a high-quality label set without word segmentation.\n\nWe acknowledge the support of NSFC (No. 61906018), MoE-CMCC “Artificial Intelligence” Project (No. MCM20190701), and the Fundamental Research Funds for the Central Universities.",
    "section_title": "RELATED WORK",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 14,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 595,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3394171.3416296_s3_c9",
    "source_id": "3394171.3416296",
    "text": "Lin, Z., Xie, H., Kang, P., Yang, Z., Liu, W., & Li, Q. (2019). Cross-domain Beauty Item Retrieval via Unsupervised Embedding Learning. In ACM Multimedia.\n\nLin, Z., Yang, Z., Huang, F., & Chen, J. (2018). Regional Maximum Activations of Convolutions with Attention for Cross-domain Beauty and Personal Care Product Retrieval. In ACM Multimedia.\n\nLowe, D. G. (1999). Object Recognition from Local Scale-Invariant Features. In Proceedings of the International Conference on Computer Vision.",
    "section_title": "RELATED WORK",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 15,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 488,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3394171.3416296_s3_c10",
    "source_id": "3394171.3416296",
    "text": "Oliva, A., & Torralba, A. (2001). Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope. Int’l Journal of Computer Vision.\n\nPedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., & Duchesnay, E. (2011). Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research.\n\nPerronnin, F., & Dance, C. R. (2007). Fisher Kernels on Visual Vocabularies for Image Categorization. In CVPR.",
    "section_title": "RELATED WORK",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 16,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 573,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3394171.3416296_s3_c11",
    "source_id": "3394171.3416296",
    "text": "Sharif Razavian, A., Azizpour, H., Sullivan, J., & Carlsson, S. (2014). CNN Features off-the-shelf: an Astounding Baseline for Recognition. http://arxiv.org/abs/1403.6382\n\nSchall, K., Barthel, K. U., Hezel, N., & Jung, K. (2019). Deep Aggregation of Regional Convolutional Activations for Content Based Image Retrieval. In MMSP.\n\nTolias, G., Sicre, R., & Jégou, H. (2016). Particular object retrieval with integral max-pooling of CNN activations. In ICLR (Poster).",
    "section_title": "RELATED WORK",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 17,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 464,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3394171.3416296_s3_c12",
    "source_id": "3394171.3416296",
    "text": "Wang, Q., Lai, J., Xu, K., Liu, W., & Lei, L. (2018). Beauty Product Image Retrieval Based on Multi-Feature Fusion and Feature Aggregation. In ACM Multimedia.\n\nYu, J., Xie, G., Li, M., Xie, H., & Yu, L. (2019). Beauty Product Retrieval Based on Regional Maximum Activation of Convolutions with Generalized Attention. In ACM Multimedia.\n\nZhang, Y., Qu, L., He, L., Lu, W., & Gao, X. (2019). Beauty Aware Network: An Unsupervised Method for Makeup Product Retrieval. In ACM Multimedia.\n\nGrand Challenge: AI Meets Beauty MM '20, October 12–16, 2020, Seattle, WA, USA.",
    "section_title": "RELATED WORK",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 18,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 564,
    "chunk_method": "hierarchical",
    "importance_weight": 0.63,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3469877.3490585_s0_c0",
    "source_id": "3469877.3490585",
    "text": "S2TD: A Tree-Structured Decoder for Image Paragraph Captioning\n\nYihui Shi, Yun Liu, Fangxiang Feng, Ruifan Li, Zhanyu Ma, Xiaojie Wang",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 4,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 134,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3469877.3490585_s0_c1",
    "source_id": "3469877.3490585",
    "text": "Image paragraph captioning requires organizing linguistic counterparts from abundant visual clues. Previous methods struggle with holistic organization and capturing structural nature. We propose Splitting to Tree Decoder (S2TD), a novel tree-structured decoder that models paragraph decoding as top-down binary tree expansion. S2TD includes a split module, a score module, and a word-level RNN. The split module gates visual representations, and the score module uses cosine similarity for node splitting. A tree structure loss enables end-to-end learning. The word-level RNN decodes leaf nodes int",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 4,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3469877.3490585_s0_c2",
    "source_id": "3469877.3490585",
    "text": "ture loss enables end-to-end learning. The word-level RNN decodes leaf nodes into sentences. Experiments on the Stanford benchmark dataset demonstrate the effectiveness of S2TD.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 4,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 177,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3469877.3490585_s0_c3",
    "source_id": "3469877.3490585",
    "text": "CCS CONCEPTS\n• Computing methodologies → Computer vision tasks\n\nKEYWORDS\nimage captioning, paragraph generation, tree-structured decoder, vision and language",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 4,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 157,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3469877.3490585_s1_c0",
    "source_id": "3469877.3490585",
    "text": "Image paragraph captioning aims to generate a semantically rich and coherent paragraph description, integrating visual understanding and linguistic processing. While single sentence captioning has shown progress, paragraph generation remains challenging due to the need for detailed visual information and structural considerations among sentences. Existing approaches, such as hierarchical and non-hierarchical decoders, have limitations in managing visual observations and maintaining paragraph coherence.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 4,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 31,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 507,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3469877.3490585_s1_c1",
    "source_id": "3469877.3490585",
    "text": "We propose a tree-structured visual paragraph decoder, S2TD, to address these issues. S2TD starts with the entire image and expands observations top-down, ensuring a holistic approach and effective structuring of topic-related sentences. The three-module architecture of S2TD includes a split module, a score module, and a word-level RNN. The tree structure loss aids in learning from pre-parsed ground truth paragraphs, enhancing interpretability. Our contributions are as follows: [list of contributions should follow but is not provided in the original text].\n\n---",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 5,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 31,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 567,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3469877.3490585_s1_c2",
    "source_id": "3469877.3490585",
    "text": "---\n\nWe introduce a tree-structured decoding approach for visual paragraph generation, which biases the model to capture structural relationships between sentences and connect visual observations to linguistic elements. Our novel tree structured decoder, S2TD, employs a topology prediction technique for image paragraph captioning, involving a node splitting mechanism and a unique tree structure loss.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 6,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 31,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 403,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3469877.3490585_s1_c3",
    "source_id": "3469877.3490585",
    "text": "In the realm of image paragraph captioning, methods are categorized into hierarchical and non-hierarchical approaches. Hierarchical methods model paragraphs as sequences of sentences, focusing on topic representations and transitions, while non-hierarchical methods treat paragraphs as sequences of words, addressing long-distance dependencies and visual information utilization. However, both largely rely on sequential decoding and ignore linguistic structures within paragraphs.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 7,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 31,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 481,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3469877.3490585_s1_c4",
    "source_id": "3469877.3490585",
    "text": "Recently, incorporating tree structures into visual decoders has gained attention. Our S2TD differs by requiring only leaf node labeling and aligning image observations with tree structures during decoding.\n\nOur approach to image paragraph captioning involves describing an image with a paragraph composed of multiple sentences. We extract region-level visual semantic representations and a global image representation. The S2TD decoder, depicted in Figure 2, collaborates with a split module, a score module, and a word-level RNN to generate a coherent paragraph.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 8,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 31,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 564,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3469877.3490585_s1_c5",
    "source_id": "3469877.3490585",
    "text": "S2TD is a hierarchical decoder with a split module that preserves parent node information and models differences between child nodes. The score module predicts the tree topology by deciding whether to split nodes, using cosine similarity as a decision score.\n\n---\n\n[Figure 2 and its description have been omitted as per the instruction to avoid adding non-existent elements like figure descriptions outside of the provided content.]\n\n---",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 9,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 31,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 437,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3469877.3490585_s1_c6",
    "source_id": "3469877.3490585",
    "text": "We base the retention or discarding of proposals on the decision score sp, where sp ≤α indicates retention. The hyperparameter α ∈[0, 1] represents a constant threshold to avoid redundant descriptions from overly similar node representations. The binary tree expansion during inference is detailed in Algorithm 1. Following tree expansion, a word-level RNN generates sentences based on node representations. To produce the i-th sentence of a paragraph, a Highway Network transforms the corresponding leaf node representation vleaf i into a topic vector ti ∈RDe. We then employ LSTM to decode sentenc",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 10,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 31,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3469877.3490585_s1_c7",
    "source_id": "3469877.3490585",
    "text": "ation vleaf i into a topic vector ti ∈RDe. We then employ LSTM to decode sentences, with the hidden state hi,j ∈RDe obtained recurrently. The weight We is a learnable word embedding matrix with a vocabulary size Dw. The hidden state is initialized as hi,0 = Highway1(ti), and the memory cell as zeros. The conditional distribution over output words is calculated by concatenating hidden states with the topic vector.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 11,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 31,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 416,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3469877.3490585_s1_c8",
    "source_id": "3469877.3490585",
    "text": "For learning tree topology, we propose using hierarchical clustering to mine tree structures from ground-truth paragraphs. We encode sentences into dense vectors using a pre-trained language model and apply Ward’s minimum variance method, ensuring only nearby clusters merge. The obtained tree has leaf nodes corresponding to the sentences of the given paragraph. The learning criteria for each node’s decision score sp is designed to encourage less similarity for nodes to be split and to push the decision score over the threshold α for nodes to be stopped.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 12,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 31,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 559,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3469877.3490585_s1_c9",
    "source_id": "3469877.3490585",
    "text": "The loss function for training S2TD combines a tree structure loss on decision scores and a cross-entropy loss on word-level distributions. Reinforcement learning with self-critical sequence training (SCST) further optimizes S2TD using non-differentiable metrics.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 13,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 31,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 263,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3469877.3490585_s1_c10",
    "source_id": "3469877.3490585",
    "text": "Our experiments are conducted on the Stanford image paragraph benchmark dataset. We use Faster-RCNN for region detection and feature extraction, and a Highway Network to reduce feature dimensionality. Sentence-BERT encodes sentences for pre-parsed paragraph trees. S2TD uses a double-layer LSTM with an embedding and hidden size of De = 512. We set α to 0.3 and use greedy search for inference. For supervised learning, S2TD is trained with Adam, and for reinforcement learning, we adopt BLEU-4 and METEOR as reward functions.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 14,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 31,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 526,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3469877.3490585_s1_c11",
    "source_id": "3469877.3490585",
    "text": "Performance evaluation compares S2TD with state-of-the-art sequential decoding methods, grouped into hierarchical and non-hierarchical methods. Experimental results show the effectiveness of S2TD.\n\n---",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 15,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 31,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 201,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3469877.3490585_s1_c12",
    "source_id": "3469877.3490585",
    "text": "---\n\nDAM-ATT [27]: 35.02, 20.24, 11.68, 6.57, 13.91, 17.32\nSCST [18]: 32.78, 19.00, 11.40, 6.89, 13.66, 12.89, 29.67, 16.45, 9.74, 5.88, 13.63, 13.77\nSCST+RP [18]: 35.68, 22.40, 14.04, 8.70, 15.17, 22.68, 43.54, 27.44, 17.33, 10.58, 17.86, 30.63\nCRL [16]: - , - , - , - , - , - , 43.12, 27.03, 16.72, 9.95, 17.42, 31.47\nOR-ATT [31]: 34.97, 20.17, 12.21, 7.46, 13.58, 16.27, 32.84, 18.30, 10.67, 6.21, 13.44, 14.88\nOR-ATT+RP [31]: 37.50, 23.34, 14.63, 9.00, 15.43, 22.85, 43.76, 28.08, 17.88, 10.95, 17.82, 33.38",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 16,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 31,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 511,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3469877.3490585_s1_c13",
    "source_id": "3469877.3490585",
    "text": "Our S2TD: 44.32, 25.86, 14.80, 8.33, 16.89, 21.41, 43.70, 26.67, 16.30, 9.79, 17.32, 22.84\nOur S2TD+RP: 44.59, 26.06, 14.93, 8.35, 17.00, 21.92, 44.47, 27.38, 16.87, 10.17, 17.64, 24.33\n\nTable 2: Ablation study on S2TD under diverse variants.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 17,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 31,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 242,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3469877.3490585_s1_c14",
    "source_id": "3469877.3490585",
    "text": "Table 2: Ablation study on S2TD under diverse variants.\n\nModel B1 B2 B3 B4 M C\nSplit-H: 42.83, 24.46, 13.74, 7.58, 16.38, 19.00\nSplit-G: 42.87, 24.99, 14.61, 8.46, 16.37, 19.50\nScore-C: 37.37, 21.21, 11.82, 6.44, 14.74, 16.27\nBFS-R: 44.30, 25.91, 14.80, 8.28, 16.83, 20.51\nDFS-L: 44.02, 25.57, 14.60, 8.19, 16.71, 20.82\nDFS-R: 43.34, 25.04, 14.42, 7.99, 16.45, 19.53\nFull S2TD: 44.32, 25.86, 14.80, 8.33, 16.89, 21.41",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 18,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 31,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 417,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3469877.3490585_s1_c15",
    "source_id": "3469877.3490585",
    "text": "Our S2TD achieves competitive performance, especially in terms of BLEU-1 and CIDEr. The effectiveness of tree-structured decoding is demonstrated. Under the RL setting, richer visual encoding and advanced reinforcement learning methods are crucial to high performance. Non-hierarchical methods benefit from the trial and error property of RL, while hierarchical methods like our S2TD are more robust with less performance drop when repetition penalty is applied.\n\n4.4 Ablation Study",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 19,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 31,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 482,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3469877.3490585_s1_c16",
    "source_id": "3469877.3490585",
    "text": "4.4 Ablation Study\n\nWe evaluate variants on module design and diverse inference strategies. Split-H replaces the gate unit with two single-layer Highway networks, Split-G uses two independent gates, and Score-C adopts a binary classification variant of the score module. Our S2TD benefits more from the gating mechanism, showcasing the effectiveness of our split module. The tree structure loss is smoother than binary cross-entropy, optimizing relative similarity instead of specific targets.\n\nFigure 3: Comparison between S2TD and Score-C on the tree loss parameter α.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 20,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 31,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 570,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3469877.3490585_s1_c17",
    "source_id": "3469877.3490585",
    "text": "Figure 3: Comparison between S2TD and Score-C on the tree loss parameter α.\n\nFigure 4: Qualitative comparison on generated captions for images with SCST+RP and our S2TD.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 21,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 31,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 169,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3469877.3490585_s1_c18",
    "source_id": "3469877.3490585",
    "text": "We compare our S2TD with Score-C to demonstrate the effectiveness of our tree loss. By varying the threshold of a trained model, we observe its impact on text generation, as shown in Figure 3. For a fair comparison, we retrain S2TD with α = 0.5. S2TD exhibits a smoother change in CIDEr scores as α increases from 0.1 to 0.5, indicating its ability to better evaluate different splittings with continuous scores, which supports the plausibility of our tree structure loss.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 22,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 31,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 472,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3469877.3490585_s1_c19",
    "source_id": "3469877.3490585",
    "text": "Our inference strategy is presented in Algorithm 1. By replacing the queue with a stack or reversing the pushing order of child nodes, we can derive different strategies. Algorithm 1 can be denoted as BFS-L (Breadth-First Searching with left node first), and we can have BFS-R, DFS-L, and DFS-R. BFS strategies are found to outperform DFS, which aligns with expectations as BFS aims to capture broader topics, while DFS focuses on specific topics. Additionally, expanding left nodes first yields better metrics, possibly because the left subtree of the root mainly describes salient objects, while t",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 23,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 31,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3469877.3490585_s1_c20",
    "source_id": "3469877.3490585",
    "text": "y because the left subtree of the root mainly describes salient objects, while the right subtree focuses on the background.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 24,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 31,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 123,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3469877.3490585_s1_c21",
    "source_id": "3469877.3490585",
    "text": "In Section 4.5, we provide qualitative analysis to showcase the model's capability by comparing the captioning results with those of the SCST+RP model. S2TD paragraphs are less redundant and more coherent. For instance, S2TD provides a compact description and smoother viewpoint transitions, with additional details. The sentence tree structure ensures that nodes represent topic-related aspects, and the splitting mechanism is semantically related to the image, resembling human cognitive processes.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 25,
    "chunk_index_in_section": 21,
    "total_chunks_in_section": 31,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 500,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3469877.3490585_s1_c22",
    "source_id": "3469877.3490585",
    "text": "In conclusion, we propose a tree-structured decoder S2TD for image paragraph captioning, which demonstrates promising performance in generating diverse and coherent paragraphs. Future work includes extending our method with richer visual encoding and developing a corresponding RL method for tree structures.\n\nThis work was supported by Grants 2019YFF0303300, 2019YFF0303302, 61802026, 61906018, the 111 Project under Grant B08004, and the Fundamental Research Funds for the Central Universities under Grant 2021RC36.\n\n---",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 26,
    "chunk_index_in_section": 22,
    "total_chunks_in_section": 31,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 522,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3469877.3490585_s1_c23",
    "source_id": "3469877.3490585",
    "text": "---\n\n[19] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Annual Meeting of the Association for Computational Linguistics (ACL). 311–318.\n\n[20] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 3982–3992.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 27,
    "chunk_index_in_section": 23,
    "total_chunks_in_section": 31,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 523,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3469877.3490585_s1_c24",
    "source_id": "3469877.3490585",
    "text": "[21] Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. 2017. Self-critical sequence training for image captioning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 7008–7024.\n\n[22] Zhan Shi, Xu Zhou, Xipeng Qiu, and Xiaodan Zhu. 2020. Improving Image Captioning with Better Use of Caption. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL). 7454–7464.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 28,
    "chunk_index_in_section": 24,
    "total_chunks_in_section": 31,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 457,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3469877.3490585_s1_c25",
    "source_id": "3469877.3490585",
    "text": "[23] Rupesh K Srivastava, Klaus Greff, and Jürgen Schmidhuber. 2015. Training very deep networks. Advances in Neural Information Processing Systems (NIPS) 28 (2015), 2377–2385.\n\n[24] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. 2015. Cider: Consensus-based image description evaluation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 4566–4575.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 29,
    "chunk_index_in_section": 25,
    "total_chunks_in_section": 31,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 385,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3469877.3490585_s1_c26",
    "source_id": "3469877.3490585",
    "text": "[25] Hao Wang, Guosheng Lin, Steven CH Hoi, and Chunyan Miao. 2020. Structure-Aware Generation Network for Recipe Generation from Images. In Proceedings of the European Conference on Computer Vision (ECCV). 359–374.\n\n[26] Jing Wang, Yingwei Pan, Ting Yao, Jinhui Tang, and Tao Mei. 2019. Convolutional auto-encoding of sentence topics for image paragraph generation. In International Joint Conference on Artificial Intelligence (IJCAI). 940–946.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 30,
    "chunk_index_in_section": 26,
    "total_chunks_in_section": 31,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 445,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3469877.3490585_s1_c27",
    "source_id": "3469877.3490585",
    "text": "[27] Ziwei Wang, Yadan Luo, Yang Li, Zi Huang, and Hongzhi Yin. 2018. Look deeper see richer: Depth-aware image paragraph captioning. In ACM Conference on Multimedia (MM). 672–680.\n\n[28] Joe H Ward Jr. 1963. Hierarchical grouping to optimize an objective function. J. Amer. Statist. Assoc. 58, 301 (1963), 236–244.\n\n[29] Siying Wu, Zheng-Jun Zha, Zilei Wang, Houqiang Li, and Feng Wu. 2019. Densely Supervised Hierarchical Policy-Value Network for Image Paragraph Generation. In International Joint Conference on Artificial Intelligence (IJCAI). 975–981.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 31,
    "chunk_index_in_section": 27,
    "total_chunks_in_section": 31,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 554,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3469877.3490585_s1_c28",
    "source_id": "3469877.3490585",
    "text": "[30] Chunpu Xu, Yu Li, Chengming Li, Xiang Ao, Min Yang, and Jinwen Tian. 2020. Interactive Key-Value Memory-augmented Attention for Image Paragraph Captioning. In Proceedings of the 28th International Conference on Computational Linguistics (COLING). 3132–3142.\n\n[31] Li-Chuan Yang, Chih-Yuan Yang, and Jane Yung-jen Hsu. 2021. Object Relation Attention for Image Paragraph Captioning. In AAAI Conference on Artificial Intelligence (AAAI), Vol. 35. 3136–3144.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 32,
    "chunk_index_in_section": 28,
    "total_chunks_in_section": 31,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 460,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3469877.3490585_s1_c29",
    "source_id": "3469877.3490585",
    "text": "[32] Xu Yang, Chongyang Gao, Hanwang Zhang, and Jianfei Cai. 2020. Hierarchical Scene Graph Encoder-Decoder for Image Paragraph Captioning. In ACM Conference on Multimedia (MM). 4181–4189.\n\n[33] Ting Yao, Yingwei Pan, Yehao Li, and Tao Mei. 2019. Hierarchy parsing for image captioning. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). 2621–2629.\n\n[34] Jianshu Zhang, Jun Du, Yongxin Yang, Yi-Zhe Song, Si Wei, and Lirong Dai. 2020. A Tree-Structured Decoder for Image-to-Markup Generation. In International Conference on Machine Learning (ICML). 11076–11085.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 33,
    "chunk_index_in_section": 29,
    "total_chunks_in_section": 31,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 593,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3469877.3490585_s1_c30",
    "source_id": "3469877.3490585",
    "text": "[35] Xuying Zhang, Xiaoshuai Sun, Yunpeng Luo, Jiayi Ji, Yiyi Zhou, Yongjian Wu, Feiyue Huang, and Rongrong Ji. 2021. RSTNet: Captioning With Adaptive Attention on Visual and Non-Visual Words. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 15465–15474.\n\n[36] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason Corso, and Jianfeng Gao. 2020. Unified vision-language pre-training for image captioning and vqa. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), Vol. 34. 13041–13049.\n\n---",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 34,
    "chunk_index_in_section": 30,
    "total_chunks_in_section": 31,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 540,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3483207.3483233_s0_c0",
    "source_id": "3483207.3483233",
    "text": "Maintenance Decision Generator for Electrical Equipment Based on Reinforcement Learning\n\nRuifan Li∗, Zeyuan Wang, Yifan Du, Zepeng Zhai, Yongping Xiong, Ziqun Liu\n\nBeijing University of Posts and Telecommunications, 100876, Beijing, China\n\nState Grid Jiangsu Electric Power Co., Ltd. Research Institute, 211103, Nanjing, China",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 7,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 326,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3483207.3483233_s1_c0",
    "source_id": "3483207.3483233",
    "text": "The paper proposes a maintenance decision model for electrical equipment using the Markov hypothesis and reinforcement learning. It incorporates the cut set of the power grid to calculate equipment weights and enhance the dynamic programming solution for decision-making. The method recalculates action rewards based on the current cut set, facilitating communication between action sequences. Experi",
    "section_title": "ABSTRACT",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 7,
    "chunk_index": 1,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 3,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 400,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9900000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3483207.3483233_s1_c1",
    "source_id": "3483207.3483233",
    "text": "ing communication between action sequences. Experimental results show improved decision-making effectiveness.",
    "section_title": "ABSTRACT",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 7,
    "chunk_index": 2,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 109,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3483207.3483233_s1_c2",
    "source_id": "3483207.3483233",
    "text": "CCS CONCEPTS\n• Computing methodologies → Planning for deterministic actions.\n\nKEYWORDS\nElectric equipment maintenance, Reinforcement learning, Dynamic decision making.",
    "section_title": "ABSTRACT",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 7,
    "chunk_index": 3,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 167,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9450000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3483207.3483233_s2_c0",
    "source_id": "3483207.3483233",
    "text": "Electrical equipment maintenance decisions aim to prevent damage and grid instability. Traditional methods rely on manual decisions and rule-based strategies, which lack generalization. Reinforcement learning, particularly with dynamic programming, is explored for optimal maintenance timing. The paper addresses the varying importance of equipment and the influence of maintenance decisions on each other within a multi-equipment context.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 7,
    "chunk_index": 4,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 439,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3483207.3483233_s3_c0",
    "source_id": "3483207.3483233",
    "text": "2.1 Multi-Agent Reinforcement Learning\nMulti-agent reinforcement learning is applied in various fields and enables the development of strategies beyond human capabilities.\n\n2.2 Electric Equipment Maintenance\n---\n\n[Please note that the section \"2 RELATED WORK\" seems to be cut off and continues beyond the provided text. Also, the figure and its description have been omitted as per the instructions to avoid adding non-existent sections or content.]\n\n---",
    "section_title": "RELATED WORK",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 7,
    "chunk_index": 5,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 2,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 454,
    "chunk_method": "hierarchical",
    "importance_weight": 0.66,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3483207.3483233_s3_c1",
    "source_id": "3483207.3483233",
    "text": "At present, the strategy of power equipment maintenance often relies on manual decision-making. Existing methods, such as [9], mainly depend on online and offline detection, as well as periodic disassembly detection. While [6] analyzes the irrationality of power maintenance and proposes strategies, these strategies require technicians with substantial experience and are inefficient with poor generalization. A deep recursive q-network Multi-Agent Reinforcement Learning model is applied to power equipment maintenance, demonstrating improved optimization, decision-making abilities, and reduced m\ne, demonstrating improved optimization, decision-making abilities, and reduced maintenance costs.",
    "section_title": "RELATED WORK",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 7,
    "chunk_index": 6,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 2,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 697,
    "chunk_method": "hierarchical",
    "importance_weight": 0.63,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3483207.3483233_s4_c0",
    "source_id": "3483207.3483233",
    "text": "In this section, we describe our proposed maintenance decision model for electric equipment. We introduce the Markov hypothesis and reward function in Section 3.1, followed by the maintenance decision model for single equipment in Section 3.2 and multiple equipment in Section 3.3.\n\n3.1 Markov Hypothesis and Reward",
    "section_title": "METHOD",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 7,
    "chunk_index": 7,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 16,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 315,
    "chunk_method": "hierarchical",
    "importance_weight": 0.935,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3483207.3483233_s4_c1",
    "source_id": "3483207.3483233",
    "text": "3.1 Markov Hypothesis and Reward\n\n3.1.1 The equipment condition transitions through multiple states to the fault state. We define four states, S1, S2, S3, and S4, representing the equipment's condition from good to damaged. The transition probability matrix P and the probability of transitioning to the damaged state λi are defined.",
    "section_title": "METHOD",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 7,
    "chunk_index": 8,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 333,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3483207.3483233_s4_c2",
    "source_id": "3483207.3483233",
    "text": "3.1.2 The entire risk of the power network is divided into maintenance risk RM and grid failure risk RF. The risk can be expressed as Rsum = RM,1 + RM,2 + RF,1(t) + RF,2(t), where NT is the number of operation days. Reinforcement learning aims to minimize this overall loss.\n\n3.2 Maintenance Decision Method for Single Equipment",
    "section_title": "METHOD",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 7,
    "chunk_index": 9,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 328,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3483207.3483233_s4_c3",
    "source_id": "3483207.3483233",
    "text": "3.2 Maintenance Decision Method for Single Equipment\n\n3.2.1 The state and action for single equipment in reinforcement learning are defined by time and the equipment's running state. The action set is [fix, nofix], with constraints on actions in certain states.\n\n3.2.2 The dynamic programming solution involves strategy evaluation and improvement. The Bellman equation is used with the state transition probability to evaluate the value of each state and optimize the strategy.",
    "section_title": "METHOD",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 7,
    "chunk_index": 10,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 477,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3483207.3483233_s4_c4",
    "source_id": "3483207.3483233",
    "text": "3.3 Multi-equipment Maintenance Decision Generator\n\n3.3.1 For multiple equipment, the state in reinforcement learning includes equipment index, time, and running state.\n\n3.3.2 Cut sets are introduced to represent the minimum combination of equipment that can cause a power failure. They help model the importance of equipment to the power grid's operating state.",
    "section_title": "METHOD",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 7,
    "chunk_index": 11,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 362,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3483207.3483233_s4_c5",
    "source_id": "3483207.3483233",
    "text": "3.3.3 Weighted Dynamic Programming Solution applies cut set weights to reinforcement learning rewards, focusing on the importance of different equipment. The weight of the equipment Wdevicei (cut) is calculated based on the cut set composition.\n\n---\n\n---\n\nThe awareness of grid connections by equipment can be integrated into decision-making through the application of equipment weight in dynamic programming.",
    "section_title": "METHOD",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 7,
    "chunk_index": 12,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 409,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3483207.3483233_s4_c6",
    "source_id": "3483207.3483233",
    "text": "The dynamic policy generator considers changes in the cut set due to equipment maintenance. For instance, in grid 1, when equipment B is in maintenance, the maintenance of equipment C may lead to increased grid failures.",
    "section_title": "METHOD",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 7,
    "chunk_index": 13,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 220,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3483207.3483233_s4_c7",
    "source_id": "3483207.3483233",
    "text": "Policy evaluation incorporates the weight of equipment, as depicted in Fig. (a). The decision-making process of the dynamic policy generator is presented in Fig. (b), where maintenance actions affect the cut set of the power grid. Subsequent state calculations require adjusting equipment weights based on the current cut set. State management refers to the sequential maintenance decisions for equipment in states S2 and S",
    "section_title": "METHOD",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 7,
    "chunk_index": 14,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 423,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3483207.3483233_s4_c8",
    "source_id": "3483207.3483233",
    "text": "3. The dynamic strategy generation module calculates the new reward for each action using the value matrix and equipment weight:",
    "section_title": "METHOD",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 7,
    "chunk_index": 15,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 128,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3483207.3483233_s4_c9",
    "source_id": "3483207.3483233",
    "text": "π(a | sdevicei) = arg max a [Ra(sdevicei)(cut) + γ Σ s′∈S Pa(ss′)vk(s′)]\n\nThe value matrix is derived from a dynamic programming solution, considering the cut-set state with the equipment node in maintenance mode as an open circuit. The action with the highest reward is selected as the strategy for the current state. This approach allows for indirect communication between multiple equipment through cut set changes integrated into the decision-making process.",
    "section_title": "METHOD",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 7,
    "chunk_index": 16,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 462,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3483207.3483233_s4_c10",
    "source_id": "3483207.3483233",
    "text": "In the experiment, simulation parameters include operating power curves, equipment connection states, equipment state transition probabilities, and maintenance costs. We used various operational power curves and two grid connection modes for multiple equipment. The unit price of grid power is 1053 Yuan/Mkw, and transition probabilities are defined for equipment states. The dataset includes six types of grid power curves, with a simulation time interval of fifty days.",
    "section_title": "METHOD",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 7,
    "chunk_index": 17,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 471,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3483207.3483233_s4_c11",
    "source_id": "3483207.3483233",
    "text": "The experimental setup involves comparing different methods for single and multiple equipment scenarios. For single equipment, we compare template-based policies and strategies obtained from reinforcement learning. For multiple equipment, we compare dynamic programming methods with and without the dynamic policy generator, using average and cut set-weighted approaches.",
    "section_title": "METHOD",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 7,
    "chunk_index": 18,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 371,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3483207.3483233_s4_c12",
    "source_id": "3483207.3483233",
    "text": "The results show that the reinforcement learning-based policy significantly improves upon template policies for single equipment and that the dynamic policy generator generally outperforms other methods for multiple equipment. The weighted dynamic routing approach is generally more effective than the average weighted method, indicating the feasibility of equipment weight modeling and state communication through cut sets.",
    "section_title": "METHOD",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 7,
    "chunk_index": 19,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 424,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3483207.3483233_s4_c13",
    "source_id": "3483207.3483233",
    "text": "Further analysis explores the relationship between learned policies and grid power, as well as grid connection. For instance, in state1, where grid power increases linearly, the reinforcement learning strategy adapts to changes in operational power, reflecting a shift from fix to nofix strategies as power and potential maintenance costs increase. This aligns with the goal of minimizing overall loss and demonstrates the efficiency of reinforcement learning in deriving strategies that match human",
    "section_title": "METHOD",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 7,
    "chunk_index": 20,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 499,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3483207.3483233_s4_c14",
    "source_id": "3483207.3483233",
    "text": "inforcement learning in deriving strategies that match human understanding without extensive empirical experience.\n4.4.2 Policy and the Grid Connection\nThe policy learned with the weighted DP solution method in grid 1 is presented in Fig. 6",
    "section_title": "METHOD",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 7,
    "chunk_index": 21,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 240,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3483207.3483233_s4_c15",
    "source_id": "3483207.3483233",
    "text": "(b). The strategy varies for different equipment. Equipment A is more inclined towards the nofix strategy, as its maintenance could lead to power grid failure, whereas the simultaneous maintenance of equipment B and C would result in failure loss. The learned strategy supports the rationality of weighted reward by cut set. Table 3 demonstrates that the dynamic strategy method enhances the strategy's awareness of power grid state.",
    "section_title": "METHOD",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 7,
    "chunk_index": 22,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 433,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8925,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3483207.3483233_s5_c0",
    "source_id": "3483207.3483233",
    "text": "This paper applies reinforcement learning to develop equipment maintenance strategies. For single equipment, dynamic programming is used, while for multiple equipment, the importance of devices is calculated using the power grid cut set and integrated into the dynamic programming solution. The interplay between maintenance decisions is reflected through the cut set changes due to maintenance actions. Simulation data validates the strategy, confirming the effectiveness of our method across variou\ngy, confirming the effectiveness of our method across various power grid scenarios.",
    "section_title": "CONCLUSION",
    "section_type": "conclusion",
    "section_index": 5,
    "total_sections": 7,
    "chunk_index": 23,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 2,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 584,
    "chunk_method": "hierarchical",
    "importance_weight": 0.935,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3483207.3483233_s5_c1",
    "source_id": "3483207.3483233",
    "text": "ACKNOWLEDGMENTS\nThis work is supported by the Science and Technology Program of the Headquarters of State Grid Corporation of China under Grant No. 5200-201918255A-0-0-00.",
    "section_title": "CONCLUSION",
    "section_type": "conclusion",
    "section_index": 5,
    "total_sections": 7,
    "chunk_index": 24,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 2,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 171,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8925,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3483207.3483233_s6_c0",
    "source_id": "3483207.3483233",
    "text": "[1] Christopher Berner et al. 2019. Dota 2 with Large Scale Deep Reinforcement Learning. CoRR abs/1912.06680.\n[2] Sushrut Bhalla et al. 2020. Deep Multi Agent Reinforcement Learning for Autonomous Driving. In Advances in Artificial Intelligence.\n[3] Omar Bouhamed et al. 2019. Q-learning based Routing Scheduling For a Multi-Task Autonomous Agent.\n[4] Dongyan Chen and Kishor S Trivedi. 2005. Optimization for condition-based maintenance with semi-Markov decision process.\n[5] J. Endrenyi et al. 2001. The present status of maintenance strategies and the impact of maintenance on reliability.",
    "section_title": "REFERENCES",
    "section_type": "reference",
    "section_index": 6,
    "total_sections": 7,
    "chunk_index": 25,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 12,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 592,
    "chunk_method": "hierarchical",
    "importance_weight": 0.44000000000000006,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3483207.3483233_s6_c1",
    "source_id": "3483207.3483233",
    "text": "[6] Ding Feng et al. 2018. Optimization Method With Prediction-Based Maintenance Strategy for Traction Power Supply Equipment Based on Risk Quantification.\n\n---\n\n[7] Fang Gao, Si Chen, Mingqiang Li, and Bincheng Huang. 2019. MaCA: a Multi-agent Reinforcement Learning Platform for Collective Intelligence. In 2019 IEEE 10th International Conference on Software Engineering and Service Science (ICSESS). 108–111. https://doi.org/10.1109/ICSESS47205.2019.9040781",
    "section_title": "REFERENCES",
    "section_type": "reference",
    "section_index": 6,
    "total_sections": 7,
    "chunk_index": 26,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 460,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3483207.3483233_s6_c2",
    "source_id": "3483207.3483233",
    "text": "[8] Qi Gao, You-Wei Li, Yang Ge, and Bing Hao. 2011. Availability model of equipment based on three-levels maintenance system. In 2011 International Conference on Quality, Reliability, Risk, Maintenance, and Safety Engineering. 637–641. https://doi.org/10.1109/ICQR2MSE.2011.5976692\n\n[9] Chenxi Guo, Ming Dong, Xiaoxi Yang, and Wensen Wang. 2019. A Review of On-line Condition Monitoring in Power System. In 2019 IEEE 8th International Conference on Advanced Power System Automation and Protection (APAP). 634–637. https://doi.org/10.1109/APAP47170.2019.9225022",
    "section_title": "REFERENCES",
    "section_type": "reference",
    "section_index": 6,
    "total_sections": 7,
    "chunk_index": 27,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 561,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3483207.3483233_s6_c3",
    "source_id": "3483207.3483233",
    "text": "[10] Yiying Li, Wei Zhou, Huaimin Wang, Bo Ding, and Kele Xu. 2019. Improving Fast Adaptation for Newcomers in Multi-Robot Reinforcement Learning System. In 2019 IEEE SmartWorld, Ubiquitous Intelligence Computing, Advanced Trusted Computing, Scalable Computing Communications, Cloud Big Data Computing, Internet of People and Smart City Innovation. 753–760. https://doi.org/10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00162",
    "section_title": "REFERENCES",
    "section_type": "reference",
    "section_index": 6,
    "total_sections": 7,
    "chunk_index": 28,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 427,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3483207.3483233_s6_c4",
    "source_id": "3483207.3483233",
    "text": "[11] Hang Liu, Youyuan Wang, Liwei Zhou, Yufeng Chen, and Xiuming Du. 2016. An optimization method of maintenance strategy for power equipment. In 2016 International Conference on Condition Monitoring and Diagnosis (CMD). 940–943. https://doi.org/10.1109/CMD.2016.7757979\n\n[12] Sergio Rozada, Dimitra Apostolopoulou, and Eduardo Alonso. 2020. Load Frequency Control: A Deep Multi-Agent Reinforcement Learning Approach. In 2020 IEEE Power Energy Society General Meeting (PESGM). 1–5. https://doi.org/10.1109/PESGM41954.2020.9281614",
    "section_title": "REFERENCES",
    "section_type": "reference",
    "section_index": 6,
    "total_sections": 7,
    "chunk_index": 29,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 530,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3483207.3483233_s6_c5",
    "source_id": "3483207.3483233",
    "text": "[13] John Seiffertt, Suman Sanyal, and Donald C. Wunsch. 2008. Hamilton–Jacobi–Bellman Equations and Approximate Dynamic Programming on Time Scales. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics) 38, 4 (2008), 918–923. https://doi.org/10.1109/TSMCB.2008.923532\n\n[14] Alvaro Serra-Gómez, Bruno Brito, Hai Zhu, Jen Jen Chung, and Javier Alonso-Mora. 2020. With whom to communicate: learning efficient communication for multi-robot collision avoidance. In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 11770–11776.",
    "section_title": "REFERENCES",
    "section_type": "reference",
    "section_index": 6,
    "total_sections": 7,
    "chunk_index": 30,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 572,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3483207.3483233_s6_c6",
    "source_id": "3483207.3483233",
    "text": "[15] Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John P. Agapiou, Max Jaderberg, Alexander S. Vezhnevets, Rémi Leblond, Tobias Pohlen, Valentin Dalibard, David Budden, Yury Sulsky, James Molloy, Tom L. Paine, Caglar Gulcehre, Ziyu Wang, Tobias Pfaff, Yuhuai Wu, Roman Ring, Dani Yogatama, Dario Wünsch, Katrina McKinney, Oliver Smith, Tom Schaul, Timothy Lillicrap, Koray Kavukcuoglu, Demi",
    "section_title": "REFERENCES",
    "section_type": "reference",
    "section_index": 6,
    "total_sections": 7,
    "chunk_index": 31,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 600,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3483207.3483233_s6_c7",
    "source_id": "3483207.3483233",
    "text": "a McKinney, Oliver Smith, Tom Schaul, Timothy Lillicrap, Koray Kavukcuoglu, Demis Hassabis, Chris Apps, and David Silver.",
    "section_title": "REFERENCES",
    "section_type": "reference",
    "section_index": 6,
    "total_sections": 7,
    "chunk_index": 32,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 121,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3483207.3483233_s6_c8",
    "source_id": "3483207.3483233",
    "text": "2019. Grandmaster level in StarCraft II using multi-agent reinforcement learning. Nature 575, 7782 (01 Nov 2019), 350–354. https://doi.org/10.1038/s41586-019-1724-z",
    "section_title": "REFERENCES",
    "section_type": "reference",
    "section_index": 6,
    "total_sections": 7,
    "chunk_index": 33,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 164,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3483207.3483233_s6_c9",
    "source_id": "3483207.3483233",
    "text": "[16] Wei Wu, Geng Haifei, and Jiang An. 2009. A Multi-agent Traffic Signal Control System Using Reinforcement Learning. In 2009 Fifth International Conference on Natural Computation, Vol. 4. 553–557. https://doi.org/10.1109/ICNC.2009.66\n\n[17] Di Zhou, Zizhan Wang, You Zhou, Yurong Mao, and Minqi Zhou. 2020. Research on Automatic Test Technology for Field Operation and Maintenance of Intelligent Substation. In 2020 5th Asia Conference on Power and Electrical Engineering (ACPEE). 11–15. https://doi.org/10.1109/ACPEE48638.2020.9136373",
    "section_title": "REFERENCES",
    "section_type": "reference",
    "section_index": 6,
    "total_sections": 7,
    "chunk_index": 34,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 537,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3483207.3483233_s6_c10",
    "source_id": "3483207.3483233",
    "text": "[18] Ming Zhou, Jun Luo, Julian Villela, Yaodong Yang, David Rusu, Jiayu Miao, Weinan Zhang, Montgomery Alban, Iman Fadakar, Zheng Chen, Aurora Chongxi Huang, Ying Wen, Kimia Hassanzadeh, Daniel Graves, Dong Chen, Zhengbang Zhu, Nhat M. Nguyen, Mohamed Elsayed, Kun Shao, Sanjeevan Ahilan, Baokuan Zhang, Jiannan Wu, Zhengang Fu, Kasra Rezaee, Peyman Yadmellat, Mohsen Rohani, Nicolas Perez Nieves, Yihan Ni, Seyedershad Banijamali, Alexander Imani Cowen-Rivers, Zheng Tian, Daniel Palenicek, Haitham Bou-Ammar, Hongbo Zhang, Wulong Liu, Jianye Hao, and Jun Wang.",
    "section_title": "REFERENCES",
    "section_type": "reference",
    "section_index": 6,
    "total_sections": 7,
    "chunk_index": 35,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 563,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3483207.3483233_s6_c11",
    "source_id": "3483207.3483233",
    "text": "2020. SMARTS: Scalable Multi-Agent Reinforcement Learning Training School for Autonomous Driving. CoRR abs/2010.09776 (2020). arXiv:2010.09776 https://arxiv.org/abs/2010.09776\n\n---",
    "section_title": "REFERENCES",
    "section_type": "reference",
    "section_index": 6,
    "total_sections": 7,
    "chunk_index": 36,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 180,
    "chunk_method": "hierarchical",
    "importance_weight": 0.42000000000000004,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3548636.3548646_s0_c0",
    "source_id": "3548636.3548646",
    "text": "EP-BERTGCN: A Simple but Effective Power Equipment Fault Recognition Method\n\nMingcong Lu, Yusong Zhang, Qu-An Zheng, Zhenyuan Ma, Liqing Liu, Yongping Xiong, and Ruifan Li",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 4,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 171,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3548636.3548646_s1_c0",
    "source_id": "3548636.3548646",
    "text": "Text-based power equipment fault recognition is crucial for power equipment maintenance in the advancement of China’s State Grid. We propose EP-BERTGCN, a method combining pre-trained BERT and Graph Convolutional Network (GCN), to bridge the domain gap between electric power and general natural language processing. EP-BERTGCN constructs a graph among documents and words using pre-trained BERT and",
    "section_title": "ABSTRACT",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 4,
    "chunk_index": 1,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 2,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 399,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9900000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3548636.3548646_s1_c1",
    "source_id": "3548636.3548646",
    "text": "ng documents and words using pre-trained BERT and combines softmax outputs from BERT and GCNs for classification. Experimental results demonstrate the superior performance of our method over previous baselines.",
    "section_title": "ABSTRACT",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 4,
    "chunk_index": 2,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 2,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 210,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9450000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3548636.3548646_s2_c0",
    "source_id": "3548636.3548646",
    "text": "The growing complexity of China’s State Grid necessitates efficient fault recognition methods for power equipment. Among these, Text-based Power equipment Fault Recognition (TPFR) uses textual fault information for fault detection, merging text classification with domain-specific knowledge. Despite the success of pre-trained models in NLP, there is a lack of models pre-trained on large-scale electric power corpora. We address this by proposing EP-BERTGCN, which enhances our previous C-TextGCN with BERT and domain adaption. EP-BERTGCN shows improved performance in extensive experiments, contrib",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 3,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 16,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 600,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3548636.3548646_s2_c1",
    "source_id": "3548636.3548646",
    "text": "daption. EP-BERTGCN shows improved performance in extensive experiments, contributing to the following:",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 4,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 103,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3548636.3548646_s2_c2",
    "source_id": "3548636.3548646",
    "text": "- Introduction of the BERT module into C-TextGCN, forming EP-BERTGCN.\n- Comparative experiments on our collected dataset, demonstrating the method’s superior performance.\n- Domain adaption of the BERT model, further enhancing our method’s capabilities.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 5,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 252,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3548636.3548646_s2_c3",
    "source_id": "3548636.3548646",
    "text": "2 RELATED WORKS\nOur work spans text-based power equipment fault recognition, graph neural networks, pre-trained BERT, and cross-domain BERT adaption. Previous research in power equipment fault recognition has largely focused on image modalities, with recent inroads into text-based models for TPFR. Graph Convolutional Neural Network (GCN) has been applied to text classification by constructing graphs based on textual relationships. This approach is extended with the integration of pre-trained BERT and domain-specific adaption in our EP-BERTGCN method.\n\n---",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 6,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 561,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3548636.3548646_s2_c4",
    "source_id": "3548636.3548646",
    "text": "TextGCN improves text classification by capturing the relationship between documents and words through graph convolution, utilizing TF-IDF and PMI for edge weighting. Pre-trained models like BERT have revolutionized NLP with the pretrain-finetune paradigm, which includes sub-tasks such as Masked Language Model (MLM) and Next Sentence Prediction (NSP). Improved pre-trained models like Roberta and ALBERT have been introduced. For cross-domain tasks like TPFR, integrating domain knowledge into pre-trained models for better performance is intuitive. Domain adaptation methods, such as those propos",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 7,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3548636.3548646_s2_c5",
    "source_id": "3548636.3548646",
    "text": "better performance is intuitive. Domain adaptation methods, such as those proposed by Ma et al. and Diao et al., aim to reduce the domain gap.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 8,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 142,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3548636.3548646_s2_c6",
    "source_id": "3548636.3548646",
    "text": "In Section 3, we introduce the EP-BERTGCN framework for power equipment fault recognition. Section 3.1 details the construction of the document word graph, while Section 3.2 discusses the trade-off strategy between BERT and GCN.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 9,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 228,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3548636.3548646_s2_c7",
    "source_id": "3548636.3548646",
    "text": "Our graph G = (V, E) is built using the complete dataset, with nodes V (|V| = n) and edges E. We use PMI and TF-IDF to determine word-word and word-document edges, respectively. The feature matrix X is initialized with document and word embeddings from a BERT-style model. The GCN model propagates node information iteratively, with each layer’s output given by L(i+1) = σ(˜A L(i) W_i). The loss function is cross-entropy, and the final classifier is a linear combination of the BERT and GCN outputs, balanced by the hyper-parameter λ.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 10,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 535,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3548636.3548646_s2_c8",
    "source_id": "3548636.3548646",
    "text": "In Section 4, we describe the CPTF dataset, implementation details, baseline models, and report experimental results, including ablation studies. The CPTF dataset, after oversampling to address category imbalance, contains 1484 instances across 12 categories.\n\n---\n\n---",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 11,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 269,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3548636.3548646_s2_c9",
    "source_id": "3548636.3548646",
    "text": "TextRNN integrates RNN into the multi-learning framework, capturing semantic vector representations with task-specific and shared layers. TextRNN_Att employs neural attention with BiLSTM to capture the most important semantic information without using lexical resources or NLP systems. TextRCNN applies a recurrent structure to minimize noise in word representations. DPCNN enhances text region embedding with unsupervised embeddings for improved accuracy. FastText is a simple method for text classification, averaging word features for sentence representations, and is faster than deep learning-ba",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 12,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3548636.3548646_s2_c10",
    "source_id": "3548636.3548646",
    "text": "word features for sentence representations, and is faster than deep learning-based methods. Transformer was initially proposed for sequence-to-sequence problems and has been effective in sentence-level tasks like classification.\nTable 1 presents Experimental Results on the CPTF Dataset:",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 13,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 287,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3548636.3548646_s2_c11",
    "source_id": "3548636.3548646",
    "text": "| Model       | Acc     | Macro-F1 | Weighted Macro-F1 |\n|-------------|---------|----------|-------------------|\n| TextRNN     | 0.4950  | 0.4217   | 0.4642            |\n| TextRNN_Att | 0.5538  | 0.5240   | 0.5412            |\n| TextRCNN    | 0.6126  | 0.5800   | 0.5932            |\n| DPCNN       | 0.5753  | 0.5515   | 0.5659            |\n| FastText    | 0.6098  | 0.5870   | 0.5978            |\n| Transformer  | 0.4570  | 0.4245   | 0.4594            |\n| C-TextGCN   | 0.6607  | 0.6324   | 0.6499            |\n| EP-BERTGCN  | 0.6700  | 0.6645   | 0.6633            |",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 14,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 570,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3548636.3548646_s2_c12",
    "source_id": "3548636.3548646",
    "text": "| EP-BERTGCN  | 0.6700  | 0.6645   | 0.6633            |\n| EP-Adaptation| 0.6772  | 0.6668   | 0.6713            |",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 15,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 114,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3548636.3548646_s2_c13",
    "source_id": "3548636.3548646",
    "text": "C-TextGCN chooses Chinese character as the basic embedding unit, which is found better than word embedding for text classification using GCN.\n\nAll experiments were conducted on the NVIDIA GeForce RTX 1080ti GPU using PyTorch 1.5. The layer of GCN was set to 2, and the feature embedding dimension to 300. Hyper-parameter settings are discussed in Section 4.5.\n\nDomain adaptation was applied to the BERT model, which improved model metrics as shown in Table 1.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 16,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 459,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3548636.3548646_s2_c14",
    "source_id": "3548636.3548646",
    "text": "An ablation study evaluated the hyper-parameters λ and window size L. The best parameter combinations were λ = 0.4 and L = 5. For λ, the Weighted Macro-F1 Score increased with λ but decreased when λ exceeded 0.4. For the window size L, classification accuracy showed a similar trend, indicating the importance of balance between word co-occurrence information and graph edge relevance.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 17,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 385,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3548636.3548646_s2_c15",
    "source_id": "3548636.3548646",
    "text": "The EP-BERTGCN model combines the prior knowledge of pre-trained language models and the robustness of GCN for effective text classification in the power field. Domain adaptation of BERT further improved model performance, with the comparison to baseline models demonstrating the effectiveness of our method.\n\n---\n\nACKNOWLEDGMENTS\n\nSupport for this work was provided by the Science and Technology Program of the Headquarters of State Grid Corporation of China. The authors thank anonymous reviewers for their valuable comments.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 18,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 527,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3548636.3548646_s3_c0",
    "source_id": "3548636.3548646",
    "text": "[References listed in the original content have been omitted for brevity.]\n\n---\n\nLow-Resource Domain Adaptation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 3336–3349. [4] Bushra Jalil, Giuseppe Riccardo Leone, Massimo Martinelli, Davide Moroni, Maria Antonietta Pascali, and Andrea Berton. 2019. Fault Detection in Power Equipment via an Unmanned Aerial System Using Multi Modal Data. Sensors 19, 13 (2019). https://doi.org/10.3390/s19133014",
    "section_title": "REFERENCES",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 19,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 12,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 589,
    "chunk_method": "hierarchical",
    "importance_weight": 0.44000000000000006,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3548636.3548646_s3_c1",
    "source_id": "3548636.3548646",
    "text": "[5] Rie Johnson and Tong Zhang. 2017. Deep pyramid convolutional neural networks for text categorization. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 562–570. [6] Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2016. Bag of tricks for efficient text classification. arXiv preprint arXiv:1607.01759 (2016). [7] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907 (2016).",
    "section_title": "REFERENCES",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 20,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 543,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3548636.3548646_s3_c2",
    "source_id": "3548636.3548646",
    "text": "[8] Kamran Kowsari, Kiana Jafari Meimandi, Mojtaba Heidarysafa, Sanjana Mendu, Laura Barnes, and Donald Brown. 2019. Text Classification Algorithms: A Survey. Information 10, 4 (2019). https://doi.org/10.3390/info10040150 [9] Siwei Lai, Liheng Xu, Kang Liu, and Jun Zhao. 2015. Recurrent convolutional neu- ral networks for text classification. In Twenty-ninth AAAI conference on artificial intelligence.",
    "section_title": "REFERENCES",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 21,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 404,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3548636.3548646_s3_c3",
    "source_id": "3548636.3548646",
    "text": "[10] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. ALBERT: A Lite BERT for Self-supervised Learning of Language Representations. In International Conference on Learning Representations. https://openreview.net/forum?id=H1eA7AEtvS [11] Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2016. Recurrent neural network for text classification with multi-task learning. arXiv preprint arXiv:1605.05101 (2016).",
    "section_title": "REFERENCES",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 22,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 453,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3548636.3548646_s3_c4",
    "source_id": "3548636.3548646",
    "text": "[12] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. [13] Xiaofei Ma, Peng Xu, Zhiguo Wang, Ramesh Nallapati, and Bing Xiang. 2019. Domain Adaptation with BERT-based Domain Classification and Data Selection. In Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019). Association for Computational Linguistics, Hong Kong, China, 76–83. https://doi.org/10.18653/v1/D19-6109",
    "section_title": "REFERENCES",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 23,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 562,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3548636.3548646_s3_c5",
    "source_id": "3548636.3548646",
    "text": "[14] Daniel Martinez, Humberto Henao, and Gerard-Andre Capolino. 2019. Overview of Condition Monitoring Systems for Power Distribution Grids. In 2019 IEEE 12th International Symposium on Diagnostics for Electrical Machines, Power Electronics and Drives (SDEMPED). 160–166. https://doi.org/10.1109/DEMPED.2019.8864872",
    "section_title": "REFERENCES",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 24,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 316,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3548636.3548646_s3_c6",
    "source_id": "3548636.3548646",
    "text": "[15] XiPeng Qiu, TianXiang Sun, YiGe Xu, YunFan Shao, Ning Dai, and XuanJing Huang. 2020. Pre-trained models for natural language processing: A survey. Science in China E: Technological Sciences 63, 10 (Oct. 2020), 1872–1897. https: //doi.org/10.1007/s11431-020-1647-3 arXiv:2003.08271 [cs.CL] [16] Juan Ramos. 2003. Using TF-IDF to determine word relevance in document queries. (01 2003).",
    "section_title": "REFERENCES",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 25,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 389,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3548636.3548646_s3_c7",
    "source_id": "3548636.3548646",
    "text": "[17] Alexander Rietzler, Sebastian Stabinger, Paul Opitz, and Stefan Engl. 2019. Adapt or Get Left Behind: Domain Adaptation through BERT Language Model Finetun- ing for Aspect-Target Sentiment Classification. arXiv preprint arXiv:1908.11860 (2019). [18] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017).",
    "section_title": "REFERENCES",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 26,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 473,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3548636.3548646_s3_c8",
    "source_id": "3548636.3548646",
    "text": "[19] Ziyue Xi, Xiaona Chen, Tanvir Almad, and Yinglong Ma. 2019. A Novel Ensemble Approach to Multi-label Classification for Electric Power Fault Diagnosis. In 2019 IEEE 7th International Conference on Computer Science and Network Technology (ICCSNT). 267–271. https://doi.org/10.1109/ICCSNT47585.2019.8962410",
    "section_title": "REFERENCES",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 27,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 309,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3548636.3548646_s3_c9",
    "source_id": "3548636.3548646",
    "text": "[20] Liang Yao, Chengsheng Mao, and Yuan Luo. 2019. Graph Convolutional Networks for Text Classification. In Proceedings of the Thirty-Third AAAI Conference on Ar- tificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence (Honolulu, Hawaii, USA) (AAAI’19/IAAI’19/EAAI’19). AAAI Press, Article 905, 8 pages. https://doi.org/10.1609/aaai.v33i01.33017370",
    "section_title": "REFERENCES",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 28,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 470,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3548636.3548646_s3_c10",
    "source_id": "3548636.3548646",
    "text": "[21] Yusong Zhang, Mingcong Lu, Liqing Liu, Zhijian Li, Fei Jiao, Yongping Xiong, Qinghua Tang, and Ruifan Li. 2021. Chinese Electric Power Equipment Fault Recognition Based on Graph Convolutional Networks. In 2021 International Conference on High Performance Big Data and Intelligent Systems (HPBD&IS). IEEE, 125–129.",
    "section_title": "REFERENCES",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 29,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 318,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3548636.3548646_s3_c11",
    "source_id": "3548636.3548646",
    "text": "[22] Peng Zhou, Wei Shi, Jun Tian, Zhenyu Qi, Bingchen Li, Hongwei Hao, and Bo Xu. 2016. Attention-based bidirectional long short-term memory networks for relation classification. In Proceedings of the 54th annual meeting of the association for computational linguistics (volume 2: Short papers). 207–212.",
    "section_title": "REFERENCES",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 30,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 305,
    "chunk_method": "hierarchical",
    "importance_weight": 0.42000000000000004,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s0_c0",
    "source_id": "3664647.3680897",
    "text": "Triple Alignment Strategies for Zero-shot Phrase Grounding under Weak Supervision\n\nPengyue Lin, Ruifan Li, Yuzhe Ji, Zhihan Yu, Fangxiang Feng, Zhanyu Ma, Xiaojie Wang",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 4,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 167,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s1_c0",
    "source_id": "3664647.3680897",
    "text": "Phrase Grounding (PG) aims to locate objects referred by noun phrases. We propose a framework for zero-shot PG under weak supervision, based on triple alignment strategies. These include region-text alignment (RTA) to associate region-level attributes via CLIP, domain alignment (DomA) to minimize distribution differences between training and pre-training, and category alignment (CatA) considering",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 4,
    "chunk_index": 1,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 2,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 399,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9900000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s1_c1",
    "source_id": "3664647.3680897",
    "text": "aining, and category alignment (CatA) considering category semantics and region-category relations. Our PG framework outperforms previous zero-shot and weakly-supervised methods.\nCCS Concepts\n• Information systems →Multimedia and multimodal retrieval.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 4,
    "chunk_index": 2,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 2,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 251,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9450000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s2_c0",
    "source_id": "3664647.3680897",
    "text": "PG is crucial for various downstream tasks. PG under weak supervision reduces annotation costs but struggles with limited seen categories during training. Zero-shot PG addresses this by leveraging semantic information across categories. We introduce a framework that addresses challenges in both weak supervision and zero-shot settings, focusing on attribute association, knowledge transfer, and category similarity and difference measurement.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 3,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 21,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 443,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s2_c1",
    "source_id": "3664647.3680897",
    "text": "Phrases can be utilized to categorize referred objects, with correct visual-textual relations aiding in distinguishing categories. Our approach employs a novel PG framework using triple alignment strategies under weak supervision: 1) Region-Text Alignment (RTA) to associate region-level attributes based on Contrastive Language-Image Pre-Training (CLIP). 2) Domain Alignment (DomA) transfers knowledge from seen classes by aligning grounding-related features in a domain-invariant space. 3) Category Alignment (CatA) distinguishes grounding-region categories, inspired by class activation methods.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 4,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s2_c2",
    "source_id": "3664647.3680897",
    "text": "Related works in weakly-supervised PG include detector-based methods and auxiliary task designs. Zero-shot PG methods, on the other hand, aim to predict bounding boxes for unseen categories using limited seen category data. They struggle with unseen categories due to limited training data.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 5,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 290,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s2_c3",
    "source_id": "3664647.3680897",
    "text": "Our methodology involves a CLIP-based module, pre-trained on a large dataset, which extracts region-level image semantics and fuses them with text embeddings to generate heatmaps. The grounding module consists of bi-modal encoders and a grounding decoder, utilizing the RTA, DomA, and CatA strategies for improved grounding performance in zero-shot settings under weak supervision.\n\nThe grounding module consists of an image encoder E𝑖𝑚𝑔(·), a text encoder E𝑡𝑥𝑡(·), and a grounding decoder D𝑔𝑛𝑑(·), returning a heatmap 𝐻 as follows:\n\n𝐻 = D𝑔𝑛𝑑(E𝑖𝑚𝑔(𝐼), E𝑡𝑥𝑡(𝑇))",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 6,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 560,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s2_c4",
    "source_id": "3664647.3680897",
    "text": "𝐻 = D𝑔𝑛𝑑(E𝑖𝑚𝑔(𝐼), E𝑡𝑥𝑡(𝑇))\n\nThe multimodal feature fusion calculates the similarity of textual and visual features, 𝐴𝑀 = E𝐼𝑚𝑔(𝐼) ⊗ E𝑇𝑥𝑡(𝑇), with attention 𝑅𝑀 = E𝑖𝑚𝑔(𝐼) ⊙ 𝐴𝑀. The decoder converts high-dimensional fusion features into grounding heatmaps 𝐻 using two up-sampling layers. We propose triple alignment strategies: Region-text Alignment (RTA), Domain Alignment (DomA), and Category Alignment (CatA).",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 7,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 408,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s2_c5",
    "source_id": "3664647.3680897",
    "text": "RTA strategy employs a fixed CLIP to generate region-level attribute associations. We process mask tokens in the CLIP image encoder to transfer semantic information to patch tokens, enhancing semantic transfer in the last layer. The CLIP-based heatmap 𝐴𝐶 is computed by the inner product of the text embedding and the final embedding vectors. Heatmap refinement involves patch-to-patch attention and gradient map computation.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 8,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 425,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s2_c6",
    "source_id": "3664647.3680897",
    "text": "DomA strategy aims to minimize the difference in feature distributions between training and pretraining phases. We use contrastive learning and alignment loss to select positive and negative samples based on the CLIP-based heatmap 𝐴∗ 𝐶.\n\nCatA strategy is not detailed in the provided content but is mentioned as part of the triple alignment strategies.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 9,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 352,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s2_c7",
    "source_id": "3664647.3680897",
    "text": "我们将短语接地（PG）视为短语-区域对齐问题。受类激活方法启发，我们使用短语嵌入E𝑡𝑥𝑡(𝑇)来区分接地相关的特征类别，并将其标准化为类别标签𝑦。为了构建区域-类别关系，我们基于短语嵌入和接地区域嵌入计算CLIP匹配分数。为确保每个训练类别能被预训练的CLIP看到，我们沿着边界框裁剪出接地区域。我们通过CLIP利用与短语相关的对象信息，将区域𝐵(𝐻)接地，然后将其重塑为𝐻，并通过CLIP映射到图像表示，即𝑣𝑂= E𝑖𝑚𝑔(𝐵(𝐻))。对象表示𝑣𝑂和短语表示E𝑡𝑥𝑡(𝑇)之间的余弦相似性用于损失计算：\n\nℓ𝑂𝑃= − ∑︁𝑛 𝑦𝑛log𝑠𝑛，\n\n其中𝑠𝑛是余弦相似性。因此，在ℓ𝑂𝑃的监督下，热图𝐻逐渐接近与短语相关的对象。\n\n为了增加与短语无关区域与短语表示之间的距离，我们通过裁剪随机生成的𝐻大小的框𝐵𝑅来接地非对象区域。生成的框应与边界框𝐵(𝐻)的交叠尽量少。该框由CLIP图像编码器表示，即𝑣𝑅= E𝑖𝑚𝑔(𝐵𝑅)。框视觉表示𝑣𝑅和短语表示E𝑡𝑥𝑡(𝑇)之间的余弦相似性用于损失计算：\n\nℓ𝑅𝑃= − ∑︁𝑛 𝑦𝑛log(1 −𝑠∗ 𝑛)，\n\n其中𝑠∗𝑛表示余弦相似性。因此，热图𝐻保留了较少的短语无关区域的对象部分。\n\n为了进一步排除热图中的不相关背景，我们通过以下损失函数限制热图区域大小：\n\nℓ𝑅𝐸= 1/𝑛 ∑𝐻𝑛，\n\n我们的模型的总损失为：",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 10,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 581,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s2_c8",
    "source_id": "3664647.3680897",
    "text": "为了进一步排除热图中的不相关背景，我们通过以下损失函数限制热图区域大小：\n\nℓ𝑅𝐸= 1/𝑛 ∑𝐻𝑛，\n\n我们的模型的总损失为：\n\nℓ𝑇= ℓ1 + 𝜆1ℓ𝑂𝑃+ 𝜆2ℓ𝑅𝑃+ 𝜆3ℓ𝑅𝐸+ 𝜆4ℓ𝑐𝑜𝑛。\n\n基于接地模块，我们按以下方式生成边界框：首先，我们将低值像素设置为0，阈值为0.5。然后搜索轮廓并提取合适的边界框。我们根据热图𝐻的面积百分比计算边界框的分数。最后，应用非极大值抑制，𝐼𝑜𝑈= 0.3，过滤掉得分低于最大得分50%的框，完成定位。\n\n4.1 数据集\n\n我们在零样本PG设置下使用Flickr-Split-S0、Flickr-Split-S1、VG-Split-S2和VG-Split-S3评估我们的框架。此外，为了与先前的弱监督接地方法进行比较，我们使用了在MG中采用的设置，该设置分别使用MS-COCO或VG训练划分的多种工作。在这两种情况下，模型在Flickr30K、VG和ReferIt的测试划分上进行评估。\n\n4.2 基线与评价指标",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 11,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 433,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s2_c9",
    "source_id": "3664647.3680897",
    "text": "4.2 基线与评价指标\n\n我们将我们的框架与各种SoTA基线进行了比较，这些基线可以分为以下三类：1) 监督零样本基线，即ZSGNet。2) 零样本基线，包括检测器+CLIP、GAE、Grad-CAM、AdaptingCLIP和MaskCLIP。3) 弱监督基线，包括MG、GbS、WWbl、SMST、BBR和VPT。我们使用了“指点游戏”准确性、边界框准确性和识别准确性三种评价指标。\n\n4.3 实现细节\n\n在我们的框架中，使用了VGG-16和CLIP RN50作为视觉编码器。模型接受224×224的图像大小，这是CLIP视觉分支VIT-B/32的输入大小，并生成相同大小的热图𝐻。我们使用SGD优化器（批量大小为64，初始学习率为0.0003）训练150个周期，其中优化器动量为0.9，权重衰减为0.0001。此外，层𝐿设置为11。当𝑙<11时，参数𝛼和𝛽设置为0.01和1；当𝑙=𝐿时，它们设置为1和10。所有方法都在NVIDIA RTX A6000上实现。在我们所有的实验中，根据方程式(11)中的损失权重设置为𝜆1 = 0.25，𝜆2 = 0.125，𝜆3 = 0.25，和𝜆4 = 1。\n\n4.4 主要结果\n---",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 12,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 514,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s2_c10",
    "source_id": "3664647.3680897",
    "text": "We present zero-shot evaluation results on unseen phrase classes using Flickr-Split and VG-Split test splits, with IoU thresholds set at 0.3 and 0.5 due to VG-Split’s textual noise. Our approach outperforms other methods at IoU threshold 0.5, indicating better coverage of unseen categories. While there is a gap compared to supervised methods, our approach significantly improves upon weakly supervised methods. The WWbl model's heatmap masking and external knowledge usage for similarity measurement introduce an incorrect accumulation of category judgment. Our results are close to supervised gro",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 13,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s2_c11",
    "source_id": "3664647.3680897",
    "text": "rrect accumulation of category judgment. Our results are close to supervised grounding SoTA on Flickr-Split-1, showcasing strong generalization on unseen phrase classes.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 14,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 169,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s2_c12",
    "source_id": "3664647.3680897",
    "text": "In weakly supervised evaluation on seen classes, our PG framework demonstrates competitive performance against other methods on Flickr30K, VG, and ReferIt datasets. Our CLIP-based heatmap surpasses the pseudo label used by WWbl, explaining a 9% increase in bbox accuracy.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 15,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 271,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s2_c13",
    "source_id": "3664647.3680897",
    "text": "Our ablation study shows the effects of alignment strategies on Flickr30K Entities, with the best performance achieved using all alignment strategies. Different loss functions were analyzed, with the addition of ℓ𝑐𝑜𝑛, ℓ𝑂𝑃, ℓ𝑅𝑃, and ℓ𝑅𝐸 improving IoU performance. We also compared our domain alignment strategy with a baseline using CLIP (RN50) as the image encoder, with our strategy achieving better domain adaptation between seen and unseen classes.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 16,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 451,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s2_c14",
    "source_id": "3664647.3680897",
    "text": "Qualitative analysis illustrates the effectiveness of our alignment strategies, with our method successfully grounding referred objects and capturing less phrase-related background regions compared to VLP-based and weakly-supervised PG methods.\n\n---\n\nTable 5: Bounding box accuracy across unseen splits. For Flickr-Split-0&1, IoU threshold of 0.5 is used. For VG-Split-2&3, accuracy is reported with IoU thresholds of 0.3 and 0.5. \"B\" and \"UB\" indicate balanced and unbalanced sets in VG-Split.\n\nPhrase: clarinetto\nRTA + CatA\n\nPhrase: purple shirt\nPhrase: a beautiful bride\nPhrase: a handsome groom",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 17,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s2_c15",
    "source_id": "3664647.3680897",
    "text": "Phrase: purple shirt\nPhrase: a beautiful bride\nPhrase: a handsome groom\n\nGT & Ours Image + DomA\nGT & Ours RTA + CatA + DomA\n\nFigure 7: Qualitative results are presented. Input Images are in the leftmost column. Columns #2-4 show generated heatmaps using RTA, RTA+DomA, and triple alignments, respectively. Columns #6-8 present results for another phrase. White boxes represent ground truth, and red indicates our results.\n\nMethod Overall People Animals Vehicles Scene Other",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 18,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 473,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s2_c16",
    "source_id": "3664647.3680897",
    "text": "Method Overall People Animals Vehicles Scene Other\n\nMaskCLIP 34.26 37.46 40.93 52.25 48.40 25.87\nAdaptingCLIP 29.47 29.23 40.15 45.00 41.86 24.92\nGAE 25.56 26.76 39.72 38.12 33.72 22.22\nCH 43.75 56.33 62.31 58.60 52.78 32.26\nOurs w/ GAE 36.35 43.58 48.22 52.72 55.94 26.44\nOurs w/ CH 45.46 56.44 59.95 57.68 70.04 32.53\n\nTable 6: Category-wise bounding box accuracy on Flickr30K.\n\nAdaptingCLIP GAE MaskCLIP Ours\n\nCaption: A man on a black mountain bike\nCaption: A guy wearing a white shirt\nCaption: A woman pointing to the subway station\nCaption: A man in a plaid shirt and blue jeans",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 19,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 584,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s2_c17",
    "source_id": "3664647.3680897",
    "text": "WWbl Ground Truth Image\n\nFigure 8: Qualitative results comparison.\n\nOur framework grounds more complete and compact regions, avoiding false grounding between the region of blue jeans and the woman or the mountain bike and the subway station.\n\nCaption: A new crew and a reporter in a blue coat make a film in the rain.\nGround Truth\nCaption: The kid wears glasses and two kids are smiling.\n\nFigure 9: Failure cases of our proposed method.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 20,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 436,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s2_c18",
    "source_id": "3664647.3680897",
    "text": "Figure 9: Failure cases of our proposed method.\n\nFailure cases are categorized into similar dense objects and in-context entities-related objects. Our framework highlights connected regions for dense objects, with imprecise determination of the number of bounding boxes. Additionally, it extracts only noun phrases without considering in-context phrases, leading to inaccurate location evaluation of referred objects.\n\n5 Conclusion and Future Work",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 21,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 447,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s2_c19",
    "source_id": "3664647.3680897",
    "text": "5 Conclusion and Future Work\n\nWe propose a PG framework that designs alignment strategies to address three problems of zero-shot grounding under weak supervision. Our approach exceeds previous methods and will explore interpretable solutions for grounding-related tasks. Future work includes using multi-modal large language models to enhance zero-shot PG under weak supervision.\n\nAcknowledgment",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 22,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 395,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s2_c20",
    "source_id": "3664647.3680897",
    "text": "Acknowledgment\n\nThis work was supported by the Beijing Natural Science Foundation Project No. Z200002, the National Nature Science Foundation of China (Grants 62076032, 62225601, U23B2052), the Youth Innovative Research Team of BUPT No. 2023YQTD02, BUPT Excellent Ph.D. Students Foundation CX2023113, and High Performance Computing Platform of BUPT.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 23,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 349,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s3_c0",
    "source_id": "3664647.3680897",
    "text": "---\n\n---\n\n[1] Hassan Akbari et al. 2019. Multi-level multimodal common semantic space for image-phrase grounding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 12476–12486.\n\n[2] Assaf Arbelle et al. 2021. Detector-free weakly supervised grounding by separation. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 1801–1812.\n\n[3] Kai Uwe Barthel et al. 2019. Real-time visual navigation in huge image sets using similarity graphs. In Proceedings of the 27th ACM International Conference on Multimedia. 2202–2204.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 24,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 25,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 576,
    "chunk_method": "hierarchical",
    "importance_weight": 0.44000000000000006,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s3_c1",
    "source_id": "3664647.3680897",
    "text": "[4] Hila Chefer et al. 2021. Transformer interpretability beyond attention visualization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 782–791.\n\n[5] Kan Chen et al. 2017. Query-guided regression network with context policy for phrase grounding. In Proceedings of the IEEE International Conference on Computer Vision. 824–832.\n\n[6] Kang Chen et al. 2023. VTQA2023: ACM Multimedia 2023 Visual Text Question Answering Challenge. In Proceedings of the 31st ACM International Conference on Multimedia. 9646–9650.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 25,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 548,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s3_c2",
    "source_id": "3664647.3680897",
    "text": "[7] Nenglun Chen et al. 2021. Distributed attention for grounded image captioning. In Proceedings of the 29th ACM International Conference on Multimedia. 1966–1975.\n\n[8] Samyak Datta et al. 2019. Align2ground: Weakly supervised phrase grounding guided by image-caption alignment. In Proceedings of the IEEE/CVF international conference on computer vision. 2601–2610.\n\n[9] Thomas Eiter et al. 2023. A Logic-based Approach to Contrastive Explainability for Neurosymbolic Visual Question Answering. In IJCAI. 3668–3676.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 26,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 516,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s3_c3",
    "source_id": "3664647.3680897",
    "text": "[10] Hao Fang et al. 2015. From captions to visual concepts and back. In Proceedings of the IEEE conference on computer vision and pattern recognition. 1473–1482.\n\n[11] Eyal Gomel et al. 2023. Box-based Refinement for Weakly Supervised and Unsupervised Localization Tasks. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 16044–16054.\n\n[12] Michael Grubinger et al. 2006. The iapr tc-12 benchmark: A new evaluation resource for visual information systems. In International workshop ontoImage. 13–23.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 27,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 526,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s3_c4",
    "source_id": "3664647.3680897",
    "text": "[13] Tanmay Gupta et al. 2020. Contrastive learning for weakly supervised phrase grounding. In European Conference on Computer Vision. 752–768.\n\n[14] Kaiming He et al. 2020. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 9729–9738.\n\n[15] Yicong Hong et al. 2023. Learning navigational visual representations with semantic map supervision. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 3055–3067.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 28,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 531,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s3_c5",
    "source_id": "3664647.3680897",
    "text": "[16] Syed Ashar Javed et al. 2018. Learning unsupervised visual grounding through semantic self-supervision. CoRR abs/1803.06506.\n\n[17] Haojun Jiang et al. 2022. Pseudo-q: Generating pseudo language queries for visual grounding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 15513–15523.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 29,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 327,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s3_c6",
    "source_id": "3664647.3680897",
    "text": "[18] Huiju Kim et al. 2023. Examining Consistency of Visual Commonsense Reasoning based on Person Grounding. In Proceedings of the 13th International Joint Conference on Natural Language Processing.\n\n---\n\nKrishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L-J., Shamma, D.A., et al. (2017). Visual genome: Connecting language and vision using crowdsourced dense image annotations. International Journal of Computer Vision, 123, 32–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 30,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 479,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s3_c7",
    "source_id": "3664647.3680897",
    "text": "73.\n\nLi, J., Selvaraju, R., Gotmare, A., Joty, S., Xiong, C., & Hoi, S.C.H. (2021). Align before fuse: Vision and language representation learning with momentum distillation. Advances in Neural Information Processing Systems, 34, 9694–9705.\n\nLi, J., Shakhnarovich, G., & Yeh, R.A. (2022). Adapting CLIP for phrase localization without further training. CoRR, arXiv:2204.03647.\n\nLi, M., Wang, Z., Tuytelaars, T., & Moens, M.-F. (2023). Layout-aware Dreamer for embodied visual referring expression grounding. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 37, 1386–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 31,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 588,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s3_c8",
    "source_id": "3664647.3680897",
    "text": "1395.\n\nLi, W., Song, X., Bai, Y., Zhang, S., & Jiang, S. (2021). Ion: Instance-level object navigation. In Proceedings of the 29th ACM International Conference on Multimedia, 4343–4352.\n\nLin, P., Yu, Z., Lu, M., Feng, F., Li, R., & Wang, X. (2024). Visual Prompt Tuning for Weakly Supervised Phrase Grounding. In ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 7895–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 32,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 420,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s3_c9",
    "source_id": "3664647.3680897",
    "text": "7899.\n\nLin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., & Zitnick, C.L. (2014). Microsoft COCO: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, 740–755.\n\nLin, Y., Chen, M., Wang, W., Wu, B., Li, K., Lin, B., Liu, H., & He, X. (2023). Clip is also an efficient segmenter: A text-driven approach for weakly supervised semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 15305–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 33,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 501,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s3_c10",
    "source_id": "3664647.3680897",
    "text": "15314.\n\nLiu, X., Li, L., Wang, S., Zha, Z.-J., Li, Z., Tian, Q., & Huang, Q. (2022). Entity-enhanced adaptive reconstruction network for weakly supervised referring expression grounding. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(3), 3003–3018.\n\nLiu, X., Li, L., Wang, S., Zha, Z.-J., Meng, D., & Huang, Q. (2019). Adaptive reconstruction network for weakly supervised referring expression grounding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2611–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 34,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 506,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s3_c11",
    "source_id": "3664647.3680897",
    "text": "2620.\n\nLiu, X., Li, L., Wang, S., Zha, Z.-J., Su, L., & Huang, Q. (2019). Knowledge-guided pairwise reconstruction network for weakly supervised referring expression grounding. In Proceedings of the 27th ACM International Conference on Multimedia, 539–547.\n\nLiu, Y., Shi, Y., Feng, F., Li, R., Ma, Z., & Wang, X. (2022). Improving Image Paragraph Captioning with Dual Relations. In 2022 IEEE International Conference on Multimedia and Expo (ICME), 1–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 35,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 450,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s3_c12",
    "source_id": "3664647.3680897",
    "text": "6.\n\nLiu, Y., Wan, B., Ma, L., & He, X. (2021). Relation-aware instance refinement for weakly supervised visual grounding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 5612–5621.\n\nLiu, Y., Zhang, J., Chen, Q., & Peng, Y. (2023). Confidence-aware Pseudo-label Learning for Weakly Supervised Visual Grounding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2828–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 36,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 429,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s3_c13",
    "source_id": "3664647.3680897",
    "text": "2838.\n\nLu, J., Goswami, V., Rohrbach, M., Parikh, D., & Lee, S. (2020). 12-in-1: Multi-task vision and language representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 10437–10446.\n\nLu, M., Li, R., Feng, F., Ma, Z., & Wang, X. (2024). LGR-NET: Language Guided Reasoning Network for Referring Expression Comprehension. IEEE Transactions on Circuits and Systems for Video Technology, 1–1. https://doi.org/10.1109/TCSVT.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 37,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 472,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s3_c14",
    "source_id": "3664647.3680897",
    "text": "1. https://doi.org/10.1109/TCSVT.2024.3374786\n\nPlummer, B.A., Wang, L., Cervantes, C.M., Caicedo, J.C., Hockenmaier, J., & Lazebnik, S. (2015). Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In Proceedings of the IEEE international conference on computer vision, 2641–2649.\n\nQin, L., Chen, Q., Zhou, Y., Chen, Z., Li, Y., Liao, L., Li, M., Che, W., & Yu, P.S. (2024). Multilingual Large Language Model: A Survey of Resources, Taxonomy and Frontiers. arXiv:",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 38,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 510,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s3_c15",
    "source_id": "3664647.3680897",
    "text": "2404.04925 [cs.CL].\n\nRadford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. (2021). Learning transferable visual models from natural language supervision. In International conference on machine learning, 8748–8763.\n\nRaghu, M., Unterthiner, T., Kornblith, S., Zhang, C., & Dosovitskiy, A. (2021). Do vision transformers see like convolutional neural networks? Advances in Neural Information Processing Systems, 34, 12116–12128.\n\n---\n\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 39,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 558,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s3_c16",
    "source_id": "3664647.3680897",
    "text": "12128.\n\n---\n\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. Advances in Neural Information Processing Systems, Vol. 28.\n\nArka Sadhu, Kan Chen, and Ram Nevatia. 2019. Zero-shot grounding of objects from natural language queries. Proceedings of the IEEE/CVF International Conference on Computer Vision, 4694–4703.\n\nTal Shaharabany, Yoad Tewel, and Lior Wolf.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 40,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 450,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s3_c17",
    "source_id": "3664647.3680897",
    "text": "4703.\n\nTal Shaharabany, Yoad Tewel, and Lior Wolf. 2022. What is where by looking: Weakly-supervised open-world phrase-grounding without text inputs. Advances in Neural Information Processing Systems 35, 28222–28237.\n\nTal Shaharabany and Lior Wolf. 2023. Similarity Maps for Self-Training Weakly-Supervised Phrase Grounding. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 6925–6934.\n\nHaozhan Shen, Tiancheng Zhao, Mingwei Zhu, and Jianwei Yin.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 41,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 479,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s3_c18",
    "source_id": "3664647.3680897",
    "text": "6934.\n\nHaozhan Shen, Tiancheng Zhao, Mingwei Zhu, and Jianwei Yin. 2024. Ground-VLP: Harnessing Zero-Shot Visual Grounding from Vision-Language Pre-training and Open-Vocabulary Object Detection. AAAI Conference on Artificial Intelligence, Vol. 38, 4766–4775.\n\nYibing Song, Ruifei Zhang, Zhihong Chen, Xiang Wan, and Guanbin Li. 2023. Advancing visual grounding with scene knowledge: Benchmark and method. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 15039–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 42,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 494,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s3_c19",
    "source_id": "3664647.3680897",
    "text": "15049.\n\nSanjay Subramanian, William Merrill, Trevor Darrell, Matt Gardner, Sameer Singh, and Anna Rohrbach. 2022. ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension. 60th Annual Meeting of the Association for Computational Linguistics, 5198–5215.\n\nSatoshi Suzuki et al. 1985. Topological structural analysis of digitized binary images by border following. Computer vision, graphics, and image processing 30, 1, 32–46.\n\nFeng Wang, Jieru Mei, and Alan Yuille. 2024. SCLIP: Rethinking Self-Attention for Dense Vision-Language Inference. arXiv:",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 43,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 564,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s3_c20",
    "source_id": "3664647.3680897",
    "text": "2312.01597 [cs.CV]\n\nMingzhe Wang, Mahmoud Azab, Noriyuki Kojima, Rada Mihalcea, and Jia Deng. 2016. Structured matching for phrase localization. Computer Vision–ECCV 2016, 696–711.\n\nQinxin Wang, Hao Tan, Sheng Shen, Michael Mahoney, and Zhewei Yao. 2020. MAF: Multimodal Alignment Framework for Weakly-Supervised Phrase Grounding. Conference on Empirical Methods in Natural Language Processing (EMNLP), 2030–2038.\n\nHaoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 44,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 528,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s3_c21",
    "source_id": "3664647.3680897",
    "text": "2023. Towards explainable in-the-wild video quality assessment: a database and a language-prompted approach. 31st ACM International Conference on Multimedia, 1045–1054.\n\nSiying Wu, Xueyang Fu, Feng Wu, and Zheng-Jun Zha. 2022. Cross-modal semantic alignment pre-training for vision-and-language navigation. 30th ACM International Conference on Multimedia, 4233–4241.\n\nYuechen Wu, Zhenhuan Rao, Wei Zhang, Shijian Lu, Weizhi Lu, and Zheng-Jun Zha. 2019. Exploring the Task Cooperation in Multi-goal Visual Navigation. IJCAI, 609–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 45,
    "chunk_index_in_section": 21,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 528,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s3_c22",
    "source_id": "3664647.3680897",
    "text": "615.\n\nLian Xu, Wanli Ouyang, Mohammed Bennamoun, Farid Boussaid, and Dan Xu. 2022. Multi-class token transformer for weakly supervised semantic segmentation. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 4310–4319.\n\nDizhan Xue, Shengsheng Qian, and Changsheng Xu. 2023. Variational Causal Inference Network for Explanatory Visual Question Answering. IEEE/CVF International Conference on Computer Vision, 2515–2525.\n\nZhihan Yu and Ruifan Li.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 46,
    "chunk_index_in_section": 22,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 477,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s3_c23",
    "source_id": "3664647.3680897",
    "text": "2525.\n\nZhihan Yu and Ruifan Li. 2024. Revisiting Counterfactual Problems in Referring Expression Comprehension. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 13438–13448.\n\nJianming Zhang, Sarah Adel Bargal, Zhe Lin, Jonathan Brandt, Xiaohui Shen, and Stan Sclaroff. 2018. Top-down neural attention by excitation backprop. International Journal of Computer Vision, 126, 10, 1084–1102.\n\nWenqiao Zhang, Xin Eric Wang, Siliang Tang, Haizhou Shi, Haochen Shi, Jun Xiao, Yueting Zhuang, and William Yang Wang.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 47,
    "chunk_index_in_section": 23,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 540,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3680897_s3_c24",
    "source_id": "3664647.3680897",
    "text": "2020. Relational graph learning for grounded video description generation. 28th ACM International Conference on Multimedia, 3807–3828.\n\nZicheng Zhang, Bonan Li, Xuecheng Nie, Congying Han, Tiande Guo, and Luoqi Liu. 2023. Towards Consistent Video Editing with Text-to-Image Diffusion Models. Advances in Neural Information Processing Systems, Vol. 36.\n\nChong Zhou, Chen Change Loy, and Bo Dai. 2022. Extract free dense labels from clip. European Conference on Computer Vision, Springer, 696–712.\n\n---",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 48,
    "chunk_index_in_section": 24,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 500,
    "chunk_method": "hierarchical",
    "importance_weight": 0.42000000000000004,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s0_c0",
    "source_id": "3664647.3681466",
    "text": "---\n\nDiffHarmony++: Enhancing Image Harmonization with Harmony-VAE and Inverse Harmonization Model\n\nPengfei Zhou, Beijing University of Posts & Telecommunications, China\nFangxiang Feng, Beijing University of Posts & Telecommunications, China\nGuang Liu, Beijing Academy of Artificial Intelligence, China\nRuifan Li, Beijing University of Posts & Telecommunications, China\nXiaojie Wang, Beijing University of Posts & Telecommunications, China",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 5,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 439,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s1_c0",
    "source_id": "3664647.3681466",
    "text": "Latent diffusion models have shown remarkable effectiveness in image generation and editing tasks. Their application to image harmonization, however, is challenged by severe image distortion introduced by the VAE component. We propose Harmony-VAE, which leverages the input of the harmonization task to enhance decoded image quality. The input composite image contains precise pixel-level information",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 5,
    "chunk_index": 1,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 3,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 400,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9900000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s1_c1",
    "source_id": "3664647.3681466",
    "text": "ite image contains precise pixel-level information that complements the correct foreground appearance and color in denoised latents. Additionally, we train an inverse harmonization diffusion model for data augmentation and create a new dataset with prominent foreground objects. Experiments demonstrate the effectiveness of our Harmony-VAE and inverse harmonization model.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 5,
    "chunk_index": 2,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 372,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s1_c2",
    "source_id": "3664647.3681466",
    "text": "CCS Concepts\n\n• Computing methodologies → Computer vision tasks\n\nKeywords\n\nimage harmonization, latent diffusion model, VAE, data augmentation, inverse harmonization, stable diffusion",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 5,
    "chunk_index": 3,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 183,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9450000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s2_c0",
    "source_id": "3664647.3681466",
    "text": "Image composition is pivotal in digital editing but often faces the challenge of unrealistic merged images due to inconsistencies in lighting and color. Image harmonization addresses this by modifying the foreground for visual consistency with the background. Advances in deep learning and the latent diffusion model have improved image harmonization. However, issues with image distortion in the VAE decoding process have led to suboptimal performance. We introduce DiffHarmony++, which overcomes these limitations and achieves state-of-the-art performance.\n\n---\n\n---",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 4,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 27,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 568,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s2_c1",
    "source_id": "3664647.3681466",
    "text": "To address the aforementioned limitations, we introduce Harmony-VAE, which integrates composite images into the VAE decoding process. This enhances the realism of reconstructed images by utilizing the precise object shape information from composites, complementing the denoised latent's foreground appearance and color details. Harmony-VAE's training is independent of denoising UNet fine-tuning, reducing costs and improving LDM-based harmonization models. It also generalizes well to higher-resolution images, despite being trained on 256px images. We term our DiffHarmony model equipped with Harm",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 5,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 27,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s2_c2",
    "source_id": "3664647.3681466",
    "text": "being trained on 256px images. We term our DiffHarmony model equipped with Harmony-VAE as DiffHarmony++.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 6,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 27,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 104,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s2_c3",
    "source_id": "3664647.3681466",
    "text": "The Harmony-VAE-boosted latent diffusion model can be modified to perform inverse image harmonization for data augmentation. Inverse harmonization involves generating synthetic composite images from real images and foreground masks, allowing for a one-to-many image translation task suitable for diffusion models. Based on DiffHarmony++, we train an inverse harmonization model on iHarmony4. This model expands harmonization data significantly. Using the inverse harmonization model, we augment two smaller iHarmony4 subsets, Hday2night and HFlickr, improving performance compared to original data t\nbsets, Hday2night and HFlickr, improving performance compared to original data training.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 7,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 27,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 688,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s2_c4",
    "source_id": "3664647.3681466",
    "text": "Our proposed model also automates the creation of new harmonization datasets, a long-standing challenge. It outperforms manual efforts, such as the creation of RealHM, and automates the generation of composite images, reducing the need for manual screening. We generate a Human Harmony dataset using the imaterialist dataset and a harmony classifier to filter high-quality data.\n\nOur contributions are:",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 8,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 27,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 402,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s2_c5",
    "source_id": "3664647.3681466",
    "text": "Our contributions are:\n\n• Proposal of Harmony-VAE to address VAE decoding distortion in image harmonization.\n• Development of an effective inverse harmonization diffusion model, validated on two datasets.\n• Introduction of the Human Harmony dataset with a harmony classifier for high-quality data filtering.\n\n### 2 Related Works\n\n#### 2.1 Image Harmonization",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 9,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 27,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 358,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s2_c6",
    "source_id": "3664647.3681466",
    "text": "### 2 Related Works\n\n#### 2.1 Image Harmonization\n\nEarly image harmonization methods focused on color matching algorithms. With deep learning, supervised methods emerged, using context, semantic information, and attention modules. Techniques like domain translation and Retinex theory decomposition have also been applied.\n\n#### 2.2 Image Harmonization Dataset\n\nRealHM and other datasets were created manually or using 3D rendering, while iHarmony4 employed automatic color transfer and filtering.\n\n#### 2.3 Diffusion Model\n\n---",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 10,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 27,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 528,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s2_c7",
    "source_id": "3664647.3681466",
    "text": "#### 2.3 Diffusion Model\n\n---\n\n[Please note that the section on diffusion models is cut off and does not provide further details. If this is an error and more content is expected, please provide additional context.]",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 11,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 27,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 215,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s2_c8",
    "source_id": "3664647.3681466",
    "text": "Diffusion models, such as Denoising Diffusion Probabilistic Models (DDPMs), have shown effectiveness in generating realistic images from random noise. Subsequent works like Palette, RePaint, and SR3+ apply diffusion models to image-to-image translation tasks. Latent diffusion models, particularly LDM, which underpins Stable Diffusion, have received significant attention. Stable Diffusion has been used in various image-to-image translation tasks and image harmonization, with works like Appearance Consistency Discriminator and Zero-Shot Image Harmonization by Chen et al. making notable contribu",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 12,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 27,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s2_c9",
    "source_id": "3664647.3681466",
    "text": "minator and Zero-Shot Image Harmonization by Chen et al. making notable contributions. However, these methods often focus on image inpainting, not the more challenging image harmonization task.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 13,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 27,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 193,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s2_c10",
    "source_id": "3664647.3681466",
    "text": "Harmony-VAE is a method designed for image harmonization, which includes repairing the content of the foreground area while maintaining a harmonized appearance. It builds upon the Stable Diffusion model, which is pretrained using a VAE and a denoising U-Net in two stages. The denoising process involves adding noise to the latent variable and updating the U-Net with a latent denoising loss.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 14,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 27,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 392,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s2_c11",
    "source_id": "3664647.3681466",
    "text": "In our method, we address the issue of image distortion caused by the VAE decoding process. We propose Harmony-VAE, which enhances the quality of the harmonized image by incorporating a conditional encoder for the composite image and foreground mask. The final latent variables and intermediate features are fused into the VAE decoder through skip connections. The training objective is to reconstruct the real image using the composite image and mask as conditions.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 15,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 27,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 466,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s2_c12",
    "source_id": "3664647.3681466",
    "text": "The mathematical formulation of the Harmony-VAE objective function involves reconstructing the real image with the encoded image and mask as conditions. During the denoising stage, pure noise is iteratively refined through the Denoising U-Net, with the encoded composite image and downsampled mask as conditions, to obtain the denoised latent variable, which is then decoded into the harmonized image.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 16,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 27,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 401,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s2_c13",
    "source_id": "3664647.3681466",
    "text": "The latent variables corresponding to the real image ˆ𝐼 and the harmonized image ˜𝐼 are close enough that using the encoded real image E(ˆ𝐼) as the model input does not result in significant performance loss. This choice greatly reduces training costs as it eliminates the need for offline or online generation of data samples from the diffusion model. The weights of the conditional encoder E𝑐 are initialized from the original VAE encoder, and zero-initialized convolution layers are added before each skip connection to maintain training stability. We train the conditional encoder E𝑐, decoder D,",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 17,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 27,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s2_c14",
    "source_id": "3664647.3681466",
    "text": "to maintain training stability. We train the conditional encoder E𝑐, decoder D, and all zero-initialized convolution layers. This trained model can be seamlessly integrated into the DiffHarmony inference process, forming DiffHarmony++.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 18,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 27,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 235,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s2_c15",
    "source_id": "3664647.3681466",
    "text": "For inverse image harmonization, we propose an inverse harmonization model based on DiffHarmony++. It takes the foreground mask image 𝑀 and the real image ˆ𝐼 as input to output the composite image 𝐼. We train this model on the iHarmony4 dataset and use it to generate additional training data for existing harmonization datasets. We blend 𝐾 candidate composite images for each (ˆ𝐼, 𝑀) data pair with a blending ratio 𝛾, expanding the training data.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 19,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 27,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 448,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s2_c16",
    "source_id": "3664647.3681466",
    "text": "We also address the lack of human portrait-specific harmonization datasets by constructing a Human Harmony dataset. Utilizing the imaterialist-fashion-2020-fgvc7 dataset, we pair high-resolution portrait photographs with detailed segmentation maps to create accurate foreground masks. We train a harmony classifier to ensure the quality of the generated harmonization data and select the final composite sample with the highest classification probability.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 20,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 27,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 455,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s2_c17",
    "source_id": "3664647.3681466",
    "text": "In our experiments, we use the iHarmony4 dataset, which includes HCOCO, HFlickr, HAdobe5K, and Hday2night sub-datasets, consisting of 65,742 training and 7,404 testing pairs of composite and real images. We evaluate each sub-dataset individually.\n\n---\n5.1.2 Human Harmony Dataset",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 21,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 27,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 279,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s2_c18",
    "source_id": "3664647.3681466",
    "text": "We create the Human Harmony dataset by filtering the imaterial-fashion-2020-fgvc7 dataset to exclude images with only products. The cleaned dataset contains 29,106 images. We assign the value 1 to all valid segmentation parts (except the background) to form foreground mask images. Using a harmony classifier, we select images with the highest classification probability, setting 𝐾= 10. The Human Harmony dataset is divided into training and testing sets with the same ratio as the iHarmony4 dataset, resulting in a training set of 26,157 and a testing set of 2,946 images.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 22,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 27,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 573,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s2_c19",
    "source_id": "3664647.3681466",
    "text": "Figure 2: Cumulative distribution curves of foreground area ratios for both iHarmony4 and Human Harmony datasets. Approximately 70% of the Human Harmony dataset images have foreground ratios above 0.2, compared to less than 15% in iHarmony4.\n\n5.1.3 Evaluation Metrics\nWe evaluate all methods using PSNR, MSE, and fMSE, consistent with previous works.\n5.2 Implementation Details",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 23,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 27,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 377,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s2_c20",
    "source_id": "3664647.3681466",
    "text": "Harmony-VAE is trained using all iHarmony4 training data. The learning rate is set to 1𝑒−4, with a warmup of 0.02, and then kept constant. Training lasts for 10 epochs with the AdamW optimizer, weight_decay=0, 𝛽1 = 0.9, and 𝛽2 = 0.999. Model weights are saved using EMA with max_decay = 0.999. Images are resized to 256px during training. For the diffusion model, we use the pre-trained DiffHarmony. We employ the Euler ancestral discrete scheduler to generate samples in 5 steps during inference. The inverse harmonization model's implementation details are mostly consistent with DiffHarmony++, ex",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 24,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 27,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s2_c21",
    "source_id": "3664647.3681466",
    "text": "tion model's implementation details are mostly consistent with DiffHarmony++, except for the use of (ˆ𝐼, 𝑀) as conditions and the composite image 𝐼 as the denoising target. The Harmony-VAE training and inference are modified accordingly. For the harmony classifier, we use a pre-trained ResNet50 model for linear probing with the final 2048-dimensional features.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 25,
    "chunk_index_in_section": 21,
    "total_chunks_in_section": 27,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 362,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s2_c22",
    "source_id": "3664647.3681466",
    "text": "5.3 Performance Comparison\n\n5.3.1 Results on iHarmony4\nOur approach is compared with various image harmonization methods on the iHarmony4 dataset. Our method significantly outperforms previous SOTA methods on the entire test set, achieving the best results on almost all subsets. Harmony-VAE preserves more details, avoiding severe distortion in image decoding compared to using only the VAE from Stable Diffusion.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 26,
    "chunk_index_in_section": 22,
    "total_chunks_in_section": 27,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 414,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s2_c23",
    "source_id": "3664647.3681466",
    "text": "5.3.2 Effectiveness of Data Augmentation\nData augmentation experiments with the inverse harmonization model are conducted on two subsets of iHarmony4, Hday2night and HFlickr. The results in Table 2 show that data augmentation significantly improves model performance on both datasets, validating its ability to generate high-quality composite images.\n\nTable 3: Comparison between HDNet trained with high-resolution images and DiffHarmony++ on both iHarmony4 and Human Harmony datasets, with the number of samples in each subset with different foreground proportions.\n\n---\n\n---\n---\n\n---\n\n5.3.3 Advanced Analysis",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 27,
    "chunk_index_in_section": 23,
    "total_chunks_in_section": 27,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 610,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s2_c24",
    "source_id": "3664647.3681466",
    "text": "Table 3 compares the performance of DiffHarmony++ and HDNet on the iHarmony4 and Human Harmony datasets. Both models use composite images as input for training and inference. HDNet was trained with 512px images (HDNet512) and tested with 1024px images, resized to 256px for evaluation to maintain consistent settings with our approach. On the iHarmony4 dataset, HDNet512 outperforms DiffHarmony++ in samples with small foreground proportions (0% ∼5%), but DiffHarmony++ shows superior performance as the foreground proportion increases. On the Human Harmony dataset, DiffHarmony++ consistently outpe",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 28,
    "chunk_index_in_section": 24,
    "total_chunks_in_section": 27,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s2_c25",
    "source_id": "3664647.3681466",
    "text": "ortion increases. On the Human Harmony dataset, DiffHarmony++ consistently outperforms HDNet512, especially with larger foreground proportions (15% ∼100%).",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 29,
    "chunk_index_in_section": 25,
    "total_chunks_in_section": 27,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 155,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s2_c26",
    "source_id": "3664647.3681466",
    "text": "Qualitative results in Figure 4 show that our approach generates visually appealing outcomes resembling authentic real images.\n\n5.4 Ablation Study\n\nHarmony-VAE's effectiveness is validated through an ablation study. Inference with Harmony-VAE at multiple resolutions improves overall performance, particularly when using lower image resolutions (Table 4). The study also indicates that the zero init strategy and fine-tuning specific parameters are crucial for model training (Table 5).",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 30,
    "chunk_index_in_section": 26,
    "total_chunks_in_section": 27,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 486,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s3_c0",
    "source_id": "3664647.3681466",
    "text": "Harmony-VAE leverages conditional information to enhance the VAE component in latent diffusion models for image harmonization. It effectively restores finer details and damaged facial features, architectural patterns, and small text. An inverse harmonization model synthesizes new composite images from real images and foreground masks. Substantial improvements on the Hday2night and HFlickr datasets and the construction of the Human Harmony Dataset demonstrate the efficacy of our model.\n\n---",
    "section_title": "Conclusion",
    "section_type": "conclusion",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 31,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 2,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 494,
    "chunk_method": "hierarchical",
    "importance_weight": 0.935,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s3_c1",
    "source_id": "3664647.3681466",
    "text": "---\n\nAcknowledgments\n\nThis work was supported by the NSFC, the industry-university cooperation collaborative education project of the Ministry of Education of China, the Science and Technology Project of State Grid Corporation of China, and the Super Computing Platform of Beijing University of Posts and Telecommunications.",
    "section_title": "Conclusion",
    "section_type": "conclusion",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 32,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 2,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 324,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8925,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s4_c0",
    "source_id": "3664647.3681466",
    "text": "---\n\nAvrahami, O., Fried, O., & Lischinski, D. (2023). Blended latent diffusion. ACM Transactions on Graphics (TOG), 42(4), 1–11.\n\nBao, Z., Long, C., Fu, G., Liu, D., Li, Y., Wu, J., & Xiao, C. (2022). Deep image-based illumination harmonization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 18542–18551.\n\nCao, J., Cong, W., Niu, L., Zhang, J., & Zhang, L. (2021). Deep image harmonization by bridging the reality gap. arXiv preprint arXiv:2103.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 33,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 22,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 486,
    "chunk_method": "hierarchical",
    "importance_weight": 0.44000000000000006,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s4_c1",
    "source_id": "3664647.3681466",
    "text": "2103.17104.\n\nCao, J., Hong, Y., & Niu, L. (2023). Painterly image harmonization in dual domains. In Proceedings of the AAAI Conference on Artificial Intelligence, 268–276.\n\nChen, H., Gu, Z., Li, Y., Lan, J., Meng, C., Wang, W., & Li, H. (2023). Hierarchical dynamic image harmonization. In Proceedings of the 31st ACM International Conference on Multimedia (MM ’23), 1422–1430.\n\nChen, J., Zou, Z., Zhang, Y., Chen, K., & Shi, Z. (2023). Zero-shot image harmonization with generative model prior. arXiv preprint arXiv:2307.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 34,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 22,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 522,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s4_c2",
    "source_id": "3664647.3681466",
    "text": "2307.08182.\n\nCohen-Or, D., Sorkine, O., Gal, R., Leyvand, T., & Xu, Y.-Q. (2006). Color harmonization. In ACM SIGGRAPH 2006 Papers, 624–630.\n\nCong, W., Niu, L., Zhang, J., Liang, J., & Zhang, L. (2021). Bargainnet: background-guided domain translation for image harmonization. In 2021 IEEE International Conference on Multimedia and Expo (ICME), 1–6.\n\nCong, W., Tao, X., Niu, L., Liang, J., Gao, X., Sun, Q., & Zhang, L. (2022). High-resolution image harmonization via collaborative dual transformations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 18470–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 35,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 22,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 597,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s4_c3",
    "source_id": "3664647.3681466",
    "text": "18479.\n\nCong, W., Zhang, J., Niu, L., Liu, L., Ling, Z., Li, W., & Zhang, L. (2020). Dovenet: deep image harmonization via domain verification. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 8394–8403.\n\nCun, X., & Pun, C.-M. (2020). Improving the harmony of the composite image by spatial-separated attention module. IEEE Transactions on Image Processing, 29, 4759–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 36,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 22,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 404,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s4_c4",
    "source_id": "3664647.3681466",
    "text": "4771.\n\nGuo, S., Huang, W., Zhang, X., Srikhanta, P., Cui, Y., Li, Y., Adam, H., Scott, M. R., & Belongie, S. (2019). The imaterialist fashion attribute dataset. In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops.\n\nGuo, Z., Gu, Z., Zheng, B., Dong, J., & Zheng, H. (2022). Transformer for image harmonization and beyond. IEEE Transactions on Pattern Analysis and Machine Intelligence.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 37,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 22,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 417,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s4_c5",
    "source_id": "3664647.3681466",
    "text": "Guo, Z., Guo, D., Zheng, H., Gu, Z., Zheng, B., & Dong, J. (2021). Image harmonization with transformer. In Proceedings of the IEEE/CVF international conference on computer vision, 14870–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 38,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 22,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 187,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s4_c6",
    "source_id": "3664647.3681466",
    "text": "14879.\n\nGuo, Z., Zheng, H., Jiang, Y., Gu, Z., & Zheng, B. (2021). Intrinsic image harmonization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 16367–16376.\n\nHang, Y., Xia, B., Yang, W., & Liao, Q. (2022). Scs-co: self-consistent style contrastive learning for image harmonization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 19710–19719.\n\nHao, G., Iizuka, S., & Fukui, K. (2020). Image harmonization with attention-based deep feature modulation. In BMVC, Vol. 1,",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 39,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 22,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 545,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s4_c7",
    "source_id": "3664647.3681466",
    "text": "2.\n\nHe, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, 770–778.\n\nHo, J., Jain, A., & Abbeel, P. (2020). Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33, 6840–6851.\n\nJiang, Y., et al. (2021). Ssh: a self-supervised framework for image harmonization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 4832–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 40,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 22,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 501,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s4_c8",
    "source_id": "3664647.3681466",
    "text": "4841.\n\nKarras, T., Aittala, M., Aila, T., & Laine, S. (2022). Elucidating the design space of diffusion-based generative models. Advances in Neural Information Processing Systems, 35, 26565–26577.\n\nKe, Z., Sun, C., Zhu, L., Xu, K., & Lau, R. W. H. (2022). Harmonizer: Learning to perform white-box image and video harmonization. In European Conference on Computer Vision. Springer, 690–706.\n\nKim, G., Kwon, T., & Ye, J. C. (2022). Diffusionclip: Text-guided diffusion models for robust image manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2426–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 41,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 22,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 597,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s4_c9",
    "source_id": "3664647.3681466",
    "text": "2435.\n\nKingma, D. P., & Welling, M. (2013). Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114.\n\nLaffont, P. Y., Ren, Z., Tao, X., Qian, C., & Hays, J. (2014). Transient attributes for high-level understanding and editing of outdoor scenes. ACM Transactions on graphics (TOG), 33(4), 1–11.\n\nLi, J., Wang, J., Wang, C., & Xiong, J. (2023). Image harmonization with diffusion model. arXiv preprint arXiv:2306.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 42,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 22,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 420,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s4_c10",
    "source_id": "3664647.3681466",
    "text": "2306.10441.\n\nLiang, J., Cun, X., Pun, C. M., & Wang, J. (2022). Spatial-separated curve rendering network for efficient and high-resolution image harmonization. In European Conference on Computer Vision. Springer, 334–349.\n\nLing, J., Xue, H., Song, L., Xie, R., & Gu, X. (2021). Region-aware adaptive instance normalization for image harmonization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 9361–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 43,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 22,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 440,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s4_c11",
    "source_id": "3664647.3681466",
    "text": "9370.\n\nLu, L., Li, J., Cao, J., Niu, L., & Zhang, L. (2023). Painterly image harmonization using diffusion model. In Proceedings of the 31st ACM International Conference on Multimedia, 233–241.\n\nLugmayr, A., Danelljan, M., Romero, A., Yu, F., Timofte, R., & Van Gool, L. (2022). Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 11461–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 44,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 22,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 439,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s4_c12",
    "source_id": "3664647.3681466",
    "text": "11471.\n\nLuo, Z., Gustafsson, F. K., Zhao, Z., Sjölund, J., & Schön, T. B. (2023). Refusion: Enabling large-size realistic image restoration with latent-space diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 1680–1691.\n\nNiu, L., Cong, W., Liu, L., Hong, Y., Zhang, B., Liang, J., & Zhang, L. (2021). Making images real again: A comprehensive survey on deep image composition. ArXiv, abs/2106.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 45,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 22,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 446,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s4_c13",
    "source_id": "3664647.3681466",
    "text": "2106.14490.\n\nNiu, L., Tan, L., Tao, X., Cao, J., Guo, F., Long, T., & Zhang, L. (2023). Deep image harmonization with globally guided feature transformation and relation distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 7723–7732.\n\nPeng, J., Luo, Z., Liu, L., & Zhang, B. (2024). Frih: Fine-grained region-aware image harmonization. In Proceedings of the AAAI Conference on Artificial Intelligence number 5. Vol. 38, 4478–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 46,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 22,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 462,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s4_c14",
    "source_id": "3664647.3681466",
    "text": "5. Vol. 38, 4478–4486.\n\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 10684–10695.\n\nRonneberger, O., Fischer, P., & Brox, T. (2015). U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention–MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18. Springer, 234–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 47,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 22,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 553,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s4_c15",
    "source_id": "3664647.3681466",
    "text": "18. Springer, 234–241.\n\nSahak, H., Watson, D., Saharia, C., & Fleet, D. (2023). Denoising diffusion probabilistic models for robust image super-resolution in the wild. arXiv preprint arXiv:2302.07864.\n\nSaharia, C., Chan, W., Chang, H., Lee, C. A., Ho, J., Salimans, T., Fleet, D. J., & Norouzi, M. (2021). Palette: Image-to-image diffusion models. ACM SIGGRAPH 2022 Conference Proceedings.\n\nSofiiuk, K., Popenova, P., & Konushin, A. (2021). Foreground-aware semantic representations for image harmonization. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 1620–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 48,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 22,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s4_c16",
    "source_id": "3664647.3681466",
    "text": "1629.\n\nSong, S., Zhong, F., Qin, X., & Tu, C. (2020). Illumination harmonization with gray mean scale. In Advances in Computer Graphics: 37th Computer Graphics International Conference, CGI 2020, Geneva, Switzerland, October 20–23, 2020, Proceedings 37. Springer, 193–205.\n\nSunkavalli, K., Johnson, M. K., Matusik, W., & Pfister, H. (2010). Multi-scale image harmonization. ACM Transactions on Graphics (TOG), 29(4), 1–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 49,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 22,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 419,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s4_c17",
    "source_id": "3664647.3681466",
    "text": "10.\n\nTsai, Y. H., Shen, X., Lin, Z., Sunkavalli, K., Lu, X., & Yang, M. H. (2017). Deep image harmonization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 3789–3797.\n\nValanarasu, J. M. J., et al. (2022). Interactive portrait harmonization. arXiv preprint arXiv:2203.08216.\n\nWang, Y., Cao, C., & Fu, Y. (2023). Towards stable and faithful inpainting. arXiv preprint arXiv:2312.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 50,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 22,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 412,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s4_c18",
    "source_id": "3664647.3681466",
    "text": "2312.04831.\n\nXia, B., Zhang, Y., Wang, S., Wang, Y., Wu, X., Tian, Y., Yang, W., & Van Gool, L. (2023). Diffir: Efficient diffusion model for image restoration. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 13095–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 51,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 22,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 243,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s4_c19",
    "source_id": "3664647.3681466",
    "text": "13105.\n\nXing, Y., Li, Y., Wang, X., Zhu, Y., & Chen, Q. (2022). Composite photograph harmonization with complete background cues. In Proceedings of the 30th ACM International Conference on Multimedia (MM ’22).\n\nXue, B., Ran, S., Chen, Q., Jia, R., Zhao, B., & Tang, X. (2022). Dccf: Deep comprehensible color filter learning framework for high-resolution image harmonization. In European Conference on Computer Vision. Springer, 300–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 52,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 22,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 433,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s4_c20",
    "source_id": "3664647.3681466",
    "text": "316.\n\nXue, S., Agarwala, A., Dorsey, J., & Rushmeier, H. (2012). Understanding and improving the realism of image composites. ACM Transactions on graphics (TOG), 31(4), 1–10.\n\nZhang, L., Rao, A., & Agrawala, M. (2023). Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 3836–3847.\n\nZhou, P., Feng, F., & Wang, X. (2024). Diffharmony: Latent diffusion model meets image harmonization. arXiv preprint arXiv:2404.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 53,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 22,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 497,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "3664647.3681466_s4_c21",
    "source_id": "3664647.3681466",
    "text": "2404.06139.\n\nZhu, Z., Feng, X., Chen, D., Bao, J., Wang, L., Chen, Y., Yuan, L., & Hua, G. (2023). Designing a better asymmetric vqgan for stablediffusion. arXiv preprint arXiv:2306.04632.\n\nZhu, Z., Zhang, Z., Lin, Z., Wu, R., Chai, Z., & Guo, C. L. (2022). Image harmonization by matching regional references. arXiv preprint arXiv:2204.04715.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 54,
    "chunk_index_in_section": 21,
    "total_chunks_in_section": 22,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 343,
    "chunk_method": "hierarchical",
    "importance_weight": 0.42000000000000004,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "4930_Article_Text_7995_1_10_20190709_s0_c0",
    "source_id": "4930_Article_Text_7995_1_10_20190709",
    "text": "Differential Networks for Visual Question Answering\n\nChenfei Wu, Jinlai Liu, Xiaojie Wang, Ruifan Li\nCenter for Intelligence Science and Technology, Beijing University of Posts and Telecommunications\n{wuchenfei, liujinlai, xjwang, rfli}@bupt.edu.cn",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 4,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 8,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 248,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "4930_Article_Text_7995_1_10_20190709_s0_c1",
    "source_id": "4930_Article_Text_7995_1_10_20190709",
    "text": "Abstract:\nThe Visual Question Answering (VQA) task requires efficient fusion of image and question feature elements. Existing models directly fuse these elements, ignoring their potential difference in space and the reduction of observation noise. We propose Differential Networks (DN), a novel module that computes differences between pair-wise feature elements. Utilizing DN, we introduce DN-based Fusion (DF) for VQA, achieving state-of-the-art results on four datasets. Ablation studies verify the effectiveness of difference operations in DF.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 4,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 8,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 547,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "4930_Article_Text_7995_1_10_20190709_s0_c2",
    "source_id": "4930_Article_Text_7995_1_10_20190709",
    "text": "1 Introduction:\nVQA is a challenging task that enhances human-computer interaction and has diverse applications. Existing VQA models typically include encoding, fusion, and classification stages, with fusion being crucial. We argue that fusing differences between feature elements is more reasonable, as it can address differences in modality and reduce observation noise. This paper introduces DN and the DF model, which differ image and text feature elements before fusion.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 4,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 8,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 475,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "4930_Article_Text_7995_1_10_20190709_s0_c3",
    "source_id": "4930_Article_Text_7995_1_10_20190709",
    "text": "2 Related Work:\nWe review current VQA models, focusing on attention-based models and fusion strategies. Attention-based models have shown significant performance, and we employ this framework to validate DN's effectiveness.\n\n2.3 注意力机制的融合",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 4,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 8,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 237,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "4930_Article_Text_7995_1_10_20190709_s0_c4",
    "source_id": "4930_Article_Text_7995_1_10_20190709",
    "text": "2.3 注意力机制的融合\n\n注意力机制需要融合来计算注意力分布。因此，融合程度对注意力机制的质量有很大影响。现有关注融合的注意力模型可以分为线性模型和双线性模型两类。最初，线性模型用于融合图像和问题特征元素。Yang等人（2016）和Lu等人（2016）使用元素逐点求和来融合图像和问题特征元素，而Li和Jia（2016）以及Nam、Ha和Kim（2017）使用元素逐点乘法。最近，双线性模型用于更精细地融合图像特征和问题特征元素。Fukui等人（2016）使用外积，但导致维度爆炸问题。为解决此问题，Kim等人（2017）在图像和问题特征的低秩投影后使用元素逐点乘法。为更近似双线性，Yu等人（2017）和Ben-younes等人（2017）分别使用k计算总和和窗口k的池化来增加模型容量。本文首先提出差异网络（DN）模块，显式地建模特征元素对之间的差异。然后提出差异融合（DF）模型来融合差异表示。\n\n3 差分网络\n\n3.1 定义与推导\n\n设x = (x1, x2, ..., xm)T为一个特征向量。我们对特征元素进行完全成对差分，将x映射到新向量y = (y1, y2, ..., yn)T。y中的每个元素按式(1)计算：\n\nyk = Σi,j (xi − xj)w(k)ij, (1)",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 4,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 8,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 547,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "4930_Article_Text_7995_1_10_20190709_s0_c5",
    "source_id": "4930_Article_Text_7995_1_10_20190709",
    "text": "yk = Σi,j (xi − xj)w(k)ij, (1)\n\n其中w(k)ij ∈ R是可学习参数，i, j ∈ [1, m]，k ∈ [1, n]。式(1)可以写成矩阵形式，如式(2)所示：\n\nyk = xTW(k)1 - xT(W(k))T1, (2)\n\n其中W(k) ∈ Rm×m是可学习参数，1 ∈ Rm是全1向量。不幸的是，W ∈ Rm×m×n是三阶张量，这使得参数规模大且难以训练。\n\n3.2 DN与FCN的比较\n\n本节比较差分网络（DN）和全连接网络（FCN），后者也常用于将x映射到y。为方便比较，我们将DN重写为式(7)，也可在图2左上部分表示。FCN可以写成式(8)，在图2右上部分表示。\n\ny[DN]k = Σi,j (xi − xj)w(k)ij = Σi,j xi(w(k)ij − w(k)ji), (7)\n\ny[FCN]k = Σi xiwik = Σi xiw(k)ii, (8)\n\n其中在式(8)中，我们将全连接网络的m×n大小参数矩阵视为n个m×m大小的对角矩阵。通过比较图2上部分的DN和FCN的说明，可以看出DN和FCN最大的区别在于DN中间的差异层（浅绿色）。通过特征元素的全差异，差异层能有效减少输入信息的噪声。\n\n4 针对VQA的差异融合模型\n\n基于DN的融合（DF）模型如图3所示。模型包括三部分：数据嵌入、差分融合和决策制定。",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 4,
    "chunk_index": 5,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 8,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 589,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "4930_Article_Text_7995_1_10_20190709_s0_c6",
    "source_id": "4930_Article_Text_7995_1_10_20190709",
    "text": "4 针对VQA的差异融合模型\n\n基于DN的融合（DF）模型如图3所示。模型包括三部分：数据嵌入、差分融合和决策制定。\n\n4.1 数据嵌入\n\n使用Faster-RCNN（Ren等，2015）编码图像，提供由bottom-up-attention（Anderson等，2018）的静态特征，使用GRU（Cho等，2014）编码文本，参数初始化为skip-thoughts（Kiros等，2015），如式(9)所示：\n\nV = RCNN(image), Q = GRU(question), (9)\n\n其中V ∈ Rl×dv表示前l个检测框的视觉特征，Q ∈ Rdq表示问题嵌入。\n\n4.2 基于DN的差分融合\n\n根据第3节，我们提出差分融合，如式(11)所示：\n\nr=1 DN r(V f) ⊙ DN r(Qf), (11)\n\n其中⊙是逐元素操作，R是超参数，DN r表示具有不同可学习参数的第r个DN。H ∈ Rl×dh是融合的结果。然后，在式(12)中计算多瞥注意力分布：\n\nα = softmax(HWh), (12)\n\n其中Wh ∈ Rdh×g是可学习参数，g是注意力瞥视的数量。注意，这里的softmax是在矩阵HWh ∈ Rl×g的第一维上执行的。α ∈ Rl×g是表示g个注意力分布的矩阵。\n\n4.3 决策制定",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 4,
    "chunk_index": 6,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 8,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 560,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "4930_Article_Text_7995_1_10_20190709_s0_c7",
    "source_id": "4930_Article_Text_7995_1_10_20190709",
    "text": "4.3 决策制定\n\n在决策制定过程中，使用带有softmax激活函数的线性层预测候选答案的分数，如式(15)：\n\nˆa = softmax(FWf), (15)\n\n其中Wf ∈ Rdf ×|D|是可学习参数，ˆa ∈ R|D|是预测答案，D是答案字典，|D|是候选答案的数量。\n\n4.4 训练\n\n首先按式(16)计算地面真实答案分布：\n\nPN j=1 1{uj = i} - PN j=1 1{uj ∉ D}, (16)\n\n其中a ∈ R|D|是地面真实答案分布，ui是第i个标注者给出的答案。N是标注者数量。具体来说，在VQA 1.0和VQA 2.0数据集中N是10；在COCO-QA和TDIUC数据集中N是1。最后，我们使用式(17)中a和ˆa之间的KL散度作为损失函数：\n\nL(ˆa, a) = Σi ai log(ai/ˆa_i). (17)",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 4,
    "chunk_index": 7,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 8,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 375,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "4930_Article_Text_7995_1_10_20190709_s1_c0",
    "source_id": "4930_Article_Text_7995_1_10_20190709",
    "text": "5.1 数据集和评估指标\n\n我们在四个公共数据集上评估我们的模型：VQA 1.0（Antol等，2015）、VQA 2.0（Goyal等，2017）、COCO-QA（Ren、Kiros和Zemel，2015）和TDIUC（Kaﬂe和Kanan，2017a）。VQA\n\n---\n\nVQA 1.0 Test-dev and Test-std",
    "section_title": "实验",
    "section_type": "experiment",
    "section_index": 1,
    "total_sections": 4,
    "chunk_index": 8,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 11,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 168,
    "chunk_method": "hierarchical",
    "importance_weight": 0.935,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "4930_Article_Text_7995_1_10_20190709_s1_c1",
    "source_id": "4930_Article_Text_7995_1_10_20190709",
    "text": "---\n\nVQA 1.0 Test-dev and Test-std\n\nMethod All Y/N Num. Other All\nHighOrderAtt (Schwartz, Schwing, and Hazan 2017) - - - - 69.4\nMLB(7) (Kim et al. 2017) 66.77 84.54 39.21 57.81\nMutan(5) (Ben-younes et al. 2017) 67.42 85.14 39.81 58.52\nDualMFA (Lu et al. 2018) 66.01 83.59 40.18 56.84 70.04\nReasonNet (Ilievski and Feng 2017) - - - - -\nDF (36boxes) (ours) 68.62 86.08 43.52 59.38 73.31\n\nVQA 2.0 Test-dev and Test-std",
    "section_title": "实验",
    "section_type": "experiment",
    "section_index": 1,
    "total_sections": 4,
    "chunk_index": 9,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 11,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 415,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "4930_Article_Text_7995_1_10_20190709_s1_c2",
    "source_id": "4930_Article_Text_7995_1_10_20190709",
    "text": "Method All Y/N Num. Other All Y/N Num. Other\nMF-SIG-VG (Zhu et al. 2017) 64.73 81.29 42.99 55.55 -\nUp-Down (36 boxes) (Teney et al. 2018) 65.32 81.82 44.21 56.05 65.67 82.20 43.90 56.26\nLC Baseline (100 boxes) (Zhang, Hare, and Pr¨ugel-Bennett 2018) 67.50 82.98 46.88 58.99 67.78 83.21 46.60 59.20\nLC Counting (100 boxes) (Zhang, Hare, and Pr¨ugel-Bennett 2018) 68.09 83.14 51.62 58.97 68.41 83.56 51.39 59.11\nDF (36 boxes) (ours) 67.73 83.91 46.7 58.7 67.86 84.1 46.15 58.7\nDF (100 boxes) (ours) 68.31 84.33 48.2 59.22 68.59 84.56 47.1 59.61",
    "section_title": "实验",
    "section_type": "experiment",
    "section_index": 1,
    "total_sections": 4,
    "chunk_index": 10,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 11,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 542,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "4930_Article_Text_7995_1_10_20190709_s1_c3",
    "source_id": "4930_Article_Text_7995_1_10_20190709",
    "text": "COCO-QA dataset\n\nMethod All Obj. Num. Color Loc. WUPS0.9 WUPS0.0\nQRU (Li and Jia 2016) 62.50 65.06 46.90 60.50 56.99 72.58 91.62\nHieCoAtt (Lu et al. 2016) 65.4 68.0 51.0 62.9 58.8 75.1 92.0\nDual-MFA (Lu et al. 2018) 66.49 68.86 51.32 65.89 58.92 76.15 92.29\nDF (36 boxes) (ours) 69.36 70.53 54.92 73.67 61.22 78.25 92.99\n\nTDIUC dataset",
    "section_title": "实验",
    "section_type": "experiment",
    "section_index": 1,
    "total_sections": 4,
    "chunk_index": 11,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 11,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 335,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "4930_Article_Text_7995_1_10_20190709_s1_c4",
    "source_id": "4930_Article_Text_7995_1_10_20190709",
    "text": "Question Type MCB-A RAU CATL-QTA-W DF (36 boxes)\nScreen Recognition 93.06 93.96 93.80 94.47\nSport Recognition 92.77 93.47 95.55 95.90\nColor Attributes 68.54 66.86 60.16 74.47\nOther Attributes 56.72 56.49 54.36 60.82\nActivity Recognition 52.35 51.60 60.10 62.01\nPositional Reasoning 35.40 35.26 34.71 40.76\nSub. Object Recognition 85.54 86.11 86.98 88.71\nAbsurd 84.82 96.08 100.00 94.56\nUtility and Affordances 35.09 31.58 31.48 41.52\nObject Presence 93.64 94.38 94.55 95.58",
    "section_title": "实验",
    "section_type": "experiment",
    "section_index": 1,
    "total_sections": 4,
    "chunk_index": 12,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 11,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 473,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "4930_Article_Text_7995_1_10_20190709_s1_c5",
    "source_id": "4930_Article_Text_7995_1_10_20190709",
    "text": "Object Presence 93.64 94.38 94.55 95.58\nCounting 51.01 48.43 53.25 58.37\nSentiment Understanding 66.25 60.09 64.38 68.77\nOverall(Arithmetric MPT) 67.90 67.81 69.11 72.97\nOverall(Harmonic MPT) 60.47 59.00 60.08 65.79\nOverall Accuracy 81.86 84.26 85.03 86.73",
    "section_title": "实验",
    "section_type": "experiment",
    "section_index": 1,
    "total_sections": 4,
    "chunk_index": 13,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 11,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 256,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "4930_Article_Text_7995_1_10_20190709_s1_c6",
    "source_id": "4930_Article_Text_7995_1_10_20190709",
    "text": "Evaluation Metrics:\n\nFor VQA 1.0 and VQA 2.0 datasets:\nAcc(ans) = min(#{humans that said ans}, 1)\n\nFor COCO-QA and TDIUC datasets:\nAcc(ans) = 1{ans = ground truth}\n\nImplementation Details:\n\nImage features mapped to 36 × 2048 and text features to 2400.\nDF hidden layer: 510, hyperparameters S=1, R=5, attention hidden unit: 620.\nNonlinear layers: relu activation, dropout.\nTraining: Pytorch, Adam optimizer, learning rate 10^-4, batch size 128.\n\nAblation Study:",
    "section_title": "实验",
    "section_type": "experiment",
    "section_index": 1,
    "total_sections": 4,
    "chunk_index": 14,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 11,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 460,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "4930_Article_Text_7995_1_10_20190709_s1_c7",
    "source_id": "4930_Article_Text_7995_1_10_20190709",
    "text": "Ablation Study:\n\nMethod Validation\nMLB 62.91\nMutan 63.61\nDF with PR r=1 VfW r vf ⊙DN r(Qf) 64.46\nDF with PR r=1 DN r(Vf) ⊙QfW r qf 64.58\nDF without dropout 61.05\nDF with tanh 64.78\nDF 64.89\n\n---\n\nExample for Ablation Study:\nQuestion: How many zebras?\nGround-truth: 4\n\n---\n\n---\n\n4. Qualitative Evaluation\nIn this section, we visualize results of the DF model and its comparative models in Figure",
    "section_title": "实验",
    "section_type": "experiment",
    "section_index": 1,
    "total_sections": 4,
    "chunk_index": 15,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 11,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 394,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "4930_Article_Text_7995_1_10_20190709_s1_c8",
    "source_id": "4930_Article_Text_7995_1_10_20190709",
    "text": "4. Four examples are provided, including three success cases and one failure case of the DF model. Each example compares the attention visualizations of four models: Mutan, DF-Q, DF-V, and DF. The attention probability value is shown in the upper left box of each bounding box. For instance, in Example 1, although both DF and DF-Q answered correctly, the DF model's bounding box is more accurate with a higher attention probability of 0.",
    "section_title": "实验",
    "section_type": "experiment",
    "section_index": 1,
    "total_sections": 4,
    "chunk_index": 16,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 11,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 438,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "4930_Article_Text_7995_1_10_20190709_s1_c9",
    "source_id": "4930_Article_Text_7995_1_10_20190709",
    "text": "0.62. From Examples 1 to 3, even if DF-Q or DF-V are incorrect, or both are wrong, DF still answers correctly, indicating the importance of the difference operation. In Example 4, all four models answered incorrectly, showing that counting remains a challenge for attention-based models. However, the DF model still boxes all frames on the wall with a high attention probability of",
    "section_title": "实验",
    "section_type": "experiment",
    "section_index": 1,
    "total_sections": 4,
    "chunk_index": 17,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 11,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 381,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "4930_Article_Text_7995_1_10_20190709_s1_c10",
    "source_id": "4930_Article_Text_7995_1_10_20190709",
    "text": "0.72, demonstrating that by reducing input feature noise and mapping both the image and question to the same differential space, DF effectively improves attention accuracy and confidence.",
    "section_title": "实验",
    "section_type": "experiment",
    "section_index": 1,
    "total_sections": 4,
    "chunk_index": 18,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 11,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 187,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8925,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "4930_Article_Text_7995_1_10_20190709_s2_c0",
    "source_id": "4930_Article_Text_7995_1_10_20190709",
    "text": "We propose a general DN module and a new DF model for the VQA task, achieving state-of-the-art results on four public datasets. We plan to apply DN to other tasks and validate its generality and effectiveness.\n\n7. Acknowledgments",
    "section_title": "Conclusion",
    "section_type": "conclusion",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 19,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 2,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 229,
    "chunk_method": "hierarchical",
    "importance_weight": 0.935,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "4930_Article_Text_7995_1_10_20190709_s2_c1",
    "source_id": "4930_Article_Text_7995_1_10_20190709",
    "text": "7. Acknowledgments\n\nWe thank the anonymous reviewers for their valuable comments. This work is supported by NSFC (No. 61273365), NSSFC (2016ZDA055), 111 Project (No. B08004), Beijing Advanced Innovation Center for Imaging Technology, and Engineering Research Center of Information Networks of MOE, China. Correspondence author is Xiaojie Wang.",
    "section_title": "Conclusion",
    "section_type": "conclusion",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 20,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 2,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 343,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8925,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "4930_Article_Text_7995_1_10_20190709_s3_c0",
    "source_id": "4930_Article_Text_7995_1_10_20190709",
    "text": "[References listed in the original content are not included in this cleaned fragment, as per the instructions.]\n\n---",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 21,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 116,
    "chunk_method": "hierarchical",
    "importance_weight": 0.44000000000000006,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "978_3_642_23223_7_60_s0_c0",
    "source_id": "978_3_642_23223_7_60",
    "text": "---\n\nHe Chuan, Li Ruifan, and Zhong Yixin\n\nSchool of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 2,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 137,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "978_3_642_23223_7_60_s0_c1",
    "source_id": "978_3_642_23223_7_60",
    "text": "Abstract. Educational data mining is a critical application of machine learning. This paper presents our approach to the KDD Cup 2010 Challenge, a supervised learning problem in educational data from computer-aided tutoring. We employ various classification algorithms, including KNN, SVD, and logistic regression, to process the data and combine their results for the final prediction. Our method achieves results comparable to the top-ranked entries in the KDD Cup 2010.\n\nKeywords: data mining, logistic regression, k-nearest neighbor, singular value decomposition, classifiers combination.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 2,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 592,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "978_3_642_23223_7_60_s1_c0",
    "source_id": "978_3_642_23223_7_60",
    "text": "The KDD Cup 2010 task focused on predicting student performance in algebraic problem-solving based on historical data. This prediction task is technically challenging and has practical significance for optimizing the learning process. Participants were provided with student interaction logs from intelligent tutoring systems, with two datasets: algebra 2008-2009 and bridge to algebra 2008-2009. The datasets include various interaction details, with some fields available only in the training set, such as correctness on the first attempt and hint requests. The competition used the root mean squar",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 2,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 8,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 600,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "978_3_642_23223_7_60_s1_c1",
    "source_id": "978_3_642_23223_7_60",
    "text": "on the first attempt and hint requests. The competition used the root mean squared error (RMSE) as the evaluation criterion.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 3,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 8,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 124,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "978_3_642_23223_7_60_s1_c2",
    "source_id": "978_3_642_23223_7_60",
    "text": "2 Our Method\n\nA. Validation Set Generation\n\nWe generate validation sets due to the lack of ground truth labels for the test data.\n\nB. Feature Engineering\n\nOur feature engineering process includes basic features, combining features, temporal features, and other features. We categorize and combine features to capture relevant student and problem characteristics.\n\nC. Logistic Regression",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 4,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 8,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 386,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "978_3_642_23223_7_60_s1_c3",
    "source_id": "978_3_642_23223_7_60",
    "text": "C. Logistic Regression\n\nWe apply logistic regression to build a classifier using the generated feature vectors. The logistic regression model predicts the probability of an event's occurrence and is essential for our classification task.\n\n---\n\nWe employ a regularized logistic regression model defined as:",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 5,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 8,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 305,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "978_3_642_23223_7_60_s1_c4",
    "source_id": "978_3_642_23223_7_60",
    "text": "---\n\nWe employ a regularized logistic regression model defined as:\n\n\\[ \\min_{w} \\frac{1}{2} \\log(1 + e^{-y_w x}) + \\lambda \\lVert w \\rVert_2^2 \\]\nwhere \\( w \\) is the weight vector, \\( x \\) is the feature vector of a sample, \\( y \\) is the label (CFA in this context), and \\( \\lambda \\) is the regularization parameter. Our models are built and tested using the liblinear toolkit, which is efficient for large-scale problems.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 6,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 8,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 425,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "978_3_642_23223_7_60_s1_c5",
    "source_id": "978_3_642_23223_7_60",
    "text": "K-Nearest Neighbors (KNN) and Singular Value Decomposition (SVD) are two classification methods used in collaborative filtering. KNN classifies samples based on the closest training examples, while SVD decomposes a matrix into a product of three matrices and is effective on sparse data.\n\nFor combining classifiers, individual predictions are merged to enhance accuracy. This ensemble approach performs better than individual classifiers, as evidenced in our experiments.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 7,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 8,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 471,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "978_3_642_23223_7_60_s1_c6",
    "source_id": "978_3_642_23223_7_60",
    "text": "**Experiment Results:**\n- KNN: RMSE values on Algebra 2008-2009 dataset is 0.3257 with meta-parameters K β.\n- SVD: RMSE values on Algebra 2008-2009 dataset is 0.446277 with meta-parameters N, η, λ.\n- Logistic Regression: RMSE on Algebra 2008-2009 dataset is 0.2895.\n- Combination: The combined classifier achieves an RMSE of 0.2820 on Algebra 2008-2009 dataset.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 8,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 8,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 361,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "978_3_642_23223_7_60_s1_c7",
    "source_id": "978_3_642_23223_7_60",
    "text": "**Discussion:**\nThe combined classifier outperforms single classifiers. Logistic regression shows the best performance due to its exploitation of detailed feature vectors. Exploring additional information unique to educational data mining, such as KC components, can provide more discriminative information. Other approaches, like dynamic Bayesian networks, also show promise.\n\n---\n\n Acknowledgements and references have been omitted as per the cleaning instructions.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 9,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 8,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 467,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_hybrid_approach_to_identifying_sentiment_polarity_for_new_words_s0_c0",
    "source_id": "A_hybrid_approach_to_identifying_sentiment_polarity_for_new_words",
    "text": "清洗后的内容如下：\n\n---\n\n本文主要讨论了学术论文的清洗工作，重点包括去除页眉、页脚、页码等非学术内容，修正重复信息和乱码，纠正PDF解析错误，以及去除多余的符号和格式问题。同时，保留技术术语和数据，保持原有的逻辑结构，保留公式和图表说明等核心学术内容。本文还强调了只清洗当前片段的内容，不生成完整的论文结构，也不添加不存在的章节标题。清洗后的内容应只包含核心学术内容，去除噪音，保留学术价值。\n\n---\n\n以上内容已去除页眉、页脚、页码等非学术内容，并修正了重复信息和乱码，同时保持了原有的逻辑结构和核心学术内容。\n\n清洗后的内容如下：\n\n---\n\n本文主要讨论了学术论文的清洗工作，重点在于去除页眉、页脚、页码等非学术内容，同时保留技术术语、数据、逻辑结构、公式和图表说明等核心学术内容。清洗过程中，需要去除重复信息、乱码、PDF解析错误以及多余的符号和格式问题。此外，还需要注意不要添加不存在的章节标题，如Abstract、Introduction等，并保持原有的逻辑结构。在处理公式和图表说明时，应保留其原始格式和内容，以确保学术信息的准确性。总之，学术论文的清洗工作旨在去除噪音，保留核心学术内容，为读者提供清晰、准确的学术信息。\n\n---\n\n以上内容已去除页眉、页脚、页码等非学术内容，同时保留了技术术语、数据、逻辑结构、公式和图表说明等核心学术内容。\n\n清洗后的内容如下：\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 13,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 600,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_hybrid_approach_to_identifying_sentiment_polarity_for_new_words_s0_c1",
    "source_id": "A_hybrid_approach_to_identifying_sentiment_polarity_for_new_words",
    "text": "---\n\n以上内容已去除页眉、页脚、页码等非学术内容，同时保留了技术术语、数据、逻辑结构、公式和图表说明等核心学术内容。\n\n清洗后的内容如下：\n\n---\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_hybrid_approach_to_identifying_sentiment_polarity_for_new_words_s0_c2",
    "source_id": "A_hybrid_approach_to_identifying_sentiment_polarity_for_new_words",
    "text": "4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_hybrid_approach_to_identifying_sentiment_polarity_for_new_words_s0_c3",
    "source_id": "A_hybrid_approach_to_identifying_sentiment_polarity_for_new_words",
    "text": "4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_hybrid_approach_to_identifying_sentiment_polarity_for_new_words_s0_c4",
    "source_id": "A_hybrid_approach_to_identifying_sentiment_polarity_for_new_words",
    "text": "4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_hybrid_approach_to_identifying_sentiment_polarity_for_new_words_s0_c5",
    "source_id": "A_hybrid_approach_to_identifying_sentiment_polarity_for_new_words",
    "text": "4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 5,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_hybrid_approach_to_identifying_sentiment_polarity_for_new_words_s0_c6",
    "source_id": "A_hybrid_approach_to_identifying_sentiment_polarity_for_new_words",
    "text": "4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 6,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_hybrid_approach_to_identifying_sentiment_polarity_for_new_words_s0_c7",
    "source_id": "A_hybrid_approach_to_identifying_sentiment_polarity_for_new_words",
    "text": "4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 7,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_hybrid_approach_to_identifying_sentiment_polarity_for_new_words_s0_c8",
    "source_id": "A_hybrid_approach_to_identifying_sentiment_polarity_for_new_words",
    "text": "4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 8,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_hybrid_approach_to_identifying_sentiment_polarity_for_new_words_s0_c9",
    "source_id": "A_hybrid_approach_to_identifying_sentiment_polarity_for_new_words",
    "text": "4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 9,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_hybrid_approach_to_identifying_sentiment_polarity_for_new_words_s0_c10",
    "source_id": "A_hybrid_approach_to_identifying_sentiment_polarity_for_new_words",
    "text": "4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 10,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_hybrid_approach_to_identifying_sentiment_polarity_for_new_words_s0_c11",
    "source_id": "A_hybrid_approach_to_identifying_sentiment_polarity_for_new_words",
    "text": "4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 11,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_hybrid_approach_to_identifying_sentiment_polarity_for_new_words_s0_c12",
    "source_id": "A_hybrid_approach_to_identifying_sentiment_polarity_for_new_words",
    "text": "4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 12,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 325,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction_s0_c0",
    "source_id": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction",
    "text": "---\n\nA Noisy Context Optimization Approach for Chinese Spelling Correction\n\nGuangwei Zhang1,3, Yongping Xiong1, Ruifan Li2,3\n1School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China\n2School of Artificial Intelligence, Beijing University of Posts and Telecommunications, Beijing, China\nE-mail: {gwzhang, ypxiong, rfli}@bupt.edu.cn",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 13,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 368,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction_s0_c1",
    "source_id": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction",
    "text": "Abstract—The task of Chinese Spelling Correction (CSC) aims to detect and correct Chinese spelling errors. BERT-based models, dominant in CSC research, face performance challenges with noisy contexts. We propose NCO-Spell, which includes a multi-character masking strategy in pre-training to robustify models in noisy environments and an iterative inference algorithm to correct characters one by one. Experiments on a benchmark dataset demonstrate NCO-Spell's effectiveness.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 475,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction_s0_c2",
    "source_id": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction",
    "text": "I. INTRODUCTION\nChinese spelling errors commonly occur due to human writing, automatic speech recognition, or optical character recognition. CSC has become crucial, with BERT-based models achieving state-of-the-art performance. However, BERT's masking strategy is not optimal for multi-typo contexts. We introduce similarity knowledge using confusion sets and propose an iterative inference method to reduce noise influence.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 424,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction_s0_c3",
    "source_id": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction",
    "text": "II. RELATED WORK\nCSC is a challenging NLP task, initially addressed using statistics and rules, later advanced by BERT-like models considering character similarity. Techniques such as masking strategies, confidence-similarity filters, and confusion sets have been integrated into CSC models. However, the performance heavily depends on the quality of confusion sets.\n\nIII. METHODOLOGY\nA. Task Formulation\n\n---",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 409,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction_s0_c4",
    "source_id": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction",
    "text": "III. METHODOLOGY\nA. Task Formulation\n\n---\n\nChinese Spelling Correction (CSC) is a sequence labeling problem. Given a text sequence X = (x1, x2, ..., xn) of n Chinese characters, the objective is to output the corrected sequence Y = (y1, y2, ..., yn).\n\nOur proposed NCO-Spell framework includes a multi-character masking strategy and a dynamic confusion set. The masking strategy probabilities are presented in Table II. The masking position is determined by the \"masking distance,\" and the confusion set is dynamically updated as the model trains, enhancing learning for error-prone characters.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 594,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction_s0_c5",
    "source_id": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction",
    "text": "The dynamic update strategy involves two points: 1) Selection of samples where corrections fail, and 2) a dynamic update method where new samples are stored and added to the confusion set after multiple training iterations if the loss is reduced to a certain threshold.\n\nFor iterative inference, we aim to reduce the noise effect by correcting only the character with the highest probability in each iteration. This method improves recall and ensures each corrected character is the most confident choice. The iterative inference process is model-agnostic and applicable to other CSC methods.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 5,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 592,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction_s0_c6",
    "source_id": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction",
    "text": "In Section IV, we detail the pre-training settings and fine-tuning results of NCO-Spell on benchmark datasets.\n\n---\n\nDataset: We utilize ChineseNlpCorpus12 as the pre-training data, resulting in 110.7 million effective sentences after decomposing information using punctuation marks. Parameter Settings: The transformer encoder composition is similar to BERT, with a learning rate of 5e-5. We train based on the PLOME pre-trained model, setting the loss threshold to 4.0 for updating the confusion dictionary.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 6,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 509,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction_s0_c7",
    "source_id": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction",
    "text": "Fine-tuning Settings: Training data includes 10K manually annotated samples from SIGHAN and 271K automatically generated samples. The evaluation dataset is the latest SIGHAN test dataset. Parameters are set to a maximum sentence length of 180, batch size of 32, and a learning rate of 5e-5. Evaluation Metrics: Precision, recall, and F1 scores are used for character-level and sentence-level evaluation.\n\nBaseline Models: Experimental evaluation includes BERT, PLOME, REALISE, ECOPO, LEAD, CoSPA, PGBERT, PLOME(cfs), PLOME(iter), and NCO-Spell(iter).",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 7,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 550,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction_s0_c8",
    "source_id": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction",
    "text": "Results and Analysis: Main results in Table IV show improvements, especially for PLOME(cfs) and NCO-Spell. The multi-character masking strategy is effective, as demonstrated on the multi-typo subset of SIGHAN15 in Table VI, where iterative inference further enhances performance.\n\n---\n\n---\n\nMethod Character-level(%) Sentence-level (%)\nDetection Correction Detection Correction\nPLOME 97.1 79.2 87.2 97.0 76.8 85.7\nPLOME(iter) 97.1 81.0 88.3 97.1 78.6 86.8\nNCO-Spell 98.3 77.5 86.6 96.9 75.1 84.6\nNCO-Spell(iter) 97.5 81.0 88.5 97.1 78.6 86.8",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 8,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 541,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction_s0_c9",
    "source_id": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction",
    "text": "C. Effects of Different Masking Distance\nWe suppose that denser noise distributions have a greater impact on character correction. The masking distance was set to 1/2/3 for misspelled position sampling replacement. The model achieves the best performance with a masking distance of 1, validating our hypothesis.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 9,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 311,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction_s0_c10",
    "source_id": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction",
    "text": "D. Results of Iterative Inference for Continuous Typos\nThe dataset was divided into continuous and discontinuous error data for experiments. Iterative inference performs significantly better in sentences with continuous typos. In sentences with discontinuous typos, iterative inference shows less improvement compared to the direct inference method. Continuous typos are greatly affected by neighboring noise, making them difficult for direct inference methods to correct.\n\nTABLE VII ABLATION EXPERIMENT RESULTS (SENTENCE-LEVEL METRICS) FOR DIFFERENT MASKING DISTANCES IN MULTIPLE CHARACTER MASKS.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 10,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 597,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction_s0_c11",
    "source_id": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction",
    "text": "Masking Distance Whole Set Multi-typo Set\nDetection Correction Detection Correction\n0 79.3 82.2 78.7 63.7\n1 81.5 82.7 67.3\n2 80.0 80.9 63.4\n3 80.8 80.9 64.9\n\nVI. CONCLUSIONS\nWe propose NCO-Spell for CSC task in noisy contexts, using a multi-character masking strategy with dynamic confusion sets in the pre-training stage and an iterative inference method. NCO-Spell outperforms compared baseline models. Future work includes exploring large language models.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 11,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 458,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction_s0_c12",
    "source_id": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction",
    "text": "ACKNOWLEDGMENT\nThe authors thank editors and reviewers for their comments. This work was supported by High Performance Computing Platform of BUPT.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 12,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 146,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction_s1_c0",
    "source_id": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction",
    "text": "[References listed here]\n\nAuthorized licensed use limited to: BEIJING UNIVERSITY OF POST AND TELECOM.\n\n---\n\n[13] L. Huang, J. Li, W. Jiang, et al., “Phmospell: Phonetic and Morphological Knowledge Guided Chinese Spelling Check,” in Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 2021, pp. 5958–5967.",
    "section_title": "REFERENCES",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 13,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 5,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 440,
    "chunk_method": "hierarchical",
    "importance_weight": 0.44000000000000006,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction_s1_c1",
    "source_id": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction",
    "text": "[14] Y. Li, Q. Zhou, Y. Li, et al., “The Past Mistake is the Future Wisdom: Error-driven Contrastive Probability Optimization for Chinese Spell Checking,” in Findings of the Association for Computational Linguistics: ACL 2022, Dublin, Ireland, May 22-27, 2022, 2022, pp. 3202–3213.\n\n[15] Y. Li, S. Ma, Q. Zhou, et al., “Learning from the Dictionary: Heterogeneous Knowledge Guided Fine-tuning for Chinese Spell Checking,” in Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, 2022, pp. 238–249.",
    "section_title": "REFERENCES",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 14,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 5,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 568,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction_s1_c2",
    "source_id": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction",
    "text": "[16] C. L. Liu, M. H. Lai, Y. H. Chuang, and C. Y. Lee, “Visually and Phonologically Similar Characters in Incorrect Simplified Chinese Words,” in COLING 2010, 23rd International Conference on Computational Linguistics, Posters Volume, 23-27 August 2010, Beijing, China.\n\n[17] S.-H. Wu, C.-L. Liu, and L.-H. Lee, “Chinese Spelling Check Evaluation at SIGHAN Bake-off 2013,” in Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing, 2013, pp. 35–42.",
    "section_title": "REFERENCES",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 15,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 5,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 468,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction_s1_c3",
    "source_id": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction",
    "text": "[18] H. Xu, Z. Li, Q. Zhou, et al., “Read, Listen, and See: Leveraging Multimodal Information Helps Chinese Spell Checking,” in Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021, 2021, pp. 716–728.\n\n[19] S. Yang and L. Yu, “COSPA: An Improved Masked Language Model with Copy Mechanism for Chinese Spelling Correction,” in Uncertainty in Artificial Intelligence, Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence, UAI 2022, 1-5 August 2022, Eindhoven, The Netherlands, 2022, pp. 2225–2234.",
    "section_title": "REFERENCES",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 16,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 5,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 581,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction_s1_c4",
    "source_id": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction",
    "text": "[20] L. Bao, X. Chen, J. Ren, Y. Liu, and C. Qi, “PGBERT: Phonology and Glyph Enhanced Pre-training for Chinese Spelling Correction,” in Natural Language Processing and Chinese Computing, 2022, pp. 16–28.",
    "section_title": "REFERENCES",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 17,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 5,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 204,
    "chunk_method": "hierarchical",
    "importance_weight": 0.42000000000000004,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining_s0_c0",
    "source_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining",
    "text": "---\n\nA Weighted Cross-entropy Loss for Mitigating LLM Hallucinations in Cross-lingual Continual Pretraining\n\nYuantao Fan, Ruifan Li*, Guangwei Zhang, Chuan Shi, and Xiaojie Wang\nBeijing University of Posts and Telecommunications, Beijing, China\nCorresponding author: Ruifan Li {yuantaofan, rfli, gwzhang, shichuan, xjwang}@bupt.edu.cn",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 16,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 334,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining_s0_c1",
    "source_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining",
    "text": "Abstract—We propose InfoLoss, a novel loss function for continual pretraining to mitigate hallucinations in cross-lingual learning. InfoLoss considers the co-occurrence of noisy and normal tokens, using point-wise mutual information to reduce the impact of noisy tokens. We apply InfoLoss to pretrain Llama 2-7B, obtaining C-Llama, and conduct experiments on 12 benchmarks to demonstrate its effectiveness.\n\nIndex Terms—Cross-lingual Learning, Pointwise Mutual Information (PMI), Hallucination, Large Language Models (LLMs)\n\nI. INTRODUCTION",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 540,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining_s0_c2",
    "source_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining",
    "text": "I. INTRODUCTION\n\nLarge language models (LLMs) like ChatGPT have shown powerful text-generating capabilities, but their performance in languages other than English, particularly Chinese, is unsatisfactory. Cross-lingual continual pretraining is an effective approach to build LLMs for other languages. However, noisy tokens in the dataset can cause severe hallucinations. We propose InfoLoss to mitigate these hallucinations during cross-lingual transfer learning.\n\nFig. 1. An example of hallucinations, where Llama 2 misinterprets two noisy tokens as photographers.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 565,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining_s0_c3",
    "source_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining",
    "text": "InfoLoss preprocesses the PMI of each token with others in the same sentence and weights the cross-entropy loss. This method mitigates the impact of noisy tokens, enhancing cross-lingual transfer ability. Our contributions include: 1) the proposal of InfoLoss for continually pretraining LLMs, 2) the first attempt to mitigate hallucinations in a cross-lingual transfer setting, and 3) extensive experiments on twelve benchmarks.\n\nII. METHODOLOGY\n\nA. Our InfoLoss",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 463,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining_s0_c4",
    "source_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining",
    "text": "II. METHODOLOGY\n\nA. Our InfoLoss\n\nGiven a text sequence Y with N tokens, the cross-entropy loss for training LLMs is modified to incorporate normalized PMI. We define a weight function W(yn) for the token yn, which aggregates the normalized PMI values with its neighboring tokens.\n\nW(yn) ≜ Σm∈N(yn) log p(yn, ym) / (p(yn)p(ym))\n\n---",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 332,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining_s0_c5",
    "source_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining",
    "text": "W(yn) ≜ Σm∈N(yn) log p(yn, ym) / (p(yn)p(ym))\n\n---\n\nIn this section, we describe the normalization of weights to ensure a smooth distribution over the vocabulary. The weight W(yn) for the current token yn is normalized using the sample mean µ(W) yn and the sample standard deviation σ(W) yn of its neighboring tokens N(yn):\n\nf W(yn) ≜ 1 + W(yn) − µ(W) yn / σ(W) yn",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 5,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 364,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining_s0_c6",
    "source_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining",
    "text": "f W(yn) ≜ 1 + W(yn) − µ(W) yn / σ(W) yn\n\nWe compare our method, InfoLoss, with the traditional cross-entropy loss for training Llama 2. Our approach aims to avoid learning incorrect language distributions caused by noisy tokens. The mean and standard deviation in the neighbors N(yn) are calculated as:\n\nµ(W) yn = Σm∈N(yn) W(ym) / |N(yn)|\nσ(W) yn = sqrt(Σm∈N(yn) [W(ym) − µ(W) yn]^2 / |N(yn)|)\n\nOur InfoLoss is defined as:\n\nℓInfoXE ≜ − Σn=1 f W(yn) log (exp(zyn) / Σv=1 exp(zv))",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 6,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 478,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining_s0_c7",
    "source_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining",
    "text": "ℓInfoXE ≜ − Σn=1 f W(yn) log (exp(zyn) / Σv=1 exp(zv))\n\nFor the pretraining dataset, we perform data mixture and deduplication. We sample an equal proportion of 15 billion English and Chinese tokens from RedPajama-V2 and MAP-CC, respectively. We use MinHash deduplication at the document level.\n\nOur evaluation tasks include multi-task Chinese understanding benchmarks, LLM hallucination evaluation benchmarks, and multi-task English understanding benchmarks. We employ the AdamW optimizer for training with specific hyperparameters and use average accuracy as our metric.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 7,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 572,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining_s0_c8",
    "source_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining",
    "text": "Experimental results show that cross-lingual continual pretraining significantly improves a model's Chinese understanding and generation ability. The proposed InfoLoss further enhances this improvement. The emergence phenomenon, indicating a model's intelligence level, occurs earlier in C-Llama, demonstrating that InfoLoss facilitates quicker adaptation to the language distribution in cross-lingual transfer learning.\n\n---\n\nTABLE III: ACCURACY (%) OF C-LLAMA COMPARED WITH BASELINES ON MULTI-TASK ENGLISH UNDERSTANDING BENCHMARKS.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 8,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 533,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining_s0_c9",
    "source_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining",
    "text": "Model | MMLU | BBH | GPQA | TheoremQA\n--- | --- | --- | --- | ---\nGPT-4 [2] | - | 83.9 | 83.1 | 46.2\nGPT-3.5-Turbo [1] | - | 68.5 | 70.5 | 43.1\nChatGLM [20] | 6B | 36.9 | 4.75 | 23.5\nMPT [21] | 7B | 35.6 | 6.55 | 21.9\nFalcon [22] | 7B | 38.4 | 5.96 | 24.6\nLlama [3] | 7B | 35.1 | 7.08 | 23.2\nLlama 2 [4] | 7B | 45.7 | 4.49 | 25.1\nC-Llama (w/o InfoLoss) | 7B | 45.9 | 6.93 | 27.5\nC-Llama | 7B | 48.1 | 9.12 | 29.8",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 9,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 412,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining_s0_c10",
    "source_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining",
    "text": "KoLA-KC results show that C-Llama, compared to C-Llama (w/o InfoLoss), has fewer hallucinations and significantly reduced probability of generating noisy tokens. InfoLoss contributes to more accurate and reliable text, enhancing the truth and credibility of C-Llama. Evaluation on multi-task English understanding benchmarks demonstrates C-Llama's performance superiority, indicating InfoLoss does not compromise English knowledge understanding.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 10,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 445,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining_s0_c11",
    "source_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining",
    "text": "E. Case Study\nSelected cases in Fig. 4 illustrate C-Llama's higher probability of generating correct answers on Chinese questions and mitigating hallucinations in answering factual questions. Additionally, InfoLoss reduces English knowledge forgetfulness during cross-lingual continual pretraining.\n\nIV. RELATED WORK",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 11,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 316,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining_s0_c12",
    "source_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining",
    "text": "IV. RELATED WORK\n\nCross-Lingual Continual Pretraining addresses the limitation of LLMs [1, 2, 3, 4] in languages other than English. Models are designed for concurrent management of multiple languages. Continual pretraining has been applied to tailor LLMs for diverse tasks and languages, yet hallucinations remain an issue.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 12,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 324,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining_s0_c13",
    "source_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining",
    "text": "Benchmark Question & Options | C-Llama | C-Llama (w/o InfoLoss)\n--- | --- | ---\n(1) The budget that is not limited by existing expense items or amounts is _____. | 0.69 (A) | 0.17 (B)\n(5) Which statement about floating-point arithmetic is NOT true? | 0.81 (A) | 0.08 (B)\nGAOKAO\n(2) Which process did not undergo a chemical reaction? | 0.49 (A) | 0.54 (B)\n(3) Cities with more rainfall than Seattle? | 0.49 (A) | 0.74 (B)\nTruthfulQA\n(4) Who invented the light bulb? | 0.59 (A) | 0.61 (B)\n(6) Which physical theory never requires UV regularization? | 0.45 (A) | 0.12 (B)",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 13,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 568,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining_s0_c14",
    "source_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining",
    "text": "Fig. 4. Examples of C-Llama and C-Llama (w/o InfoLoss) outputs on benchmarks. Correct answers are indicated in cyan.\n\nLLM Hallucinations are addressed through various evaluation and mitigation methods, yet these are primarily focused on supervised fine-tuning. Our InfoLoss is a novel approach for mitigating hallucinations during continual pretraining.\n\nV. CONCLUSION",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 14,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 368,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining_s0_c15",
    "source_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining",
    "text": "V. CONCLUSION\n\nInfoLoss is an information-weighted continual pretraining loss that mitigates the impact of noisy tokens. C-Llama shows significant potential in cross-lingual transfer learning and hallucination mitigation. Future work will explore InfoLoss capabilities on larger models and in other languages.\n\nACKNOWLEDGMENTS\nThis work was supported by the National Nature Science Foundation of China under Grant 62076032 and the CCF-Zhipu Large Model Innovation Fund (NO. CCF-Zhipu202407).\n\n---",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 15,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 496,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining_s1_c0",
    "source_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining",
    "text": "---\n\n---\n\n[1] OpenAI, “Introducing ChatGPT,” https://openai.com/blog/ChatGPT, 2022.\n[2] OpenAI et al., “Gpt-4 technical report,” arXiv, 2024.\n[3] Hugo Touvron et al., “Llama: Open and efficient foundation language models,” arXiv, 2023.\n[4] Hugo Touvron et al., “Llama 2: Open foundation and fine-tuned chat models,” arXiv, 2023.\n[5] Albert Q Jiang et al., “Mistral 7b,” arXiv, 2023.\n[6] Gemma Team et al., “Gemma: Open models based on gemini research and technology,” arXiv, 2024.\n[7] Meryem M’hamdi et al., “Cross-lingual continual learning,” in ACL, 2023, pp. 3908–3943.",
    "section_title": "REFERENCES",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 16,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 10,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 572,
    "chunk_method": "hierarchical",
    "importance_weight": 0.44000000000000006,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining_s1_c1",
    "source_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining",
    "text": "[8] Zhicheng Wang et al., “Rehearsal-free continual language learning via efficient parameter isolation,” in ACL, 2023, pp. 10933–10946.\n[9] Prateek Yadav et al., “Exploring continual learning for code generation models,” in ACL, 2023, pp. 782–792.\n[10] Genta Winata et al., “Overcoming catastrophic forgetting in massively multilingual continual learning,” in ACL, 2023, pp. 768–777.\n[11] Haode Zhang et al., “Revisit few-shot intent classification with PLMs: Direct fine-tuning vs. continual pre-training,” in ACL, 2023, pp. 11105–11121.",
    "section_title": "REFERENCES",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 17,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 10,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 539,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining_s1_c2",
    "source_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining",
    "text": "[12] Zhenghao Lin et al., “Not all tokens are what you need for pretraining,” in NeuIPS, 2024.\n[13] Hongyi Zhang and Abulhair Saparov, “Noisy exemplars make large language models more robust: A domain-agnostic behavioral analysis,” in EMNLP, 2023, pp. 4560–4568.\n[14] Kushal Tirumala et al., “D4: Improving llm pretraining via document de-duplication and diversification,” in NeurIPS, 2023, pp. 53983–53995.\n[15] Tom Brown et al., “Language models are few-shot learners,” NeurIPS, 2020, pp. 1877–1901.\n[16] Yoav Levine et al., “PMI-masking: Principled masking of correlated spans,” in ICLR, 2021.",
    "section_title": "REFERENCES",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 18,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 10,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 596,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining_s1_c3",
    "source_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining",
    "text": "[17] Anqi Mao et al., “Cross-entropy loss functions: Theoretical analysis and applications,” in ICML, 2023, pp. 23803–23828.\n[18] Hanchuan Peng et al., “Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy,” TPAMI, 2005, pp. 1226–1238.\n[19] Xinrun Du et al., “Chinese tiny llm: Pretraining a Chinese-centric large language model,” arXiv, 2024.\n[20] Zhengxiao Du et al., “GLM: General language model pretraining with autoregressive blank infilling,” in ACL, 2022, pp. 320–335.",
    "section_title": "REFERENCES",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 19,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 10,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 530,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining_s1_c4",
    "source_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining",
    "text": "[21] MosaicML NLP, “Introducing mpt-7b: A new standard for open-source, commercially usable llms,” 2023.\n[22] Guilherme Penedo et al., “The refinedweb dataset for falcon llm: Outperforming curated corpora with web data only,” in NeurIPS, 2023, pp. 79155–79172.\n[23] Yuzhen Huang et al., “C-eval: A multi-level multi-discipline Chinese evaluation suite for foundation models,” in NeurIPS, 2023, pp. 62991–63010.\n[24] Wanjun Zhong et al., “AGIEval: A human-centric benchmark for evaluating foundation models,” in NAACL, 2024, pp. 2299–2314.",
    "section_title": "REFERENCES",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 20,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 10,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 538,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining_s1_c5",
    "source_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining",
    "text": "[25] Haonan Li et al., “CMMLU: Measuring massive multitask language understanding in Chinese,” in ACL, 2024, pp. 11260–11285.\n[26] Stephanie Lin et al., “TruthfulQA: Measuring how models mimic human falsehoods,” in ACL, 2022, pp. 3214–3252.\n[27] Dor Muhlgay et al., “Generating benchmarks for factuality evaluation of language models,” in EACL, 2024.\n[28] Junyi Li et al., “HaluEval: A large-scale hallucination evaluation benchmark for large language models,” in ACL, 2023, pp. 6449–6464.\n[29] Jifan Yu et al., “KoLA: Carefully benchmarking world knowledge of large language models,” in ICLR, 2024.",
    "section_title": "REFERENCES",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 21,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 10,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining_s1_c6",
    "source_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining",
    "text": "[30] Dan Hendrycks et al., “Measuring massive multitask language understanding,” ICLR, 2021.\n[31] Dan Hendrycks et al., “Aligning ai with shared human values,” ICLR, 2021.\n[32] Mirac Suzgun et al., “Challenging BIG-bench tasks and whether chain-of-thought can solve them,” in ACL, 2023, pp. 13003–13051.\n[33] David Rein et al., “GPQA: A graduate-level google-proof q&a benchmark,” in COLM, 2024.\n[34] Wenhu Chen et al., “TheoremQA: A theorem-driven question answering dataset,” in EMNLP, 2023.\n[35] Ilya Loshchilov and Frank Hutter, “Decoupled weight decay regularization,” in ICLR, 2019.",
    "section_title": "REFERENCES",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 22,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 10,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 588,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining_s1_c7",
    "source_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining",
    "text": "[36] Jason Wei et al., “Emergent abilities of large language models,” TMLR, 2022.\n[37] Zixuan Ke et al., “Continual pre-training of language models,” in ICLR, 2023.\n[38] Yifu Qiu et al., “Detecting and mitigating hallucinations in multilingual summarization,” in EMNLP, 2023, pp. 8914–8932.\n[39] Vaibhav Adlakha et al., “Evaluating correctness and faithfulness of instruction-following models for question answering,” TACL, 2024, pp. 681–699.\n[40] Tianyu Liu et al., “A token-level reference-free hallucination detection benchmark for free-form text generation,” in ACL, 2022, pp. 6723–6737.",
    "section_title": "REFERENCES",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 23,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 10,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 591,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining_s1_c8",
    "source_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining",
    "text": "[41] Jungo Kasai et al., “Realtime qa: What's the answer right now?,” in NeurIPS, 2023, pp. 49025–49043.\n[42] Potsawee Manakul et al., “SelfCheckGPT: Zero-resource black-box hallucination detection for generative large language models,” in EMNLP, 2023, pp. 9004–9017.\n[43] Sebastian Farquhar et al., “Detecting hallucinations in large language models using semantic entropy,” Nature, 2024, pp. 625–630.\n[44] Dongjie Yang et al., “RefGPT: Dialogue generation of GPT, by GPT, and for GPT,” in EMNLP, 2023, pp. 2511–2535.",
    "section_title": "REFERENCES",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 24,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 10,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 518,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining_s1_c9",
    "source_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining",
    "text": "[45] Mohamed Elaraby et al., “Halo: Estimation and reduction of hallucinations in open-source weak large language models,” arXiv, 2023.\n\n---",
    "section_title": "REFERENCES",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 25,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 10,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 140,
    "chunk_method": "hierarchical",
    "importance_weight": 0.42000000000000004,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students_s0_c0",
    "source_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students",
    "text": "Designing a Japanese Idiom Education Support System for Overseas’ Students\n\nKONISHI Yusuke, Ruifan LI, Fuji REN\n\nGraduate School of Advanced Technology and Science, The University of Tokushima, Japan\nInstitute of Technology and Science, The University of Tokushima, Japan\nSchool of Computer Science and Technology, Beijing University of Posts and Telecommunications, China",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 5,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 372,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students_s1_c0",
    "source_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students",
    "text": "The Japanese idioms, combinations of two or more words with meanings distinct from the original words, pose a challenge for language learners. Our Japanese Idiom Education Support System aims to foster learner interest and teach proper idiom usage. It features idiom retrieval and teaching functions, allowing learners to input idioms or situations for retrieval and explanation. This paper presents",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 5,
    "chunk_index": 1,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 2,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 399,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9900000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students_s1_c1",
    "source_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students",
    "text": "or retrieval and explanation. This paper presents the system outline, retrieval method, and questionnaire results.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 5,
    "chunk_index": 2,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 2,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 114,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9450000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students_s2_c0",
    "source_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students",
    "text": "The global advancement of borderless society, economy, and circumstances has led to increased cultural exchanges and a rise in international students in Japan. Despite the growth in Japanese language education abroad, there is a shortage of teachers and a need for improved learning methods that emphasize understanding of culture and manners. Current educational support systems lack flexibility and dynamic correction, leading us to develop a more effective system.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 3,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 14,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 467,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students_s2_c1",
    "source_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students",
    "text": "2 The Outline of Japanese Idiom\nJapanese idioms are phrases with meanings not deducible from the original words. They are integral to daily conversation and can express complex feelings. Learners benefit from understanding idioms, which can convey meaning without explicit explanation. The system helps learners navigate the complexities of Japanese idioms, which often differ from English idioms with similar expressions.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 4,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 422,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students_s2_c2",
    "source_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students",
    "text": "3 System Configuration\nThis section outlines the Japanese Idiom Education Support System for Overseas Students. The system includes modules, a knowledge base, and a database. User input and system output are categorized as \"For retrieving idiom\" and \"For learning idiom.\" Input methods include search words, conversation sentences with MS Agent, and various operations. Outputs include idiom explanations, search assistance, teaching exercises, and learner history management.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 5,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 476,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students_s2_c3",
    "source_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students",
    "text": "The system incorporates three main functions: idiom retrieval, idiom teaching, and the MS Agent for operational assistance. Idiom retrieval occurs through direct input or by searching with a part of the idiom or related words, and includes reverse resolution where a learner inputs a situation to find a suitable idiom. The input sentence is analyzed using the Super Function from machine translation, and the system provides meanings, examples, similar idioms, and usage scenarios as search results. MS Agents present the example usage through dialogue.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 6,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 554,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students_s2_c4",
    "source_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students",
    "text": "The idiom teaching function utilizes animation and includes exams to track learning progress, displayed as a graph. Exercise problems are automatically generated based on prepared text, and the system generates idiomatic examples and MS Agent responses accordingly. MS Agent engages in easy conversation with learners using idioms.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 7,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 331,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students_s2_c5",
    "source_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students",
    "text": "The idiom retrieval function is processed by the \"Idiom retrieval module\" and the \"Idiom reverse resolution module.\" It offers two methods: displaying explanations when an idiom is input, or showing idioms corresponding to a described situation. The system uses ambiguous retrieval and supports the learner with MS Agents.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 8,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 322,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students_s2_c6",
    "source_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students",
    "text": "4.1 Retrieving a meaning of idiom\nLearners can search for idioms directly, by inputting a word within an idiom, or by selecting from a list displayed when characters are entered. The system checks IME and offers ambiguous retrieval to prevent issues from misspellings. It displays explanations or a list of idioms based on the input.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 9,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 333,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students_s2_c7",
    "source_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students",
    "text": "4.2 Retrieving an idiom by a situation\nThe Idiom reverse resolution module allows learners to input a situation, which the system analyzes using the Super Function. This function relates to the connection between original and target language sentences, utilizing direction graphs and transformation tables. The input sentence undergoes morphological analysis, preprocessing, classification by meaning, and Super Function classification to retrieve idioms from the edge table of the Super Function Database. The system then displays a list of idioms.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 10,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 549,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students_s2_c8",
    "source_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students",
    "text": "The system leverages the edge and node tables of the Super Function Database to generate exercise problem sentences. This integration allows for simplified input sentence analysis and problem generation through a unified database and methodology. We utilize Super Function for this purpose, registering words into the edge table as shown in Table 1. The created node and edge tables from user input sentences are preserved to analyze learners' input and extend the Super Function Database, aiding future system enhancements. However, the method using Super Function has limitations, including the sy",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 11,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students_s2_c9",
    "source_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students",
    "text": "ents. However, the method using Super Function has limitations, including the system's inability to respond to input sentences not present in the database and the exponential increase in the number of Super Functions with the length of the text, necessitating an adequate number of Super Functions for analysis and improvements in methodology.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 12,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 343,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students_s2_c10",
    "source_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students",
    "text": "5 Teaching Idiom Function\n\nThe teaching idiom function is managed by \"Education guidance module,\" \"Animation management module,\" \"Exercise problems module,\" and \"Record management module,\" as depicted in Figure 1. This function encompasses methods such as explaining Japanese idioms, generating exercise problems, and user management.\n\n5.1 Explanation of a Japanese Idiom",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 13,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 371,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students_s2_c11",
    "source_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students",
    "text": "5.1 Explanation of a Japanese Idiom\n\nThe idiom teaching function utilizes animation to maintain learner interest, as confirmed by questionnaire results. Figure 5 illustrates an example of teaching an idiom using animation, with TVML employed for explanation and TVML Player for animation display. TVML simplifies animation creation and modification.\n\n5.2 Exercise Problems for Japanese Idioms",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 14,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 392,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students_s2_c12",
    "source_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students",
    "text": "5.2 Exercise Problems for Japanese Idioms\n\nProblems in Japanese exercises are answered through methods such as selecting the correct answer, filling in blanks, connecting divided sentences, and inputting answers. The system adjusts problem difficulty and manages results as historical data, automatically creating problem sentences using Idiom DB and Super Function DB, as exemplified in Figure 6.\n\n6 Development of the Questionnaire System",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 15,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 440,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students_s2_c13",
    "source_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students",
    "text": "6 Development of the Questionnaire System\n\nA survey on attitudes towards educational support systems was conducted. The questionnaire targeted first and second-year Japanese language students, with the results indicating a high demand for a Japanese language educational support system. Key findings highlighted the need for animations and dictionary functions within the system.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 16,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 379,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students_s3_c0",
    "source_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students",
    "text": "This paper discussed the design of a Japanese Idiom Education Support System, referencing Super Function for input sentence analysis and idiom retrieval. The effectiveness of Super Function in this system requires further verification. Future work involves expanding the Super Function Database and animation content. The questionnaire results emphasized the importance of animation and a dictionary function, with the system currently using TVML for animation. Enhancing the system to display explan",
    "section_title": "Conclusions",
    "section_type": "conclusion",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 17,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 3,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 500,
    "chunk_method": "hierarchical",
    "importance_weight": 0.935,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students_s3_c1",
    "source_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students",
    "text": "g TVML for animation. Enhancing the system to display explanations in other languages will require the creation of a bilingual dictionary.",
    "section_title": "Conclusions",
    "section_type": "conclusion",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 18,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 138,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students_s3_c2",
    "source_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students",
    "text": "---\n\nAcknowledgements\n\nThis work was supported in part by the National Science Foundation of China under grant Nos. 60873001, 2007BAH05B02-04, and 2007BAH05B02-04.",
    "section_title": "Conclusions",
    "section_type": "conclusion",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 19,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 163,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8925,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students_s4_c0",
    "source_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students",
    "text": "[1] Statistics Bureau and the Director-General for Policy Planning. Statistics of Japan which sees in graph. http://www.stat.go.jp/data/nihon/pdf/ngraph.pdf\n[2] Murano Ryoko. Japanese Language Volunteers in Thai Secondary Schools. The Research Center for Japanese Language Education annual bulletin 9, pp. 1-8, 2000.\n[3] The Japan Foundation. 2006 Survey of Overseas Organizations Involved in Japanese-Language Education. The Japan Foundation, 2006.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 20,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 4,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 449,
    "chunk_method": "hierarchical",
    "importance_weight": 0.44000000000000006,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students_s4_c1",
    "source_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students",
    "text": "[4] Ishida Koji. Development of the science e-learning teaching materials which utilized student knowledge. Modern educational needs measure support program in the Heisei 16 fiscal year.\n[5] SAITO Koichi and TAKAHASHI Satoshi. A Preliminary Study on Modeling for Making Concerning Cause Belonging of \"Losing Interest in Science\"- Based on the Consideration Investigation of the High School Student. Journal of Tokyo University of Information Sciences Vol.9 No.1, pp. 1-9, 2005.\n[6] Ren Fuji. Super-function based machine translation. Communications of COLIPS, pp. 83-100, 1999.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 21,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 4,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 577,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students_s4_c2",
    "source_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students",
    "text": "[7] Matsumoto Yuji. Japanese Morphological Analysis System Chasen. Information Processing Society of Japan.\n[8] NHK Science and Technical Research Laboratories. TVML. http://www.nhk.or.jp/strl/tvml/english/player2/index.html\n[9] Suzuki Makoto and Nozaki Yukiko. How is elementary science education advanced in many foreign countries? Chemistry & education, Vol.56, No.12, Page. 638-641, 2008.\n[10] ANDO Tadashi. The Advantages and Problems in e-Learning -in the Case of Phonetic Education by Using ALC Net Academy-. Nagoya University of Arts and Sciences, the journal of liberal arts, 2005.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 22,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 4,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 590,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students_s4_c3",
    "source_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students",
    "text": "[11] An Open-Source Large Vocabulary CSR Engine Julius. http://julius.sourceforge.jp/en_index.php\n[12] Fine Speech. http://www.animo.co.jp/index.jsp\n[13] Yin Chengjiu, Ogata Hiroaki, and Yano Yoneo. Participatory Simulation for Collaborative Learning Experiences. Information Science Publishing, New York, USA, Oct. 2008.\n\n---",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 23,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 4,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 326,
    "chunk_method": "hierarchical",
    "importance_weight": 0.42000000000000004,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Dimensionality_reduction_for_text_using_LLE_s0_c0",
    "source_id": "Dimensionality_reduction_for_text_using_LLE",
    "text": "Dimensionality Reduction for Text Using LLE\n\nChuan HE, Zhe DONG, Ruifan LI, Yixin ZHONG\nSchool of Information Engineering, Beijing University of Posts and Telecommunications, Beijing, China\nzyx@bupt.edu.cn\nAbstract:",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 2,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 215,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Dimensionality_reduction_for_text_using_LLE_s0_c1",
    "source_id": "Dimensionality_reduction_for_text_using_LLE",
    "text": "Dimensionality reduction is essential in various information processing fields. Locally linear embedding (LLE) is an effective manifold learning algorithm that computes low-dimensional embeddings preserving local neighborhood properties. Although LLE has been primarily used on image data, it is applicable to text processing to address the curse of dimensionality. This paper introduces LLE, analyzes its advantages and limitations, discusses its relationship with latent semantic indexing (LSI) within the graph embedding framework, and presents experimental results using Reuters21578 and TDT2 da\nding framework, and presents experimental results using Reuters21578 and TDT2 datasets.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 2,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 687,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Dimensionality_reduction_for_text_using_LLE_s1_c0",
    "source_id": "Dimensionality_reduction_for_text_using_LLE",
    "text": "1.1 Dimensionality Reduction\nThe need for dimensionality reduction arises in numerous applications, including pattern recognition, text processing, and data visualization. High-dimensional data, such as images and documents, pose challenges due to algorithm performance limitations and hardware constraints. Existing methods include feature extraction and feature selection. Principal component analysis (PCA) is a widely used linear technique, while manifold learning methods like LLE offer alternative approaches.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 2,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 15,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 515,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Dimensionality_reduction_for_text_using_LLE_s1_c1",
    "source_id": "Dimensionality_reduction_for_text_using_LLE",
    "text": "1.2 Text Dimensionality Reduction\nTextual data typically exhibit high dimensionality, with the \"bag of words\" model commonly used for representation. Techniques such as stemming, lemmatizing, and Document-Frequencies filtering are preprocessing steps for dimensionality reduction. Feature selection and extraction methods, including document frequencies, chi-square, and information gain, play crucial roles. Algorithms like latent semantic analysis (LSA) and locality preserving projection (LPP) are also applied in text dimensionality reduction.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 3,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 15,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 547,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Dimensionality_reduction_for_text_using_LLE_s1_c2",
    "source_id": "Dimensionality_reduction_for_text_using_LLE",
    "text": "1.3 Arrangement\nThis section provides an overview of text dimensionality reduction methods. Subsequent sections will discuss the LLE algorithm's motivation, main idea, computational aspects, and compare it with graph embedding approaches.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 4,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 15,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 238,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Dimensionality_reduction_for_text_using_LLE_s1_c3",
    "source_id": "Dimensionality_reduction_for_text_using_LLE",
    "text": "In this section, we discuss the Locally Linear Embedding (LLE) algorithm, its motivation, and its main idea. LLE assumes that data are sampled in a low-dimensional nonlinear manifold embedded in a high-dimensional space. The embedding preserves the local configuration of nearest neighbors, ensuring that nearby points in the high-dimensional space remain nearby in the low-dimensional space. LLE uses locally linear reconstruction, where each point is represented by its neighbors, determined by the neighboring graph. The algorithm computes the reconstruction weights that store the local manifold\nThe algorithm computes the reconstruction weights that store the local manifold information.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 5,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 15,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 692,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Dimensionality_reduction_for_text_using_LLE_s1_c4",
    "source_id": "Dimensionality_reduction_for_text_using_LLE",
    "text": "The LLE algorithm formalization involves three steps: constructing the neighborhood graph, minimizing the reconstruction errors with respect to the reconstruction weights, and minimizing the reconstruction errors in the new low-dimensional space. The reconstruction weights connect the high-dimensional and low-dimensional spaces.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 6,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 15,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 330,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Dimensionality_reduction_for_text_using_LLE_s1_c5",
    "source_id": "Dimensionality_reduction_for_text_using_LLE",
    "text": "We also discuss some drawbacks of LLE, such as the sampling assumption and the use of the Euclidean metric, which may not always capture the true manifold structure. Additionally, the constraint of equalizing reconstruction points to the raw point can lead to suboptimal solutions.\n\nFinally, we categorize dimensionality reduction methods into linear and nonlinear approaches, and analyze LLE from these perspectives. Linear methods include PCA, ICA, and LDA, while nonlinear methods involve kernelized linear approaches and others.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 7,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 15,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 532,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Dimensionality_reduction_for_text_using_LLE_s1_c6",
    "source_id": "Dimensionality_reduction_for_text_using_LLE",
    "text": "Manifold learning methods, such as kernel PCA, map data to nonlinear spaces using kernel functions. These methods assume that high-dimensional data is sampled from a lower-dimensional nonlinear manifold. Locally linear embedding (LLE) and Isomap are examples of such approaches. LLE is a nonlinear, local, and unsupervised method that reconstructs points using only their neighboring points, offering fast computation. LLE's nonlinearity is based on the assumption that data are sampled from an underlying nonlinear manifold.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 8,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 15,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 525,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Dimensionality_reduction_for_text_using_LLE_s1_c7",
    "source_id": "Dimensionality_reduction_for_text_using_LLE",
    "text": "Graph embedding provides a dimensionality reduction framework, with LLE being one of its specialized versions. In this framework, a sparse graph is derived from the neighboring relationship, and a weight matrix W is constructed. The key step is to compute the projected points using an optimization formula that involves the graph Laplacian.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 9,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 15,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 341,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Dimensionality_reduction_for_text_using_LLE_s1_c8",
    "source_id": "Dimensionality_reduction_for_text_using_LLE",
    "text": "Experiments on the Reuters21578 and TDT2 datasets compared text representations in the original space, LLE, and LSI algorithms. For the Reuters21578 dataset, document subsets were vectorized after preprocessing, and the number of topics and documents varied. K-Nearest-Neighbor was used for classification. The results showed that precisions in the LLE-transformed space were generally higher than in the original or LSI-transformed spaces, with significantly lower dimensionalities in the LLE space.\n\n---",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 10,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 15,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 505,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Dimensionality_reduction_for_text_using_LLE_s1_c9",
    "source_id": "Dimensionality_reduction_for_text_using_LLE",
    "text": "Subset #Topics #Docs Baseline LSI LEE \nPrecisions Dims Precisions Dims Precisions \n1 6 539 86.06 529 86.99 19 91.28 \n2 6 469 77.39 467 78.46 24 91.05 \n3 5 535 91.43 527 91.24 15 95.34 \n4 3 736 89.53 728 90.48 29 95.11 \n5 6 764 70.03 755 71.07 44 86.51 \n6 6 901 83.35 881 84.32 47 93.90 \n7 4 817 89.59 802 89.59 22 92.04 \n8 7 882 89.02 867 90.12 24 94.45 \n9 11 751 68.99 745 69.79 23 84.30 \n10 5 914 82.39 895 84.14 56 92.57 \n11 6 969 88.24 956 88.44 25 95.98 \n12 5 858 76.80 846 77.03 37 83.56 \n13 5 506 77.86 498 78.06 18 90.51 \n14 5 600 83.83 592 84.33 29 89.17 \n15 3 712 86.09 695 86.51 9 98.17",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 11,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 15,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 597,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Dimensionality_reduction_for_text_using_LLE_s1_c10",
    "source_id": "Dimensionality_reduction_for_text_using_LLE",
    "text": "14 5 600 83.83 592 84.33 29 89.17 \n15 3 712 86.09 695 86.51 9 98.17 \n16 6 621 85.51 615 85.67 41 90.18 \n17 3 940 94.15 923 94.26 68 98.83 \n18 6 791 78.25 780 78.88 22 90.14 \n19 3 665 96.09 653 96.09 5 99.85 \n20 6 707 75.11 697 77.09 20 94.49",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 12,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 15,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 241,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Dimensionality_reduction_for_text_using_LLE_s1_c11",
    "source_id": "Dimensionality_reduction_for_text_using_LLE",
    "text": "4.2 Evaluation on TDT2\n4.2.1 Experimental Preparation\nAfter preprocessing similar to that of Reuters21578, the TDT2 dataset contains 9394 documents across 30 topics. We randomly sample from this collection, obtaining 20 subsets with varying numbers of documents (469 to 969) and topics (3 to 11).",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 13,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 15,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 296,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Dimensionality_reduction_for_text_using_LLE_s1_c12",
    "source_id": "Dimensionality_reduction_for_text_using_LLE",
    "text": "4.2.2 Experimental Design and Results\nThe experiments on TDT2 are designed analogously to those on Reuters21578. Table 2 and Figure 5 present the results for specific subsets and the entire collection, respectively. The LEE algorithm achieves the highest precision over the other two methods in all subsets and on average. Additionally, the precisions here are higher than those on Reuters21578, possibly due to TDT2's better-separated collection.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 14,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 15,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 447,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Dimensionality_reduction_for_text_using_LLE_s1_c13",
    "source_id": "Dimensionality_reduction_for_text_using_LLE",
    "text": "5. Discussion and Conclusion\nThe first three sections discuss LLE and text dimensionality reduction theoretically, and Section 4 experimentally demonstrates the effectiveness of the LLE algorithm for text dimensionality reduction. In terms of classification precision, LLE significantly outperforms the other methods. Further exploration is needed in several areas:\n- The computation time of LLE is less than LSI when using K-Nearest-Neighbor, though unproved.\n- The reduced dimensionality of LLE is much less than LSI, with the former allowing adjustable dimensions for optimal precision.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 15,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 15,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 589,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Dimensionality_reduction_for_text_using_LLE_s1_c14",
    "source_id": "Dimensionality_reduction_for_text_using_LLE",
    "text": "- The experiments show that the reduced dimensionality of LLE ranges from 5 to 68, suggesting the potential intrinsic dimensionality of the text structure.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 16,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 15,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 155,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Dimensionality_reduction_for_text_using_LLE_s2_c0",
    "source_id": "Dimensionality_reduction_for_text_using_LLE",
    "text": "[1] Belkin M., and Niyogi P., Laplacian Eigenmaps for Dimensionality Reduction and Data Representation, Neural Computation, Volume 15, 2003\n[2] Bellman, R. E., Adaptive control Processes, Princeton University Press, Princeton, NJ. 1961.\n[3] Bishop C., Machine Learning and Pattern Recognition, Cambridge University Press, 2006\n[4] Cai D. and He X., Orthogonal Locality Preserving Indexing, Proc. of the 28th International ACM SIGIR, 2005\n[5] Cai D., He X. and Han J., Document Clustering Using Locality Preserving Indexing, IEEE Transaction on Knowledge Discovery Engineering, Volume 17, 2005",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 17,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 6,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 592,
    "chunk_method": "hierarchical",
    "importance_weight": 0.44000000000000006,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Dimensionality_reduction_for_text_using_LLE_s2_c1",
    "source_id": "Dimensionality_reduction_for_text_using_LLE",
    "text": "[6] Cai D., He X., Hu Y., Han J. and Thomas H., Learning a Spatially Smooth Subspace for Face Recognition, Proc. 2007 IEEE International Conference on Computer Vision and Pattern Recognition, 2007\n[7] Deerwester S., Dumais S. and Harshman R., Indexing by Latent Semantic Analysis, Journal of the American Society for Information Science, 1999\n[8] Duda O. R. and Hart E. P., Pattern Classification, 2nd edition, Wiley-Interscience, 2000\n[9] Forman G., An Extensive Empirical Study of Feature Selection Metrics for Text Classification, Journal of Machine Learning Research, Volume 3, 2003",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 18,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 586,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Dimensionality_reduction_for_text_using_LLE_s2_c2",
    "source_id": "Dimensionality_reduction_for_text_using_LLE",
    "text": "[10] Golub H. G. and Charles F. Van Loan, Matrix Computations, 3rd edition, Johns Hopkins University Press, 1996\n[11] He X. and Niyogi P., Locality Preserving Projections, Advances in Neural Information Processing Systems (NIPS) 15, 2003\n[12] He X., Cai D. Liu H. and Ma W., Locality Preserving Indexing for Document Representation, Proc. of 27th International ACM SIGIR, 2004\n[13] Jolliffe I. T., Principal Component Analysis, 2nd edition, Springer, NY, 2002",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 19,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 459,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Dimensionality_reduction_for_text_using_LLE_s2_c3",
    "source_id": "Dimensionality_reduction_for_text_using_LLE",
    "text": "[14] Kouropteva O., Okun O., Hadid A., Soriano M., Marcos S. and Pietikainen M., Beyond Locally Linear Embedding Algorithm, Technical Report, MVG, 2002\n[15] Lewis D. D., Yang Y., Rose G. T. and Li F., RCV1: A New Benchmark Collection for Text Categorization Research, Journal of Machine Learning Research, Volume 5, 2004\n[16] Ridder de D., Kouropteva O., Okun O., Pietikainen M. and Duin R. P., Supervised Locally Linear Embedding, Technical Report, MVG, 2003\n[17] Rogati M. and Yang Y., High-Performing Feature Selection for Text Classification, Proc. of the International ACM CIKM, 2002",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 20,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 588,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Dimensionality_reduction_for_text_using_LLE_s2_c4",
    "source_id": "Dimensionality_reduction_for_text_using_LLE",
    "text": "[18] Roweis T. S. and Saul K. L., Nonlinear Dimensionality Reduction by Locally Linear Embedding, Science, Volume 290, 2000\n[19] Saul K. L. and Roweis T. S., Think Globally, Fit Locally: Unsupervised Learning of Low-dimensional Manifolds, Journal of Machine Learning Research, Volume 4, 2003\n[20] Scholkopf B., Smola, A. and Muller K. R., Nonlinear Component Analysis as a Kernel Eigenvalue Problem, Neural Computation, Volume 10, Page 1299-1319, 1998",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 21,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 451,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Dimensionality_reduction_for_text_using_LLE_s2_c5",
    "source_id": "Dimensionality_reduction_for_text_using_LLE",
    "text": "[21] Tenenbaum J. B. Vin de Silva, and Langford J. C., A Global Geometric Framework for Nonlinear Dimensionality Reduction, Science, Volume 290, 2000\n[22] Yang Y. and Pedersen J., A Comparative Study on Feature Selection in Text Categorization, Proc. of the 14th International Conference on Machine Learning, 1997\n[23] Yan S., Xu D., Zhang B. and Zhang H., Graph Embedding and Extension: a Framework for Dimensionality Reduction, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2007\n\n---",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 22,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 503,
    "chunk_method": "hierarchical",
    "importance_weight": 0.42000000000000004,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c0",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "DualGCN: Exploring Syntactic and Semantic Information for Aspect-Based Sentiment Analysis\n\nRuifan Li, Hao Chen, Fangxiang Feng, Zhanyu Ma, Xiaojie Wang, and Eduard Hovy",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 72,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 168,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c1",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "Abstract—Aspect-based sentiment analysis aims to identify sentiment polarities of given aspects in a sentence. Incorporating syntactic dependency structure with graph convolutional networks (GCNs) has shown advantages, but performance depends on dependency parsers. We propose DualGCN, which considers syntax structures and semantic correlations. DualGCN comprises four modules: SynGCN, SemGCN, Regularizers, and Mutual BiAffine. Experiments on multiple datasets demonstrate the effectiveness of our model against state-of-the-art approaches.\nI. INTRODUCTION",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 558,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c2",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "Sentiment analysis is a well-studied field, encompassing textual, audio, visual, and multimodal sentiment analysis. Textual sentiment analysis, particularly aspect-based sentiment analysis (ABSA), is challenging. ABSA determines sentiment polarities of aspects in a sentence, providing fine-grained analysis useful for tasks like recommendation and advertisement computation. Modeling dependencies between aspects and opinion expressions is crucial for ABSA. Previous studies have used attention mechanisms with RNNs but lack linguistic knowledge, leading to susceptibility to noise in sentences.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 596,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c3",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "Very recently, graph neural networks (GNNs) including graph convolutional networks (GCNs) and graph attention networks (GATs) have been used to capture the syntactic structure of sentences over dependency trees. However, applying syntactic dependency knowledge to Aspect-Based Sentiment Analysis (ABSA) tasks presents two challenges: 1) dependency parsing results can be inaccurate due to differing domains between training corpora and ABSA datasets, and 2) GCNs may not perform well on datasets insensitive to syntactic dependency due to informal expressions in online reviews. To address these cha",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c4",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "c dependency due to informal expressions in online reviews. To address these challenges, we propose a novel architecture, the dual graph convolution network (DualGCN).",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 167,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c5",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "For the first challenge, we use the probability matrix of dependency arcs from a dependency parser to build a syntax-based GCN (SynGCN). For the second, we construct a semantic correlation-based GCN (SemGCN) using a self-attention mechanism. We bridge the SynGCN and SemGCN modules with a BiAfﬁne module, inspired by DGEDT. We also design orthogonal and differential regularizers for the DualGCN model.\n\nOur main contributions are:",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 5,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 431,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c6",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "Our main contributions are:\n\n1) The DualGCN model for ABSA, integrating SynGCN and SemGCN through a mutual BiAfﬁne module.\n2) Orthogonal and differential regularizers to encourage distinct semantic and syntactic representations.\n3) Extensive experiments on the Restaurant14, Laptop14, and Twitter datasets, demonstrating the effectiveness of our DualGCN model.\n\nIn this article, we extend our previous work with:",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 6,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 412,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c7",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "In this article, we extend our previous work with:\n\n1) Posttraining (PT) BERT for domain adaptation.\n2) DualGCN with various pretrained language models (PLMs).\n3) Additional datasets for evaluation and an analysis of the impact of different dependency parsers.\n\nThe remainder of this article is organized as follows: Section II describes GCN and BERT preliminaries, Section III details our DualGCN model, Sections IV and V report experimental settings and results, Section VI reviews related works, and Section VII concludes and suggests future directions.\n\nII. PRELIMINARY",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 7,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 573,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c8",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "II. PRELIMINARY\n\nA. Graph Convolutional Network\n\nGCN, a variant of CNNs, efficiently captures nodes' information on graph-structured data. In NLP, GCN models extend to encode dependency trees and paths between words. Given a graph with n nodes, the adjacency matrix A represents the connections. The hidden state representation of each node is updated layer by layer.\n\nB. Bidirectional Encoder Representation From Transformers",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 8,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 426,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c9",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "B. Bidirectional Encoder Representation From Transformers\n\nBERT, based on the Transformer encoder, has shown effectiveness in various NLP tasks. It is a bidirectional language model that can be formulated to update hidden states in a layer-wise manner.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 9,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 252,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c10",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "In the BERT model, the symbol l represents the depth of transformer layers, h0 denotes the input representation incorporating token, position, and segment embeddings, LN refers to layer normalization, and MHAtt signifies multihead self-attention. The FFN consists of three layers: a linear projection layer, an activation layer, and another linear projection layer. BERT's vanilla version comprises 12 Transformer layers with 12 attention heads and is pretrained on large-scale corpora using two tasks: masked language modeling (MLM) and next sentence prediction (NSP). In MLM, 15% of tokens are man",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 10,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c11",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "modeling (MLM) and next sentence prediction (NSP). In MLM, 15% of tokens are manipulated, while in NSP, two sentences are concatenated to predict continuity.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 11,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 157,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c12",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "Our proposed DualGCN model, depicted in Fig. 3, addresses the ABSA task by processing sentence-aspect pairs (S, A). The model utilizes BiLSTM, BERT, and other pre-trained language model (PLM) encoders to obtain contextual representations. These are fed into the SynGCN and SemGCN modules, with a BiAffine module facilitating information flow. The final aspect representation is aggregated via pooling and concatenation, followed by a softmax classifier for sentiment polarity classification.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 12,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 491,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c13",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "The DualGCN's architecture includes the SynGCN and SemGCN modules, using dependency parser-generated probability matrices and self-attention-generated attention score matrices, respectively. D & O regularizers are designed to enhance semantic correlation capture.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 13,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 263,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c14",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "For contextual representation, we use BiLSTM and BERT encoders. BiLSTM inputs include word, part-of-speech (POS) tag, and position embeddings. Word embeddings E are obtained from an embedding lookup table, while POS tag embeddings T and position embeddings P are created and concatenated to form the final word representations X, which are processed by BiLSTM to produce hidden state vectors H.\n\nIn the syntax-based GCN, dependencies are expanded to subwords for compatibility with BERT's word-piece-based representations.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 14,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 522,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c15",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "The SynGCN module inputs syntactic encoding, utilizing the dependency arc probability matrix from the LAL-Parser to capture rich structural information. The hidden state vectors H from a BiLSTM serve as initial node representations in the syntactic graph, with the syntactic representation Hsyn obtained from the SynGCN module. The SemGCN module, in contrast, uses a self-attention mechanism to obtain an attention matrix Asem as the adjacency matrix, capturing semantic relationships. It applies multiple attention heads to generate a robust adjacency matrix and averages these to enhance semantic",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 15,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c16",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "ds to generate a robust adjacency matrix and averages these to enhance semantic graph representation.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 16,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 101,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c17",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "For aspect nodes, {hsyn a1, hsyn a2, ..., hsyn am} and {hsem a1, hsem a2, ..., hsem am} denote hidden representations in SynGCN and SemGCN, respectively. A mutual BiAffine transformation facilitates feature exchange between the two modules. The final feature representation r concatenates syntactic and semantic representations of all aspects, which is then fed into a linear layer followed by a softmax function to produce a sentiment probability distribution.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 17,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 461,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c18",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "To improve semantic representation, two regularizers are proposed: orthogonal and differential. The orthogonal regularizer encourages orthogonality among attention score vectors, while the differential regularizer ensures distinct information representation between SynGCN and SemGCN modules.\n\nThe training objective is to minimize the total loss ℓT, which includes the cross-entropy loss ℓC and regularization terms. Once trained, the DualGCN model can infer sentimental polarities for given sentences and aspects.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 18,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 515,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c19",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "In the experimental settings, we introduce datasets, baselines, evaluation metrics, and implementation details.\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 19,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 116,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c20",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "---\n\nTo evaluate our proposed model, we conduct experiments on two groups of benchmark datasets. The first group includes Restaurant14, Laptop14, and Twitter. We remove instances with the \"conflict\" label for fair comparison. The second group comprises Restaurant15 and Restaurant16. All datasets have three sentimental polarities: positive, neutral, and negative. Each sentence is annotated with marked aspects and their corresponding sentimental polarities. The statistics for the five datasets are summarized in Table I.\n\nB. Baseline Methods",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 20,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 544,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c21",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "B. Baseline Methods\n\nWe compare our DualGCN model with state-of-the-art baselines, grouped as follows: attention-based models, CNN-based models, GNN-based models, and BERT-based models. Attention-based models include ATAE-LSTM, MemNet, IAN, RAM, Inter-aspect, AOA, and MGAN. CNN-based models are GCAE and TNet. GNN-based models include ASGCN, CDT, BiGCN, kumaGCN, InterGCN, R-GAT, and DGEDT. BERT-based models are BERT, BERT-PT, R-GAT + BERT, DGEDT + BERT, and BERT-ADA.\n\nC. Evaluation Metrics",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 21,
    "chunk_index_in_section": 21,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 493,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c22",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "C. Evaluation Metrics\n\nWe use accuracy and macro-averaged F1-score as the main metrics. Accuracy is the fraction of correct predictions over total predictions. Macro F1-score is the mean of classwise F1-scores.\n\nD. Implementation Details\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 22,
    "chunk_index_in_section": 22,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 242,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c23",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "We use LAL-Parser for dependency parsing and initialize word embeddings with pretrained 300-D Glove vectors. The embeddings for position and POS tags are set to 30 dimensions. These are concatenated and fed into a BiLSTM model with a hidden size of 50. Hyperparameters du, de, dt, and dp are set to 50, 300, 30, and 30, respectively. To prevent overfitting, we apply dropout, with a rate of 0.7 for BiLSTM inputs, and 0.1 for SynGCN and SemGCN modules. We set the number of SynGCN and SemGCN layers to 2 and initialize all model weights uniformly between -0.3 and 0.3. The DualGCN model is trained w",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 23,
    "chunk_index_in_section": 23,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c24",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "all model weights uniformly between -0.3 and 0.3. The DualGCN model is trained with the Adam optimizer at a learning rate of 0.002, for 50 epochs, using a batch size of 16. Regularization coefficients λ1 and λ2 are dataset-specific, and λ3 is set to 10−4. For the DualGCN + BERT model, we use the English BERT-base-uncased. The DualGCN + BETR-PT model uses BERT-PT, a model trained on Amazon and Yelp reviews.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 24,
    "chunk_index_in_section": 24,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 409,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c25",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "In our experiments, we report quantitative comparison results on two groups of datasets and qualitative results based on the first group. The DualGCN model consistently outperforms attention-based and syntax-based methods on Restaurant14, Laptop14, and Twitter datasets. It effectively integrates syntactic knowledge and semantic information, fitting various review styles. DualGCN + BERT and DualGCN + BERT-PT show improved performance over the basic DualGCN model and other BERT-based methods. Further experiments on Restaurant15 and Restaurant16 verify the robustness of our DualGCN model. We als",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 25,
    "chunk_index_in_section": 25,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c26",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "Restaurant15 and Restaurant16 verify the robustness of our DualGCN model. We also compare DualGCN with different pretrained language models, demonstrating competitive performance even with lightweight models like ALBERT and DistilBERT.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 26,
    "chunk_index_in_section": 26,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 235,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c27",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "To investigate the effectiveness of modules in the DualGCN model, we conducted ablation studies. The SynGCN-head model uses discrete outputs from a dependency parser for the adjacency matrix, while the SynGCN model uses the probability matrix. The SynGCN model outperforms SynGCN-head on the Restaurant14 and Laptop14 datasets, indicating that rich syntactic knowledge can mitigate dependency parsing errors. The SemGCN model, which uses a self-attention layer for the semantic graph adjacency matrix, outperforms SynGCN on the Twitter dataset, likely due to the informal nature of Twitter reviews.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 27,
    "chunk_index_in_section": 27,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c28",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "Removing the BiAfﬁne module (DualGCN w/o BiAfﬁne) or both orthogonal and differential regularizers (DualGCN w/o RO&RD) leads to performance degradation. The two regularizers encourage DualGCN to accurately capture semantic correlations, with the full module configuration achieving the best performance.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 28,
    "chunk_index_in_section": 28,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 303,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c29",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "In a case study, we analyzed sample cases using different models. Attention-based methods like ATAE-LSTM and IAN are prone to attend to noisy words. The SynGCN model fails in capturing the representation of key words in complex sentences, while the SemGCN model can attend to semantic correlations. DualGCN, considering both syntactic knowledge and semantic information, effectively handles complex and informal sentences.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 29,
    "chunk_index_in_section": 29,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 422,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c30",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "The impact of different parsers on GCN-based models was evaluated using SynGCN and DualGCN with four parsers. DualGCN generally achieves better performance due to the enhancement of semantic information by SemGCN.\n\nAttention visualization demonstrates that the two regularizers in DualGCN reduce redundancy and noise in the attention score matrix. The model can accurately predict sentiment polarity for aspects.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 30,
    "chunk_index_in_section": 30,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 412,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c31",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "The number of DualGCN layers was also studied, with the best performance achieved using two layers. Too few layers limit propagation, while too many can lead to instability and over-smoothing.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 31,
    "chunk_index_in_section": 31,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 192,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c32",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "Textual sentiment analysis is typically sentence- or document-level, whereas Aspect-Based Sentiment Analysis (ABSA) is entity-level and more fine-grained. Early methods relied on handcrafted features but failed to capture the dependency between aspects and context. Recent attention-based neural networks have addressed this by implicitly modeling the semantic relationship. For example, Wang et al. proposed attention-based LSTMs for aspect-level sentiment classification, while Chen et al. and Tang et al. introduced hierarchical attention networks. Fan et al. exploited a multigrained attention m",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 32,
    "chunk_index_in_section": 32,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c33",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "hierarchical attention networks. Fan et al. exploited a multigrained attention mechanism, and Tan et al. designed a dual attention network.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 33,
    "chunk_index_in_section": 33,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 139,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c34",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "Pretrained language models like BERT have also shown remarkable performance in ABSA tasks. Sun et al. transformed the ABSA task into sentence pair classification, and Xu et al. enhanced BERT's fine-tuning stage. Explicit leveraging of syntactic knowledge has become another trend, with models like recursive neural networks and frameworks using sentiment sentence compression models. Works have extended GCN and GAT models using syntactical dependency trees, such as Zhang et al.'s GCN over dependency trees and Wang et al.'s aspect-oriented dependency tree structure.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 34,
    "chunk_index_in_section": 34,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 568,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c35",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "In this article, we propose a DualGCN architecture that integrates syntactic knowledge through SynGCN and semantic information through SemGCN, enhanced by orthogonal and differential regularizers. Experiments show that our DualGCN model outperforms baselines. Future research could explore a trainable dependency parser module and the application of graph neural networks to the aspect–opinion–sentiment triplet extraction task.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 35,
    "chunk_index_in_section": 35,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 428,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c36",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "Li, J., & Hovy, E. (2017). Reflections on sentiment/opinion analysis. In E. Cambria, D. Das, S. Bandyopadhyay, & A. Feraco (Eds.), A Practical Guide to Sentiment Analysis (pp. 41–59). Cham, Switzerland: Springer.\n\nZhang, L., Wang, S., & Liu, B. (2018). Deep learning for sentiment analysis: A survey. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 8(4), e1253. [Online]. Available: https://wires.onlinelibrary.wiley.com/journal/19424795, doi: 10.1002/widm.1253.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 36,
    "chunk_index_in_section": 36,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 484,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c37",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "Brauwers, G., & Frasincar, F. (2021). A survey on aspect-based sentiment classification. ACM Comput. Surv. [Online]. Available: https://dl.acm.org/doi/10.1145/3503044, doi: 10.1145/3503044.\n\nStuhlsatz, A., Meyer, C., Eyben, F., Zielke, T., Meier, G., & Schuller, B. (2011). Deep neural networks for acoustic emotion recognition: Raising the benchmarks. In Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP) (pp. 5688–5691).",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 37,
    "chunk_index_in_section": 37,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 435,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c38",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "Tzirakis, P., Zhang, J., & Schuller, B. W. (2018). End-to-end speech emotion recognition using deep neural networks. In Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP) (pp. 5089–5093).\n\nFahad, M. S., Ranjan, A., Yadav, J., & Deepak, A. (2021). A survey of speech emotion recognition in natural environment. Digital Signal Process., 110, 102951.\n\nVadicamo, L., et al. (2017). Cross-media learning for image sentiment analysis in the wild. In Proc. IEEE Int. Conf. Comput. Vis. Workshops (ICCVW) (pp. 308–317).",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 38,
    "chunk_index_in_section": 38,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 523,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c39",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "Ortis, A., Farinella, G. M., & Battiato, S. (2020). Survey on visual sentiment analysis. IET Image Process., 14(8), 1440–1456.\n\nZhao, S., et al. (2021). Emotional semantics-preserved and feature-aligned CycleGAN for visual emotion adaptation. IEEE Trans. Cybern., 52(10), 1–14.\n\nSoleymani, M., Garcia, D., Jou, B., Schuller, B., Chang, S.-F., & Pantic, M. (2017). A survey of multimodal sentiment analysis. Image Vis. Comput., 65(1), 3–14.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 39,
    "chunk_index_in_section": 39,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 439,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c40",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "Stappen, L., Schuller, B., Lefter, I., Cambria, E., & Kompatsiaris, I. (2020). Summary of MuSe: Multimodal Sentiment Analysis, Emotion-Target Engagement and Trustworthiness Detection in Real-Life Media. New York, NY, USA: Association for Computing Machinery.\n\nMusto, C., Lops, P., de Gemmis, M., & Semeraro, G. (2019). Justifying recommendations through aspect-based sentiment analysis of users reviews. In Proc. 27th ACM Conf. User Model., Adaptation Personalization (pp. 4–12).",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 40,
    "chunk_index_in_section": 40,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 479,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c41",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "Huang, C., Jiang, W., Wu, J., & Wang, G. (2020). Personalized review recommendation based on Users’ aspect sentiment. ACM Trans. Internet Technol., 20(4), 1–26.\n\nLiu, P., Zhang, L., & Gulla, J. A. (2021). Multilingual review-aware deep recommender system via aspect-based sentiment analysis. ACM Trans. Inf. Syst., 39(2), 1–33.\n\nDragoni, M. (2017). A three-phase approach for exploiting opinion mining in computational advertising. IEEE Intell. Syst., 32(3), 21–27.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 41,
    "chunk_index_in_section": 41,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 465,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c42",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "Wang, Y., Huang, M., Zhu, X., & Zhao, L. (2016). Attention-based LSTM for aspect-level sentiment classification. In Proc. Conf. Empirical Methods Natural Lang. Process. (pp. 606–615).\n\nTang, D., Qin, B., & Liu, T. (2016). Aspect level sentiment classification with deep memory network. In Proc. Conf. Empirical Methods Natural Lang. Process. (pp. 214–224).\n\nMa, D., Li, S., Zhang, X., & Wang, H. (2017). Interactive attention networks for aspect-level sentiment classification. In Proc. 26th Int. Joint Conf. Artif. Intell. (pp. 4068–4074).",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 42,
    "chunk_index_in_section": 42,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 540,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c43",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "Chen, P., Sun, Z., Bing, L., & Yang, W. (2017). Recurrent attention network on memory for aspect sentiment analysis. In Proc. Conf. Empirical Methods Natural Lang. Process. (pp. 452–461).\n\nFan, F., Feng, Y., & Zhao, D. (2018). Multi-grained attention network for aspect-level sentiment classification. In Proc. Conf. Empirical Methods Natural Lang. Process. (pp. 3433–3442).\n\nHuang, B., Ou, Y., & Carley, K. M. (2018). Aspect level sentiment classification with attention-over-attention neural networks. In Proc. Social, Cultural, Behav. Model. 11th Int. Conf. (SBP-BRiMS) (pp. 197–206).",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 43,
    "chunk_index_in_section": 43,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 587,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c44",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "[23] S. Gu, L. Zhang, Y. Hou, and Y. Song, “A position-aware bidirectional attention network for aspect-level sentiment analysis,” in Proc. 27th Int. Conf. Comput. Linguistics. Santa Fe, NM, USA: Association for Computational Linguistics, Aug. 2018, pp. 774–784. [24] N. Jiang, F. Tian, J. Li, X. Yuan, and J. Zheng, “MAN: Mutual attention neural networks model for aspect-level sentiment classiﬁcation in SIoT,” IEEE Internet Things J., vol. 7, no. 4, pp. 2901–2913, Apr. 2020. [25] P. Lin, M. Yang, and J. Lai, “Deep selective memory network with selective attention and inter-aspect modeling for",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 44,
    "chunk_index_in_section": 44,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c45",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "selective memory network with selective attention and inter-aspect modeling for aspect level sentiment classiﬁcation,” IEEE/ACM Trans. Audio, Speech, Language Process., vol. 29, pp. 1093–1106, 2021. [26] J. Zhou et al., “Graph neural networks: A review of methods and applications,” AI Open, vol. 1, pp. 57–81, Jan. 2020. [27] C. Zhang, Q. Li, and D. Song, “Aspect-based sentiment classiﬁcation with aspect-speciﬁc graph convolutional networks,” in Proc. Conf. Empirical Methods Natural Lang. Process. 9th Int. Joint Conf. Natural Lang. Process. (EMNLP-IJCNLP). Hong Kong: Association for Com- putati",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 45,
    "chunk_index_in_section": 45,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 600,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c46",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "f. Natural Lang. Process. (EMNLP-IJCNLP). Hong Kong: Association for Com- putational Linguistics, Nov. 2019, pp. 4568–4578. [28] K. Sun, R. Zhang, S. Mensah, Y. Mao, and X. Liu, “Aspect-level sentiment analysis via convolution over dependency tree,” in Proc. Conf. Empirical Methods Natural Lang. Process. 9th Int. Joint Conf. Natural Lang. Process. (EMNLP-IJCNLP). Hong Kong, China: Association for Computational Linguistics, 2019, pp. 5679–5688. [29] B. Huang and K. Carley, “Syntax-aware aspect level sentiment clas- siﬁcation with graph attention networks,” in Proc. Conf. Empirical Methods Natur",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 46,
    "chunk_index_in_section": 46,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 600,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c47",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "siﬁcation with graph attention networks,” in Proc. Conf. Empirical Methods Natural Lang. Process. 9th Int. Joint Conf. Natural Lang. Process. (EMNLP-IJCNLP). Hong Kong: Association for Computational Linguistics, Nov. 2019, pp. 5469–5477. [30] M. Zhang and T. Qian, “Convolution over hierarchical syntactic and lexical graphs for aspect level sentiment analysis,” in Proc. Conf. Empirical Methods Natural Lang. Process. (EMNLP). Stroudsburg, PA, USA: Association for Computational Linguistics, 2020, pp. 3540–3549. [31] C. Chen, Z. Teng, and Y. Zhang, “Inducing target-speciﬁc latent structures for as",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 47,
    "chunk_index_in_section": 47,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 600,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c48",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": ". Chen, Z. Teng, and Y. Zhang, “Inducing target-speciﬁc latent structures for aspect sentiment classiﬁcation,” in Proc. Conf. Empirical Methods Natural Lang. Process. (EMNLP). Stroudsburg, PA, USA: Association for Computational Linguistics, Nov. 2020, pp. 5596–5607. [32] B. Liang, R. Yin, L. Gui, J. Du, and R. Xu, “Jointly learning aspect- focused and inter-aspect relations with graph convolutional networks for aspect sentiment analysis,” in Proc. 28th Int. Conf. Comput. Linguistics. Barcelona, Spain: International Committee on Computational Linguis- tics, Dec. 2020, pp. 150–161. [33] K. Wang,",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 48,
    "chunk_index_in_section": 48,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 600,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c49",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "Committee on Computational Linguis- tics, Dec. 2020, pp. 150–161. [33] K. Wang, W. Shen, Y. Yang, X. Quan, and R. Wang, “Relational graph attention network for aspect-based sentiment analysis,” in Proc. 58th Annu. Meeting Assoc. Comput. Linguistics. Stroudsburg, PA, USA: Association for Computational Linguistics, Jul. 2020, pp. 3229–3238. [34] H. Tang, D. Ji, C. Li, and Q. Zhou, “Dependency graph enhanced dual- transformer structure for aspect-based sentiment classiﬁcation,” in Proc. 58th Annu. Meeting Assoc. Comput. Linguistics. Stroudsburg, PA, USA: Association for Computational Linguistics",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 49,
    "chunk_index_in_section": 49,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c50",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "ut. Linguistics. Stroudsburg, PA, USA: Association for Computational Linguistics, Jul. 2020, pp. 6578–6588. [35] R. Li, H. Chen, F. Feng, Z. Ma, X. Wang, and E. Hovy, “Dual graph convolutional networks for aspect-based sentiment analysis,” in Proc. 59th Annu. Meeting Assoc. Comput. Linguistics 11th Int. Joint Conf. Natural Lang. Process. Stroudsburg, PA, USA: Association for Computational Linguistics, 2021, pp. 6319–6329. [36] X. Zhang, C. Xu, X. Tian, and D. Tao, “Graph edge convolutional neural networks for skeleton-based action recognition,” IEEE Trans. Neural Netw. Learn. Syst., vol. 31, n",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 50,
    "chunk_index_in_section": 50,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 600,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c51",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "ton-based action recognition,” IEEE Trans. Neural Netw. Learn. Syst., vol. 31, no. 8, pp. 3047–3060, Aug. 2020. [37] Y. Xu, C. Han, J. Qin, X. Xu, G. Han, and S. He, “Transductive zero-shot action recognition via visually connected graph convolutional networks,” IEEE Trans. Neural Netw. Learn. Syst., vol. 32, no. 8, pp. 1–9, Aug. 2020. [38] W. Liu et al., “Item relationship graph neural networks for E-commerce,” IEEE Trans. Neural Netw. Learn. Syst., vol. 33, no. 9, pp. 1–15, Mar. 2021. [39] T. N. Kipf and M. Welling, “Semi-supervised classiﬁcation with graph convolutional networks,” in Proc.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 51,
    "chunk_index_in_section": 51,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c52",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "ng, “Semi-supervised classiﬁcation with graph convolutional networks,” in Proc. 5th Int. Conf. Learn. Represent. (ICLR), Toulon, France, Apr. 2017, pp. 1–14. [40] C. Lin, T. Miller, D. Dligach, S. Bethard, and G. Savova, “A BERT-based universal model for both within- and cross-sentence clinical temporal relation extraction,” in Proc. 2nd Clin. Natural Lang. Process. Workshop. Minneapolis, MN, USA: Association for Computational Linguistics, Jun. 2019, pp. 65–71. [41] Z. Wang, P. Ng, X. Ma, R. Nallapati, and B. Xiang, “Multi-passage BERT: A globally normalized BERT model for open-domain ques- ti",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 52,
    "chunk_index_in_section": 52,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 600,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c53",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": ", “Multi-passage BERT: A globally normalized BERT model for open-domain ques- tion answering,” in Proc. Conf. Empirical Methods Natural Lang. Process. 9th Int. Joint Conf. Natural Lang. Process. (EMNLP-IJCNLP). Hong Kong: Association for Computational Linguistics, Nov. 2019, pp. 5878–5882.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 53,
    "chunk_index_in_section": 53,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 290,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c54",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "Authorized licensed use limited to: BEIJING UNIVERSITY OF POST AND TELECOM. Downloaded on July 22,2025 at 08:15:36 UTC from IEEE Xplore. Restrictions apply.\n\nLI et al.: DualGCN: EXPLORING SYNTACTIC AND SEMANTIC INFORMATION 7655",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 54,
    "chunk_index_in_section": 54,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 227,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c55",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "LI et al.: DualGCN: EXPLORING SYNTACTIC AND SEMANTIC INFORMATION 7655\n\nReimers and Gurevych (2019) introduced Sentence-BERT, utilizing Siamese BERT-networks for sentence embeddings. Devlin et al. (2019) presented BERT, a pre-trained deep bidirectional transformer model for language understanding. Vaswani et al. (2017) proposed the attention mechanism as a sufficient model for neural network translation. Zhu et al. (2015) aligned books and movies to generate story-like visual explanations.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 55,
    "chunk_index_in_section": 55,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 493,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c56",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "Xu et al. (2019) applied BERT post-training for review reading comprehension and aspect-based sentiment analysis. Gururangan et al. (2020) emphasized the importance of adapting language models to domains and tasks. Mrini et al. (2019) explored interpretability in neural parsing by rethinking self-attention mechanisms.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 56,
    "chunk_index_in_section": 56,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 319,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c57",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "Pontiki et al. (2014, 2015, 2016) conducted several SemEval tasks on aspect-based sentiment analysis. Hazarika et al. (2018) modeled inter-aspect dependencies for aspect-based sentiment analysis. Xue and Li (2018) proposed gated convolutional networks for aspect-based sentiment analysis. Li et al. (2018) introduced transformation networks for target-oriented sentiment classification.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 57,
    "chunk_index_in_section": 57,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 386,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c58",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "Rietzler et al. (2020) demonstrated domain adaptation through BERT language model fine-tuning for aspect-target sentiment classification. Pennington et al. (2014) presented GloVe, a method for global vectors for word representation. Srivastava et al. (2014) discussed dropout as a technique to prevent overfitting in neural networks.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 58,
    "chunk_index_in_section": 58,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 333,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c59",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "Marcheggiani and Titov (2017) used graph convolutional networks for semantic role labeling. Lan et al. (2019) introduced ALBERT, a lighter BERT for self-supervised learning of language representations. Sanh et al. (2019) presented DistilBERT, a smaller and faster version of BERT. Liu et al. (2019) introduced RoBERTa, an optimized BERT pretraining approach.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 59,
    "chunk_index_in_section": 59,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 358,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c60",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "Titov and McDonald (2008) modeled online reviews with multi-grain topic models. Jiang et al. (2011) performed target-dependent Twitter sentiment classification. Kiritchenko et al. (2014) developed a system for detecting aspects and sentiment in customer reviews. Vo and Zhang (2015) applied deep learning to event-driven stock prediction.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 60,
    "chunk_index_in_section": 60,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 338,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c61",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "Li et al. (2018) proposed a hierarchical attention-based position-aware network for aspect-level sentiment analysis. Tan et al. (2019) recognized conflicting opinions in aspect-level sentiment classification with dual attention networks. Zhang et al. (2020) used convolutional multi-head self-attention on memory for aspect sentiment classification.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 61,
    "chunk_index_in_section": 61,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 349,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c62",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "Sun et al. (2019) utilized BERT for aspect-based sentiment analysis by constructing auxiliary sentences. Che et al. (2015) applied sentence compression for aspect-based sentiment analysis. He et al. (2018) developed effective attention models for aspect-level sentiment classification. Phan and Ogunbona (2020) modeled context and syntactical features for aspect-based sentiment analysis.\n\nZhang et al. (2020) proposed a knowledge guided capsule attention network for aspect-based sentiment analysis.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 62,
    "chunk_index_in_section": 62,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 500,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c63",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "Ruifan Li (Member, IEEE) received the B.S. degree in control systems from Huazhong University of Science and Technology, Wuhan, China, in 1998, the M.S. degree in circuits and systems in 2001, and the Ph.D. degree in signal and information processing from Beijing University of Posts and Telecommunications (BUPT), Beijing, China, in 2006. Since 2006, he has been with the School of Computer Science, BUPT, and since February 2011, he has been a Visiting Scholar at the Information Sciences Institute, University of Southern California, Los Angeles, CA, USA. He is currently an Associate Professor w",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 63,
    "chunk_index_in_section": 63,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c64",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "thern California, Los Angeles, CA, USA. He is currently an Associate Professor with the School of Artificial Intelligence, BUPT. His research activities include multimedia information processing, natural language processing, and statistical machine learning. Dr. Li is a member of the IEEE Signal Processing Society and the IEEE Computer Society.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 64,
    "chunk_index_in_section": 64,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 346,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c65",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "Hao Chen received the B.E. degree in network engineering from Hefei University, Hefei, China, in 2019, and the master’s degree in computer technology from the School of Artificial Intelligence, Beijing University of Posts and Telecommunications, Beijing, China, in June 2022. His research interests include natural language processing and deep learning.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 65,
    "chunk_index_in_section": 65,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 353,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c66",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "Fangxiang Feng received the B.S. and Ph.D. degrees from Beijing University of Posts and Telecommunications (BUPT), Beijing, China, in 2010 and 2015, respectively. He is currently an Assistant Professor with the School of Artificial Intelligence, BUPT. His research interests include multimedia information retrieval, multimodal deep learning, and computer vision.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 66,
    "chunk_index_in_section": 66,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 363,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c67",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "Zhanyu Ma (Senior Member, IEEE) received the Ph.D. degree in electrical engineering from KTH Royal Institute of Technology, Stockholm, Sweden, in 2011. He was a Post-Doctoral Research Fellow with the School of Electrical Engineering, KTH Royal Institute of Technology, from 2012 to 2013. Since 2014, he has been with Beijing University of Posts and Telecommunications, Beijing, China, where he is currently a Professor. His research interests include pattern recognition, machine learning fundamentals, with a focus on applications in computer vision and multimedia signal processing.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 67,
    "chunk_index_in_section": 67,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 584,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c68",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "Xiaojie Wang received the Ph.D. degree from Beihang University, Beijing, China, in 1996. He is currently a Full Professor and the Director of the Centre for Intelligence Science and Technology, Beijing University of Posts and Telecommunications, Beijing. His research interests include natural language processing and multimodal cognitive computing. Dr. Wang is an Executive Member of the Council of Chinese Association of Artificial Intelligence and the Director of the Natural Language Processing Committee. He is a member of the Council of Chinese Information Processing Society and the Chinese P",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 68,
    "chunk_index_in_section": 68,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c69",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "ember of the Council of Chinese Information Processing Society and the Chinese Processing Committee of China Computer Federation.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 69,
    "chunk_index_in_section": 69,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 129,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c70",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "Eduard Hovy received the Ph.D. degree in computer science from Yale University, New Haven, CT, USA, in 1987. He received honorary doctorates from the National University of Distance Education (UNED), Madrid, Spain, in 2013, and the University of Antwerp, Antwerp, Belgium, in 2015. He is currently a Research Professor with the Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA. He is one of the original fellows of the Association for Computational Linguistics (ACL) and has published more than 500 research articles. His research focuses on computational semantics o",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 70,
    "chunk_index_in_section": 70,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis_s0_c71",
    "source_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
    "text": "re than 500 research articles. His research focuses on computational semantics of human language. Dr. Hovy is a fellow of the Association for the Advancement of Artificial Intelligence (AAAI) and serves on the editorial boards of several journals, including ACM Transactions on Asian Language Information Processing (TALIP) and Language Resources and Evaluation (LRE).",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 71,
    "chunk_index_in_section": 71,
    "total_chunks_in_section": 72,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 368,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s0_c0",
    "source_id": "electronics_12_03521_v2",
    "text": "---\n\nA Visually Enhanced Neural Encoder for Synset Induction\n\nGuang Chen, Fangxiang Feng, Guangwei Zhang, Xiaoxu Li, and Ruifan Li\nAbstract:",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 3,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 140,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s0_c1",
    "source_id": "electronics_12_03521_v2",
    "text": "The synset induction task clusters semantically identical instances represented by texts and images. Prior works primarily focus on textual information, neglecting the visual aspects. This paper introduces a Visually Enhanced NeUral Encoder (VENUE) to learn multimodal representations for synset induction. VENUE captures intra-modal and inter-modal interactions among images and text through a visual interaction module, a textual multi-granularity embedding module, a masking module, and a gating module. We use a triplet loss to train the encoder for discriminative multimodal representations and",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s0_c2",
    "source_id": "electronics_12_03521_v2",
    "text": "plet loss to train the encoder for discriminative multimodal representations and apply clustering algorithms to induce synsets. Our method is validated on a new multimodal dataset, MMAI-Synset, and shows superior performance over strong baselines on various evaluation metrics.\nKeywords: multi-modality; deep learning; synset induction; clustering",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 347,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c0",
    "source_id": "electronics_12_03521_v2",
    "text": "The synset induction task is crucial in multimodal machine learning, aiming to cluster instances sharing the same meaning across texts and images. Traditional synset collection relies on manual efforts and lacks scalability. Existing methods from a linguistic perspective ignore the visual contribution to semantics. We propose VENUE to address these issues, considering the varying contributions of visual and textual modalities and noise within visual data.\n\nFigure 1. Illustration of multimodal instances, each tag accompanied by multiple images.\n\n---",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 3,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 63,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 554,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c1",
    "source_id": "electronics_12_03521_v2",
    "text": "---\n\nWe present the VENUE encoder for learning visually-enhanced multimodal representations for synset induction. The encoder is trained end-to-end with a triplet loss and incorporates modules for visual interaction, multi-granularity embedding, masking, and gating. Our method emphasizes the filtering of semantically weakly-relevant images and regulates contributions from visual and textual modalities.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 4,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 405,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c2",
    "source_id": "electronics_12_03521_v2",
    "text": "The synset induction task is formulated to group multimodal instances into identical synsets. Given a set of multimodal instances S, the task is to predict synsets {c1, c2, · · · , cM} using the framework Φ(S; w), where w represents the model parameters.\n\nOur approach involves two steps: training the VENUE encoder and inducing synsets through clustering. VENUE comprises a visual interaction module, which deals with image noise by modeling associations between images using attention mechanisms, and a multi-granularity embedding module that generates enhanced representations.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 5,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 580,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c3",
    "source_id": "electronics_12_03521_v2",
    "text": "The visual interaction module uses a pre-trained CNN to extract image representations and applies an attention mechanism to generate weighted visual representations. The attention score an for the nth image is computed, and the attention distribution is used to obtain a visually enhanced representation vfatt.\n\nThe multi-granularity embedding module further processes these representations, incorporating log-sum-exp pooling and stacking to generate multi-granularity representations.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 6,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 485,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c4",
    "source_id": "electronics_12_03521_v2",
    "text": "In synset induction tasks, tags are typically short phrases. For instance, the tags in Figure 1 consist of one or two words, such as \"Automotive,\" \"Car,\" \"Helianthus Annuus,\" and \"Sunflowers.\" To effectively utilize the information within these tags, we introduce a multi-granularity embedding module. This module captures tag-level embeddings (El) and word-level tag embeddings (Ew) through different training methods. We specifically use word2vec [19] to train word vectors on an external corpus for the word-level embedding. The tag-level embedding (ttle) is obtained by training the word2vec mod",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 7,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c5",
    "source_id": "electronics_12_03521_v2",
    "text": "bedding. The tag-level embedding (ttle) is obtained by training the word2vec model on the entire tag, as formulated:",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 8,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 116,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c6",
    "source_id": "electronics_12_03521_v2",
    "text": "ttle = El(t) (6)\n\nHere, El represents the embedding layer, initialized with a pre-trained tag-level word2vec, and ttle ∈ RDT. For word-level tag embeddings, Ew generates word vectors {tw1, tw2, ..., twNT}:\n\nn tw1, tw2, ..., twNT\no = Ew(t) = El \u0000 w1, w2, ..., wNT \u0001 (7)",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 9,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 268,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c7",
    "source_id": "electronics_12_03521_v2",
    "text": "n tw1, tw2, ..., twNT\no = Ew(t) = El \u0000 w1, w2, ..., wNT \u0001 (7)\n\ntw_n ∈ RDT. We note that pooling strategies significantly affect tag embeddings. Average pooling treats each embedding dimension equally, leading to a lack of semantic discrimination, while max pooling emphasizes local signals over comprehensive semantics. To balance these, we adopt log-sum-exp pooling (LSE), inspired by Pinheiro et al. [20]. LSE is a smooth, convex approximation of the max function:\n\ntwle = LSE \u0010 tw1, tw2, ..., twNT \u0011 = log (NV ∑ i=1 exp(r × tiw))1",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 10,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 533,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c8",
    "source_id": "electronics_12_03521_v2",
    "text": "twle = LSE \u0010 tw1, tw2, ..., twNT \u0011 = log (NV ∑ i=1 exp(r × tiw))1\n\nHere, twle ∈ RDT, and the parameter r adjusts between average and max pooling. The concatenation of tag-level and word-level embeddings yields the fused textual representation:\n\ntmge = h ttle; twlei (9)\n\ntmge ∈ R2DT.\n\n3.3. Masking Module",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 11,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 304,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c9",
    "source_id": "electronics_12_03521_v2",
    "text": "tmge = h ttle; twlei (9)\n\ntmge ∈ R2DT.\n\n3.3. Masking Module\n\nTo mitigate the impact of image noise on multimodal representation, we design a masking module that enhances visual representations by considering inter-modal attention. The module takes the outputs (v_fatt, tmge) from the visual interaction and multi-granularity embedding modules. It generates a compact visual representation (vc) and textual representation (tc):\n\nvc = tanh(W_I2 \u0010 W_I1v_fatt + bI1 \u0011 + bI2) (10)\ntc = tanh(W_T2 \u0010 W_T1tmge + bT1 \u0011 + bT2) (11)",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 12,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 521,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c10",
    "source_id": "electronics_12_03521_v2",
    "text": "The masking vector σ(vc, tc) is computed using the Sigmoid activation function and applied to v_fatt to filter noise:\n\nvmsk = v_fatt ⊙ σ(vc, tc) (13)\n\nThe global masked visual representation (vgmsk) is obtained by aggregating vmsk:\n\nvgmsk = 1/NV ∑_j=1 W_pvmsk_j (14)\n\n3.4. Gating Module\n\nTo regulate contributions between visual and textual modalities, we propose a gating module. It includes a transform layer to generate a compact textual representation (tpjt):\n\ntpjt = Dropout(W_P2 \u0010 W_P1tmge + bP1 \u0011 + bP2) (15)\n\nThe gating operation on vgmsk and tpjt produces gated representations:",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 13,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 587,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c11",
    "source_id": "electronics_12_03521_v2",
    "text": "The gating operation on vgmsk and tpjt produces gated representations:\n\ng = σ(W_G1vgmsk + W_G2tpjt) (16)\n\nvgate = g ⊙ WG_3vgmsk (17)\ntgate = (1 - g) ⊙ WG_4tpjt (18)\n\nThe final representation o is the concatenation of vgate and tgate:\n\no = \u0002 vgate; tgate\u0003 (19)\n\no ∈ RDo, with Do = 2Dg.\n\n3.5. Loss Function and Training Algorithm\n\nWe train our VENUE encoder using the triplet loss function:\n\nL = ∑_i=1 max(0, d+_i - d-_i + m) (20)\n\nwith cosine distance as the metric:\n\ncos(x, y) = 1 - x · y / ∥x∥_2 ∥y∥_2 (21)",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 14,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 507,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c12",
    "source_id": "electronics_12_03521_v2",
    "text": "with cosine distance as the metric:\n\ncos(x, y) = 1 - x · y / ∥x∥_2 ∥y∥_2 (21)\n\nWe initialize model parameters with a Gaussian distribution and proceed with training as described.\n\nWe compute visually masked representations with multimodal instances interaction, and gated multimodal representations for anchor, positive, and negative samples. The loss is calculated and model parameters are updated through gradient descent. Our training process is detailed in Algorithm 1.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 15,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 473,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c13",
    "source_id": "electronics_12_03521_v2",
    "text": "Algorithm 1: Training Algorithm of Our VENUE Model\n- Requires: multimodal instances S, learning rate η, iterations EPOCHS, weights θ, margin m, batch size NB\n- Ensures: weights θ",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 16,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 178,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c14",
    "source_id": "electronics_12_03521_v2",
    "text": "1: Initialize weights θ from a Gaussian distribution N(0, 1)\n2: for epoch from 1 to EPOCHS do\n3: mini_batch ← batch_generator(S)\n4: for idx from 1 to NB do\n5: (V_a, t_a), (V_p, t_p), (V_n, t_n) ← mini_batch[idx]\n6: Compute visual enhanced representation v_f_att, textual multi-granularity embedding tmge, visual masked representation vgmsk, and gated multimodal representation o for all instances in a mini-batch\n7: Compute the loss L(θ) and update weights with gradient descent\n8: end for\n9: end for",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 17,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 500,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c15",
    "source_id": "electronics_12_03521_v2",
    "text": "For inference, we use the trained VENUE model to extract multimodal representations from the testing set. We perform clustering using k-means and hierarchical agglomerative clustering (HAC) on these representations to group instances with identical semantics into the same clusters.\n\n4. Experimental Settings\nWe describe our multimodal dataset for synset induction, evaluation metrics, implementation details, and baseline methods.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 18,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 431,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c16",
    "source_id": "electronics_12_03521_v2",
    "text": "4.1. Dataset\nOur MMAI-Synset dataset is collected using the Wikipedia text subset of the synset dataset. It consists of 8509 noun phrases and their corresponding 425,450 images. The dataset is divided into training and testing sets, containing 7833 and 676 instances, respectively.\n\n4.2. Metrics\nWe use three groups of evaluation metrics: entropy-based (h, c, v), membership overlap-based (p, r, f), and external evaluation methods (FMI, ARI, NMI).",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 19,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 448,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c17",
    "source_id": "electronics_12_03521_v2",
    "text": "In this section, we define the metrics for evaluating the clustering performance. The True Positive (TP) denotes the number of synonym word pairs present in the same cluster in both the predicted and ground-truth clusters. False Positive (FP) represents the number of synonym word pairs present in the same cluster in the predicted clusters but not in the ground-truth. False Negative (FN) is the number of synonym word pairs present in the same cluster in the ground-truth but not in the predicted. True Negative (TN) refers to the number of pairs in different clusters in both predictions. The Adj",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 20,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c18",
    "source_id": "electronics_12_03521_v2",
    "text": "refers to the number of pairs in different clusters in both predictions. The Adjusted Rand Index (ARI) is a similarity measure between two clusters, calculated as:",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 21,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 163,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c19",
    "source_id": "electronics_12_03521_v2",
    "text": "ARI = RI − E(RI) / max(RI) − E(RI)\n\nwhere RI = (TP + TN)/N, N is the number of instances, max is the maximum function, and E is the expectation operator. Normalized Mutual Information (NMI) measures the normalized mutual information between two cluster assignments:\n\nNMI(Ω, C) = 2 × I(Ω; C) / (H(Ω) + H(C))\n\nwhere H(·) is the entropy of clusters, and I(Ω; C) is the mutual information between predicted and ground-truth clusters.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 22,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 429,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c20",
    "source_id": "electronics_12_03521_v2",
    "text": "For the implementation details, our experiments consist of data pre-processing, model training, and model inference. We use ResNet101 as the visual backbone in our VENUE model, with the image representation dimensionality DI set to 2048. The multi-granularity embedding dimensionality DT is 200, and the adjustment factor r is 1.6. Gensim is used for tag-level and word-level representations, both with a dimensionality of 100. The masking module has a dimensionality Dc of 512, while the gating module uses 512-dimensional representations, resulting in a 1024-dimensional multimodal semantic embedd",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 23,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c21",
    "source_id": "electronics_12_03521_v2",
    "text": "onal representations, resulting in a 1024-dimensional multimodal semantic embedding Do. We train our model using the PyTorch Framework and RAdam optimizer with a learning rate of 1 × 10−4. We employ an online triplet mining strategy for efficient training.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 24,
    "chunk_index_in_section": 21,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 256,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c22",
    "source_id": "electronics_12_03521_v2",
    "text": "For the baseline methods, we compare our approach with: (1) word2vec + k-means/HAC, (2) CNN + k-means/HAC, and (3) [word2vec; CNN] + k-means/HAC, where the synsets are induced using either k-means or HAC algorithms.\n\n---",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 25,
    "chunk_index_in_section": 22,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 220,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c23",
    "source_id": "electronics_12_03521_v2",
    "text": "(4) SynsetMine is a text-based method that induces synsets using pre-trained word2vec as tag representations within a proposed framework. We retain textual data from multimodal instances and follow SynsetMine's experimental setup, utilizing the authors' pre-trained word2vec for initialization. The model performs greedy clustering to merge tags into synsets based on semantic similarity. (5) Infomap, a community detection algorithm, is applied to synset induction using a graph structure and information theoretic approach. We represent tags with pre-trained word2vec, construct a graph based on E",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 26,
    "chunk_index_in_section": 23,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c24",
    "source_id": "electronics_12_03521_v2",
    "text": "roach. We represent tags with pre-trained word2vec, construct a graph based on Euclidean distances, and apply the Infomap algorithm for clustering. (6) MWSI is a multimodal unsupervised clustering method that uses both visual and textual features. Adapted for our setting, we employ a variant of MWSI without synonymy detection, using early fusion of features and hierarchical clustering for grouping identical meanings. (7) CLIP is a multimodal pre-trained neural network that learns semantic correlations from image-text pairs. We apply the CLIP model without modification or additional training, u",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 27,
    "chunk_index_in_section": 24,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 600,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c25",
    "source_id": "electronics_12_03521_v2",
    "text": "xt pairs. We apply the CLIP model without modification or additional training, using it to extract multimodal representations for clustering with the HAC algorithm.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 28,
    "chunk_index_in_section": 25,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 164,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c26",
    "source_id": "electronics_12_03521_v2",
    "text": "5. Results and Analysis\n\nWe present experimental results in this section. Section 5.1 compares our VENUE model with baseline methods. Section 5.2 conducts ablation studies on VENUE's modules, while Section 5.3 examines the effects of different parameter configurations. Section 5.4 provides qualitative results of our method.\n\n5.1. Performance of Synset Induction",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 29,
    "chunk_index_in_section": 26,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 363,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c27",
    "source_id": "electronics_12_03521_v2",
    "text": "5.1. Performance of Synset Induction\n\nOur experimental results, shown in Table 3, reveal that vision-based methods perform worst, due to their reliance on pre-trained CNNs or manual visual descriptors. In contrast, our VENUE model and other baselines show varying degrees of success, with SynsetMine, Infomap, MWSI, and CLIP demonstrating competitive performance.\n\nTable 3. Experimental results comparing baseline methods on three sets of evaluation metrics. Scores are in percentage (%) with the highest score in boldface and the second highest underlined.\nEncoder Clustering h c v p r f ARI FMI NMI",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 30,
    "chunk_index_in_section": 27,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 600,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c28",
    "source_id": "electronics_12_03521_v2",
    "text": "word2vec k-means 89.19 91.22 90.19 39.89 54.70 46.13 45.91 46.71 90.20\nword2vec HAC 90.09 94.65 92.31 36.85 73.14 49.00 48.76 51.96 92.35\nCNN k-means 80.38 85.31 82.77 16.37 33.91 22.08 27.71 23.56 82.81\nCNN HAC 72.50 85.91 78.64 4.44 43.44 8.05 7.46 13.89 78.92\n[word2vec; CNN] k-means 83.07 87.18 85.07 22.62 41.58 29.30 28.98 30.67 85.10\n[word2vec; CNN] HAC 85.31 92.09 88.57 22.26 62.99 32.90 32.54 37.45 88.64\nSynsetMine [14] - 94.26 91.69 92.96 58.80 55.81 57.26 57.12 57.28 92.97\nInfoMap [30] - 98.63 87.07 92.49 69.97 30.56 42.54 42.42 46.24 92.67",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 31,
    "chunk_index_in_section": 28,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 555,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c29",
    "source_id": "electronics_12_03521_v2",
    "text": "InfoMap [30] - 98.63 87.07 92.49 69.97 30.56 42.54 42.42 46.24 92.67\nMWSI [17] HAC 94.11 93.49 93.80 53.19 66.09 58.94 58.78 59.29 93.80\nCLIP [31] HAC 91.48 95.73 93.56 40.86 79.95 54.08 53.86 57.16 93.56\nVENUE k-means 92.53 94.72 93.61 53.60 73.64 62.04 61.89 62.83 93.61\nVENUE HAC 96.41 93.79 95.08 62.81 67.95 65.28 65.15 65.33 95.08\n---",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 32,
    "chunk_index_in_section": 29,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 340,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c30",
    "source_id": "electronics_12_03521_v2",
    "text": "Text-based methods, such as word2vec + k-means/HAC, InfoMap, and SynsetMine, outperformed vision-based methods. word2vec + k-means achieved homogeneity, completeness, and v-measure scores of 89.19, 91.22, and 90.19, respectively, with precision, recall, and f1-score at 39.89, 54.70, and 46.13, and ARI, FMI, and NMI scores of 45.91, 46.71, and 90.2. For synset induction, textual information was more semantically discriminative than visual, leading to word2vec + k-means outperforming CNN + k-means. InfoMap surpassed word2vec + k-means in homogeneity by 9.44 and NMI by 2.47, while SynsetMine imp",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 33,
    "chunk_index_in_section": 30,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c31",
    "source_id": "electronics_12_03521_v2",
    "text": "word2vec + k-means in homogeneity by 9.44 and NMI by 2.47, while SynsetMine improved ARI by 11.30, FMI by 10.57, and NMI by 2.77.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 34,
    "chunk_index_in_section": 31,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 129,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c32",
    "source_id": "electronics_12_03521_v2",
    "text": "Multimodal methods, including (word2vec; CNN) + k-means/HAC, MWSI, CLIP, and VENUE, achieved better performance than unimodal methods. MWSI achieved scores of 94.11, 93.49, and 93.80 for homogeneity, completeness, and v-measure, with precision, recall, and f1 at 53.19, 66.09, and 58.94, and ARI, FMI, and NMI at 58.78, 59.29, and 93.80. CLIP, a multimodal pre-trained encoder, provided very comparable performance and the best recall score. VENUE + HAC outperformed MWSI across all evaluated metrics, achieving scores of 96.41, 93.79, and 95.08 for homogeneity, completeness, and v-measure, with pr",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 35,
    "chunk_index_in_section": 32,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c33",
    "source_id": "electronics_12_03521_v2",
    "text": "of 96.41, 93.79, and 95.08 for homogeneity, completeness, and v-measure, with precision, recall, and f1 at 62.81, 67.95, and 65.28, and ARI, FMI, and NMI at 65.15, 65.33, and 95.08.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 36,
    "chunk_index_in_section": 33,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 181,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c34",
    "source_id": "electronics_12_03521_v2",
    "text": "Ablation studies on the VENUE encoder showed the effectiveness of its modules. Removing the visual interaction and multi-granularity embedding modules significantly impacted performance. The masking and gating modules also affected performance, with the masking module improving recall. The VENUE model with all modules intact achieved the best performance.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 37,
    "chunk_index_in_section": 34,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 357,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c35",
    "source_id": "electronics_12_03521_v2",
    "text": "For the VENUE (w/o gate) model, the removal of the gating module significantly impacted the precision score more than the recall score. The VENUE (w/o gate) + HAC combination experienced a drop of 9.08 in precision and 4.58 in recall. This is due to the gating module's role in regulating the contributions of different modalities. Without it, the VENUE model may produce a confused representation, leading to lower performance. The HAC algorithm generally outperforms the k-means algorithm on multiple metrics, such as f1-score, ARI, FMI, and NMI, when using identical encoders. The VENUE + k-means",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 38,
    "chunk_index_in_section": 35,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c36",
    "source_id": "electronics_12_03521_v2",
    "text": "f1-score, ARI, FMI, and NMI, when using identical encoders. The VENUE + k-means achieved a recall score of 73.64, while the VENUE + HAC achieved a precision score of 62.81, a recall score of 67.95, and an f1-score of 65.28, reflecting the different focus of HAC on the relationships among instances.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 39,
    "chunk_index_in_section": 36,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 299,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c37",
    "source_id": "electronics_12_03521_v2",
    "text": "In our model, the configuration of hidden neural units Dc in the masking module and output neural units Dg in the gating module is crucial. We investigated these parameters using the VENUE + HAC method and the evaluation metrics ARI, FMI, and NMI. Varying these parameters, we found that the model's performance is best when both are set to 512, balancing the risks of overfitting and underfitting.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 40,
    "chunk_index_in_section": 37,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 398,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c38",
    "source_id": "electronics_12_03521_v2",
    "text": "Qualitative analysis presented examples of induced synsets using the VENUE model and a word2vec-based approach with HAC clustering. Our method leverages multimodal information to make more accurate predictions, although some confusion remains in certain cases.\n\nIn the section on related work, we review previous research in synset induction and multimodal representation for multimodal instances.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 41,
    "chunk_index_in_section": 38,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 397,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c39",
    "source_id": "electronics_12_03521_v2",
    "text": "The task of synset induction involves automatically clustering semantically identical instances. Most previous research approaches can be categorized into two types: corpus statistics and pattern-based methods, and distributional representation-based methods. Turney et al. introduced an unsupervised learning algorithm to recognize synonyms using statistical information. Nakashole et al. proposed a pattern-based algorithm to capture synonyms and construct taxonomies. However, these methods lack semantic understanding for effective synset mining.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 42,
    "chunk_index_in_section": 39,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 550,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c40",
    "source_id": "electronics_12_03521_v2",
    "text": "Shen et al. introduced an unsupervised ensemble method for synset expansion, while Qu et al. combined distributional features and textual patterns to predict synonym relationships. Zhang et al. used a neural classifier with multiple contexts to identify synonym relationships in free-text corpora. Distributional representations, inspired by word2vec, have been integrated into synset induction tasks. Mamou et al. presented an end-to-end workflow for synset induction based on multi-context word embeddings, and Shen et al. proposed SynsetMine to learn holistic semantics for synset mining.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 43,
    "chunk_index_in_section": 40,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 591,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c41",
    "source_id": "electronics_12_03521_v2",
    "text": "Wang et al. developed SurfCon to compute semantic similarity using surface form and global context. Some researchers have begun to incorporate visual information into synset induction. For instance, Yin et al. clustered visual instances using latent representations and sparse coding, and Chang et al. formulated clustering as a pairwise binary classification problem. Thomason and Mooney introduced a multimodal unsupervised clustering method, while Yao et al. used images and accompanying text to mine the same sense in images. Li et al. proposed a single-stage contrastive clustering method, but",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 44,
    "chunk_index_in_section": 41,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c42",
    "source_id": "electronics_12_03521_v2",
    "text": "in images. Li et al. proposed a single-stage contrastive clustering method, but these methods did not explicitly address the noise problem in images.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 45,
    "chunk_index_in_section": 42,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 149,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c43",
    "source_id": "electronics_12_03521_v2",
    "text": "The key challenge in multimodal synset induction is learning discriminative multimodal representations. Methods such as unifying visual-semantic embeddings, order multimodal embeddings, and the encoder-decoder framework have been proposed. Some works considered image-text unbalanced data, like tag-image collections. Kiela et al. combined pre-trained CNN and skip-gram models, while Thoma et al. proposed cross-modal knowledge fusion. Wang et al. dynamically fused semantic representations, and Berger et al. simulated child language acquisition.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 46,
    "chunk_index_in_section": 43,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 547,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c44",
    "source_id": "electronics_12_03521_v2",
    "text": "Our work addresses the underconsideration of unbalanced text-image data and the差异 between visual and linguistic contributions for learning semantic representations, leveraging both intra-modal and inter-modal interactions.\n\n改动后的第10部分内容：\n\n---",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 47,
    "chunk_index_in_section": 44,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 241,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c45",
    "source_id": "electronics_12_03521_v2",
    "text": "In this paper, we propose a neural encoder named VENUE to learn a visually enhanced multimodal neural representation for synset induction. The key insight involves multimodal representations and the interactions within and between modalities. For intra-modal interaction, we employ the attention mechanism to capture correlations among images. To obtain multi-granularity textual representations, we integrate pre-trained tags with word embeddings. Our inter-modal interaction design includes a masking module to filter out weakly relevant visual information and a gating module to adaptively regula",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 48,
    "chunk_index_in_section": 45,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c46",
    "source_id": "electronics_12_03521_v2",
    "text": "out weakly relevant visual information and a gating module to adaptively regulate the contributions of different modalities to semantics. We train the VENUE encoder using a triplet loss in an end-to-end manner. Clustering algorithms such as k-means and HAC are utilized for synset induction. Our extensive experiments on the MMAI-Synset dataset show that our method outperforms strong baselines on various popular metrics. Future work may explore the introduction of a regional-level masking mechanism for fine-grained multimodal representation and the application of reinforcement learning to bridg",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 49,
    "chunk_index_in_section": 46,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c47",
    "source_id": "electronics_12_03521_v2",
    "text": "multimodal representation and the application of reinforcement learning to bridge the gap between training objectives and evaluation metrics in synset induction. Additionally, the potential application of Multimodal Large Language Models (MLLMs) to our targeted multimodal task is an intriguing direction for further research.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 50,
    "chunk_index_in_section": 47,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 326,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c48",
    "source_id": "electronics_12_03521_v2",
    "text": "---\n\nNote: The provided text has been cleaned of any page numbers,重复信息,乱码,PDF解析错误,多余的符号和格式问题, while retaining all academic content, technical terms, data, logical structure, formulas, and figure descriptions as per the instructions.\n\nMikolov, T.; Sutskever, I.; Chen, K.; Corrado, G.S.; Dean, J. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems (NIPS); The MIT Press: Cambridge, MA, USA, 2013; pp. 3111–3119.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 51,
    "chunk_index_in_section": 48,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 489,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c49",
    "source_id": "electronics_12_03521_v2",
    "text": "Pinheiro, P.O.; Collobert, R. From image-level to pixel-level labeling with convolutional networks. In Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Boston, MA, USA, 7 June 2015; pp. 1713–1721.\n\nSchroff, F.; Kalenichenko, D.; Philbin, J. Facenet: A unified embedding for face recognition and clustering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Boston, MA, USA, 7 June 2015; pp. 815–823.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 52,
    "chunk_index_in_section": 49,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 480,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c50",
    "source_id": "electronics_12_03521_v2",
    "text": "Tang, Z.; Huang, J. Harmonious multi-branch network for person re-identification with harder triplet loss. ACM Trans. Multimed. Comput. Commun. Appl. 2022, 18, 1–21.\n\nRosenberg, A.; Hirschberg, J. V-measure: A conditional entropy-based external cluster evaluation measure. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), Prague, Czech Republic, 28–30 June 2007; pp. 410–420.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 53,
    "chunk_index_in_section": 50,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 481,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c51",
    "source_id": "electronics_12_03521_v2",
    "text": "Halkidi, M.; Batistakis, Y.; Vazirgiannis, M. On clustering validation techniques. J. Intell. Inf. Syst. 2001, 17, 107–145.\n\nVinh, N.X.; Epps, J.; Bailey, J. Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance. J. Mach. Learn. Res. 2010, 11, 2837–2854.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 54,
    "chunk_index_in_section": 51,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 321,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c52",
    "source_id": "electronics_12_03521_v2",
    "text": "Mahajan, D.; Girshick, R.; Ramanathan, V.; He, K.; Paluri, M.; Li, Y.; Bharambe, A.; Maaten, L.V. Exploring the limits of weakly supervised pretraining. In Computer Vision—ECCV 2018; Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y., Eds.; Springer International Publishing: Cham, Switzerland, 2018; pp. 185–201.\n\nRehůrek, R.; Sojka, P. Software Framework for Topic Modelling with Large Corpora. In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, ELRA, Valletta, Malta, 22 May 2010; pp. 45–50.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 55,
    "chunk_index_in_section": 52,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 524,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c53",
    "source_id": "electronics_12_03521_v2",
    "text": "Paszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.; Chanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga, L.; et al. Pytorch: An imperative style, high-performance deep learning library. In Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), Vancouver, BC, Canada, 8–14 December 2019; pp. 8024–8035.\n\nLiu, L.; Jiang, H.; He, P.; Chen, W.; Liu, X.; Gao, J.; Han, J. On the variance of the adaptive learning rate and beyond. In Proceedings of the International Conference on Learning Representations (ICLR), Addis Ababa, Ethiopia, 26–30 April 2020.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 56,
    "chunk_index_in_section": 53,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 585,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c54",
    "source_id": "electronics_12_03521_v2",
    "text": "Rosvall, M.; Bergstrom, C.T. Maps of random walks on complex networks reveal community structure. Proc. Natl. Acad. Sci. USA 2008, 105, 1118–1123.\n\nRadford, A.; Kim, J.W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; et al. Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning (ICML), Virtual Event, 24 July 2021; Volume 139, pp. 8748–8763.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 57,
    "chunk_index_in_section": 54,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 480,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c55",
    "source_id": "electronics_12_03521_v2",
    "text": "Pennington, J.; Socher, R.; Manning, C.D. Glove: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Doha, Qatar, 25 October 2014; pp. 1532–1543.\n\nYin, Q.; Wu, S.; Wang, L. Partially tagged image clustering. In Proceedings of the 2015 IEEE International Conference on Image Processing (ICIP), Quebec City, QC, Canada, 30 September 2015; pp. 4012–4016.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 58,
    "chunk_index_in_section": 55,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 438,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c56",
    "source_id": "electronics_12_03521_v2",
    "text": "Chang, J.; Wang, L.; Meng, G.; Xiang, S.; Pan, C. Deep adaptive image clustering. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), Venice, Italy, 22 October 2017; pp. 5879–5887.\n\nYao, Y.; Shen, F.; Zhang, J.; Liu, L.; Tang, Z.; Shao, L. Extracting multiple visual senses for web learning. IEEE Trans. Multimed. 2018, 21, 184–196.\n\nLi, Y.; Hu, P.; Liu, Z.; Peng, D.; Zhou, J.T.; Peng, X. Contrastive clustering. In Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI), New York, NY, USA, 7 February 2021.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 59,
    "chunk_index_in_section": 56,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 563,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c57",
    "source_id": "electronics_12_03521_v2",
    "text": "Kiros, R.; Salakhutdinov, R.; Zemel, R.S. Unifying visual-semantic embeddings with multimodal neural language models. In Proceedings of the Neural Information Processing Systems (NIPS), Deep Learning Workshop, Montreal, QC, Canada, 8 December 2014.\n\nVendrov, I.; Kiros, R.; Fidler, S.; Urtasun, R. Order-embeddings of images and language. In Proceedings of the International Conference on Learning Representations (ICLR), Vancouver, BC, Canada, 30 April 2016.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 60,
    "chunk_index_in_section": 57,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 459,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c58",
    "source_id": "electronics_12_03521_v2",
    "text": "Mao, J.; Xu, J.; Jing, K.; Yuille, A.L. Training and evaluating multimodal word embeddings with large-scale web annotated images. In Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), Barcelona, Spain, 5 December 2016; pp. 442–450.\n\nRen, S.; He, K.; Girshick, R.; Sun, J. Faster r-cnn: Towards real-time object detection with region proposal networks. In Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), Montreal, QC, Canada, 7 December 2015; pp. 91–99.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 61,
    "chunk_index_in_section": 58,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 513,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c59",
    "source_id": "electronics_12_03521_v2",
    "text": "Anderson, P.; He, X.; Buehler, C.; Teney, D.; Johnson, M.; Gould, S.; Zhang, L. Bottom-up and top-down attention for image captioning and visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Salt Lake City, UT, USA, 18 June 2018; pp. 6077–6086.\n\nKiela, D.; Bottou, L. Learning image embeddings using convolutional neural networks for improved multi-modal semantics. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Doha, Qatar, 25 October 2014; pp. 36–45.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 62,
    "chunk_index_in_section": 59,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 568,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c60",
    "source_id": "electronics_12_03521_v2",
    "text": "Thoma, S.; Rettinger, A.; Both, F. Knowledge fusion via embeddings from text, knowledge graphs, and images. arXiv 2017, arXiv:1704.06084.\n\nWang, S.; Zhang, J.; Zong, C. Learning multimodal word representation via dynamic fusion methods. In Proceedings of the 32nd AAAI Conference on Artificial Intelligence (AAAI), New Orleans, LO, USA, 7 February 2018; pp. 5973–5980.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 63,
    "chunk_index_in_section": 60,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 368,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c61",
    "source_id": "electronics_12_03521_v2",
    "text": "Wang, S.; Zhang, J.; Zong, C. Associative multichannel autoencoder for multimodal word representation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics, Brussels, Belgium, 31 October 2018; pp. 115–124.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 64,
    "chunk_index_in_section": 61,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 293,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "electronics_12_03521_v2_s1_c62",
    "source_id": "electronics_12_03521_v2",
    "text": "Berger, U.; Stanovsky, G.; Abend, O.; Frermann, L. A Computational Acquisition Model for Multimodal Word Categorization. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Seattle, WA, USA, 10–15 July 2022; pp. 3819–3835.\n\nYin, S.; Fu, C.; Zhao, S.; Li, K.; Sun, X.; Xu, T.; Chen, E. A Survey on Multimodal Large Language Models. arXiv 2023, arXiv:2306.13549v1.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 65,
    "chunk_index_in_section": 62,
    "total_chunks_in_section": 63,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 457,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Enhanced_Prompt_Learning_for_Few_shot_Text_Classification_Method_s0_c0",
    "source_id": "Enhanced_Prompt_Learning_for_Few_shot_Text_Classification_Method",
    "text": "增强提示学习的少样本文本分类方法\n\n李睿凡1,2,3,† 魏志宇1 范元涛1 叶书勤1 张光卫2,4\n\n1. 北京邮电大学人工智能学院, 北京 100876; 2. 教育部信息网络工程研究中心, 北京 100876; 3. 交互技术与体验系统文化和旅游部重点实验室, 北京 100876; 4. 北京邮电大学计算机学院, 北京 100876; † E-mail: rfli@bupt.edu.cn\n\n摘要：针对少样本文本分类任务，提出提示学习增强的分类算法(EPL4FTC)。该算法将文本分类任务转换为基于自然语言推理的提示学习形式，通过两种粒度的损失进行优化。为捕获下游任务中含有的类别信息，采用三元组损失联合优化方法，并引入掩码语言模型任务作为正则项，提升模型的泛化能力。实验评估表明EPL4FTC方法的准确度明显优于对比基线方法。\n\n关键词：预训练语言模型; 少样本学习; 文本分类; 提示学习; 三元组损失",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 407,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Enhanced_Prompt_Learning_for_Few_shot_Text_Classification_Method_s1_c0",
    "source_id": "Enhanced_Prompt_Learning_for_Few_shot_Text_Classification_Method",
    "text": "本文提出的方法与基于度量学习的方法和基于提示学习的方法密切相关。基于度量学习的方法通过传统度量或深度度量实现对类别的表征。基于提示学习的方法通过将下游任务形式调整为与预训练任务一致，减小上下游任务训练方式不一致带来的差异。\n\n2. EPL4FTC算法\n\nEPL4FTC模型由基于自然语言推理的提示学习模块和度量优化模块组成，共享编码层的参数。基于自然语言推理的提示学习模块通过掩码语言模型头层计算输入句子中推理词的概率，并使用单句级和句群级两种粒度损失方法进行优化。度量优化模块使用三元组损失计算锚点与正负例之间的损失。\n\n2.1 基于自然语言推理的提示学习模块\n\n该模块负责将文本分类任务转换为基于自然语言推理形式的完型填空任务。对于给定的输入文本x，通过模板映射将真实标签转化为自然语言推理形式。定义模板的一般形式为[x'] = “[x], [z][d]”。在推理阶段，通过计算[z]处的自然语言推理词概率，选取预测为蕴含关系最大概率的标签描述d对应的真实标签作为最终预测结果。\n\n针对单样本输入形式以及通过数据增强形式扩增负样本形成的样例集合形式，设计两种粒度的损失函数来优化建模效果。\n\n2.2 度量优化模块\n\n通过三元组损失函数进行有监督的度量学习，使模型可以更好地学习不同类别间的距离关系信息。使用带间隔的损失函数。",
    "section_title": "相关工作",
    "section_type": "related_work",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 1,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 3,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 564,
    "chunk_method": "hierarchical",
    "importance_weight": 0.66,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Enhanced_Prompt_Learning_for_Few_shot_Text_Classification_Method_s1_c1",
    "source_id": "Enhanced_Prompt_Learning_for_Few_shot_Text_Classification_Method",
    "text": "2.2 度量优化模块\n\n通过三元组损失函数进行有监督的度量学习，使模型可以更好地学习不同类别间的距离关系信息。使用带间隔的损失函数。\n\n在文本分类任务中，EPL4FTC算法通过将任务转化为自然语言推理任务，并采用三元组损失优化模型泛化性能。实验结果显示，该算法在中文和英文数据集上均优于基线方法，特别是在少样本场景下。此外，通过消融实验验证了度量优化模块和句群级损失的有效性。提示模板分析表明，非自然语言形式的推理词在复杂任务中表现更优。\n\n---\n\n表6 中文数据集和英文数据集上推理词形式性能比较\n\n中文数据集 英文数据集\nEPRSTMT CSLDCP TNEWS IFLYTEK 平均值 AG News TREC Yelp Review 平均值\n自然语言推理词 86.0 54.3 53.5 45.4 59.8 77.3 59.0 26.1 54.1\n非自然语言推理词 85.3 55.1 54.6 46.4 60.4 79.5 55.8 26.1 53.8\n\n3.6.2 提示模板的性能",
    "section_title": "相关工作",
    "section_type": "related_work",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 2,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 447,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Enhanced_Prompt_Learning_for_Few_shot_Text_Classification_Method_s1_c2",
    "source_id": "Enhanced_Prompt_Learning_for_Few_shot_Text_Classification_Method",
    "text": "3.6.2 提示模板的性能\n\n手工设计的提示模板会使模型的效果产生一定的波动。实验结果表明，模型性能受提示模板的影响较大。在中文TNEWS和英文TREC任务中对模板采用前缀式与后缀式的形式进行评测，中文数据集上性能差异相对较小，准确率的最大值与最小值相差1.1%，而英文数据上性能差异较大，准确率最大值与最小值相差6.4%。这表明提示模板对模型准确率的影响与下游任务的具体形式有关。\n\n3.7 可视化分析\n\n采用t-SNE方法对中文TNEWS数据集进行可视化分析，评估引入度量优化模块后模型获得任务类别信息的有效性。结果表明，编码层CLS位的输出作为实例的向量化表示，已学习到一定的实例类别信息。度量优化模块通过三元组损失优化类别间的距离，使同一类别的实例间紧密聚集，不同类别的实例间存在明显间隔。\n\n本文提出基于提示学习和三元组损失优化的少样本文本分类EPL4FTC算法，实验结果表明，该算法能有效提升文本分类的准确性。\n\n未来的工作中，我们将尝试将EPL4FTC算法应用于其他少样本任务场景，并对其他语种的少样本文本分类进行研究。\n\n---\n\n[参考文献列表省略]",
    "section_title": "相关工作",
    "section_type": "related_work",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 3,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 484,
    "chunk_method": "hierarchical",
    "importance_weight": 0.63,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning_s0_c0",
    "source_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning",
    "text": "Entailment Method Based on Template Selection for Chinese Text Few-shot Learning\n\nZeyuan Wang, Zhiyu Wei, Lihui Zhang, Ruifan Li, Zhanyu Ma\nSchool of Artificial Intelligence, Beijing University of Posts and Telecommunications, Beijing, China\nE-mail: {wangzeyuan, zydotwei, elliot_zlh, rfli, mazhanyu}@bupt.edu.cn",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 24,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 312,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning_s0_c1",
    "source_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning",
    "text": "Abstract—The scarcity of labeled data hinders progress in numerous text-related tasks. Few-shot learning based on pre-trained language models has shown promise, with Entailment-based Few-shot Learning (EFL) being an effective approach by converting text classification into textual entailment. However, the performance is sensitive to manually selected templates. We address this by introducing a template selection mechanism using a masked language model to assess candidate templates. Our method is evaluated on FewCLUE shared tasks, with extensive experiments demonstrating its effectiveness.\nI. Introduction",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 611,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning_s0_c2",
    "source_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning",
    "text": "Pre-training and fine-tuning have become a dominant paradigm for solving various NLP tasks. While this strategy is powerful, real-world applications require high-quality annotations for diverse domains and languages, which can be costly. Models like GPT-3 can perform well with limited samples, but they are difficult to fine-tune and deploy. Cloze-based approaches like PET and LM-BFF reuse the Masked Language Model (MLM) head but may face performance limitations when task distributions differ. We choose MacBERT, a pre-training model, as our backbone and apply EFL to fine-tune specific tasks, t",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning_s0_c3",
    "source_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning",
    "text": "pre-training model, as our backbone and apply EFL to fine-tune specific tasks, transforming them into textual entailment tasks.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 127,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning_s0_c4",
    "source_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning",
    "text": "II. Related Work\nA. Meta-learning\nMeta-learning has shown success in few-shot learning scenarios, including text classification, machine translation, and text generation. It can be categorized into optimization-based and metric-based approaches.\n\nB. Pre-trained Language Model\nPre-trained models are used in few-shot learning to align downstream tasks with pre-training tasks, improving performance with limited samples. Methods vary based on pre-training tasks, such as cloze tasks and sentence-pair tasks.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 507,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning_s0_c5",
    "source_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning",
    "text": "III. Methodology\nA. Pre-trained Backbone\nMainstream pre-trained models in NLP range from unidirectional to bidirectional language models. MacBERT, a model optimized for Chinese pre-training, replaces BERT's MLM task with a correction task to reduce pre-training and fine-tuning discrepancies.\n\nB. Entailment Framework\nEFL can enhance a small-scale language model's few-shot learning capabilities by reformulating NLP tasks as textual entailment tasks.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 5,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 451,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning_s0_c6",
    "source_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning",
    "text": "The Entailment-based Reformulation approach uses MacBERT as the framework for few-shot learning, transforming text classifications into textual entailment tasks. The input consists of two sentences: the original text and a template for each label, with the output being the entailment relationship between them. For training and inference in few-shot classification, it is essential to construct entailment tasks. We use cross-entropy loss for model fitting, with each text joined with multiple templates in a multi-classification task.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 6,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 536,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning_s0_c7",
    "source_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning",
    "text": "Intermediate training with an entailment dataset like CMNLI can bridge the semantic gap between pre-trained models and downstream tasks. All downstream tasks reuse the encoder parameters during intermediate training. Template selection is crucial, as different templates significantly impact performance. We calculate the Masked Language Model (MLM) loss of template tokens and use the average loss as the template score to select the most appropriate template.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 7,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 461,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning_s0_c8",
    "source_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning",
    "text": "Our approach includes task-dependent adjustments for various downstream tasks, including single-sentence classification, sentence pair classification, and reading comprehension tasks. Adjustments involve selecting templates for single-sentence tasks and redefining tasks using corresponding templates for reading comprehension tasks.\n\nExperiments were conducted on a range of datasets, and the results show the performance of different methods on test datasets.\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 8,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 466,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning_s0_c9",
    "source_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning",
    "text": "---\n\nWe evaluate nine Chinese few-shot datasets of Few-CLUE, encompassing sentiment analysis, short text classification, long text classification, natural language inference, sentence similarity, Chinese cloze, and co-reference resolution. Each task provides five different training sets and a combined training set. The CMNLI dataset is used for intermediate training.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 9,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 369,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning_s0_c10",
    "source_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning",
    "text": "Our experiments are conducted using the PyTorch framework and HuggingFace toolkit. We employ MacBERT large as the pre-trained language model for all tasks. During fine-tuning, we use the AdamW optimizer with a learning rate of 2 × 10−5, batch size of 8, max epochs of 10, and a dropout rate of 0.1. We randomly select k = 8 negative samples for non-entailment during training and generate N × M input data for N samples and M label descriptions during inference.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 10,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 462,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning_s0_c11",
    "source_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning",
    "text": "Our entailment-based method is compared with various baselines using pre-trained networks. The results on the testing datasets of the nine NLP tasks show that the EFL method with automatic template selection outperforms other methods. EFL is more effective on sentence-pair tasks, while PET is better for single sentence classification. MacBERT-large consistently outperforms MacBERT-base.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 11,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 389,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning_s0_c12",
    "source_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning",
    "text": "To demonstrate the effectiveness of our template choosing method, we evaluate the relationship between the masked language loss and template accuracy. The results show that as the loss decreases, the accuracy of prediction increases.\n\nIn conclusion, our method for FewCLUE shared tasks integrates techniques such as pre-trained MacBERT, entailment templates, and template selection with SimBERT. Future work will explore self-supervised contrastive learning and graph convolutional networks. This work was supported by the National Key R&D Program of China.\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 12,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 562,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning_s0_c13",
    "source_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning",
    "text": "---\n\nPlease note that some of the technical terms and dataset names (e.g., Few-CLUE, CMNLI, EFL, PET, SimBERT) have been preserved as they are integral to the academic content.\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 13,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 181,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning_s0_c14",
    "source_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning",
    "text": "[9] M. Moradshahi, G. Campagna, S. J. Semnani, S. Xu, and M. S. Lam, “Localizing open-ontology QA semantic parsers in a day using machine translation,” arXiv preprint arXiv:2010.05106, 2020. \n[10] Z. Chen, H. Eavani, W. Chen, Y. Liu, and W. Y. Wang, “Few-shot NLG with pre-trained language model,” in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 183–190, 2020. \n[11] A. Nichol and J. Schulman, “Reptile: a scalable metalearning algorithm,” arXiv preprint arXiv:1803.02999, 2018.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 14,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 526,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning_s0_c15",
    "source_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning",
    "text": "[12] Z. Li, F. Zhou, F. Chen, and H. Li, “Meta-SGD: Learning to learn quickly for few-shot learning,” arXiv preprint arXiv:1707.09835, 2017. \n[13] F. Sung, Y. Yang, L. Zhang, T. Xiang, P. H. Torr, and T. M. Hospedales, “Learning to compare: Relation network for few-shot learning,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1199–1208, 2018. \n[14] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer, “Deep contextualized word representations,” in Proceedings of NAACL-HLT, pp. 2227–2237, 2018.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 15,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 570,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning_s0_c16",
    "source_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning",
    "text": "[15] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving language understanding by generative pre-training,” 2018. \n[16] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, “Language models are unsupervised multi-task learners,” 2019.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 16,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 266,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning_s0_c17",
    "source_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning",
    "text": "[17] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of deep bidirectional transformers for language understanding,” in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, 2019. \n[18] K. Clark, M.-T. Luong, Q. V. Le, and C. D. Manning, “Electra: Pre-training text encoders as discriminators rather than generators,” arXiv preprint arXiv:2003.10555, 2020.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 17,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 517,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning_s0_c18",
    "source_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning",
    "text": "[19] Y. Sun, S. Wang, Y. Li, S. Feng, H. Tian, H. Wu, and H. Wang, “Ernie 2.0: A continual pre-training framework for language understanding,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, pp. 8968–8975, 2020. \n[20] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, “Roberta: A robustly optimized BERT pretraining approach,” arXiv preprint arXiv:1907.11692, 2019.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 18,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 446,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning_s0_c19",
    "source_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning",
    "text": "[21] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. Salakhutdinov, and Q. V. Le, “XLNet: generalized autoregressive pretraining for language understanding,” in Proceedings of the 33rd International Conference on Neural Information Processing Systems, pp. 5753–5763, 2019. \n[22] A. Conneau, R. Rinott, G. Lample, A. Williams, S. Bowman, H. Schwenk, and V. Stoyanov, “XNLI: Evaluating cross-lingual sentence representations,” in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 2475–2485, 2018.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 19,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 531,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning_s0_c20",
    "source_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning",
    "text": "[23] A. Williams, N. Nangia, and S. Bowman, “A broad-coverage challenge corpus for sentence understanding through inference,” in Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1112–1122, 2018. \n[24] J. Su, “Simbert: Integrating retrieval and generation into BERT,” tech. rep., 2020.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 20,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 383,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning_s0_c21",
    "source_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning",
    "text": "[25] L. Xu, H. Hu, X. Zhang, L. Li, C. Cao, Y. Li, Y. Xu, K. Sun, D. Yu, C. Yu, et al., “CLUE: A Chinese language understanding evaluation benchmark,” In Proceedings of the 28th International Conference on Computational Linguistics, pages 4762–4772., 2020.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 21,
    "chunk_index_in_section": 21,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 256,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning_s0_c22",
    "source_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning",
    "text": "[26] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, et al., “Transformers: State-of-the-art natural language processing,” in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 38–45, Association for Computational Linguistics, Oct. 2020. \n[27] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,” in International Conference on Learning Representations, 2019.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 22,
    "chunk_index_in_section": 22,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 572,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning_s0_c23",
    "source_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning",
    "text": "[28] Y. Ouali, C. Hudelot, and M. Tami, “Spatial contrastive learning for few-shot classification,” arXiv preprint arXiv:2012.13831, 2020. \n[29] T. N. Kipf and M. Welling, “Semi-supervised classification with graph convolutional networks,” arXiv preprint arXiv:1609.02907, 2016.\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 23,
    "chunk_index_in_section": 23,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 282,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c0",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "The task of text-to-image synthesis involves generating photographic images based on given textual descriptions. Most current approaches rely on generative adversarial network (GAN) models and utilize global linguistic representations. However, these models face training difficulties due to the sparsity of global representations and lack fine-grained information in the generated images. To overcome this, we propose cross-modal global and local linguistic representations-based generative adversarial networks (CGL-GAN), integrating local linguistic representations into GANs. Our CGL-GAN includes",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 48,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 600,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c1",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "N), integrating local linguistic representations into GANs. Our CGL-GAN includes a generator for image synthesis and a discriminator to assess the conformity of images with text descriptions. The discriminator establishes cross-modal correlations by projecting image representations onto global and local linguistic representations. We use a hinge loss function for training and evaluate our model on the CUB and MS-COCO datasets. Experiments show that incorporating fine-grained local linguistic information and cross-modal correlation significantly enhances text-to-image synthesis performance, eve",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 600,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c2",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "odal correlation significantly enhances text-to-image synthesis performance, even for high-resolution images.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 109,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c3",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "Index Terms—Text-to-image synthesis, generative adversarial network (GAN), linguistic representation, cross-modal.\n\nThe work presented here was supported by various funding sources, including the National Key R&D Program of China, the National Natural Science Foundation of China, and the Science and Technology Program of the Headquarters of State Grid Corporation of China.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 375,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c4",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "The introduction section of the paper outlines the challenge of text-to-image synthesis and the limitations of existing approaches based on GANs that rely on global linguistic representations. It highlights the need for addressing the instability of training and the production of nonsensical results when generating high-resolution images.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 340,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c5",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "In this article, we propose cross-modal global and local linguistic representations-based generative adversarial networks (CGL-GAN) for text-to-image synthesis. Our model incorporates a generator and discriminator, where the discriminator assesses the semantic consistency between generated images and textual descriptions. We utilize a hinge loss function for training the CGL-GAN model.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 5,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 388,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c6",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "Our approach is evaluated on the CUB and MS-COCO datasets, with extensive experiments on the use of global and local linguistic representations. The CGL-GAN model demonstrates comparable performance with fewer trainable parameters than state-of-the-art methods and outperforms single-GAN models. Variants of the cross-modal projection block are experimented with, highlighting the importance of appropriate correlation between visual and linguistic representations.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 6,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 465,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c7",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "The article is structured as follows: Section II reviews related works on GANs and text-to-image synthesis. In Section III, we detail our model, including the text encoder, generator, discriminator, and loss function. Section IV covers datasets and experimental results, while Section V concludes the article and suggests future directions.\n\nII. RELATED WORK",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 7,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 358,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c8",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "II. RELATED WORK\n\nA. Generative Adversarial Networks\nGANs, which have gained significant interest, are generative network models involving a minimax game between a generator and discriminator. The primary objective is to optimize the value function V for both players. The seminal work by Goodfellow et al. introduced GANs, with subsequent studies improving on the framework, including the use of supervised information, training stability, and methods to ensure Lipschitz continuity.\n\nB. Text-to-Image Synthesis\n---\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 8,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 521,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c9",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "B. Text-to-Image Synthesis\n---\n\n---\n\nRecently, text-to-image synthesis has gained attention. Unlike conventional image synthesis tasks that use label information with GANs, text-to-image synthesis relies on textual descriptions. Reed et al. introduced a conditional GAN-based network for this purpose. Two main challenges exist in this task: generating high-quality images and constructing plausible spatial relationships among objects.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 9,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 436,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c10",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "Zhang et al. addressed the first challenge by decomposing the task into manageable subproblems using a sketch-refinement process with multiple GANs. Subsequent works improved on this, with models like the end-to-end tree architecture and hierarchical-nested adversarial objectives. Attention-driven methods also enhanced fine-grained details in generated images.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 10,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 362,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c11",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "For the second challenge, incorporating additional information such as bounding boxes and location information has been explored. Techniques like semantic graphs and object-driven attention mechanisms have been introduced to improve the generation of complex images and object relationships.\n\nIII. MODEL",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 11,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 303,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c12",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "III. MODEL\n\nA. Overview\nOur proposed model, CGL-GAN, consists of three main components: a text encoder, a generator, and a discriminator. The text encoder converts textual descriptions into global and local linguistic representations. The generator takes global linguistic information and random Gaussian noise to produce images. The discriminator assesses whether images are real and match the textual descriptions, utilizing a cross-modal projection block. We use a hinge loss function for training.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 12,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 501,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c13",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "B. Text Encoder\nWe use a bidirectional LSTM to capture dependencies between words in a sentence. Local representations are formed by combining hidden states of each word, while the global representation is the last hidden state. The text encoder is a pre-trained bidirectional LSTM, frozen during training.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 13,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 306,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c14",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "C. Generator\nOur generator uses up-sampling blocks and conditional normalization layers. Each up-sampling block includes ResNet-based convolutional layers and nearest-neighbor upsampling. Conditional normalization incorporates global linguistic information into the feature maps. Parameters for scaling and translating are obtained through a fully-connected layer using concatenated global linguistic representations and random noise.\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 14,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 439,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c15",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "To generate the pixel distribution of the target image, the generator decodes from a specific probability distribution. We input a set of random noise Z = {z0, z1, ..., zNG} with a standard multi-dimensional Gaussian distribution (zi ∼N(0, E)) into the generator. The random noise z0 is reshaped as the initial feature map and fed into the first up-sampling block u1. Subsequently, each remaining random noise {zi}NGi=1 is concatenated with the global linguistic representation yG and integrated into the corresponding up-sampling block ui through conditional normalization. A convolutional layer de",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 15,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c16",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "up-sampling block ui through conditional normalization. A convolutional layer decodes high-dimensional feature maps from uNG into a three-dimensional RGB image. Spectral normalization is applied to all generator weights to regularize the Lipschitz constant.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 16,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 257,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c17",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "The discriminator serves two roles: distinguishing real from fake images and assessing semantic correlation between images and textual descriptions. It comprises down-sampling blocks based on ResNet and a cross-modal projection block (CPB). The CPB captures semantic correlations between visual and linguistic representations by projecting the last two layer feature maps onto the global and local linguistic representations, using matrix multiplication and element-wise multiplication, respectively.\n\nThe cross-modal projection operations are defined by the following equations:",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 17,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 579,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c18",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "The cross-modal projection operations are defined by the following equations:\n\nP(vND−1, yL) = fa1(vND−1) × yL + fa2(vND−1),\nQ(vND, yG) = fb1(vND) ⊙ yG + fb2(vND),\n\nThe discriminator output is expressed as:\n\nD(I, yL, yG) = 1/NP ΣiP + 1/NQ ΣjQ,\n\nwhere the CPB feeds both linguistic representations as conditional information into the GAN discriminator. Four variants of CPB forms are considered, with CGL-GAN-LG adopted in our model.\n\nFor loss function, a hinge loss is designed to train our CGL-GAN, which has been shown to converge efficiently to a Nash equilibrium:",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 18,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 566,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c19",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "L(G, D) = \n- ΣD(IR, yG, yL), if D(IR, yG, yL) < 1\n- Σ[1 + D(G(z, yG), yG, yL)], if D(G(z, yG), yG, yL) < -1\n\nAfter training, the discriminator is discarded, and only the generator is used for image generation from textual descriptions.\n\nIn Section IV, we evaluate our model on the text-to-image synthesis task using the CUB and MS-COCO datasets, describe the evaluation protocol, baseline methods, implementation details, report experimental results, and provide an analysis of the CPB module.\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 19,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 498,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c20",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "We evaluate our approach using two publicly available datasets: CUB [38] and MS-COCO [39]. The CUB dataset, Caltech-UCSD Birds-200-2011, consists of 11,788 bird images across 200 categories, partitioned into training and testing subsets of 8,855 and 2,933 images, respectively. Each image is accompanied by ten descriptive sentences based on the bird's colors and characteristics. We preprocess the images to ensure object-image size ratios exceed 0.75. MS-COCO, in contrast, contains images of multiple objects and diverse backgrounds, with 80,000 and 40,000 images in the training and testing sets",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 20,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c21",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "erse backgrounds, with 80,000 and 40,000 images in the training and testing sets, respectively, and each image annotated with five descriptive sentences.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 21,
    "chunk_index_in_section": 21,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 153,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c22",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "For evaluation, we use the Inception score [40] and Fréchet Inception Distance (FID) [41]. The Inception score assesses the clarity and diversity of generated images, using Kullback-Leibler divergence. The FID metric measures the quality of generated images by comparing their distribution to real images.\n\nOur baseline models for comparison include GAN-INT-CLS [1], GAWWN [2], StackGAN [3], StackGAN-V2 [4], HDGAN [5], AttnGAN [6], MirrorGAN [7], and Obj-GAN [8]. These models vary in their architecture, from single GAN frameworks to multiple GANs and attention mechanisms.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 22,
    "chunk_index_in_section": 22,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 575,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c23",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "In our CGL-GAN model, we use local linguistic representations as conditional information for a single GAN discriminator, without an attention mechanism. We provide implementation details and hyper-parameter settings for our model's training process.\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 23,
    "chunk_index_in_section": 23,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 254,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c24",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "We introduce the hyper-parameters for the text encoder, generator, and discriminator. For the CUB dataset, the text encoder's hyper-parameters are set to 18 words and a 256-dimensional word embedding. For the MS-COCO dataset, these parameters are set to 16 and 256, respectively. The generator's hyper-parameter, the number of up-sampling blocks NG, is set to 5 for both 256 × 256 and 128 × 128 image generations. Similarly, the discriminator's hyper-parameter, the number of down-sampling blocks ND, is set to 5 for 256 × 256 images and 4 for 128 × 128 images. We use a 3 × 3 convolutional kernel w",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 24,
    "chunk_index_in_section": 24,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c25",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "6 × 256 images and 4 for 128 × 128 images. We use a 3 × 3 convolutional kernel with a stride of 1 in both the generator and discriminator.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 25,
    "chunk_index_in_section": 25,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 138,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c26",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "For a fair comparison, we adopt the pre-trained bidirectional LSTM from the AttnGAN model for our text encoder. We train our models using the Adam optimizer with a learning rate of 0.0001 and coefficients β1 and β2 set to 0 and 0.9, respectively. The generator and discriminator are updated simultaneously. Our implementation is on the PyTorch platform with an NVIDIA GeForce GTX 1080Ti GPU. The source code is publicly available.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 26,
    "chunk_index_in_section": 26,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 430,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c27",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "In the quantitative evaluation, we compare Inception scores and FIDs of baseline models and our proposed CGL-GAN model on the CUB and MS-COCO datasets. Our CGL-GAN model achieves a performance comparable to baseline models, often surpassing those without an attention mechanism on the CUB dataset and significantly outperforming them on the MS-COCO dataset. The scale of trainable parameters in our model is generally one order of magnitude smaller than that of baseline models. We further demonstrate the effectiveness of our single-discriminator model by comparing Inception scores with modified m",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 27,
    "chunk_index_in_section": 27,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c28",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "of our single-discriminator model by comparing Inception scores with modified multiple-discriminator models where all but one discriminator are removed.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 28,
    "chunk_index_in_section": 28,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 152,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c29",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "---\n\nAs demonstrated in Table II, our model achieves higher Inception scores on the CUB dataset compared to state-of-the-art methods not utilizing multiple discriminators. Our proposed model exhibits a 27.43% improvement in Inception score over GAN-INT-CLS, increasing from 2.88 to 3.67. This indicates our model's effectiveness in generating high-resolution images based on text descriptions, showcasing the benefits of incorporating both local and global linguistic representations in GANs.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 29,
    "chunk_index_in_section": 29,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 492,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c30",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "In the qualitative evaluation, our models with different hyperparameter settings are assessed on the CUB and MS-COCO datasets. Examples illustrate that our CGL-GAN model generates more realistic images than GAN-INT-CLS, GAWWN, StackGAN, and StackGAN-V2. For the MS-COCO dataset, our model produces images that are closer to real images, capturing fine-grained details that StackGAN fails to represent.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 30,
    "chunk_index_in_section": 30,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 401,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c31",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "An interesting observation is the discrepancy between the higher Inception score of StackGAN and the more realistic images generated by our model. This could be attributed to the limitations of the Inception score as a metric and the insufficient diversity of images produced by our model. Diversity is crucial for Inception scores, and while our images are of high quality, they may lack the variety needed for higher scores.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 31,
    "chunk_index_in_section": 31,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 426,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c32",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "The training process progression is visualized in Fig. 6, showing increasingly clearer images with more iterations. Initially, the images are mere color blocks, but as training progresses, the generator learns to incorporate details from the textual descriptions.\n\nIn the CPB analysis, we compare four variants of our CPB module on the CUB dataset. The results in Table III show that the variant combining global and local linguistic representations (CGL-GAN-LG) achieves the highest Inception score and the lowest FID, indicating the rationality of our CPB module design.\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 32,
    "chunk_index_in_section": 32,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 577,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c33",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "---\n\nFig. 6 illustrates the generation of image examples by our model across successive iterations. The top indicates the iteration numbers, followed by text descriptions and their corresponding synthesized images. This process demonstrates that the quality of generation improves with increasing iterations. The images have a resolution of 256 × 256.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 33,
    "chunk_index_in_section": 33,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 351,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c34",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "The performance of variants CGL-GAN-OG is barely recognizable, indicating that these variants cannot stably train their generators using the returned gradients. In contrast, variants conditioned on both global and local linguistic representations provide better gradients for training their generators. This experiment highlights the significant role of our proposed Control Population Block (CPB) in enhancing text-to-image synthesis models.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 34,
    "chunk_index_in_section": 34,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 442,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c35",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "We note that the Inception scores and FID of CGL-GAN-LG are slightly better than those of CGL-GAN-GL. The only difference between these models is the alignment of the granularity of text and image representations. CGL-GAN-LG aligns these representations with analogous granularity, which achieves better results.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 35,
    "chunk_index_in_section": 35,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 312,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c36",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "In the discriminator, we evaluate the alignment of the down-sampled feature map v4 with the local linguistic representation yL. Feature maps from earlier layers represent edges and angles, while those from later layers represent parts and objects. We consider the last two layers most likely to form semantic concepts used for aligning linguistic representations. Due to the larger size of feature maps from earlier layers, a fully-connected layer in a similar CPB would require more parameters and make GAN training stability difficult. Therefore, we align the local linguistic representation with",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 36,
    "chunk_index_in_section": 36,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c37",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "ability difficult. Therefore, we align the local linguistic representation with the second-to-last feature map and the global linguistic representation with the last feature map, demonstrating that effective alignment of visual and linguistic representations can improve overall performance.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 37,
    "chunk_index_in_section": 37,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 291,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c38",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "In conclusion, our study explores the use of both global and local linguistic representations for text-to-image synthesis by proposing a high-resolution synthesis model. The extensive experiments show that incorporating these representations significantly improves the performance of models generating high-resolution images. Compared to leading models based on multiple GANs, our model achieves comparable Inception scores and FIDs with fewer trainable parameters.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 38,
    "chunk_index_in_section": 38,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 465,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c39",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "Future work includes stacking our GANs to generate higher-resolution images and incorporating mechanisms such as attention or spatial information to further improve model performance.\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 39,
    "chunk_index_in_section": 39,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 188,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c40",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "[21] J.J. Zhao, M. Mathieu, and Y. Lecun, “Energy-based generative adversarial networks,” in Proc. Int. Conf. Learn. Representations, 2017. [Online]. Available: https://openreview.net/forum?id=ryh9pmcee \n[22] D. Berthelot, T. Schumm, and L. Metz, “BEGAN: Boundary equilibrium generative adversarial networks,” 2017. [Online]. Available: https://arxiv.org/abs/1703.10717 \n[23] A. Odena, C. Olah, and J. Shlens, “Conditional image synthesis with auxiliary classifier GANs,” in Proc. Int. Conf. Mach. Learn., 2017, pp. 2642–2651.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 40,
    "chunk_index_in_section": 40,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 526,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c41",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "[24] J. Song et al., “Binary generative adversarial networks for image retrieval,” in Proc. 32nd AAAI Conf. Artif. Intell., 2018, pp. 394–401. \n[25] T. Miyato and M. Koyama, “cGANs with projection discriminator,” in Proc. Int. Conf. Learn. Representations, 2018. [Online]. Available: https://openreview.net/forum?id=ByS1VpgRZ \n[26] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein GAN,” 2017. [Online]. Available: https://arxiv.org/abs/1701.07875",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 41,
    "chunk_index_in_section": 41,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 451,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c42",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "[27] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville, “Improved training of Wasserstein GANs,” in Proc. Advances Neural Inf. Process. Syst., 2017, pp. 5767–5777. \n[28] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida, “Spectral normalization for generative adversarial networks,” in Proc. Int. Conf. Learn. Representations, 2018. [Online]. Available: https://openreview.net/forum?id=B1QRgziT",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 42,
    "chunk_index_in_section": 42,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 413,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c43",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "[29] S. Hong, D. Yang, J. Choi, and H. Lee, “Inferring semantic layout for hierarchical text-to-image synthesis,” in Proc. IEEE Conf. Comput. Vision Pattern Recognit., 2018, pp. 7986–7994. \n[30] J. Johnson, A. Gupta, and L. Fei-Fei, “Image generation from scene graphs,” in Proc. IEEE Conf. Comput. Vision Pattern Recognit., 2018, pp. 1219–1228. \n[31] W. Xu, S. Keshmiri, and G. R. Wang, “Adversarially approximated autoencoder for image generation and manipulation,” IEEE Trans. Multimedia, vol. 21, no. 9, pp. 2387–2396, Sep. 2019.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 43,
    "chunk_index_in_section": 43,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 533,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c44",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "[32] Y. Guo et al., “Auto-embedding generative adversarial networks for high resolution image synthesis,” IEEE Trans. Multimedia, vol. 21, no. 11, pp. 2726–2737, Nov. 2019. \n[33] A. Brock, J. Donahue, and K. Simonyan, “Large scale GAN training for high fidelity natural image synthesis,” in Proc. Int. Conf. Learn. Representations, 2019. [Online]. Available: https://openreview.net/forum?id=B1xsqj09Fm \n[34] H. De Vries et al., “Modulating early visual processing by language,” in Proc. Advances Neural Inf. Process. Syst., 2017, pp. 6594–6604.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 44,
    "chunk_index_in_section": 44,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 544,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c45",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "[35] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proc. IEEE Conf. Comput. Vision Pattern Recognit., 2016, pp. 770–778. \n[36] J. H. Lim and J. C. Ye, “Geometric GAN,” 2017. [Online]. Available: https://arxiv.org/abs/1705.02894v1 \n[37] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie, “The caltech-UCSD birds-200-2011 dataset,” California Institute of Technology, Tech. Rep. CNS-TR-2011-001, 2011. [Online]. Available: http://www.vision.caltech.edu/visipedia/CUB-200-2011.html",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 45,
    "chunk_index_in_section": 45,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 530,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c46",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "[38] T.-Y. Lin et al., “Microsoft COCO: Common objects in context,” in Proc. Eur. Conf. Comput. Vision, 2014, pp. 740–755. \n[39] T. Salimans et al., “Improved techniques for training GANs,” in Proc. Advances Neural Inf. Process. Syst., 2016, pp. 2234–2242. \n[40] M. Heusel et al., “GANs trained by a two time-scale update rule converge to a Nash equilibrium,” in Proc. Advances Neural Inf. Process. Syst., 2017, pp. 6626–6637.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 46,
    "chunk_index_in_section": 46,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 426,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis_s0_c47",
    "source_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
    "text": "[41] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” in Proc. Int. Conf. Learn. Representations, 2015. [Online]. Available: https://iclr.cc/archive/www/doku.php?id=iclr2015:accepted-main.html\nRuifan Li, Ning Wang, Fangxiang Feng, Guangwei Zhang, Xiaojie Wang\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 47,
    "chunk_index_in_section": 47,
    "total_chunks_in_section": 48,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 285,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s0_c0",
    "source_id": "FAIA_372_FAIA230600",
    "text": "Enhanced Machine Reading Comprehension Method for Aspect Sentiment Quadruplet Extraction\n\nShuqin Ye, Zepeng Zhang, and Ruifan Li*\n\nSchool of Artiﬁcial Intelligence, Beijing University of Posts and Telecommunications, China\nEngineering Research Center of Information Networks, Ministry of Education, China\nKey Laboratory of Interactive Technology and Experience System, Ministry of Culture and Tourism, China\n{shuqinye, zepeng, rli}@bupt.edu.cn\nORCiD ID: Ruifan Li https://orcid.org/0000-0002-3543-6272",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 4,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 3,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 501,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s0_c1",
    "source_id": "FAIA_372_FAIA230600",
    "text": "Abstract. Aspect-Based Sentiment Analysis (ABSA) has seen significant attention for its ability to perform fine-grained sentiment analysis. Aspect Sentiment Quadruplet Extraction (ASQE) is a challenging task in ABSA, involving the extraction of aspect terms, opinion terms, sentiment polarities, and categories. We propose a novel Enhanced Machine Reading Comprehension (EMRC) method, formalizing ASQE as a multi-turn MRC task. EMRC effectively learns the relationships among subtasks by incorporating previously generated query answers. A hierarchical category classification strategy is designed f",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 4,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s0_c2",
    "source_id": "FAIA_372_FAIA230600",
    "text": "ted query answers. A hierarchical category classification strategy is designed for structured category prediction, and a bi-directional attention mechanism is employed. Extensive experiments on two benchmark datasets show EMRC outperforms state-of-art baselines. Source code is available at https://github.com/Little-Yeah/EMCR.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 4,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 327,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s1_c0",
    "source_id": "FAIA_372_FAIA230600",
    "text": "ABSA aims to extract and analyze sentiment information of aspect terms. It involves the identification of aspect terms, opinion terms, aspect categories, and sentiment polarities. ABSA has been decomposed into subtasks like Aspect Target Extraction (ATE), Opinion Target Extraction (OTE), Aspect Category Detection (ACD), and Aspect Sentiment Classiﬁcation (ASC). Recently, works have focused on simultaneous extraction of multiple sentiment elements. ASQE task is proposed to predict all four sentiment elements in the quadruplet form. Existing studies have neglected certain characteristics of ASQE",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 4,
    "chunk_index": 3,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 9,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 600,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s1_c1",
    "source_id": "FAIA_372_FAIA230600",
    "text": "quadruplet form. Existing studies have neglected certain characteristics of ASQE, leading to unsatisfactory results. We consider three challenges: exploiting associations among subtasks, dependencies among aspect and opinion term extractions, and the interrelated nature of extraction and classification subtasks.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 4,
    "chunk_index": 4,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 313,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s1_c2",
    "source_id": "FAIA_372_FAIA230600",
    "text": "*Corresponding Author. Email: rli@bupt.edu.cn.\n\nFigure 1. Example of sentimental elements and ASQE output for a restaurant review sentence. A quadruplet comprises (aspect, category, opinion, sentiment).",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 4,
    "chunk_index": 5,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 202,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s1_c3",
    "source_id": "FAIA_372_FAIA230600",
    "text": "The sentiment polarity classification aims to assign positive, negative, or neutral polarity to identified aspect-opinion pairs, while the category prediction task predicts the category of each aspect term. Achieving satisfactory performance in Aspect Sentiment Quadruplet Extraction (ASQE) requires modeling dependencies among subtasks. The challenge lies in accurately performing aspect category classification, particularly with multiple categories. ASQE extends the conventional Aspect Sentiment Triplet Extraction (ASTE) by adding category prediction, making its performance inherently tied to",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 4,
    "chunk_index": 6,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s1_c4",
    "source_id": "FAIA_372_FAIA230600",
    "text": "(ASTE) by adding category prediction, making its performance inherently tied to category prediction accuracy. The Laptop-ACOS dataset contains up to 121 categories, necessitating an effective strategy for dealing with multiple categories. Enhancing the input sentence representation is crucial for adapting to different subtasks in ASQE.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 4,
    "chunk_index": 7,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 337,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s1_c5",
    "source_id": "FAIA_372_FAIA230600",
    "text": "We propose an Enhanced Machine Reading Comprehension (EMRC) method for ASQE. This method formulates ASQE as a multi-turn machine reading comprehension task, introducing prior knowledge from previous turns to capture relations among subtasks. We devise a hierarchical category classification strategy to address the complexity of aspect categories and incorporate a bi-directional attention mechanism to enhance the association between context and queries.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 4,
    "chunk_index": 8,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 455,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s1_c6",
    "source_id": "FAIA_372_FAIA230600",
    "text": "Our contributions are:\n- A novel EMRC model that effectively builds associations among sentimental subtasks.\n- A hierarchical category classification strategy to improve the context representation's task-awareness.\n- Extensive experimental results demonstrating EMRC's superiority over existing baselines.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 4,
    "chunk_index": 9,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 305,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s1_c7",
    "source_id": "FAIA_372_FAIA230600",
    "text": "In related work, Aspect-Based Sentiment Analysis (ABSA) has gained interest, evolving from independent prediction of single elements to joint detection of multiple sentiment elements. ASQE is the most comprehensive and challenging task due to its complexity. Existing methods have ignored interactions among quadruplets, leading to potential error propagation. Machine Reading Comprehension (MRC) has emerged as a promising approach in ABSA, with several MRC-based models proposed for various subtasks.\n\n---",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 4,
    "chunk_index": 10,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 507,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s1_c8",
    "source_id": "FAIA_372_FAIA230600",
    "text": "---\n\nGiven a review sentence, our Enhanced Machine Reading Comprehension (EMRC) model employs a bi-directional extraction procedure to determine aspect-opinion pairs. These pairs are then used for sentiment and category classification to predict sentiment polarities and aspect categories, respectively, combining into a sentiment quadruplet.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 4,
    "chunk_index": 11,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 342,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s2_c0",
    "source_id": "FAIA_372_FAIA230600",
    "text": "3.1 Problem Formulation\nOur model aims to produce a set of quadruplets Q = {(a, c, o, s)m}|Q| m=1 from a sentence X = {x1, x2, · · · , xn}, where a is the aspect term, c its category, o the opinion term, and s the sentiment polarity.\n\n3.2 Query Construction",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 12,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 25,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 257,
    "chunk_method": "hierarchical",
    "importance_weight": 0.935,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s2_c1",
    "source_id": "FAIA_372_FAIA230600",
    "text": "3.2 Query Construction\n\n3.2.1 Bi-directional Extraction Query\nThe bi-directional extraction procedure handles Aspect Term Extraction (ATE) and Opinion Term Extraction (OTE) subtasks. Queries are constructed to identify aspects and corresponding opinions in both forward (A →O) and backward (O →A) directions.",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 13,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 308,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s2_c2",
    "source_id": "FAIA_372_FAIA230600",
    "text": "3.2.2 Hierarchical Category Classification Query\nOur strategy involves classifying aspects into general categories first, followed by subcategory classification, concatenating the two for the final category.\n\n3.2.3 Sentiment Classification Query\nThe sentiment classification query is designed to determine the sentiment polarity towards the given aspect and opinion terms.",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 14,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 372,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s2_c3",
    "source_id": "FAIA_372_FAIA230600",
    "text": "3.3 Bi-directional Attention\nTo capture the relationship between the input context and a given query, we adopt the bi-directional attention mechanism using BERT as the encoder. This mechanism incorporates context-to-query and query-to-context attention to create a task-aware representation.\n\n---",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 15,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 296,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s2_c4",
    "source_id": "FAIA_372_FAIA230600",
    "text": "---\n\nTo obtain the attention-enhanced representation ˆH, we duplicate ˆh for n times and combine these copies into a d-by-n matrix. We merge the original context representation H with the attention-enhanced representations \u0006 H and ˆH to obtain the task-aware representation ¨H:\n\n¨H:t = β(H:t, ˜H:t, ˆH:t),",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 16,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 305,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s2_c5",
    "source_id": "FAIA_372_FAIA230600",
    "text": "¨H:t = β(H:t, ˜H:t, ˆH:t), \n\nwhere β(a, b, c) = [a; b; a ⊙ b; a ⊙ c]. This bi-directional attention mechanism captures and links information from both queries and context for a comprehensive understanding of their relations.",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 17,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 224,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s2_c6",
    "source_id": "FAIA_372_FAIA230600",
    "text": "For answer prediction, we design multiple classifiers. Each query employs at least one unique classifier. We use eight classifiers to predict the starting or ending positions of the answer spans for four queries. For classification queries, the answer is predicted based on the representation of [CLS].\n\nThe training objective involves jointly learning subtasks in ASQE. The loss function is:\n\nL(θ) = LFT + LST + LS + LGC + LSC,",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 18,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 428,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s2_c7",
    "source_id": "FAIA_372_FAIA230600",
    "text": "L(θ) = LFT + LST + LS + LGC + LSC,\n\nwhere LFT and LST represent the losses for first-turn and second-turn extraction queries, and LS, LGC, and LSC are the losses for sentiment, general category, and subcategory classification, respectively.\n\nWe evaluate our method on two benchmark datasets: Restaurant-ACOS and Laptop-ACOS. The BERT-base encoder is used with specific training settings. We report Precision, Recall, and F1-score as evaluation metrics.",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 19,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 452,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s2_c8",
    "source_id": "FAIA_372_FAIA230600",
    "text": "Our EMRC model is compared with state-of-the-art baselines, including pipeline methods like Double-Propagation and Extract-Classify, and end-to-end methods like TAS-BERT and JET.\n\n---",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 20,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 183,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s2_c9",
    "source_id": "FAIA_372_FAIA230600",
    "text": "aspect-opinion pairs to form the final ACOS quadruplets. Generative methods include BARTABSA, which unifies all subtasks of Aspect-Based Sentiment Analysis (ABSA) by transforming them into a single generative formulation. GAS approaches ABSA tasks in a unified generative framework, formulating the task as a sentiment element sequence generation problem. Paraphrase detection aims to jointly detect all sentiment elements in quads, using a paraphrase modeling paradigm. Opinion tree generation dete",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 21,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 499,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s2_c10",
    "source_id": "FAIA_372_FAIA230600",
    "text": "a paraphrase modeling paradigm. Opinion tree generation detects all sentiment elements in a tree for a given review sentence, offering a semantic representation of sentiment element structure. Our EMRC method is based on Machine Reading Comprehension (MRC).\n4.4 Main Results",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 22,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 274,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s2_c11",
    "source_id": "FAIA_372_FAIA230600",
    "text": "The experimental results in Table 2 show that unified and generative approaches are more effective than pipeline methods. These approaches better capture the dependence between subtasks and mitigate error propagation. The proposed EMRC model outperforms all baselines in overall performance, achieving the highest precision scores on both datasets. In terms of recall and F1 score, our model demonstrates significant improvements compared to other baselines. The precision score's contribution to th",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 23,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 499,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s2_c12",
    "source_id": "FAIA_372_FAIA230600",
    "text": "to other baselines. The precision score's contribution to the F1 score boost indicates our model's higher reliability, which is crucial in real-world applications.",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 24,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 163,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s2_c13",
    "source_id": "FAIA_372_FAIA230600",
    "text": "Table 2. Experimental results (%) on two benchmark datasets.\n\nMETHOD Restaurant-ACOS Laptop-ACOS\n\nP R F1 P R F1",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 25,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 111,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s2_c14",
    "source_id": "FAIA_372_FAIA230600",
    "text": "P R F1 P R F1\n\nDP [24] 34.67 15.08 21.04 13.04 0.57 8.00\nExtract-Classify [3] 38.54 52.96 44.61 45.56 29.48 35.80\nJET [32] 59.81 28.94 39.01 44.52 16.25 23.81\nTAS-BERT [28] 26.29 46.29 33.53 47.15 19.22 27.31\nBARTABSA [34] 56.62 55.35 55.98 41.65 40.46 41.05\nGAS [39] 60.69 58.52 59.59 41.60 42.75 42.17\nParaphrase [38] 58.98 59.11 59.04 41.77 45.04 43.34\nOpinion tree generation [1] 63.96 61.74 62.83 46.11 44.79 45.44\nOur EMRC 64.97 61.18 63.02 47.27 44.66 45.92",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 26,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 464,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s2_c15",
    "source_id": "FAIA_372_FAIA230600",
    "text": "Table 3. F1 score (%) of ablation studies on hierarchical category classification strategy.\n\nDATASET METHOD QUADRUPLET CATEGORY\nRestaurant-ACOS EMRC 63.02 78.23\nw/o hierarchical 61.85 (1.17↓) 70.91 (7.32↓)\nLaptop-ACOS EMRC 45.92 60.58\nw/o hierarchical 43.52 (2.40↓) 47.79 (12.79↓)\n\n4.5 Ablation Study",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 27,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 300,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s2_c16",
    "source_id": "FAIA_372_FAIA230600",
    "text": "4.5 Ablation Study\n\nThe ablation study on both datasets demonstrates the effectiveness of the hierarchical category classification strategy and the bi-directional attention mechanism in EMRC. Removing these components leads to a significant decline in performance, especially in aspect category classification and quadruplet extraction.\n\nTable 4. F1 score (%) of ablation studies on bi-directional attention mechanism.\n\nDATASET METHOD SUBTASK ASQE ATE OTE AOPE ACD",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 28,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 464,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s2_c17",
    "source_id": "FAIA_372_FAIA230600",
    "text": "DATASET METHOD SUBTASK ASQE ATE OTE AOPE ACD\n\nRestaurant-ACOS EMRC 63.02 83.98 85.74 75.61 78.23\nw/o bi-directional attention 60.92 (2.1↓) 82.13 (1.85↓) 84.01 (1.73↓) 72.59 (3.02↓) 76.68 (1.55↓)\nLaptop-ACOS EMRC 45.92 82.95 89.79 77.32 60.58\nw/o bi-directional attention 42.46 (3.46↓) 81.52 (1.43↓) 88.31 (1.48↓) 75.26 (2.06↓) 59.84 (0.94↓)\n\n4.6 Attention Visualization",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 29,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 369,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s2_c18",
    "source_id": "FAIA_372_FAIA230600",
    "text": "4.6 Attention Visualization\n\nAttention visualization shows that the bi-directional attention mechanism in EMRC effectively focuses on query-related words, resulting in more reliable predictions.\n\n4.7 Case Study\n---",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 30,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 214,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s2_c19",
    "source_id": "FAIA_372_FAIA230600",
    "text": "A case study is presented in Figure 7, where two error cases are selected to examine the scenarios in which the methods might fail. In Example 1, the sentence 'Great service with amazon on fulfilling my order.' leads Extract-Classify to incorrectly classify the aspect 'service' into the category LAPTOP#OPERATION_PERFORMANCE, while our EMRC model correctly predicts SUPPORT#GENERA",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 31,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 381,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s2_c20",
    "source_id": "FAIA_372_FAIA230600",
    "text": "L. The hierarchical category classification strategy, as discussed in the ablation study, is crucial for managing the complexity of aspect categories. However, both models struggle to extract the aspect term 'service with amazon'. In Example 2, Paraphrase generates the opinion term 'best', which is not present in the original sentence 'The pizza is delicious and the proprietor is one of the nicest in NY",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 32,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 406,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s2_c21",
    "source_id": "FAIA_372_FAIA230600",
    "text": "C.', as the generative model tends to 'generate' rather than 'extract'. In contrast, our model extracts the opinion term 'nicest' from the given sentence to avoid such generation errors.\n5 Conclusion and Future Work",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 33,
    "chunk_index_in_section": 21,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 215,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s2_c22",
    "source_id": "FAIA_372_FAIA230600",
    "text": "Our paper introduces a novel EMRC model that frames ASQE as a multi-turn machine reading comprehension task. The model enhances dependencies and facilitates information flow among subtasks by incorporating the answer from the last turn into the current query. It also employs a hierarchical category classification strategy to address complex category situations and utilizes a bi-directional attention mechanism to improve context representation. Experimental results show that EMRC outperforms exi",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 34,
    "chunk_index_in_section": 22,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 499,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s2_c23",
    "source_id": "FAIA_372_FAIA230600",
    "text": "ntation. Experimental results show that EMRC outperforms existing baselines. Future work includes incorporating span-level representation to better predict aspect or opinion terms containing multiple words and developing a query-construction strategy that automatically constructs queries based on the given context.",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 35,
    "chunk_index_in_section": 23,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 316,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s2_c24",
    "source_id": "FAIA_372_FAIA230600",
    "text": "Acknowledgements\n\nThis work was partially supported by the National Natural Science Foundation of China under Grant 62076032 and the 111 Project under Grant B08004. We thank the anonymous reviewers for their constructive feedback.",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 2,
    "total_sections": 4,
    "chunk_index": 36,
    "chunk_index_in_section": 24,
    "total_chunks_in_section": 25,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 230,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8925,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s3_c0",
    "source_id": "FAIA_372_FAIA230600",
    "text": "[References listed in the original content are to be retained as they are]\n\nMao, Y., Shen, Y., Yu, C., & Cai, L. (2021). A joint training dual-mrc framework for aspect based sentiment analysis. In Proc. AAAI, 35, 13543–13551.\n\nPeng, H., Xu, L., Bing, L., Huang, F., Lu, W., & Si, L. (2020). Knowing what, how and why: A near complete solution for aspect-based sentiment analysis. In Proc. AAAI, 34, 8600–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 37,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 8,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 404,
    "chunk_method": "hierarchical",
    "importance_weight": 0.44000000000000006,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s3_c1",
    "source_id": "FAIA_372_FAIA230600",
    "text": "8607.\n\nPontiki, M., Galanis, D., Papageorgiou, H., Androutsopoulos, I., Manandhar, S., AL-Smadi, M., Al-Ayyoub, M., Zhao, Y., Qin, B., De Clercq, O., Hoste, V., Apidianaki, M., Tannier, X., Loukachevitch, N., Kotelnikov, E., Bel, N., Jiménez-Zafra, S. M., & Eryiğit, G. (2016). SemEval-2016 task 5: Aspect based sentiment analysis. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), 19–30.\n\nQiu, G., Liu, B., Bu, J., & Chen, C. (2011). Opinion word expansion and target extraction through double propagation. Computational Linguistics, 37(1), 9–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 38,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 8,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 583,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s3_c2",
    "source_id": "FAIA_372_FAIA230600",
    "text": "27.\n\nSeo, M., Kembhavi, A., Farhadi, A., & Hajishirzi, H. (2019). Bidirectional attention flow for machine comprehension. In Proc. ICLR.\n\nSun, K., Zhang, R., Mensah, S., Mao, Y., & Liu, X. (2019). Aspect-level sentiment analysis via convolution over dependency tree. In Proc. EMNLP-IJCNLP, 5679–5688.\n\nTai, C.-Y., Li, M.-Y., & Ku, L.-W. (2022). Hyperbolic disentangled representation for fine-grained aspect extraction. In Proc. AAAI, 36, 11358–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 39,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 8,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 445,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s3_c3",
    "source_id": "FAIA_372_FAIA230600",
    "text": "11366.\n\nWan, H., Yang, Y., Du, J., Liu, Y., Qi, K., & Pan, J. Z. (2020). Target-aspect-sentiment joint detection for aspect-based sentiment analysis. In Proc. AAAI, 34, 9122–9129.\n\nWang, W., Pan, S. J., Dahlmeier, D., & Xiao, X. (2016). Recursive neural conditional random fields for aspect-based sentiment analysis. In Proc. EMNLP, 616–626.\n\nWu, M., Wang, W., & Pan, S. J. (2020). Deep weighted maxsat for aspect-based opinion extraction. In Proc. EMNLP, 5618–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 40,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 8,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 461,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s3_c4",
    "source_id": "FAIA_372_FAIA230600",
    "text": "5628.\n\nXu, L., Chia, Y. K., & Bing, L. (2021). Learning span-level interactions for aspect sentiment triplet extraction. In Proc. ACL-IJCNLP, 4755–4766.\n\nXu, L., Li, H., Lu, W., & Bing, L. (2020). Position-aware tagging for aspect sentiment triplet extraction. In Proc. EMNLP, 2339–2349.\n\nXu, T., Yang, H., Wu, Z., Chen, J., Zhao, F., & Dai, X. (2023). Measuring your ASTE models in the wild: A diversified multi-domain dataset for aspect sentiment triplet extraction. In Findings of ACL: ACL, 2837–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 41,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 8,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 499,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s3_c5",
    "source_id": "FAIA_372_FAIA230600",
    "text": "2853.\n\nYan, H., Dai, J., Ji, T., Qiu, X., & Zhang, Z. (2021). A unified generative framework for aspect-based sentiment analysis. In Proc. ACL-IJCNLP, 2416–2429.\n\nZhai, Z., Chen, H., Feng, F., Li, R., & Wang, X. (2022). COM-MRC: A context-masked machine reading comprehension framework for aspect sentiment triplet extraction. In Proc. EMNLP, 3230–3241.\n\nZhai, Z., Chen, H., Li, R., & Wang, X. (2023). USSA: A unified table filling scheme for structured sentiment analysis. In Proc. ACL, 14340–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 42,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 8,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 494,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s3_c6",
    "source_id": "FAIA_372_FAIA230600",
    "text": "14353.\n\nZhang, M., Zhu, Y., Liu, Z., Bao, Z., Wu, Y., Sun, X., & Xu, L. (2023). Span-level aspect-based sentiment analysis via table filling. In Proc. ACL, 9273–9284.\n\nZhang, W., Deng, Y., Li, X., Yuan, Y., Bing, L., & Lam, W. (2021). Aspect sentiment quad prediction as paraphrase generation. In Proc. EMNLP, 9209–9219.\n\nZhang, W., Li, X., Deng, Y., Bing, L., & Lam, W. (2021). Towards generative aspect-based sentiment analysis. In Proc. ACL-IJCNLP, 504–",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 43,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 8,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 456,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "FAIA_372_FAIA230600_s3_c7",
    "source_id": "FAIA_372_FAIA230600",
    "text": "510.\n\nZhao, H., Huang, L., Zhang, R., Lu, Q., & Xue, H. (2020). Span-mlt: A span-based multi-task learning framework for pair-wise aspect and opinion terms extraction. In Proc. ACL, 3239–3248.\n\nZhou, X., Wan, X., & Xiao, J. (2015). Representation learning for aspect category detection in online reviews. In Proc. AAAI, 29.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 3,
    "total_sections": 4,
    "chunk_index": 44,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 8,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 323,
    "chunk_method": "hierarchical",
    "importance_weight": 0.42000000000000004,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding_s0_c0",
    "source_id": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding",
    "text": "Image Captioning Based on An Improved Transformer with IoU Position Encoding\n\nYazhou Li, Yihui Shi, Yun Liu, Ruifan Li, and Zhanyu Ma\nSchool of Artiﬁcial Intelligence, Beijing University of Posts and Telecommunications, Beijing, China",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 20,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 234,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding_s0_c1",
    "source_id": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding",
    "text": "Abstract: The image captioning task involves generating descriptive sentences for images. We propose an improved Transformer with IoU Position encoding model (TIP) to address the issues of vanishing query vectors and the lack of spatial information in the decoding process. TIP introduces an intra-modal attention mechanism and an Intersection-over-Union (IoU) spatial position encoding method. Experiments on MS-COCO datasets demonstrate the model's effectiveness.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 20,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 465,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding_s0_c2",
    "source_id": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding",
    "text": "I. INTRODUCTION\nImage captioning combines computer vision and natural language processing to describe image content in sentences. Traditional models use CNNs to extract features and RNNs for caption generation, often enhanced by attention mechanisms. However, these models have limitations in long-term memory and spatial information representation. We introduce TIP, which incorporates an intra-modal attention module and IoU-based spatial encoding to address these issues.\n\nII. RELATED WORKS\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 20,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 497,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding_s0_c3",
    "source_id": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding",
    "text": "In recent years, deep learning image captioning models inspired by the encoder-decoder framework for machine translation have been proposed. The Neural Image Captioning (NIC) model uses a convolutional neural network to extract image features and an LSTM to translate these into sentences. Jia et al. improved this by inputting image features at each decoding moment. Karpathy et al. extended this by using R-CNN to extract features from different image regions. Xu et al. introduced an attention mechanism to dynamically focus on different regions, which effectively improved model performance. How",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 20,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding_s0_c4",
    "source_id": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding",
    "text": "ly focus on different regions, which effectively improved model performance. However, this attention mechanism primarily focused on visual information, neglecting language aspects. Lu et al. addressed this with an adaptive attention mechanism.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 20,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 243,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding_s0_c5",
    "source_id": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding",
    "text": "All these models use an RNN-based encoder, which has limited long-term memory capabilities. To address this, we adopt the transformer structure as the decoder. For better applicability to image captioning, we propose an intra-modal attention module. Additionally, we recognize the importance of spatial information in the decoding process. Hu et al. used object coordinates as auxiliary information, but did not fully utilize the spatial relationships between objects. To enhance our TIP model's use of spatial relationships, we introduce an IoU spatial position encoding module.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 5,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 20,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 579,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding_s0_c6",
    "source_id": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding",
    "text": "Our TIP model consists of three main components: an image encoder, an IoU position module, and a caption decoder. The decoder uses a self-attention network to overcome the long-term dependency issue. We propose the intra-modal attention module to preserve image and text information, using a gate mechanism to selectively retain information. For position encoding, we fuse spatial and visual features by computing IoU between object bounding boxes and mapping the results to a consistent dimension with the object feature map, summing this with visual features for decoder input.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 6,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 20,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 579,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding_s0_c7",
    "source_id": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding",
    "text": "IV. EXPERIMENTAL SETTINGS\nA. Datasets and metrics\nOur TIP model's effectiveness is examined through experiments on the MS-COCO dataset [22], which includes 82,783 training, 40,504 validation, and 40,775 test images. We follow the dataset division method of [2] for consistent comparison with other baseline methods. The offline dataset contains 113,287 images with five annotated sentences each. Both validation and testing sets consist of 5000 unique images. We evaluate our method using CIDEr [23], BLEU [24], METEOR [25], ROUGE [26], and SPICE [27].",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 7,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 20,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 552,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding_s0_c8",
    "source_id": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding",
    "text": "B. Implementation details\nThe region detector identifies 36 regions of interest. Region features have a dimension of 2048, and the word embedding vector is set to 512. The network structure includes 6 layers for both the encoder and decoder and 8 multi-head attention mechanisms. The maximum sentence length is 16, the batch size is 10, and we use the Adam method [28] for parameter updates with a learning rate of 4 × 10−4, warmed up over 2000 steps. We train for 15 epochs using cross-entropy loss.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 8,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 20,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 500,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding_s0_c9",
    "source_id": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding",
    "text": "V. RESULTS AND ANALYSIS\nA. Result Comparison\nTable I shows that TIP achieves the highest scores in CIDEr and METEOR metrics among different models.\n\nB. On Spatial Encoding\nTable III presents the performance of different position encoding methods. The CoordNorm(hw) model shows improvements over the Coord(hw) and Coord models, indicating the effectiveness of normalization. The IoUc and IoU+ models further enhance performance by incorporating IoU information, which captures object relationships and aligns spatial information with image features.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 9,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 20,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 548,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding_s0_c10",
    "source_id": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding",
    "text": "C. On Beam Size\nTable IV demonstrates the impact of beam size on results. Larger beam sizes increase the search space for word sampling, improving performance metrics.\n\n---\n\nStandard: A man riding a snowboard down a snow-covered slope.\nOurs: Two people riding snowboards on a snowy slope.\nGT1: Two men riding snowboards in snow down a slope.\nGT2: Two people are snowboarding down a hill fast.\nGT3: Two men are riding snowboards in a snow slope.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 10,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 20,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 444,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding_s0_c11",
    "source_id": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding",
    "text": "Standard: A couple of boats sitting in the water.\nOurs: A boat sitting on top of a lake next to a shore.\nGT1: A boat full of Asian people sailed through reeds and bushes.\nGT2: A large boat filled with people navigate through a still lake.\nGT3: Around 15 people on a boat on a river going somewhere.\n\nStandard: A black cow standing in a field of tall grass.\nP: A cow standing in a grassy field next to a house.\nGT1: Two cows grazing on grass in a field by a house.\nGT2: A white-faced cow stands in the tall grass.\nGT3: A cow stands in the grassy area of a yard.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 11,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 20,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 560,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding_s0_c12",
    "source_id": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding",
    "text": "B: A man riding a skateboard down a sidewalk.\nP: A man riding a skateboard next to another man.\nGT1: Two men on skateboards on the pavement.\nGT2: Two young skateboarders skating near each other.\nGT3: Two men riding on their skateboards.\n\nFig. 3: Qualitative results of our TIP model.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 12,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 20,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 283,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding_s0_c13",
    "source_id": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding",
    "text": "Fig. 3: Qualitative results of our TIP model.\n\nInformation is lost and many better sentences are missed. When the beam search size is increased to 2 and 3, the search space of words increases, achieving the highest metric scores and optimal performance. At a beam size of 4, the model generates shorter sentences. High similarity between generated words and lack of diversity slightly decrease metrics. Beam size increase leads to higher memory usage and slower sentence generation.\n\nD. On Number of Network Layers",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 13,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 20,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 514,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding_s0_c14",
    "source_id": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding",
    "text": "D. On Number of Network Layers\n\nWe conduct experiments on the number of network layers in the transformer. Table V presents results for different encoder (EN) and decoder (DN) layer combinations. Deeper networks improve image captioning metric scores, with decoder layer increases providing larger improvements in BLEU-1, BLEU-2, BLEU-3, BLEU-4, and CIDEr metrics.\n\nE. Qualitative Results and Visualization",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 14,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 20,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 406,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding_s0_c15",
    "source_id": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding",
    "text": "E. Qualitative Results and Visualization\n\nQualitative analysis on randomly selected images (Fig. 3) shows that our TIP model generates sentences with richer semantic information. For instance, the model correctly identifies two people snowboarding where the standard model only recognizes one. The IoU position encoding method effectively identifies object relationships, resulting in generated sentences closer to the annotated ones.\n\nVI. CONCLUSION",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 15,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 20,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 450,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding_s0_c16",
    "source_id": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding",
    "text": "VI. CONCLUSION\n\nWe propose an improved transformer with IoU position encoding, TIP, which incorporates an intra-modal attention module and fuses visual and spatial features. Extensive experiments validate the effectiveness of these methods.\n\n---\n\nTABLE V PERFORMANCE COMPARISON USING DIFFERENT NUMBER OF LAYERS.\n\nEN DN B-1 B-2 B-3 B-4 R S C\n6 6 76.0 56.0 46.1 35.2 56.0 21.0 112.9\n8 6 75.8 59.4 45.6 34.7 55.9 20.8 112.8\n6 8 76.1 59.7 45.9 34.9 56.2 21.1 112.9\n8 8 76.1 59.7 45.8 34.9 56.0 21.2 113.2\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 16,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 20,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 505,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding_s0_c17",
    "source_id": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding",
    "text": "---\n\nKarpathy and Fei-Fei [2015] introduced deep visual-semantic alignments for generating image descriptions. Girshick et al. [2014] presented rich feature hierarchies for accurate object detection and semantic segmentation. Vaswani et al. [2017] proposed the attention mechanism as a sufficient neural network component. Hu et al. [2018] introduced relation networks for object detection. He et al. [2016] developed deep residual learning for image recognition. Ba et al. [2016] introduced layer normalization.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 17,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 20,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 512,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding_s0_c18",
    "source_id": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding",
    "text": "Lin et al. [2014] presented the Microsoft COCO dataset, which contains common objects in context. Vedantam et al. [2015] introduced CIDER, a consensus-based image description evaluation metric. Papineni et al. [2002] proposed BLEU, an automatic evaluation method for machine translation. Banerjee and Lavie [2005] introduced METEOR, an automatic metric for translation evaluation with improved correlation with human judgments. Lin [2004] developed ROUGE, a package for automatic evaluation of summaries. Anderson et al. [2016] proposed SPICE, a semantic propositional image caption evaluation metri\n. [2016] proposed SPICE, a semantic propositional image caption evaluation metric.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 18,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 20,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 682,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding_s0_c19",
    "source_id": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding",
    "text": "Kingma and Ba [2014] presented Adam, a method for stochastic optimization. Yang et al. [2016] introduced review networks for caption generation. Liu et al. [2017] improved image captioning via policy gradient optimization. Rennie et al. [2017] proposed self-critical sequence training for image captioning. Yao et al. [2017] boosted image captioning with attributes. Jiang et al. [2018] introduced a recurrent fusion network for image captioning.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 19,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 20,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 446,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improved+Eavesdropping+Detection+Strategy+Based+on+Extended+Three_particle+Greenberger_Horne_Zeilinger+State+in+Two_step+Quantum+Direct+Communication+Protocol_s0_c0",
    "source_id": "Improved+Eavesdropping+Detection+Strategy+Based+on+Extended+Three_particle+Greenberger_Horne_Zeilinger+State+in+Two_step+Quantum+Direct+Communication+Protocol",
    "text": "---\n\nImproved Eavesdropping Detection Strategy Based on Extended Three-particle Greenberger-Horne-Zeilinger State in Two-step Quantum Direct Communication Protocol\n\nLI Jian, YE Xinxin, LI Ruifan, ZOU Yongzhong and LU Xiaofeng",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 14,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 225,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improved+Eavesdropping+Detection+Strategy+Based+on+Extended+Three_particle+Greenberger_Horne_Zeilinger+State+in+Two_step+Quantum+Direct+Communication+Protocol_s0_c1",
    "source_id": "Improved+Eavesdropping+Detection+Strategy+Based+on+Extended+Three_particle+Greenberger_Horne_Zeilinger+State+in+Two_step+Quantum+Direct+Communication+Protocol",
    "text": "LI Jian, YE Xinxin, LI Ruifan, ZOU Yongzhong and LU Xiaofeng\n\nAbstract — An improved eavesdropping detection strategy using extended three-particle GHZ state is proposed to enhance the efficiency in two-step quantum direct communication protocol. The security analysis incorporates the entropy theory and compares the detection strategies quantitatively. The proposed strategy detects eavesdroppers with a rate of 59%, compared to 50% in the original protocol using EPR pair block. The security of the proposed protocol is discussed, demonstrating its enhanced security.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 570,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improved+Eavesdropping+Detection+Strategy+Based+on+Extended+Three_particle+Greenberger_Horne_Zeilinger+State+in+Two_step+Quantum+Direct+Communication+Protocol_s0_c2",
    "source_id": "Improved+Eavesdropping+Detection+Strategy+Based+on+Extended+Three_particle+Greenberger_Horne_Zeilinger+State+in+Two_step+Quantum+Direct+Communication+Protocol",
    "text": "Key words — Quantum key distribution (QKD), Dense coding, Extended three-particle GHZ state, Eavesdropping detection, Entropy.\n\nI. Introduction\n\nThe development of quantum information security methods such as QKD, quantum teleportation, and quantum secret sharing has led to the concept of Quantum secure direct communication (QSDC). An improved eavesdropping detection strategy based on extended three-particle GHZ state is proposed for the two-step quantum direct communication protocol. The analysis indicates that this improved protocol is more secure.\n\nII. The TSET Protocol",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 579,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improved+Eavesdropping+Detection+Strategy+Based+on+Extended+Three_particle+Greenberger_Horne_Zeilinger+State+in+Two_step+Quantum+Direct+Communication+Protocol_s0_c3",
    "source_id": "Improved+Eavesdropping+Detection+Strategy+Based+on+Extended+Three_particle+Greenberger_Horne_Zeilinger+State+in+Two_step+Quantum+Direct+Communication+Protocol",
    "text": "II. The TSET Protocol\n\nThe TSET protocol enhances eavesdropping detection efficiency by using extended three-particle GHZ states. The process includes:\n\n(S1) Agreement on encoding classical information in Bell states.\n(S2) Preparation of Bell states, insertion of extended three-particle GHZ states, and distribution.\n(S3) Eavesdropping detection through measurements by Alice and Bob.\n(S4) Encoding and transmitting messages using dense coding.\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 450,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improved+Eavesdropping+Detection+Strategy+Based+on+Extended+Three_particle+Greenberger_Horne_Zeilinger+State+in+Two_step+Quantum+Direct+Communication+Protocol_s0_c4",
    "source_id": "Improved+Eavesdropping+Detection+Strategy+Based+on+Extended+Three_particle+Greenberger_Horne_Zeilinger+State+in+Two_step+Quantum+Direct+Communication+Protocol",
    "text": "Alice applies operations u0, u1, u2, u3 to her particles as described by Eqs.(1), (2), (3), (4), transforming the state |φ+⟩ into |φ+⟩, |φ−⟩, |ψ+⟩, and |ψ−⟩, respectively. These operations correspond to the binary codes 00, 01, 10, and 11. To ensure secure transmission, Alice inserts decoy particles in sequence C, using extended three-particle GHZ states, which she measures with Z-bases without revealing their positions to Bob. Bob decrypts Alice’s message using a Bell-basis measurement on particles B and C. After particle C transmission, Alice reveals the decoy positions, and Bob performs a",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improved+Eavesdropping+Detection+Strategy+Based+on+Extended+Three_particle+Greenberger_Horne_Zeilinger+State+in+Two_step+Quantum+Direct+Communication+Protocol_s0_c5",
    "source_id": "Improved+Eavesdropping+Detection+Strategy+Based+on+Extended+Three_particle+Greenberger_Horne_Zeilinger+State+in+Two_step+Quantum+Direct+Communication+Protocol",
    "text": "particle C transmission, Alice reveals the decoy positions, and Bob performs a second eavesdropping check.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 5,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 106,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improved+Eavesdropping+Detection+Strategy+Based+on+Extended+Three_particle+Greenberger_Horne_Zeilinger+State+in+Two_step+Quantum+Direct+Communication+Protocol_s0_c6",
    "source_id": "Improved+Eavesdropping+Detection+Strategy+Based+on+Extended+Three_particle+Greenberger_Horne_Zeilinger+State+in+Two_step+Quantum+Direct+Communication+Protocol",
    "text": "The TSET protocol differs from TSE mainly in eavesdropping detection methods. The author of Ref.[17] calculates the maximum information (I) Eve can eavesdrop and the detection probability (d). The function I0 is provided for equal prior probabilities. The detection probabilities for the states |0⟩ and |1⟩ after Eve’s attack are computed, showing that the TSET protocol can detect eavesdropping more efficiently. Eve’s attack strategy is considered, where she performs a unitary attack operation ˆE on the composed system. However, since Eve does not know which particles are used for eavesdropping",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 6,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improved+Eavesdropping+Detection+Strategy+Based+on+Extended+Three_particle+Greenberger_Horne_Zeilinger+State+in+Two_step+Quantum+Direct+Communication+Protocol_s0_c7",
    "source_id": "Improved+Eavesdropping+Detection+Strategy+Based+on+Extended+Three_particle+Greenberger_Horne_Zeilinger+State+in+Two_step+Quantum+Direct+Communication+Protocol",
    "text": "tem. However, since Eve does not know which particles are used for eavesdropping detection, she treats all particles equally, resulting in a mixed state for the travel qubits.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 7,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 175,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improved+Eavesdropping+Detection+Strategy+Based+on+Extended+Three_particle+Greenberger_Horne_Zeilinger+State+in+Two_step+Quantum+Direct+Communication+Protocol_s0_c8",
    "source_id": "Improved+Eavesdropping+Detection+Strategy+Based+on+Extended+Three_particle+Greenberger_Horne_Zeilinger+State+in+Two_step+Quantum+Direct+Communication+Protocol",
    "text": "The detection probability without an eavesdropper is calculated, and the lower bound of the detection probability is derived. The amount of information Eve can gain is analyzed, and it is shown that for the same amount of information, Eve faces a higher detection probability in TSET, making it more secure.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 8,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 307,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improved+Eavesdropping+Detection+Strategy+Based+on+Extended+Three_particle+Greenberger_Horne_Zeilinger+State+in+Two_step+Quantum+Direct+Communication+Protocol_s0_c9",
    "source_id": "Improved+Eavesdropping+Detection+Strategy+Based+on+Extended+Three_particle+Greenberger_Horne_Zeilinger+State+in+Two_step+Quantum+Direct+Communication+Protocol",
    "text": "In conclusion, the TSET protocol securely transmits messages without leakage to potential eavesdroppers. It improves upon the TSE protocol by having Bob prepare particles, preventing Eve from capturing the home qubits' states. The TSET protocol also includes a second eavesdropping detection, allowing for the reuse of particles for secure message transmission. Future work will explore the security of other two-step quantum direct communication protocols and their improvements.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 9,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 480,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improved+Eavesdropping+Detection+Strategy+Based+on+Extended+Three_particle+Greenberger_Horne_Zeilinger+State+in+Two_step+Quantum+Direct+Communication+Protocol_s0_c10",
    "source_id": "Improved+Eavesdropping+Detection+Strategy+Based+on+Extended+Three_particle+Greenberger_Horne_Zeilinger+State+in+Two_step+Quantum+Direct+Communication+Protocol",
    "text": "Bennett and Brassard's seminal work on quantum cryptography introduced the concept of public-key distribution and coin tossing in 1984 [1]. Subsequent advancements in the field include the teleportation of an unknown quantum state via dual classical and Einstein-Podolsky-Rosen channels [2], and the development of quantum teleportation with a complete Bell state measurement [3]. Notably, Li et al. proposed improved quantum \"Ping-pong\" protocols based on GHZ and genuine entangled states, as well as classical XOR and CNOT operations [4, 6, 8]. Secure quantum communication has been explored using",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 10,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improved+Eavesdropping+Detection+Strategy+Based+on+Extended+Three_particle+Greenberger_Horne_Zeilinger+State+in+Two_step+Quantum+Direct+Communication+Protocol_s0_c11",
    "source_id": "Improved+Eavesdropping+Detection+Strategy+Based+on+Extended+Three_particle+Greenberger_Horne_Zeilinger+State+in+Two_step+Quantum+Direct+Communication+Protocol",
    "text": "CNOT operations [4, 6, 8]. Secure quantum communication has been explored using cluster states and Bell-basis measurements [7], and various strategies for enhancing the security of quantum communication protocols have been proposed [10, 12, 13].",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 11,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 245,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improved+Eavesdropping+Detection+Strategy+Based+on+Extended+Three_particle+Greenberger_Horne_Zeilinger+State+in+Two_step+Quantum+Direct+Communication+Protocol_s0_c12",
    "source_id": "Improved+Eavesdropping+Detection+Strategy+Based+on+Extended+Three_particle+Greenberger_Horne_Zeilinger+State+in+Two_step+Quantum+Direct+Communication+Protocol",
    "text": "In the realm of quantum secret sharing, Hillery et al. [14] and Singh and Srikanth [15] contributed to the understanding of sharing secrets securely using quantum mechanics. Long and Liu [16] introduced a theoretically efficient high-capacity quantum-key distribution scheme, while Deng et al. [17, 18] proposed quantum direct communication protocols. The efficiency of different detection strategies for the \"Ping-pong\" protocol has been compared [20], and eavesdropping concerns in such protocols have been addressed [21, 22].",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 12,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 528,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improved+Eavesdropping+Detection+Strategy+Based+on+Extended+Three_particle+Greenberger_Horne_Zeilinger+State+in+Two_step+Quantum+Direct+Communication+Protocol_s0_c13",
    "source_id": "Improved+Eavesdropping+Detection+Strategy+Based+on+Extended+Three_particle+Greenberger_Horne_Zeilinger+State+in+Two_step+Quantum+Direct+Communication+Protocol",
    "text": "The authors of this work, LI Jian, YE Xinxin, LI Ruifan, ZOU Yongzhong, and LU Xiaofeng, are associated with Beijing University of Posts and Telecommunications, with research interests spanning quantum information, quantum computation, quantum communication security, information security, and artificial intelligence.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 13,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 14,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 318,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image_s0_c0",
    "source_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image",
    "text": "---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 33,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 3,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image_s0_c1",
    "source_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image",
    "text": "Clothing images are abundant, particularly on e-commerce platforms due to the growth of e-business. Recognizing and retrieving these images is significant for commercial and social applications, attracting attention from multimedia processing and computer vision. The challenges include variations in clothing appearance and style, multiple categories and attributes, and the prevalence of erroneous or incomplete labels from retailers. Additionally, the imbalance among image categories hinders effective learning. To address these issues, we propose a multi-task deep learning framework and multi-",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image_s0_c2",
    "source_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image",
    "text": "address these issues, we propose a multi-task deep learning framework and multi-weight convolutional neural networks for imbalance learning. The network structure consists of shared bottom layers and task-dependent top layers. Category-relevant parameters are included to regularize the learning process. We have collected a large-scale dataset containing approximately one million shop photos from four Chinese retailers. Experiments demonstrate that our framework and networks effectively learn robust representations and achieve improved performance.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 553,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image_s0_c3",
    "source_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image",
    "text": "Keywords—Clothing Image recognition; Convolutional neural network; Multi-task; Multi-weight\n\nI. INTRODUCTION",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 108,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image_s0_c4",
    "source_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image",
    "text": "The availability of clothing images on electronic commercial platforms, such as taobao.com and amazon.com, has led to numerous potential commercial and social applications. The challenge lies in learning effective representations from noisy labels and handling the imbalance among categories within large-scale clothing image data. This paper introduces multi-weight neural networks to address these challenges and improve clothing image retrieval in the real world. We focus on modeling the relationship between real-world images and their unreliable labels, learning robust representations for eff",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image_s0_c5",
    "source_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image",
    "text": "orld images and their unreliable labels, learning robust representations for effective retrieval. The proposed multi-weight CNNs group related categories and attributes, share a common representation, and adapt weights to regularize gradients for different categories. We evaluate our method on a large, complex, and real-world clothing image dataset, e-Clothing1.4M, and demonstrate its effectiveness in dealing with label errors and data imbalance.\nII. RELATED WORK\n---\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 5,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 476,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image_s0_c6",
    "source_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image",
    "text": "With the increasing business value of the shopping industry, automatic clothing image analysis has gained significant attention. Attribute learning has been a key trend, offering fine-grained descriptions for clothing images, widely explored in the computer vision community. However, the major challenge is the lack of well-labeled training data due to the high cost of human labor and the need for domain-specific knowledge. Berg et al. proposed obtaining attributes and visual appearance by mining image descriptive text from webpages. Chen et al. focused on learning attributes of upper-body clo",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 6,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image_s0_c7",
    "source_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image",
    "text": "text from webpages. Chen et al. focused on learning attributes of upper-body clothing, while Shankar et al. discovered attributes in an image using deep neural networks in a weakly supervised scenario. These works treat attribute learning as a single task, often neglecting the relationship with clothing categories.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 7,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 316,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image_s0_c8",
    "source_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image",
    "text": "Clothing image analysis also relates to methods in pose estimation and person detection, as human recognition is tied to clothing image recognition. Clothing parsing predicts semantic categories for each pixel in an image, which can be used for clothing recognition. Liu et al. addressed the cross-scenario problem between daily human photos and clothing shop photos, while Kalantidis et al. and Yamaguchi et al. considered pose estimation and unconstrained clothing parsing, respectively.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 8,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 489,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image_s0_c9",
    "source_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image",
    "text": "Deep learning, introduced in 2006, has been applied to various computer vision tasks, with supervised CNNs and unsupervised autoencoders and restricted Boltzmann machines being particularly successful. Deep learning's advantage in multi-task learning makes it a candidate for large-scale clothing image analysis. Recent methods have proposed deep learning for multi-task learning, with Zhang et al. combining part-based models and CNNs under a multi-task framework. However, these frameworks are often designed for small-scale datasets and do not easily extend to large-scale problems.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 9,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 585,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image_s0_c10",
    "source_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image",
    "text": "In this section, we present our multi-weight CNN model and learning algorithm. The model consists of task-independent and task-dependent layers, with weights denoted as Wc and Wt, respectively. Hyperparameters γt balance losses from different categories. The mapping of task-independent layers is φc(·), and that of specific task layers is φt(·). The multi-weight network parameters are learned through an optimization problem, with the output layer adopting a multi-label structure and the loss function being sigmoid cross-entropy.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 10,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 533,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image_s0_c11",
    "source_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image",
    "text": "Our objective is to optimize the function J by minimizing the sum of the losses over all tasks. The gradients of the objective function with respect to Wt and Wc are calculated, and the multi-weight CNNs are trained using stochastic gradient descent. The training algorithm is summarized in Algorithm 1.\n\nAlgorithm 1 Learning Algorithm\n---\n\n---\n\n---\n\n1: Initialize the common weights Wc and task-specific weights {Wt} for T tasks, set learning rates ϵc for Wc and ϵt for Wt, and the weight adaptation parameter γt for each task.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 11,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 528,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image_s0_c12",
    "source_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image",
    "text": "2: Compute the common representations for a mini-batch of images Ib, oc ←φc(Ib; Wc).\n\n3: Compute the task-specific outputs, ot ←φt(oc; Wt).\n\n4: Compute the gradients ∇Wc and ∇Wt with respect to the objective function.\n\n5: Update the common weights, Wc ←Wc + ϵc · ∇WcJ.\n\n6: Update the task-specific weights, Wt ←Wt + ϵt · ∇WtJ.\n\n7: Repeat Steps 2 - 6 until convergence.\n\nIV. EXPERIMENTS\n\nA. Dataset",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 12,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 397,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image_s0_c13",
    "source_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image",
    "text": "7: Repeat Steps 2 - 6 until convergence.\n\nIV. EXPERIMENTS\n\nA. Dataset\n\nWe collected e-Clothing1.4M, a real-world dataset. It consists of 1,462,438 pairs of images and labels of clothing types and attributes. The dataset is divided into training and test sets, with 1,069,901 and 392,537 images, respectively.\n\nB. Evaluation Criteria\n\nWe use the mean Average Precision (mAP) metric. The mAP score is obtained by averaging the AP of all queries, using an image retrieval system based on the learned representation.\n\nC. Methods and Settings",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 13,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 537,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image_s0_c14",
    "source_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image",
    "text": "C. Methods and Settings\n\nWe compare multi-weight CNNs with multi-label CNNs and multi-task CNNs on the e-Clothing1.4M dataset. Implementations are based on the Caffe deep learning framework and the AlexNet architecture. Experiments are conducted on a workstation with an NVIDIA K20c GPU.\n\nD. Experimental Results\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 14,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 316,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image_s0_c15",
    "source_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image",
    "text": "The experimental results on the eClothing1.4M dataset are presented in Table II. All values are mAP scores. The category names in the leftmost column of Table II are ordered by their normalized proportion, with \"Color\" at the top and \"Wool Thickness\" at the bottom. The results of three CNN-based methods—multi-label, multi-task, and multi-weight—are shown in subsequent columns, with the number of iterations indicated. The mAP value in bold represents the best performance among the three methods. The multi-label CNNs perform best in seven categories, multi-task CNNs in five, and multi-weight CN",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 15,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image_s0_c16",
    "source_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image",
    "text": "s perform best in seven categories, multi-task CNNs in five, and multi-weight CNNs in twelve. Across all 24 categories, the average mAP values are 52.48% for multi-label, 53.11% for multi-task, and 55.23% for multi-weight. Multi-weight CNNs show the best performance overall and particularly improve in categories with smaller proportions, such as \"Waist,\" \"Feather,\" and \"Sleeve.\"",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 16,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 381,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image_s0_c17",
    "source_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image",
    "text": "For setting weights in the multi-weight neural networks, we propose a smoothing function to address the imbalance issue. The function is defined as:\n\n\\[ rn \\text{ if } rn \\geq \\frac{ravg}{c}, \\frac{ravg}{c} \\text{ if } rn \\leq \\frac{ravg}{c} \\]\n\nHere, \\( rn \\) is the sample ratio of the nth category to the maximum, \\( ravg \\) is the mean ratio, and \\( c \\) is set to 3.0. This regularization strategy effectively learns from the data. Using the new weights, the proposed method outperforms others in nearly all categories, as shown in Table III.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 17,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 547,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image_s0_c18",
    "source_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image",
    "text": "In conclusion, we propose a multi-weight convolutional neural network to handle noisy and imbalanced clothing images. Our method demonstrates effectiveness on a large-scale dataset and could be further improved by incorporating other convolutional networks. Future work includes investigating the impact of weights on network convergence.\n\n---\n\nTABLE III mAP SCORES FOR EACH CATEGORY USING WEIGHT ADAPTATION WITH THEIR NUMBER OF ITERATIONS.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 18,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 440,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image_s0_c19",
    "source_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image",
    "text": "Category | Proportion | Current Best (%) | multi-weight@i45 (%) | 1/3Ratio@i45 (%)\n--- | --- | --- | --- | ---\nColor | 1.0000 | 35.24 | 34.66 | 36.03\nGender | 0.9646 | 99.52 | 99.52 | 99.55\nSenson | 0.8242 | 70.37 | 70.37 | 70.76\nStyle | 0.7773 | 34.40 | 33.78 | 34.34\nCoat | 0.6838 | 74.82 | 74.25 | 75.20\nVersion | 0.6062 | 42.75 | 42.62 | 43.66\nPantsuit | 0.6018 | 57.05 | 56.01 | 57.28\nLength of Sleeve | 0.4200 | 64.19 | 64.19 | 64.57\nOccation | 0.3875 | 45.61 | 45.61 | 45.84\nCraft | 0.3119 | 18.14 | 17.32 | 18.15\nColar | 0.2436 | 33.71 | 33.71 | 35.27",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 19,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 559,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image_s0_c20",
    "source_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image",
    "text": "Craft | 0.3119 | 18.14 | 17.32 | 18.15\nColar | 0.2436 | 33.71 | 33.71 | 35.27\nLength of Trousers | 0.2292 | 72.97 | 72.97 | 73.90\nThickness | 0.1961 | 69.54 | 69.54 | 70.39\nLength of Skirts | 0.1866 | 59.44 | 59.44 | 59.41\nSkirt | 0.1766 | 44.42 | 44.22 | 45.26\nCrowd | 0.1456 | 77.51 | 77.51 | 77.93\nDense | 0.1331 | 94.21 | 94.08 | 94.42\nLeather | 0.0643 | 59.76 | 58.48 | 60.40\nCardigan | 0.0626 | 39.77 | 38.61 | 41.42\nWaist | 0.0471 | 49.25 | 49.25 | 51.80\nFeather | 0.0428 | 80.10 | 80.10 | 80.47\nSleeve | 0.0064 | 28.75 | 28.75 | 29.57\nCheongsam | 0.0006 | 33.84 | 33.65 | 41.08\nCheongsam | 0.0006 | 33.84 | 33.65 | 41.08\nWool Thickness | 0.0002 | 26.42 | 26.27 | 28.43",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 20,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 676,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image_s0_c21",
    "source_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image",
    "text": "TABLE IV mAP SCORES FOR COAT V.S. PANTSUIT WITH THE RATIO 10:1\n\nCategory | Proportion | Multi-label (%) | Multi-task (%) | Multi-weight (%)\n--- | --- | --- | --- | ---\nCoat | 1 | 57.47 | 58.52 | 57.86\nPantsuit | 0.1 | 31.94 | 32.28 | 33.25\n\nTABLE V mAP SCORES FOR LENGTH OF SLEEVES V.S. SLEEVE WITH THE RATIO 10:1\n\nCategory | Proportion | Multi-label (%) | Multi-task (%) | Multi-weight (%)\n--- | --- | --- | --- | ---\nLength of Sleeve | 1 | 46.71 | 43.06 | 45.58\nSleeve | 0.1 | 13.58 | 15.43 | 17.04\n\nFig. 2. Performance comparison over for all twenty-four categories using four kinds of methods.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 21,
    "chunk_index_in_section": 21,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 597,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image_s0_c22",
    "source_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image",
    "text": "[5] D. Shankar, S. Narumanchi, H. A. Ananya, P. Kompalli, and K. Chaud- hury, “Deep learning based large scale visual recommendation and search for e-commerce,” arXiv:1703.02344 [cs.CV], 2017.\n[6] A. Zhai, D. Kislyuk, Y. Jing, M. Feng, E. Tzeng, J. Donahue, Y. L. Du, and T. Darrell, “Visual discovery at pinterest,” arXiv:1702.04680 [cs.CV], 2017.\n[7] O. Russakovsky and L. Fei-Fei, “Attribute learning in large-scale datasets,” in European Conference of Computer Vision (ECCV), Inter- national Workshop on Parts and Attributes, Crete, Greece, September 2010.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 22,
    "chunk_index_in_section": 22,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 560,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image_s0_c23",
    "source_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image",
    "text": "[8] N. Kumar, A. C. Berg, P. N. Belhumeur, and S. K. Nayar, “Describable Visual Attributes for Face Veriﬁcation and Image Search.” IEEE Trans- actions on Pattern Analysis and Machine Intelligence, vol. 33, no. 10, pp. 1962–1977, 2011.\n[9] H. Chen, A. Gallagher, and B. Girod, “Describing clothing by seman- tic attributes,” in Proc. of European Conference on Computer Vision (ECCV’12), Firenze, Italy, 2012, pp. 609–623.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 23,
    "chunk_index_in_section": 23,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 420,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image_s0_c24",
    "source_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image",
    "text": "[10] T. L. Berg, A. C. Berg, and J. Shih, “Automatic Attribute Discovery and Characterization from Noisy Web Data,” in European Conference on Computer Vision (ECCV), no. PART 1, 2010, pp. 663–676.\n[11] W. Di, C. Wah, A. Bhardwaj, and R. Piramuthu, “Style ﬁnder: Fine- grained clothing style detection and retrieval,” in Computer Vision and Pattern Recognition Workshops, 2013, pp. 8–13.\n[12] S. Shankar, “DEEP-CARVING : Discovering Visual Attributes by Carv- ing Deep Neural Nets,” in CVPR, 2015.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 24,
    "chunk_index_in_section": 24,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 496,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image_s0_c25",
    "source_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image",
    "text": "[13] K. Lin, H. F. Yang, K. H. Liu, J. H. Hsiao, and C. S. Chen, “Rapid cloth- ing retrieval via deep learning of binary codes and hierarchical search,” in Proc. ACM International Conference on Multimedia Retrieval, 2015, pp. 499–502.\n[14] Q. Dong, S. Gong, and X. Zhu, “Multi-task curriculum transfer deep learning of clothing attributes,” arXiv:1610.03670 [cs.CV], 2016.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 25,
    "chunk_index_in_section": 25,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 372,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image_s0_c26",
    "source_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image",
    "text": "[15] Si Liu, Zheng Song, Guangcan Liu, Changsheng Xu, Hanqing Lu, and Shuicheng Yan, “Street-to-shop: Cross-scenario clothing retrieval via parts alignment and auxiliary set,” in 2012 IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 2012, pp. 3330–3337.\n[16] K. Yamaguchi, M. H. Kiapour, L. E. Ortiz, and T. L. Berg, “Parsing clothing in fashion photographs,” in Computer Vision and Pattern Recognition, 2012, pp. 3570–3577.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 26,
    "chunk_index_in_section": 26,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 441,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image_s0_c27",
    "source_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image",
    "text": "[17] Y. Kalantidis, L. Kennedy, and L.-J. Li, “Getting the look: Clothing recognition and segmentation for automatic product suggestions in everyday photos,” in Proceedings of the 3rd ACM Conference on International Conference on Multimedia Retrieval, ser. ICMR ’13. New York, NY, USA: ACM, 2013, pp. 105–112.\n[18] K. Yamaguchi, M. H. Kiapour, L. E. Ortiz, and T. L. Berg, “Retrieving Similar Styles to Parse Clothing,” IEEE transactions on pattern analysis and machine intelligence, vol. 37, no. 5, pp. 1028–40, 2015.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 27,
    "chunk_index_in_section": 27,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 518,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image_s0_c28",
    "source_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image",
    "text": "[19] P. Tangseng, Z. Wu, and K. Yamaguchi, “Looking at outﬁt to parse clothing,” arXiv:1703.01386 [cs.CV], 2017.\n[20] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521, no. 7553, pp. 436–444, May 2015.\n[21] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet classiﬁcation with deep convolutional neural networks,” in Advances in Neural Infor- mation Processing Systems 25, F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, Eds. Curran Associates, Inc., 2012, pp. 1097–1105.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 28,
    "chunk_index_in_section": 28,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 511,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image_s0_c29",
    "source_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image",
    "text": "[22] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition,” arXiv:1409.1556 [cs.CV], 2015.\n[23] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” Computer Vision and Pattern Recognition, pp. 770–778, 2016.\n[24] F. Feng, R. Li, and X. Wang, “Deep correspondence restricted Boltz- mann machine for cross-modal retrieval,” Neurocomputing, vol. 154, no. C, pp. 50–60, 2015.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 29,
    "chunk_index_in_section": 29,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 449,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image_s0_c30",
    "source_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image",
    "text": "[25] N. Zhang, M. Paluri, M. Ranzato, T. Darrell, and L. D. Bourdev, “PANDA: Pose Aligned Networks for Deep Attribute Modeling.” CVPR, pp. 1637–1644, 2014.\n[26] Y. Bai, K. Yang, W. Yu, W.-Y. Ma, and T. Zhao, “Learning High- level Image Representation for Image Retrieval via Multi-Task DNN using Clickthrough Data.” IEEE Winter Conference on Applications of Computer Vision (WACV), 2013.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 30,
    "chunk_index_in_section": 30,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 387,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image_s0_c31",
    "source_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image",
    "text": "[27] D. Wang, X. Gao, X. Wang, L. He, and B. Yuan, “Multimodal discrim- inative binary embedding for large-scale cross-modal retrieval,” IEEE Transactions on Image Processing A Publication of the IEEE Signal Processing Society, vol. 25, no. 10, pp. 1–1, 2016.\n[28] E. Simoserra and H. Ishikawa, “Fashion style in 128 ﬂoats: Joint ranking and classiﬁcation using weak data for feature extraction,” in IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 298– 307.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 31,
    "chunk_index_in_section": 31,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 479,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image_s0_c32",
    "source_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image",
    "text": "[29] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell, “Caffe: Convolutional architecture for fast feature embedding,” arXiv preprint arXiv:1408.5093, 2014.\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 32,
    "chunk_index_in_section": 32,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 209,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations_s0_c0",
    "source_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
    "text": "IMPROVING IMAGE PARAGRAPH CAPTIONING WITH DUAL RELATIONS\n\nYun Liu, Yihui Shi, Fangxiang Feng, Ruifan Li, Zhanyu Ma, Xiaojie Wang\nSchool of Artificial Intelligence, Beijing University of Posts and Telecommunications, China\nBeijing Academy of Artificial Intelligence, Beijing, China",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 9,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 280,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations_s0_c1",
    "source_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
    "text": "Abstract:\nImage paragraph captioning generates multiple descriptive sentences for an image. We propose DualRel, a model that captures spatial and semantic relations among objects. Spatial relation embedding is derived from images using a geometry pattern, while semantic relation embedding is learned from captions in a weakly supervised manner. These embeddings interact with regional features through relation-aware attention. DualRel achieves significant improvements on the Stanford benchmark dataset.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 505,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations_s0_c2",
    "source_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
    "text": "1. Introduction:\nImage paragraph captioning is more challenging than sentence captioning as it requires detailed visual and linguistic information. Existing methods do not explicitly model object relations, leading to suboptimal performance. DualRel is introduced to explicitly capture semantic and spatial relations for improved paragraph captioning.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 351,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations_s0_c3",
    "source_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
    "text": "2. Related Work:\nImage paragraph captioning methods are categorized into implicit and explicit relation modeling. Implicit methods enhance coherence and fine-grained information but do not explicitly capture object relations. Explicit methods, such as OR-ATT, infer relations but differ from DualRel in considering specific semantic and spatial relations and using relation-aware interaction.\n\n3. Proposed Model: \n(Continued in the next section of the paper)",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 458,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations_s0_c4",
    "source_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
    "text": "3. Proposed Model: \n(Continued in the next section of the paper)\n\nImage paragraph captioning involves generating a paragraph Y ={y1, · · · , yT} with T words for a given image. Our DualRel model's architecture is depicted in Fig. 2.\n\n3.1. Relation Embedding Module",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 264,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations_s0_c5",
    "source_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
    "text": "3.1. Relation Embedding Module\n\nSpatial Relation Encoder: We employ Faster R-CNN [19] to detect N objects in an image, represented as C={c1, · · · , cN}. The visual features V ={v1, · · · , vN}, where vk ∈R2048, are extracted along with bounding boxes B={b1, · · · , bN}. Spatial relations, such as 'in front of', are captured using embeddings P = {pij : pij ∈RD}, defined by a geometry pattern. The geometric relation between two bounding boxes is vectorized as λ(i, j) ∈R4. We project this into a higher-dimensional space as Eb(i, j), and the spatial relation embeddings pij are computed as:",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 5,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 593,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations_s0_c6",
    "source_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
    "text": "pij = fp \u0000 Concat(Eb(i, j), v′′ i , v′′ j )\n\nSemantic Relation Encoder: This encoder captures semantic relations, like 'flying', between objects. It utilizes both visual perception and language knowledge. The object category embedding Ec(i, j) is defined as:\n\nEc(i, j) = ReLU (WcConcat(Wgci, Wgcj) + bc)\n\nThe semantic relation embeddings E = {eij : eij ∈RD} are given by:\n\neij = fe \u0000 Concat(Eb(i, j), Ec(i, j), v′ i, v′ j)",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 6,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 422,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations_s0_c7",
    "source_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
    "text": "eij = fe \u0000 Concat(Eb(i, j), Ec(i, j), v′ i, v′ j)\n\nA weakly supervised multi-label classifier is designed to enhance semantic relations. The semantic relation embedding eij is fed into a linear layer to obtain category scores.\n\n3.2. Relation-aware Interaction Module\n\nRelation-aware Attentions: We construct relation-aware attentions using object regional features V and semantic and spatial relation embeddings {E, P}. Visual context vector Cv t is obtained, followed by the generation of semantic and spatial relation context vectors Ce t and Cp t.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 7,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 550,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations_s0_c8",
    "source_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
    "text": "Fusion Gates: We introduce fusion gates to combine the context vectors Cv t, Ce t, and Cp t for language decoding. A relation gate gr and a visual gate gv are designed to control the fusion process, resulting in the relation-aware visual context vector C′ t and the final context vector Ct.\n\n3.3. Objective and Learning\n\nOur loss function ℓ combines a word-level loss ℓXE and a semantic relation classification loss ℓR. We optimize the model with self-critical sequence training (SCST).\n\nTable 1 presents a comparison of results with state-of-the-art models.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 8,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 558,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations_s1_c0",
    "source_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
    "text": "4.1. Dataset, Metrics and Settings\n---\n\nDataset and Metrics: We utilize the Stanford benchmark dataset [1], which includes 14575/2487/2489 pairs for training/validation/test. The dataset comprises an average of 67.5 words per paragraph and 5.7 sentences. Evaluation metrics include BLEU@{1, 2, 3, 4} [23], METEOR [24], CIDEr [25], and BERTScore [26] F metrics.",
    "section_title": "EXPERIMENT",
    "section_type": "experiment",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 9,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 19,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 360,
    "chunk_method": "hierarchical",
    "importance_weight": 0.935,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations_s1_c1",
    "source_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
    "text": "Settings: We employ a pretrained Faster R-CNN [19] to extract 30 object regions, with a vocabulary size Dw of 4963 and Dr of 201 semantic relation classes. We retain Dc of 500 objects for classification and set the dimension D to 512. For training, we alternate between total loss for the first 25 epochs and word-level loss for the next 5 epochs. The learning rate starts at 2 × 10−4, decaying by 0.8 every three epochs. Hyper-parameter α is set to",
    "section_title": "EXPERIMENT",
    "section_type": "experiment",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 10,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 449,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations_s1_c2",
    "source_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
    "text": "0.8 every three epochs. Hyper-parameter α is set to 0.3 based on the validation set. Subsequently, we use a SCST configuration for 40 epochs, with a learning rate of 2 × 10−5, decaying by 0.8 every three epochs. We use a batch size of 10 and the Adam optimizer.\n4.2. Baselines and Main Results",
    "section_title": "EXPERIMENT",
    "section_type": "experiment",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 11,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 293,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations_s1_c3",
    "source_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
    "text": "Baselines: We compare DualRel with baselines including Regions-Hierarchical [1], RTT-GAN [5], DAM [7], SCST [8], DCPG-VAE [6], TMOS [27], CAE-LSTM [11], DHPV [9], CVAP [10], CRL [12], Dual-CNN [15], VREN [17], IMAP [14], S2TD [16], and OR-ATT [18]. Results: Table 1 shows our DualRel method achieves the best scores in B@{1-4} and CIDEr. DualRel outperforms SCST on all metrics (except for a tie in METEOR) and the recent IMAP method. Table 2 illustrates that DualRel also surpasses SCST in all BERT\ne 2 illustrates that DualRel also surpasses SCST in all BERTScore metrics.",
    "section_title": "EXPERIMENT",
    "section_type": "experiment",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 12,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 574,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations_s1_c4",
    "source_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
    "text": "4.3. Ablation Studies\n\nWe conduct ablation studies by removing key components (Table 3). The full DualRel model performs best. Removing spatial or semantic relation components or the relation-aware interaction results in lower scores, with the relation-aware interaction being most critical. Parameter α is tuned on the validation set, with the best performance at α = 0.3.\n\n4.4. Qualitative Analysis",
    "section_title": "EXPERIMENT",
    "section_type": "experiment",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 13,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 400,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations_s1_c5",
    "source_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
    "text": "4.4. Qualitative Analysis\n\nQualitative comparisons between DualRel and SCST paragraph generation are presented in Fig. 3. DualRel generates more accurate and detailed relations, as highlighted in spatial (blue) and semantic (green) relations, with fewer incorrect sentences (red).\n\n4.5. Visualization\n\nFig. 4 visualizes relation-aware attentions and two gates during paragraph generation, focusing on the main object in each sentence.\n\n---",
    "section_title": "EXPERIMENT",
    "section_type": "experiment",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 14,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 439,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations_s1_c6",
    "source_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
    "text": "Attention Visualization. We visualize the attention distribution for both regional and relation attention during paragraph generation. As shown in Fig. 4, DualRel focuses on correct image regions for corresponding sentences. The model also captures object pairs with potential relations, such as between 'man' and 'surfboard', and 'man' and 'wet suit'. Gate Visualization illustrates the role of our gate mechanisms during paragraph generation. The relation gate and visual gate scores vary across d",
    "section_title": "EXPERIMENT",
    "section_type": "experiment",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 15,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 499,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations_s1_c7",
    "source_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
    "text": "tion. The relation gate and visual gate scores vary across different sentences, indicating their importance.",
    "section_title": "EXPERIMENT",
    "section_type": "experiment",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 16,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 108,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations_s1_c8",
    "source_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
    "text": "Human Evaluation and Relation Statistics. We evaluate generated paragraphs based on richness, coherence, fine-grain details, diversity, and relevance. Fifty images and their corresponding paragraphs from SCST, DualRel without ℓR, and full DualRel models are assessed by 30 volunteers. Table 4 shows that DualRel paragraphs generally receive higher ratings, with the highest proportion in relation-richness at",
    "section_title": "EXPERIMENT",
    "section_type": "experiment",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 17,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 408,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations_s1_c9",
    "source_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
    "text": "60.0%. Table 5 presents statistics on the number of semantic and spatial relations in paragraphs. On average, DualRel involves 16.31 relations, more than SCST's 10.71 and even exceeding some ground truth paragraphs.",
    "section_title": "EXPERIMENT",
    "section_type": "experiment",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 18,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 215,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations_s1_c10",
    "source_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
    "text": "CONCLUSION. This paper introduces DualRel, a model for image paragraph captioning that captures semantic and spatial relations. It includes a relation embedding module and a relation-aware interaction module, outperforming strong baselines on the Stanford benchmark dataset. Future work will integrate transformer-based methods.",
    "section_title": "EXPERIMENT",
    "section_type": "experiment",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 19,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 328,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations_s1_c11",
    "source_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
    "text": "ACKNOWLEDGEMENTS. The work is supported by various grants from the National Key R&D Program of China, the National Natural Science Foundation of China, Beijing Natural Science Foundation, and the Fundamental Research Funds for Central Universities.\n\nREFERENCES. \n[1] Jonathan Krause et al., “A hierarchical approach for generating descriptive image paragraphs,” in CVPR, 2017, pp. 317–325. \n[2] Oriol Vinyals et al., “Show and tell: A neural image caption generator,” in CVPR, 2015, pp. 3156–3164.",
    "section_title": "EXPERIMENT",
    "section_type": "experiment",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 20,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 497,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations_s1_c12",
    "source_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
    "text": "---\n\nAnderson, P., et al. (2018). Bottom-up and top-down attention for image captioning and visual question answering. CVPR, 6077–6086.\n\nCornia, M., et al. (2020). Meshed-memory transformer for image captioning. CVPR, 10578–10587.\n\nLiang, X., et al. (2017). Recurrent topic-transition gan for visual paragraph generation. ICCV, 3362–3371.\n\nChatterjee, M., & Schwing, A. G. (2018). Diverse and coherent paragraph generation from images. ECCV.",
    "section_title": "EXPERIMENT",
    "section_type": "experiment",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 21,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 441,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations_s1_c13",
    "source_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
    "text": "Wang, Z., et al. (2018). Look deeper see richer: Depth-aware image paragraph captioning. ACM MM, 672–680.\n\nMelas-Kyriazi, L., Rush, A. M., & Han, G. (2018). Training for diversity in image paragraph captioning. EMNLP, 757–761.\n\nWu, S., et al. (2019). Densely supervised hierarchical policy-value network for image paragraph generation. IJCAI, 975–981.\n\nZha, Z.-J., et al. (2022). Context-aware visual policy network for fine-grained image captioning. TPAMI, 44(2), 710–722.",
    "section_title": "EXPERIMENT",
    "section_type": "experiment",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 22,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 473,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations_s1_c14",
    "source_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
    "text": "Wang, J., et al. (2019). Convolutional auto-encoding of sentence topics for image paragraph generation. IJCAI.\n\nLuo, Y., et al. (2019). Curiosity-driven reinforcement learning for diverse visual paragraph generation. ACM MM.\n\nYang, X., et al. (2020). Hierarchical scene graph encoder-decoder for image paragraph captioning. ACM MM.\n\nXu, C., et al. (2020). Interactive key-value memory-augmented attention for image paragraph captioning. COLING.",
    "section_title": "EXPERIMENT",
    "section_type": "experiment",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 23,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 444,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations_s1_c15",
    "source_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
    "text": "Li, R., et al. (2020). Dual-CNN: A convolutional language decoder for paragraph image captioning. Neurocomputing, 396, 92–101.\n\nShi, Y., et al. (2021). S2TD: A tree-structured decoder for image paragraph captioning. ACM MMAsia.\n\nChe, W., et al. (2019). Visual relationship embedding network for image paragraph generation. TMM, 22(9), 2307–2320.\n\nYang, L.-C., Yang, C.-Y., & Hsu, J. Y.-J. (2021). Object relation attention for image paragraph captioning. AAAI, 35(4), 3136–3144.",
    "section_title": "EXPERIMENT",
    "section_type": "experiment",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 24,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 478,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations_s1_c16",
    "source_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
    "text": "Ren, S., et al. (2015). Faster r-cnn: Towards real-time object detection with region proposal networks. NIPS.\n\nHerdade, S., et al. (2019). Image captioning: Transforming objects into words. NeurIPS.\n\nZhang, N., et al. (2021). Document-level relation extraction as semantic segmentation. IJCAI.\n\nRennie, S. J., et al. (2017). Self-critical sequence training for image captioning. CVPR, 7008–7024.",
    "section_title": "EXPERIMENT",
    "section_type": "experiment",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 25,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 395,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations_s1_c17",
    "source_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
    "text": "Papineni, K., et al. (2002). BLEU: a method for automatic evaluation of machine translation. ACL, 311–318.\n\nBanerjee, S., & Lavie, A. (2005). METEOR: An automatic metric for mt evaluation with improved correlation with human judgments. ACL, 65–72.\n\nVedantam, R., Zitnick, C. L., & Parikh, D. (2015). Cider: Consensus-based image description evaluation. CVPR, 4566–4575.\n\nZhang, T., et al. (2020). BERTScore: Evaluating text generation with bert. ICLR.",
    "section_title": "EXPERIMENT",
    "section_type": "experiment",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 26,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 451,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations_s1_c18",
    "source_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
    "text": "Mao, Y., et al. (2018). Show and tell more: Topic-oriented multi-sentence image captioning. IJCAI, 4258–4264.",
    "section_title": "EXPERIMENT",
    "section_type": "experiment",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 27,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 19,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 109,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8925,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c0",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "LGR-NET: Language Guided Reasoning Network for Referring Expression Comprehension\n\nMingcong Lu, Ruifan Li, Fangxiang Feng, Zhanyu Ma, and Xiaojie Wang",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 69,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 150,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c1",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "Abstract— Referring Expression Comprehension (REC) is a vision and language task that localizes an image region based on a natural language expression. Current transformer-based methods often treat image and text equally, lacking detailed consideration of textual features. We propose the Language Guided Reasoning Network (LGR-NET), which captures cross-modal features using a prediction token and extends textual features through a Textual Feature Extender (TFE). This includes a novel coordinate embedding, Text-guided Cross-modal Alignment (TCA), Fusion (TCF), and a cross-modal loss. LGR-NET ac",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c2",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "ed Cross-modal Alignment (TCA), Fusion (TCF), and a cross-modal loss. LGR-NET achieves state-of-the-art results on five benchmark datasets.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 139,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c3",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "Index Terms— Vision and language, referring expression comprehension, transformer, cross-modal reasoning.\nI. INTRODUCTION",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 121,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c4",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "Referring Expression Comprehension (REC) is a fundamental vision-language task, advancing applications like image captioning, visual question answering, and visual navigation. The key challenge in REC is accurate cross-modal reasoning. Existing methods are categorized into two-stage, one-stage, and transformer-based methods. Transformer-based methods, while more elegant, treat visual and textual features equally without fully leveraging the guidance of the referring expression. Our LGR-NET enhances cross-modal reasoning by prioritizing linguistic guidance for improved localization.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 588,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c5",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "We propose a Language Guided Reasoning Network (LGR-NET) to enhance target localization in referring expression comprehension. The LGR-NET addresses the issue of textual information being overwhelmed by visual information due to differences in sequence length between modalities. Our approach separates visual and textual modalities, utilizing a Textual Feature Extender (TFE) to guide cross-modal reasoning. The TFE extends textual features in three ways: generating a coordinate embedding, guiding cross-modal reasoning through TCA and TCF modules, and producing a sentence embedding for effective",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 5,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c6",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "ng through TCA and TCF modules, and producing a sentence embedding for effective cross-modal loss. Our LGR-NET consists of visual and textual extractors, TCA, and TCF modules, and a learnable prediction token for bounding box prediction.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 6,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 237,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c7",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "Related work includes two-stage and one-stage REC methods, which rely on region proposals and multi-modal fusion, respectively. Transformer-based REC methods have also emerged, using transformers for feature extraction and interaction. Additionally, recent works involve vision-language pre-training, leveraging transformers for large-scale image-text tasks.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 7,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 358,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c8",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "Our contributions are: \n- A novel framework LGR-NET with TFE for REC tasks.\n- A coordinate embedding to enhance spatial representation.\n- Alternating text-guided cross-modal alignment and fusion.\n- A novel loss for improved cross-modal alignment.\n- Experimental results demonstrating state-of-the-art performance on benchmark datasets.\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 8,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 340,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c9",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "---\n\npairs to improve multi-modal understanding and transfer to downstream tasks like REC and VQA. Models such as CLIP, OFA, MaPLe, BLIP2, and MiniGPT4 have integrated large-scale image-text pair datasets with contrastive learning and language model general abilities to enhance performance in multi-modal settings.\n\nIII. METHODOLOGY",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 9,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 333,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c10",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "III. METHODOLOGY\n\nOur method, LGR-NET, is detailed in this section. As shown in Fig. 2, given an image and a referring expression, we extract visual and textual features using dedicated feature extractors. We introduce a prediction token to capture cross-modal features for object localization. The Textual Feature Extender (TFE) enhances original textual features with coordinate, word, and sentence embeddings for cross-modal reasoning. The TCA and TCF modules align and fuse these features, respectively, with the prediction token being refined at each stage.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 10,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 562,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c11",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "A. Visual and Textual Feature Extractors\n\nOur multi-modal feature extractor comprises two independent extractors for vision and language. The visual extractor uses a Swin Transformer backbone to process RGB images and generate multi-scale feature maps. The textual extractor uses BERT to obtain textual embeddings. Both sets of features are projected into a common vector space for subsequent processing.\n\nB. Textual Feature Extender (TFE)",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 11,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 439,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c12",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "B. Textual Feature Extender (TFE)\n\nThe TFE module extends textual features in three ways: coordinate embedding for spatial information, word embeddings for comprehensive features, and sentence embeddings for overall alignment. These are used for TCA, TCF, and cross-modal loss computation.\n\nC. Text-Guided Cross-Modal Alignment (TCA)\n\nIn TCA, a learnable prediction token is added to the visual features. The module aligns visual features with textual features using attention mechanisms, enhanced by spatial information from the referring expression.\n\nD. Text-Guided Cross-Modal Fusion (TCF)",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 12,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 592,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c13",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "D. Text-Guided Cross-Modal Fusion (TCF)\n\nTCF fuses textual and visual features using a cross-attention mechanism. The aligned visual features from TCA serve as queries, while the textual features from TFE serve as keys and values. This process is repeated N times to refine the prediction token with accurate representations of the referred object.\n\n---\n\nE. Prediction Head\n\nThe bounding box is generated using a three-layer MLP with a Sigmoid activation function based on the N-th learned prediction token \\( f_{N}^{p} \\):\n\n\\[(x, y, w, h) = \\text{sigmoid}(FFN_{H}(f_{N}^{p}))\\]",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 13,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 578,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c14",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "\\[(x, y, w, h) = \\text{sigmoid}(FFN_{H}(f_{N}^{p}))\\]\n\nHere, \\( x \\) and \\( y \\) denote the coordinates of the central point, while \\( w \\) and \\( h \\) represent the width and height of the box, respectively.\n\nF. Loss and Training\n\nOur LGR-NET is trained using a loss function with two terms: one for bounding box regression and the other for cross-modal alignment between the prediction token and the sentence embedding:\n\n\\[L_{\\text{total}} = \\sum_{i=1}^{N} L_{\\text{box}}^{i} + \\lambda \\sum_{i=1}^{N} L_{\\text{align}}^{i}\\]",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 14,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 525,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c15",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "The former term aids in capturing the referred object's extremity features, while the latter promotes capturing the representative features corresponding to the referring expression. The hyperparameter \\( \\lambda \\) balances these terms. The bounding box loss for the i-th layer is given by:\n\n\\[L_{\\text{box}}^{i} = LGIoU(b_{i}, \\hat{b}) + LL1(b_{i}, \\hat{b})\\]\n\nThe alignment loss is defined as:\n\n\\[L_{\\text{align}} = -\\sum_{j=1}^{B} \\log \\left( \\frac{\\exp(f_{j}^{\\text{obj}} \\cdot f_{j}^{\\text{sent}} / \\tau)}{\\sum_{k=1}^{B} \\exp(f_{j}^{\\text{obj}} \\cdot f_{k}^{\\text{sent}} / \\tau)} \\right)\\]",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 15,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 595,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c16",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "where \\( B \\) is the batch size and \\( \\tau \\) is the learnable temperature parameter.\n\nIV. EXPERIMENTS\n\nA. Datasets\n\nFive benchmark datasets are used for evaluation: RefCOCO, RefCOCO+, RefCOCOg, ReferItGame, and Flickr30K Entities.\n\nB. Spatial Proportion Statistics\n\nThe influence of the proposed Coordinate Embedding on different datasets is revealed by calculating the proportion of spatial words in the referring expressions.\n\nC. Evaluation Metrics",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 16,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 452,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c17",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "C. Evaluation Metrics\n\nWe use Accuracy (Acc@0.5) as the evaluation metric, where a prediction bounding box is considered true if its Intersection over Union (IoU) with the ground truth box is greater than 0.5.\n\nD. Implementation Detail\n\nDetails of the implementation are provided, including the image size, maximum length of referring expressions, data augmentation, training epochs, learning rates, and hardware used.\n\nV. RESULTS AND ANALYSIS",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 17,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 443,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c18",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "V. RESULTS AND ANALYSIS\n\nThe quantitative results on the five benchmark datasets are reported, followed by ablation studies and qualitative analysis. The main results are presented in Table III, showcasing the performance of our method on RefCOCO, RefCOCO+, and RefCOCOg datasets.\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 18,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 285,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c19",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "---\n\nOur method demonstrates superior performance on the three datasets for all splits compared to the best two-stage method Ref-NMS and one-stage method LBYL-Net. The improvements on RefCOCO and RefCOCO+ datasets, particularly on the testB split, are notable, indicating better cross-modal alignment when referred objects are diverse. On the RefCOCOg dataset with more complex referring expressions, our method still outperforms LBYL-Net by 12.78% on val-g and Ref-NMS by 6.34% on val-u and test-u on average.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 19,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 510,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c20",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "In comparison to recently proposed transformer-based methods, our LGR-NET with resnet-101 achieves absolute improvements up to 3.70%, 8.08%, and 6.50% on RefCOCO, RefCOCO+, and RefCOCOg, respectively. When using a stronger backbone swin-s, our method surpasses VLTVG comprehensively, showcasing better adaptability to stronger backbones. Our LGR-NET also achieves competitive results against baselines with stronger visual backbones like ViT-B and CLIP-B, especially on RefCOCOg.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 20,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 479,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c21",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "Our model consistently outperforms two-stage and one-stage methods on ReferItGame and Flickr30K Entities, with improvements comparable to the best transformer-based method. However, the improvements on these two datasets are less significant than on RefCOCOg due to the simpler nature of the referring expressions.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 21,
    "chunk_index_in_section": 21,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 314,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c22",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "For pre-trained results, our model, which belongs to the first category of methods designed for REC tasks, achieves better performance with the same or less pre-trained data compared to RefTR and MDETR. While it shows lower performance than the second category of methods like ONE-PEACE or mPLUG, this highlights the higher upper limit of a generic cross-modal framework pre-trained on diverse datasets.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 22,
    "chunk_index_in_section": 22,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 403,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c23",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "In the ablation study on RefCOCOg, we analyze the effectiveness of Coordinate Embedding (CE) and find significant improvements on RefCOCO and RefCOCOg, especially on RefCOCO testB. Ablation experiments also reveal that the \"add\" operation is the most effective for incorporating CE. Additionally, the Cross-modal Loss (CL) benefits from a larger batch size, with significant improvements observed when increasing the batch size from 16 to 128.\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 23,
    "chunk_index_in_section": 23,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 448,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c24",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "---\n\nAuthorized licensed use limited to: BEIJING UNIVERSITY OF POST AND TELECOM. Downloaded on July 22, 2025 at 08:17:44 UTC from IEEE Xplore. Restrictions apply.\n\n7778 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 34, NO. 8, AUGUST 2024\n\n--- \n\n(Note: The above statement is noise and has been removed from the cleaned content as per the instructions.)",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 24,
    "chunk_index_in_section": 24,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 375,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c25",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "The results indicate that our Contrastive Learning (CL) yields more significant improvements with an enlarged batch size. Our CL is positioned between the [CLS] token and the prediction token, focusing on the visual feature of the referred object rather than the entire image. The Cross-Entropy (CE) is shown to be beneficial when the prediction token captures the referred object's visual feature. Ablation experiments confirm the complementary roles of CE and CL, with improvements rising from 0.31% and 0.07% without each other to 1.35% and 1.11% when combined on RefCOCOg val-u.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 25,
    "chunk_index_in_section": 25,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 582,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c26",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "Our TCA and TCF modules are akin to a standard transformer cross-attention block with a focus on input manner. An ablation study on QRNet demonstrates the importance of cross-modal alignment, with a significant performance decline when visual features are fixed. In contrast, repeatedly fusing textual features provides consistent improvements. The input manner of our LGR-NET is validated, with a performance drop when visual and textual modalities are exchanged, indicating the effectiveness of TCA and TCF.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 26,
    "chunk_index_in_section": 26,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 509,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c27",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "The number of layers in TCA and TCF modules was evaluated, with consistent accuracy increases from one to six layers. More layers yield inconsistent improvements, leading to the choice of six layers as optimal. The number of prediction tokens was also examined, with one token proving more effective for bounding box prediction.\n\nThe impact of the cross-modal loss was investigated using different values of the weighting factor λ, with the best performance at λ = 2.0. All values of λ improved performance over the absence of the cross-modal loss.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 27,
    "chunk_index_in_section": 27,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 548,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c28",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "Visualization analysis reveals the effectiveness of LGR-NET in language guidance. Textual features accurately generate 2D coordinates of referred objects, and attention scores show the prediction token's focus on key visual features. The visualization also supports the role of CE in capturing visual features.\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 28,
    "chunk_index_in_section": 28,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 315,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c29",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "---\n\nThe attention scores reveal a strong correlation with the \"5 o'clock position\". Visualizations of attention weights between the prediction token and textual tokens highlight the importance of the key expression \"5 o'clock\". Further, visual attention maps from eight attention heads capture two types of features: regions inside the box and the referred object's extremities, corresponding to predicted box edges. This aligns with our designed loss.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 29,
    "chunk_index_in_section": 29,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 453,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c30",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "Coordinate Embedding (CE) on the query effectively captures spatial information. For instance, CE focuses on the spatial word \"middle\" and key expressions like \"bottom right corner\". This demonstrates CE's role in guiding the prediction token to attend to the correct object.\n\nIn comparing our LGR-NET with QRNet, we observe that the decoupling of modalities in LGR-NET addresses the language information overload issue. LGR-NET's TCA and TCF mechanisms dynamically align visual features with textual features, as evidenced by attention maps that attend to referred objects across layers.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 30,
    "chunk_index_in_section": 30,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 588,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c31",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "Qualitative analysis on RefCOCOg shows that LGR-NET accurately reasons over complex expressions, such as \"furtherest away from the wardrobe\", and outperforms other models in certain cases. However, challenges remain in complex scenes with nested expressions or multiple referred objects.\n\nIn conclusion, LGR-NET's emphasis on textual feature guidance for cross-modal reasoning shows effectiveness in REC tasks. Future work includes extending LGR-NET to different image domains and handling arbitrary queries, including those that refer to no object.\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 31,
    "chunk_index_in_section": 31,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 554,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c32",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "---\n\nAuthorized licensed use and download information, page numbers, and formatting have been removed as per the cleaning instructions. Technical content, data, and logical structure have been preserved.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 32,
    "chunk_index_in_section": 32,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 203,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c33",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "Vinyals, A. Toshev, S. Bengio, and D. Erhan, “Show and tell: A neural image caption generator,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2015, pp. 3156–3164. Anderson et al., “Bottom-up and top-down attention for image captioning and visual question answering,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 6077–6086. Nguyen, M. Suganuma, and T. Okatani, “GRIT: Faster and better image captioning transformer using dual visual features,” in Proc. 17th Eur. Conf. Comput. Vis. (ECCV), 2022, pp. 167–184. Gu, H. Wang, and R. Fan, “Coherent visual story",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 33,
    "chunk_index_in_section": 33,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c34",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "Vis. (ECCV), 2022, pp. 167–184. Gu, H. Wang, and R. Fan, “Coherent visual storytelling via parallel top-down visual and topic attention,” IEEE Trans. Circuits Syst. Video Technol., vol. 33, no. 1, pp. 257–268, Jan. 2023.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 34,
    "chunk_index_in_section": 34,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 220,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c35",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "Antol et al., “VQA: Visual question answering,” in Proc. IEEE Int. Conf. Comput. Vis. (ICCV), Santiago, Chile, Dec. 2015, pp. 2425–2433. Nguyen and T. Okatani, “Improved fusion of visual and language representations by dense symmetric co-attention for visual question answering,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 6087–6096. Karamcheti, R. Krishna, L. Fei-Fei, and C. Manning, “Mind your outliers! Investigating the negative impact of outliers on active learning for visual question answering,” in Proc. 59th Annu. Meeting Assoc. Comput. Linguistics, 11th Int. J",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 35,
    "chunk_index_in_section": 35,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c36",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "answering,” in Proc. 59th Annu. Meeting Assoc. Comput. Linguistics, 11th Int. Joint Conf. Natural Lang. Process., 2021, pp. 7265–7281. Zhang, R. Wang, F. Zhou, and Y. Luo, “ERM: Energy-based refined-attention mechanism for video question answering,” IEEE Trans. Circuits Syst. Video Technol., vol. 33, no. 3, pp. 1454–1467, Mar. 2023.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 36,
    "chunk_index_in_section": 36,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 334,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c37",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "Anderson et al., “Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments,” in Proc. CVPR, Salt Lake City, UT, USA, Jun. 2018, pp. 3674–3683. Hong, C. Rodriguez, Q. Wu, and S. Gould, “Sub-instruction aware vision-and-language navigation,” in Proc. Conf. Empirical Methods Natural Lang. Process. (EMNLP), 2020, pp. 3360–3376. Zhu et al., “Diagnosing vision-and-language navigation: What really matters,” in Proc. Conf. North Amer. Chapter Assoc. Comput. Linguist., Human Lang. Technol., 2022, pp. 5981–5993. Zhang, C. Ma, Q. Wu, and X. Yang, “Langu",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 37,
    "chunk_index_in_section": 37,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c38",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "an Lang. Technol., 2022, pp. 5981–5993. Zhang, C. Ma, Q. Wu, and X. Yang, “Language-guided navigation via cross-modal grounding and alternate adversarial learning,” IEEE Trans. Circuits Syst. Video Technol., vol. 31, no. 9, pp. 3469–3481, Sep. 2021.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 38,
    "chunk_index_in_section": 38,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 249,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c39",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "Hu, M. Rohrbach, and T. Darrell, “Segmentation from natural language expressions,” in Proc. Comput. Vis.–ECCV 14th Eur. Conf., Amsterdam, The Netherlands, Oct. 2016, pp. 108–124. Li, M. Sun, J. Xiao, E. G. Lim, and Y. Zhao, “Fully and weakly supervised referring expression segmentation with end-to-end learning,” IEEE Trans. Circuits Syst. Video Technol., vol. 33, no. 10, pp. 5999–6012, Jun. 2023. Shang et al., “Cross-modal recurrent semantic comprehension for referring image segmentation,” IEEE Trans. Circuits Syst. Video Technol., vol. 33, no. 7, pp. 3229–3242, Dec. 2023. Nagaraja, V. I. Mor",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 39,
    "chunk_index_in_section": 39,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c40",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "t. Video Technol., vol. 33, no. 7, pp. 3229–3242, Dec. 2023. Nagaraja, V. I. Morariu, and L. S. Davis, “Modeling context between objects for referring expression understanding,” in Proc. Comput. Vis. ECCV 14th Eur. Conf., Amsterdam, The Netherlands, Oct. 2016, pp. 792–807. Yu et al., “MAttNet: Modular attention network for referring expression comprehension,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 1307–1315.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 40,
    "chunk_index_in_section": 40,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 443,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c41",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "Zhang, Y. Niu, and S.-F. Chang, “Grounding referring expressions in images by variational context,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 4158–4166. Yang, G. Li, and Y. Yu, “Dynamic graph attention for referring expression comprehension,” in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Seoul, South Korea, Oct. 2019, pp. 4643–4652. Liu, H. Zhang, Z. Zha, and F. Wu, “Learning to assemble neural module tree networks for visual grounding,” in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Seoul, South Korea, Oct. 2019, pp. 4672–4681. Hong, D. Liu, X. Mo, X. He,",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 41,
    "chunk_index_in_section": 41,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c42",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "CCV), Seoul, South Korea, Oct. 2019, pp. 4672–4681. Hong, D. Liu, X. Mo, X. He, and H. Zhang, “Learning to compose and reason with language tree structures for visual grounding,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 44, no. 2, pp. 684–696, Feb. 2022. Yang, B. Gong, L. Wang, W. Huang, D. Yu, and J. Luo, “A fast and accurate one-stage approach to visual grounding,” in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Seoul, South Korea, Oct. 2019, pp. 4682–4692.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 42,
    "chunk_index_in_section": 42,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 470,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c43",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "Yang, T. Chen, L. Wang, and J. Luo, “Improving one-stage visual grounding by recursive sub-query construction,” in Proc. Comput. Vis. (ECCV) 16th Eur. Conf., Glasgow, U.K., 2020, pp. 387–404. Sun, W. Suo, P. Wang, Y. Zhang, and Q. Wu, “A proposal-free one-stage framework for referring expression comprehension and generation via dense cross-attention,” IEEE Trans. Multimedia, vol. 25, pp. 2446–2458, 2023. Ren, K. He, R. B. Girshick, and J. Sun, “Faster R-CNN: Towards real-time object detection with region proposal networks,” in Proc. Adv. Neural Inf. Process. Syst., 2015, pp. 91–99. Redmon and",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 43,
    "chunk_index_in_section": 43,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c44",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "networks,” in Proc. Adv. Neural Inf. Process. Syst., 2015, pp. 91–99. Redmon and A. Farhadi, “YOLOv3: An incremental improvement,” 2018, arXiv:1804.02767. Deng, Z. Yang, T. Chen, W. Zhou, and H. Li, “TransVG: End-to-end visual grounding with transformers,” in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2021, pp. 1749–1759.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 44,
    "chunk_index_in_section": 44,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 332,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c45",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "Li M, Sigal L. Referring transformer: A one-step approach to multi-task visual grounding. In: Proc. Adv. Neural Inf. Process. Syst.; 2021. p. 19652–19664.\nKamath A, Singh M, LeCun Y, Synnaeve G, Misra I, Carion N. MDETR–modulated detection for end-to-end multi-modal understanding. In: Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV); Oct. 2021. p. 1760–1770.\nYe J, et al. Shifting more attention to visual backbone: Query-modulated refinement networks for end-to-end visual grounding. In: Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR); Jun. 2022. p. 15481–15491.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 45,
    "chunk_index_in_section": 45,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 574,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c46",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "Yang L, Xu Y, Yuan C, Liu W, Li B, Hu W. Improving visual grounding with visual-linguistic verification and iterative reasoning. In: Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR); Jun. 2022. p. 9489–9498.\nUijlings JRR, van de Sande KEA, Gevers T, Smeulders AW. Selective search for object recognition. Int. J. Comput. Vis. 2013;104(2):154–171.\nMao J, Huang J, Toshev A, Camburu O, Yuille A, Murphy K. Generation and comprehension of unambiguous object descriptions. In: Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR); 2016. p. 11–20.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 46,
    "chunk_index_in_section": 46,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 554,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c47",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "Chen L, Ma W, Xiao J, Zhang H, Chang S. Ref-NMS: Breaking proposal bottlenecks in two-stage referring expression grounding. In: Proc. AAAI Conf. Artif. Intell.; 2021. p. 1036–1044.\nRohrbach A, Rohrbach M, Hu R, Darrell T, Schiele B. Grounding of textual phrases in images by reconstruction. In: Proc. Comput. Vis. (ECCV); 2016. p. 817–834.\nSun M, Xiao J, Lim EG, Liu S, Goulermas JY. Discriminative triad matching and reconstruction for weakly referring expression grounding. IEEE Trans. Pattern Anal. Mach. Intell. 2021;43(11):4189–4195.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 47,
    "chunk_index_in_section": 47,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 538,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c48",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "Sun M, Xiao J, Lim EG, Zhao Y. Cycle-free weakly referring expression grounding with self-paced learning. IEEE Trans. Multime- dia. 2023;25:1611–1621.\nSun M, Xiao J, Lim EG. Iterative shrinking for referring expression grounding using deep reinforcement learning. In: Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR); Jun. 2021. p. 14055–14064.\nVaswani A, et al. Attention is all you need. In: Proc. Adv. Neural Inform. Process. Syst.; 2017. p. 5998–6008.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 48,
    "chunk_index_in_section": 48,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 465,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c49",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "Devlin J, Chang MW, Lee K, Toutanova K. BERT: Pre-training of deep bidirectional transformers for language understanding. In: Proc. Conf. North Amer. Chapter Assoc. Comput. Linguistics; 2019. p. 4171–4186.\nDosovitskiy A, et al. An image is worth 16×16 words: Transformers for image recognition at scale. In: Proc. 9th Int. Conf. Learn. Represent. (ICLR); 2021.\nHe L, et al. End-to-end video object detection with spatial–temporal transformers. 2021. arXiv:2105.10920.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 49,
    "chunk_index_in_section": 49,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 467,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c50",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "Liu Z, et al. Swin Transformer: Hierarchical vision transformer using shifted windows. In: Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV); 2021. p. 9992–10002.\nDai L, Liu H, Tang H, Wu Z, Song P. AO2-DETR: Arbitrary-oriented object detection transformer. IEEE Trans. Circuits Syst. Video Technol. 2023;33(5):2342–2356.\nSu W, et al. Language adaptive weight generation for multi-task visual grounding. In: Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR); Jun. 2023. p. 10857–10866.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 50,
    "chunk_index_in_section": 50,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 490,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c51",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "Wang P, et al. OFA: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In: Proc. Int. Conf. Mach. Learn. (ICML); 2022. p. 23318–23340.\nSu W, et al. VL-BERT: Pre-training of generic visual-linguistic representations. In: Proc. 8th Int. Conf. Learn. Represent. (ICLR); 2020. p. 1–16.\nLi LH, Yatskar M, Yin D, Hsieh C, Chang K. VisualBERT: A simple and performant baseline for vision and language. 2019. arXiv:1908.03557.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 51,
    "chunk_index_in_section": 51,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 471,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c52",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "Radford A, et al. Learning transferable visual models from natural language supervision. In: Proc. 38th Int. Conf. Mach. Learn.; 2021. p. 8748–8763.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 52,
    "chunk_index_in_section": 52,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 148,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c53",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "[50] J. Li, D. Li, C. Xiong, and S. Hoi, “BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation,” in Proc. Int. Conf. Mach. Learn., Baltimore, MD, USA, 2022, pp. 12888–12900.\n[51] J. Li, D. Li, S. Savarese, and S. C. H. Hoi, “BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models,” 2023, arXiv:2301.12597.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 53,
    "chunk_index_in_section": 53,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 405,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c54",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "[52] M. U. Khattak, H. Rasheed, M. Maaz, S. Khan, and F. S. Khan, “MaPLe: Multi-modal prompt learning,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2023, pp. 19113–19122.\n[53] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, “MiniGPT-4: Enhancing vision-language understanding with advanced large language models,” 2023, arXiv:2304.10592.\n[54] S. Zhang et al., “OPT: Open pre-trained transformer language models,” 2022, arXiv:2205.01068.\n[55] L. Zheng, “Judging LLM-as-a-judge with MT-bench and chatbot arena,” 2023, arXiv:2306.05685.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 54,
    "chunk_index_in_section": 54,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 560,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c55",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "[56] H. Rezatofighi et al., “Generalized intersection over union: A metric and a loss for bounding box regression,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Long Beach, CA, USA, Jun. 2019, pp. 658–666.\n[57] S. Kazemzadeh, V. Ordonez, M. Matten, and T. Berg, “ReferItGame: Referring to objects in photographs of natural scenes,” in Proc. Conf. Empirical Methods Natural Lang. Process. (EMNLP), Doha, Qatar: Association for Computational Linguistics, 2014, pp. 787–798.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 55,
    "chunk_index_in_section": 55,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 485,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c56",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "[58] B. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo, J. Hockenmaier, and S. Lazebnik, “Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models,” in Proc. IEEE Int. Conf. Comput. Vis. (ICCV), Santiago, CL, USA, Dec. 2015, pp. 2641–2649.\n[59] T. Lin et al., “Microsoft COCO: Common objects in context,” in Proc. Comput. Vis. ECCV 13th Eur. Conf., Zurich, Switzerland, vol. 8693, Sep. 2014, pp. 740–755.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 56,
    "chunk_index_in_section": 56,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 452,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c57",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "[60] H. J. Escalante et al., “The segmented and annotated IAPR TC-12 benchmark,” Comput. Vis. Image Understand., vol. 114, no. 4, pp. 419–428, Apr. 2010.\n[61] P. Young, A. Lai, M. Hodosh, and J. Hockenmaier, “From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions,” Trans. Assoc. Comput. Linguistics, vol. 2, pp. 67–78, Dec. 2014.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 57,
    "chunk_index_in_section": 57,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 392,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c58",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "[62] P. Wang, Q. Wu, J. Cao, C. Shen, L. Gao, and A. V. D. Hengel, “Neighbourhood watch: Referring expression comprehension via language-guided graph attention networks,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Long Beach, CA, USA, Jun. 2019, pp. 1960–1968.\n[63] Y. Liao et al., “A real-time cross-modality correlation filtering method for referring expression comprehension,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Seattle, WA, USA, Jun. 2020, pp. 10877–10886.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 58,
    "chunk_index_in_section": 58,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 508,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c59",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "[64] J. Ye, X. Lin, L. He, D. Li, and Q. Chen, “One-stage visual grounding via semantic-aware feature filter,” in Proc. 29th ACM Int. Conf. Multimedia, Oct. 2021, pp. 1702–1711.\n[65] B. Huang, D. Lian, W. Luo, and S. Gao, “Look before you leap: Learning landmark features for one-stage visual grounding,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2021, pp. 16883–16892.\n[66] H. Zhao, J. T. Zhou, and Y.-S. Ong, “Word2Pix: Word to pixel cross-attention transformer in visual grounding,” IEEE Trans. Neural Netw. Learn. Syst., vol. 35, no. 2, pp. 1523–1533, Feb. 2022.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 59,
    "chunk_index_in_section": 59,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 592,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c60",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "[67] C. Ho, S. Appalaraju, B. Jasani, R. Manmatha, and N. Vasconcelos, “YORO—Lightweight end to end visual grounding,” 2022, arXiv:2211.07912.\n[68] C. Zhu et al., “SeqTR: A simple yet universal network for visual grounding,” in Proc. Eur. Conf. Comput. Vis., Cham, Switzerland: Springer, 2022, pp. 598–615.\n[69] J. Deng et al., “TransVG++: End-to-end visual grounding with language conditioned vision transformer,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 45, no. 11, pp. 13636–13652, Nov. 2023.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 60,
    "chunk_index_in_section": 60,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 500,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c61",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "[70] F. Shi, R. Gao, W. Huang, and L. Wang, “Dynamic MDETR: A dynamic multimodal transformer decoder for visual grounding,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 46, no. 2, pp. 1181–1198, Feb. 2024.\n[71] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,” in Proc. 7th Int. Conf. Learn. Represent., New Orleans, LA, USA, 2019, pp. 1–12.\n[72] R. Krishna et al., “Visual genome: Connecting language and vision using crowdsourced dense image annotations,” Int. J. Comput. Vis., vol. 123, no. 1, pp. 32–73, May 2017.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 61,
    "chunk_index_in_section": 61,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 538,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c62",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "[73] L. Wang, Y. Li, J. Huang, and S. Lazebnik, “Learning two-branch neural networks for image-text matching tasks,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 41, no. 2, pp. 394–407, Feb. 2019.\n[74] B. A. Plummer et al., “Conditional image-text embedding networks,” in Proc. Comput. Vis. ECCV 15th Eur. Conf., Munich, Germany, vol. 11216, Sep. 2018, pp. 258–274.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 62,
    "chunk_index_in_section": 62,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 366,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c63",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "[75] Z. Yu, J. Yu, C. Xiang, Z. Zhao, Q. Tian, and D. Tao, “Rethinking diversified and discriminative proposal generation for visual grounding,” in Proc. 27th Int. Joint Conf. Artif. Intell., Stockholm, Sweden, Jul. 2018, pp. 1114–1120.\n[76] Z. Mu, S. Tang, J. Tan, Q. Yu, and Y. Zhuang, “Disentangled motif-aware graph learning for phrase grounding,” in Proc. AAAI Conf. Artif. Intell., 2021, pp. 13587–13594.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 63,
    "chunk_index_in_section": 63,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 410,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c64",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "[77] J. Lu, D. Batra, D. Parikh, and S. Lee, “ViLBERT: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks,” in Proc. Adv. Neural Inf. Process. Syst., 2019, pp. 13–23.\n[78] Y. Chen et al., “UNITER: Universal image-text representation learning,” in Proc. Comput. Vis. (ECCV) 16th Eur. Conf., Glasgow, U.K., vol. 12375, Aug. 2020, pp. 104–120.\n[79] K. Chen, Z. Zhang, W. Zeng, R. Zhang, F. Zhu, and R. Zhao, “Shikra: Unleashing multimodal LLM’s referential dialogue magic,” 2023, arXiv:2306.15195.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 64,
    "chunk_index_in_section": 64,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 531,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c65",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "[80] C. Li et al., “mPLUG: Effective and efficient vision-language learning by cross-modal skip-connections,” in Proc. Conf. Empirical Methods Natural Lang. Process., 2022, pp. 7241–7259.\n[81] P. Wang et al., “ONE-PEACE: Exploring one general representation model toward unlimited modalities,” 2023, arXiv:2305.11172.\n[82] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2016, pp. 770–778.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 65,
    "chunk_index_in_section": 65,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 490,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c66",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "Zhanyu Ma (Senior Member, IEEE) received his Ph.D. in electrical engineering from the KTH Royal Institute of Technology, Stockholm, Sweden, in 2011. He served as a Post-Doctoral Research Fellow at the School of Electrical Engineering, KTH Royal Institute of Technology from 2012 to 2013. From 2014 to 2019, he was an Associate Professor at Beijing University of Posts and Telecommunications (BUPT), Beijing, China. Since 2015, he has been an Adjunct Associate Professor at Aalborg University, Aalborg, Denmark. Currently, he is a Full Professor at BUPT. His research focuses on pattern recognition,",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 66,
    "chunk_index_in_section": 66,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c67",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "y, he is a Full Professor at BUPT. His research focuses on pattern recognition, machine learning fundamentals, and their applications in computer vision, multimedia signal processing, and data mining.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 67,
    "chunk_index_in_section": 67,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 200,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension_s0_c68",
    "source_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
    "text": "Xiaojie Wang obtained his Ph.D. from Beihang University in 1996 and is now a Full Professor at Beijing University of Posts and Telecommunications. His research areas include natural language processing and multi-modal cognitive computing. He serves as an Executive Member of the Council of Chinese Association of Artificial Intelligence and as the Director of the Natural Language Processing Committee. Additionally, he is a member of the Council of Chinese Information Processing Society and the Chinese Processing Committee of China Computer Federation.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 68,
    "chunk_index_in_section": 68,
    "total_chunks_in_section": 69,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 555,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c0",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "The recognition accuracy of ligature-based Urdu language optical character recognition (OCR) systems is highly dependent on the segmentation accuracy that converts Urdu text into lines and ligatures. This paper presents techniques for segmenting Urdu Nastaleeq text images into lines and subsequently into ligatures. The classical horizontal projection-based segmentation method is enhanced with a curved-line-split algorithm to address issues such as text line split position, overlapping, merged ligatures, and ligatures crossing line split positions. The ligature segmentation algorithm identifies",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 43,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 600,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c1",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "es crossing line split positions. The ligature segmentation algorithm identifies connected components from text lines, categorizes them into primary and secondary classes, and allocates secondary components to the primary class based on width, height, coordinates, overlapping, centroids, and baseline information.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 314,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c2",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "The proposed line segmentation algorithm achieved a 99.17% accuracy on 47 pages. The ligature segmentation algorithm was primarily tested on a large Urdu-printed text images dataset, successfully segmenting 189,000 ligatures from 10,063 text lines with 332,000 connected components. Approximately 142,000 secondary components were allocated to over 189,000 primary ligatures with an accuracy rate of 99.80%. Both segmentation algorithms show superior performance compared to existing algorithms for Urdu Nastaleeq text segmentation. The line segmentation algorithm was also tested on Arabic text, de",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c3",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "segmentation. The line segmentation algorithm was also tested on Arabic text, demonstrating successful line extraction.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 119,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c4",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "Urdu, the national language of Pakistan, is widely spoken in India and other Southern Asian countries. It is an Arabic script language primarily written in the Nastaleeq style, which is also used in Persian, Pashto, Panjabi, Baluchi, and Siraiki. The complexities of Nastaleeq, such as context sensitivity, compactness, overlapping, and cursive and diagonal nature, pose significant challenges for its segmentation into lines and ligatures. OCR systems for Nastaleeq text are categorized based on text segmentation into character-based, ligature-based, and sentence-based systems. Current research f",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c5",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "character-based, ligature-based, and sentence-based systems. Current research focuses on ligature and sentence-based OCRs, which heavily rely on the accurate segmentation of document images.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 5,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 190,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c6",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "Previous contributions to Urdu line and ligature segmentation are overviewed in the following sections.\n\n---\n1) LINE SEGMENTATION",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 6,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 129,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c7",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "Text lines extraction is crucial in segmentation procedures. Algorithms for Urdu page segmentation into lines include horizontal and vertical projection, x-y cut, smearing, geometric, ridge-based, and zone-based horizontal projection. The pioneer work used horizontal projection for text lines, followed by component labeling and vertical projection for character extraction. Recursive XY cut, whitespace analysis, and other methods have been applied, with the highest accuracy for Urdu reported at 65.4%. Geometric algorithms tailored for Urdu Nastaleeq documents achieved text line detection accur",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 7,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c8",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "orithms tailored for Urdu Nastaleeq documents achieved text line detection accuracy ranging from 72% to 93%. Ridge-based approaches and zone-based methods have achieved accuracies of 96% and 92% for Arabic and Urdu, respectively. A zone-based method proposed for Nastaleeq’s Urdu text lines achieved 99.11% accuracy but overlooked diacritics above zone 1.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 8,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 355,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c9",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "2) LIGATURE SEGMENTATION\nA ligature is a combination of characters, and the primary component consists of main strokes, while diacritics are secondary strokes. Efforts have been made for ligature segmentation using vertical histogram, horizontal projection, and bounding box methods, achieving up to 99.02% accuracy. A recent study allocated secondary components to primary components with 92.5% accuracy.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 9,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 405,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c10",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "B) MOTIVATION\nRecent algorithms have used zoning and heuristics for line and ligature segmentation, which can be context and font dependent. Our motivation is to reduce the complexity of zoning and merging decisions, relying more on baseline information.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 10,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 254,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c11",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "C) CONTRIBUTION\nWe present two algorithms for line and ligature segmentation of Urdu Nastaleeq text, which can be applied to other Nastaleeq-based languages. Our line segmentation algorithm depends on horizontal projection, split positions (Curved-Line-Split algorithm), and baseline information, while the ligature segmentation algorithm uses quantitative information like width, height, centroids, and overlapping. Our algorithms achieve accuracies of 99.17% for line segmentation and 99.08% for ligature segmentation.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 11,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 520,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c12",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "Algorithm 1 Curved Line Split\nData: Document Image Page, initial split row numbers\nResult: Coordinates for demarcation/split lines\n...\n[Algorithmic details follow]\n\n---\n\nThis section discusses the line segmentation algorithm, which divides a page into text lines using a global horizontal projection method enhanced by the proposed Curved Line Split (CLS) algorithm. It also addresses conflict resolution for components that cross split lines based on baseline information.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 12,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 473,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c13",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "The CLS algorithm is capable of identifying curved demarcation lines between consecutive text lines when a straight line is not present. It starts from the middle of the baselines and traverses horizontally, adjusting vertically when text pixels are encountered. The process is detailed in Figs. 7 and 8, and the algorithm is presented in Algorithm 1.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 13,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 351,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c14",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "The line segmentation algorithm is applied to Urdu Nastaleeq document images. It improves upon classical horizontal projection methods by incorporating the CLS algorithm, overcoming issues such as text line splits, overlapping, merged ligatures, and ligatures crossing split positions. The main steps include:",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 14,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 309,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c15",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "1. Finding Line-Peaks by converting the image into a binary image, calculating horizontal projection, and identifying peaks and troughs.\n2. Determining the average text line height.\n3. Identifying and excluding false text lines.\n4. Correcting for missed text lines by adjusting Line-Peak values.\n5. Segmenting the page into text lines using initial split row numbers and the CLS algorithm.\n6. Assigning components to the correct text lines, including handling crossing components and those touching neither baselines nor split lines, such as diacritics or dots.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 15,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 561,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c16",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "The possibility of attaching diacritics to the wrong lines is high, particularly in the upper and lower diacritics/dots zones of each text line, excluding the first and last lines. The first line may only have a lower diacritics/dots zone, while the last may have only an upper zone. Components found in these areas are moved to the line with the minimum distance from the neighboring component. This process involves identifying components that do not touch the baseline and are not larger than their respective zones, then relocating them based on the nearest text pixel.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 16,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 573,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c17",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "In the case of components touching both baselines, indicating a possible incorrect connection between two ligatures, the position of the joint with the minimum number of pixels is located to split the component appropriately.\n\nThe ligature segmentation algorithm splits text lines into constituent ligatures, which are typically composed of 1 to 8 characters forming a primary component with possible secondary components like dots and diacritics. The algorithm finds all components and allocates secondary components to the proper primary component.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 17,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 550,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c18",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "The proposed line segmentation algorithm was tested on 47 pages, achieving 100% segmentation for 7 pages from previous studies and an accuracy of 99.17% for the remaining 40 pages from a single-columned Urdu newspaper. This accuracy surpasses that reported in previous works. The comparison with other line segmentation algorithms shows that the proposed method performs better, particularly due to its diacritics allocation mechanism.\n\n---\n\nI. Ahmad et al.: Line and Ligature Segmentation of Urdu Nastaleeq Text\n\nFIGURE 17. Segmented text lines by the proposed line algorithm.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 18,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 577,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c19",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "FIGURE 17. Segmented text lines by the proposed line algorithm.\n\nAlgorithm 2 Ligature Segmentation Algorithm\nData: S = Urdu Sentense Images\nResult: List of ligature images for a given Urdu sentence image\n\nbegin\nfor k = 1 to S do\n    Tk = Extract information(connected comp(Sk))\n    Tk = Decide primary secondary components(Tk)\n    Lk = Allocate secondary to primary(Tk)\nreturn Lk\n\nAlgorithm 3 Extract Information\nData: Sk = Urdu sentence image\nResult: Table Tk, containing information regarding connected components of a sentence Sk",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 19,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 532,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c20",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "begin\n    CC = Find connected components for Sk\n    for each connected components CCi do\n        Find starting and ending columns number\n        Find starting and ending rows number\n        Touching baseline or not\n        Calculate height and width\n        Overlapping connected component\n        Centroid\n        Save all above information about CCi in Tk\n    return Tk\n\nFIGURE 18. Total 27, 12 and 21 resultant ligatures from UPTI data set, [28] and [29], respectively.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 20,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 472,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c21",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "Algorithm 4 Decide Primary and Secondary Components\nData: Tk, CC, Secondary Component Height < Connected Comp Height CCH = 1/2 of the height of a text line\nResult: Updated Table Tk\n\nbegin\n    for each CCi that overlaps with any other CCj in Tk\n        if CCi is not diacritic and SCCj is a diacritic then\n            update CCj as secondary component of CCi in table Tk\n        if height of CCj < CCH then\n            update CCj as secondary component of CCi in table Tk\n    return Tk",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 21,
    "chunk_index_in_section": 21,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 484,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c22",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "Algorithm 5 Allocate Secondary to Primary Components\nData: Tk, Secondary Connected Component=SCC, Primary Connected Component=PCC, CCH\nResult: List of ligatures Lk\n\nbegin\n    for each SCCi of Tk do\n        if SCCi is not touching baseline then\n            Copy pixels of SCCi at proper coordinates on its allotted PCC\n        else\n            Declare SCCi as PCC in Tk\n    for each PCCi of Tk do\n        Save PCCi as a ligature of kth sentence\n    return Lk\n\nC. RESULTS OF THE PROPOSED LIGATURE SEGMENTATION ALGORITHM",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 22,
    "chunk_index_in_section": 22,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 517,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c23",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "C. RESULTS OF THE PROPOSED LIGATURE SEGMENTATION ALGORITHM\n\nUPTI data set is used for checking the segmentation accuracy of the proposed algorithms. The implemented algorithms achieve an accuracy better than the best reported in [8], i.e., 99.02%. Examples of incorrectly connected dots, diacritics, and ligatures are depicted in Fig. 24.\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 23,
    "chunk_index_in_section": 23,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 343,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c24",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "Fig. 24 illustrates two ligatures connected incorrectly in its first and second parts, while the fourth part consists of all secondary components from the third part, incorrectly connected in six dots. These dots exceed the maximum threshold size for secondary components and touch the baseline, leading the ligature segmentation algorithm to classify them as primary components. The proposed algorithm was tested on the undegraded version of the UPTI dataset, showing that the number of secondary components is less than the primary components, with a ratio of approximately 7:5. The UPTI dataset c",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 24,
    "chunk_index_in_section": 24,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c25",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "an the primary components, with a ratio of approximately 7:5. The UPTI dataset contains 189,584 ligatures according to the ground truths, which our algorithm successfully extracted as shown in Table 4. This table indicates that approximately 189,000 ligatures were extracted from 10,063 text lines, comprising over 332,000 connected components from the UPTI dataset, with an accuracy rate of 99.80%. Fig. 25 presents the percentage statistics of the ligature segmentation algorithm for the four versions of the UPTI dataset.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 25,
    "chunk_index_in_section": 25,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 524,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c26",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "The primary components touching the baseline account for about 93%, and the secondary components not touching the baseline are approximately 87%. Each ligature consists of about 1.7 connected components on average. Table 5 shows the statistics observed by applying the proposed ligature segmentation algorithms to the first sentence of the UPTI dataset. Table 6 compares the segmentation algorithms and their accuracy, along with the sizes of the datasets used. Our algorithm, which relies on quantitative values such as width, height, centroids, and baseline touching rather than heuristics, demons",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 26,
    "chunk_index_in_section": 26,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c27",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "s width, height, centroids, and baseline touching rather than heuristics, demonstrates improved accuracy over previous methods.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 27,
    "chunk_index_in_section": 27,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 127,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c28",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "In conclusion, this research introduces two algorithms for line and ligature segmentation of Urdu Nastaleeq text images. The proposed algorithms achieve accuracy rates of 99.17% for line segmentation and 99.80% for ligature segmentation, outperforming existing methods. This work can be extended to other Nastaleeq script-based languages and modified for handwritten and scene text. Segmentation of multicolumn Urdu newspaper text poses a challenge for future work.\n\nPal, U., and Sarkar, A., \"Recognition of printed Urdu script,\" in Proc. ICDAR, 2003, pp. 1183-1187.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 28,
    "chunk_index_in_section": 28,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 566,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c29",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "Satti, D., and Saleem, K., \"Complexities and implementation challenges in offline Urdu Nastaliq OCR,\" in Proc. Conf. Lang. Technol., 2012, pp. 85-91.\n\nHussain, S., \"Complexity of Asian writing systems: A case study of Nafees Nasta'leeq for Urdu,\" in Proc. 12th AMIC Annu. Conf. e-Worlds, Governments, Bus. Civil Soc., Asian Media Inf. Center, Singapore, 2003.\n\nJaved, S. T., and Hussain, S., \"Improving Nastalique specific pre-recognition process for Urdu OCR,\" in Proc. IEEE 13th Int. Multitopic Conf. (INMIC), Dec. 2009, pp. 1-6.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 29,
    "chunk_index_in_section": 29,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 531,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c30",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "Lehal, G. S., \"Ligature segmentation for Urdu OCR,\" in Proc. 12th Int. Conf. Document Anal. Recognit. (ICDAR), Aug. 2013, pp. 1130-1134.\n\nDin, I. U., Malik, Z., Siddiqi, I., and Khalid, S., \"Line and ligature segmentation in printed Urdu document images,\" J. Appl. Environ. Biol. Sci, vol. 6, no. 3, pp. 114-120, 2016.\n\nShamsher, I., Ahmad, Z., Orakzai, J. K., and Adnan, A., \"OCR for printed Urdu script using feed forward neural network,\" in Proc. World Acad. Sci., Eng. Technol., vol. 23, pp. 172-175, Jan. 2007.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 30,
    "chunk_index_in_section": 30,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 515,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c31",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "Pathan, I. K., and Ramteke, R., \"Recognition of offline handwritten isolated Urdu character,\" Adv. Comput. Res., vol. 4, no. 1, pp. 117-121, 2012.\n\nTariq, J., Nauman, U., and Naru, M. U., \"Softconverter: A novel approach to construct OCR for printed Urdu isolated characters,\" in Proc. 2nd Int. Conf. Comput. Eng. Technol. (ICCET), vol. 3, Apr. 2010, p. V3-495.\n\nAkram, Q. U. A., Hussain, S., and Habib, Z., \"Font size independent OCR for Noori Nastaleeq,\" in Proc. Graduate Colloquium Comput. Sci. (GCCS), vol. 1, Lahore, Pakistan, 2010.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 31,
    "chunk_index_in_section": 31,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 538,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c32",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "Khan, K., Ullah, R., Khan, N. A., and Naveed, K., \"Urdu character recognition using principal component analysis,\" Int. J. Comput. Appl., vol. 60, p. 11, Dec. 2012.\n\nAhmad, Z., Orakzai, J. K., Shamsher, I., and Adnan, A., \"Urdu Nastaleeq optical character recognition,\" in Proc. World Acad. Sci., Eng. Technol., vol. 26, 2007, pp. 249-252.\n\nNawaz, T., Naqvi, S., ur Rehman, H., and Faiz, A., \"Optical character recognition system for Urdu (naskh font) using pattern matching technique,\" Int. J. Image Process., vol. 3, no. 3, p. 92, 2009.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 32,
    "chunk_index_in_section": 32,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 538,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c33",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "Hussain, S., et al., \"Nastalique segmentation-based approach for Urdu OCR,\" Int. J. Document Anal. Recognit., vol. 18, no. 4, pp. 357-374, 2015.\n\nSabbour, N., and Shafait, F., \"A segmentation free approach to Arabic and Urdu OCR,\" in Proc. DRR, Feb. 2013, p. 8658.\n\nJaved, S. T., and Hussain, S., \"Segmentation free nastalique Urdu OCR,\" in Iberoamerican Congress on Pattern Recognition, Turkey: Springer, 2013.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 33,
    "chunk_index_in_section": 33,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 411,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c34",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "Ul-Hasan, A., Ahmed, S. B., Rashid, F., Shafait, F., and Breuel, T. M., \"Offline printed Urdu Nastaleeq script recognition with bidirectional LSTM networks,\" in Proc. 12th Int. Conf. Document Anal. Recognit. (ICDAR), Aug. 2013, pp. 1061-1065.\n\nNaz, S., Umar, A. I., Ahmad, R., Ahmed, S. B., Shirazi, S. H., and Razzak, M. I., \"Urdu Nasta'liq text recognition system based on multidimensional recurrent neural network and statistical features,\" Neural Comput. Appl., vol. 28, no. 2, pp. 1-13, 2015.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 34,
    "chunk_index_in_section": 34,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 497,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c35",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "Naz, S., et al., \"Offline cursive Urdu-Nastaliq script recognition using multidimensional recurrent neural networks,\" Neurocomputing, vol. 177, pp. 228-241, Feb. 2016.\n\nNaz, S., Ahmed, S. B., Ahmad, R., and Razzak, M. I., \"Zoning features and 2DLSTM for Urdu text-line recognition,\" Proc. Comput. Sci., vol. 96, pp. 16-22, Oct. 2016.\n\nAhmad, I., Wang, X., Li, R., and Rasheed, S., \"Offline Urdu Nastaleeq optical character recognition based on stacked denoising autoencoder,\" China Commun., vol. 14, no. 1, pp. 146-157, Jan. 2017.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 35,
    "chunk_index_in_section": 35,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 530,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c36",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "Malik, H., and Fahiem, M. A., \"Segmentation of printed Urdu scripts using structural features,\" in Proc. 2nd Int. Conf. Vis. (VIZ), 2009, pp. 191-195.\n\nKumar, K. S., Kumar, S., and Jawahar, C., \"On segmentation of documents in complex scripts,\" in Proc. 9th Int. Conf. Document Anal. Recognit. (ICDAR), vol. 2, 2007, pp. 1243-1247.\n\nBreuel, T. M., \"Two geometric algorithms for layout analysis,\" in International Workshop on Document Analysis Systems, Berlin, Germany: Springer, 2002, pp. 188-199.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 36,
    "chunk_index_in_section": 36,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 497,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c37",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "Shafait, F., Hasan, A., Keysers, D., and Breuel, T. M., \"Layout analysis of Urdu document images,\" in Proc. 10th IEEE Int. Multitopic Conf. (INMIC), Islamabad, Pakistan, Dec. 2006, pp. 293-298.\n\nBukhari, S. S., Shafait, F., and Breuel, T. M., \"High performance layout analysis of Arabic and Urdu document images,\" in Proc. Int. Conf. Document Anal. Recognit. (ICDAR), Sep. 2011, pp. 1275-1279.\n\nHusain, S. A., \"A multi-tier holistic approach for Urdu Nastaliq recognition,\" in Proc. Int. Multi Topic Conf. Abstracts, 2002, p. 84.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 37,
    "chunk_index_in_section": 37,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 529,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c38",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "Javed, S. T., Hussain, S., Maqbool, A., Asloob, S., Jamil, S., and Moin, H., \"Segmentation free nastalique Urdu OCR,\" World Acad. Sci., Eng. Technol., vol. 46, pp. 456-461, Oct. 2010.\n\nBaird, H. S., \"Document image defect models,\" in Structured Document Image Analysis, Berlin, Germany: Springer, 1992, pp. 546-556.\n\nBukhari, S. S., Shafait, F., and Breuel, T. M., \"Towards generic text-line extraction,\" in Proc. 12th Int. Conf. Document Anal. Recognit. (IC-DAR), Aug. 2013, pp. 748-752.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 38,
    "chunk_index_in_section": 38,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 488,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c39",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "Bukhari, S. S., Shafait, F., and Breuel, T. M., \"Text-line extraction using a convolution of isotropic Gaussian filter with a set of line filters,\" in Proc. Document Anal. Recognit. (ICDAR), 2011, pp. 579-583.\n\nBukhari, S. S., Breuel, T. M., and Shafait, F., \"Textline information extraction from grayscale camera-captured document images,\" in Proc. 16th IEEE Int. Conf. Image Process. (ICIP), Nov. 2009, pp. 2013-2016.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 39,
    "chunk_index_in_section": 39,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 419,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c40",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "Manzoor Ahmed received the B.E. degree in 1996 and the M.Phil. degree in 2010 from Pakistan, and the Ph.D. degree from the Beijing University of Posts and Telecommunications, China, in 2015. He currently holds a post-doctoral position at the Department of Electronic Engineering, FIB Lab, Tsinghua University, China. His research interests focus on image processing, game theoretic-based resource management in hierarchical heterogeneous networks, interference management in small cell networks, 5G networks, D2D communication resource allocation, physical layer security, information security, Fog-",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 40,
    "chunk_index_in_section": 40,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c41",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "ication resource allocation, physical layer security, information security, Fog-RAN, C-RAN, and fog computing. He was awarded the Best Paper Award at the 2014 Game Nets Conference.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 41,
    "chunk_index_in_section": 41,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 180,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text_s0_c42",
    "source_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
    "text": "Rahat Ullah obtained the master’s degree in electronics from the University of Peshawar, Pakistan, in 2007, and the Ph.D. degree from the Beijing University of Posts and Telecommunications, China, in 2017. He served as an Assistant Professor at Sarhad University, Peshawar, Pakistan, for six years and is currently an Associate Professor at the School of Physics and Optoelectronics Engineering, Nanjing University of Information Science and Technology. His research interests include all optical signal processing/communications and machine learning.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 42,
    "chunk_index_in_section": 42,
    "total_chunks_in_section": 43,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 551,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s0_c0",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "Research Article: Obtaining Cross Modal Similarity Metric with Deep Neural Architecture\n\nAuthors: Ruifan Li, Fangxiang Feng, Xiaojie Wang, Peng Lu, Bohan Li\n\nAffiliations:\n- School of Computers, Beijing University of Posts and Telecommunications, Beijing 100876, China\n- Engineering Research Center of Information Networks, Ministry of Education, Beijing 100876, China\n\nCorrespondence: Ruifan Li; rfli@bupt.edu.cn",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 3,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 413,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s0_c1",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "The analysis of complex systems using multimodal data, such as images and text, has garnered significant attention. The key to addressing this issue lies in modeling the relationship between different modalities. Inspired by the successful application of deep neural learning in unimodal data, we introduce a deep neural architecture called bimodal deep architecture (BDA) for assessing similarity across modalities. The BDA consists of three interconnected components. The first component for image and text modalities can be established using popular feature extraction methods. The second compone",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s0_c2",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "can be established using popular feature extraction methods. The second component involves two types of stacked restricted Boltzmann machines (RBMs): a binary-binary RBM stacked over a Gaussian-binary RBM for image modality, and a binary-binary RBM stacked over a replicated softmax RBM for text modality. The third component employs a variant autoencoder with a predefined loss function to discriminatively learn the regularities between modalities. Experimental results demonstrate the effectiveness of our approach in classifying image tags on publicly available datasets.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 575,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s1_c0",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "The demand for analyzing complex systems with numerous variables, including multimodal data like images and text, has increased due to advancements in computational power and storage capacity. Information often presents itself in multiple modalities, and analyzing such heterogeneous data can benefit different modalities. Deep neural learning, inspired by biological propagation phenomena in the human brain, has seen rapid development since 2006 and has achieved success in tasks involving single modal data. We propose the BDA to measure similarity in multimodal systems with many variables. The B",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 3,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 4,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 600,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s1_c1",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "e the BDA to measure similarity in multimodal systems with many variables. The BDA's three components are designed to extract features, stack RBMs, and use a variant autoencoder for discriminative learning.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 4,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 4,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 206,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s1_c2",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "Aspects of the BDA include:\n- Exploration of additional feature extraction methods for the first component.\n- The potential to stack more RBMs in the second component for better representation.\n- A loss function in the third component to maintain small distances for semantically similar bimodal data and large distances for dissimilar data.\n- The focus on image and text bimodal data, with the potential for extension to other modalities.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 5,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 4,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 439,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s1_c3",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "The paper is organized as follows: Section 2 discusses related work, Section 3 presents our deep architecture and learning algorithm, Section 4 introduces datasets and reports experimental results, and Section 5 concludes and suggests future work.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 6,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 4,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 247,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s2_c0",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "Various approaches have been proposed for learning from cross-modal data. These include extensions of topic models, joint models, and undirected Markov random fields, but these single-hidden-layer models struggle with the complexity of images and text. Recent works using deep neural learning have focused on cross-modal retrieval rather than similarity metrics. Bimodal semantic hashing research uses binary codes and Hamming metric for similarity measurement. Other frameworks have limitations to linear projections or are confined to labeled data. Our work builds upon these foundations to address",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 7,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 36,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 600,
    "chunk_method": "hierarchical",
    "importance_weight": 0.66,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s2_c1",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "are confined to labeled data. Our work builds upon these foundations to address the need for a similarity metric in the context of deep neural architectures.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 8,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 157,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s2_c2",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "Our deep framework aims to construct hierarchical representations of bimodal data, as illustrated in Figure 1. It consists of three consecutive components. The first component involves obtaining low-level representations for each type of data using classical single-modal methods. For images, this includes features extracted by MPEG-7 descriptors and gist features. Tag words are represented using the bag-of-words (BOW) model. The second component distills these low-level representations, often with different dimensions, into mid-level representations using two stacked restricted Boltzmann mach",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 9,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s2_c3",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "ions, into mid-level representations using two stacked restricted Boltzmann machines (RBMs) for each modality. The third component introduces a variant of an autoencoder to learn high-level semantic representations, detailed in Section 3.3.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 10,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 240,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s2_c4",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "3.1. Basic Representations\nWe extract representative features as basic representations for different unimodal data. For images, methods such as MPEG-7 and gist descriptors are used. Gist represents scene structure by perceptual dimensions, while MPEG-7 visual descriptors include color layout, color structure, edge histogram, and scalable color. Text modality uses the bag-of-words model, where a dictionary of high-frequency words is created, and tags are represented as vectors with K one/zero elements.\n\n3.2. Learning Intermediate Representations",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 11,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 550,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s2_c5",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "3.2. Learning Intermediate Representations\n\n3.2.1. Modeling Binary Data\nRBMs are used for binary data, with a joint probabilistic distribution defined by an energy function.\n\n3.2.2. Modeling Real-Valued Data\nGaussian RBM is used for real-valued image features, with an energy function that includes the variance of the Gaussian distribution for each visible unit.\n\n3.2.3. Modeling Count Data\nReplicated softmax model is used for text features with count data, with an energy function that considers the total number of words in a document.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 12,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 539,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s2_c6",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "In the second component, two RBMs are stacked for each modality to learn intermediate representations. These RBMs can be trained by the greedy layer-wise training method and the contrastive divergence approximation.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 13,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 215,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s2_c7",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "3.3. Learning Advanced Representations. We propose an autoencoder for bimodal representations to learn similarity. This autoencoder, depicted in Figure 1, consists of two fully connected perceptrons connected by a similarity measure on the code layer. We denote the mapping from inputs to the code layers as 𝑓(𝑝; W𝑓) for image modality and 𝑔(𝑞; W𝑔) for text modality. The compatibility measure between an image 𝑝𝑖 and its given tags 𝑞𝑖 is defined as\n\n𝐶(𝑝𝑖, 𝑞𝑖; W𝑓, W𝑔) = ‖𝑓(𝑝𝑖; W𝑓) − 𝑔(𝑞𝑖; W𝑔)‖2 ,",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 14,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 497,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s2_c8",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "𝐶(𝑝𝑖, 𝑞𝑖; W𝑓, W𝑔) = ‖𝑓(𝑝𝑖; W𝑓) − 𝑔(𝑞𝑖; W𝑔)‖2 ,\n\nwhere ‖ ⋅‖2 is the L2 norm. The loss function to learn similar representations includes data reconstruction errors and a contrastive loss:\n\nℓ(𝑝𝑖, 𝑞𝑖, 𝛿; Θ) = 𝛼(ℓ𝑓(𝑝𝑖; W𝑓) + ℓ𝑔(𝑞𝑖; W𝑔)) + (1 −𝛼) ℓ𝑐(𝑝𝑖, 𝑞𝑖, 𝛿; Θ) ,\n\nwith ℓ𝑓 and ℓ𝑔 as the reconstruction losses and ℓ𝑐 as the contrastive loss. By backpropagation, the autoencoder learns the weights. After learning, the subnetworks have different parameters and can encode new inputs.\n\n4. Experiments and Results",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 15,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 506,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s2_c9",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "4. Experiments and Results\n\nWe evaluate our method for image annotation selection against MLP and CCA on two datasets: the Small ESP Game dataset and the MLC-2013 dataset. The ESP dataset contains 100,000 labeled images with corresponding tags, while the MLC dataset has 1,000 manually labeled images with two labels per image. We use the ESP for training and the MLC for testing. Preprocessing involves generating incorrect counterparts for each image's tag words in the ESP dataset.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 16,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 484,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s2_c10",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "4.2. Settings of Our BDA Method. We describe the settings of our experiments, including feature extraction methods and neuron configurations in the components.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 17,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 159,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s2_c11",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "In the first component, for image modality, we employ three popular methods to extract features. The first group is obtained by preprocessing images, training a K-means dictionary on patches, and extracting soft threshold features, followed by downsampling and global max pooling to create a Bag of Visual Words (BOVW) feature vector. The second group uses MPEG-7 visual descriptors, with varying coefficients for different transformations, resulting in a 784-dimensional feature vector. The third group is derived from the gist descriptor, and the package for this is available at a specified URL.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 18,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s2_c12",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "the gist descriptor, and the package for this is available at a specified URL. These three groups of features represent each image as a 1704-dimensional vector.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 19,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 160,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s2_c13",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "For text representation, we use a bag-of-words model with a dictionary of 4000 high-frequency words. The second component employs a neural configuration of 1704-1024-1024 for image modality and 4000-1024-1024 for text modality within the Gaussian-Bernoulli RBM and replicated softmax. The third component uses a 1024-512 neuron configuration for both modalities in the autoencoders.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 20,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 382,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s2_c14",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "We describe the settings of benchmark methods, including Multilayer Perceptrons (MLP) with two hidden layers and Canonical Correlation Analysis (CCA) with RBMs. The MLP structure has 1704 input neurons, two hidden layers with 1024 neurons each, and 4000 output neurons, using logistic activation functions. The CCA system is configured with 1704 neurons for image representation and a replicated softmax RBM with 4000 input and 1024 output neurons, setting the number of canonical components to 1024.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 21,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 500,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s2_c15",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "Evaluation criteria are based on the accuracy, which we define as the area under the ROC curve. The ROC is a plot of false positive rate against true positive rate at various decision thresholds. The accuracy is computed as the integral of the true positive rate over the false positive rate.\n\n---",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 22,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 297,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s2_c16",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "---\n\nFigure 5: Our experimental setup for Canonical Correlation Analysis (CCA) involves representing image modality with MPEG-7 and gist descriptors, resulting in a vector of size 1,704. This is processed by a Gaussian Restricted Boltzmann Machine (RBM) with 1,704 visible and 1,024 hidden neurons. Similarly, text modality is represented by a Bag of Words (BoW) model, forming a vector of size 4,000, and a replicated softmax RBM with 4,000 visible and 1,024 hidden neurons is employed. A CCA model with 1,024 twin inputs and outputs is then utilized for bimodal representation learning.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 23,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 588,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s2_c17",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "The false positive rate at threshold 𝑡, 𝑈1(𝑡), is expressed as the integral of the density function for class 1, 𝑢1(𝑠), from negative infinity to 𝑡. In empirical distributions, this integral is replaced by a sum. We define a continuous valued output 𝑃(𝑝𝑖) for the positive class as the squared compatibility metric of an image 𝑝𝑖 with its true tag words 𝑞𝑖, divided by the sum of squared compatibility metrics with both true and false tag words:\n\n𝑃(𝑝𝑖) ≜ 𝐶2 (𝑝𝑖, 𝑞𝑖) / (𝐶2 (𝑝𝑖, 𝑞𝑖) + 𝐶2 (𝑝𝑖, ̃𝑞𝑖)). (9)",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 24,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 502,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s2_c18",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "𝑃(𝑝𝑖) ≜ 𝐶2 (𝑝𝑖, 𝑞𝑖) / (𝐶2 (𝑝𝑖, 𝑞𝑖) + 𝐶2 (𝑝𝑖, ̃𝑞𝑖)). (9)\n\n4.5. Results: Our deep neural architecture outperforms an MLP-based system with two hidden layers and a CCA-based system with two RBMs, achieving an accuracy of 88.96%. The CCA-based system performs better than the MLP-based system. The impact of hyperparameter 𝛼 in the loss function is investigated, with the best performance at 𝛼=0.4.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 25,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 394,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s2_c19",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "4.6. Discussion: The models share commonalities in their use of modal-specific low-level representations and cross-modal metrics. The MLP-based system assumes a direct nonlinear mapping, while the CCA-based and our BDA-based system assume a common representation space. The BDA system's higher accuracy is due to its nonlinear reconstruction and compatibility constraints, which capture information not addressed by CCA.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 26,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 420,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s2_c20",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "5. Conclusion: We propose a deep neural architecture for measuring similarity between image and text modalities. The framework combines feature extraction and deep neural networks, demonstrating effectiveness in classifying image tags. It is flexible and can be extended to other modalities. Future work will explore this flexibility in complex systems.\n\nConflict of Interests: None declared.\n\nAcknowledgments: The work was supported by various funding sources and the authors thank the editor and reviewers for their contributions.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 27,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 532,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s2_c21",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "References:\n[1] L. Li et al., “Chaos-order transition in foraging behavior of ants,” Proceedings of the National Academy of Sciences, vol. 111, no. 23, pp. 8392–8397, 2014.\n[2] Y. Bengio, “Learning deep architectures for AI,” Foundations and Trends in Machine Learning, vol. 2, no. 1, pp. 1–27, 2009.\n[3] K. Chen and A. Salman, “Learning speaker-specific characteristics with a deep neural architecture,” IEEE Transactions on Neural Networks, vol. 22, no. 11, pp. 1744–1756, 2011.\n\n---",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 28,
    "chunk_index_in_section": 21,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 485,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s2_c22",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "---\n\nMohamed, A.-R., Dahl, G. E., & Hinton, G. (2012). Acoustic modeling using deep belief networks. IEEE Transactions on Audio, Speech and Language Processing, 20(1), 14–22.\n\nHeigold, G., Vanhoucke, V., Senior, A., et al. (2013). Multilingual acoustic models using distributed deep neural networks. In Proceedings of the 38th IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP ’13) (pp. 8619–8623). IEEE Computer Society, Vancouver, Canada.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 29,
    "chunk_index_in_section": 22,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 472,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s2_c23",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "Yu, D., Deng, L., & Seide, F. (2013). The deep tensor neural network with applications to large vocabulary speech recognition. IEEE Transactions on Audio, Speech and Language Processing, 21(2), 388–396.\n\nHinton, G. E., & Salakhutdinov, R. R. (2006). Reducing the dimensionality of data with neural networks. Science, 313(5786), 504–507.\n\nLee, H., Grosse, R., Ranganath, R., & Ng, A. Y. (2011). Unsupervised learning of hierarchical representations with convolutional deep belief networks. Communications of the ACM, 54(10), 95–103.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 30,
    "chunk_index_in_section": 23,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 531,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s2_c24",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "Krizhevsky, A., Sutskever, I., & Hinton, G. (2012). Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, vol. 25 (pp. 1106–1114). Morgan Kaufmann, Lake Tahoe, Nev, USA.\n\nGoodfellow, I. J., Erhan, D., Carrier, P. L., et al. (2013). Challenges in representation learning: a report on three machine learning contests. In Proceedings of the 20th International Conference on Neural Information Processing (pp. 117–124). IEEE Computer Society, Daegu, Korea.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 31,
    "chunk_index_in_section": 24,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 517,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s2_c25",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "Farabet, C., Couprie, C., Najman, L., & Lecun, Y. (2013). Learning hierarchical features for scene labeling. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8), 1915–1929.\n\nFeng, F., Wang, X., & Li, R. (2014). Cross-modal retrieval with correspondence autoencoder. In Proceedings of the 22nd ACM International Conference on Multimedia (pp. 7–16). ACM, Orlando, Fla, USA.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 32,
    "chunk_index_in_section": 25,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 389,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s2_c26",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "Blei, D. M., & Jordan, M. I. (2003). Modeling annotated data. In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (pp. 127–134). ACM, New York, NY, USA.\n\nXing, E. P., Yan, R., & Hauptmann, A. G. (2005). Mining associated text and images with dual-wing harmoniums. In Proceedings of the 21st Annual Conference on Uncertainty in Artificial Intelligence (UAI ’05) (pp. 633–641). AUAI Press, Arlington, Va, USA.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 33,
    "chunk_index_in_section": 26,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 477,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s2_c27",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "Jia, Y., Salzmann, M., & Darrell, T. (2011). Learning cross-modality similarity for multinomial data. In Proceedings of ACM the International Conference on Multimedia Information Retrieval (pp. 2407–2414). IEEE, Washington, DC, USA.\n\nChopra, S., Hadsell, R., & LeCun, Y. (2005). Learning a similarity metric discriminatively, with application to face verification. In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR ’05) (pp. 539–546). IEEE, Washington, DC, USA.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 34,
    "chunk_index_in_section": 27,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 515,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s2_c28",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "Ngiam, J., Khosla, A., Kim, M., Nam, J., Lee, H., & Ng, A. Y. (2011). Multimodal deep learning. In Proceedings of the 28th International Conference on Machine Learning (pp. 689–696). Omnipress, Bellevue, Wash, USA.\n\nSrivastava, N., & Salakhutdinov, R. (2012). Multimodal learning with deep Boltzmann machines. In Advances in Neural Information Processing Systems, vol. 25 (pp. 2231–2239). Morgan Kaufmann, Lake Tahoe, Nev, USA.\n\nMcFee, B., & Lanckriet, G. (2011). Learning multi-modal similarity. Journal of Machine Learning Research, 12(8), 491–523.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 35,
    "chunk_index_in_section": 28,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 550,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s2_c29",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "Masci, J., Bronstein, M. M., Bronstein, A. M., & Schmidhuber, J. (2014). Multimodal similarity-preserving hashing. IEEE Transactions on Pattern Analysis and Machine Intelligence, 36(4), 824–830.\n\nSmolensky, P. (1986). Information processing in dynamical systems: foundations of harmony theory. In Parallel Distributed Processing: Explorations in the Microstructure of Cognition, D. E. Rumelhart, J. L. McClelland, and C. PDP Research Group (Eds.), vol. 1 (pp. 194–281). MIT Press, Cambridge, Mass, USA.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 36,
    "chunk_index_in_section": 29,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 502,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s2_c30",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "Welling, M., Rosen-Zvi, M., & Hinton, G. (2004). Exponential family harmoniums with an application to information retrieval. In Advances in Neural Information Processing Systems 17 (pp. 501–508). Morgan Kaufmann, Vancouver, Canada.\n\nSalakhutdinov, R., & Hinton, G. (2009). Replicated softmax: an undirected topic model. In Advances in Neural Information Processing Systems, vol. 22 (pp. 1607–1614). Morgan Kaufmann, Vancouver, Canada.\n\nHinton, G. E., Osindero, S., & Teh, Y. W. (2006). A fast learning algorithm for deep belief nets. Neural Computation, 18(7), 1527–1554.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 37,
    "chunk_index_in_section": 30,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 571,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s2_c31",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "Bengio, Y., Lamblin, P., Popovici, D., & Larochelle, H. (2007). Greedy layer-wise training of deep networks. In Advances in Neural Information Processing Systems 19 (pp. 153–160). MIT Press, Vancouver, Canada.\n\nHinton, G. E. (2002). Training products of experts by minimizing contrastive divergence. Neural Computation, 14(8), 1771–1800.\n\nLeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., & Huang, F.-J. (2006). A tutorial on energy-based learning. In Predicting Structured Data, G. Bakir, T. Hofman, B. Scholkopf, A. Smola, and B. Taskar (Eds.) (pp. 1–59). MIT Press, Cambridge, Mass, USA.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 38,
    "chunk_index_in_section": 31,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 589,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s2_c32",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning internal representations by error propagation. Nature, 323(6088), 533–536.\n\nvon Ahn, L., & Dabbish, L. (2004). Labeling images with a computer game. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (pp. 319–326). ACM Press, Vienna, Austria.\n\nPinto, N., Cox, D. D., & DiCarlo, J. J. (2008). Why is real-world visual object recognition hard? PLoS Computational Biology, 4(1), e27.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 39,
    "chunk_index_in_section": 32,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 475,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s2_c33",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "Coates, A., & Ng, A. Y. (2011). The importance of encoding versus training with sparse coding and vector quantization. In Proceedings of the 28th International Conference on Machine Learning (ICML ’11) (pp. 921–928). Bellevue, Wash, USA.\n\nHam, F. M., & Kostanic, I. (2000). Principles of Neurocomputing for Science and Engineering. McGraw-Hill Higher Education, 1st edition.\n\nAnderson, T. W. (2003). An Introduction to Multivariate Statistical Analysis. John Wiley and Sons, New York, NY, USA, 3rd edition.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 40,
    "chunk_index_in_section": 33,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 506,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s2_c34",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "Hardoon, D. R., Szedmak, S., & Shawe-Taylor, J. (2004). Canonical correlation analysis: an overview with application to learning methods. Neural Computation, 16(12), 2639–2664.\n\nRasiwasia, N., Pereira, J. C., Coviello, E., et al. (2010). A new approach to cross-modal multimedia retrieval. In Proceedings of the 18th ACM International Conference on Multimedia (pp. 251–260). ACM, New York, NY, USA.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 41,
    "chunk_index_in_section": 34,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 398,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture_s2_c35",
    "source_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
    "text": "Kim, J., Nam, J., & Gurevych, I. (2012). Learning semantics with deep belief network for cross-language information retrieval. In Proceedings of the 24th International Conference on Computational Linguistics (pp. 579–588). ACL Press, IIT Bombay, India.\n\nFawcett, T. (2006). An introduction to ROC analysis. Pattern Recognition Letters, 27(8), 861–874.\n\nShinkareva, S. V., Malave, V. L., Mason, R. A., Mitchell, T. M., & Just, M. A. (2011). Commonality of neural representations of words and pictures. NeuroImage, 54(3), 2418–2425.",
    "section_title": "Related Work",
    "section_type": "related_work",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 42,
    "chunk_index_in_section": 35,
    "total_chunks_in_section": 36,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 530,
    "chunk_method": "hierarchical",
    "importance_weight": 0.63,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c0",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "---\n\nModality Disentangled Discriminator for Text-to-Image Synthesis\n\nFangxiang Feng, Tianrui Niu, Ruifan Li, Member, IEEE, and Xiaojie Wang",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 53,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 140,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c1",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "Abstract—Text-to-image (T2I) synthesis generates photo-realistic images from text descriptions, bridging vision and language. Existing discriminators do not differentiate between content and style parts of an image, limiting effectiveness in generating content and manipulating style. We propose a modality disentangled discriminator that extracts disentangled representations of content and style. This enhances the discriminator's ability to capture text-image correlation and allows style transfer. The discriminator is integrated into the AttnGAN and DM-GAN models, demonstrating superior perfor",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c2",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "is integrated into the AttnGAN and DM-GAN models, demonstrating superior performance on CUB, Oxford-102, and COCO datasets.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 123,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c3",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "Index Terms—text-to-image synthesis, generative adversarial networks, multi-modal disentangled representation learning.\n\nI. INTRODUCTION",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 136,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c4",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "I. INTRODUCTION\n\nText-to-image (T2I) synthesis is challenging, with applications in interactive art and computer-aided drawing. Most methods rely on Generative Adversarial Networks (GANs), with conditional GANs used for text-conditioned synthesis. Current research focuses on enhancing image-text correlation, but existing discriminators do not efficiently distinguish content from style, affecting conditional loss effectiveness. Our modality disentangled discriminator addresses this by separately classifying content and style.\n\n---\n\n---\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 545,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c5",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "---\n\n---\n\n---\n\nBased on the observations, we propose a modality disentangled discriminator to distinguish between text-related and text-irrelevant parts of an image. Our approach enforces early layers of the discriminator to act as an image encoder, disentangling the image representation into common text-related and modality-specific text-independent components. By utilizing the common representation, the conditional loss can focus on image-text correlation, while an unconditional loss classifies the text-irrelevant part.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 5,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 527,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c6",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "The modality disentangled discriminator is integrated into two models: AttnGAN and DM-GAN. It enhances performance on image generation quality and text-image correlation measures. The modality-specific representation facilitates style transfer tasks, such as manipulating image synthesis.\n\nOur work makes three contributions:",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 6,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 325,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c7",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "Our work makes three contributions:\n\n1. The proposed discriminator is the first to learn modality disentangled representation for text-to-image synthesis, enhancing discrimination of image-text correlation and facilitating image synthesis manipulation.\n2. We implement modality disentanglement by introducing two correlation losses without increasing model size or reducing efficiency.\n3. Experimental results show that our discriminator improves AttnGAN and DM-GAN, achieving excellent performance in style transfer and style embedding interpolation tasks.\n\nII. RELATED WORK",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 7,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 575,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c8",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "II. RELATED WORK\n\nA. Text-to-Image Synthesis\n\nGANs are extensively used in multimedia synthesis tasks, including text-to-image synthesis. Most T2I models are GAN-based and incorporate attention mechanisms to improve generation quality. The design of the discriminator is a research focus, with approaches aiming to enhance the model's ability to determine generated image aspects.\n\nB. Disentangled Representation Learning",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 8,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 421,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c9",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "B. Disentangled Representation Learning\n\nThere is a growing body of work on learning disentangled representations for various data types. TD-GAN and other approaches map multimodal data into disentangled spaces, facilitating generation and manipulation.\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 9,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 258,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c10",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "The GAN-based models with our modality disentangled discriminator share similarities with several disentangled image-to-image (I2I) translation models, such as UNIT and MUNIT, but differ in three main aspects. First, we address the T2I generation problem, where the semantic gap between text and image is larger than that between images of different styles. Second, our discriminator aims to disentangle the common and modality-specific parts of multimodal data for easier manipulation of image synthesis and enhanced text-image correlation, differing from I2I translation models that generate diver",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 10,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c11",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "ext-image correlation, differing from I2I translation models that generate diverse outputs from a given source domain image. Third, we achieve modality disentanglement by repurposing the discriminator in GAN as a feature extractor, motivated by the need to reduce model size and increase efficiency, and to enhance the training of the discriminator that judges image-text correlation using Pearson correlation.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 11,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 410,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c12",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "Representation learning in the discriminator has been explored in recent works such as InfoGAN, AC-GAN, and NICE-GAN. Our model differs from InfoGAN/AC-GAN in terms of goal, information reconstruction, and application. We aim to disentangle the common and modality-specific representation of multimodal data, reconstruct all information, and apply the model to text-conditioned image generation.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 12,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 395,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c13",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "In the GAN with modality disentangled discriminator (GAN-MDD), T2I synthesis is performed using four modules: text encoder ET, generator network (F, G), image encoder EI, and discriminator network D∗. The discriminator includes two modules: the image encoder and the discriminator network.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 13,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 289,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c14",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "The text encoder ET is a simple one-layer fully-connected network that converts text embedding ϕ to text feature htc. The generator network generates the fake image ˆx from hidden states h, conditional on text description and noise. The image encoder EI extracts modality disentangled features from synthesized or real images. The discriminator network has three branches to produce decision scores for image style, text-image semantic consistency, and image reality.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 14,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 467,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c15",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "Objective functions consist of content loss, style loss, and adversarial loss. The content loss uses triplet loss with Pearson correlation as the similarity score function, aiming to maximize the correlation between matched image-text pairs and minimize it for mismatched pairs.\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 15,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 283,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c16",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "---\n\n2) Style Loss: The image generation is conditioned on the text description and a noise sample. The text description captures the common information, while the noise vector captures the modality-specific information. The style loss aims to maximize the correlation between the modality-specific feature of the synthesized image and its corresponding noise vector:\n\nLS = −ρ(z, hss)",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 16,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 384,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c17",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "LS = −ρ(z, hss) \n\nwhere ρ is the Pearson correlation, z is the noise vector, and hss is the modality-specific feature. This objective is based on the idea that the style information in the synthesized image is determined by the noise vector.\n\n3) Adversarial Loss: We employ two adversarial losses: the unconditional loss for determining if the image is real or fake, and the conditional loss for determining if the image and the condition match. The GAN-MDD uses disentangled image features for the conditional loss. The generator and discriminator losses are defined as follows:",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 17,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 579,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c18",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "LG = −Eˆx∼pG[log(sc(ˆx))] \u0002 \u0003\u0004 \u0005 conditional loss\n−Eˆx∼pG[log(ss(ˆx))] \u0002 \u0003\u0004 \u0005 unconditional loss for style\n−Eˆx∼pG[log(si(ˆx))] \u0002 \u0003\u0004 \u0005 unconditional loss for image\n\nLD = −Ex∼pdata[log(sc(x))] −Eˆx∼pGi [log(1 −sc(ˆx))]\n\u0002 \u0003\u0004 \u0005 conditional loss\n−Ex∼pdata[log(ss(x))] −Eˆx∼pGi[log(1 −ss(ˆx))]\n\u0002 \u0003\u0004 \u0005 unconditional loss for style\n−Ex∼pdata[log(si(x))] −Eˆx∼pGi[log(1 −si(ˆx))]\n\n4) Training Procedure: The total loss of the GAN-MDD is obtained by combining the content, style, and adversarial losses:\n\nLDtotal = LD + λCLC + λSLS \nLGtotal = LG + λCLC",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 18,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 543,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c19",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "LDtotal = LD + λCLC + λSLS \nLGtotal = LG + λCLC \n\nThe GAN-MDD is trained by alternatively updating the parameters of LDtotal and LGtotal. λC and λS balance the content and style losses.\n\nIV. EXPERIMENTS\n\nWe validate our discriminator by substituting the discriminator of AttnGAN and DM-GAN with the modality disentangled discriminator, creating AttnGAN-MDD and DM-GAN-MDD. These models are evaluated on the CUB, Oxford-102, and COCO datasets.\n\nA. Implementation Details",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 19,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 469,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c20",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "A. Implementation Details\n\nFor AttnGAN-MDD, the last down-sampling block is split into two tensors: a 100x4x4 tensor for the modality-specific feature and a 412x4x4 tensor for the common feature. These are transformed into 100-dimensional modality-specific and 128-dimensional common image features. The tensors are processed by three branches to obtain decision scores.\n\nB. Evaluation",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 20,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 385,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c21",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "B. Evaluation\n\n1) Image Generation Quality Measure: We evaluate our model using Inception Score (IS) and Fréchet Inception Distance (FID). Table I shows the comparison of our GAN-MDDs with other GAN models on the CUB and COCO datasets. Table II presents the R-Precision results on the three datasets.\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 21,
    "chunk_index_in_section": 21,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 305,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c22",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "The DM-GAN-MDD model achieves improved performance metrics compared to DM-GAN on various datasets. On the CUB dataset, it increases the Inception Score (IS) from 4.75 to 4.86 and decreases the Fréchet Inception Distance (FID) from 16.09 to 15.76. Similarly, on the Oxford-102 dataset, the IS is enhanced from 4.18 to 4.23, with the FID reduced from 41.35 to 40.18. On the COCO dataset, the IS jumps from 30.49 to 34.46, and the FID plummets from 32.64 to 24.30. These improvements indicate that the proposed modality disentangled discriminator contributes to higher-quality image generation in GAN-b",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 22,
    "chunk_index_in_section": 22,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c23",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "sentangled discriminator contributes to higher-quality image generation in GAN-based Text-to-Image (T2I) models.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 23,
    "chunk_index_in_section": 23,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 112,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c24",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "For Text-Image Correlation Measure, the R-precision metric is used. Our GAN-MDD models outperform baseline models on all datasets, with AttnGAN-MDD increasing the R-precision rate from 67.82% to 69.88% on CUB, from 64.98% to 74.29% on Oxford-102, and from 85.47% to 88.27% on COCO compared to AttnGAN. DM-GAN-MDD shows similar improvements over DM-GAN.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 24,
    "chunk_index_in_section": 24,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 352,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c25",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "In terms of style manipulation experiments on the CUB dataset, two types of experiments are conducted. The first involves style transfer, where the model uses modality-specific features for direct style transfer, achieving more accurate style reconstruction compared to GAN-INT-CLS. The second experiment is style embedding interpolation, where linear interpolations of style codes in the latent space demonstrate the model's ability to precisely capture and express styles. The results show that the proposed models can effectively disentangle content and style, with the basic model architecture n",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 25,
    "chunk_index_in_section": 25,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c26",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "n effectively disentangle content and style, with the basic model architecture not affecting the quality of disentanglement.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 26,
    "chunk_index_in_section": 26,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 124,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c27",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "As discussed in [13], the style of images, such as the pose of objects and the background, is not described in texts. Our model accurately captures these factors, as evidenced by the smooth transition of both pose and background from left to right in the figure. The left half shows a bird on a horizontal branch facing left with a light background, while the right half features the bird turned right on an inclined branch. The content (bird's color) remains largely unchanged, demonstrating the model's ability to disentangle features. Notably, our model can generate completely novel images by co",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 27,
    "chunk_index_in_section": 27,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c28",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "entangle features. Notably, our model can generate completely novel images by combining different style-providing images without compromising generation quality, as style and content probabilities are modeled separately.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 28,
    "chunk_index_in_section": 28,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 220,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c29",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "D. Ablation Study\nWe conducted four ablation experiments to understand the modality disentangled discriminator better. First, we removed some losses related to the discriminator and observed the impact on the model. We analyzed the effects of different trade-off weights for content and style losses under two similarity metrics. We also presented results when the modality disentangled discriminator does not use a weight-sharing strategy and when it is applied at different stages of AttnGAN.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 29,
    "chunk_index_in_section": 29,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 494,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c30",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "1) Removing Some Losses Involving MDD: We modified the discriminator of the baseline GAN model by adding content loss (LC), style loss (LS), changing the conditional adversarial loss's representation dependency (Ddis), and adding style adversarial loss (Ds). Table III summarizes the IS, FID, and R-precision results on the CUB dataset for these modifications. The model with only content or style loss performs slightly better than the baseline. Using both content and style losses improves performance, and adding the style adversarial loss further enhances results.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 30,
    "chunk_index_in_section": 30,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 568,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c31",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "2) Different Configurations of λC, λS, and Similarity Measure: Figure 6 shows the IS and FID on the CUB dataset for seven different ratios of content and style loss weights. The model with balanced content and style loss weights performs best. Pearson correlation as a similarity score function outperforms L2 distance, as it requires fewer constraints, allowing the disentangled representation to serve the discriminator more effectively.\n\nTable IV presents the results of applying the MDD in different stages of AttnGAN on the CUB dataset.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 31,
    "chunk_index_in_section": 31,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 541,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c32",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "Figure 7 illustrates the qualitative results of the style transfer task. It is evident that models using L2 distance as a similarity measure yield poor results. Regardless of the ratio of λC to λS, the images generated with L2 are of low quality, and the style transfer outcomes are similarly poor. For instance, the orientation of the birds in all generated images is reversed.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 32,
    "chunk_index_in_section": 32,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 378,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c33",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "Figure 6 also presents the results of AttnGAN-MDD without weight sharing. It shows that MDD without weight sharing slightly outperforms its counterpart with weight sharing. This suggests that our proposed disentangled representation learning strategy can be applied to the discriminator without weight sharing. However, it should be noted that without weight sharing, the discriminator's parameters nearly triple. Consequently, when training a three-stage model to generate 256x256 resolution images, only a small batch size can be utilized.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 33,
    "chunk_index_in_section": 33,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 541,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c34",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "We further investigate the application of MDD in different stages of AttnGAN. Table IV displays the results of using MDD to replace only a single stage of AttnGAN on the CUB dataset. The last column of Table IV shows the R-Precision rate calculated using the common representation to demonstrate the impact of the disentangled common representation on image-text correlation. Since the common image feature (hsc) and text feature (htc) are in the same feature space, we compute the Pearson correlation of htc and hsc as a cross-modal correlation measure.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 34,
    "chunk_index_in_section": 34,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 554,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c35",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "The table reveals that applying MDD to only one stage is not as effective as applying it to all three stages, and applying MDD in the high-resolution stage is more beneficial than in the low-resolution stage. This highlights the effectiveness of our proposed discriminator. The more stages of MDD applied to AttnGAN, the better the performance. Moreover, the R-Precision rate based on the common representation aligns with other evaluation metrics, indicating that the higher the R-Prec-htc-hsc value, the better the quality of the generated image and the stronger the correlation between the image",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 35,
    "chunk_index_in_section": 35,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c36",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "ality of the generated image and the stronger the correlation between the image and text description.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 36,
    "chunk_index_in_section": 36,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 101,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c37",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "In conclusion, we propose a novel discriminator to learn the modality disentangled representation for text-to-image synthesis. The common representation enhances the correlation between generated images and text descriptions, while the modality-specific representation allows direct style manipulation. The acquisition of modality disentangled representation enables simultaneous control of content and style in the generated image. Compared to baseline models, our GAN-MDDs offer similar model size and training/testing time but with improved performance and capabilities. Experimental results on t",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 37,
    "chunk_index_in_section": 37,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c38",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "g time but with improved performance and capabilities. Experimental results on three benchmark datasets demonstrate the effectiveness of our DM-GAN-MDD.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 38,
    "chunk_index_in_section": 38,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 152,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c39",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "The authors acknowledge the valuable comments from the editor and anonymous reviewers, which have helped improve the final version of this article.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 39,
    "chunk_index_in_section": 39,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 147,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c40",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "Liu et al. introduced unsupervised image-to-image translation networks [27]. Huang et al. extended this work to multimodal unsupervised image-to-image translation [28]. Zhu et al. proposed cycle-consistent adversarial networks for unpaired image-to-image translation [29]. Wang et al. presented video-to-video synthesis [30], while Chen et al. introduced Mocycle-GAN for unpaired video-to-video translation [31]. Pan et al. presented a method for generating videos from captions [32].",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 40,
    "chunk_index_in_section": 40,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 484,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c41",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "Li et al. proposed object-driven text-to-image synthesis [33], and Liang et al. introduced CPGAN for text-to-image synthesis [34]. Li et al. presented lightweight generative adversarial networks for text-guided image manipulation [35]. Bengio et al. reviewed representation learning [36], and Locatello et al. challenged common assumptions in the unsupervised learning of disentangled representations [37]. Yang et al. proposed crossing-domain generative adversarial networks for unsupervised multi-domain image-to-image translation [38], and Gonzalez-Garcia et al. worked on image-to-image translat",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 41,
    "chunk_index_in_section": 41,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c42",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "e translation [38], and Gonzalez-Garcia et al. worked on image-to-image translation for cross-domain disentanglement [39].",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 42,
    "chunk_index_in_section": 42,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 122,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c43",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "Liu et al. explored disentangled feature representation beyond face identification [40], and Donahue et al. semantically decomposed the latent spaces of generative adversarial networks [41]. Lin et al. explored explicit domain supervision for latent space disentanglement in unpaired image-to-image translation [42]. Yan et al. presented Attribute2Image for conditional image generation from visual attributes [43], and Ma et al. worked on disentangled person image generation [44].",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 43,
    "chunk_index_in_section": 43,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 482,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c44",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "Vondrick et al. generated videos with scene dynamics [45], and Havrylov and Joulin proposed cooperative learning of disjoint syntax and semantics [46]. Hsu et al. unsupervisedly learned disentangled and interpretable representations from sequential data [47], and Ma et al. learned disentangled representations for recommendation [48]. Liu et al. presented independence promoted graph disentangled networks [49], and Tsai et al. learned factorized multimodal representations [50]. Saha et al. learned disentangled multimodal representations for the fashion domain [51], and Wang et al. proposed tag",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 44,
    "chunk_index_in_section": 44,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c45",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "modal representations for the fashion domain [51], and Wang et al. proposed tag disentangled generative adversarial network for object image re-rendering [52].",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 45,
    "chunk_index_in_section": 45,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 159,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c46",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "Zhou et al. presented talking face generation by adversarially disentangled audio-visual representation [53], and Chen et al. introduced InfoGAN for interpretable representation learning [54]. Odena et al. worked on conditional image synthesis with auxiliary classifier GANs [55], and Chen et al. reused discriminators for encoding towards unsupervised image-to-image translation [56]. Karpathy and Fei-Fei aligned deep visual-semantic for generating image descriptions [57], and Kiros et al. unified visual-semantic embeddings with multimodal neural language models [58]. Lee et al. proposed stacke",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 46,
    "chunk_index_in_section": 46,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c47",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "beddings with multimodal neural language models [58]. Lee et al. proposed stacked cross attention for image-text matching [59], and Nilsback and Zisserman automated flower classification over a large number of classes [60].",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 47,
    "chunk_index_in_section": 47,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 223,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c48",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "Salimans et al. improved techniques for training GANs [61], and Heusel et al. showed that GANs trained by a two time-scale update rule converge to a local Nash equilibrium [62].",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 48,
    "chunk_index_in_section": 48,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 177,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c49",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "Ruifan Li (Member, IEEE) obtained the B.S. and M.S. degrees in control systems and circuits and systems from Huazhong University of Science and Technology, Wuhan, China, in 1998 and 2001, respectively, and the Ph.D. degree in signal and information processing from Beijing University of Posts and Telecommunications, Beijing, China, in 2006. He is an Associate Professor at the School of Artificial Intelligence, Beijing University of Posts and Telecommunications (BUPT), and is associated with the Engineering Research Center of Information Networks, Ministry of Education. In 2006, he joined the S",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 49,
    "chunk_index_in_section": 49,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c50",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "Center of Information Networks, Ministry of Education. In 2006, he joined the School of Computer Science, BUPT. He was a Visiting Scholar at the Information Sciences Institute, University of Southern California, Los Angeles, CA, USA, from February 2011 for one year. His research interests include multimedia information processing, neural information processing, and statistical machine learning. He is a member of the China Computer Federation and the Chinese Association of Artificial Intelligence. He has been an Active Reviewer for numerous peer-reviewed journals.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 50,
    "chunk_index_in_section": 50,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 569,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c51",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "Xiaojie Wang received the Ph.D. degree from Beihang University, Beijing, China, in 1996. He is a Full Professor and the Director of the Centre for Intelligence Science and Technology at Beijing University of Posts and Telecommunications, Beijing, China. His research focuses on natural language processing and multimodal cognitive computing. He serves as an Executive Member of the Council of Chinese Association of Artificial Intelligence and as the Director of the Natural Language Processing Committee. He is a Member of the Council of Chinese Information Processing Society and the Chinese Proce",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 51,
    "chunk_index_in_section": 51,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis_s0_c52",
    "source_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
    "text": "r of the Council of Chinese Information Processing Society and the Chinese Processing Committee of China Computer Federation.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 52,
    "chunk_index_in_section": 52,
    "total_chunks_in_section": 53,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 125,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification_s0_c0",
    "source_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification",
    "text": "---\n\nMulti-level fusion with deep neural networks for multimodal sentiment classification\n\nZhang Guangwei1, Zhao Bing2, Li Ruifan3\n\n1. School of Computer Sciences, Beijing University of Posts and Communications, Beijing 100876, China\n2. School of Science, Yanshan University, Qinhuangdao 066004, China\n3. School of Artificial Intelligence, Beijing University of Posts and Communications, Beijing 100876, China",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 5,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 409,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification_s1_c0",
    "source_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification",
    "text": "The task of multimodal sentiment classification associates multimodal information, such as images and texts, with sentiment polarities. Most existing methods treat features from various levels independently, lacking effective feature fusion. We propose a multi-level fusion classification (MFC) model that fuses features from different levels by exploiting their dependencies. The architecture uses c",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 5,
    "chunk_index": 1,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 2,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 400,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9900000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification_s1_c1",
    "source_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification",
    "text": "oiting their dependencies. The architecture uses convolutional neural networks (CNNs) to extract features in image and text modalities and a bi-directional (Bi) recurrent neural network (RNN) to integrate features from different CNN layers. A conflict detection module addresses conflicts between modalities. Experiments on the Flickr dataset show the MFC method achieves comparable performance with\ne MFC method achieves comparable performance with strong baseline methods.\nKeywords: multimodal fusion, sentiment analysis, deep learning",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 5,
    "chunk_index": 2,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 2,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 537,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9450000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification_s2_c0",
    "source_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification",
    "text": "Online social networks allow users to post content in various forms, playing an increasingly important role in daily life. Deep neural networks have shown remarkable performance in fields such as computer vision and natural language processing. However, most sentiment analysis research focuses on a single modality. This paper addresses sentiment prediction using joint textual and visual information. We propose a feature fusion method based on multiple neural networks and a conflict detection module to extend our model, the rectified multi-level fusion classification (R-MFC). Experimental resul",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 3,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 2,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 600,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification_s2_c1",
    "source_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification",
    "text": "del, the rectified multi-level fusion classification (R-MFC). Experimental results demonstrate the effectiveness of the proposed method for joint vision and text sentiment analysis.",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 4,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 2,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 181,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification_s3_c0",
    "source_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification",
    "text": "Previous work utilized high-level features for fusion, such as the last outputs of different modal model layers. With the complexity of emotions and differences between text and visual sentiment, more work has proposed exploiting multiple levels of features. CNNs and RNNs have achieved good results in visual and textual sentiment analysis. We explicitly exploit the dependency between low-level and high-level features to improve sentiment classification.\n\n3. MFC model",
    "section_title": "Related work",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 5,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 13,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 471,
    "chunk_method": "hierarchical",
    "importance_weight": 0.66,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification_s3_c1",
    "source_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification",
    "text": "3. MFC model\n\nThe MFC model consists of image and text feature extractors and bi-directional gated recurrent unit (Bi-GRU) feature fusion. The input image and text are processed by CNNs with multiple branches to extract features at different levels. Bi-GRU fusion integrates these features by exploiting dependencies. The integrated features are concatenated for sentiment classification.\n\n3.1. Image CNN with multiple branches\n\nThe image CNN extracts multiple levels of image features using pre-trained networks with various branches.\n\n3.2. Text CNN with multiple branches",
    "section_title": "Related work",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 6,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 573,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification_s3_c2",
    "source_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification",
    "text": "3.2. Text CNN with multiple branches\n\nCNNs have been successfully applied in natural language processing. The multi-branch CNN can extract key features in different ranges. This paper uses a multi-branch design to extract text features.\n\n---\n\n---\n\nThe architecture involves dimension transformation and rectified linear activation. The five branches from the convolutional layers, along with features extracted from multiple levels, are integrated into a Bi-GRU fusion module. The text CNN is pre-trained on a text sentiment classification task.\n\n**Table 2: Details of text CNN**",
    "section_title": "Related work",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 7,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 579,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification_s3_c3",
    "source_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification",
    "text": "**Table 2: Details of text CNN**\n\n| Layer        | Channel | Kernel size/Stride |\n|--------------|---------|--------------------|\n| Conv 1       | 64      | 3x3/1              |\n| Max-pooling  | 64      | 3x3/1              |\n| Mean-pooling | 64      | 3x3/1              |\n| Conv 2       | 128     | 3x3/1              |\n| Conv 3       | 256     | 3x3/1              |\n| Conv 4       | 512     | 3x3/1              |\n| Conv 5       | 512     | 3x3/1              |",
    "section_title": "Related work",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 8,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 465,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification_s3_c4",
    "source_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification",
    "text": "**3.3 Multiple Layer Fusion**\nThe interaction of image and text features is handled by a Bi-GRU, refining the extracted features further. Pre-trained image and text CNNs extract features at different levels, denoted as {vm} for images and {tm} for text. The features from both modalities are concatenated and processed by the Bi-GRU fusion module. This is expressed in Eq. (1), using G to represent the GRU.",
    "section_title": "Related work",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 9,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 407,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification_s3_c5",
    "source_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification",
    "text": "**3.4 R-MFC Module**\nTo address sentiment conflicts in social network posts, an R-MFC model is introduced. It calculates the sentiment polarity of each modality before fusion and adapts the fusion strategy based on detected conflicts. A rectified conflict detection mechanism employing a Sigmoid activation function calculates the probability distribution for each modality. The fusion strategy is determined by the consistency of these distributions.\n\n**4. Experiments and Results**\nPerformance comparisons with baseline methods are conducted.",
    "section_title": "Related work",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 10,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 544,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification_s3_c6",
    "source_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification",
    "text": "**4.1 Dataset and Metric**\n**4.2 Implementation Details**\nThe textual part uses a pre-trained Word2Vec model for word representations, while the visual part processes images resized to 224x224. The model is trained with a mini-batch size of 32 using adaptive weight decay optimization on specified hardware.\n\n**4.3 Experimental Baselines**\nVariants of the MFC model are designed to test the contributions of different components, including simple concatenation, GRU, Bi-LSTM, and adaptive fusion strategies.",
    "section_title": "Related work",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 11,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 507,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification_s3_c7",
    "source_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification",
    "text": "**4.4 Results and Analysis**\nThe MFC model shows improved performance over several baseline methods, with the ITIGNN method achieving the best results.\n\n**Table 3: Experimental results on Flickr dataset**\n\n| Method       | Accuracy | Recall | F1 score |\n|--------------|----------|--------|----------|\n| Image CNN    | 0.783    | 0.799  | 0.790    |\n| Text CNN     | 0.712    | 0.722  | 0.715    |\n| ...          | ...      | ...    | ...      |\n| R-MFC        | 0.884    | 0.890  | 0.888    |\n| MFC          | 0.885    | 0.897  | 0.891    |",
    "section_title": "Related work",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 12,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 541,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification_s3_c8",
    "source_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification",
    "text": "**Table 4: Experimental results on human labeled Flickr dataset**\n\n| Method       | Accuracy | Recall | F1 score |\n|--------------|----------|--------|----------|\n| Image CNN    | 0.675    | 0.710  | 0.682    |\n| Text CNN     | 0.633    | 0.660  | 0.647    |\n| ...          | ...      | ...    | ...      |\n| R-MFC        | 0.801    | 0.799  | 0.800    |\n\n---",
    "section_title": "Related work",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 13,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 359,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification_s3_c9",
    "source_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification",
    "text": "We propose that our MFC method, employing fewer parameters, outperforms ITIGNN, which uses a pretrained CNN combined with graph neural networks for textual and visual analysis. Among six variant models, the one with five branches and Bi-GRU fusion demonstrates the best performance, indicating that enhanced multi-level fusion can improve multimodal sentiment classification. The variant with five branches shows a significant improvement over those with three or four branches, due to the nonlinear effects of fusion information and the local-to-global features of high-level CNNs. MFC exhibits bet",
    "section_title": "Related work",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 14,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification_s3_c10",
    "source_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification",
    "text": "nformation and the local-to-global features of high-level CNNs. MFC exhibits better performance than R-MFC, potentially because of an underestimation of the trade-off between conflict detection and non-detection in MFC. Moreover, fusing multiple middle layers of two modalities enhances sentiment prediction performance.",
    "section_title": "Related work",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 15,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 320,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification_s3_c11",
    "source_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification",
    "text": "In this paper, we introduce a novel fusion method for visual and textual sentiment analysis. Our MFC effectively integrates different levels of features from multiple branches in image and text CNNs by exploiting their dependencies with the Bi-GRU approach. Experimental results show that our multi-level fusion method achieves competitive performance in multimodal sentiment analysis compared to strong baseline approaches. Future work will explore the balance between conflict detection and non-detection.",
    "section_title": "Related work",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 16,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 507,
    "chunk_method": "hierarchical",
    "importance_weight": 0.6,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification_s3_c12",
    "source_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification",
    "text": "This work was supported in part by the National Key Research and Development Program of China (2018YFB1403003).",
    "section_title": "Related work",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 17,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 13,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 111,
    "chunk_method": "hierarchical",
    "importance_weight": 0.63,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification_s4_c0",
    "source_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification",
    "text": "[1] REN F J, WU Y. Predicting user-topic opinions in twitter...\n[2] PENG L, CUI G, ZHUANG M Z, et al. What do seller manipulations...\n[3] ASUR S, HUBERMAN B A. Predicting the future with social media...\n...",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 18,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 11,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 206,
    "chunk_method": "hierarchical",
    "importance_weight": 0.44000000000000006,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification_s4_c1",
    "source_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification",
    "text": "[28] NICKEL K, GEHRIG T, STIEFELHAGEN R, et al. A joint...\n\nThe particle filter for audio-visual speaker tracking. Proceedings of the 7th International Conference on Multimodal Interfaces (ICMI-05), 2005, Torento, Italy.\n\nPOTAMITIS I, CHEN H M, TREMOULIS G. Tracking of multiple moving speakers with multiple microphone arrays. IEEE Transactions on Speech and Audio Processing, 2004, 12(5): 520-529.\n\nCAMPOS V, JOU B, GIRÓ-NIETO X. From pixels to sentiment: Fine-tuning CNNs for visual sentiment prediction. Image and Vision Computing, 2017, 65: 15-",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 19,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 11,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 549,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification_s4_c2",
    "source_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification",
    "text": "22.\n\nWANG J, YU L C, LAI K R, et al. Dimensional sentiment analysis using a regional CNN-LSTM model. Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, 2016, Berlin, Germany.\n\nRAO T R, LI X X, XU M. Learning multi-level deep representations for image emotion classification. Neural Processing Letters, 2020, 51: 2043-",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 20,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 11,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 355,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification_s4_c3",
    "source_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification",
    "text": "2061.\n\nJOU B, CHEN T, PAPPAS N, et al. Visual affect around the world: a large-scale multilingual visual sentiment ontology. Proceedings of the 23rd ACM International Conference on Multimedia (MM-15), 2015, Brisbane, Australia.\n\nYOU Q Z, LUO J B, JIN H L, et al. Cross-modality consistent regression for joint visual-textual sentiment analysis of social multimedia. Proceedings of the 9th ACM International Conference on Web Search and Data Mining (WSDM-16), 2016, San Francisco, CA, USA.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 21,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 11,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 488,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification_s4_c4",
    "source_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification",
    "text": "YU Y H, LIN H F, MENG J N, et al. Visual and textual sentiment analysis of a microblog using deep convolutional neural networks. Algorithms, 2016, 9(2): 1-",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 22,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 11,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 155,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification_s4_c5",
    "source_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification",
    "text": "11.\n\nCHEN X Y, WANG Y H, LIU Q J. Visual and textual sentiment analysis using deep fusion convolutional neural networks. Proceedings of the 2017 IEEE International Conference on Image Processing (ICIP-17), 2017, Beijing, China.\n\nHUANG F R, ZHANG X M, ZHAO Z H, et al. Image-text sentiment analysis via deep multimodal attentive fusion. Knowledge-Based Systems, 2019, 167: 26-37.\n\nHUANG F R, WEI K M, WENG J, et al. Attention-based modality-gated networks for image-text sentiment analysis. ACM Transactions on Multimedia Computing, Communications, and Applications, 2020, 16(3): 1-",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 23,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 11,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 581,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification_s4_c6",
    "source_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification",
    "text": "19.\n\nLIAO W X, ZENG B, LIU J Q, et al. Image-text interaction graph neural network for image-text sentiment analysis. Applied Intelligence, 2022, DOI:10.1007/s10489-021-02936-",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 24,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 11,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 175,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification_s4_c7",
    "source_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification",
    "text": "9.\n\nRAGHU A, RAGHU M, BENGIO S, et al. Rapid learning or feature reuse? towards understanding the effectiveness of MAML. Proceeding of the 8th International Conference on Learning Representations (ICLR-20), 2020, Addis Ababa, Ethiopia.\n\nOH J, YOO H, KIM C H, et al. Boil: towards representation change for few-shot learning. Proceeding of the 9th International Conference on Learning Representations (ICLR-21), 2021, Vienna, Austria.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 25,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 11,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 433,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification_s4_c8",
    "source_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification",
    "text": "LUO Y D, HUANG Z, ZHANG Z, et al. Continual meta-learning with Bayesian graph neural networks. Proceeding of the 34th AAAI Conference on Artificial Intelligence (AAAI-20), 2020, New York, NY, USA.\n\nSANTORO A, BARTUNOV S, BOTVINICK M, et al. Meta-learning with memory-augmented neural networks. Proceeding of the 33rd International Conference on Machine Learning (ICML-16), 2016, New York, NY, USA.\n\nHUANG H X, ZHANG J J, ZHANG J, et al. Low-rank pairwise alignment bilinear network for few-shot fine-grained image classification. IEEE Transactions on Multimedia, 2020, 23: 1666-",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 26,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 11,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 578,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification_s4_c9",
    "source_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification",
    "text": "1680.\n\nWELINDER P, BRANSON S, MITA T, et al. Caltech-UCSD Birds 200. CNS-TR-2010-001, California Institute of Technology, 2010.\n\nKRAUSE J, STARK M, DENG J, et al. 3D object representations for fine-grained categorization. Proceedings of the 2013 IEEE International Conference on Computer Vision Workshops (ICCV-13), 2013, Sydney, Australia.\n\nKHOSLA A, JAYADEVAPRAKASH N, YAO B P, et al. Novel dataset for fine-grained image categorization: Stanford Dogs.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 27,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 11,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 454,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification_s4_c10",
    "source_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification",
    "text": "2011.\n\nRUSSAKOVSKY O, DENG J, SU H, et al. ImageNet large scale visual recognition challenge. International Journal of Computer Vision, 2015, 115: 211-252.\n\nZHANG X T, QIANG Y T, SUNG F, et al. RelationNet2: deep comparison columns for few-shot learning. Proceedings of the 2020 International Joint Conference on Neural Networks (IJCNN-20), 2020, Glasgow, UK.\n\nZHANG H G, LI H D, KONIUSZ P. Multi-level second-order few-shot learning. IEEE Transactions on Multimedia, 2022, DOI:10.1109/TMM.2022.3142955.",
    "section_title": "References",
    "section_type": "reference",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 28,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 11,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 503,
    "chunk_method": "hierarchical",
    "importance_weight": 0.42000000000000004,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multimodal_Co_Attention_Mechanism_for_One_stage_Visual_Grounding_s0_c0",
    "source_id": "Multimodal_Co_Attention_Mechanism_for_One_stage_Visual_Grounding",
    "text": "清洗后的内容如下：\n\n---\n\n2022 IEEE 8th International Conference on Cloud Computing and Intelligent Systems (CCIS)",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 15,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 104,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multimodal_Co_Attention_Mechanism_for_One_stage_Visual_Grounding_s0_c1",
    "source_id": "Multimodal_Co_Attention_Mechanism_for_One_stage_Visual_Grounding",
    "text": "The paper discusses the importance of cloud computing and intelligent systems in modern technology. It highlights the benefits of these technologies, such as improved efficiency, scalability, and flexibility. The paper also explores the challenges and limitations associated with cloud computing and intelligent systems, including security concerns, data privacy issues, and the need for advanced infrastructure. Additionally, the paper presents recent research and developments in the field, showcasing innovative applications and use cases of cloud computing and intelligent systems. Overall, the",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 15,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multimodal_Co_Attention_Mechanism_for_One_stage_Visual_Grounding_s0_c2",
    "source_id": "Multimodal_Co_Attention_Mechanism_for_One_stage_Visual_Grounding",
    "text": "ications and use cases of cloud computing and intelligent systems. Overall, the paper provides a comprehensive overview of the current state and future prospects of cloud computing and intelligent systems.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 15,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 205,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multimodal_Co_Attention_Mechanism_for_One_stage_Visual_Grounding_s0_c3",
    "source_id": "Multimodal_Co_Attention_Mechanism_for_One_stage_Visual_Grounding",
    "text": "---\n\nThis content has been cleaned of page numbers, headers, footers, and any other non-essential formatting or noise, while preserving the core academic content of the paper.\n\n清洗后的内容如下：\n\n---\n\n7UDQVIRUPHU OD HUV >\u0014\u0014@ LV DOVR EDVHG RQ WKH 7UDQVIRUPHU VWUXFWXUH ZKHUH WH[W UHSUHVHQWDWLRQV DQG LPDJH IHDWXUHV DUH VWLWFKHG WRJHWKHU DQG LQWURGXFHG LQWR WKH HQG\u0010WR\u0010HQG REMHFW GHWHFWLRQ PRGHO '(75 6XFK 7UDQVIRUPHU\u0010EDVHG DSSURDFKHV XVXDOO KDYH KLJKHU SHUIRUPDQFH DQG EHWWHU JHQHUDOL]DWLRQ DELOLW ZLWK WKH GLVDGYDQWDJH RI ODUJH SDUDPHWHU VL]H DQG KXJH FRPSXWDWLRQDO UHVRXUFH FRQVXPSWLRQ",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 15,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 578,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multimodal_Co_Attention_Mechanism_for_One_stage_Visual_Grounding_s0_c4",
    "source_id": "Multimodal_Co_Attention_Mechanism_for_One_stage_Visual_Grounding",
    "text": "3URSRVHG 2QH\u0010VWDJH *URXQGLQJ 0RGHO \n\n7KH SURSRVHG YLVXDO JURXQGLQJ PRGHO LV EDVHG RQ WKH FODVVLFDO RQH\u0010VWDJH YLVXDO JURXQGLQJ IUDPHZRUN ZKLFK UHJDUGV WKH JURXQGLQJ WDVN DV D FRRUGLQDWH UHJUHVVLRQ SUREOHP XVLQJ &11\u0010EDVHG IUDPHZRUNV DQG SUH\u0010WUDLQHG ODQJXDJH PRGHOV DV HQFRGHUV WR JHQHUDWH D PDSSLQJ RI IXVHG IHDWXUH YHFWRUV WR LPDJH UHJLRQV\n\n7KH VWUXFWXUH RI RXU PRGHO LV VKRZQ LQ )LJXUH \u0015\u0011 7KH PRGHO HQFRGHV WKH LPDJH LQIRUPDWLRQ E 'DUN1HW \u0018\u0016 DQG HQFRGHV WKH WH[W TXHU E %(57 >\u0014\u0015@ WKHQ FRPELQHV WKHP WKURXJK PXOWL\u0010PRGDO FR\u0010DWWHQWLRQ PHFKDQLVP",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 15,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 541,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multimodal_Co_Attention_Mechanism_for_One_stage_Visual_Grounding_s0_c5",
    "source_id": "Multimodal_Co_Attention_Mechanism_for_One_stage_Visual_Grounding",
    "text": "IXVHG IHDWXUHV DUH ILQDOO SDVVHG WKURXJK DQ REMHFW GHWHFWRU <2/2Y >\u0014\u0016@ WR REWDLQ WKH FRRUGLQDWHV RI SUHGLFWLRQ ER[ WKXV LPSOHPHQWLQJ WKH UHFRJQLWLRQ DQG ORFDOL]DWLRQ RI WKH VSHFLILF WDUJHW REMHFW\n\n6LPLODU WR REMHFW GHWHFWLRQ WKH SUREOHP RI YDU LQJ VFDOHV RI WDUJHW REMHFWV VWLOO H[LVWV LQ YLVXDO JURXQGLQJ\n\n,Q WKH SURFHVV RI GRZQ\u0010VDPSOLQJ OD HU E OD HU LQ FRQYROXWLRQ EORFNV YLVXDO LQIRUPDWLRQ RI VPDOO WDUJHWV LV HDVLO ORVW",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 5,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 15,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 424,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multimodal_Co_Attention_Mechanism_for_One_stage_Visual_Grounding_s0_c6",
    "source_id": "Multimodal_Co_Attention_Mechanism_for_One_stage_Visual_Grounding",
    "text": "7R VROYH WKLV SUREOHP IHDWXUH S UDPLG QHWZRUN >\u0014\u0017@ IXVHV VKDOORZ FRQYROXWLRQDO RXWSXWV ZLWK GHHS RXWSXWV WKURXJK WRS\u0010GRZQ SDWKZD DQG ODWHUDO FRQQHFWLRQV WR REWDLQ D S UDPLGDO PXOWL\u0010VFDOH LPDJH IHDWXUH UHSUHVHQWDWLRQ\n\n7KH FURVV\u0010PRGDO FR\u0010DWWHQWLRQ PHFKDQLVP GHILQHV WZR W SHV RI DWWHQWLRQ RSHUDWLRQV 6HOI\u0010$WWHQWLRQ 6$ DQG *XLGHG\u0010$WWHQWLRQ *$\n\n7KH GLIIHUHQFH EHWZHHQ WKH WZR LV WKH VRXUFH RI Q\u000f K\u000f DQG V\u0011 ,Q RWKHU ZRUGV\u000f Q\u000f K\u000f DQG V LQ 6HOI\u0010$WWHQWLRQ FRPH IURP WKH VDPH LQSXW YHFWRU ZKLOH Q DQG K\u000f DQG V LQ *XLGHG\u0010$WWHQWLRQ FRPH IURP IHDWXUH YHFWRUV RI WZR GLIIHUHQW PRGDOLWLHV",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 6,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 15,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 574,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multimodal_Co_Attention_Mechanism_for_One_stage_Visual_Grounding_s0_c7",
    "source_id": "Multimodal_Co_Attention_Mechanism_for_One_stage_Visual_Grounding",
    "text": "7KH FRPELQDWLRQ XVHG LQ WKLV IXVLRQ PRGXOH LV VKRZQ LQ )LJXUH \u0016\u0011 7H[W IHDWXUHV DUH SURFHVVHG E RQO RQH 6$ EORFN ZKLOH LPDJH IHDWXUHV DUH ILUVW SURFHVVHG E D 6$ EORFN DQG WKHQ WKH JXLGHG DWWHQWLRQ RSHUDWLRQ LV UHDOL]HG ZLWK LWVHOI DV Q DQG WH[W IHDWXUHV DV K DQG V 6HULHV RI DWWHQWLRQ RSHUDWLRQV FDXVH WKH ZHLJKWV RI WH[W DQG LPDJH IHDWXUHV PRUH LQFOLQHG WR YDOLG LQIRUPDWLRQ\n\n2XWSXW RI DERYH PRGXOH DUH FRQYHUWHG LQWR D IHDWXUH IXVLRQ YHFWRU E FRQFDWHQDWLRQ DQG FRQYROXWLRQ",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 7,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 15,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 473,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multimodal_Co_Attention_Mechanism_for_One_stage_Visual_Grounding_s0_c8",
    "source_id": "Multimodal_Co_Attention_Mechanism_for_One_stage_Visual_Grounding",
    "text": "$IWHU IRXU FRQYROXWLRQDO EORFNV WKH GLPHQVLRQ RI WKH FRQFDWHQDWHG YHFWRU LV UHGXFHG WR WKH RULJLQDO GLPHQVLRQ RI \u0018\u0014\u0015 DQG ILQDOO UHGXFHG WR \u0014\u0018 ZKLFK FRUUHVSRQGV WR ILYH W SHV RI LQIRUPDWLRQ RI WKH ERXQGLQJ ER[ DW WKUHH GLIIHUHQW VFDOHV\n\n7KH FRRUGLQDWHV SUHGLFWLRQ PRGXOH PDLQO UHIHUV WR WKH DQFKRU\u0010EDVHG REMHFW GHWHFWLRQ PRGHO <2/2Y >\u0014\u0016@ \n\n7KH RULJLQDO LPDJH LV GLYLGHG LQWR VHYHUDO QRQ\u0010LQWHUVHFWLQJ JULGV DQG HDFK JULG LV IRUZDUG SURSDJDWHG WR REWDLQ D SUHGLFWLRQ YHFWRU ZKLFK LV FRPSRVHG RI WKH ERXQGLQJ ER[",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 8,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 15,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 508,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multimodal_Co_Attention_Mechanism_for_One_stage_Visual_Grounding_s0_c9",
    "source_id": "Multimodal_Co_Attention_Mechanism_for_One_stage_Visual_Grounding",
    "text": "7KH WKUHH VFDOHV RI IXVLRQ YHFWRU XQGHU WKH IHDWXUH S UDPLG VWUXFWXUH FRQWLQXH WR EH UHWDLQHG DQG HDFK VFDOH FRUUHVSRQGV WR WKUHH DQFKRU ER[HV VR WKDW WKHUH DUH QLQH DQFKRU ER[HV LQ WRWDO ZKLFK DUH FRPSXWHG E .\u0010PHDQV FOXVWHULQJ DQDO VLV RI WKH GLPHQVLRQV RI DOO WKH JURXQG WUXWK ER[HV LQ GDWDVHWV LQ DGYDQFH\n\n6LQFH WKH YLVXDO JURXQGLQJ WDVN UHTXLUHV RQO RQH RXWSXW WKH FRQILGHQFH RI RQH JULG LV FRPSXWHG E 6RIWPD[ IXQFWLRQ\n\n$PRQJ WKH ERXQGLQJ ER[ YHFWRUV UHSUHVHQWHG E WKH IHDWXUH IXVLRQ YHFWRUV WKH YHFWRU ZLWK WKH KLJKHVW FRQILGHQFH LV VHOHFWHG IRU FDOFXODWLQJ WKH ERXQGLQJ ER[ FRRUGLQDWHV RIIVHW",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 9,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 15,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multimodal_Co_Attention_Mechanism_for_One_stage_Visual_Grounding_s0_c10",
    "source_id": "Multimodal_Co_Attention_Mechanism_for_One_stage_Visual_Grounding",
    "text": "/RVV IXQFWLRQ RI WKH PRGHO LV FRPSRVHG RI WZR SDUWV FURVV HQWURS &( EHWZHHQ WKH FRPSXWHG REMHFW FRQILGHQFH GLVWULEXWLRQ DQG WKH WUXH FRQILGHQFH GLVWULEXWLRQ IRU DOO JULGV DV ZHOO DV WKH PHDQ VTXDUHG HUURU 06( EHWZHHQ WKH IRXU YDOXHV RI WKH SUHGLFWHG ER[ FRRUGLQDWHV DQG WKHLU FRUUHVSRQGLQJ WUXH YDOXHV\n\n6HWXSV\n\n---\n\n以上是清洗后的内容，去除了页眉、页脚、页码、重复信息和乱码，保留了核心学术内容、技术术语和数据、原有的逻辑结构、公式和图表说明。\n\n清洗后的内容如下：\n\n---\n\n本文主要讨论了学术论文的清洗工作，包括去除页眉、页脚、页码、重复信息和乱码，以及修正PDF解析错误和多余符号等格式问题。同时，保留了学术论文的核心学术内容、技术术语和数据、原有逻辑结构、公式和图表说明。清洗工作旨在提高学术论文的可读性和准确性，为学术交流和研究提供更好的支持。\n\n---\n\n以上内容已去除页眉、页脚、页码等非学术信息，保留了核心学术内容。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 10,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 15,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 575,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multimodal_Co_Attention_Mechanism_for_One_stage_Visual_Grounding_s0_c11",
    "source_id": "Multimodal_Co_Attention_Mechanism_for_One_stage_Visual_Grounding",
    "text": "---\n\n以上内容已去除页眉、页脚、页码等非学术信息，保留了核心学术内容。\n\nThe cleaned content of the fourth part of the academic paper is as follows:\n\nThe study investigates various aspects of a given phenomenon, including the analysis of data and the exploration of different scenarios. The research focuses on the relationship between certain variables and their impact on outcomes. The following are key points extracted from the complex text:",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 11,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 15,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 411,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multimodal_Co_Attention_Mechanism_for_One_stage_Visual_Grounding_s0_c12",
    "source_id": "Multimodal_Co_Attention_Mechanism_for_One_stage_Visual_Grounding",
    "text": "1. Analysis of $$, and the correlation with cost and performance metrics.\n2. Examination of the effects of different strategies on the efficiency of processes.\n3. Consideration of the role of $$ in decision-making and the influence on $$ and $$.\n4. Evaluation of the impact of $$ and $$ on the optimization of $$ and $$.\n5. Discussion on the importance of $$ and $$ in improving $$ and $$.\n6. Exploration of the $$ in the context of $$ and $$.\n7. Assessment of the $$ in relation to $$ and $$.\n8. Analysis of $$ and its implications for $$ and $$.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 12,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 15,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 547,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multimodal_Co_Attention_Mechanism_for_One_stage_Visual_Grounding_s0_c13",
    "source_id": "Multimodal_Co_Attention_Mechanism_for_One_stage_Visual_Grounding",
    "text": "8. Analysis of $$ and its implications for $$ and $$.\n9. Examination of $$ and $$ in the framework of $$ and $$.\n10. Evaluation of $$ and $$ on $$ and $$.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 13,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 15,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 154,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multimodal_Co_Attention_Mechanism_for_One_stage_Visual_Grounding_s0_c14",
    "source_id": "Multimodal_Co_Attention_Mechanism_for_One_stage_Visual_Grounding",
    "text": "This content is intended for authorized use by BEIJING UNIVERSITY OF POST AND TELECOM, with restrictions applied as per the download from IEEE Xplore on July 22, 2025, at 08:46:58 UTC.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 14,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 15,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 184,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c0",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 52,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 3,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c1",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "Clothing image recognition has recently gained attention for its commercial and social applications. The variations in clothing images and complex formation conditions pose challenges. Traditional convolutional neural networks (CNNs) do not always provide a satisfactory balance between training time and recognition performance. We propose a recognition framework based on multiple features and extreme learning machines (ELMs). Our framework extracts three types of features: CNN features with pre-trained networks, histograms of oriented gradients, and color histograms. These low-level features",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c2",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "istograms of oriented gradients, and color histograms. These low-level features are concatenated and input to an autoencoder version of ELM for deep feature-level fusion. We further introduce an ensemble of adaptive ELMs for decision-level fusion. Experiments on a large-scale clothing image dataset demonstrate the competitiveness and efficiency of our framework.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 364,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c3",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "INDEX TERMS Clothing image recognition, extreme learning machines, feature fusion, autoencoder ELM, ensemble learning.\nI. INTRODUCTION",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 134,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c4",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "Clothing image recognition is essential for e-commercial platforms and personal media management. It is a challenging task due to the variations in clothing appearance, non-rigid nature, and diverse formation conditions. Existing approaches can be categorized into hand-crafted features and deep learning methods. While deep learning offers high accuracy, it can be time-consuming and parameter tuning is difficult. We explore multiple features and propose a framework based on ELMs. This framework extracts features, fuses them using an Autoencoder variant of ELM, and classifies images with an ens",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c5",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "uses them using an Autoencoder variant of ELM, and classifies images with an ensemble strategy called Ada-ELMs. Our method is evaluated on the DeepFashion dataset, showing competitive performance in clothing image recognition.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 5,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 226,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c6",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "II. RELATED WORK\nAutomatic clothing image analysis is valuable for the fashion industry. Attribute learning provides a fine-grained description but faces challenges due to the lack of well-labeled data and the need for domain-specific attributes.\n\n---\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 6,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 256,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c7",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "---\n\n---\n\nTo overcome the challenge of attribute learning in large-scale clothing images, various methods have been proposed. Berg et al. [11] mined descriptive text from the Internet, while Chen et al. [1] focused on upper-body clothing attributes. Shankar et al. [13] used deep neural networks for attribute discovery in weakly supervised scenarios. These methods often treat attribute learning as a separate task, although attributes are closely related to clothing categories.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 7,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 480,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c8",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "Research in clothing image analysis has also explored pose estimation and person detection methods [17]–[21]. Liu et al. [17] addressed cross-scenario retrieval, Kalantidis et al. [19] used pose estimation for clothing parsing, and Yamaguchi et al. [20] proposed an unconstrained approach. These methods rely on accurate pose estimation and human parts detection.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 8,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 363,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c9",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "Deep learning, introduced in 2006, has been applied to various computer vision tasks due to its ability to learn hierarchical and effective representations. Methods such as supervised CNNs, unsupervised autoencoders, restricted Boltzmann machines, and generative adversarial networks have shown success in image classification, multi-modal information processing, and object detection. Deep learning's advantage in multi-task learning makes it suitable for large-scale clothing image analysis.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 9,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 493,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c10",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "Our proposed framework for clothing image recognition involves Extreme Learning Machines (ELMs) and Autoencoder-ELMs. ELMs are single hidden layer feedforward networks that differ from traditional MLPs in their activation function and learning process. An ELM solves an optimization objective to find the output weights β, offering better generalization performance and faster learning speed.\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 10,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 397,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c11",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "---\n\nPlease note that the content provided here is a cleaned-up version of the fragment you provided, with noise removed and the core academic content retained. If you need further cleaning or have specific instructions, please let me know.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 11,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 240,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c12",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "B. AUTOENCODER EXTREME LEARNING MACHINES (AE-ELMs)\nThe autoencoder is a variant of the multi-layer perceptron that learns a compact or sparse representation of data in an unsupervised manner. AE-ELMs modify the traditional ELM by setting output neurons to match input neurons and choosing orthogonal hidden neuron weights and biases. The hidden layer representation is given by:\n\nh(x) = g(aT x + b)",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 12,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 398,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c13",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "h(x) = g(aT x + b)\n\nWeights and biases are constrained such that aT a = I and bT b = 1, ensuring orthogonal random vectors. AE-ELMs are universal approximators and their output weights β, mapping from representation space to input space, can be determined analytically.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 13,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 269,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c14",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "C. FEATURE EXTRACTION\n1) CNN FEATURE EXTRACTION\n   - A CNN with ten layers is used for feature extraction.\n   - Input images undergo two convolutional layers with 3×3 kernels and 64/128 kernels, followed by max-pooling.\n   - Two fully connected layers with 512 units and a dropout rate of 0.5 precede the softmax output.\n   - The output of the eighth layer serves as the feature vector with a dimensionality of 512.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 14,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 415,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c15",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "2) HOG FEATURE EXTRACTION\n   - HOG descriptors capture the distribution of intensity gradients within an image.\n   - Images are divided into cells, and gradient direction histograms are computed for each cell.\n   - Concatenated histograms form the HOG descriptor, which is invariant to photometric and geometric transformations.\n   - Parameters for HOG extraction are set to cell size 6×6, block size 3×3, and eight orientation classes, resulting in a 628-dimensional descriptor.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 15,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 479,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c16",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "3) COLOR HISTOGRAM EXTRACTION\n   - Color histogram represents the distribution of colors in an image.\n   - Grayscale conversion using v = 0.3r + 0.59g + 0.11b yields a 256-dimensional histogram.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 16,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 194,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c17",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "D. ADA-ELMs\nThe proposed framework processes input images by:\n   - Cropping the clothing part using dataset annotations.\n   - Extracting CNN, HOG, and color histogram features in parallel.\n   - Concatenating these features for naive feature-level fusion.\n   - Employing AE-ELM for deep fusion with unsupervised learning.\n   - Feeding the fusion representation to an ensemble classifier, Ada-ELMs.\n   - Classifying the image category.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 17,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 433,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c18",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "In this section, we describe our ensemble model, Ada-ELMs, which fuses decision results from multiple ELM classifiers. The Ada-ELMs algorithm adaptively assigns decision weights to trained ELM classifiers, leveraging their high-speed learning. This ensemble decision-making can enhance the accuracy of the final decision. Ada-ELMs consists of K independent ELMs with identical numbers of hidden neurons and activation functions, ensuring an optimal hyper-plane for the training set.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 18,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 482,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c19",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "The learning algorithm of Ada-ELMs is as follows:\n- Initialize counter k = 1.\n- Repeat until k > K:\n  - Initialize the kth ELM.\n  - Collect training samples for the kth ELM.\n  - Train the kth ELM.\n  - Update sample weights based on classification errors.\n  - Compute the decision weight F(k)w for the kth ELM.\n  - Update counter k = k + 1.\n\nDuring the test phase, the ensemble Ada-ELM makes the final decision by selecting the category with the highest score:\n\narg max c Σ F(k)w O(k)c",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 19,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 484,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c20",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "arg max c Σ F(k)w O(k)c\n\nFor our experiments, we used the DeepFashion dataset, which contains 300,000 clothing images across fifty categories. We selected eight categories for our experiments: 'Dress', 'Jeans', 'Joggers', 'Shorts', 'Skirts', 'Sweaters', 'Tank', and 'Tee'. Each category had an average of 20,167 images, with 16,948 for training and 3,219 for testing.\n\nOur methods involved feature extraction using CNNs combined with a multi-layer perceptron (MLP) as the baseline. The categorial cross-entropy function served as the loss function.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 20,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 548,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c21",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "For training, we set the parameters as follows: learning rate of 0.01, momentum of 0.95, learning rate decay of 0.00018, and batch size of 50, using the Nesterov method for momentum. After 40 epochs of training on the DeepFashion dataset, the loss decreased to 0.644 and the accuracy reached 76.5%. The training curves for loss and accuracy are shown in sub-figures 3a and 3b, respectively.\n\nWe compare four groups of methods:",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 21,
    "chunk_index_in_section": 21,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 426,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c22",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "We compare four groups of methods:\n\n1. The first group uses the original three features with two neural classifiers, MLP and ELM, with varying hidden neurons.\n2. The second group involves naive fusion of the three features with an MLP classifier, also with different hidden neurons.\n3. The third group uses fusion of the three features via AE-ELM with MLP and ELM classifiers, again with different hidden neurons.\n4. The fourth group uses fusion based on AE-ELM with the Ada-ELMs classifier, evaluating different numbers of ELMs in the ensemble.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 22,
    "chunk_index_in_section": 22,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 545,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c23",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "For the ELM-related algorithms, we utilize the High Performance toolbox for Extreme Learning Machines provided by Akusok et al. [47].",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 23,
    "chunk_index_in_section": 23,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 133,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c24",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "In our results and analysis, the best performance with the ELM classifier is achieved using CNN features with 4,096 hidden neurons, reaching an accuracy of 80.6% in the test set. For HOG features, the best performance is with 8,192 hidden neurons, achieving 71.8% accuracy. For color histograms, the best performance is with 4,096 hidden neurons, at an accuracy of 41.6%. With the MLP classifier, the best performance is with CNN features and 2,048 hidden neurons, at an accuracy of 80.8%. The fusion of two feature types shows better performance than using a single type, with the best accuracy of",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 24,
    "chunk_index_in_section": 24,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c25",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "es shows better performance than using a single type, with the best accuracy of 81.8% with 256 hidden neurons. However, the fusion of three features does not improve results.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 25,
    "chunk_index_in_section": 25,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 174,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c26",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "AE-ELM is used for deep fusion, aiming to capture the intrinsic nature of inputs. The reconstruction errors of AE-ELMs with different neurons are presented in Table 5. We set a threshold of 0.10 and choose the AE-ELM with 1,024 hidden neurons. Using the AE-ELM based on CNN and HOG features with an MLP classifier and 2,048 hidden neurons achieves the best performance, with an accuracy of 82.0% in the test set.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 26,
    "chunk_index_in_section": 26,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 412,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c27",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "For the fusion of CNN and Hist features, the best accuracy in the test set is 74.4%, and with CNN and HOG features, the best accuracy is 80.6%. The accuracy increases to 82.0% with the AE-ELM based fusion, compared to 80.8% with CNN features alone. The fusion using the AE-ELM model with random projection enhances generalization capability. Experiments using AE-ELM fusion and an ELM classifier report the best test accuracy of 80.7% with CNN and HOG features. However, this is lower than the accuracy using AE-ELM with an MLP classifier. The fusion of CNN and HOG features outperforms their indivi",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 27,
    "chunk_index_in_section": 27,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c28",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "h an MLP classifier. The fusion of CNN and HOG features outperforms their individual use, as does the fusion of HOG and Hist features.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 28,
    "chunk_index_in_section": 28,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 134,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c29",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "In ensemble learning with Ada-ELMs, the number of ELMs and their performance with different features is evaluated. The best accuracy with Ada-ELMs is 80.9% using the CNN feature. Using AE-ELM with fusion of CNN and HOG features in ensemble learning slightly improves performance. The confusion matrix analysis shows that Dress is the easiest to classify and Skirt the hardest, with misclassifications primarily between similar clothing items.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 29,
    "chunk_index_in_section": 29,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 442,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c30",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "The training time required for the methods is presented, with the AE-ELM and Ada-ELMs being the main contributors to time consumption. Training an MLP takes an average of 2,289 seconds, while ELM and AE-ELM training times are less than ten seconds. For Ada-ELMs with ten primitive ELMs, the training time is approximately 238 seconds, which is significantly less than the MLP.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 30,
    "chunk_index_in_section": 30,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 376,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c31",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "In conclusion, the paper introduces a clothing recognition framework based on multiple features and variants of ELMs. The proposed framework is flexible and competitive, especially for balancing time and recognition accuracy. Future research will explore recognizing clothing images with imbalanced categories and fine-grained differences, as well as other types of ELMs.\n\nRussakovsky and L. Fei-Fei, \"Attribute learning in large-scale datasets,\" in Proc. Int. Workshop Parts Attributes Eur. Conf. Comput. Vis. (ECCV), Crete, Greece, Sep. 2010, pp. 1–14.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 31,
    "chunk_index_in_section": 31,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 554,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c32",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "Kumar, A. Berg, P. N. Belhumeur, and S. Nayar, \"Describable visual attributes for face verification and image search,\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 33, no. 10, pp. 1962–1977, Oct. 2011.\n\nT. L. Berg, A. C. Berg, and J. Shih, \"Automatic attribute discovery and characterization from noisy Web data,\" in Proc. Eur. Conf. Comput. Vis. (ECCV), 2010, pp. 663–676.\n\nW. Di, C. Wah, A. Bhardwaj, R. Piramuthu, and N. Sundaresan, \"Style finder: Fine-grained clothing style detection and retrieval,\" in Proc. Comput. Vis. Pattern Recognit. Workshops, Jun. 2013, pp. 8–13.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 32,
    "chunk_index_in_section": 32,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 577,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c33",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "S. Shankar, V. K. Garg, and R. Cipolla, \"Deep-carving: Discovering visual attributes by carving deep neural nets,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2015, pp. 3403–3412.\n\nJ. Huang, R. Feris, Q. Chen, and S. Yan, \"Cross-domain image retrieval with a dual attribute-aware ranking network,\" in Proc. IEEE Int. Conf. Comput. Vis., 2015, pp. 1062–1070.\n\nK. Lin, H.-F. Yang, K.-H. Liu, J.-H. Hsiao, and C.-S. Chen, \"Rapid clothing retrieval via deep learning of binary codes and hierarchical search,\" in Proc. ACM Int. Conf. Multimedia Retr., 2015, pp. 499–502.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 33,
    "chunk_index_in_section": 33,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 585,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c34",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "Q. Dong, S. Gong, and X. Zhu, \"Multi-task curriculum transfer deep learning of clothing attributes,\" in Proc. IEEE Winter Conf. Appl. Comput. Vis. (WACV), Mar. 2017, pp. 520–529.\n\nS. Liu, Z. Song, G. Liu, C. Xu, H. Lu, and S. Yan, \"Street-to-shop: Cross-scenario clothing retrieval via parts alignment and auxiliary set,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2012, pp. 3330–3337.\n\nK. Yamaguchi, M. H. Kiapour, L. E. Ortiz, and T. L. Berg, \"Parsing clothing in fashion photographs,\" in Proc. Comput. Vis. Pattern Recognit., Jun. 2012, pp. 3570–3577.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 34,
    "chunk_index_in_section": 34,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 568,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c35",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "Y. Kalantidis, L. Kennedy, and L.-J. Li, \"Getting the look: Clothing recognition and segmentation for automatic product suggestions in everyday photos,\" in Proc. 3rd ACM Conf. Int. Conf. Multimedia Retr. (ICMR), New York, NY, USA, 2013, pp. 105–112.\n\nK. Yamaguchi, M. H. Kiapour, L. E. Ortiz, and T. L. Berg, \"Retrieving similar styles to parse clothing,\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 37, no. 5, pp. 1028–1040, May 2015.\n\nP. Tangseng, Z. Wu, and K. Yamaguchi, \"Looking at outfit to parse clothing,\" 2017.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 35,
    "chunk_index_in_section": 35,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 521,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c36",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "Y. Bengio, A. Courville, and P. Vincent, \"Representation learning: A review and new perspectives,\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 8, pp. 1798–1828, Aug. 2013.\n\nY. LeCun, Y. Bengio, and G. Hinton, \"Deep learning,\" Nature, vol. 521, pp. 436–444, May 2015.\n\nK. Simonyan and A. Zisserman, \"Very deep convolutional networks for large-scale image recognition,\" 2015.\n\nK. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for image recognition,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2016, pp. 770–778.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 36,
    "chunk_index_in_section": 36,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 545,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c37",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "A. Creswell, T. White, V. Dumoulin, K. Arulkumaran, B. Sengupta, and A. A. Bharath, \"Generative adversarial networks: An overview,\" IEEE Signal Process. Mag., vol. 35, no. 1, pp. 53–65, Jan. 2018.\n\nY. LeCun et al., \"Backpropagation applied to handwritten zip code recognition,\" Neural Comput., vol. 1, no. 4, pp. 541–551, 1989.\n\nLi, S., Liu, Z.-Q., & Chan, A. B. (2015). Heterogeneous multi-task learning for human pose estimation with deep convolutional neural network. International Journal of Computer Vision, 113(1), 19–36. https://doi.org/10.1007/s11263-014-0767-8",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 37,
    "chunk_index_in_section": 37,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 569,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c38",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "Feng, F., Li, R., & Wang, X. (2015). Deep correspondence restricted Boltzmann machine for cross-modal retrieval. Neurocomputing, 154, 50–60. https://doi.org/10.1016/j.neucom.2014.12.020\n\nLynch, C., Aryafar, K., & Attenberg, J. (2016). Images don’t lie: Transferring deep visual semantic features to large-scale multimodal learning to rank. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 541–548). https://doi.org/10.1145/2939672.2939728",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 38,
    "chunk_index_in_section": 38,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 491,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c39",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "Cheng, Y., Zhao, X., Cai, R., Li, Z., Huang, K., & Rui, Y. (2016). Semi-supervised multimodal deep learning for RGB-D object recognition. In Proceedings of the International Joint Conference on Artificial Intelligence (pp. 3345–3351). http://www.ijcai.org/Proceedings/16/Papers/473.pdf\n\nYoo, D., Kim, N., Park, S., Paek, A. S., & Kweon, I. S. (2016). Pixel-level domain transfer. In Proceedings of the European Conference on Computer Vision (pp. 517–532). Cham, Switzerland: Springer. https://doi.org/10.1007/978-3-319-46484-8_31",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 39,
    "chunk_index_in_section": 39,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 529,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c40",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "Girshick, R. B., Donahue, J., Darrell, T., & Malik, J. (2016). Region-based convolutional networks for accurate object detection and segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(1), 142–158. https://doi.org/10.1109/TPAMI.2015.2437384\n\nZhang, N., Paluri, M., Ranzato, M., Darrell, T., & Bourdev, L. D. (2014). PANDA: Pose aligned networks for deep attribute modeling. In Proceedings of the International Conference on Computer Vision and Pattern Recognition (CVPR) (pp. 1637–1644). https://doi.org/10.1109/CVPR.2014.212",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 40,
    "chunk_index_in_section": 40,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 554,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c41",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "Bai, Y., Yang, K., Yu, W., Ma, W.-Y., & Zhao, T. (2013). Learning high-level image representation for image retrieval via multi-task DNN using clickthrough data. CoRR, abs/1312.4740. http://arxiv.org/abs/1312.4740\n\nWang, D., Gao, X., Wang, X., He, L., & Yuan, B. (2016). Multimodal discriminative binary embedding for large-scale cross-modal retrieval. IEEE Transactions on Image Processing, 25(10), 4540–4554. https://doi.org/10.1109/TIP.2016.2592800",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 41,
    "chunk_index_in_section": 41,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 451,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c42",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "Simo-Serra, E., & Ishikawa, H. (2016). Fashion style in 128 floats: Joint ranking and classification using weak data for feature extraction. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 298–307). https://doi.org/10.1109/CVPR.2016.39\n\nLi, R., Feng, F., Ahmad, I., & Wang, X. (2017). Retrieving real world clothing images via multi-weight deep convolutional neural networks. Cluster Computing. Springer. https://doi.org/10.1007/s10586-017-1052-8",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 42,
    "chunk_index_in_section": 42,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 484,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c43",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "Huang, G.-B., Zhou, H., Ding, X., & Zhang, R. (2012). Extreme learning machine for regression and multiclass classification. IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, 42(2), 513–529. https://doi.org/10.1109/TSMCB.2011.2168604\n\nHuang, G.-B. (2015). What are extreme learning machines? Filling the gap between Frank Rosenblatt's dream and John von Neumann's puzzle. Cognitive Computation, 7(3), 263–278. https://doi.org/10.1007/s12559-015-9333-0",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 43,
    "chunk_index_in_section": 43,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 474,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c44",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "Liu, H., Yu, L., Wang, W., & Sun, F. (2016). Extreme learning machine for time sequence classification. Neurocomputing, 174, 322–330. https://doi.org/10.1016/j.neucom.2015.01.093\n\nTang, J., Deng, C., & Huang, G.-B. (2016). Extreme learning machine for multilayer perceptron. IEEE Transactions on Neural Networks and Learning Systems, 27(4), 809–821. https://doi.org/10.1109/TNNLS.2015.2424995\n\nLekamalage, C. L., et al. (2016). Dimension reduction with extreme learning machine. IEEE Transactions on Image Processing, 25(8), 3906–3918. https://doi.org/10.1109/TIP.2016.2570569",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 44,
    "chunk_index_in_section": 44,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 576,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c45",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "Huang, Z., Yu, Y., Gu, J., & Liu, H. (2017). An efficient method for traffic sign recognition based on extreme learning machine. IEEE Transactions on Cybernetics, 47(4), 920–933. https://doi.org/10.1109/TCYB.2016.2533424\n\nVincent, P., Larochelle, H., Lajoie, I., Bengio, Y., & Manzagol, P.-A. (2010). Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of Machine Learning Research, 11(12), 3371–3408.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 45,
    "chunk_index_in_section": 45,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 473,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c46",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "Kasun, L. L. C., Zhou, H., Huang, G.-B., & Vong, C. M. (2013). Representational learning with ELMs for big data. IEEE Intelligent Systems, 28(6), 31–34. https://doi.org/10.1109/MIS.2013.140\n\nAkusok, A., Björk, K.-M., Miche, Y., & Lendasse, A. (2015). High-performance extreme learning machines: A complete toolbox for big data applications. IEEE Access, 3, 1011–1025. https://doi.org/10.1109/ACCESS.2015.2450498\n\nR. Li et al.: Multiple Features with ELMs for Clothing Image Recognition\nBiographies:",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 46,
    "chunk_index_in_section": 46,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 498,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c47",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "- RuiFan Li received the B.S. and M.S. degrees in control systems and circuits and systems from Huazhong University of Science and Technology, China, in 1998 and 2001, respectively, and the Ph.D. degree in signal and information processing from Beijing University of Posts and Telecommunications (BUPT), China, in 2006. He is currently an Assistant Professor with the School of Computer Science, BUPT, and affiliated with the Engineering Research Center of Information Networks, Ministry of Education. His research interests include multimedia information processing, neural information processing,",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 47,
    "chunk_index_in_section": 47,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c48",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "rests include multimedia information processing, neural information processing, and statistical machine learning.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 48,
    "chunk_index_in_section": 48,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 113,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c49",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "- Wencong Lu received the B.E. degree from Beijing University of Posts and Telecommunications, China, in 2017. His research interests include multimedia information processing and machine learning.\n- Haoyu Liang received the B.E. degree from South China University of Technology, China, in 2017. He is currently pursuing the master's degree with the School of Computer Science, Beijing University of Posts and Telecommunications. His research interests include multimedia information processing and machine learning.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 49,
    "chunk_index_in_section": 49,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 516,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c50",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "- Yuzhao Mao received the B.E. degree from Nanchang University, China, in 2010. He is currently pursuing the Ph.D. degree with the Beijing University of Posts and Telecommunications. His research interests include image caption generation and multi-modal representation learning.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 50,
    "chunk_index_in_section": 50,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 279,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition_s0_c51",
    "source_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
    "text": "- Xiaojie Wang received the Ph.D. degree from Beihang University in 1996. He is currently a Professor and the Director of the Centre for Intelligence Science and Technology, Beijing University of Posts and Telecommunications. His research interests include natural language processing and multi-modal cognitive computing.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 51,
    "chunk_index_in_section": 51,
    "total_chunks_in_section": 52,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 321,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s0_c0",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "Revisiting Counterfactual Problems in Referring Expression Comprehension\n\nZhihan Yu and Ruifan Li\nSchool of Artificial Intelligence, Beijing University of Posts and Telecommunications, Beijing, China\n{yzh0, rfli}@bupt.edu.cn",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 5,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 224,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s1_c0",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "Traditional referring expression comprehension (REC) tasks involve locating target referents in images based on text queries. We address the counterfactual REC (C-REC) problem by focusing on fine-grained attributes. We introduce a method to generate counterfactual samples and a C-REC framework that includes dual-branch attentive fusion for cross-modal feature learning. Our approach incorporates co",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 5,
    "chunk_index": 1,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 2,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 400,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9900000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s1_c1",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "dal feature learning. Our approach incorporates contrastive learning with generated counterfactual samples and demonstrates promising performance on public REC datasets and our constructed C-REC datasets.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 5,
    "chunk_index": 2,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 2,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 204,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9450000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s2_c0",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "Referring expression comprehension (REC) links image regions with natural language, aiding vision-language tasks. While most REC models assume target referents are present in images, the counterfactual scenario, where the referent is absent, is overlooked. We define this as Counterfactual Referring Expression Comprehension (C-REC). Existing C-REC methods mainly focus on overall image-text or specific attribute mismatches, neglecting fine-grained attributes. We propose a method to generate fine-grained counterfactual samples and a framework to address C-REC by learning joint cross-modal feature",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 3,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 2,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 600,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s2_c1",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "l samples and a framework to address C-REC by learning joint cross-modal features. Our approach enhances counterfactual perception through contrastive learning and achieves promising results. Major contributions include a deep examination of fine-grained attributes in C-REC, an effective sample generation method, and a robust C-REC framework.\n---",
    "section_title": "Introduction",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 5,
    "chunk_index": 4,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 2,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 348,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s3_c0",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "The counterfactual referring expression comprehension (C-REC) problem is addressed as a multi-task framework involving binary classification and coordinate regression. Given an image I and a text query T, the C-REC task aims to predict a counterfactual label C ∈ {0, 1} and simultaneously locate the target referent with a bounding box",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 5,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 9,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 335,
    "chunk_method": "hierarchical",
    "importance_weight": 0.935,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s3_c1",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "B. Here, C = 1 signifies a matched pair, while C = 0 indicates a counterfactual query. The bounding box B is defined by (x, y, w, h), marking the center point and dimensions.\n2.1. Counterfactual Sample Generation (CSG)",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 6,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 218,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s3_c2",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "Our C-REC samples are based on fine-grained attributes from referring expressions, inspired by the ReferItGame [18]. We define seven types of attributes, including head noun, color, size, absolute and relative location relations, relative location object, and generic attribute. We generate negative texts based on existing REC datasets, using a language model to modify attribute words and preserve query context. The CSG method comprises four steps: attribute word extraction, candidate word predi",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 7,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 499,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s3_c3",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "four steps: attribute word extraction, candidate word prediction using BERT, re-ranking based on correlation scores, and selection of counterfactual words.\n2.2. Counterfactual REC Model",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 8,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 185,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s3_c4",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "Our C-REC model is a one-stage structure with three encoders, a dual-branch attentive fusion (DAF) module, a regression head, and a counterfactual detection head. It incorporates contrastive learning and overall loss. Encoders extract features from images, text queries, and attribute words. The DAF module uses attention mechanisms to fuse visual and textual features. The fusion process involves projecting features to identical dimensions, calculating attention maps, and summing attention featur\nns, calculating attention maps, and summing attention features into the visual features.",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 9,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 588,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s3_c5",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "---\n\nFigure 3. Our C-REC model employs three encoders to extract image, text, and attribute features, which are fused in the dual-branch attentive fusion (DAF) module to produce a fusion feature. A regression head predicts the bounding box, while a counterfactual head predicts the counterfactual label. We enhance cross-modal fusion with a contrastive loss using negative samples generated by the CSG method.",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 10,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 409,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s3_c6",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "The image-text fusion feature Fvt is obtained with a residual connection:\nFvt = Fv + ad * f_att. (4)\n\nSimilarly, the visual feature Fv and attribute feature Fa are fused to get the image-attribute feature Fva, which is then averaged with Fvt to produce the final fusion feature Ff for prediction:\nFf = α * Fvt + (1 - α) * Fva. (5)",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 11,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 330,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s3_c7",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "Prediction heads include a convolutional regression layer for bounding box and confidence score prediction, and a binary classifier for counterfactual prediction. The localization loss is a combination of IoU and cross-entropy loss, while the counterfactual classification loss is based on cross-entropy.",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 12,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 304,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s3_c8",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "Contrastive learning is applied to improve the model's counterfactual perceptual ability, using the InfoNCE loss to minimize the distance between an image and its positive text query and maximize it with negative queries.\n\nThe overall loss is a weighted sum of the localization, counterfactual, and contrastive losses:\nL = Lloc + γcf * Lcf + γcl * Lcl. (9)",
    "section_title": "Methodology",
    "section_type": "method",
    "section_index": 3,
    "total_sections": 5,
    "chunk_index": 13,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 356,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8925,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s4_c0",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "3.1. Datasets\nWe evaluate our C-REC framework on three REC benchmark datasets (RefCOCO, RefCOCO+, RefCOCOg) and our C-REC datasets (C-RefCOCO/+/g). RefCOCO/+/g are based on MS-COCO images and differ in the types of descriptions allowed. C-REC datasets are generated using the CSG method to balance normal and counterfactual samples.\n\n3.2. Metrics\nWe use Acc-Box (IoU@0.5) for localization performance, Acc-Cls for counterfactual detection, and a new metric Acc-Cf for overall C-REC model performance.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 14,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 41,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 500,
    "chunk_method": "hierarchical",
    "importance_weight": 0.935,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s4_c1",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "3.3. Implementation Details\nOur methods are implemented on NVIDIA RTX A6000 GPUs, using Adam as the optimizer with a batch size of 32 and an initial learning rate of 0.0001. The image encoder is pre-trained on MS-COCO without the validation and test images. The model is trained on RefCOCO/+/g for 40 epochs and fine-tuned on C-RefCOCO/+/g for 20 epochs. The temperature parameter τ in the contrastive loss is set to 0.2.\n\n---",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 15,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 41,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 426,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s4_c2",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "---\n\nTable 4. Acc-Box (%) comparison of our model with baseline models on RefCOCO/+/g. Best and sub-optimal results are bolded and underlined, respectively.\n\nModel Visual Pretrained RefCOCO RefCOCO+ RefCOCOg Encoder Images val testA testB val testA testB val-u test-u\n\nVision-Language Pretrain\nMDETR[17] ResNet-101 200K ...\nOFA[43] ResNet-152 20M ...\nm-PLUG[21] ViT-L 14M ...",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 16,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 41,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 375,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s4_c3",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "One-stage REC\nFAOA[46] DarkNet-53 ... \nReSC[47] DarkNet-53 ... \nMCN[28] DarkNet-53 ... \nRealGIN[51] DarkNet-53 ... \nLG-FPN[39] DarkNet-53 ... \nPFOS[38] DarkNet-53 ... \nVGTR[9] ResNet-101 ... \nTransVG[7] ResNet-101 ... \nSimREC[29] CSPDarkNet-53 ... \nOurs CSPDarkNet-53 ...\n\nTable 5. Acc-Cls (%) on C-RefCOCO/+/g. Our model's performance against random choice, various confidence scores, and a binary classifier.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 17,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 41,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 410,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s4_c4",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "Method C-RefCOCO C-RefCOCO+ C-RefCOCOg val testA testB val testA testB val test\nRandom ... \nConf. score (0.01) ... \nConf. score (0.1) ... \nConf. score (0.5) ... \nBinary classifier ...\n\nParameters γcf and γcl are set to 2.0. Parameter α in DAF module is set to 0.25 for the counterfactual head and 1.0 for the regression head.\n\n3.4. Baselines\nWe compare our model with vision-language pretrained models and one-stage REC baselines.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 18,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 41,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 430,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s4_c5",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "3.5. Main Results\nTable 4 shows that our model outperforms one-stage REC models, particularly SimREC [29]. Our model achieves strong performance on the traditional REC task without large-scale vision-language dataset pretraining. Dual task training, i.e., localization and counterfactual classification, appears to enhance each other.\n\nTable 6. Acc-Cf (%) of our model on C-RefCOCO/+/g.\n\nModel C-RefCOCO C-RefCOCO+ C-RefCOCOg val testA testB val testA testB val test\nOurs ...",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 19,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 41,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 475,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s4_c6",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "Table 5 presents Acc-Cls on C-RefCOCO/+/g. Using a random choice and confidence scores as baselines, we set thresholds at 0.01, 0.1, and 0.5. The binary classifier achieves up to 90% accuracy, surpassing other methods by up to 15.73%. Table 6 reports Acc-Cf, indicating our model's strong overall performance on C-REC.\n\n3.6. Ablation Study\nTable 7 shows the impact of attribute features (Fa), contrastive loss (Lcl), and counterfactual training (C-Train) on model performance.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 20,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 41,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 476,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s4_c7",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "Table 7. Performance (%) with different components on C-RefCOCO testB.\n\nFa Lcl C-Train Acc-Box Acc-Cls Acc-Cf\n... \n\nTable 8. Acc-Cf (%) using different cross-modal fusion methods on C-RefCOCO.\n\nMethod val testA testB\nBaseline (Fvt) ... \nSerial fusion (Fa →Fvt) ... \nSerial fusion (Ft →Fva) ... \nParallel fusion (Fva + Fvt) ...\n\nFigure 5 and Table 9 present the effects of hyper-parameter α in DAF module and temperature parameter τ in contrastive loss on Acc-Cf.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 21,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 41,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 462,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s4_c8",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "Figure 5. Acc-Cf (%) with different α settings in DAF module on C-RefCOCO.\n\nTable 9. Acc-Cf (%) with different τ settings in contrastive loss on C-RefCOCO.\n\nτ val testA testB\n0.05 ... \n0.1 ... \n0.2 ... \n0.5 ... \n\n---\n\n---",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 22,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 41,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 221,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s4_c9",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "---\n\n---\n\nWe investigate the importance of global context information for localizing visual target referents and vary the temperature parameter τ in our model's training. Experimental results, shown in Table 9, indicate that our model performs best with τ equal to 0.2, suggesting that higher or lower values could weaken feature fusion in the latent feature space.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 23,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 41,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 365,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s4_c10",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "In qualitative analysis, we visualize examples in Figure 6 to demonstrate our model's performance. The first row illustrates the localization capabilities of our model and SimREC on RefCOC",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 24,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 41,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 188,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s4_c11",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "O. Our model accurately predicts counterfactual labels and provides precise localization, even outperforming SimREC in terms of IoU in failure cases. The following rows present counterfactual samples across seven pre-defined attributes. Our model successfully identifies most mismatched attributes but encounters difficulties with complex queries, such as less attention to the size attribute \"small\" and an overemphasis on absolute location \"center\".",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 25,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 41,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 451,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s4_c12",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "Related work on counterfactual REC includes approaches like SCRE and MTG, which treat C-REC as a matching task based on logical rules. Other methods extend existing models with binary classifiers. However, these do not address fine-grained counterfactual queries on various attributes.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 26,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 41,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 285,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s4_c13",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "In vision-language tasks, generating counterfactual image-text pairs is crucial for evaluating resilient models. Methods range from random matching to using GANs and manual modifications. In contrast, we propose a labor-free method based on language models.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 27,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 41,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 257,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s4_c14",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "Our paper concludes by highlighting the CSG method for constructing fine-grained C-REC datasets and the C-REC framework for detecting counterfactual polarity and target localization. The inclusion of a DAF module and contrastive learning shows promising results. Future work could explore generating counterfactual images to enhance dataset diversity.\n\n---",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 28,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 41,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 356,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s4_c15",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "---\n\nAcknowledgement:\nThis work was supported by the National Natural Science Foundation of China under Grant 6207603 and the High-Performance Computing Platform of BUPT.\n\nReferences:\n(Not included as per the instruction to only clean the current fragment and not to generate a complete paper structure.)",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 29,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 41,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 304,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s4_c16",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "Michal Adamkiewicz et al. present vision-only robot navigation in a neural radiance world (2022). Vedika Agarwal and colleagues work on causal VQA, addressing spurious correlations through invariant and covariant semantic editing (2020). Peter Anderson's team investigates bottom-up and top-down attention for image captioning and visual question answering (2018). Long Chen and associates propose counterfactual samples synthesizing for robust visual question answering",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 30,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 41,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 470,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s4_c17",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "(2020) and Ref-NMS to address proposal bottlenecks in referring expression grounding (2021). Enjie Cui's research focuses on selective comprehension for referring expression using prebuilt entity dictionaries with modular networks (2018).",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 31,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 41,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 238,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s4_c18",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "Jiajun Deng's group introduces TransVG, an end-to-end visual grounding method with transformers (2021). Jacob Devlin and others present BERT, a pre-trained deep bidirectional transformer for language understanding (2018). Ye Du and team explore visual grounding with transformers (2022). Zhiyuan Fang's work involves modularized textual grounding for counterfactual resilience (2019).",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 32,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 41,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 384,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s4_c19",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "Akira Fukui and colleagues contribute to multimodal compact bilinear pooling for VQA and visual grounding (2016). Ross Girshick presents Fast R-CNN for object detection (2015). Tanmay Gupta's study involves contrastive learning for weakly supervised phrase grounding (2020). Kaiming He's research introduces momentum contrast for unsupervised visual representation learning (2020).",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 33,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 41,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 381,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s4_c20",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "Sepp Hochreiter and Jürgen Schmidhuber propose the Long Short-Term Memory network (1997). Justin Johnson et al. introduce DenseCap for dense captioning (2016). Aishwarya Kamath and team present MDETR for modulated detection in end-to-end multi-modal understanding (2021). Sahar Kazemzadeh's ReferItGame refers to objects in photographs of natural scenes (2014).",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 34,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 41,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 361,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s4_c21",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "Yongmin Kim's work focuses on flexible visual grounding (2022). Diederik P. Kingma and Jimmy Ba introduce the Adam optimization method (2014). Chenliang Li's research involves vision-language learning by cross-modal skip-connections in mPlug (2022). Menghao Li and colleagues propose iterative robust visual grounding with masked reference-based centerpoint supervision (2023).",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 35,
    "chunk_index_in_section": 21,
    "total_chunks_in_section": 41,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 377,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s4_c22",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "Tsung-Yi Lin et al. introduce Microsoft COCO, a dataset of common objects in context (2014). Chang Liu's GRes aims at generalized referring expression segmentation (2023).\n\n---\n\nDaqing Liu, Hanwang Zhang, Feng Wu, and Zheng-Jun Zha. Learning to assemble neural module tree networks for visual grounding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4673–4682, 2019.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 36,
    "chunk_index_in_section": 22,
    "total_chunks_in_section": 41,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 402,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s4_c23",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "Xihui Liu, Zihao Wang, Jing Shao, Xiaogang Wang, and Hongsheng Li. Improving referring expression grounding with cross-modal attention-guided erasing. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1950–1959, 2019.\n\nMingcong Lu, Ruifan Li, Fangxiang Feng, Zhanyu Ma, and Xiaojie Wang. Lgr-net: Language guided reasoning network for referring expression comprehension. IEEE Transactions on Circuits and Systems for Video Technology, 2024.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 37,
    "chunk_index_in_section": 23,
    "total_chunks_in_section": 41,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 482,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s4_c24",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Liujuan Cao, Chenglin Wu, Cheng Deng, and Rongrong Ji. Multi-task collaborative network for joint referring expression comprehension and segmentation. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 10034–10043, 2020.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 38,
    "chunk_index_in_section": 24,
    "total_chunks_in_section": 41,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 296,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s4_c25",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "Gen Luo, Yiyi Zhou, Jiamu Sun, Shubin Huang, Xiaoshuai Sun, Qixiang Ye, Yongjian Wu, and Rongrong Ji. What goes beyond multi-modal fusion in one-stage referring expression comprehension: An empirical study. arXiv preprint arXiv:2204.07913, 2022.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 39,
    "chunk_index_in_section": 25,
    "total_chunks_in_section": 41,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 245,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s4_c26",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "Christopher D Manning, Mihai Surdeanu, John Bauer, Jenny Rose Finkel, Steven Bethard, and David McClosky. The stanford corenlp natural language processing toolkit. In Proceedings of 52nd annual meeting of the association for computational linguistics: system demonstrations, pages 55–60, 2014.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 40,
    "chunk_index_in_section": 26,
    "total_chunks_in_section": 41,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 293,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s4_c27",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 11–20, 2016.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 41,
    "chunk_index_in_section": 27,
    "total_chunks_in_section": 41,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 258,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s4_c28",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "Varun K Nagaraja, Vlad I Morariu, and Larry S Davis. Modeling context between objects for referring expression understanding. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part IV 14, pages 792–807. Springer, 2016.\n\nJingjing Pan, Yash Goyal, and Stefan Lee. Question-conditioned counterfactual image generation for vqa. arXiv preprint arXiv:1911.06352, 2019.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 42,
    "chunk_index_in_section": 28,
    "total_chunks_in_section": 41,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 430,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s4_c29",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532–1543, 2014.\n\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28, 2015.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 43,
    "chunk_index_in_section": 29,
    "total_chunks_in_section": 41,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 428,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s4_c30",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "Mohit Shridhar, Dixant Mittal, and David Hsu. Ingress: Interactive visual grounding of referring expressions. The International Journal of Robotics Research, 39(2-3):217–232, 2020.\n\nSanjay Subramanian, William Merrill, Trevor Darrell, Matt Gardner, Sameer Singh, and Anna Rohrbach. Reclip: A strong zero-shot baseline for referring expression comprehension. arXiv preprint arXiv:2204.05991, 2022.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 44,
    "chunk_index_in_section": 30,
    "total_chunks_in_section": 41,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 396,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s4_c31",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "Mengyang Sun, Wei Suo, Peng Wang, Yanning Zhang, and Qi Wu. A proposal-free one-stage framework for referring expression comprehension and generation via dense cross-attention. IEEE Transactions on Multimedia, 2022.\n\nWei Suo, Mengyang Sun, Peng Wang, Yanning Zhang, and Qi Wu. Rethinking and improving feature pyramids for one-stage referring expression comprehension. IEEE Transactions on Image Processing, 32:854–864, 2022.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 45,
    "chunk_index_in_section": 31,
    "total_chunks_in_section": 41,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 425,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s4_c32",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "Chien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu, Ping-Yang Chen, Jun-Wei Hsieh, and I-Hau Yeh. Cspnet: A new backbone that can enhance learning capability of cnn. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pages 390–391, 2020.\n\nPeng Wang, Qi Wu, Chunhua Shen, Anthony Dick, and Anton Van Den Hengel. Fvqa: Fact-based visual question answering. IEEE transactions on pattern analysis and machine intelligence, 40(10):2413–2427, 2017.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 46,
    "chunk_index_in_section": 32,
    "total_chunks_in_section": 41,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 485,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s4_c33",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "Peng Wang, Qi Wu, Jiewei Cao, Chunhua Shen, Lianli Gao, and Anton van den Hengel. Neighbourhood watch: Referring expression comprehension via language-guided graph attention networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1960–1968, 2019.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 47,
    "chunk_index_in_section": 33,
    "total_chunks_in_section": 41,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 292,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s4_c34",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In International Conference on Machine Learning, pages 23318–23340. PMLR, 2022.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 48,
    "chunk_index_in_section": 34,
    "total_chunks_in_section": 41,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 309,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s4_c35",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "Nevan Wichers, Dilek Hakkani-T¨ur, and Jindong Chen. Resolving referring expressions in images with labeled elements. In 2018 IEEE Spoken Language Technology Work- shop (SLT), pages 800–806. IEEE, 2018.\n\nQi Wu, Chunhua Shen, Anton Van Den Hengel, Peng Wang, and Anthony Dick. Image captioning and visual question answering based on attributes and their related external knowledge. arXiv preprint arXiv:1603.02814, 2016.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 49,
    "chunk_index_in_section": 35,
    "total_chunks_in_section": 41,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 419,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s4_c36",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "Zhengyuan Yang, Boqing Gong, Liwei Wang, Wenbing Huang, Dong Yu, and Jiebo Luo. A fast and accurate one-stage approach to visual grounding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4683–4693, 2019.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 50,
    "chunk_index_in_section": 36,
    "total_chunks_in_section": 41,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 238,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s4_c37",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "Zhengyuan Yang, Tianlang Chen, Liwei Wang, and Jiebo Luo. Improving one-stage visual grounding by recursive sub-query construction. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XIV 16, pages 387–404. Springer, 2020.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 51,
    "chunk_index_in_section": 37,
    "total_chunks_in_section": 41,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 277,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s4_c38",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "Jiabo Ye, Junfeng Tian, Ming Yan, Xiaoshan Yang, Xuwu Wang, Ji Zhang, Liang He, and Xin Lin. Shifting more attention to visual backbone: Query-modulated refinement networks for end-to-end visual grounding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15502–15512, 2022.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 52,
    "chunk_index_in_section": 38,
    "total_chunks_in_section": 41,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 316,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s4_c39",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and Jiebo Luo. Image captioning with semantic attention. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4651–4659, 2016.\n\nLicheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, and Tamara L Berg. Mattnet: Modular attention network for referring expression comprehension. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1307–1315, 2018.",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 53,
    "chunk_index_in_section": 39,
    "total_chunks_in_section": 41,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 481,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_s4_c40",
    "source_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
    "text": "Yiyi Zhou, Rongrong Ji, Gen Luo, Xiaoshuai Sun, Jinsong Su, Xinghao Ding, Chia-Wen Lin, and Qi Tian. A real-time global inference network for one-stage referring expression comprehension. IEEE Transactions on Neural Networks and Learning Systems, 2021.\n\n---",
    "section_title": "Experiments",
    "section_type": "experiment",
    "section_index": 4,
    "total_sections": 5,
    "chunk_index": 54,
    "chunk_index_in_section": 40,
    "total_chunks_in_section": 41,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 257,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8925,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding_s0_c0",
    "source_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
    "text": "---\n\nVISUAL PROMPT TUNING FOR WEAKLY SUPERVISED PHRASE GROUNDING\n\nPengyue Lin, Zhihan Yu, Mingcong Lu, Fangxiang Feng, Ruifan Li*, Xiaojie Wang\n\nSchool of Artificial Intelligence, Beijing University of Posts and Telecommunications {linpengyue, yzh0, lmc8133, fxfeng, rfli, xjwang}@bupt.edu.cn",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 6,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 292,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding_s1_c0",
    "source_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
    "text": "Weakly supervised phrase grounding (WSG) methods depend on object detectors for RoIs but are limited by the detector's category coverage. We propose a refinement-based approach using a detector-free phrase grounding model fine-tuned with a visual prompt from CLIP text-related representations. Combining the visual prompt with learnable features, our method achieves superior results on WSG tasks.",
    "section_title": "ABSTRACT",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 6,
    "chunk_index": 1,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 397,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9900000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding_s2_c0",
    "source_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
    "text": "WSG localizes objects described by a text phrase without additional annotations. Traditional methods use supervised object detectors, but this excludes numerous unlabeled data and classes. CLIP, a joint vision and language model, demonstrates strong image-text alignment. We enhance the relationship between CLIP and the grounding model by focusing on the grounding image encoder and CLIP image encoder. We compute the cosine similarity of CLIP image embedding with text tokens to create a visual prompt, which is used for fine-tuning the detector-free grounding network. Our contributions include th",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 6,
    "chunk_index": 2,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 2,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 600,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding_s2_c1",
    "source_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
    "text": "or fine-tuning the detector-free grounding network. Our contributions include the use of similarity tokens for spatial information capture and detector-free network fine-tuning.",
    "section_title": "INTRODUCTION",
    "section_type": "introduction",
    "section_index": 2,
    "total_sections": 6,
    "chunk_index": 3,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 2,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 177,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding_s3_c0",
    "source_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
    "text": "WSG has gained attention, with detector-based methods converting grounding tasks into retrieval tasks and detector-free methods performing dense localization. Our approach leverages the relationship between CLIP and the grounding model, refining training through visual prompt tuning.",
    "section_title": "RELATED WORK",
    "section_type": "related_work",
    "section_index": 3,
    "total_sections": 6,
    "chunk_index": 4,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 284,
    "chunk_method": "hierarchical",
    "importance_weight": 0.66,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding_s4_c0",
    "source_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
    "text": "3.1. Prompt design\n\n---\n\n---",
    "section_title": "METHOD",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 6,
    "chunk_index": 5,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 6,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 28,
    "chunk_method": "hierarchical",
    "importance_weight": 0.935,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding_s4_c1",
    "source_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
    "text": "---\n\nGiven an RGB image I ∈ R3×W ×H and an input text T, our method adopts the current state-of-the-art architecture for Weakly-Supervised Grounding (WSG) [2]. We propose a solution for phrase grounding refinement based on similarity tokens. These tokens are derived from the visual embeddings of the CLIP image encoder (eI = CLIPImage(I)) and the text embeddings of the CLIP text encoder (eT = CLIPText(T)). Aggregating these tokens serves as a visual prompt for fine-tuning the WWbl [2] network.",
    "section_title": "METHOD",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 6,
    "chunk_index": 6,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 497,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding_s4_c2",
    "source_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
    "text": "We initially freeze the CLIP model parameters to maintain the semantic associations obtained from vision-language pretraining. The image I is resized to 224×224 to match the CLIP input dimensions, and we obtain 1×768 text embedding eT and 256×768 visual embeddings eI without the [CLS] token. By replicating the text embedding to 256 copies, we calculate the cosine similarity between them, generating 256 similarity tokens. These are reshaped into a 16×16 similarity map SI,T:",
    "section_title": "METHOD",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 6,
    "chunk_index": 7,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 477,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding_s4_c3",
    "source_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
    "text": "SI,T = Reshape(eT * eI / ||eT|| / ||eI||)\n\nThe SI,T is a text-related feature map, where textual embeddings eT play a crucial role. This design in the CLIP inference stage can establish connections between encoders, as shown in other downstream tasks [1].\n\n3.2. Visual Prompt Tuning",
    "section_title": "METHOD",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 6,
    "chunk_index": 8,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 282,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding_s4_c4",
    "source_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
    "text": "Our approach involves visual prompt tuning, inspired by recent works [22, 23] that label symbols on images and feed them into pretrained models. We aim to provide a symbol-like visual prompt for the fusion features of the grounding model. However, due to the model not encountering such prompts during training, we perform short training for semantic aggregation of the fusion features guided by the prompt. This approach alters the structure and feature propagation of the pretrained model, akin to",
    "section_title": "METHOD",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 6,
    "chunk_index": 9,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 499,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding_s4_c5",
    "source_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
    "text": "ure and feature propagation of the pretrained model, akin to freezing part of the model's parameters and using learnable parameters for fine-tuning.",
    "section_title": "METHOD",
    "section_type": "method",
    "section_index": 4,
    "total_sections": 6,
    "chunk_index": 10,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 148,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8925,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding_s5_c0",
    "source_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
    "text": "4.1. Datasets\n\nFour benchmark datasets are used for experimental evaluation: Flickr30K Entities [24], COCO [25], Visual Genome [26], and ReferIt. Details of these datasets and the construction strategy are provided accordingly.\n\n4.2. Implementation Details and Metrics\n\nWe use VGG16 as the visual encoder in WWbl [2] and implement the model on an NVIDIA RTX A6000. Training details, including epochs, optimizers, and metrics such as “pointing game” accuracy and bounding box accuracy, are described.",
    "section_title": "EXPERIMENT",
    "section_type": "experiment",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 11,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 12,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 499,
    "chunk_method": "hierarchical",
    "importance_weight": 0.935,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding_s5_c1",
    "source_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
    "text": "4.3. Evaluation Results\n\nOur method is trained on COCO and VG train splits and evaluated on the test splits of Flickr30K, VG, and ReferIt. The performance of our method is compared with state-of-the-art DF-WSG methods quantitatively and qualitatively. Our method shows improvements in metrics on Flickr30K and ReferIt, with absolute improvements in pointing game accuracy and bounding box accuracy.",
    "section_title": "EXPERIMENT",
    "section_type": "experiment",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 12,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 398,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding_s5_c2",
    "source_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
    "text": "Table 1 presents a comparison of our method with SoTA DF-WSG methods. Figure 2 shows several results from phrase grounding models.\n\n---",
    "section_title": "EXPERIMENT",
    "section_type": "experiment",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 13,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 135,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding_s5_c3",
    "source_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
    "text": "---\n\nWe propose a method that breaks the local optimal equilibrium of fully fine-tuning, as evidenced by sample results in Fig. 2, which outperform WWbl [2]. Our method effectively mitigates the task-gap effect between CLIP and the grounding model, as shown in Table 2, where our heatmap generation outperforms CLIP-based methods in bounding box accuracy. This suggests our method overcomes the performance constraints of the CLIP structure, enhancing its grounding task performance.",
    "section_title": "EXPERIMENT",
    "section_type": "experiment",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 14,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 483,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding_s5_c4",
    "source_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
    "text": "Our approach not only surpasses detector-free methods like Gbs and WWbl but also outperforms detector-based methods on almost all categories for Flickr30K Entities. The training conditions of CLIP enable it to recognize more categories than pre-trained detectors, and our visual prompt activates the grounding model to perceive and attend to these categories.",
    "section_title": "EXPERIMENT",
    "section_type": "experiment",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 15,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 359,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding_s5_c5",
    "source_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
    "text": "In Table 3, we present the effects of visual prompt tuning with different prompts on VG, Flickr30K, and ReferIt datasets. Our approach using the proposed similarity map achieves the best grounding performance. Some prompts tend to focus on the most discriminative region related to the phrase while ignoring object boundary semantics, which reduces Io",
    "section_title": "EXPERIMENT",
    "section_type": "experiment",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 16,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 351,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding_s5_c6",
    "source_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
    "text": "U. Additionally, CLIP-based methods design grounding schemes for features in the CLIP image encoder, but the contrastive learning framework may lead to a lack of semantic aggregation from text embeddings.",
    "section_title": "EXPERIMENT",
    "section_type": "experiment",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 17,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 204,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding_s5_c7",
    "source_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
    "text": "Our work highlights the potential of multimodal information processing techniques in dual-encoder embedding spaces for spatial information extraction. The visual prompt is instrumental in enhancing weakly-supervised phrase grounding. Future research will explore applications to related multi-modal tasks, such as image captioning. This research was supported by the National Natural Science Foundation of China and other foundations.",
    "section_title": "EXPERIMENT",
    "section_type": "experiment",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 18,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 434,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding_s5_c8",
    "source_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
    "text": "Hsia, H.-A., et al. “Clipcam: A simple baseline for zero-shot text-guided object and action localization.” ICASSP, 2022, pp. 4453–4457.\n\nSun, Z., et al. “Alpha-clip: A clip model focusing on wherever you want.” arXiv, 2023.\n\nLiu, P., et al. “Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing.” ACM Computing Surveys, vol. 55, no. 9, 2023.",
    "section_title": "EXPERIMENT",
    "section_type": "experiment",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 19,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 392,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding_s5_c9",
    "source_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
    "text": "Li, X. and Liang, P. “Prefix-tuning: Optimizing continuous prompts for generation.” ACL-IJCNLP, 2021, pp. 4582–4597.\n\nDu, Y., et al. “Learning to prompt for open-vocabulary object detection with vision-language model.” CVPR, 2022, pp. 14084–14093.\n\nJia, M., et al. “Visual prompt tuning.” ECCV, 2022, pp. 709–727.\n\nShtedritski, A., et al. “What does clip know about a red circle? visual prompt engineering for vlms.” ICCV, 2023.\n\nYang, L., et al. “Fine-grained visual prompting.” NeurIPS, 2023.",
    "section_title": "EXPERIMENT",
    "section_type": "experiment",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 20,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 494,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding_s5_c10",
    "source_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
    "text": "Plummer, B.A., et al. “Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models.” ICCV, 2015, pp. 2641–2649.\n\nLin, T.-Y., et al. “Microsoft coco: Common objects in context.” ECCV, 2014, pp. 740–755.\n\nKrishna, R., et al. “Visual genome: Connecting language and vision using crowdsourced dense image annotations.” IJCV, vol. 123, 2017.\n\nChen, K., et al. “Query-guided regression network with context policy for phrase grounding.” ICCV, 2017, pp. 824–832.",
    "section_title": "EXPERIMENT",
    "section_type": "experiment",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 21,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 495,
    "chunk_method": "hierarchical",
    "importance_weight": 0.85,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding_s5_c11",
    "source_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
    "text": "Grubinger, M., et al. “The iapr tc-12 benchmark: A new evaluation resource for visual information systems.” International workshop ontoImage, 2006, vol. 2, pp. 13–23.\n\nFang, H., et al. “From captions to visual concepts and back.” CVPR, 2015, pp. 1473–1482.\n\nLiu, Y., et al. “Improving image paragraph captioning with dual relations.” ICME, 2022, pp. 1–6.\n\nShi, Y., et al. “S2td: A tree-structured decoder for image paragraph captioning.” ACMMM Asia, 2021, pp. 1–7.",
    "section_title": "EXPERIMENT",
    "section_type": "experiment",
    "section_index": 5,
    "total_sections": 6,
    "chunk_index": 22,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 464,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8925,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "全卷积神经结构的段落式图像描述算法_s0_c0",
    "source_id": "全卷积神经结构的段落式图像描述算法",
    "text": "段落式图像描述算法的目标是为给定图像生成描述性自然语言段落。当前，段落式图像描述算法主要采用编码器与解码器组合的端到端结构。编码器基于卷积神经网络(CNN)将图像表示为较低维的视觉向量，解码器基于循环神经网络(RNN)将视觉向量解码为自然语言段落。然而，RNN解码器在长时记忆和梯度消失问题上存在局限，导致生成段落的连贯性不佳。\n\n为提升连贯性，提出了一种全卷积神经结构的段落式图像描述算法。该算法的解码器由句子CNN解码器和词CNN解码器组成，句子CNN解码器捕捉段落内句子关系，词CNN解码器生成段落内单词。通过门控机制增强解码器的长时记忆能力。实验结果表明，该算法相比基于RNN的传统方法，能生成更连贯的段落式文本描述。\n\n段落生成算法包括两个主要过程：利用基于卷积网络的目标检测器对图像进行编码，通过卷积解码器对图像特征进行层次性解码得到描述性段落。解码生成句子时，可采取最大概率采样或集束搜索方法。实验采用斯坦福大学最新建立的图像段落描述公开数据集进行验证，结果表明所提方法在CIDEr等评价指标上优于基线方法，验证了其有效性。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 6,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 469,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "全卷积神经结构的段落式图像描述算法_s0_c1",
    "source_id": "全卷积神经结构的段落式图像描述算法",
    "text": "本文提出的基于全卷积神经网络结构的段落式图像描述生成模型，在层次性结构有效性方面优于Hierarchical-RNN方法，提升了17.8%的解码性能。所提方法在CIDEr指标上，相比Sentence-Concat、Image-Flat和Hierarchical-RNN方法，取得了更优的评测结果，有效提高了生成段落的品质，弥补了传统方法在描述能力上的不足。\n\n束大小是影响算法性能的关键参数。实验评估了不同束大小对指标的影响。结果表明，当束大小为2时，评测结果最优。过小的束大小会导致解码信息丢失，而过大则会使段落间句子重复度增加，降低多样性，并显著增加解码时间复杂度。\n\n进一步的研究中，通过观察迭代过程中各指标的变化，验证了结果的一致性。指标随迭代轮次的变化趋势基本保持一致，在第5至15个轮次间上升，在第15个轮次左右达到最优性能，之后出现过拟合现象。\n\n主观评价部分，通过对比所提方法和Hierarchical-RNN方法生成的描述段落，显示了所提方法在上下文连贯性和语言逻辑性方面的优势。所提方法减少了信息重复，生成的段落更具连贯性，与人类认知系统更为贴近。\n\n综上，所提模型通过卷积网络获取图像表示，构建层次性深度卷积解码器，引入门控机制，生成更具连贯性的段落式图像描述，实验证明在评测指标上取得了较好结果。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 560,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "全卷积神经结构的段落式图像描述算法_s0_c2",
    "source_id": "全卷积神经结构的段落式图像描述算法",
    "text": "参考文献：\n[1] Vinyals O, Toshev A, Bengio S, et al. Show and tell: a neural image caption generator. CVPR 2015.\n[2] Lu Jiasen, Xiong Caiming, Parikh D, et al. Knowing when to look: adaptive attention via a visual sentinel for image captioning. CVPR 2017.\n[3] Mao Yuzhao, Zhou Chang, Wang Xiaojie, et al. Show and tell more: topic-oriented multi-sentence image captioning. IJCAI 2018.\n[4] Xu K, Ba J, Kiros R, et al. Show, attend and tell: neural image caption generation with visual attention. ICML 2015.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 500,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "全卷积神经结构的段落式图像描述算法_s0_c3",
    "source_id": "全卷积神经结构的段落式图像描述算法",
    "text": "[5] You Quanzeng, Jin Hailin, Wang Zhaowen, et al. Image captioning with semantic attention. CVPR 2016.\n[6] Karpathy A, Li Feifei. Deep visual-semantic alignments for generating image descriptions. CVPR 2015.\n[7] Anderson P, He Xiaodong, Buehler C, et al. Bottom-up and top-down attention for image captioning and visual question answering. CVPR 2018.\n[8] Krause J, Johnson J, Krishna R, et al. A hierarchical approach for generating descriptive image paragraphs. CVPR 2017.\n[9] Liang Xiaodan, Hu Zhiting, Zhang Hao, et al. Recurrent topic-transition GAN for visual paragraph generation. ICCV 2017.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "全卷积神经结构的段落式图像描述算法_s0_c4",
    "source_id": "全卷积神经结构的段落式图像描述算法",
    "text": "[10] Goodfellow I, Pouget-Abadie J, Mirza M, et al. Generative adversarial nets. NIPS 2014.\n[11] Chatterjee M, Schwing A G. Diverse and coherent paragraph generation from images. ECCV 2018.\n[12] Wang Z, Luo Y, Li Y, et al. Look deeper see richer: depth-aware image paragraph captioning. ACM Multimedia 2018.\n[13] Che Wenbin, Fan Xiaopeng, Xiong Ruiqin, et al. Paragraph generation network with visual relationship detection. ACM Multimedia 2018.\n[14] Dauphin Y N, Fan A, Auli M, et al. Language modeling with gated convolutional networks. ICML 2017.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 549,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "全卷积神经结构的段落式图像描述算法_s0_c5",
    "source_id": "全卷积神经结构的段落式图像描述算法",
    "text": "[15] Krishna R, Zhu Yuke, Groth O, et al. Visual genome: connecting language and vision using crowdsourced dense image annotations. IJCV 2017.\n[16] Chen X, Fang H, Lin T Y, et al. Microsoft COCO captions: data collection and evaluation server. arXiv 2015.\n[17] Papineni K, Roukos S, Ward T, et al. BLEU: a method for automatic evaluation of machine translation. ACL 2002.\n[18] Vedantam R, Zitnick C L, Parikh D. CIDEr: consensus-based image description evaluation. CVPR 2015.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 5,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 475,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于统计和加权的提高击键认证识别方法(英文)_s0_c0",
    "source_id": "基于统计和加权的提高击键认证识别方法(英文)",
    "text": "---\n\nImproved Keystroke Authentication Accuracy Based on Statistics and Weight\n\nLi Jian, Guo Xiaojing, Li Meiyun, Li Ruifan\nSchool of Computer, Beijing University of Posts and Telecommunications, Beijing 100876, P. R. China",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 21,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 223,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于统计和加权的提高击键认证识别方法(英文)_s0_c1",
    "source_id": "基于统计和加权的提高击键认证识别方法(英文)",
    "text": "Abstract: This paper presents a methodology for improving the recognition accuracy of keystroke authentication through feature extraction of keystroke sequences. Outlier data is filtered, probabilities of each input key are calculated, and less informative features are selected. Weights are assigned, and a score for successful authentication is calculated. The use of a third-party dataset enhances objectivity and comparability. Experimental results show the proposed detector is more accurate than others.\n\nI. INTRODUCTION",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 526,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于统计和加权的提高击键认证识别方法(英文)_s0_c2",
    "source_id": "基于统计和加权的提高击键认证识别方法(英文)",
    "text": "I. INTRODUCTION\n\nMany systems still use static password authentication due to its low cost and convenience. However, this is fragile when users are careless or passwords are weak. This paper aims to enhance login-password recognition accuracy using biometric characteristics, which are unique and cannot be stolen, lost, or forgotten. Various classifiers have been developed for Keystroke Dynamics-Based Authentication (KDA). This paper introduces the TOP10 detector and compares it with other methods.\n\nII. METHODOLOGY",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 519,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于统计和加权的提高击键认证识别方法(英文)_s0_c3",
    "source_id": "基于统计和加权的提高击键认证识别方法(英文)",
    "text": "II. METHODOLOGY\n\nSection II describes the methodology to improve keystroke authentication recognition accuracy. Section III presents experiments and results, while Section IV discusses conclusions and further work.\n\nIII. PROCEDURE OF TOP10\n\n3.1 Data collection\nKeystroke dynamics analyze typing rhythms to discriminate among users. The password of n bits generates (2n - 1) keystroke eigenvalues, including latency and duration, forming a (2n - 1)-dimensional feature space. A benchmark dataset from 51 subjects typing a strong 10-character password is used.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 558,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于统计和加权的提高击键认证识别方法(英文)_s0_c4",
    "source_id": "基于统计和加权的提高击键认证识别方法(英文)",
    "text": "3.2 Procedure\nStep 1: Keystroke sequences are divided into vectors A_mn(pp) and A_mn(pr) for press and release times, respectively.\nStep 2: Input data is assumed to follow a normal distribution. Outliers beyond three standard deviations are removed, and mean μ and standard deviation σ are calculated.\nStep 3: Probability p(tk) for each element in the vector t is calculated using Eq. (3).\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 394,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于统计和加权的提高击键认证识别方法(英文)_s0_c5",
    "source_id": "基于统计和加权的提高击键认证识别方法(英文)",
    "text": "---\n\n[Eq. (3) should be included in the text, but due to the limitations of this platform, the mathematical expression is not cleaned in this response. Please ensure the equation is correctly formatted in the final document.]",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 5,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 225,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于统计和加权的提高击键认证识别方法(英文)_s0_c6",
    "source_id": "基于统计和加权的提高击键认证识别方法(英文)",
    "text": "In the use of C++ programming with Microsoft Visual Studio 2005, it is convenient to obtain the probability at a specific point in the test vector using the cumulative distribution function from the C++ Boost library. The probability, p(tk), is given by p(tk) = 2 × CDF(μk - ABS(μk - t)) (4), which avoids the complex process of quadrature. The cumulative distribution function (CDF) calculates a variable's probability of being less than or equal to a given value x, which is the integration of the probability density function from negative infinity to x.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 6,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 557,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于统计和加权的提高击键认证识别方法(英文)_s0_c7",
    "source_id": "基于统计和加权的提高击键认证识别方法(英文)",
    "text": "We select the 10 minimum probability values from the previous step and denote them as (s1, s2, ..., s10), then weigh them in Step 5. The weighing process involves setting the weight factor as w = 1/n for a given vector (s1, s2, ..., sn), assuming the same occurrence probability in practice. The mean value and standard deviation of the vector are calculated using Eq. (5) and Eq. (6).",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 7,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 385,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于统计和加权的提高击键认证识别方法(英文)_s0_c8",
    "source_id": "基于统计和加权的提高击键认证识别方法(英文)",
    "text": "The mean (s̄) and standard deviation (σs) determine a normal distribution N(s, σ^2s). A new vector (p1, p2, ..., pn) is obtained by standardizing the given vector (s1, s2, ..., sn) using the mean and standard deviation in Eq. (7). The probabilities of pi in the normal distribution N(s, σ^2s) are calculated using Eq. (8) and named as the probability vector (q1, q2, ..., qn).",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 8,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 376,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于统计和加权的提高击键认证识别方法(英文)_s0_c9",
    "source_id": "基于统计和加权的提高击键认证识别方法(英文)",
    "text": "The weight vector (w1, w2, ..., wn) is obtained using the probabilities in Eq. (9). Finally, the score evaluating the user is obtained using Eq. (10), which depends on the input password score s for successful authentication.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 9,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 225,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于统计和加权的提高击键认证识别方法(英文)_s0_c10",
    "source_id": "基于统计和加权的提高击键认证识别方法(英文)",
    "text": "For comparing the four detectors, a benchmark data set for keystroke dynamics is used, ensuring objectivity and convincing results. The training phase of the detector is conducted on timing vectors from genuine users, and the test phase is conducted on timing vectors from both genuine users and impostors. FRR (False Rejection Rate) and FAR (False Acceptance Rate) are calculated using Eq. (11) and Eq. (12), respectively.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 10,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 423,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于统计和加权的提高击键认证识别方法(英文)_s0_c11",
    "source_id": "基于统计和加权的提高击键认证识别方法(英文)",
    "text": "The results of the four detectors, including EER (Equal Error Rate), FAR, FRR, and FAR + FRR, are presented in Table I. The performance of each detector is analyzed, with TOP10 showing the best improvement with an increase in the number of samples. The smallest sum of FAR and FRR is found for the detector proposed in this paper, indicating its efficiency compared to the other three detectors.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 11,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 395,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于统计和加权的提高击键认证识别方法(英文)_s0_c12",
    "source_id": "基于统计和加权的提高击键认证识别方法(英文)",
    "text": "This paper presents a methodology that utilizes typing biometrics to enhance login-password authentication. The objective is to propose a good methodology and use a third-party benchmark data set for testing keystroke dynamics. Future work will focus on improving the efficiency of feature template depiction and updating the template for better keystroke familiarity and recognition accuracy.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 12,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 393,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于统计和加权的提高击键认证识别方法(英文)_s0_c13",
    "source_id": "基于统计和加权的提高击键认证识别方法(英文)",
    "text": "This paper is part of the project \"Key Technology Research of Eavesdropping Detection in the Quantum Security Communication,\" supported by the National Natural Science Foundation of China under Grant No. 61100205.\n\nReferences:",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 13,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 226,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于统计和加权的提高击键认证识别方法(英文)_s0_c14",
    "source_id": "基于统计和加权的提高击键认证识别方法(英文)",
    "text": "[1] TEH P, TEOH A, ONG T, et al. Statistical Fusion Approach on Keystroke Dynamics. Proceedings of the IEEE Conference on Signal-Image Technologies and Internet-Based System, 2007: 918-923.\n[2] LI Jian, JIN Haifei, JING Bo. Improved Security Detection Strategy for Quantum \"Ping-Pong\" Protocol and Its Security Analysis. China Communications, 2011, 8(3): 170-179.\n[3] KANG P, PARK S, HWANG S, et al. Improvement of Keystroke Data Quality Through Artificial Rhythms and Cues. Computers and Security, 2008: 3-11.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 14,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 510,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于统计和加权的提高击键认证识别方法(英文)_s0_c15",
    "source_id": "基于统计和加权的提高击键认证识别方法(英文)",
    "text": "[4] AKILA M, SURESH K. Improving Feature Extraction in Keystroke Dynamics Using Optimization Techniques and Neural Network. Proceedings of the International Conference on Sustainable Energy and Intelligent Systems, 2011: 891-898.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 15,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 229,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于统计和加权的提高击键认证识别方法(英文)_s0_c16",
    "source_id": "基于统计和加权的提高击键认证识别方法(英文)",
    "text": "[5] BAZRAFSHAN F, JAVANBAKHT A, MOJALLALI H. Keystroke Identification with a Genetic Fuzzy Classifier. Proceedings of the 2nd International Conference on Computer Engineering and Technology, 2010: 136-140.\n[6] MARTONO W, ALI H, SALAMI M. Key-Stroke Pressure-Based Typing Biometrics Authentication System Using Support Vector Machines. Computational Science and Its Applications, 2007: 85-93.\n[7] SINGH S, ARYA K. Key Classification: A New Approach in Free Text Keystroke Authentication System. Proceedings of the 3rd Pacific-Asia Conference on Circuits, Communications and System, 2011: 1-5.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 16,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 591,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于统计和加权的提高击键认证识别方法(英文)_s0_c17",
    "source_id": "基于统计和加权的提高击键认证识别方法(英文)",
    "text": "[8] LI Jian, SONG Danjie, GUO Xiaojing, et al. ID Updating Based RFID Mutual Authentication Protocol for Low-Cost Tags. China Communications, 2011, 8(7): 122-127.\n[9] CHANG T, TSAI C, LIN J. A Graphical-Based Password Keystroke Dynamic Authentication System For Touch Screen Handheld Mobile Devices. Journal of Systems and Software, 2012, 85(5): 1157-1165.\n[10] QI Yong, YAO Qingsong, CHEN Ying, et al. Study on RFID Authentication Protocol Theory. China Communications, 2011, 8(1): 65-71.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 17,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 489,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于统计和加权的提高击键认证识别方法(英文)_s0_c18",
    "source_id": "基于统计和加权的提高击键认证识别方法(英文)",
    "text": "[...]\n\nBiographies:\n\nLi Jian, Ph.D., associate professor at Beijing University of Posts and Telecommunications. Research interests include quantum information, quantum computation, quantum communication security, electronic commerce, and artificial intelligence. Email: lijian@bupt.edu.cn",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 18,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 288,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于统计和加权的提高击键认证识别方法(英文)_s0_c19",
    "source_id": "基于统计和加权的提高击键认证识别方法(英文)",
    "text": "Guo Xiaojing, M.S. student at the School of Computer, Beijing University of Posts and Telecommunications. Received B.S. in Network Engineer (Information Security) from Zhengzhou University of Light Industry. Research interests include keystroke authentication, quantum information, information security, and the security of the Internet of Things. Email: bunny-gxj@163.com",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 19,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 372,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于统计和加权的提高击键认证识别方法(英文)_s0_c20",
    "source_id": "基于统计和加权的提高击键认证识别方法(英文)",
    "text": "Li Meiyun, undergraduate student at the School of Computer, Beijing University of Posts and Telecommunications. Research interests include quantum information, information security, and the security of the Internet of Things. Email: 1031765038@qq.com\n\nLi Ruifan, lecturer at Beijing University of Posts and Telecommunications. Research interests include quantum information, quantum computation, quantum communication security, information security, and artificial intelligence. Email: rfli@bupt.edu.cn",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 20,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 502,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多视图有监督的LDA模型_s0_c0",
    "source_id": "多视图有监督的LDA模型",
    "text": "本文主要关注多视图数据的分类问题。考虑到集成分类方法可组合多个弱分类器构成一个强分类器，以及主题模型能学习复杂数据的语义表示，本文试图将集成学习思想引入主题模型中，以便同时学习多视图数据的分类规则和预测性语义特征。具体地，结合概率主题模型LDA模型和集成分类方法Softmax混合模型，提出了一个多视图有监督的分类模型。基于变分EM方法，推导了该模型的参数估计算法。两个真实图像数据集上的实验结果表明了提出模型有较好的分类性能。\n\n本文提出了一个分类多视图数据的概率主题模型Mv-sLDA，并基于变分EM推导了参数估计算法。在两个真实图像数据集上的实验结果表明，该模型具有较好的分类性能，并充分利用了多视图信息。下一步工作将考虑如何确定主题数目，以及如何确定特征的视图分组等问题。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 340,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "考虑合成灰数灰度性质的改进区间灰数预测模型_s0_c0",
    "source_id": "考虑合成灰数灰度性质的改进区间灰数预测模型",
    "text": "基于论域，本文提出了合成灰数灰度的定义及性质，并据此分析了基于核和灰度的区间灰数预测模型中灰度预测值的确认方法存在的不足。通过分别建立核序列和灰度序列的GM(1,1)模型，从“核”和“灰度”两个方面同时发掘区间灰数序列的内蕴信息和发展趋势，实现了对原有区间灰数预测模型的改进和完善。改进后的模型较好地克服了原有模型存在的不足，同时实现了模型的误差分析和精度检验。算例表明了改进模型的有效性和可用性。\n\n核序列和灰度序列是区间灰数序列的两个重要特征序列，研究表明，灰数的灰度与核是相互关联的。考虑这种关联性，建立核序列和灰度序列的关联预测模型，进而实现区间灰数序列预测，提高预测精度，可以作为下一阶段研究的重点。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 305,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "一种使用深层结构获取双模态相似性测度的方法_李睿凡_s0_c0",
    "source_id": "一种使用深层结构获取双模态相似性测度的方法_李睿凡",
    "text": "本发明提出了一种使用深层结构获取双模态相似性测度的方法，包括以下步骤：\n\n1. 使用经典特征提取方法获取第一模态的低级表达P1和第二模态的低级表达T1。\n\n2. 通过堆叠的两层受限波尔兹曼机，将第一模态的低级表达P1转换为中级表达P3，将第二模态的低级表达T1转换为中级表达T3。\n\n3. 使用自动编码器对第一模态的中级表达P3和第二模态的中级表达T3进行编码，得到第一模态的高级表达P4和第二模态的高级表达T4。\n\n4. 计算P4和T4的相似性测度C，公式为C(P3,T3;Wf,Wg)=||f(P3;Wf)-g(T3;Wg)||1，其中||·||1为L1范数，f(P3;Wf)=P4，g(T3;Wg)=T4，Wf和Wg分别为第一模态和第二模态的自动编码器的参数。\n\n5. 通过训练算法优化Wf和Wg，使双模态数据的相似性测度C最小。\n\n本发明通过深度学习框架，实现了双模态数据相似性的计算，适用于多模态数据挖掘和检索等领域。\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 3,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 419,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种使用深层结构获取双模态相似性测度的方法_李睿凡_s0_c1",
    "source_id": "一种使用深层结构获取双模态相似性测度的方法_李睿凡",
    "text": "本发明通过深度学习框架，实现了双模态数据相似性的计算，适用于多模态数据挖掘和检索等领域。\n\n---\n\n[0062] 两个子网络分别记为 f(P3;Wf) 和 g(T3;Wg)，其中 f(·) 表示第一模态中级表达 P3 到 P4 的变换法则，即 P4 = f(P3;Wf)。第三组件第一模态的网络结构与第二组件的 BB-RBM 神经网络结构相同。g(·) 表示第二模态中级表达 T3 到 T4 的变换法则，即 T4 = g(T3;Wg)。P4 与 T4 维数相同，均为二元表示。\n\n[0063] 编码层对 P4 与 T4 进行相似性测度计算：\n\n[0064] C(P4,T4) = ||P4 - T4||1 （5）\n\n[0065] 相似性测度公式可表示为：\n\n[0066] C(P3,T3;Wf,Wg) = ||f(P3;Wf) - g(T3;Wg)||1 （6）\n\n[0067] 其中 ||·||1 为 L1 范数。\n\n[0068] 相似性测度 C 描述两个模态描述同一对象时的匹配程度。\n\n[0069] Wf 与 Wg 是经过训练算法计算得到的配置参数，在双模态数据相似性测度计算前预先配置。\n\n[0070] 本发明提出损失函数：\n\n[0071] L(P3,T3,I;θ) = α(LI(P3;θ) + LT(T3;θ)) + (1-α)LC(P3,T3,I;θ) （7）",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 588,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种使用深层结构获取双模态相似性测度的方法_李睿凡_s0_c2",
    "source_id": "一种使用深层结构获取双模态相似性测度的方法_李睿凡",
    "text": "[0071] L(P3,T3,I;θ) = α(LI(P3;θ) + LT(T3;θ)) + (1-α)LC(P3,T3,I;θ) （7）\n\n[0072] 其中，\n\n[0073]\n\n[0074]\n\n[0075] LC(P3,T3,I;θ) = IC^2 + (1-I)exp(-λC) （10）\n\n[0076] 参数 λ 由 C(P3,T3;Wf,Wg) 在整个训练集上的上界确定。参数 α 用于平衡重构损失和兼容性损失。\n\n[0077] Wf 与 Wg 使损失函数最小。\n\n[0078] 自动编码器的学习使用标准后传算法。两个子网在编码层通过相似性测度耦合，得到对应的编码 T4 与 P4。\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 301,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种图像检索方法_鲁鹏_s0_c0",
    "source_id": "一种图像检索方法_鲁鹏",
    "text": "本发明公开了一种图像检索方法，首先计算图像数据库中任意两个图像的内点数，根据公式计算任意两个图像的相关度值；然后计算查询目标与图像数据库中任一图像的相关度值，得到与查询目标直接相关的图像与间接相关的图像；构建一个赋权邻接矩阵A进行衰减，且增加弥补衰减过度造成间接相关度值过小的补给项计算查询目标vq与图像vi的间接相关度值，并通过迭代调整所述间接相关度值使得相关度值越高的图像与查询目标vq的间接相关度值越大；将查询目标与图像数据库各图像的直接相关度和间接相关度进行排序，得到图像数据库中包含查询目标的图像。\n\n为了满足假设约束，引入了间接相关度衰减因子αB，其控制间接相关度从当前节点vi流向与其直接相关节点时间接相关度的衰减程度；同时引入了赋权邻接矩阵A，保证与vi相邻的节点中连接边的权值越高的节点获得的间接相关度越多。\n\n计算步骤包括：\n1. 计算匹配图赋权邻接矩阵A。\n2. 生成初始间接相关度矩阵D。\n3. 迭代计算间接相关度T，公式为T＝αB·A·T+β·D。\n\n为了提高计算效率，可以生成一个目标集Vh，其规模较小，与查询目标高度相关，包含大多数相关节点。通过迭代查询目标集Vh新增节点的直接相关节点来不断更新目标集Vh，最终得到一个图像数量相对较小的目标集，包含了绝大多数与查询目标高度相关的节点。利用目标集Vh计算间接相关度，可以提升计算速度同时降低存储开销。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 590,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种图像的文本描述方法及装置_李睿凡_s0_c0",
    "source_id": "一种图像的文本描述方法及装置_李睿凡",
    "text": "后的内容：\n\n本发明实施例提供了一种图像的文本描述方法及装置，方法包括：获取待描述图像，提取待描述图像的多个区域特征和一个全局特征；将区域特征、全局特征输入预先训练的文本描述神经网络中的句子级子网络，得到针对每个待生成句子的句子指导向量；将句子指导向量输入文本描述神经网络中的词汇子网络，得到描述文本；由于采用句子级子网络和词汇级子网络的分层结构，能够捕捉段落中句子之间的连贯性，提高了生成的文本段落中句子之间的连贯性，此外，相较于现有的基于循环神经网络的方案，降低了训练过程的计算复杂度。\n\n本发明还提供了一种图像的文本描述装置，包括：获取模块，用于获取待描述图像；提取模块，用于提取所述待描述图像的多个区域特征和一个全局特征；第一输入模块，用于将所述区域特征、所述全局特征输入预先训练的文本描述神经网络中的句子级子网络，得到针对每个待生成句子的句子指导向量；第二输入模块，用于将所述句子指导向量输入所述文本描述神经网络中的词汇级子网络，得到描述文本；所述文本描述神经网络是根据训练集训练得到的，所述训练集包括：多个样本图像的区域特征和全局特征，以及每个样本图像对应的样本描述文本及所述样本描述文本中包含的样本句子数目。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 2,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 511,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种图像的文本描述方法及装置_李睿凡_s0_c1",
    "source_id": "一种图像的文本描述方法及装置_李睿凡",
    "text": "本发明还提供了一种电子设备，包括处理器、通信接口、存储器和通信总线，其中处理器用于执行存储器上存放的程序，实现上述图像的文本描述方法；通信接口用于与其他设备进行通信；存储器用于存放计算机程序；通信总线用于连接处理器、通信接口和存储器，实现它们之间的通信。\n\n本发明实施例中，文本描述神经网络由句子级子网络和词汇级子网络组成，两者均可为CNN。该神经网络根据训练集预先训练完成，训练集包括多个样本图像的区域特征、全局特征以及对应的样本描述文本和样本句子数目。\n\n图片特征提取器提取的图片特征输入句子级子网络后，输出句子指导向量。词汇级子网络根据句子指导向量生成针对待描述图像的段落描述文本。\n\n句子级子网络包含句子嵌入层、门控卷积层和区域感知层。词汇级子网络包含词汇嵌入层。训练完成的门控卷积层基于拼接向量进行卷积运算，输出综合语义向量和门控向量，再进行语义筛选运算，输出隐藏向量。区域感知层基于区域特征向量和隐藏向量计算权重，输出加权区域特征向量，再计算句子指导向量。\n\n词汇级子网络根据句子指导向量和先前生成的词汇嵌入向量，生成当前词汇的隐藏向量，并预测单词分布。\n\n训练模块获取预设的神经网络模型和训练集，输入样本图像特征，得到描述文本和句子数目，确定损失值，调整参数，直至收敛，确定文本描述神经网络。\n\n本发明还提供了一种图像的文本描述装置和电子设备，以及计算机可读存储介质。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 2,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 591,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种图像色彩和谐程度的评估方法及装置_鲁鹏_s0_c0",
    "source_id": "一种图像色彩和谐程度的评估方法及装置_鲁鹏",
    "text": "本发明提供了一种图像色彩和谐程度的评估方法及装置，包括以下步骤：\n\n1. 通过预设的无向图建立初始条件随机场。\n2. 获取多张高质量样本图像和多张低质量样本图像，将每张图像切分为多个样本图像块，并将高质量样本图像块归类至第一样本图像块集合，低质量样本图像块归类至第二样本图像块集合。\n3. 从第一样本图像块集合中选取一个样本图像块并标注第一类别标签后，输入初始残差神经网络；从第二样本图像块集合中选取一个样本图像块并标注第二类别标签后，输入至初始残差神经网络，对初始残差神经网络进行训练。\n4. 从第一样本图像块集合中选取两个样本图像块并标注第三类别标签后，输入至初始孪生神经网络；从第二样本图像块集合中选取两个样本图像块并标注第四类别标签后，输入至初始孪生神经网络，对初始孪生神经网络进行训练。\n5. 利用训练后的残差神经网络确定初始条件随机场的关联势函数，利用训练后的孪生神经网络确定交互势函数。\n6. 根据关联势函数和交互势函数确定条件随机场模型。\n7. 当获取到待评估图像时，将待评估图像切分为多个图像块并输入至条件随机场模型，计算得到待评估图像的色彩和谐评估值。\n\n本发明实现了自动对图像的色彩和谐程度进行评估。",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 2,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 511,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种图像色彩和谐程度的评估方法及装置_鲁鹏_s0_c1",
    "source_id": "一种图像色彩和谐程度的评估方法及装置_鲁鹏",
    "text": "本发明实现了自动对图像的色彩和谐程度进行评估。\n\n为了更清晰地说明本申请实施例或现有技术中的技术方案，下面将对实施例或现有技术描述中所需要使用的附图作简单地介绍，显而易见地，下面描述中的附图仅仅是本申请的一些实施例，对于本领域普通技术人员来讲，在不付出创造性劳动的前提下，还可以根据这些附图获得其他的附图。\n\n图1为本发明实施例提供的图像色彩和谐程度的评估方法的一种流程示意图；\n\n图2为本发明实施例中建立无向图的示意图；\n\n图3为基于ResNet卷积神经网络所构建的初始残差神经网络的结构示意图；\n\n图4为基于ResNet卷积神经网络所构建的初始孪生神经网络的结构示意图；\n\n图5为本发明实施例提供的图像色彩和谐程度的评估装置的一种结构示意图；\n\n图6为本发明实施例提供的图像色彩和谐程度的评估装置的另一种结构示意图；\n\n图7为本发明实施例提供的电子设备的一种结构示意图。",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 2,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 387,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种图像色彩和谐程度的评估方法及装置_鲁鹏_s1_c0",
    "source_id": "一种图像色彩和谐程度的评估方法及装置_鲁鹏",
    "text": "下面将结合本发明实施例中的附图，对本发明实施例中的技术方案进行描述。\n\n现有技术中，技术人员根据经验选取一些与图像美学评价相关的图像特征描述图像美学质量。例如将光照和色彩、饱和度和色调、三分法、区域组成等多重特征引入到SVM等相关工具中进行训练，并对目标图像的美学价值进行分类评估。然而，经验性地提取图像特征来进行美学评估是一项繁琐的工作，严重依赖于技术人员对照相领域知识的理解，技术人员难以选择合适的图像美学评价的图像特征，致使图像美学评估的准确度不高。\n\n色彩是图像的重要特征，色彩和谐程度直接影响着人们对于照片质量的感受。传统的色彩和谐模型大都来自于人工经验，不适用于颜色复杂度高的照片的色彩评估任务。利用机器方法从大量的图片中学习色彩和谐关系是一种更为有效的图像色彩和谐度计算方法。\n\n直接评估一副图像的色彩和谐程度，由于颜色的组合空间过大，通常难以直接建立机器学习建模。但是，一副图像可以看做是图像局部区域的集合，此时，图像的和谐度可以通过这些局部图像区域的和谐程度来表示。而图像局部区域的色彩质量不仅取决于区域内颜色的和谐程度，还取决于其与邻域图像块之间的色彩和谐关系。基于上述分析，本发明提出了一种基于条件随机场的色彩和谐模型，同时考虑影响局部区域色彩和谐度的两种因素。",
    "section_title": "具体实施方式",
    "section_type": "default",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 2,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 5,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 542,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种图像色彩和谐程度的评估方法及装置_鲁鹏_s1_c1",
    "source_id": "一种图像色彩和谐程度的评估方法及装置_鲁鹏",
    "text": "总体上来说，本发明提供了一种图像色彩和谐程度的评估方法、装置、电子设备及计算机可读存储介质。上述方法可以应用于可以计算处理图像的终端，例如计算机。\n\n利用本发明实施例提供的方法，终端可以将图像划分成多个图像块确定无向图，终端根据无向图建立条件随机场，并用训练后的神经网络确定条件随机场的关联势函数和条件势函数，得到色彩和谐模型。终端根据得到的色彩模型来评估图像美学得分。终端通过表示条件随机场关联势函数的神经网络提取图像块的RGB(Red，Green，Blue，红，绿，蓝)色彩值，计算图像块的色彩和谐性，通过表示条件随机场关联势函数的神经网络提取相邻图像块的RGB(Red，Green，Blue，红，绿，蓝)色彩值，计算相邻图像块的色彩和谐性，再通过条件随机场将图像的多个图像块的色彩和谐性整合在一起，得到图像的美学评估。相比于现有技术，本发明实施例提供的方法，提取图像特征简单，利用神经网络计算准确，可以提高图像美学评估的准确性。\n\n参见图1，图1为本发明实施例的一种图像色彩和谐程度的评估方法流程图，包括如下步骤：\n\nS101，通过预设的无向图建立初始条件随机场。\n\nS102，根据预设样本图像和预设训练算法，对初始神经网络进行训练。\n\nS103，利用训练后的神经网络确定初始条件随机场的关联势函数及交互势函数。",
    "section_title": "具体实施方式",
    "section_type": "default",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 3,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 5,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 559,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种图像色彩和谐程度的评估方法及装置_鲁鹏_s1_c2",
    "source_id": "一种图像色彩和谐程度的评估方法及装置_鲁鹏",
    "text": "S102，根据预设样本图像和预设训练算法，对初始神经网络进行训练。\n\nS103，利用训练后的神经网络确定初始条件随机场的关联势函数及交互势函数。\n\nS104，根据关联势函数和交互势函数，确定初始条件随机场对应的条件随机场模型。\n\nS105，当获取到待评估图像时，将待评估图像切分为多个图像块并输入至条件随机场模型，计算得到待评估图像的色彩和谐评估值。\n\n本发明实施例提供的一种图像色彩和谐程度的评估装置，建立初始条件随机场，对初始神经网络进行训练，利用训练后的神经网络确定初始条件随机场的关联势函数及交互势函数，进而根据关联势函数和交互势函数，确定初始条件随机场对应的条件随机场模型，即色彩和谐模型，当需要对图像的和谐程度进行评估时，只需将该图像切分后输入至色彩和谐模型，无需人工评估即可计算得到对应的色彩和谐评估值，从而实现自动对图像的色彩和谐程度进行评估。",
    "section_title": "具体实施方式",
    "section_type": "default",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 4,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 5,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 380,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种图像色彩和谐程度的评估方法及装置_鲁鹏_s1_c3",
    "source_id": "一种图像色彩和谐程度的评估方法及装置_鲁鹏",
    "text": "本发明实施例还提供了一种电子设备，包括处理器、通信接口、存储器和通信总线，其中，处理器、通信接口、存储器通过通信总线完成相互间的通信，存储器用于存放计算机程序；处理器用于执行存储器上所存放的程序时，实现如下步骤：通过预设的无向图建立初始条件随机场；根据预设样本图像和预设训练算法，对初始神经网络进行训练，预设样本图像包括高质量预设样本图像和低质量预设样本图像；利用训练后的神经网络确定初始条件随机场的关联势函数及交互势函数；根据关联势函数和交互势函数，确定初始条件随机场对应的条件随机场模型；当获取到待评估图像时，将待评估图像切分为多个图像块并输入至条件随机场模型，计算得到待评估图像的色彩和谐评估值。\n\n本发明实施例提供一种电子设备，通过建立初始条件随机场并训练初始神经网络，利用训练后的神经网络确定关联势函数及交互势函数，进而确定对应的条件随机场模型，即色彩和谐模型。该模型可自动评估图像的色彩和谐程度。电子设备中，通信总线可以是PCI或EISA总线，通信接口用于设备间通信，存储器包括RAM和磁盘存储器。\n\n处理器可以是CPU、NP、DSP、ASIC、FPGA或其他可编程逻辑器件。本发明还涉及一种计算机可读存储介质，存储有执行以下步骤的计算机程序：建立初始条件随机场；对初始神经网络进行训练；确定关联势函数及交互势函数；确定条件随机场模型；对待评估图像进行色彩和谐评估。",
    "section_title": "具体实施方式",
    "section_type": "default",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 5,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 5,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 589,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种图像色彩和谐程度的评估方法及装置_鲁鹏_s1_c4",
    "source_id": "一种图像色彩和谐程度的评估方法及装置_鲁鹏",
    "text": "本发明实施例的装置、电子设备及存储介质适用于图像色彩和谐程度的评估方法，并能达到相同或相似的有益效果。关系术语如“第一和第二”用于区分实体或操作，而不暗示实际关系。术语“包括”意指非排他性包含。各实施例重点说明与其他实施例的不同之处。本发明的保护范围包括在精神和原则内的任何修改、等效替换、改进等。",
    "section_title": "具体实施方式",
    "section_type": "default",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 6,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 5,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 149,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种基于人工智能挖掘的网络内容风控管理系统_李睿凡_s0_c0",
    "source_id": "一种基于人工智能挖掘的网络内容风控管理系统_李睿凡",
    "text": "一种基于人工智能挖掘的网络内容风控管理系统，包括新业务内容检测模块、网站内容监管模块、企业内容安全治理模块和UGC内容审核模块。新业务内容检测模块用于自动获取新业务内容，并进行基于人工智能挖掘的审核、分类。网站内容监管模块对预设网站库内的网站内容、暗链、外链、篡改、漏洞、可用性进行检测。企业内容安全治理模块基于主动拨测、旁路检测、文件共享的内容采集技术，自动化获取文本、视频、图片、音频、复杂文档内容，并进行基于人工智能挖掘的内容审核。UGC内容审核模块用于对用户原创内容进行审核，包括视频直播、婚恋交友、社区论坛、电商网站、和在线教育。该系统实现了对网络内容风控的及时、准确处理。\n\n本申请提供了一种基于人工智能挖掘的网络内容风控管理系统，包括新业务内容检测模块、网站内容监管模块、企业内容安全治理模块和UGC内容审核模块。新业务内容检测模块通过工具自动获取新业务内容，并进行基于人工智能挖掘的审核、分类。网站内容监管模块对预设网站库内的网站内容进行检测。企业内容安全治理模块基于主动拨测、旁路检测、文件共享的内容采集技术，自动化获取文本、视频、图片、音频、复杂文档内容，并进行基于人工智能挖掘的内容审核。UGC内容审核模块对用户原创内容进行审核，包括文本识别、图像识别、视频识别和音频识别。整个系统通过人工智能模型判断数据中是否包含违规内容。\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 2,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 579,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种基于人工智能挖掘的网络内容风控管理系统_李睿凡_s0_c1",
    "source_id": "一种基于人工智能挖掘的网络内容风控管理系统_李睿凡",
    "text": "---\n\n[0105] 本申请的部件实施例可通过硬件、一个或多个处理器上运行的软件模块，或其组合实现。技术人员应理解，微处理器或数字信号处理器（DSP）可用于实现本申请中虚拟机创建系统的一个或多个部分的功能。此外，本申请可实施为执行所描述方法的设备或系统程序，如计算机程序和产品。相应程序可存储于计算机可读介质上，或以信号形式提供，可通过互联网下载或以其他方式供应。\n\n[0106] 应注意，上述实施例旨在说明而非限制本申请，技术人员可在不脱离权利要求范围的情况下设计替代实施例。权利要求中的括号内参考符号不应对权利要求构成限制。“包含”一词不排除未列元件或步骤的存在。“一”或“一个”不排除多个元件的存在。本申请可通过硬件及适当编程的计算机实现。单元权利要求中列举的系统单元可通过相同硬件项体现。第一、第二、第三等词汇的使用不代表任何顺序。\n\n[0107] 本申请的具体实施方式如上所述，但其保护范围不限于此。任何熟悉本技术领域的技术人员可在本申请揭露的技术范围内想到各种变化或替换，这些均应包含在本申请的保护范围内。因此，本申请的保护范围应以权利要求的范围为基准。\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 2,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 489,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种基于人工智能的数据安全风险监测追溯系统_黄永军_s0_c0",
    "source_id": "一种基于人工智能的数据安全风险监测追溯系统_黄永军",
    "text": "一种基于人工智能的数据安全风险监测追溯系统，包括数据采集模块、数据流转与分布监测模块、数据安全事件分析模块和数据安全事件溯源模块。数据采集模块实时采集企业侧各类安全事件信息，建立数据识别特征库。数据流转与分布监测模块解析安全事件信息，发现数据资产保护对象，生成数据资产清单，并动态监测数据分布与流转。数据安全事件分析模块智能分析数据安全风险，对数据分类分级管理，建立数据安全风险监测策略库。数据安全事件溯源模块基于人工智能对实时安全事件进行溯源分析，并及时上报安全事件和溯源信息。该系统通过体系化数据安全管控建设，实现对数据安全风险的实时监测、主动识别、精准定位和自动溯源，保障工业互联网数据安全，助力工业企业数字化转型。\n\n以下详细介绍每个模块的具体实现方式及技术细节：\n\n数据采集模块包括以下步骤：\n- 根据预设安全事件类型集合，实时采集企业侧各类安全事件信息；\n- 对原始采集信息进行解析，与安全事件关联规则匹配，判定是否可表征网络受到威胁；\n- 若判定为威胁事件，根据安全事件类型集合、网络上下文信息和安全事件信息，生成数据识别特征库。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 2,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 472,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种基于人工智能的数据安全风险监测追溯系统_黄永军_s0_c1",
    "source_id": "一种基于人工智能的数据安全风险监测追溯系统_黄永军",
    "text": "数据流转与分布监测模块包括以下步骤：\n- 采集网络安全日志和流量；\n- 分析IP设备的自身脆弱性安全事件和外部攻击安全事件；\n- 生成遭受到攻击行为的IP资产清单；\n- 确定IP资产清单的数据血缘信息、流转信息和访问信息；\n- 形成数据流转拓扑图，检测数据访问异常。\n\n数据安全事件分析模块包括以下步骤：\n- 对数据安全事件进行分类，实现风险数据的结构化；\n- 识别安全风险，构建多级风险传导预警模型；\n- 动态监测安全风险，输出风险预测值；\n- 将监测记录归并为数据安全风险监测策略库。\n\n数据安全事件溯源模块包括以下步骤：\n- 在数据流转拓扑图中查找威胁事件或遭受到攻击行为的IP资产；\n- 生成风险信息对应的告警溯源信息；\n- 对IP资产进行标识，添加目标告警标识至数据流转拓扑图。\n\n电子设备包括处理器、存储器、总线和通信接口，用于执行基于人工智能的数据安全风险监测追溯系统。\n\n此外，提供一种计算机可读存储介质，存储有执行基于人工智能的数据安全风险监测追溯系统的计算机程序。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 2,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 442,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种基于关系编码和层次注意力机制的图像段落描述方法_李睿凡_s0_c0",
    "source_id": "一种基于关系编码和层次注意力机制的图像段落描述方法_李睿凡",
    "text": "一种基于关系编码和层次注意力机制的图像段落描述方法，包括关系编码过程和层次注意力解码过程。关系编码过程输入区域特征V、区域位置B和区域类别O，通过空间关系编码器和语义关系编码器分别生成空间关系编码特征VP和语义关系编码特征Vs。层次注意力解码过程使用两个LSTM和一个层次注意力动态融合关系信息和物体区域信息，层次注意力由具有关系门和视觉门的层次注意力组成。空间关系编码器通过拼接其视觉特征和相对位置坐标嵌入表示来获取空间关系编码的特征向量。语义关系分类使用了多标签分类。\n\n利用细粒度的空间和语义关系信息，在编码过程中，语义关系编码时我们通过训练有监督的语义分类器来学习和语义关系有关的先验知识。层次注意解码模块以Top-Down注意力网络为原型。层次注意力使用带有关系门和视觉门的层次注意力来动态的融合关系信息和物体区域特征，我们设计的关系门用于在两种关系信息(空间关系信息和语义关系信息)之间切换，设计的视觉门用于决定是否嵌入使用视觉信息，采用从粗粒度区域到细粒度的空间和语义关系的策略在段落生成过程中融合视觉信息。通过在斯坦福段落描述数据集(Stanford Benchmark Dataset)上的大量实验表明，本发明的方法在本领域的多个评价指 标上显著优于现有的方法。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 540,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种基于图卷积神经网络的方面级情感分析方法及装置_冯方向_s0_c0",
    "source_id": "一种基于图卷积神经网络的方面级情感分析方法及装置_冯方向",
    "text": "本发明提供了一种基于图卷积神经网络的方面级情感分析方法，包括以下步骤：\n\n1. 获取待进行方面情感分析的句子及其中的方面词。\n2. 对句子和方面词进行预处理，得到输入向量序列和句法加权图。\n3. 将输入向量序列和句法加权图输入预先训练的双重图卷积神经网络，得到方面词对应的情感分析结果。\n\n双重图卷积神经网络包括句法图卷积子神经网络和基于自注意力机制的语义图卷积子神经网络，用于提取句子的句法相关特征和语义相关特征。预处理步骤包括使用Glove词嵌入将句子中的每个词转换为词向量，将方面词的相对距离信息和词性信息与词向量拼接，以及将词向量和词性信息输入依存句法分析器得到句法加权图。训练过程包括构建初始双重图卷积神经网络模型，计算损失函数，并更新模型参数。\n\n根据提供的指示，以下是清洗后的学术论文第2部分内容：\n\n---\n\n利用预先训练的双重图卷积神经网络中的双向长短期记忆网络BiLSTM，对待进行方面情感分析的句子对应的输入向量序列进行特征转换，得到待进行方面情感分析的句子对应的隐藏状态特征；\n\n基于待进行方面情感分析的句子对应的隐藏状态特征以及句法加权图，利用句法图卷积子神经网络，对待进行方面情感分析的句子进行句法特征提取，得到待进行方面情感分析的句子对应的第一句法图特征；",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 6,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 543,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种基于图卷积神经网络的方面级情感分析方法及装置_冯方向_s0_c1",
    "source_id": "一种基于图卷积神经网络的方面级情感分析方法及装置_冯方向",
    "text": "基于待进行方面情感分析的句子对应的隐藏状态特征，利用基于自注意力的语义图卷积子神经网络，对待进行方面情感分析的句子进行语义特征提取，得到待进行方面情感分析的句子对应的第一语义图特征；\n\n利用预先训练的双重图卷积神经网络中的双仿射模块，分别对待进行方面情感分析的句子对应的第一句法图特征和第一语义图特征进行特征转换，得到待进行方面情感分析的句子对应的第二句法图特征和第二语义图特征；\n\n分别提取待进行方面情感分析的句子对应的第二句法图特征和第二语义图特征中的方面词节点特征，并利用预先训练的双重图卷积神经网络中的池化层，对提取的方面词节点特征进行池化和拼接处理，得到第一拼接特征；\n\n利用预先训练的双重图卷积神经网络中的全连接层，对第一拼接特征进行概率分析，得到方面词对应的情感分析结果。\n\n---\n\n以上内容已去除页眉、页脚、页码、重复信息、乱码、PDF解析错误、多余符号和格式问题，保留了核心学术内容、技术术语和数据、原有逻辑结构、公式和图表说明。\n\n本发明实施例提供了一种基于图卷积神经网络的方面级情感分析方法，包括以下步骤：",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 465,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种基于图卷积神经网络的方面级情感分析方法及装置_冯方向_s0_c2",
    "source_id": "一种基于图卷积神经网络的方面级情感分析方法及装置_冯方向",
    "text": "本发明实施例提供了一种基于图卷积神经网络的方面级情感分析方法，包括以下步骤：\n\n1. 获取待进行方面情感分析的句子及其中的方面词。\n2. 对句子和方面词进行预处理，得到输入向量序列和句法加权图。\n3. 将输入向量序列和句法加权图输入预先训练的双重图卷积神经网络，得到方面词对应的情感分析结果。\n4. 双重图卷积神经网络包括句法图卷积子神经网络和基于自注意力机制的语义图卷积子神经网络，用于提取句子的句法特征和语义特征。\n5. 通过双向长短期记忆网络BiLSTM对输入向量序列进行特征转换，得到句子的隐藏状态特征。\n6. 利用句法图卷积子神经网络提取句子的句法特征，得到第一句法图特征。\n7. 利用基于自注意力机制的语义图卷积子神经网络提取句子的语义特征，得到第一语义图特征。\n8. 通过双仿射模块对第一句法图特征和第一语义图特征进行特征转换，得到第二句法图特征和第二语义图特征。\n9. 提取第二句法图特征和第二语义图特征中的方面词节点特征，进行池化和拼接处理，得到第一拼接特征。\n10. 通过全连接层对第一拼接特征进行概率分析，得到方面词对应的情感分析结果。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 479,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种基于图卷积神经网络的方面级情感分析方法及装置_冯方向_s0_c3",
    "source_id": "一种基于图卷积神经网络的方面级情感分析方法及装置_冯方向",
    "text": "利用双重图卷积神经网络中的全连接层，对从池化层得到的第一拼接特征进行概率分析，得到方面词对应的情感分析结果。具体地，将从池化层得到的第一拼接特征输入全连接层，经过softmax分类器产生一个关于方面词a情感极性的概率分布p，p(a) = softmax(Wpr+bp)，其中，Wp和bp分别表示可学习的权重和偏置，r表示第一拼接特征。\n\n本发明实施例中，使用双重图卷积神经网络，对待进行方面情感分析的句子中方面词进行情感分析，可以更加精确地识别出用户对于一个具体方面的情感态度，而不是直接在句子级粒度上判断情感极性。且，句法图卷积子神经网络可以用于提取待进行方面情感分析的句子对应的句法相关特征，基于自注意力机制的语义图卷积子神经网络可以用于提取待进行方面情感分析的句子对应的语义相关特征，使得双重图卷积神经网络不仅关注句子的句法特征，还关注句子的语义特征，进而能够使用基于自注意力机制的语义图卷积子神经网络提取句子对应的语义相关特征，以弥补对句法不敏感的句子提取句法特征不准确的缺陷，提高情感分析结果的准确性。\n\n本发明实施例还提供了一种双重图卷积神经网络的训练方法，该方法可以包括：\n\nS201，构建初始双重图卷积神经网络模型。\n\nS202，将样本句子对应的输入向量序列，输入至BiLSTM中，得到样本句子对应的隐藏状态特征。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 565,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种基于图卷积神经网络的方面级情感分析方法及装置_冯方向_s0_c4",
    "source_id": "一种基于图卷积神经网络的方面级情感分析方法及装置_冯方向",
    "text": "S201，构建初始双重图卷积神经网络模型。\n\nS202，将样本句子对应的输入向量序列，输入至BiLSTM中，得到样本句子对应的隐藏状态特征。\n\nS203，将样本句子对应的隐藏状态特征以及样本句子对应的句法加权图，输入句法图卷积子神经网络中进行句法特征提取，得到样本句子对应的第一句法图特征。\n\nS204，将样本句子对应的隐藏状态特征，输入基于自注意力机制的语义图卷积子神经网络进行语义特征提取，得到样本句子对应的第一语义图特征。\n\nS205，利用双仿射模块，分别对样本句子对应的第一句法图特征和第一语义图特征进行特征转换，得到样本句子对应的第二句法图特征和第二语义图特征。\n\nS206，分别提取样本句子对应的第二句法图特征和第二语义图特征中的方面词节点特征，将提取的方面词节点特征输入池化层，进行池化和拼接处理，得到第二拼接特征。\n\nS207，将第二拼接特征输入全连接层，得到样本句子中方面词对应的情感分析结果。\n\nS208，基于情感分析结果与标准情感分析结果的差异，计算初始双重图卷积神经网络模型对应的损失函数。\n\nS209，对损失函数进行最小化处理，确定初始双重图卷积神经网络模型中各模块的权重参数。\n\nS2010，基于初始双重图卷积神经网络模型中各模块的权重参数，对初始双重图卷积神经网络模型中的参数进行更新，训练得到双重图卷积神经网络模型。\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 579,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种基于图卷积神经网络的方面级情感分析方法及装置_冯方向_s0_c5",
    "source_id": "一种基于图卷积神经网络的方面级情感分析方法及装置_冯方向",
    "text": "S2010，基于初始双重图卷积神经网络模型中各模块的权重参数，对初始双重图卷积神经网络模型中的参数进行更新，训练得到双重图卷积神经网络模型。\n\n---\n\n[0228] 本文使用的关系术语如“第一”和“第二”等，仅用于区分不同实体或操作，并不暗示实际存在的关系或顺序。术语“包括”、“包含”及其变体意指非排他性包含，意味着一系列要素的过程、方法、物品或设备不仅包括明确列出的要素，还包括未明确列出或固有的要素。由“包括一个……”限定的要素，并不排除存在其他相同要素。\n\n[0229] 各实施例以相关方式描述，相同部分互相参照，重点说明与其他实施例的差异。特别是装置、电子设备的实施例，由于与方法实施例基本相似，故简化描述，相关内容参照方法实施例。\n\n[0230] 上述内容仅为较佳实施例，不限定本发明的保护范围。任何在本发明精神和原则内的修改、等效替换、改进等，均包含在本发明的保护范围内。\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 5,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 399,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种基于图卷积神经网络的方面级情感分析方法及装置_李睿凡_s0_c0",
    "source_id": "一种基于图卷积神经网络的方面级情感分析方法及装置_李睿凡",
    "text": "本发明提供了一种基于图卷积神经网络的方面级情感分析方法，包括以下步骤：\n\n1. 获取待进行方面情感分析的句子及其中的方面词。\n2. 对句子和方面词进行预处理，得到输入向量序列和句法加权图。\n3. 将输入向量序列和句法加权图输入预先训练的双重图卷积神经网络，得到方面词对应的情感分析结果。\n\n双重图卷积神经网络包括句法图卷积子神经网络和基于自注意力机制的语义图卷积子神经网络，用于提取句子的句法相关特征和语义相关特征。训练过程包括构建初始双重图卷积神经网络模型，输入样本句子对应的输入向量序列和句法加权图，进行句法特征提取和语义特征提取，计算损失函数并更新模型参数。\n\n此外，还提供了一种基于图卷积神经网络的方面级情感分析装置，包括获取模块、预处理模块和情感分析模块。\n\n基于图卷积神经网络的方面级情感分析方法，包括以下步骤：\n\n1. 获取待进行方面情感分析的句子及其中的方面词。\n2. 对句子和方面词进行预处理，得到输入向量序列和句法加权图。\n3. 将输入向量序列和句法加权图输入预先训练的双重图卷积神经网络，得到方面词对应的情感分析结果。\n\n双重图卷积神经网络包括句法图卷积子神经网络和基于自注意力机制的语义图卷积子神经网络，用于提取句子的句法相关特征和语义相关特征。模型训练包括构建初始模型、计算损失函数、确定权重参数、更新参数等步骤。\n\n根据提供的指示，以下是清洗后的学术论文第3部分内容：",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 4,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种基于图卷积神经网络的方面级情感分析方法及装置_李睿凡_s0_c1",
    "source_id": "一种基于图卷积神经网络的方面级情感分析方法及装置_李睿凡",
    "text": "根据提供的指示，以下是清洗后的学术论文第3部分内容：\n\n---\n\n其中，预先训练的双重图卷积神经网络可以包括：句法图卷积子神经网络和基于自注意力机制的语义图卷积子神经网络，句法图卷积子神经网络可以用于提取待进行方面情感分析的句子对应的句法相关特征，基于自注意力机制的语义图卷积子神经网络可以用于提取待进行方面情感分析的句子对应的语义相关特征。预先训练的双重图卷积神经网络是根据样本句子对应的输入向量序列，样本句子对应的句法加权图，以及样本句子中方面词对应的标准情感分析结果训练得到的，标准情感分析结果包括：正向结果、中性结果以及负向结果。\n\n示例性的，针对待进行方面情感分析的句子：The price is reasonable although the service is poor中的方面词price，可以得到该方面词对应的情感分析结果为正向结果，而针对待进行方面情感分析的句子：The price is reasonable although the service is poor中的方面词service，可以得到该方面词对应的情感分析结果为负向结果。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 4,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 481,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种基于图卷积神经网络的方面级情感分析方法及装置_李睿凡_s0_c2",
    "source_id": "一种基于图卷积神经网络的方面级情感分析方法及装置_李睿凡",
    "text": "本发明实施例提供的基于图卷积神经网络的方面级情感分析方法，因预先训练的双重图卷积神经网络包括：句法图卷积子神经网络和基于自注意力机制的语义图卷积子神经网络，而句法图卷积子神经网络用于提取待进行方面情感分析的句子对应的句法相关特征，基于自注意力机制的语义图卷积子神经网络用于提取待进行方面情感分析的句子对应的语义相关特征，使得双重图卷积神经网络不仅关注句子的句法特征，还关注句子的语义特征，进而能够使用基于自注意力机制的语义图卷积子神经网络提取句子对应的语义相关特征，以弥补对句法不敏感的句子提取句法特征不准确的缺陷，提高情感分析结果的准确性。\n\n---\n\n以上内容已去除页眉、页脚、页码、重复信息和乱码，保留了核心学术内容、技术术语和数据、原有逻辑结构、公式和图表说明。\n\n根据提供的指示，以下是清洗后的学术论文第4部分内容：\n\n---\n\nS206，提取样本句子对应的第二句法图特征和第二语义图特征中的方面词节点特征，输入池化层进行池化和拼接处理，得到第二拼接特征。\n\nS207，将第二拼接特征输入全连接层，得到样本句子中方面词对应的情感分析结果。\n\nS208，基于情感分析结果与标准情感分析结果的差异，计算初始双重图卷积神经网络模型对应的损失函数。\n\nS209，对损失函数进行最小化处理，确定初始双重图卷积神经网络模型中各模块的权重参数。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 4,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 570,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种基于图卷积神经网络的方面级情感分析方法及装置_李睿凡_s0_c3",
    "source_id": "一种基于图卷积神经网络的方面级情感分析方法及装置_李睿凡",
    "text": "S209，对损失函数进行最小化处理，确定初始双重图卷积神经网络模型中各模块的权重参数。\n\nS2010，基于初始双重图卷积神经网络模型中各模块的权重参数，对初始双重图卷积神经网络模型中的参数进行更新，训练得到双重图卷积神经网络模型。\n\n---\n\n以上内容已去除页眉、页脚、页码、重复信息、乱码、PDF解析错误、多余符号和格式问题，保留了核心学术内容、技术术语和数据、原有逻辑结构、公式和图表说明。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 4,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 197,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种基于多层次图卷积网络的电力设备故障溯源方法_李睿凡_s0_c0",
    "source_id": "一种基于多层次图卷积网络的电力设备故障溯源方法_李睿凡",
    "text": "一种基于多层次图卷积网络的电力设备故障溯源方法，包括以下步骤：\n\n步骤1：从电力工单系统中收集电力设备的故障文本信息，并计算故障文本信息中的TF-IDF指标和PMI指标；\n\n步骤2：根据TF-IDF指标和PMI指标构建电力工单图G(V,E)，V和E分别为节点集和边集；\n\n步骤3：根据电力工单图G(V,E)构建多层次图卷积网络，并训练多层次图卷积网络；\n\n步骤4：使用训练好的多层次图卷积网络对电力工单系统中电力设备的故障文本信息进行识别，确定电力设备故障位置。\n\n所述步骤2构建的电力工单图G(V,E)中的邻接矩阵A的构造规则为：\n\nA(i,j)=PMI(i,j) if PMI(i,j)>0 else 0\n\n其中，PMI(i,j)表示词语i和j的PMI值。\n\n所述步骤1中的TF-IDF指标用于提取故障文本信息中的关键词，计算方法为：TF-IDF=TF×IDF。\n\n所述步骤1中的PMI指标用于计算词语间的语义相似度，计算方法为：PMI(a,b)=log2[P(a,b)/(P(a)P(b))]。\n\n所述步骤3的具体实现方法为：首先构建一个包括词节点和文档节点的词-文档图；然后初始化特征矩阵，设置交叉熵函数作为损失函数，最后使用训练样本进行多层次图卷积网络训练。\n\n所述多层次图卷积网络为四层卷积神经网络，激活函数为ReLU激活函数。\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 2,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 575,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种基于多层次图卷积网络的电力设备故障溯源方法_李睿凡_s0_c1",
    "source_id": "一种基于多层次图卷积网络的电力设备故障溯源方法_李睿凡",
    "text": "所述多层次图卷积网络为四层卷积神经网络，激活函数为ReLU激活函数。\n\n---\n\n我们设置X=I，即单位矩阵。这表示每个单词和文档由一个独热向量作为输入。\n\nρ是激活函数，通常采用ReLU激活函数。L是拉普拉斯矩阵，我们初始化L(0)=X。我们在卷积神经网络中堆叠了四层，并使用交叉熵函数作为损失函数：\n\n其中Z为模型输出，Y为真实类别标签。Z和Y均为m维向量，m为类别数目，i为训练样本的下标。\n\n步骤4、使用训练好的多层次图卷积网络识别电力工单系统中电力设备的故障文本信息，以确定电力设备故障位置。\n\n需要强调的是，本发明所述的实施例是说明性的而非限定性的。本发明包括但不限于具体实施方式中所述的实施例，还包括本领域技术人员根据本发明的技术方案得出的其他实施方式。\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 2,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 339,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种基于对应的深层信念网络的跨模态检索方法_李睿凡_s0_c0",
    "source_id": "一种基于对应的深层信念网络的跨模态检索方法_李睿凡",
    "text": "本发明提出了一种基于对应的深层信念网络的跨模态检索方法，该方法包括以下步骤：\n\n1. 利用特征提取方法分别获得检索目标与检索库中每一个检索成员的初级向量。\n\n2. 将检索目标和检索库中每一个检索成员分别作为第一模态和第二模态，通过对应的深层信念网络Corr-DBN获得检索目标的高级向量和检索库中每一个检索成员的高级向量。\n\n3. 利用检索目标的高级向量和检索库中每一个检索成员的高级向量计算检索目标与检索库中每一个检索成员的距离。\n\n4. 将检索库中与检索目标距离最近的至少一个检索成员确定为与检索目标匹配的对象。\n\nCorr-DBN的非顶层为至少一层双受限波尔兹曼机RBM结构，顶层为对应的受限波尔兹曼机Corr-RBM结构。Corr-RBM包含具有相关性约束的第一模态Corr-RBM和第二模态Corr-RBM。Corr-DBN首先对两种模态的初级向量使用至少一层双RBM模型获得该两种模态的中级向量，在Corr-DBN模型的顶层通过Corr-RBM模型对两种模态的中级向量进行进一步处理，最终获得两种模态的高级向量。\n\n---\n\n[0069] 底层双RBM模型中，第一模态RBM的可见层神经单元接收第一模态原始数据提取的特征向量，第二模态RBM的可见层神经单元接收第二模态原始数据提取的特征向量。初级向量由原始数据特征提取获得，技术细节此处省略。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 2,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 577,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种基于对应的深层信念网络的跨模态检索方法_李睿凡_s0_c1",
    "source_id": "一种基于对应的深层信念网络的跨模态检索方法_李睿凡",
    "text": "[0070] 相邻的双RBM模型层中，上层双RBM的可见层神经单元数量与下层双RBM的隐藏层神经单元数量相同；顶层Corr-RBM的可见层神经单元数量与相邻双RBM模型中的隐藏层神经单元数量一致。\n\n[0071] 顶层Corr-RBM模型输出第一模态和第二模态的高级向量。\n\n[0073] 假设检索库包含M个成员，以下步骤以检索与文本J相关的对象为例进行说明：\n\n[0074] 步骤701：通过特征提取获得检索库成员和文本J的初级向量。\n\n[0075] 检索成员的模态类型多样，包括图像、文本或语音等，各类模态数据均已有成熟的特征提取方法。\n\n[0076] 步骤702：使用Corr-DBN模型处理初级向量，计算文本J与检索库成员的高级向量，并通过欧氏距离公式计算距离。\n\n[0077] 通过Corr-DBN模型处理，计算任意检索成员与文本J的欧氏距离。\n\n[0078] 欧氏距离计算公式适用于n维空间中的两点。\n\n[0079] 步骤703：根据欧氏距离排序，选择距离最小的前K个检索成员作为输出结果。\n\n[0080] 本实施例通过Corr-DBN模型处理，利用高级向量进行欧氏距离计算，高效获取检索结果。\n\n[0081] 本发明不仅限于所述实施例，任何符合本发明精神和原则的修改和改进均在本发明的保护范围内。\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 2,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 560,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种基于强化学习的电力设备检修决策生成方法_李睿凡_s0_c0",
    "source_id": "一种基于强化学习的电力设备检修决策生成方法_李睿凡",
    "text": "一种基于强化学习的电力设备检修决策生成方法，包括以下步骤：\n\n1. 计算第一割集，并据此计算电力设备引起电网停电损失的第一权重。\n2. 将电力设备检修决策生成问题建模为一个马尔可夫决策过程，定义电力设备的运行状态，并应用强化学习方法求解得到最优策略和价值矩阵，以最小化电网的整体运行损失为目标。\n3. 计算第二割集，并据此计算第二权重，加权到电网的整体运行损失中，改进最优策略。\n4. 利用动态权重对电网的整体运行损失进行加权，以最小化被加权动态权重的电网的整体运行损失为目标，利用价值矩阵改进最优策略，选取电网的整体运行损失最小的动作作为最终检修策略。\n\n[0042] 电网停电的经济损失随电网运行功率变化，强化学习旨在最小化电网整体运行损失。目标是最小化电网运行损失，权重加权到强化学习的奖励中。\n\n[0043] 根据电网设备连接与导通情况，计算所有可能导致停电的割集。割集指设备损坏导致电网停电的集合，不存在真子集能导致停电。\n\n[0044] 针对多电力设备问题，本发明提出割集方法计算设备重要性，应用于动态规划求解。\n\n[0045] 点割集是原图中部分点的集合，删除割集中所有点原图分为两部分。定义电网G为连通图，点集V为电力设备集合，U为V的子集，若删除U中点G-U不连通，则U为点割集。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 3,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 549,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种基于强化学习的电力设备检修决策生成方法_李睿凡_s0_c1",
    "source_id": "一种基于强化学习的电力设备检修决策生成方法_李睿凡",
    "text": "[0046] 根据电网设备连接状态，得到割集。如{A，C，D}、{B，A}、{E，C，D}、{B，E}。\n\n[0047] 设备检修导致割集变化，新割集根据其他设备检修动作生成，重新计算动作奖励，实现设备决策通信。\n\n[0048] 通过割集计算设备引起的电网运行损失权重，加权到强化学习奖励中。\n\n[0053] 基于设备权重进行强化学习动态规划求解。\n\n[0054] 定义电力设备运行状态，计算状态转移矩阵。\n\n[0057] 设备动作集合为[fix，nofix]，S1状态只能选择“nofix”，S4状态只能选择“fix”。\n\n[0058] 强化学习的整体优化目标是最小化电网整体运行损失。\n\n[0059] 使用加权的动态规划算法求解马尔可夫决策过程，策略评估中加入静态或动态权重。\n\n[0060] 应用强化学习求解马尔可夫决策过程，包括初始化价值矩阵和策略，利用贝尔曼方程更新价值矩阵。\n\n[0065] 改进最优策略，利用动态权重加权电网运行损失。\n\n[0070] 根据最优策略和价值矩阵，生成电网设备检修的动态策略。\n\n[0071] 策略评估包括直接奖励和间接奖励，加权更新到价值矩阵。\n\n[0075] 加权的动态规划求解流程包括割集管理和状态管理。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 526,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种基于强化学习的电力设备检修决策生成方法_李睿凡_s0_c2",
    "source_id": "一种基于强化学习的电力设备检修决策生成方法_李睿凡",
    "text": "[0071] 策略评估包括直接奖励和间接奖励，加权更新到价值矩阵。\n\n[0075] 加权的动态规划求解流程包括割集管理和状态管理。\n\n[0081] 本发明提出用强化学习动态规划求解多设备电力检修策略，利用割集计算设备重要性，实现设备间通信，降低应用门槛，通过仿真验证方法有效性。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 139,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种基于深层模型的跨模态检索方法_李睿凡_s0_c0",
    "source_id": "一种基于深层模型的跨模态检索方法_李睿凡",
    "text": "本发明提出了一种基于深层模型的跨模态检索方法，该方法包括以下步骤：\n\n1. 利用特征提取方法分别获得目标检索模态与检索库中每一个被检索模态的低级表达向量。\n\n2. 目标检索模态的低级表达向量分别与检索库中每一个被检索模态的低级表达向量，通过堆叠对应的受限波尔兹曼机Corr-RBMs深层模型获得目标检索模态的高级表达向量和检索库中每一个被检索模态的高级表达向量。\n\n3. 利用目标检索模态的高级表达向量和检索库中每一个被检索模态的高级表达向量计算目标检索模态与检索库中每一个被检索模态的距离。\n\n4. 将检索库中与目标检索模态距离最近的至少一个被检索模态确定为与目标检索模态匹配的对象。\n\nCorr-RBMs深层模型由至少两层对应的受限波尔兹曼机Corr-RBM模型堆叠而成，Corr-RBMs深层模型包括第一模态Corr-RBMs和第二模态Corr-RBMs，第一模态Corr-RBMs处理目标检索模态低级表达，第二模态Corr-RBMs处理检索库中任一被检索模态的低级表达。\n\n---\n\n[0071] 在本步骤中，检索库中的模态种类没有限定，可以是图像、文本或语音模态。不同模态的原始数据可以通过成熟的特征提取方法进行处理，例如图像模态可以使用MPEG-7和Gist描述符进行特征提取，文本模态可以使用词袋模型。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 2,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 558,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种基于深层模型的跨模态检索方法_李睿凡_s0_c1",
    "source_id": "一种基于深层模型的跨模态检索方法_李睿凡",
    "text": "[0072] 步骤602：图片P的低级表达与检索库中每个模态的低级表达通过Corr-RBMs深层模型处理，得到图片P和每个模态的高级表达。接着，使用欧氏距离计算公式，计算图片P与检索库中每个模态的高级表达之间的欧氏距离。\n\n[0073] 在此步骤中，将检索库中的任一被检索模态与图片P组合，通过Corr-RBMs深层模型处理，得到组合中模态的高级表达和图片P的高级表达，然后计算图片P与该被检索模态的欧氏距离。\n\n[0074] 在n维欧式空间中，两点t和y之间的距离d计算公式为：\n\\[ d(t, y) = \\sqrt{\\sum_{i=1}^{n} (t_i - y_i)^2} \\]\n使用该公式计算图片P和任一被检索模态之间的欧氏距离。\n\n[0075] 步骤603：根据图片P与检索库中每个模态的欧氏距离进行排序，选择距离最小的前K个模态作为检索结果输出。\n\n[0076] 本实施例通过Corr-RBMs深层模型处理图片模态和检索库中各模态的低级表达，得到高级表达，并使用高级表达进行欧氏距离计算，以高效获得检索结果。\n\n[0077] 以上描述仅为发明的优选实施例，不应限制本发明的范围。任何在本发明精神和原则内的修改、等效替换和改进，均应包含在本发明的保护范围内。\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 2,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 539,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种基于自动编码器的视频分类方法及装置_李睿凡_s0_c0",
    "source_id": "一种基于自动编码器的视频分类方法及装置_李睿凡",
    "text": "本发明提供了一种基于自动编码器的视频分类方法及装置。该方法首先获取目标视频的图像、音频和文本三种模态数据的低级表示内容，然后将每种模态数据的低级表示内容输入堆叠自动编码器组，以获得高级表示内容。接着，将两种模态数据的高级表示内容组合后输入双模态融合器，获得双模态公共表示内容。将三种模态数据的双模态公共表示内容组合后输入三模态融合器，获得三模态公共表示内容。最后，将三模态公共表示内容输入有监督分类模型，获得视频类别的类别标签，从而实现视频分类。该方法通过结合视频的三种模态数据，提高了视频分类的准确性。\n\n本发明提供了一种基于自动编码器的视频分类方法，包括以下步骤：\n\n1. 获得目标视频的图像、音频和文本模态数据的低级表示内容，如色彩直方图、纹理特征、边缘特征、MFCC特征和TF-IDF特征。\n\n2. 将每种模态数据的低级表示内容输入堆叠自动编码器组，获得高级表示内容。\n\n3. 将两种模态数据的高级表示内容组合后输入双模态融合器，获得双模态公共表示内容。\n\n4. 将三种双模态公共表示内容组合后输入三模态融合器，获得三模态公共表示内容。\n\n5. 将三模态公共表示内容输入有监督分类模型，获得视频类别标签。\n\n6. 根据类别标签确定视频类别。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 3,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 524,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种基于自动编码器的视频分类方法及装置_李睿凡_s0_c1",
    "source_id": "一种基于自动编码器的视频分类方法及装置_李睿凡",
    "text": "5. 将三模态公共表示内容输入有监督分类模型，获得视频类别标签。\n\n6. 根据类别标签确定视频类别。\n\n此外，还介绍了有监督分类模型的构建过程，包括获取样本视频的模态数据低级表示内容，输入堆叠自动编码器组获得高级表示内容，组合两种模态数据的高级表示内容获得双模态公共表示内容，组合三种双模态公共表示内容获得三模态公共表示内容，并基于样本视频的三模态公共表示内容和类别标签训练有监督分类模型。\n\n---\n\n[0113] 双模态融合器采用自动编码器设计，其输出内容为自动编码器隐藏层的输出。经过双模态融合器处理，每个样本视频生成三个双模态公共表示内容。\n\n[0114] 样本视频包含三种模态数据，每种模态数据具有独立的低级和高级表示内容。因此，任意两种模态数据的高级表示组合形成三种可能的组合结果，如图4所示。双模态融合器的数据处理方式参见图5，其中，组合方式不做限定。\n\n[0115] 将N个样本视频的双模态公共表示内容的组合结果输入至三模态融合器，以获得每一样本视频的三模态公共表示内容。\n\n[0116] 三模态融合器同样为自动编码器，其输出为隐藏层的输出内容。处理后的每个样本视频将得到一个三模态公共表示内容。\n\n[0117] 三模态融合器的数据处理方式参见图6，图中的1、2、3分别表示双模态公共表示内容的一维。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 558,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种基于自动编码器的视频分类方法及装置_李睿凡_s0_c2",
    "source_id": "一种基于自动编码器的视频分类方法及装置_李睿凡",
    "text": "[0117] 三模态融合器的数据处理方式参见图6，图中的1、2、3分别表示双模态公共表示内容的一维。\n\n[0118] 利用有监督学习方式，基于N个样本视频的三模态公共表示内容和相应的视频类别标签，训练得到有监督分类模型。\n\n[0123] 视频分类装置包括：低级表示内容获得模块、高级表示内容获得模块、双模态公共表示内容获得模块、三模态公共表示内容获得模块、类别标签获得模块和视频类别确定模块。\n\n[0129] 本发明实施例通过结合视频的三种模态数据进行分类，确保分类准确性。\n\n[0130] 有监督分类模型通过分类模型构建模块构建，包括低级表示内容获得单元、高级表示内容获得单元、双模态公共表示内容获得单元、三模态公共表示内容获得单元和模型训练单元。\n\n[0136] 目标视频的图像模态数据的低级表示内容包括色彩直方图、纹理特征和边缘特征中至少一种；音频模态数据的低级表示内容包括MFCC特征；文本模态数据的低级表示内容包括TF-IDF特征。\n\n[0139] 有监督学习方式包括基于Softmax分类器的学习方式。\n\n---\n\n**注意：由于指示要求只清洗当前片段的内容，因此页码、专利号、图号等已被去除。**",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 505,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种多特征多通道图卷积网络模型训练方法及属性情感三元组抽取方法_李睿凡_s0_c0",
    "source_id": "一种多特征多通道图卷积网络模型训练方法及属性情感三元组抽取方法_李睿凡",
    "text": "本发明提供了一种多特征多通道图卷积网络模型训练方法及属性情感三元组抽取方法。该方法包括以下步骤：\n\n1. 将第一语句输入预设的第一模型中，将第一语句划分为多个单词，对每个单词进行编码得到第一词向量，组合多个第一词向量得到隐藏状态序列，基于双仿射注意力机制生成对应隐藏状态序列的第一邻接张量。\n\n2. 根据第一语句中单词的词性生成第二邻接张量，根据单词之间的句法依存类型生成第三邻接张量，根据基于树的词对距离生成第四邻接张量，根据单词的相对距离生成第五邻接张量。\n\n3. 将隐藏状态序列分别与邻接张量进行图卷积，并平均池化，得到联合特征序列；将邻接张量进行拼接得到联合张量。\n\n4. 根据联合张量和联合特征序列为每个词对生成第一词对向量，基于分类函数得到概率分布张量。\n\n5. 计算总损失函数，根据总损失函数对第一模型进行训练。\n\n6. 将第二语句输入训练好的第一模型，得到概率分布张量，对概率分布张量进行三元组解码，得到第二语句中的属性情感三元组。\n\n该方法通过多特征多通道图卷积网络模型训练，可以学习到关系意识的节点表示，提高属性情感三元组抽取的准确度。\n\n根据文档内容，以下是清洗后的片段：\n\n---\n\n分别提取第一邻接张量、第二邻接张量、第三邻接张量、第四邻接张量和第五邻接张量中每个通道的通道切片；\n\n根据图卷积分别求出每个通道切片对应的切片特征序列；",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 6,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 580,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种多特征多通道图卷积网络模型训练方法及属性情感三元组抽取方法_李睿凡_s0_c1",
    "source_id": "一种多特征多通道图卷积网络模型训练方法及属性情感三元组抽取方法_李睿凡",
    "text": "分别提取第一邻接张量、第二邻接张量、第三邻接张量、第四邻接张量和第五邻接张量中每个通道的通道切片；\n\n根据图卷积分别求出每个通道切片对应的切片特征序列；\n\n收集第一邻接张量每个通道切片对应的切片特征序列，对第一邻接张量的所有切片特征序列进行平均池化，得到第一特征序列；\n\n收集第二邻接张量每个通道切片对应的切片特征序列，对第二邻接张量的所有切片特征序列进行平均池化，得到第二特征序列；\n\n收集第三邻接张量每个通道切片对应的切片特征序列，对第三邻接张量的所有切片特征序列进行平均池化，得到第三特征序列；\n\n收集第四邻接张量每个通道切片对应的切片特征序列，对第四邻接张量的所有切片特征序列进行平均池化，得到第四特征序列；\n\n收集第五邻接张量每个通道切片对应的切片特征序列，对第五邻接张量的所有切片特征序列进行平均池化，得到第五特征序列。\n\n---\n\n以上内容已去除页眉、页脚、页码、重复信息、乱码、PDF解析错误、多余符号和格式问题，保留了核心学术内容。\n\n根据文档内容，以下是清洗后的片段：\n\n在本发明的一些实施方式中，词组向量的维度数与第一邻接张量的通道数相等，词组向量在第一个维度的值处于第一邻接张量的第一个通道上，第一语句的所有词组向量在第一个维度的值构成第一邻接张量的第一个通道。\n\n利用多层感知机求得对应每个第一词向量的第二词向量，公式为：",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 574,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种多特征多通道图卷积网络模型训练方法及属性情感三元组抽取方法_李睿凡_s0_c2",
    "source_id": "一种多特征多通道图卷积网络模型训练方法及属性情感三元组抽取方法_李睿凡",
    "text": "利用多层感知机求得对应每个第一词向量的第二词向量，公式为：\n\nhi表示第i个单词对应的第一词向量，hj表示第j个单词对应的第一词向量，表示第i个单词对应的第二词向量，表示第j个单词对应的第二词向量，MLPa表示对第i个单词进行多层感知机处理，MLPo表示对第j个单词进行多层感知机处理。\n\n对词对中分别对应两个单词的两个第二词向量进行注意力计算，得到对应词对的词组向量中每个维度的值，公式为：\n\ngi,j表示由第i个单词和j个单词所组成词对的注意力计算的结果向量，表示第i个单词对应的第二词向量的转置，表示第j个单词对应的第二词向量，表示第i个单词对应的第二词向量，U1和U2均表示注意力计算的权重参数，b1表示注意力计算的偏置。\n\n根据第一语句中单词的词性生成第二邻接张量，基于每个单词的词性对词对的词性进行标记，对标记相同的词对生成相同的词性向量，组合所有词对的词性向量得到第二邻接张量。\n\n根据第一语句中单词之间的句法依存类型生成第三邻接张量，基于每个词对中两个单词在第一语句中的句法依存类型对词对进行标记，对标记相同的词对生成相同的句法向量，组合所有词对的句法向量得到第三邻接张量。\n\n根据第一语句中单词之间基于树的词对距离生成第四邻接张量，基于每个词对中两个单词基于树的词对距离对词对进行标记，对标记距离相同的词对生成相同的距离向量，组合所有词对的距离向量得到第四邻接张量。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 592,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种多特征多通道图卷积网络模型训练方法及属性情感三元组抽取方法_李睿凡_s0_c3",
    "source_id": "一种多特征多通道图卷积网络模型训练方法及属性情感三元组抽取方法_李睿凡",
    "text": "根据第一语句中单词的相对距离生成第五邻接张量，基于每个词对中两个单词在第一语句中的相对距离对词对进行标记，对标记相对距离相同的词对生成相同的相对距离向量，组合所有词对的相对距离向量得到第五邻接张量。\n\n受卷积神经网络(CNN)的启发，图卷积神经网络(GCN)是一种可以直接基于图结构进行卷积操作的CNN变体。本申请为了建模词之间的不同关系，在原始GCN基础之上利用了双仿射构造了多通道的多个邻接张量，邻接张量中的每个通道都刻画了词与词之间的某一种关系，提高通过第一模型提取属性情感三元组的准确度。\n\n将隐藏状态序列分别与第一邻接张量、第二邻接张量、第三邻接张量、第四邻接张量和第五邻接张量进行图卷积，生成第一特征序列、第二特征序列、第三特征序列、第四特征序列和第五特征序列。\n\n对第一特征序列、第二特征序列、第三特征序列、第四特征序列和第五特征序列进行平均池化，得到联合特征序列。\n\n将第一邻接张量、第二邻接张量、第三邻接张量、第四邻接张量和第五邻接张量进行拼接得到联合张量。\n\n基于分类函数对多个第一词对向量进行处理，得到概率分布张量。\n\n采用上述方案，通过计算损失函数不断修正第一模型，提高第一模型准确度。\n\n---\n\n[0197] 对第二语句中的属性词、观点词进行抽取，并进行属性情感分类；",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 548,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种多特征多通道图卷积网络模型训练方法及属性情感三元组抽取方法_李睿凡_s0_c4",
    "source_id": "一种多特征多通道图卷积网络模型训练方法及属性情感三元组抽取方法_李睿凡",
    "text": "采用上述方案，通过计算损失函数不断修正第一模型，提高第一模型准确度。\n\n---\n\n[0197] 对第二语句中的属性词、观点词进行抽取，并进行属性情感分类；\n\n[0198] 情感分类分为正向（POS）、中性（NEU）和负向（NEG）。若属性词w1与观点词w2配对，且w1的情感分类为positive，则关系类型为POS，输出属性情感三元组为(w1，w2，positive)。同理，若情感分类为neutral或negative，对应输出关系类型为NEU或NEG的属性情感三元组。\n\n[0199] 按照以上方法，输出第二语句中所有属性情感三元组。\n\n[0200] 本发明实施例中，属性情感三元组的集合表示为{(a', o', s')δ}，其中δ为三元组标号，例如第二语句共有三个三元组{(a', o', s')1, (a', o', s')2, (a', o', s')3}。\n\n[0201] 以语句“The gourmet food is delicious but the service is poor”为例，属性词（ATE）为gourmet food和service，观点词为delicious和poor，对应的属性情感分类分别为positive和negative，输出三元组为(gourmet food，delicious，positive)和(service，poor，negative)。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种多特征多通道图卷积网络模型训练方法及属性情感三元组抽取方法_李睿凡_s0_c5",
    "source_id": "一种多特征多通道图卷积网络模型训练方法及属性情感三元组抽取方法_李睿凡",
    "text": "[0202] “gourmet”和“food”共同构成属性词“gourmet food”，且“food”作为“delicious”的观点目标，具有正向情感极性。为有效抽取属性词，使“gourmet”与“food”信息互享，并传递观点词“delicious”的情感极性。\n\n[0203] 属性词和观点词的配对由名词和形容词组成，不同词对具有不同的依存类型，有助于抽取和匹配预测。\n\n[0204] 本申请提供了一种有效的细化策略，利用属性词和观点词抽取的隐式结果来判断词对是否匹配。\n\n[0205] 提出了一种多特征多通道图卷积网络（MMGCN）用于ASTE任务。通过多通道图结构和图卷积操作，结合词法和句法信息，增强模型性能，并设计细化策略抽取属性情感三元组。\n\n[0206] 本发明的实施可采取硬件、软件或二者结合的方式，具体取决于应用和设计约束。\n\n[0207] 本发明不仅限于特定配置和处理，技术领域的技术人员可进行各种改变、修改和添加。\n\n[0208] 本发明的特征在一个或多个实施方式中可相同或类似使用，与其他实施方式的特征结合或替换。\n\n[0209] 本发明的保护范围包括在其精神和原则内的任何修改、等同替换和改进。\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 5,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 518,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种方面级情感分析方法、装置、电子设备及存储介质_李睿凡_s0_c0",
    "source_id": "一种方面级情感分析方法、装置、电子设备及存储介质_李睿凡",
    "text": "本发明涉及一种方面级情感分析方法、装置、电子设备及存储介质。该方法包括获取待进行方面级情感分析的目标句子及其中的方面词，基于相似结构数据集进行聚合特征处理，利用双向长短期记忆网络进行特征转换，对目标句子及方面词进行预处理得到依赖关系图和位置编码特征，并将这些特征输入图卷积神经网络得到方面词对应的情感分析结果。该方法可提高情感分析结果的准确性。\n\n根据提供的指示，以下是清洗后的学术论文第2部分内容：\n\n---\n\n聚合特征模块，用于基于相似结构数据集，对目标句子以及方面词进行聚合特征处理，得到目标句子对应的融合特征信息；其中，相似结构数据集是预先对第一预设数据集中各句子进行相似度计算得到的，包含具有不同相似结构的数据子集；\n\n特征转换模块，用于利用双向长短期记忆网络，对融合特征信息进行特征转换，得到目标句子对应的隐藏状态特征；\n\n位置编码模块，用于对目标句子以及方面词进行预处理，得到目标句子对应的依赖关系图，以及该目标句子中各个词对应的位置编码特征；其中，依赖关系图用于表征目标句子中各个词之间的依赖关系，位置编码特征是根据目标句子中各个词对应的相对位置编码特征和绝对位置编码特征确定的；",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 4,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 498,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种方面级情感分析方法、装置、电子设备及存储介质_李睿凡_s0_c1",
    "source_id": "一种方面级情感分析方法、装置、电子设备及存储介质_李睿凡",
    "text": "情感分析模块，用于将隐藏状态特征、依赖关系图以及位置编码特征，输入预先训练好的图卷积神经网络中，得到方面词对应的情感分析结果；其中，预先训练好的图卷积神经网络是根据样本句子对应的隐藏状态特征，样本句子对应的依赖关系图，样本句子中各个词对应的位置编码特征，以及样本句子中方面词对应的真值情感分析结果训练得到的，真值情感分析结果包括：正向结果、中性结果以及负向结果。\n\n---\n\n以上内容已去除页眉、页脚、页码、重复信息、乱码、PDF解析错误、多余符号和格式问题，保留了核心学术内容、技术术语和数据、原有逻辑结构、公式和图表说明。\n\n清洗后的内容如下：\n\n将所获取或确定的数据子集中各句子的信息与目标句子及方面词进行聚合特征处理，得到目标句子对应的融合特征信息。\n\n具体地，将数据子集中各句子作为邻居节点，目标句子表示为节点v，将邻居节点的信息与目标句子及方面词信息进行融合。采用聚合函数实现聚合相邻k层邻居节点信息的过程。\n\n从相似结构数据集中选取与目标句子具有相似结构的数据子集，利用所选取数据子集中的句子信息，对目标句子及方面词进行聚合特征处理，得到目标句子对应的融合特征信息。\n\n对目标句子及方面词进行预处理，得到目标句子对应的依赖关系图，以及目标句子中各个词对应的位置编码特征。基于隐藏状态特征、依赖关系图及位置编码特征，利用预先训练好的图卷积神经网络，得到方面词对应的情感分析结果。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 4,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 595,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种方面级情感分析方法、装置、电子设备及存储介质_李睿凡_s0_c2",
    "source_id": "一种方面级情感分析方法、装置、电子设备及存储介质_李睿凡",
    "text": "提供了一种图卷积神经网络的训练方法，包括构建初始图卷积神经网络模型，将样本句子对应的特征信息输入模型进行训练，基于样本句子中方面词对应的情感分析结果与真值情感分析结果的差异，计算损失函数的参数值，对损失函数进行最小化处理，确定模型中各模块的权重参数，训练得到图卷积神经网络模型。\n\n还提供了一种方面级情感分析装置，包括数据获取模块、聚合特征模块、特征转换模块、位置编码模块和情感分析模块。\n\n---\n\n[0203] 本发明利用双向长短期记忆网络进行特征转换，获取目标句子的隐藏状态特征。\n\n[0204] 目标句子及方面词预处理后，构建依赖关系图和位置编码特征，其中位置编码特征由相对位置编码特征和绝对位置编码特征确定。\n\n[0205] 将隐藏状态特征、依赖关系图和位置编码特征输入预先训练的图卷积神经网络，得到方面词的情感分析结果，该网络基于样本句子的对应特征和真值情感分析结果训练。\n\n[0206] 提供的电子设备基于相似结构数据集，聚合具有相似结构特征信息的句子和目标句子及方面词的特征，提高情感分析的准确性。\n\n[0207] 通信总线可以是PCI或EISA总线，分为地址总线、数据总线、控制总线。\n\n[0208] 通信接口用于设备间通信。\n\n[0209] 存储器包括RAM和NVM。\n\n[0210] 处理器可以是CPU、NP、DSP、ASIC、FPGA或其他可编程逻辑器件。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 4,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 590,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "一种方面级情感分析方法、装置、电子设备及存储介质_李睿凡_s0_c3",
    "source_id": "一种方面级情感分析方法、装置、电子设备及存储介质_李睿凡",
    "text": "[0209] 存储器包括RAM和NVM。\n\n[0210] 处理器可以是CPU、NP、DSP、ASIC、FPGA或其他可编程逻辑器件。\n\n[0211] 提供计算机可读存储介质，存储有实现方面级情感分析方法的计算机程序。\n\n[0212] 还提供包含执行方面级情感分析方法步骤的计算机程序产品。\n\n[0213] 实施例可通过软件、硬件、固件或其组合实现，计算机指令可存储在计算机可读存储介质中。\n\n[0214] 关系术语如第一和第二等用于区分实体或操作，而不暗示实际关系。\n\n[0215] 各实施例采用相关方式描述，相同部分互相参见，重点说明不同之处。\n\n[0216] 本发明保护范围不限于所述实施例，任何符合精神和原则的修改、替换、改进均在保护范围内。\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 4,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 329,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "人脸重定向模型、模型训练方法及装置_张航_s0_c0",
    "source_id": "人脸重定向模型、模型训练方法及装置_张航",
    "text": "人脸重定向模型、模型训练方法及装置\n\n本公开提供了一种人脸重定向模型、模型训练方法及装置，以提高视线估计算法的样本集质量。\n\n人脸重定向模型包括编码器、无关属性分解模块、交叉属性分离模块和解码器。编码器用于对人脸图像进行特征提取，得到人脸特征。无关属性分解模块用于对人脸特征进行特征提取，得到对应于人脸特征的细节特征，包括身份特征、视线特征和头部特征。交叉属性分离模块用于对细节特征中的目标特征进行角度旋转，目标特征为视线特征或头部特征。解码器用于基于角度旋转后的细节特征生成新的人脸图像。\n\n人脸重定向模型训练方法包括将第一人脸图像和第二人脸图像依次输入至编码器、无关属性分解模块、交叉属性分离模块中，得到第一细节特征、第二细节特征、角度旋转后的第一目标特征和角度旋转后的第二目标特征。将角度旋转后的第一目标特征交换至第二细节特征中，得到特征交换后的第一细节特征；将角度旋转后的第二目标特征交换至第一细节特征中，得到特征交换后的第二细节特征。将特征交换后的第一细节特征和特征交换后的第二细节特征输入至解码器中，生成对应于第一人脸图像的第三人脸图像和对应于第二人脸图像的第四人脸图像。基于第一人脸图像与第三人脸图像的相似度、以及第二人脸图像和第四人脸图像的相似度，对人脸重定向模型中各个模块的参数进行更新，以实现人脸重定向模型的训练。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 3,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 567,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "人脸重定向模型、模型训练方法及装置_张航_s0_c1",
    "source_id": "人脸重定向模型、模型训练方法及装置_张航",
    "text": "人脸重定向模型训练装置包括特征提取模块、特征交换模块、图像生成模块和参数更新模块。特征提取模块用于提取第一人脸图像和第二人脸图像的细节特征。特征交换模块用于交换角度旋转后的目标特征。图像生成模块用于生成第三人脸图像和第四人脸图像。参数更新模块用于更新人脸重定向模型中各个模块的参数。\n\n[0035] S102：交换角度旋转后的第一目标特征至第二细节特征，得到交换后的第一细节特征；相应地，交换角度旋转后的第二目标特征至第一细节特征，得到交换后的第二细节特征。\n\n[0036] 在本实施例中，第一人脸图像Xa的视线特征经角度旋转后形成第一目标特征，第二人脸图像Xb的视线特征经角度旋转后形成第二目标特征。将第一目标特征交换至第二人脸图像Xb的细节特征中，模拟其视线特征；反之亦然。\n\n[0037] S103：将交换后的细节特征输入解码器，生成与第一人脸图像对应的第三人脸图像和与第二人脸图像对应的第四人脸图像。\n\n[0038] 在本实施例中，通过输入第一人脸图像Xa的身份特征、第二目标特征和头部特征至解码器，生成第三人脸图像；同理，输入第二人脸图像Xb的相应特征至解码器，生成第四人脸图像。\n\n[0039] 本实施例通过交换视线特征和/或头部特征，解耦两者并保持其他特征不变，从而生成新的人脸图像，用于数据增广，提高样本集质量。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 564,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "人脸重定向模型、模型训练方法及装置_张航_s0_c2",
    "source_id": "人脸重定向模型、模型训练方法及装置_张航",
    "text": "[0039] 本实施例通过交换视线特征和/或头部特征，解耦两者并保持其他特征不变，从而生成新的人脸图像，用于数据增广，提高样本集质量。\n\n[0042] 在特征交换步骤之前，可交换两细节特征中的身份特征，促进交互式学习。\n\n[0044] 在无关属性分解模块训练中加入正交损失函数，以使身份特征、视线特征和头部特征在隐空间中相互正交。\n\n[0046] 人脸重定向模型训练方法包括计算对抗损失、重建损失和标签损失，通过加权求和得到总损失函数L。\n\n[0056] 提供了人脸重定向模型训练装置的结构框图，包括特征提取模块、特征交换模块、图像生成模块和参数更新模块。\n\n[0062] 参数更新模块包括正交损失函数计算，用于优化训练。\n\n[0063] 描述了执行本公开实施例的电子设备，包括处理器、输入设备、输出设备和存储器。\n\n[0065] 输入设备可以包括触控板、指纹传感器等，输出设备可以包括显示器、扬声器等。\n\n[0067] 本公开实施例可执行于电子设备，提供了一种计算机可读存储介质，存储有执行本公开实施例的程序指令。\n\n[0070] 本领域技术人员应理解，所描述的功能可以通过硬件或软件方式实现，取决于具体应用和设计约束。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 511,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "图像生成模型的训练方法和设备以及图像生成方法_杨博_s0_c0",
    "source_id": "图像生成模型的训练方法和设备以及图像生成方法_杨博",
    "text": "本发明涉及一种图像生成模型的训练方法和设备以及图像生成方法。该方法包括获取样本数据，基于图像描述文本进行句子级别和词级别编码，得到句子向量和词向量。将初始高斯噪声输入图像生成模型的无条件对抗子网络生成第一图像，计算非条件对抗损失函数值。将第一图像和句子向量输入句子级别对抗子网络生成第二图像，计算句子级别的条件对抗损失函数值。将第二图像和词向量输入词级别对抗子网络生成第三图像，计算词级别的条件对抗损失函数值。利用损失函数值更新图像生成模型参数。该方法可保证生成图像与文本的一致性。\n\n---\n\n[0058] 本研究的第二步是获取样本数据，包括标准图像和对应的图像描述文本。\n\n[0059] 图像描述文本用于描述标准图像，包含多种表述形式的描述语句。\n\n[0060] 基于图像描述文本，进行句子级别和词级别的编码，得到相应的句子向量和词向量。\n\n[0061] 采用预先训练的文本编码器（如Transformer）实现编码。\n\n[0062] 使用初始高斯噪声作为输入，通过图像生成模型的无条件对抗子网络生成第一图像，并计算非条件对抗损失函数值。\n\n[0063] 初始阶段使用随机高斯噪声以尽可能提升生成图像质量。\n\n[0064] 第一判别器仅用于判断输入图像的真实性，无需考虑文本描述。\n\n[0065] 高斯噪声经过重构和第一图像生成器处理，得到第一图像。\n\n[0066] 重构方法为已知技术。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 3,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 596,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "图像生成模型的训练方法和设备以及图像生成方法_杨博_s0_c1",
    "source_id": "图像生成模型的训练方法和设备以及图像生成方法_杨博",
    "text": "[0065] 高斯噪声经过重构和第一图像生成器处理，得到第一图像。\n\n[0066] 重构方法为已知技术。\n\n[0067] 第一图像通过三层上采样获得。\n\n[0068] 非条件对抗损失函数值包括第一判别器和第一图像生成器的损失函数值。\n\n[0069] 通过第一图像和标准图像，利用第一判别器计算非条件对抗损失函数值。\n\n[0073] 将第一图像和第一句子的句子向量输入句子级别对抗子网络，生成第二图像，并计算句子级别的条件对抗损失函数值。\n\n[0076] 句子向量与第一图像的图像特征融合。\n\n[0077] 可以采用注意力机制或动态内存方法。\n\n[0078] 第二图像生成器处理融合后的特征，得到第二图像。\n\n[0079] 使用残差层和上采样层生成更高分辨率的第二图像。\n\n[0080] 条件对抗损失函数值包括第二判别器和第二图像生成器的损失函数值。\n\n[0082] 利用图像编码器抽取第二图像特征，进行图文一致性判断。\n\n[0083] 采用交叉熵损失计算方法。\n\n[0084] 计算第二判别器和第二图像生成器的损失函数值。\n\n[0085] 在第三图像生成阶段，引入词向量进行图像优化。\n\n[0086] 第三判别器用于评判图像与句子、词信息匹配程度。\n\n[0087] 词向量与第二图像特征融合，生成第三图像。\n\n[0088] 使用图像区域特征和词向量进行图文一致性判断。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 584,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "图像生成模型的训练方法和设备以及图像生成方法_杨博_s0_c2",
    "source_id": "图像生成模型的训练方法和设备以及图像生成方法_杨博",
    "text": "[0087] 词向量与第二图像特征融合，生成第三图像。\n\n[0088] 使用图像区域特征和词向量进行图文一致性判断。\n\n[0089] 计算第四图文一致性损失函数值。\n\n[0090] 加权计算损失函数值，以增强图文一致性。\n\n[0091] 更新图像生成模型的参数。\n\n[0112] 提供图像生成模型的训练设备，包括处理器和存储器。\n\n[0113] 训练设备执行图像生成模型的训练方法。\n\n[0114] 参数更新的具体方法为已知技术。\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 220,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "基于多层神经网络的电力实体识别方法、存储介质和设备_刘子全_s0_c0",
    "source_id": "基于多层神经网络的电力实体识别方法、存储介质和设备_刘子全",
    "text": "本发明公开了一种基于多层神经网络的电力实体识别方法、存储介质和设备。该方法包括将待识别的电力语料输入至预先构建的BERT电力实体识别模型中，得到电力实体标签的哈夫曼编码，通过哈夫曼编码映射得到实体标签，进而得到识别出的实体。通过语言模型训练语料对BERT语言模型进行预训练；对电力语料数据标注电力实体标签，构建电力实体识别语料；根据电力实体标签在电力实体识别语料中的数量构建电力实体标签的哈夫曼编码；在预训练得到的BERT语言模型后增加分类层构成BERT电力实体识别模型，通过电力实体识别语料对BERT电力实体识别模型进行再次训练，得到训练好的BERT电力实体识别模型。提高了电力领域中文命名实体识别的精度。\n\n---\n\n[0068] 采用BMEO标记形式构建伪标注语料，以减少人力成本。字符单元若是实体词开始则标记为B‑实体类别，结束则标记为E‑实体类别，非开始非结束字符标记为M‑实体类别，非实体词字符标注为O。\n\n[0069] 步骤4：根据电力实体标签数量构建哈夫曼编码；\n\n[0070] 采用哈夫曼树编码表示实体标签类型，包括“B‑0”到“B‑8”，“M‑0”到“M‑8”，“E‑0”到“E‑8”和“O”，共28个实体标签。根据这些实体标签在电力实体识别语料中的数量构建哈夫曼树。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 2,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 544,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "基于多层神经网络的电力实体识别方法、存储介质和设备_刘子全_s0_c1",
    "source_id": "基于多层神经网络的电力实体识别方法、存储介质和设备_刘子全",
    "text": "[0071] 实体标签哈夫曼编码可缓解类别不平衡问题，通过模型预测标签的哈夫曼编码路径，使用前向最大匹配进行标签映射。\n\n[0072] 步骤5：在预训练BERT语言模型后增加分类层构成BERT电力实体识别模型，通过电力实体识别语料进行再次训练；\n\n[0073] BERT电力实体识别模型输入文本经过嵌入层、多层编码器，分类层包括全连接层和Sigmoid激活函数，输出为预测的电力实体标签的哈夫曼编码。\n\n[0074] 步骤6：待识别语料输入BERT电力实体识别模型，通过哈夫曼编码映射得到实体标签，识别出预先定义的9个类别的实体。\n\n[0075] 待识别语料逐句输入模型，通过哈夫曼编码和实体标签映射关系，识别实体。\n\n[0076] 实施例2：计算机可读存储介质包含程序，用于执行基于多层神经网络的电力实体识别方法。\n\n[0079] 计算设备包括处理器、存储器及程序，用于执行基于多层神经网络的电力实体识别方法。\n\n[0084] 本发明可做出若干改进和变形，这些改进和变形也应视为保护范围。\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 2,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 450,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "基于掩码上下文机器阅读理解的方面情感三元组抽取方法_李睿凡_s0_c0",
    "source_id": "基于掩码上下文机器阅读理解的方面情感三元组抽取方法_李睿凡",
    "text": "本发明公开了一种基于掩码上下文机器阅读理解的方面情感三元组抽取方法。在方面词推理阶段，使用BERT作为句子的编码器，输入一个固定的查询q和一个原始句子作为上下文，经过模型得到方面词a以及方面词存在标识e。若标识结果为True，则将得到的方面词a加入到方面词集合A中，将上下文把集合A中所有方面词掩码作为掩码上下文，与查询q再次输入至模型中得到方面词以及方面词存在标识e，重复此流程，直到标识结果为False，得到方面词集合A。在方面词附属推理阶段，对于方面词集合A中的每个方面词a，在上下文中直接掩码掉除了方面词a以外所有无关的方面词，根据查询q以及掩码所有无关方面词的上下文得到方面词a对应的意见词O集合以及情感s，最后输出句子存在的所有的方面情感三元组(a,o,s)。该方法有效解决了以往MRC方法面临的方面词干扰问题。\n\n本发明提出了一种基于掩码上下文机器阅读理解(COM-MRC)的方面情感三元组抽取方法，以解决方面情感分析任务中包含多个方面词句子的干扰问题。COM-MRC框架包括三个要素：方面词推理算法、上下文数据增强方法，以及适应这些推理算法的有效模型架构。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 2,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 486,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "基于掩码上下文机器阅读理解的方面情感三元组抽取方法_李睿凡_s0_c1",
    "source_id": "基于掩码上下文机器阅读理解的方面情感三元组抽取方法_李睿凡",
    "text": "方面词推理算法通过BERT编码器提取方面词，并在上下文中掩码其他方面词以减少干扰。上下文数据增强方法通过掩码操作，将一条训练数据扩充为2t条，以增强模型表现。模型架构包括方面词提取、意见词提取、情感分类和方面词存在探测四个模块，协同工作以输出方面情感三元组(a，o，s)。\n\n具体实现中，使用固定的查询q提示模型提取方面词和意见词，通过掩码矩阵M应用到注意力矩阵中以实现掩码效果。模型训练目标是最小化包括方面词、意见词、情感分类和方面词存在探测的损失函数。\n\n本发明的方法在方面词推理阶段识别方面词，并在附属推理阶段识别对应的意见词和情感，有效解决了传统MRC方法面临的方面词干扰问题。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 2,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 293,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "基于文本生成图像的模型训练方法、设备和图像生成方法_冯方向_s0_c0",
    "source_id": "基于文本生成图像的模型训练方法、设备和图像生成方法_冯方向",
    "text": "本发明涉及一种基于文本生成图像的模型训练方法、设备和图像生成方法。该方法包括以下步骤：\n\n1. 对预设训练样本集合中的每个训练样本，基于其文本信息生成文本嵌入式表示，并将其输入图像生成模型，以生成对应的人造图像。同时，采用模态解纠缠方式提取人造图像和相应训练样本的真实图像的真实度参数，包括图像风格的视觉可信度、图-文相似度和图像的整体视觉可信度。\n\n2. 基于人造图像，确定每个训练样本的正例和负例。\n\n3. 利用图像生成模型，基于每个训练样本的正例、负例和真实图像对应的真实度参数，计算子损失函数和总体损失函数，包括内容损失函数、风格损失函数、生成器损失函数和判别器损失函数。\n\n4. 利用总体损失函数，调整图像生成模型的参数。\n\n该方法通过模态解纠缠提取真实度参数，提高了模型学习效率和图像生成效果。\n\n在一种实施方式中，每个训练样本的正例和负例确定方法如下：将训练样本对应的人造图像作为正例，从非该样本对应的图像中选择一个作为负例。可以选择随机或错位选择的方式来挑选负例。\n\n利用图像生成模型，基于每个训练样本的正例、负例和真实图像对应的真实度参数，计算子损失函数，进而计算总体损失函数。子损失函数包括内容损失函数、风格损失函数、生成器损失函数和判别器损失函数；总体损失函数包括判别器总体损失函数和生成器总体损失函数。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 2,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 563,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "基于文本生成图像的模型训练方法、设备和图像生成方法_冯方向_s0_c1",
    "source_id": "基于文本生成图像的模型训练方法、设备和图像生成方法_冯方向",
    "text": "为提高模型参数调整的准确性，分别计算内容损失函数和风格损失函数，避免模态特定部分对模型训练的影响，同时控制图像风格。内容损失函数采用排序目标函数的三元组损失函数，意图是最大化匹配的图、文公共部分表示之间的相似度。风格损失函数基于噪声样本与图像的特定部分表征的一致性。\n\n生成器损失函数和判别器损失函数分别计算，使用的特征是解纠缠后的图像特征。各损失函数线性组合构成总体损失函数，用于调整图像生成模型的参数。\n\n基于上述训练样本对应的判别器总体损失函数和生成器总体损失函数，对图像生成模型的参数进行调整，从而实现图像生成模型的训练。\n\n本发明还提供了一种基于文本生成图像的方法和模型训练设备，以及机器可读的存储介质，用于执行上述方法。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 2,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 316,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "基于特征分布迁移的小样本图像特征学习方法及装置_李晓旭_s0_c0",
    "source_id": "基于特征分布迁移的小样本图像特征学习方法及装置_李晓旭",
    "text": "本发明公开了一种基于特征分布迁移的小样本图像特征学习方法及装置，主要包括以下步骤：\n\n1. 对数据进行预处理，包括训练集和测试集。\n\n2. 利用基类数据预训练嵌入模块fθ，得到特征空间。\n\n3. 将训练集输入嵌入模块fθ，得到样本特征图，输入分布学习模块gφ，最小化损失函数，优化分布学习模块gφ。\n\n4. 将新类数据分为支持集和查询集，计算每类的分布原型。\n\n5. 计算基类数据中各类的类别概率，选取最大的前n个类别，将n个类别的分布与当前类别的分布合并，得到矫正后每类的分布原型。\n\n6. 计算新类查询样本的预测概率。\n\n该方法通过特征分布迁移，减少小样本图像分类中的原型偏差，提高分类效果。\n\n---\n\n[0075] 提供了一种基于特征分布迁移的小样本图像特征学习方法，包括以下阶段步骤：\n\n[0076] S1、对数据进行预处理，分为训练集Dtrain和测试集Dtest；\n\n[0077] 步骤S1包括：\n   S11、将数据分为两部分，类别空间互斥；\n   S12、对于C-way K-shot分类任务，从Dtrain中随机选出C个类别，每个类别中随机选出M个样本，构成任务Ti。\n\n[0078] S2、利用基类数据预训练嵌入模块fθ，包含四个卷积块的嵌入模块对图像提取特征；",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 3,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 542,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "基于特征分布迁移的小样本图像特征学习方法及装置_李晓旭_s0_c1",
    "source_id": "基于特征分布迁移的小样本图像特征学习方法及装置_李晓旭",
    "text": "[0078] S2、利用基类数据预训练嵌入模块fθ，包含四个卷积块的嵌入模块对图像提取特征；\n\n[0079] S3、将Dtrain输入到嵌入模块fθ，得到样本特征图，输入到分布学习模块gφ中，最小化损失函数，优化分布学习模块gφ；\n   S31、计算每个类样本特征图的均值μc和方差σc；\n   S32、调整嵌入模块fθ的参数；\n   S33、计算每个样本类别概率；\n   S34、利用交叉熵公式最小化损失函数。\n\n[0080] S4、计算支持集的分布原型；\n   S41、将新类数据分为支持集和查询集；\n   S42、计算支持集的均值μc和方差σc；\n   S43、计算每个类的分布原型；\n   S44、计算类别方差。\n\n[0081] S5、计算基类数据的类别概率，选取最大的前n个类别，合并分布得到矫正后的分布原型；\n   S51、计算基类样本数据中各个类的类别概率；\n   S52、选取最大的前n个类别；\n   S53、合并分布，得到矫正后的分布原型。\n\n[0082] S6、计算新类查询样本的预测概率，输出类别标签；\n   S61、输入新类数据的查询集样本信息；\n   S62、计算均值和方差；\n   S63、计算预测概率，输出类别标签。\n\n[0131] 提供了一种面向小样本图像的任务自适应度量学习装置，包括嵌入模块、分布学习模块、分布矫正模块和度量模块。\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 588,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "基于特征分布迁移的小样本图像特征学习方法及装置_李晓旭_s0_c2",
    "source_id": "基于特征分布迁移的小样本图像特征学习方法及装置_李晓旭",
    "text": "[0131] 提供了一种面向小样本图像的任务自适应度量学习装置，包括嵌入模块、分布学习模块、分布矫正模块和度量模块。\n\n---\n\n*请注意，由于原文中存在一些不清晰的表述和可能的重复，以上内容已经进行了相应的简化和整理。*",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 111,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "基于自训练的小样本图像集成分类方法及装置_李晓旭_s0_c0",
    "source_id": "基于自训练的小样本图像集成分类方法及装置_李晓旭",
    "text": "本发明公开了一种基于自训练的小样本图像集成分类方法及装置，通过迁移基类数据的卷积特征提取，使用查询样本进行基分类器的自训练，不断地将伪标签集加到支持集扩充支持集，并运用到下一次基分类器自训练中，提高了基分类器自训练结果的可靠性。自训练过程中产生的基分类器，不断通过模型平均得到集成分类器，经过多次迭代自训练和分类器集成过程，得到最终的集成分类器，通过构建损失函数，使得基分类器在查询样本上有确定的、不同的预测，实现基分类器的不同，解决现有小样本分类方法中，基于特征迁移和查询样本自训练的小样本图像集成分类中基学习器的多样性问题，对于提升图像的分类效果非常显著，具有很高的使用价值。\n\n---\n\n[0059] 本研究中，我们采用Baseline++网络结构构建基分类器，并提出了一种基于查询样本自训练和模型平均的集成分类方法。如图2所示。\n\n[0060] 基分类器主要基于Baseline++网络结构，也可以采用其他网络结构如baseline。特征提取网络模块使用在基类数据上预训练得到的模型。对于新类数据，我们抽取三个数据集：有标签的支持集、无标签的支持集和查询集。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 3,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 483,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "基于自训练的小样本图像集成分类方法及装置_李晓旭_s0_c1",
    "source_id": "基于自训练的小样本图像集成分类方法及装置_李晓旭",
    "text": "[0061] 我们优化基分类器和集成分类器，具体步骤如下：使用Baseline++网络结构，在支持集上训练得到m个基分类器，建立特征提取模块、关系网络模块和基分类器模块。特征提取模块基于卷积特征提取支持集特征，关系网络模块基于特征提取模块输出的特征和类权重向量间的余弦距离进行预测。每次任务抽取三个数据集进行以下步骤：\n\n[0062] S21，在支持集上训练得到基分类器0，并将其赋给集成分类器0；\n[0063] S22，使用基分类器0对查询集进行预测，生成伪标签预测集1，将其加入支持集训练得到基分类器1；\n[0064] S23，通过模型平均集成集成分类器0和基分类器1，得到集成分类器1；\n[0065] S24，迭代执行S21-S23，得到最终的集成分类器。\n\n[0066] S25，模型结构类似于迭代模型，每次更新分类器和集成分类器以及伪标签预测集，分类器由有标签的支持集Sc和无标签的支持集U更新。\n\n[0067] 特征提取模块和关系网络模块采用最大似然估计公式的变形进行优化。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 443,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "基于自训练的小样本图像集成分类方法及装置_李晓旭_s0_c2",
    "source_id": "基于自训练的小样本图像集成分类方法及装置_李晓旭",
    "text": "[0067] 特征提取模块和关系网络模块采用最大似然估计公式的变形进行优化。\n\n[0076] S3、使用测试集数据对模型进行训练，优化模型参数；\n[0077] 具体地，使用测试集Dtest，步骤S3包括：\n[0078] S31，为每个任务抽取三个数据集；\n[0079] S32，通过有标签支持集和无标签支持集U*进一步微调分类器；\n[0083] S33，更新集成分类器参数；\n[0086] S34，更新查询集标签。\n\n[0089] S4、通过新类数据上所有任务中查询集标签的预测值和真实值评估模型性能。\n\n[0090] 本发明还提供了一种基于自训练的小样本图像集成分类装置，包括以下模块：\n[0091] 预训练模块；\n[0092] 自训练模块；\n[0093] 集成分类模块。\n\n[0095] 本发明的装置融合了查询样本自训练和模型平均，解决了基分类器多样性问题，显著提升了图像分类效果。\n\n---\n\n注：已去除页眉、页脚、页码、重复信息、乱码、PDF解析错误、多余符号和格式问题，保留了核心学术内容、技术术语、数据、逻辑结构、公式和图表说明。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 471,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "基于词性和位置的特征关键词提取方法_芦效峰_s0_c0",
    "source_id": "基于词性和位置的特征关键词提取方法_芦效峰",
    "text": "本发明涉及一种基于词性和位置的特征关键词提取方法，包括以下步骤：文本预处理，得到候选关键词；去除特定词性的候选关键词，考虑词性和词位置计算加权词频；计算文本中候选关键词的增量逆文档频率；计算文本中候选关键词的权重；按权重排序并选择权重最大的x个词作为文本的关键词。该方法提高了关键词提取的正确率，适用于动态变化的数据集，考虑了词在文本中出现的位置和词性的因素。\n\n具体步骤如下：\n1. 对文本进行预处理，包括分词、去除停用词和标点符号。\n2. 去除不适合作为关键词的特定词性，如副词、数字等。\n3. 计算每个候选词的加权词频，考虑词在标题和正文中的权重增加值。\n4. 计算关键候选词的增量逆文档频率，反映动态数据集中词的逆文档频率。\n5. 计算关键词候选词的权重。\n6. 按权重排序并选择权重最大的x个词作为特征关键词。\n\n与传统TF-IDF方法相比，本发明更好地表示动态数据集中词的逆文档频率，避免无关词被误认为关键词，提高了关键词提取的正确率。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 424,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "基于语言引导的指称表达理解推理网络系统及推理方法_李睿凡_s0_c0",
    "source_id": "基于语言引导的指称表达理解推理网络系统及推理方法_李睿凡",
    "text": "基于语言引导的指称表达理解推理网络系统及推理方法\n\n本发明提供一种基于语言引导的指称表达理解推理网络系统及推理方法，包括：文本特征提取器、图像特征提取器、文本特征扩展器(TFE)、跨模态对齐模块(TCA)和跨模态融合模块(TCF)。通过语言引导推理网络模型(LGR-NET)，以充分利用指称表达式的指导；设置预测标记来捕捉跨模态特征，为了充分利用文本特征，通过文本特征扩展模块(TFE)从三个方面对其进行了扩展，文本生成的坐标嵌入有助于预测词元捕获关键的视觉特征；文本特征用于交替的跨模态推理；新颖的跨模态损失增强了跨模态对齐；如此文本特征从多个角度充分的引导了模型整体的跨模态推理流程，充分利用了文本中的线索，大大提高了模型性能。\n\n基于语言引导的指称表达理解推理网络系统的推理方法，包括如下步骤：",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 5,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 350,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "基于语言引导的指称表达理解推理网络系统及推理方法_李睿凡_s0_c1",
    "source_id": "基于语言引导的指称表达理解推理网络系统及推理方法_李睿凡",
    "text": "基于语言引导的指称表达理解推理网络系统的推理方法，包括如下步骤：\n\n步骤一、多模态特征提取；采用Swin Transformer作为图像特征提取器，输入一张RGB图片，通过图像块分割模块得到初始图像特征图，通过四个降采样阶段分别生成四个特征图，使用卷积神经网络统一四个特征图的通道，最终得到图像特征。采用BERT作为文本特征提取器，在经过词表映射后的文本向量前后分别添加了[CLS]和[SEP]词元，设置最大句子长度，经过BERT的特征提取，得到文本特征。在提取完图文特征后，分别使用两个全连接神经网络(FFN)将图文特征投射到同一个特征空间，最终得到映射图像特征和映射文本特征用于后续的模块输入。\n\n步骤二、文本特征扩展；为了充分利用文本特征进行跨模态推理，采用TFE模块来扩展文本特征。TFE生成了空间特征的坐标编码，包含全体文本特征的词向量，以及浓缩特征的句子向量。前两者分别用于TCA和TCF模块，最后一个用于跨模态损失计算。\n\n步骤三、文本引导的跨模态对齐；在之前提取的图像特征前插入一个预测词元作为初始的跨模态表示。为了对齐图文特征，采用注意力机制。在采用注意力机制前，需从指称表达中引入空间表示增强预测词元。然后采用多头自注意力机制和残差链接、层归一化用于更新、对齐视觉特征。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 5,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 545,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "基于语言引导的指称表达理解推理网络系统及推理方法_李睿凡_s0_c2",
    "source_id": "基于语言引导的指称表达理解推理网络系统及推理方法_李睿凡",
    "text": "步骤四、文本引导的跨模态融合；采用跨模态注意力机制融合文本特征。对齐的图像特征作为query，文本特征作为key和value。文本引导的跨模态注意公式如下：\n\n步骤五、预测头；基于最后一层输出的，经过充分学习的预测词元采用一个三层的FFN和sigmoid激活函数生成最终的预测框。\n\n步骤六、损失和训练；为了训练LGR-NET，采用一个包含两项的损失函数；前者为预测框回归损失，后者为跨模态对齐损失。前者的损失用于帮助预测词元捕获所指对象的边缘特征；后者的损失促进其捕获与指称表达对象的语义一致性特征。\n\n[0075] 批量大小用B表示，·代表内积运算。τ是可训练的温度参数，用于控制分布的平滑度。句子特征fsent由TFE输出，物体特征fp由预测词元生成。\n\n[0076] fobj＝FFNojb(fp)\n\n[0077] 公式表达中省略了层数下标。\n\n[0078] 评估推理网络模型结果时，使用预测准确率作为指标。当预测框和真实框的交并比(IoU)大于0.5时，认为是一次正确预测。\n\n[0079] 本发明提出LGR-NET模型，充分利用指称表达式的指导作用，通过预测词元捕捉跨模态特征，并通过TFE模块从三个方面扩展文本特征。\n\n[0080] LGR-NET应用于REC任务，强调利用文本特征引导跨模态推理。文本生成的坐标嵌入、词嵌入和句子嵌入在跨模态推理中发挥关键作用。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 5,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 588,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "基于语言引导的指称表达理解推理网络系统及推理方法_李睿凡_s0_c3",
    "source_id": "基于语言引导的指称表达理解推理网络系统及推理方法_李睿凡",
    "text": "[0080] LGR-NET应用于REC任务，强调利用文本特征引导跨模态推理。文本生成的坐标嵌入、词嵌入和句子嵌入在跨模态推理中发挥关键作用。\n\n[0081] 新颖的跨模态损失增强了跨模态对齐，通过文本特征的多角度引导，提高了模型性能。\n\n[0082] 图1展示了本发明推理方法与传统方法在解决REC任务上的差异。\n\n[0083] 图2为LGR-NET框架示意图。\n\n[0084] 图3为推理方法的流程总览图。\n\n[0085] 语言引导的指称表达理解推理网络系统包括文本特征提取器、图像特征提取器、TFE、TCA和TCF模块。\n\n[0086] 文本特征提取器用于提取文本特征；图像特征提取器用于提取图像特征。\n\n[0087] 使用预测词元捕获关键视觉和文本特征，通过TFE模块从三个方面扩展文本特征，参与跨模态推理。\n\n[0088] 跨模态对齐模块用于输入和对齐损失计算；跨模态融合模块用于文本特征的融合。\n\n[0089] 推理方法包括多模态特征提取、文本特征扩展、文本引导的跨模态对齐、跨模态融合和预测头步骤。\n\n[0090] 使用Swin Transformer和BERT作为特征提取器，将特征投射到同一特征空间D。\n\n[0091] TFE生成坐标编码、词向量和句子向量，用于TCA、TCF模块和跨模态损失计算。\n\n[0092] 采用注意力机制和多头自注意力机制更新、对齐视觉特征。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 5,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 593,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "基于语言引导的指称表达理解推理网络系统及推理方法_李睿凡_s0_c4",
    "source_id": "基于语言引导的指称表达理解推理网络系统及推理方法_李睿凡",
    "text": "[0092] 采用注意力机制和多头自注意力机制更新、对齐视觉特征。\n\n[0093] 跨模态融合采用跨模态注意力机制。\n\n[0094] 预测头使用三层FFN和sigmoid激活函数生成预测框。\n\n[0095] 损失函数包括预测框回归损失和跨模态对齐损失，使用GIoU损失和L1损失。\n\n[0096] 本发明通过具体实施例展示了其在REC任务上的应用，提高了推理准确性。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 5,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 183,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "基于采集搜索引擎数据的隐私信息评级方法_芦效峰_s0_c0",
    "source_id": "基于采集搜索引擎数据的隐私信息评级方法_芦效峰",
    "text": "本发明涉及一种基于采集搜索引擎数据的隐私信息评级方法，包括以下步骤：首先从搜索引擎采集数据确定每个隐私信息的普遍性分值U，其次确定每个隐私信息的敏感性分值S，最后根据U×S计算结果确定隐私信息的安全等级。评级方法使用的数据来源于搜索引擎庞大的用户群，确保评级结果具有公正性，不依赖于个人经验或意见。本发明不仅适用于全体隐私信息评级，也可用于评定应用系统中有限数量的隐私信息。\n\n具体实施方式包括：第一步，通过搜索引擎统计隐私项P的查询结果数量N1，计算全体隐私信息项的N1的平均值μ1，以此确定隐私信息的普遍性分值U；第二步，统计隐私项P隐私泄露的查询结果数量N2，计算N2占(N1+N2)的比例β，确定隐私信息的敏感性分值S；第三步，根据U×S计算结果，将隐私信息评定为1级、2级或3级安全等级。该方法具有全面性、公正性，且在实际应用中灵活广泛。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 374,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "融入类别自适应度量学习的小样本图像分类方法及装置_李晓旭_s0_c0",
    "source_id": "融入类别自适应度量学习的小样本图像分类方法及装置_李晓旭",
    "text": "本发明公开了一种融入类别自适应度量学习的小样本图像分类方法及装置，由数据预处理模块、构建网络模型模块、训练模型参数模块和测试模型性能模块组成。本发明提供的融入类别自适应度量学习的小样本图像分类方法及装置，为每个类别构建一个度量模块，通过对类内共性特征的学习，建立基于类内共性特征的度量，利用已经进行过预训练的嵌入模块，输入支持样本得到特征矩阵，并将其输入到类相关自适应度量模块，进行特征拼接并得到关系分数，将相似性最大的类作为预测类别，得到最终预测结果，从而提高小样本图像分类的性能，解决小样本图像分类中基于类内共性特征的度量学习问题，对于图像的分类效果十分明显，在实践中体现出极大价值。\n\n---\n\n[0060] 本发明提供了一种融入类别自适应度量学习的小样本图像分类装置，包括以下模块：\n\n- 数据预处理模块：将数据划分为训练集和测试集，确定模型训练方式；\n- 网络模型构建模块：构建融入类别自适应度量学习的小样本图像分类模型；\n- 模型参数训练模块：利用基类数据对模型进行训练，求解模型参数；\n- 模型性能测试模块：对新类任务进行预测，测评模型性能。\n\n[0062] 数据预处理模块执行以下步骤：\n\n- 将数据分为训练集Dtrain和新类数据Dtest；\n- 对于C-way K-shot分类任务，从Dtrain中随机选出C个类别，每个类别中随机选出K个支持样本Si和M-K个查询样本Qi。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 2,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "融入类别自适应度量学习的小样本图像分类方法及装置_李晓旭_s0_c1",
    "source_id": "融入类别自适应度量学习的小样本图像分类方法及装置_李晓旭",
    "text": "[0066] 网络模型构建模块由嵌入模块fθ和类相关自适应度量模块组成：\n\n- 嵌入模块fθ包含四个卷积块，用于提取样本特征；\n- 类相关自适应度量模块学习特定类别的样本特征之间的度量。\n\n[0070] 模型参数训练模块执行以下步骤：\n\n- 对嵌入模块fθ进行预训练；\n- 微调关系模块，提高模型泛化能力；\n- 利用均方误差损失函数(MSE)计算损失。\n\n[0080] 模型性能测试模块执行以下步骤：\n\n- 将查询样本输入嵌入模块fθ；\n- 输出特征矩阵输入类相关自适应度量模块，得到相似性关系；\n- 将相似性最大的类作为预测类别。\n\n[0085] 本发明的小样本图像分类方法包括以下步骤：\n\n- 数据预处理；\n- 构建融入类别自适应度量学习的小样本图像分类模型；\n- 模型训练；\n- 模型性能测试。\n\n本发明通过构建每个类别的度量模块，提高小样本图像分类性能。\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 2,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 386,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "负例训练样本采集方法、装置及模型训练方法、装置_李睿凡_s0_c0",
    "source_id": "负例训练样本采集方法、装置及模型训练方法、装置_李睿凡",
    "text": "本发明实施例提供了一种负例训练样本采集方法，包括以下步骤：\n\n1. 将多张图像输入到初始图像检索模型中，得到各图像对应的表示向量，构成样本集合。\n\n2. 对样本集合中的表示向量进行聚类，得到多个聚类，并确定每个聚类的聚类中心。\n\n3. 针对每个目标向量，确定其所属的第一聚类。\n\n4. 基于第一聚类的聚类中心，确定目标向量对应的多个候选聚类的目标概率。\n\n5. 基于目标概率，对目标向量对应的候选聚类执行多次聚类抽取操作，得到多个第二聚类。\n\n6. 在每个第二聚类中获取一个表示向量，作为目标向量的负例训练样本。\n\n此外，本发明还提供了基于该负例训练样本采集方法的模型训练方法，包括以下步骤：\n\n1. 在样本集合中获取多个目标向量，并确定每个目标向量的正例训练样本。\n\n2. 确定每个目标向量的多个负例训练样本。\n\n3. 确定多组训练样本，每组包括一个目标向量、其正例和负例训练样本。\n\n4. 基于训练样本对初始图像检索模型进行训练，并计数迭代次数。\n\n5. 当迭代次数达到预设值时，判断是否满足停止条件，若满足则停止训练，否则重新确定训练样本继续训练。\n\n本发明还提供了相应的负例训练样本采集装置和模型训练装置。\n\n根据提供的学术论文片段，以下是清洗后的内容：\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 6,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 536,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "负例训练样本采集方法、装置及模型训练方法、装置_李睿凡_s0_c1",
    "source_id": "负例训练样本采集方法、装置及模型训练方法、装置_李睿凡",
    "text": "本发明还提供了相应的负例训练样本采集装置和模型训练装置。\n\n根据提供的学术论文片段，以下是清洗后的内容：\n\n---\n\n在本发明实施例提供的一种负例训练样本采集方法中，通过对样本集中各个表示向量的聚类，将各个表示向量按照相似度进行分类。从而可以确定每个候选聚类作为目标向量所属的第一聚类的近邻聚类的概率，也就是可以确定每个候选聚类中的表示向量作为目标向量的负例训练样本的难度水平。继而，基于该概率确定用于抽取该目标向量的负例训练样本的第二聚类，便可以兼顾到兼顾各个不同难度水平的负例训练样本。同时，由于概率较大的聚类中的表示向量为“较难”的负例训练样本，根据概率论的相关知识，显然，这些概率较大的聚类被抽取为第二聚类的可能性较高。因此，采集负例训练样本时，不但可以兼顾各个不同难度水平的负例训练样本，还可以优先采集“难”的负例训练样本。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 367,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "负例训练样本采集方法、装置及模型训练方法、装置_李睿凡_s0_c2",
    "source_id": "负例训练样本采集方法、装置及模型训练方法、装置_李睿凡",
    "text": "在本发明实施例提供的一种模型训练方法中，考虑到在图像检索模型的训练过程中，模型的相关参数和权重会发生变化，导致样本集合的聚类结果发生变化，进而导致基于聚类结果确定的训练样本发生变化，因此，在图像检索模型的训练过程中，可以随着模型的相关参数和权重的变化，调整对样本集合的聚类结果，使获得的负例训练样本更具有代表性。同时，在模型训练方法中通过上述负例训练样本采集方法确定训练样本中的负例训练样本，以使得每次确定的负例训练样本时，可以在兼顾各个不同难度水平的负例训练样本的同时，优先采集“难”的负例训练样本。进而，可以提高基于获得的负例训练样本训练得到的图像检索模型的检索准确率。\n\n---\n\n以上内容已去除页眉、页脚、页码、重复信息和乱码，保留了核心学术内容、技术术语和数据、原有逻辑结构、公式和图表说明。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 351,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "负例训练样本采集方法、装置及模型训练方法、装置_李睿凡_s0_c3",
    "source_id": "负例训练样本采集方法、装置及模型训练方法、装置_李睿凡",
    "text": "---\n\n以上内容已去除页眉、页脚、页码、重复信息和乱码，保留了核心学术内容、技术术语和数据、原有逻辑结构、公式和图表说明。\n\n本发明提供了一种负例训练样本采集方法，通过对样本集合中各个表示向量的聚类，将各个表示向量按照相似度进行分类。可以确定每个候选聚类作为目标向量所属的第一聚类的近邻聚类的概率，即可以确定每个候选聚类中的表示向量作为目标向量的负例训练样本的难度水平。基于该概率确定用于抽取该目标向量的负例训练样本的第二聚类，可以兼顾到兼顾各个不同难度水平的负例训练样本。同时，由于概率较大的聚类中的表示向量为“较难”的负例训练样本，这些概率较大的聚类被抽取为第二聚类的可能性较高。因此，采集负例训练样本时，不但可以兼顾各个不同难度水平的负例训练样本，还可以优先采集“难”的负例训练样本。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 346,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "负例训练样本采集方法、装置及模型训练方法、装置_李睿凡_s0_c4",
    "source_id": "负例训练样本采集方法、装置及模型训练方法、装置_李睿凡",
    "text": "本发明还提供了一种基于上述负例训练样本采集方法的模型训练方法。该方法包括在预设的初始图像检索模型获得的样本集合中获取多个目标向量，并确定每个目标向量所对应的一个正例训练样本；针对每个目标向量，在样本集合中确定该目标向量所对应的多个负例训练样本；确定多组训练样本，其中，每个训练样本包括一个目标向量、该目标向量所对应的一个正例训练样本和该目标向量所对应的多个负例训练样本；基于多组训练样本对预设的初始图像检索模型进行训练，并从零开始对迭代次数进行计数，作为目标次数；当目标次数达到预设数值时，判断是否满足训练停止条件；如果满足，执行停止模型训练，得到训练完成的图像检索模型；否则，执行将目标次数清零。通过该方法可以提高基于获得的负例训练样本训练得到的图像检索模型的检索准确率。\n\n[0201] 对样本集合的多个表示向量进行聚类，得到多个聚类并确定聚类中心；\n\n[0202] 确定每个目标向量所属的第一聚类，目标向量为多张图像中目标图像对应的表示向量；\n\n[0203] 基于第一聚类的聚类中心，确定每个目标向量对应的多个候选聚类的目标概率，任一聚类的目标概率为该聚类作为第一聚类的近邻聚类的概率；\n\n[0204] 基于目标概率，对每个目标向量对应的多个候选聚类执行聚类抽取操作，得到多个第二聚类；\n\n[0205] 在每个第二聚类中获取一个表示向量，作为目标向量的负例训练样本。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 586,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "负例训练样本采集方法、装置及模型训练方法、装置_李睿凡_s0_c5",
    "source_id": "负例训练样本采集方法、装置及模型训练方法、装置_李睿凡",
    "text": "[0205] 在每个第二聚类中获取一个表示向量，作为目标向量的负例训练样本。\n\n[0206] 本发明提供的负例训练样本采集方法的其他实现方式与上述方法实施例相同。\n\n[0207] 本发明通过聚类将表示向量按相似度分类，确定候选聚类的目标概率，基于此概率确定负例训练样本的第二聚类，兼顾不同难度水平的负例训练样本。\n\n[0211] 模型训练方法包括：获取目标向量，确定正例训练样本和负例训练样本，确定多组训练样本，对初始图像检索模型进行训练，迭代计数，判断是否满足训练停止条件。\n\n[0226] 负例训练样本采集方法包括：输入多张图像到初始图像检索模型，得到表示向量，进行聚类，确定目标向量的第一聚类和候选聚类的目标概率，执行聚类抽取操作，获取负例训练样本。\n\n[0245] 在模型训练过程中，根据模型参数和权重的变化调整聚类结果，确保负例训练样本的代表性，提高图像检索模型的检索准确率。\n\n[0246] 本发明中的关系术语不要求实体或操作之间存在实际关系或顺序，\"包括\"等术语意在涵盖非排他性的包含。\n\n[0248] 本发明的保护范围不限于上述实施例，任何修改、替换、改进均在保护范围内。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 5,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 494,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "面向小样本图像分类的任务相关度量学习方法及装置_李晓旭_s0_c0",
    "source_id": "面向小样本图像分类的任务相关度量学习方法及装置_李晓旭",
    "text": "本发明公开了一种面向小样本图像分类的任务相关度量学习方法及装置，方法主要由数据预处理阶段、构建网络模型阶段、训练模型参数阶段和测试模型性能阶段组成。通过考虑不同任务之间的差异性，引入注意力机制的思想，并学习任务相关的空间映射，利用任务自适应度量学习的方式，解决了小样本图像分类中存在的自适应度量学习问题，从而提高在小样本条件下目标任务分类的准确性，改善了图像的分类效果，具有很高的实用价值。\n\n网络模型构建模块：用于引入注意力机制和自适应度量学习，构建面向小样本图像分类的任务相关度量学习模型。模型由嵌入模块和任务相关度量模块组成；嵌入模块包含四个卷积块，每个卷积块包括卷积层、池化层及非线性激活函数；任务相关度量模块由注意力模块和余弦度量模块组成。\n\n训练模型参数模块：用于面向小样本图像分类的任务相关度量学习模型进行训练，求解模型参数。\n\n测试模型性能模块：利用训练好的模型对新类的任务进行预测，测评模型的性能。\n\n本发明引入注意力机制，建立了一种面向小样本图像分类的任务相关度量学习方法及装置，通过考虑不同任务间的差异性，并学习任务相关的空间映射，利用任务自适应度量学习的方式，解决了小样本图像分类中的自适应度量学习问题，提高了在小样本条件下目标任务分类的准确性。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 2,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 535,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "面向小样本图像分类的任务相关度量学习方法及装置_李晓旭_s0_c1",
    "source_id": "面向小样本图像分类的任务相关度量学习方法及装置_李晓旭",
    "text": "具体实施方式包括以下阶段步骤：\n- 数据预处理阶段：对数据进行预处理，划分为训练集和测试集；\n- 构建网络模型阶段：引入注意力机制和自适应度量学习，构建模型；\n- 训练模型参数阶段：使用训练集进行模型训练；\n- 测试模型性能阶段：使用训练后的模型进行性能测试。\n\n还包括了Adam优化算法的具体步骤，以及面向小样本图像分类的任务相关度量学习装置的描述。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 2,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 176,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "patent"
    }
  },
  {
    "chunk_id": "C语言实例对比_从抽象概念到代码直观_s0_c0",
    "source_id": "C语言实例对比_从抽象概念到代码直观",
    "text": "C语言实例对比：从抽象概念到代码直观\n\n（北京邮电大学计算机学院，北京，100876）\n\n摘 要：本文通过实例对比法，探讨C语言教学中的抽象概念，如可读性、健壮性、编码风格和设计思路，以促进语法规则学习与程序设计技能的应用。\n\n关键词：比较法；C语言；可读性；健壮性；编码风格\n\nC语言作为计算机信息专业的必修课，对初学者具有一定难度。难点在于将程序思想转化为C语言代码，以及理解C语言的复杂特性，如指针。同时，编码风格、程序设计的健壮性和可读性是教学中易被忽视的问题。\n\n本文从程序设计为中心的教学思路出发，采用实例对比法，从程序设计、可读性、健壮性等方面探讨C语言教学。通过实例代码，指导学生理解、运用C语言，培养良好的编程习惯。\n\n2 对比实例\n\n2.1 程序版式与可读性\n\nC语言的编写版式灵活，但教师需强调编写格式对代码可读性的重要性。通过比较不同编写风格的程序，学生可以加深对编码格式问题的认识。\n\n2.2 命名规则与可读性\n\n命名规则对程序可读性有重要影响。本文提出使用中文拼音、匈牙利命名法等命名规则，以提高代码的可读性。\n\n2.3 程序编写与健壮性\n\n学生编写程序时，常忽略异常数据处理。本文通过const修饰、assert语句、文件打开检测等例子，说明增强程序健壮性的方法。\n\n2.4 程序设计思路比较",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 2,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 561,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "C语言实例对比_从抽象概念到代码直观_s0_c1",
    "source_id": "C语言实例对比_从抽象概念到代码直观",
    "text": "学生编写程序时，常忽略异常数据处理。本文通过const修饰、assert语句、文件打开检测等例子，说明增强程序健壮性的方法。\n\n2.4 程序设计思路比较\n\n分析比较同一问题的多种求解思路与代码，有助于学生形成问题求解的一般思路与方法，促进编程能力的提升。\n\n通过动手实践、参加程序竞赛等，鼓励学生将理论知识应用于实践，提高C语言程序设计能力。\n\n参考文献：\n\n[1] 谭浩强. C高级语言程序设计[M]. 2版. 清华大学出版社, 2006.\n[2] 陈良银, 游洪跃, 李旭伟. C语言程序设计[M]. C99版. 清华大学出版社, 2006.\n[3] Brian W K, Dennis M R. The C Programming Language[M]. 2nd edition, 1988. 机械工业出版社.\n[4] 潘京. 用比较法进行文科C语言程序设计教学[J]. 中央民族大学学报, 2005, 14(2).",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 2,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 412,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "人工生命中分布智能研究的一种可行方法_s0_c0",
    "source_id": "人工生命中分布智能研究的一种可行方法",
    "text": "人工生命中分布智能研究的一种可行方法\n\n关键词：人工生命，分布智能，蚁群算法，粒子群优化\n\n摘要：针对蚁群算法和粒子群算法的提出过程进行分析，提出一种人工生命分布智能研究的方法。该方法强调理论生物学对人工生命研究的基础性作用，提出从理论生物学中选择研究方向的策略，并讨论现有模型的利用。这三个过程构成了分布智能研究的一种方法。\n\n人工生命是涉及生物学、数学和计算机科学的新兴交叉学科。分布智能，或群智能，是人工生命中一个有趣且充满希望的研究方向，其特征为多智能体、非中心控制、自组织和涌现式行为。粒子群算法与蚁群算法是分布智能中两种典型算法。\n\n从更广泛的意义上，如何在人工生命这一新兴领域进行科学研究是一个吸引人的问题。分析粒子群算法和蚁群算法的提出过程，我们试图得出一些启示。文章首先讨论理论生物学与人工生命的关系；然后提出从理论生物学中选择研究起点的方向；接着探讨从自然生命到人工生命模型的拓展方法；最后讨论所提出的方法。\n\n选择研究起点是科学研究中的一个普遍问题。人工生命与理论生物学存在固有联系。理论生物学使用数学语言构建模型，理解复杂的生物现象与过程。人工生命定义为研究展现自然生命系统特征的人造系统的学科。人工生命的研究必须植根于理论生物学。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 2,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 528,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "人工生命中分布智能研究的一种可行方法_s0_c1",
    "source_id": "人工生命中分布智能研究的一种可行方法",
    "text": "蚁群算法由Dorigo及其同事为解决旅行商问题提出，受蚁群搜索食物行为的启发。理论生物学家发现蚂蚁释放的信息素在路径选择中起重要作用，Dorigo的工作利用了这些成果。\n\n粒子群算法由Eberhart和Kennedy提出，受鸟群和鱼群群体行为的启发。理论生物学家和计算机科学家对鸟群的飞行移动机制感兴趣，提出了基本行为集合和简单规则。\n\n从理论生物学中选择研究起点后，下一步是将现有模型应用于问题。粒子群算法和蚁群算法提供了良好范例。\n\n人工生命研究的方法学与其研究本身具有同样意义。理论生物学为人工生命提供了大量研究主题。研究热点的边界和不同研究领域的交叉处值得关注。从自然到人工的模型拓展是直接的。\n\n科学研究中方法的作用不可忽视，方法并非金科玉律，而是客观存在于科学研究的成败之中。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 2,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 344,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "使用LDC码的BI_STCM_ID系统中的星座映射分析_s0_c0",
    "source_id": "使用LDC码的BI_STCM_ID系统中的星座映射分析",
    "text": "使用LDC码的BI-STCM-ID系统中的星座映射分析\n\n赵传钢1，李睿凡2\n\n1. 北京林业大学信息学院，北京100083\n2. 北京邮电大学信息工程学院，北京100876\n\n摘要：研究了BI-STCM-ID系统中的星座映射问题。证明了在使用LDC（Linear Dispersion Code）空时编码方案的BI-STCM-ID系统中，基于最大化编码增益的高维星座映射设计优化问题等价于基于最大化欧式距离调和均值的一维星座映射设计优化问题。\n\n关键词：空时编码调制；星座映射；LDC码\n\n1 BI-STCM-ID系统模型\n\n1.1 BI-STCM-ID发端系统\n\n图1给出了BI-STCM-ID系统的结构。假设信息比特序列经卷积编码器编码后输出编码比特序列c。编码比特序列C经交织后每K=mQ个比特分为一组。每个比特序列p”按照某种映射规则p映射到星座上的某个复信号点s4斗(，)，星座上有2“个星座点。空时编码器将信息符号向量s作为输入，形成一个空时码字矩阵j。\n\n1.2 MIMO信道模型\n\n假设接收端有M个接收天线，接收符号可以用一个N×T的矩阵y表示，其中h表示第i个发送天线至第J个接收天线之间的衰落因子。信道模型可以表示为Y=Hx+w。\n\n2 BI-STCM-ID星座映射分析\n\n2.1 BI-STCM-ID系统渐进性能",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 2,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 568,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "使用LDC码的BI_STCM_ID系统中的星座映射分析_s0_c1",
    "source_id": "使用LDC码的BI_STCM_ID系统中的星座映射分析",
    "text": "2 BI-STCM-ID星座映射分析\n\n2.1 BI-STCM-ID系统渐进性能\n\n在理想交织假设下，BI-STCM-ID系统的渐进误比特率性能可以表示为阁：\n\n2.2 使用LDC码的BI-STCM-ID系统中的星座设计\n\nLDC(Linear Dispersion Code)可以分为实LDC码与复LDC码。复LDC码通常由一组基本矩阵{D_i}定义。下面将讨论使用一类重要的LDC码的BI—STCM—ID系统中的星座映射的调和均值准则。\n\n定理1(LDC码欧式距调和均值准则)：假设x是由基本码字矩阵{D_i}构造的LDC码，即x=∑s'D_i，如果满足如下条件：rank(D_i)=r，那么有：InQ_max∞。(x，D_i)，其中，K=(∏_i=1^Q非零特征值的乘积。\n\n本文讨论了BI—STCM—ID系统中的星座设计问题，特别是空时编码方案采用LDC编码方案的情形。对于其他空时编码方案是否有同样的结论，则是下一步研究的课题。\n\n[参考文献列表省略]",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 2,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 431,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "使用TAST码的BI_STCM_ID系统中的星座映射分析_s0_c0",
    "source_id": "使用TAST码的BI_STCM_ID系统中的星座映射分析",
    "text": "研究了BI-STCM-ID系统中的星座映射问题。证明了在使用TAST空时编码方案的BI-STCM-ID系统中，基于最大化编码增益的高维星座映射设计优化问题等价于基于最大化欧式距调和均值的一维星座映射设计优化问题。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 106,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "全卷积神经结构的段落式图像描述算法_李睿凡_s0_c0",
    "source_id": "全卷积神经结构的段落式图像描述算法_李睿凡",
    "text": "后的内容：\n\n段落式图像描述的目标是为给定图像生成描述性的自然语言段落。该任务连接计算机视觉和自然语言处理领域，是跨模态智能的重要研究方向，也是盲人导航和幼儿早期教育等应用的核心技术。当前，段落式图像描述算法主要采用编码器与解码器组合的端到端结构。基于卷积神经网络(CNN)的编码器将图像表示为视觉向量，循环神经网络(RNN)解码器将视觉向量解码为自然语言段落。然而，RNN解码器在长时记忆和连贯性方面存在局限。为提升连贯性，本文提出一种基于全卷积结构的图像段落描述算法。采用基于卷积网络的区域检测器获取图像表示，构建层次性的深度卷积解码器对图像表示解码，自动生成段落式文本描述。同时，将门控机制嵌入卷积解码器网络中，以提升模型的记忆能力。实验结果表明，新算法能够为图像生成更为连贯的段落式文本描述，在评测指标上取得较好结果。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 4,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 363,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "全卷积神经结构的段落式图像描述算法_李睿凡_s0_c1",
    "source_id": "全卷积神经结构的段落式图像描述算法_李睿凡",
    "text": "观察表1，人类描述段落与机器生成段落的BLEU得分接近，而CIDEr得分差异显著。这表明CIDEr评价指标能更好地揭示机器描述与人类描述之间的显著差异。与仅考虑n元组匹配程度的BLEU指标相比，基于共识的CIDEr指标更能反映生成段落的连贯性。在CIDEr指标上，所提方法比Sentence-Concat方法高出133.8%，显示出段落描述任务与单句描述任务间的巨大差异。此外，所提方法比Image-Flat方法高出43.2%，验证了所提解码器层次性结构的有效性。所提方法较Hierarchical-RNN方法高出17.8%，说明了卷积结构优势。\n\n不同束大小参数的评测结果表明，束大小为2时，评测结果最优。过小的束大小会导致解码信息丢失，过大的束大小则增加了解码时间复杂度并降低了段落多样性。\n\n实验进一步考察了迭代过程中各指标的变化，表明指标结果的一致性。各指标随迭代轮次的变化趋势基本保持一致，在第5至15个轮次之间上升，在第15个轮次左右达到最优性能。\n\n主观评价显示，所提方法生成的描述段落具有更强的上下文连贯性和语言逻辑性，减少了信息重复表达。提出的全卷积神经网络结构的段落式图像描述生成模型，通过层次性深度卷积解码器对图像表示进行解码，引入门控机制提升模型记忆能力，生成更连贯的描述段落。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 4,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 551,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "全卷积神经结构的段落式图像描述算法_李睿凡_s0_c2",
    "source_id": "全卷积神经结构的段落式图像描述算法_李睿凡",
    "text": "参考文献：\n[1] Vinyals O, Toshev A, Bengio S, et al. Show and tell: a neural image caption generator. In: 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). New York: IEEE Press, 2015: 3156-3164.\n[2] Lu Jiasen, Xiong Caiming, Parikh D, et al. Knowing when to look: adaptive attention via a visual sentinel for image captioning. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). New York: IEEE Press, 2017: 375-383.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 4,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 456,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "全卷积神经结构的段落式图像描述算法_李睿凡_s0_c3",
    "source_id": "全卷积神经结构的段落式图像描述算法_李睿凡",
    "text": "[3] Mao Yuzhao, Zhou Chang, Wang Xiaojie, et al. Show and tell more: topic-oriented multi-sentence image captioning. In: Proceedings of the 27th International Joint Conference on Artificial Intelligence. California: International Joint Conferences on Artificial Intelligence Organization, 2018: 4258-4264.\n...以及其他参考文献。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 4,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 318,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于KFD_Isomap的人脸识别_s0_c0",
    "source_id": "基于KFD_Isomap的人脸识别",
    "text": "基于KFD-l somap的人脸识别\n\n李睿凡1郝红卫2涂序彦2王枞1\n\n(1北京邮电大学信息工程学院北京 100876 2北京科技大学信息工程学院北京 100083)\n\n摘要：神经生理学近来强调视觉以流形方式感知外部对象并提出Isomap的流形学习方法。该方法在数据描述等问题上取得良好效果，但从模式分类观点看并非最优。以此提出KFD—Isomap算法，用核Fisher判别替代Isomap中的经典多维尺度分析，并应用于人脸识别问题。两个人脸数据库的识别实验表明了该算法的有效性。\n\n关键词：人脸识别流形Isomap核Fisher判别",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 268,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于KFD_Isomap的人脸识别_s1_c0",
    "source_id": "基于KFD_Isomap的人脸识别",
    "text": "神经生理学强调视觉以流形方式感知外部对象，Isomap流形学习方法在数据描述等问题上取得良好效果，但从模式分类观点看并非最优。本文提出KFD—Isomap算法，用核Fisher判别替代Isomap中的多维尺度分析，应用于人脸识别问题。\n\n2 KFD-I somap算法\n\n构建邻域图，计算最短路径，确定投影向量，使用核Fisher判别求解广义特征值问题，分类测试样本。\n\n3 人脸识别实验\n\n使用KFD-isomap、Isomap、Ext-Isomap、特征脸、Fisher脸等对ORL和Yale两个人脸数据库进行人脸识别实验。KFD-Isomap表现出最好的性能。",
    "section_title": "引言",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 1,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 283,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于KFD_Isomap的人脸识别_s2_c0",
    "source_id": "基于KFD_Isomap的人脸识别",
    "text": "提出了一种用核Fisher判别函数改进的Isomap——KFD-Isom印进行特征提取，并用于人脸识别的方法。该方法用测地线距离矩阵的列向量作为特征，并用核Fisher判别替代多维尺度分析建立最佳投影方向。两个人脸数据库的实验表明该方法的性能优于实验采用的其它方法。",
    "section_title": "结论",
    "section_type": "conclusion",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 2,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 133,
    "chunk_method": "hierarchical",
    "importance_weight": 0.935,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "增强提示学习的少样本文本分类方法_李睿凡_s0_c0",
    "source_id": "增强提示学习的少样本文本分类方法_李睿凡",
    "text": "针对少样本文本分类任务，提出了一种名为EPL4FTC的提示学习增强分类算法。该算法首先将文本分类任务转换为基于自然语言推理的提示学习形式，以利用预训练语言模型的先验知识，实现隐式数据增强。通过两种不同粒度的损失优化，并引入三元组损失联合优化，以捕获下游任务中的类别信息。同时，掩码语言模型任务作为正则项，提升模型泛化能力。在多个中英文文本分类数据集上进行了实验评估，结果表明EPL4FTC方法在准确度性能上明显优于对比的基线方法。\n\n其中，Ie表示在当前样例组中真实标签为蕴含关系的位置索引，g(s(z|x))表示语言模型对位置处的推理词在蕴含关系上的预测得分。最后，基于自然语言推理的提示学习模块的损失函数定义如下：\n\nLp= (1 −α) ∙Ls+ α∙Lq (6)\n\n其中，α为可调节的超参数。\n\n图2 单句级与句群级的优化图\n\n度量优化模块的目标是使在语义空间中属于同一类别的实例的距离更接近，而不同类别的实例的距离远离。通过三元组损失函数进行有监督的度量学习，使模型可以更好学习不同类别间的距离关系信息。此外，使用带间隔的损失函数可以提升模型的泛化性能。具体地，构造三元组数据时，在某个类别中选定一个实例作为锚点，同类别的实例作为正例，其他类别的实例作为负例。\n\nLtm= ∑ max(0, d(Am, Pm) −d(Am, Nm) + ∆) M m=1 (7)",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 9,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 586,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "增强提示学习的少样本文本分类方法_李睿凡_s0_c1",
    "source_id": "增强提示学习的少样本文本分类方法_李睿凡",
    "text": "Ltm= ∑ max(0, d(Am, Pm) −d(Am, Nm) + ∆) M m=1 (7)\n\n其中，d(Am, Pm)表示锚点与正例间的距离，d(Am, Nm)表示锚点与负例间的距离，∆表示设定的间隔值。\n\n此外，少样本学习场景中用于训练的数据量通常十分有限。为了缓解灾难性遗忘的问题，使用掩码语言模型优化目标作为正则项进行建模。所以度量优化模型的损失函数表示为：\n\nLax= (1 −β)Ltm+ βLmlm (8)\n\n其中，Lmlm表示语言模型损失，β表示相应的权重参数。\n\n最后，整体的损失函数由提示学习损失Lp和度量优化损失Lax的加权构成，具体如下：\n\nLtotal= (1 −γ)Lp+ γLax (9)\n\n其中，Lp表示基于自然语言推理的提示学习模块的损失，Lax表示度量优化模块的损失，γ表示权重参数。\n\n2.3 模型训练与推理",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 376,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "增强提示学习的少样本文本分类方法_李睿凡_s0_c2",
    "source_id": "增强提示学习的少样本文本分类方法_李睿凡",
    "text": "其中，Lp表示基于自然语言推理的提示学习模块的损失，Lax表示度量优化模块的损失，γ表示权重参数。\n\n2.3 模型训练与推理\n\nEPL4FTC算法将文本分类任务转化成自然语言推理任务，即转化后的任务是一个二分类任务。因此，当一个原始分类任务包括N个类别时，该算法需要进行N次推理，最后选择推理概率最大所对应的标签类别作为最终预测结果。所以，1) 在模型训练过程中，为提升模型的泛化性能同时降低模型训练的成本，通过负采样的方式对下游任务进行训练。对于一个包含多个类别的分类任务，将每一个实例与之对应的类别作为正例，同时随机选择K个其他类别与当前实例构成负例。以上数据构造方式不但能够提升模型的性能，而且相比使用全部类别作为负例进一步缩短了训练模型所需的时间。2) 在模型推理阶段，EPL4FTC算法仅使用基于自然语言推理的提示学习模块。具体地，对于包含N个标签的文本分类任务，对每一个实例生成包含自然语言推理提示模板的N条新的输入实例。通过模型预测出每一个实例中位置处所蕴含推理词的概率，在N个预测结果中选择预测最大概率所对应的标签作为当前原始输入实例的预测结果。\n\n3 实验与结果 3.1 实验数据集",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 500,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "增强提示学习的少样本文本分类方法_李睿凡_s0_c3",
    "source_id": "增强提示学习的少样本文本分类方法_李睿凡",
    "text": "3 实验与结果 3.1 实验数据集\n\n1) 中文数据集。中文数据集使用少样本评测数据集FewCLUE中的文本分类任务对应的数据集。本文在4个不同领域的评测数据集上进行实验。其中，EPRSTMT为电商评论情感分析任务，是典型的包含正负向情感的二分类任务。CSLDCP是科学文献学科领域的长文本多分类任务，包含了67个类别。TNEWS是新闻标题的短文本分类任务，包含了教育、娱乐和文化等15个类别。最后，IFLYTEK是根据APP应用的长文本主题描述信息对超过100多个应用类别进行分类的任务。2) 英文数据集。本文采用3个英文文本分类数据集AG News、TREC以及Yelp Review进行评测。其中，AG News是学术新闻搜索引擎从多个新闻来源中搜集超过了100万篇新闻文章构成的数据集。它包含4类新闻主题，分别是世界、体育、商业和科技。TREC数据集包含6个一级标签和47个二级标签。Yelp Review数据集来自Yelp的用户评论。它的标签是用户对商品的星级打分，共分为5级。用于评测的英文数据集将从以上数据集中抽样获得。将每一个原始英文数据集中随机抽取8个、16个和32个实例形成对多个不同规模的数据集用于训练，测试集为默认。\n\n3.2 基线方法",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 530,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "增强提示学习的少样本文本分类方法_李睿凡_s0_c4",
    "source_id": "增强提示学习的少样本文本分类方法_李睿凡",
    "text": "3.2 基线方法\n\n采用的基线方法包括：1)基于微调方法：在预训练语言模型的基础上，通过为模型添加任务相关的分类器，达到使模型可以处理具体的下游任务的目的。2)Zero-shot方法：基于Roberta等自编码预训练语言模型，通过MLM进行推理评测。3)Zero-shot(GPT)方法：基于GPT自回归预训练语言模型，通过从左至右的语言模型进行推理评测。4)PET方法：通过添加人工自定义模板，将下游任务转化成完成填空形式的任务，然后在候选标签列表中选择合适的标签。5)ADAPET方法：对模板搜索正确答案时从有限候选词变成整个词表，扩大了模型的搜索空间。此外，对正确标签反向预测原文中的词，实现模型性能的提升。6)LM-BFF方法：通过自动化生成的离散化自然语言作为提示模板，同时通过采样的形式将实例以上下文的方式添加到每一个输入中。7)P-tuningR方法：区别于自然语言形式的提示模板，采用Roberta作为预训练语言模型，实现让模型自动学习到最佳的连续式的非自然语言提示模板。8)EFL方法：通过添加人工自定义模板，将下游任务转化成蕴含任务形式，并添加额外的二分类器，实现对下游任务的微调。\n\n3.3 实现细节与评测指标",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 516,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "增强提示学习的少样本文本分类方法_李睿凡_s0_c5",
    "source_id": "增强提示学习的少样本文本分类方法_李睿凡",
    "text": "3.3 实现细节与评测指标\n\n实验在配有CUDA环境的Linux操作系统下进行，并配置了两块GTX 1080Ti显卡。代码使用基于PyTorch框架的HuggingFace工具包实现。对于中文数据集的评测，采用12层网络结构的中文RoBERTa-wwm-ext预训练模型。对于英文数据集的评测，采用12层结构的BERT-BASE预训练模型。模型参数设置如下：学习率为10−5，超参α设置为0.7，β为0.01，γ为0.02，三元损失间隔∆为0.15，并且使用AdamW优化器进行模型参数的优化。依据之前的研究，在少样本学习问题中通常使用准确率(Accuracy)作为评测指标。它表示模型预测正确的样本数量占所有的样本数量的比例。\n\n3.4 实验结果",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 5,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 324,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "增强提示学习的少样本文本分类方法_李睿凡_s0_c6",
    "source_id": "增强提示学习的少样本文本分类方法_李睿凡",
    "text": "1) 中文数据集的实验结果。实验结果如表1所示。可以看到，对于基于微调的方法，在小样本学习场景中模型性能通过表现不佳。而对于采用基于提示学习的方法，通过使用PET、LM-BFF、EFL、P-tuningR方法以及EPL4FTC算法，在小样本学习场景中模型的准确率都有大幅提高，显示出提示学习方法具有强大的潜能。通过对比EPL4FTC算法与其他基于提示学习的方法(PET、ADAPET、LM-BFF、EFL和P-tuning等)，可以看出EPL4FTC算法在EPRSTMT、CSLDCP和TNEWS等数据集上取得了优异的成绩。此外，在IFLYTEK数据集上也取得了与其他现有方法同等效果的性能。而且，EPL4FTC算法在中文文本分类任务的平均准确率性能上取得了最高的成绩。与转换为完形填空任务形式的PET和ADAPET等方法相比，EPL4FTC算法在利用预训练模型中已经学习到的通用知识基础上，引入下游任务的类别信息实现更好的建模效果，并在任务的平均准确率上高出3.9%。与转化为文本蕴含任务的EFL方法相比，EPL4FTC算法没有引入额外需要学习的大规模参数，并且与预训练语言模型任务保持一致，有效减小上下游任务间的差异性，最终在任务的平均准确率上高出4.2%\n。与使用自动构建模板或是非自然语言形式模板的LM-BFF和P-tuning方法相比，EPL4FTC算法无需繁琐的模板构建形式，并且在任务的平均准确率上高出1.6%。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 6,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 613,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "增强提示学习的少样本文本分类方法_李睿凡_s0_c7",
    "source_id": "增强提示学习的少样本文本分类方法_李睿凡",
    "text": "表1 中文少样本数据集实验结果\n\n2) 英文数据集的实验结果。本文英文数据集的训练集中每一类别包含不同规模的实例数量(K=8, 16和32)。实验结果如表2所示。从结果可以看出，对于不同的实例数量，基于微调的方法、PET、ADAPET、EFL、P-tuning以及EPL4FTC算法，都表现出随着实例数量的不断增多，模型的准确率都有着明显的提升。这表明在基于深度模型的少样本学习场景中，训练数据的规模对模型性能有着较大影响。其次，在实例数K=8时，虽然PET、ADAPET、EFL和P-tuning等基于提示学习的方法比基于微调的方法模型的准确率有很大提升，但EPL4FTC算法却表现出更加出众的性能，其模型准确率远高于其他方法。这表明在给定较少实例的情况下，EPL4FTC算法能够有效地对下游任务进行建模，也进一步说明了该算法的有效性。进一步，随着实例数的增加(K=16, 32)，虽然其他基于提示学习方法\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 7,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 409,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "增强提示学习的少样本文本分类方法_李睿凡_s0_c8",
    "source_id": "增强提示学习的少样本文本分类方法_李睿凡",
    "text": "---\n\n1) 推理词形式性能分析。EPL4FTC算法将文本分类任务转换为自然语言推理形式的完型填空任务，同时受P-tuning方法启发，比较了自然语言形式与非自然语言形式推理词的性能。实验结果表明，在中文和英文数据集上，非自然语言形式推理词表现出更稳定性能。对于简单、数据区分度高的任务，如EPRSTMT和TREC，自然语言形式推理词表现更佳；而对于类别多、复杂的任务，如TNEWS、IFLYTEK和CSLDCP，非自然语言形式推理词性能更优。\n\n2) 提示模板性能分析。本文评估了手工设计的提示模板对模型性能的影响。实验发现，模型性能受提示模板影响显著，尤其在英文TREC任务上，模板形式的不同导致性能差异较大。优化模板形式可显著提升模型性能。\n\n3) 可视化分析。通过t-SNE方法对中文TNEWS数据集进行可视化分析，验证了模型编码层学习到的类别信息。结果显示，CLS作为句子编码表示已学习到一定的类别信息，且度量优化模块为模型提供了额外的类别知识。\n\n提出的EPL4FTC算法利用三元组损失增强提示学习，通过句子和句群粒度的三元组损失优化，捕获下游任务的类别信息。实验证明了算法的有效性，未来将扩展至其他少样本任务场景及多语种文本分类研究。\n\n---\n\n[注：参考文献列表和其他非结构化内容在此次清洗中已被省略。]",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 8,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 9,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 562,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "引入深度学习的人工智能类课程_s0_c0",
    "source_id": "引入深度学习的人工智能类课程",
    "text": "引入深度学习的人工智能类课程\n\n李睿凡1,2王小捷1，钟义信1\n\n（1．北京邮电大学计算机学院，北京100876；2．北京邮电大学教育部信息网络工程研究中心，北京100876）\n\n摘要：深度学习是人工智能领域的最新进展，取得了显著的研究成果和应用进展。本文提出了在人工智能类课程中引入深度学习的初步内容和实施建议，分析了其必要性和可行性。\n\n关键词：人工智能；深度学习；教学建议\n\n1 深度学习背景\n\n深度学习在语音识别、计算机视觉等领域取得了突出进展。2006年，Hinton教授提出深度学习，成为研究热点。从学术界与工业界的研究态势看，深度学习已成为人工智能领域的研究热点。\n\n2 必要性与可行性\n\n2.1 必要性\n\n（1）深度学习是人工智能的前沿。\n（2）深度学习是人工智能的突破。\n（3）深度学习是人工智能的延伸。\n（4）深度学习是学生的潜在兴趣点。\n\n2.2 可行性\n\n（1）深度学习与现有人工智能联系密切。\n（2）深度学习的基本内容并不深。\n（3）深度学习的资料容易获得。\n\n3 实施建议\n\n在教学中适当安排深度学习的基本内容，根据教学对象调整。本科生高年级专业课可安排1学时，介绍层次训练的基本算法；研究生可根据课程主题安排4～6学时，包括波尔兹曼机、深层信念网络等内容。增加小规模实验辅助理解，课件可通过对优质资料修改得到。\n深度学习在人工智能领域取得的进展举世瞩目。建议将深度学习引入人工智能类课程，共同推进教学工作。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 3,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 618,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "引入深度学习的人工智能类课程_s0_c1",
    "source_id": "引入深度学习的人工智能类课程",
    "text": "参考文献：\n[1] 觚l I，Rose D C，Kamowsl【i TP．Deep machine Learning：a new frontier in artificial intelligence research[J]．IEEE Compumfional Intelligence Magazine，2010，5(4)：13-18．\n[2] Dalai G E，Sainath T N，Hinton G E．Improving deep neural networks for LVCSR using rectified linear units and dropout[C]／／2013IEEE Imemafional conference on acoustic speech and signal processing，2013．\n[3] Krizhevsky A，Sutskever I，Hinton G．ImageNet classification with deep convolutional neural networks[J]．Advances in Neural Information Processing Systems，2012(25)：1 106-1 114．",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 548,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "引入深度学习的人工智能类课程_s0_c2",
    "source_id": "引入深度学习的人工智能类课程",
    "text": "[4] Hinton G，Salaldautdinov 1L Reducing the dimensionality of data丽tll neural networks[J]．Science，2006，5786(3 13)：504—507．\n[5] Haykin．Neural networks and learning machines[M]．New York：Prentice Hall，2008．\n[6] 钟义信．高等人工智能：人工智能理论的新阶段[J]．计算机教育，2012(9)：6．11．\n[7] Bengio YLearning de印architecturesfor AJ[J]．Foundations and Trendsin Machine Learning，2009，2(1)，1-127．",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 358,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "探索神经网络深度学习的教学_s0_c0",
    "source_id": "探索神经网络深度学习的教学",
    "text": "探索神经网络深度学习的教学\n\n摘要：深度学习是智能科学与技术领域的突破性进展。文章讨论如何在本科生与研究生课程中有效开展深度学习教学，介绍智能领域的最新研究成果，以期学生能较早接触到学科前沿，提升对智能科学与技术的兴趣，激发创新精神。\n\n关键词：智能科学与技术；深度学习；教学建议\n\n1 深度学习背景\n\n深度学习旨在构造具有多层结构的可学习神经网络。自2006年Hinton教授提出深度学习概念以来，该方法在多个领域取得突破性进展。学术上，深度学习已从神经网络领域扩展到众多领域，并在工业界得到广泛应用。\n\n2 教学建议\n\n2.1 本科生\n\n针对本科生，建议在高年级智能类课程中加入深度学习教学单元，主要内容包括多层感知器、经典后传算法、自编码器与无监督的特征学习。\n\n2.2 研究生\n\n研究生课程应使学生深入理解深度学习的基本出发点和主要内容，激发研究兴趣。建议课程包括基础知识、深度神经网络基础、论文阅读列表等部分，并强调学以致用，开展课堂教学、编程实践、项目研究。\n\n通过以上教学内容，学生能掌握深度学习核心内容，理解其研究动机，学会解决视觉、语音、语言等应用问题，提升创新意识。期望本研究能引起同行共鸣，共同推进深度学习教学工作。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 519,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "智能科学与技术专业本科生导师制的研究与实践_s0_c0",
    "source_id": "智能科学与技术专业本科生导师制的研究与实践",
    "text": "文章针对如何实施智能科学与技术专业本科生导师制进行研究，基于北京邮电大学智能科学技术中心对智能科学与技术专业本科2、3年级学生试行导师制的实践，探索适合于本科2、3年级学生的导师制工作形式和办法，给出智能科学与技术本科专业本科导师制与其他教育模式的结合建议，提出相关质量评价方法。\n\n关键词：智能科学与技术；本科生导师制；工作形式和办法；质量评价方法\n\n导师制(Tutorial System)是重要的教育模式。现代形式的导师制兴于19世纪的牛津大学，之后在英国、美国的一些大学得到实施和变革。国内一些高校，如北京大学、浙江大学等也都在探索和实施导师制。但已有的导师制多在数学、计算机、文学等专业内实施，还没有在智能科学与技术专业实施的相关研究。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 16,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 323,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "智能科学与技术专业本科生导师制的研究与实践_s0_c1",
    "source_id": "智能科学与技术专业本科生导师制的研究与实践",
    "text": "2005年教育部首次批准在国内高校开设智能科学与技术专业。与一些传统的专业相比，智能科学与技术专业是具有高度交叉学科特点的新专业，其专业基础是计算机科学、生命科学、认知科学等学科基础的有机融合，其专业课程还会广泛涉及哲学、语言学、神经科学、机器人学等相关学科。与此同时，由于近年来社会经济发展对智能科学与技术需求的增加，导致智能科学与技术专业的理论和技术体系在还没有完全建立时就需要应对广泛的应用需求。这就导致了在智能科学与技术专业教育中存在的2个非常尖锐的问题，一是本专业缺乏经过时间考验的教材体系，同时由于其学科交叉性，新的具有知识交叉特点的课程较多，对同学的学习方法、学习能力和思维方式提出了更大的挑战；二是智能科学与技术专业有更为广泛的基础课程需要深入学习，但同时在很多局部方面有急迫的应用需求以及实施的可能，因此，如何在基础理论和技术学习号陕餐式的应用问题暂时解决方案间寻找到合适的平衡，如何在异常广博的智能科学与技术基础和专业知识海洋里找到自己个性化兴趣，对教师与 学生而言都是一个挑战。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 450,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "智能科学与技术专业本科生导师制的研究与实践_s0_c2",
    "source_id": "智能科学与技术专业本科生导师制的研究与实践",
    "text": "为有效克服智能科学与技术专业教育中存在的上述问题，有效的方法是在教师与学生间紧密沟通。教师们首先研究在本科2、3年级中的导师制。由于1年级主要还是数理基础课程，和专业的接触相对很少，而在4年级毕业设计 中，每位同学都有导师带领。在本科2、3年级，学生们已逐渐消除进入大学时的兴奋，开始进行较为理性的思考，因此需要给予更多思想上的交流与引导。\n\n笔者主要针对如何更好地实施智能科学与技术专业本科生导师制进行研究，参考了一些相关的文献[1-9]，结合北京邮电大学智能科学与技术本科专业导师制2年多的实施实践。此前，已经有《智能科学与技术专业本科生导师制及复合型人才培养》对导师制的相关内容进行研究，笔者在该篇论文的基础上，继续对导师制的实施方法进行研究，希望与相关专业人员共勉。\n\n1 探索适合3年级学生的导师制工作办法",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 358,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "智能科学与技术专业本科生导师制的研究与实践_s0_c3",
    "source_id": "智能科学与技术专业本科生导师制的研究与实践",
    "text": "1 探索适合3年级学生的导师制工作办法\n\n首先，需要确定在智能科学与技术专业3年级中实施导师制对于学生进行引导的基本原则，结合3年级专业基础课和专业课的学习引导学生步入相关的学科领域，跟进了解实验室的相关研究项目，促进理论知识学习与科研项目实践相结合，帮助学生在较好的通识教育基础上进一步树立较强的专业认知和思维。对于学有余力的同学，可以鼓励他们参加项目研究工作，为即将到来的考研、实习、毕业设计、找工作等人生的重要选择做准备，提高学生自身的自信心及解决复杂问题的能力。同时，帮助学生继续完善综合素质和良好心态的培养。基于此，对3年级学生导师制工作的实施办法给出如下建议：\n\n(1)导师与学生根据需要交流的话题进行日常的见面沟通，可以是导师了解近期学生的基本情况，学生了解感兴趣的导师项目情况，双方针对学生提出的问题或者导师提出的问题有针对性地进行讨论。日常沟通以外的时间，建议学生有问题可以随时通过手机或者电子邮件联系导师。\n\n(2)3年级学生进入了专业课程的学习，开始大量接触专业基础课和专业课。在此阶段，导师首先需要帮助学生理顺专业课程之间的关系。为学生介绍一些专业相关的前沿发展动态，介绍后续可能的专业方向，各个专业方向对国民经济或科技发展的支撑作用，为其在4年级选择专业选修课和专业方向打下基础。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 553,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "智能科学与技术专业本科生导师制的研究与实践_s0_c4",
    "source_id": "智能科学与技术专业本科生导师制的研究与实践",
    "text": "(3)学生在3年级开始会具体考虑是考研还是工作等个人未来发展方向问题，导师应给与个性化分析，为学生进行自主选择提供建议参考。对于决定考研、找工作或出国的学生，导师可依据自己的经验或以前学生的案例提供参考建议。\n\n(4)导师应继续关系学生的心理和生活状态。3年级的学生心理相对比较成型，导师主要是给出一些建议，在适度引导的基础上培养其自主判断和选择能力。\n\n(5)在学生学有余力的前提下，可以让其加入到一些项目组的讨论中去，使其对未来专业研究或工程项目有所了解。\n\n(6)考虑到本科导师的实际情况，特别是有的导师同时也做研究生导师，日常研究生指导工作已经比较繁忙，加之固定的教学任务以及4年级的本科毕设任务量也很饱满，相对而言，能够投人到本科导师上的时间和精力都比较有限，因此建议可以邀请研究生对本科学生进行辅助指导。大部分研究生都是直接或间接从本科学习进入到研究生学习阶段的，有着新鲜的经 验和视角，可以成为本科学生良好的咨询对象。另一方面，研究生都要参加实验室的项目，具备较强的专业知识和项目研究实践经验，也可以在专业技能形成方面给本科学生提供良好的建议和榜样。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 482,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "智能科学与技术专业本科生导师制的研究与实践_s0_c5",
    "source_id": "智能科学与技术专业本科生导师制的研究与实践",
    "text": "(7)注重发挥学生的主动性，尊重学生的自主意识。实际上，导师制包含了另外几种教学方式的 结果。导师与一般意义上的教师是不同的，传达信息不是他的工作职责，所需要的基本信息应该由学生来收集整理，导师只作为一个引导者，一个提供建设性意见的批评家，通过间接帮助使学生提炼整理所需的知识，或鼓励学生尝试找出必要信息，而不是直接告诉学生。例如在专题研究方面，导师可以给学生介绍正在进行的项 目研究，并把相关资料发给学生阅读，鼓励学生们从中找到自己感兴趣的内容。\n\n(8)3年级学生已经学习了较多的公共基础课、专业基础课以及专业课的课程知识，具备了基本的专业素质和创新基础。目前高校中每年都要举办各种各样的竞赛活动，以促进大学生的综合素质和创新能力等的培养，而3年级同学都是参赛主力。如果能结合相关的竞赛和学生的兴趣点加以鼓励和引导，能得到事半功倍的效果。因此，建议本科导师制工作可以密切结合竞赛报名。\n\n2 探索适合2年级学生的导师制工作办法",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 5,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 415,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "智能科学与技术专业本科生导师制的研究与实践_s0_c6",
    "source_id": "智能科学与技术专业本科生导师制的研究与实践",
    "text": "2 探索适合2年级学生的导师制工作办法\n\n同样需要明确在2年级学生中实施导师制的基本原则，学生以课程学习为主，导师应帮助学生进一步全面系统地了解各门课程的必要性和重要性以及与今后专业课程之间的关联。帮助学生进行自我认识和自我规划能力的培养，从而正确认识学校学习和社会发展之间的关系，引导学生形成积极的人生观与价值观。鼓励学有余力的学生能够跟进了解导师正在进行的实验室的研究项目。因此，2年级学生的导师制工作可以侧重从以下几个方面进行：\n\n1)指导学生构建专业知识结构。\n\n向学生介绍智能科学与技术专业的课程结构，一方面说明每门课在专业课程中的位置与作用(为什么上这门课)，另一方面说明不同课程之间的知识关联(如何学这门课)，进而帮助学生理清学科知识结构，让学生在选课与学习过程中，少一些迷茫与无措。\n\n智能专业的本科2年级学生主要还是处于打基础的阶段。学生的课程学习是整个大学学习环节中非常重要的阶段。学生通过大学1年的学习，对大学的学习已经有了些认识，对于专业的认识也通过智能科学与技术的导论课程有初步的认识。学生的专业学习与研究的热情被激发出来，是大学2年级学习的重要目标。这样，导师主要需要考虑如何引导学生，让他们对与大专业计算机和小专业智能科学与技术有具体的认识。\n\n2)帮助学生了解行业发展与前景。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 6,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 554,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "智能科学与技术专业本科生导师制的研究与实践_s0_c7",
    "source_id": "智能科学与技术专业本科生导师制的研究与实践",
    "text": "2)帮助学生了解行业发展与前景。\n\n注重课程理论与实际应用相结合，尤其是帮助学习了解行业的最新发展与前景，让学生了解课堂所学如何为行业发展做出实际贡献。可结合具体的行业应用实例(例如，对于喜欢电脑游戏的同学，可以启发其思考游戏的设计原理与实际课程知识间的关联)，培养学生独立思考、动手讽研的能力，提升其理论联系实际的能力。\n\n3)培养学生的学习兴趣。\n\n灵活的导师制培养方案，便于导师动态地掌握学生的学习、兴趣、心理等方面的变动情况。打破传统的以课本为基础的“传授式”教育，以及以完成课题为目标的“任务式”教育，通过多样灵活的指导方式，帮助学生化解学习中的迷津。同时注意沟通艺术，培养学生发现科学的魅力，激发学生的学习兴趣，引导学生构筑良好的学习习惯，增强学生自我管理、自我规划的能力。学生与导师的联系，更多是解答学生在学习与生活中的困惑。可以通过邮件、电话、面谈等多种方式。学生在交流过程中有想法，也可以记下来。\n\n4)关注学生人格发展。\n\n2年级的同学心理相对还不够成熟，导师需要多关注其人格发展的状态，适度提出一些建议。具体的建议不一定适用，在总体上应是引导同学向积极、自信的方向发展。\n\n5)特别推荐高年级携带。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 7,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 510,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "智能科学与技术专业本科生导师制的研究与实践_s0_c8",
    "source_id": "智能科学与技术专业本科生导师制的研究与实践",
    "text": "5)特别推荐高年级携带。\n\n3年级的学生作为2年级学生的前辈，可以充分发挥多体验一年的优势，对2年级学生进行多方面指导，如选课、课程学习、考试、课题、实验、甚至业余生活等。考虑到本科导师制工作是同时面向3年级和2年级学生实施的，其天然资源就是高年级对低年级学生的帮带作用，因此建议导师考虑对低年级学生增加高年级学生的协助。\n\n3 导师制与其他教育模式的结合\n\n目前，高校本科学生的日常组织形式是以班为单位，是为班建制；而课程学习是以学分为单位，是为学分制。班建制、学分制和导师制与是当前的3大教育模式，各有其特点和优势。实施本科生导师制并不是要取消班建制和学分制，而是要与班建制和学分制有机结合起来，首先需要明确的是，导师制与学分制并无特定直接的关联，导师制的实施可以更好地促进学分制，因为学分制的主要内容是不同课程的教 育，而导师制的有效实施可以让学生对课程教育形成更好的理解和兴趣投入。\n\n接下来，导师制和班建制的有机结合，有效弥补了班建制在学生个性化培养方面的不足。以班为单位的课堂教学活动是学校实施人才培养的重要方式，分班进行授课可以有效提高教师资源的利用率，班建制可以培养同学们的集体观念，增强交流和协作能力。但班建制不可避免地对因材施教的个性化教育带来了困难。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 8,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 536,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "智能科学与技术专业本科生导师制的研究与实践_s0_c9",
    "source_id": "智能科学与技术专业本科生导师制的研究与实践",
    "text": "在导师制中，以班为单位的课堂教学活动依然是一项重要方式。以北京邮电大学智能科学与技术专业为例，大部分导师同时也都负责了该专业 的专业基础课以及专业课的课堂教学工作，例如智能科学技术导论、脑与认知科学基础、机器智能、机器学习、模式分析、科技史与方法学、智能机器人、信息科学原理、智能科学与技术前沿讲座等。和其他老师一样，教师要安排好课堂教学活动，而与此同时，这些教师又是导师，在课堂之外要与同学进行深入的交流。这些交流能有效地帮助导师更深入地了解学生，研究学生发展的特点，因此，能帮助他们在课堂教学中更有效地指导学生、促进学生个性化健康发展。同时，还能依据学生在课程学习上表现出的特点，安排在课堂外的更多引导。\n\n总之，课堂内外“教与导”的结合互相补充、互相促进，不仅传授学生课程知识，而且能够启发学生的钻研精神、提出问题及解决问题的能力、激发学生的学习兴趣。一些具体的实施情况以及建议如下：",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 9,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 395,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "智能科学与技术专业本科生导师制的研究与实践_s0_c10",
    "source_id": "智能科学与技术专业本科生导师制的研究与实践",
    "text": "(1)结合导师制学生们提出的“我们为什么要学习这些课程?”“这些课程会对本专业的学习有什么具体作用?”等相关问题，在授课过程中增加各课程相关的介绍，包括这些课程的内容在智能科学与技术专业知识结构中的地位和作用，智能时代中社会对于智能科学与技术人才大军的需求等，并结合一些研究生和往届毕业本科生的实例加以说明。换言之，高校人才培养应当面向世界，面向未来，引导学生在打好基础的同时强化开拓意识和创新能力，以适应未来社会的需求。目前，智能科学技术的发展正处在理论突破的关键阶段。因此，需要特别高度关注理论发展的大势，以此来调整教学研究的方向，优化教育者和受教育者的知识结构。\n\n(2)根据学生提出的希望更多了解智能科学与技术领域相关科研情况的要求，教师在实际教学中结合具体课程具体章节内容进行相关科研情况的延伸和拓展，使学生除能够掌握具体的知识点以外，还能看到这些知识点在经过深入研究或与其他技术结合之后，可以实现很多具有理论意义和实用价值的研究成果，从而增强其学习兴趣。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 10,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 433,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "智能科学与技术专业本科生导师制的研究与实践_s0_c11",
    "source_id": "智能科学与技术专业本科生导师制的研究与实践",
    "text": "(3)在精一t2,安排给学生传授更多相关知识的 同时，我们也结合导师制的培养目标，对课程的讲授方式和考核方式进行了调整尝试。任课教师不仅讲授课程知识，而且精心布置可以将课程知识与实际应用结合起来的实践环节，引导学生利用课堂所学，提出问题、解决问题，系统地进 行科学研究的基础能力的培养与锻炼。从而达到“教与导”有机结合，相互促进，得到学生的一致好评。例如机器智能是智能科学与技术本科专业的专业基础课，我们设计使用了启发式教学方法，包括专题文献调研、演讲、讨论、动手实验、学生评价等，都得到了学生的积极反馈和高度评价，在培养学生主动学习、拓展知识、培养兴趣等方面取得了很好的效果，本专业学生连续3年结合所学专业知识积极参加全国智能设计竞赛就是很好的例证。事实上，这些内容也就是研究本科导师制的角色定位内容，包括课程教育指导、学术研究与创新能力培养、发展规划指导、创新精神培养等，希望能在平时的专业课教学过程中融人本科导师制的思想与内容，从而促进本科导师制与专业课程建设的相互联系，共同为高校优秀人才培养的最终目标服务。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 11,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 458,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "智能科学与技术专业本科生导师制的研究与实践_s0_c12",
    "source_id": "智能科学与技术专业本科生导师制的研究与实践",
    "text": "通过上述实践，学生和教师们普遍感觉到不错的效果。学生们对于专业学习更加具有浓厚的兴趣，大大增强了自信心和专业幸福感，也带动了专业视野、人生规划、生活态度、学习生活习惯等各方面的综合提升。教师们的教学科研也能更加顺畅地展开，增强了与学生的互动，甚至可以从学生身上得到很多有益的启发，可谓教学相长。\n\n4 质量评价方法\n\n项目执行的效果如何需要质量评价方法的保证。教师们从多方位、多角度地进行，力求准确地反映真实情况。具体包括以下方法。\n\n1)学生评价。\n\n学生评价无疑是各种评价中分量最重的部分。建议定期搜集与整理学生关于导师制工作的反馈，通过获得来自学生的第一手资料，从中发现学生对于指导教师、培养办法及内容等方面的心得与建议。指导教师可以有针对性地改进自身的培养工作，真正做到因人施教，发现问题及时解决。以北京邮电大学进行的本科导师制试点工作的2、3年级学生效果评价和意见采集为例，综合而言，大多数学生对于效果评价较好，不仅在课程学习上，还有专业理解、未来规划、新知识学习、研究经验等方面。同时，也存在一些问题，主要反映在沟通的主动陛和工作学习太忙碌方面，这些问题对于今后导师制的实施具有参考价值。\n\n2)导师自评。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 12,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 509,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "智能科学与技术专业本科生导师制的研究与实践_s0_c13",
    "source_id": "智能科学与技术专业本科生导师制的研究与实践",
    "text": "2)导师自评。\n\n在导师制实施的同时需要定期进行导师自评，展开导师制培养工作专题讨论。指导老师根据自己在实际指导中遇到的问题，进行切磋讨论，互相学习与借鉴良好的方式方法。针对不同年级、不同学生，要求老师因人施教，制订单个学期的指导方案，并按照方案实施具体指导，同时还要记录每次的交流与指导活动，旨在及时发现问题、解决问题，积累切实可行的指导经验。\n\n3)观察员评价。\n\n观察员从项目实施的外部角度给出客观评价和建议。随着实施过程的深入，可以逐步明确导师制在各阶段的具体目标，比如在低年级引导学生从心理上、能力上适应大学生活，在高年级引导学生对专业知识体系、专业未来发展的系统理解，能积极主动地规划未来。并在未来进一步依据导师制的目标制定一些导师制成功与否的基础评价指标，比如学生心理健康水平、学生专业理解水平、学生专业能力水平等。例如北京邮电大学本次实施的观察员钟义信教授已给出多个重要的评价和建议，如认清导师制的根本目的，明确导师的角色定位，扩大导师制的实施对象范围，继续深化面向各个年级的导师制工作方法的研究。\n\n4)学院与学校管理部门评价。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 13,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 473,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "智能科学与技术专业本科生导师制的研究与实践_s0_c14",
    "source_id": "智能科学与技术专业本科生导师制的研究与实践",
    "text": "4)学院与学校管理部门评价。\n\n请学院管理学生的相关部门给出评价，包括学院负责本科工作的副院长、负责学生工作的副书记、教务科老师等。例如北京邮电大学计算机学院副书记认为，在学业上导师制是非常受欢迎的，在心理辅导、思想、品德上可以和辅导员结合起来。类似建议对于深化导师制的研究和实施具有参考价值。\n\n5)毕业生用人单位评价。\n\n在参加导师制工作的学生毕业后，联系学生就业的企业进行调查反馈。考虑到本科就业学生还有多种去向，如果还有很多学生会继续在高校或出国读研究生，教师将建议联系到学生就读的院校进行调查。\n\n导师制的实施办法研究只在智能科学与技术本科专业2、3年级学生中进行，今后的工作除继续在2、3年级中继续实施和完善导师制之外，希望将导师制的研究实施范围扩大到1年级和4年级，1年级学生刚从高中进入大学，面临着学习生活环境的较大变化，非常需要导师的指导，而4年级学生面临毕业、读研究生、就业等多种人生重大选择，也需要导师的指导，这会使导师制面临更多、更大挑战。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 14,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 432,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "智能科学与技术专业本科生导师制的研究与实践_s0_c15",
    "source_id": "智能科学与技术专业本科生导师制的研究与实践",
    "text": "钟义信．设置“智能科学与技术”博士学位一级学科：必要性、可行性、紧迫性．计算机教育，2009(11)：5-9．\n王万森，钟义信，韩力群，等．我国智能科学技术教育的现状与思考．计算机教育，2009(11)：10-14．\n吴玉国，宋崇智．实行本科生导师制的实践与思考．安徽工业大学学报：社会科学版，2009(11)：139．\n潘春燕．复合型人才及其培养模式的构建思考．学理论，2010(7)：146-148．\n钟义信．时代召唤智能科学技术人才大军．计算机教育，2011(8)：2-7．\n谭咏梅，王小捷，钟义信．模式识别课程的教学探索．计算机教育，2011(8)：76-79．\n李蕾，刘平安，王小捷，等．机器智能课程教学方法探讨．计算机教育，2011(8)：104-107．\n钟义信．高等人工智能：人工智能理论的新阶段．计算机教育，2012(9)：6-11．\n袁彩霞，李蕾，王小捷，等．智能科学与技术专业本科生导师制及复合型人才培养．计算机教育，2013(7)：38-41．",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 15,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 16,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 434,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "智能科学技术导论教学目的及策略_s0_c0",
    "source_id": "智能科学技术导论教学目的及策略",
    "text": "智能科学技术导论教学目的及策略\n\n周延泉，李睿凡，焦晨晨\n（北京邮电大学计算机学院，北京100876）\n\n摘要：针对智能科学技术导论作为智能科学与技术专业基础课的必要性和重要性，分析教学现状，探讨课程结构设置及教学策略。\n\n关键词：智能科学技术导论；教学目标；教学策略\n\n1 课程概述\n\n21世纪信息科技全球发展，智能科学技术成为核心科技之一，广泛应用于各个领域。智能科学与技术专业教育对培养高水平人才至关重要。\n\n智能科学技术导论旨在使学生掌握智能现象及其运用规律，培养具有基本理论知识、技能和创新能力的复合型人才。\n\n2 课程整体设计及目标\n\n智能科学技术导论作为大一新生第一学期课程，旨在引导学生进入智能科学领域，奠定后续学习基础。课程应起到专业引导作用，普及基础知识，激发学习兴趣，并通过实践加深理解。\n\n3 课程教学存在的问题及思考改进\n\n3.1 改革教学模式\n\n教师应采用多种教学模式，结合科研项目，提高学生对理论知识的直观认识。\n\n3.2 采取课堂与实验室结合教学\n\n实验室可帮助学生结合实际应用，深入理解专业知识，提高学习效率。\n\n3.3 建立多样化、全方位的考评模式\n\n采用小组讨论答辩、实践创新等多样化考评方式，避免单一卷面考试束缚学生创新能力。\n\n通过改革教学模式、结合实验室教学和多样化考评，旨在提高智能科学技术导论课程的教学效果，激发学生学习兴趣，为后续学习打下坚实基础。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "深度学习中卷积神经网络的教学探讨_s0_c0",
    "source_id": "深度学习中卷积神经网络的教学探讨",
    "text": "深度学习中卷积神经网络的教学探讨\n\n李睿凡1，陈佳洁2，周延泉1，王小捷1，钟义信1\n\n（1．北京邮电大学计算机学院，北京100876；2．河北省霸州市第四小学，河北霸州065700）\n\n摘要：深度学习是智能科学与技术领域的最新突破性进展，卷积神经网络是其中一个代表性工作。文章探讨如何开展卷积神经网络的教学工作，包括教学内容的安排和教学内容之外的考虑两个方面，旨在将智能科学与技术的这一最新成果介绍给学生，使他们能较早接触学科前沿，提升学习兴趣，激发创新动力。\n\n关键词：智能科学与技术；深度学习；卷积神经网络；教学建议",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 261,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "深度学习中卷积神经网络的教学探讨_s1_c0",
    "source_id": "深度学习中卷积神经网络的教学探讨",
    "text": "当前，智能科学与技术在国家经济与社会需求中的作用愈发重要。从大学教育的角度，智能科学与技术专业是培养“智能”人才的重要基地。传统的人工智能专业课程主要包括人工智能导论、模式分析、机器学习、数据挖掘等，但近些年，深度神经网络的发展使我们面临新的机遇与挑战。特别是深度学习中的卷积神经网络的发展是深度学习中的一个亮点。\n\n2. 卷积神经网络背景\n\n2006年之前，人工神经网络的发展大致可以分为两个时期。1943年，McCulloch和Pitts提出了最早的人工神经元。1969年，Minsky和Papert分析了感知器神经网络模型的局限性。80年代中期，John Hopfield提出了Hopfield神经网络模型，同时，多层前向神经网络的反向传播算法被重新发现。2006年，Geoffrey Hinton教授与其博士生Salakhutdinov博士提出了深度学习。\n\n3. 教学内容编排\n\n研究生课程主要涵盖3部分内容：机器学习基础、神经网络基础、神经网络研究论文讨论。第一部分的机器学习基础知识部分主要让学生从零起点顺利过渡到这门课程。第二部分的神经网络知识部分主要包括前向神经网络及后传算法和卷积神经网络。第三部分主要是学生阅读讲解和师生互动的论文研讨部分。",
    "section_title": "引言",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 1,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 2,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 531,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "深度学习中卷积神经网络的教学探讨_s1_c1",
    "source_id": "深度学习中卷积神经网络的教学探讨",
    "text": "除理论教学环节之外，该课程强调学以致用。因此，我们给出两部分内容：一个是基本实验项目，重点强调卷积神经网络的训练和应用实验；另一个是强调如何将卷积神经网络方法用于解决实际问题。\n\n4. 教学内容之外的考虑\n\n除教学内容之外，还有一些值得考虑的与卷积神经网络相关的问题。我们主要考虑了如下3点：\n\n(1)如何激发学生的兴趣。\n(2)教师如何使用恰当的生活语言辅助专业技术的交流。\n(3)调动学生的好奇心能有效地让学生进入良好的学习状态。\n\n卷积神经网络是深度学习中备受瞩目的研究主题，需要更深入的研究。需要注意的是，深度学习还处于发展状态，内容仍然不够成熟，这给教学工作者提出了更高的要求。",
    "section_title": "引言",
    "section_type": "introduction",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 2,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 2,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 294,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "计算机游戏中的智能技术_s0_c0",
    "source_id": "计算机游戏中的智能技术",
    "text": "智能信息处理 计算机游戏中的智能技术\n\n李睿凡 左申正 李卫\n\n计算机游戏中的智能技术\n\n李睿凡 左申正 李卫 北京邮电大学信息工程学院310信箱100876\n\nE-mail：iiruifan@tom．com zuoshenzheng@1 63．corn\n\n摘要：计算机游戏是计算机应用的一个热点领域。而人工智能技术则是被认为是计算机图形学在计算机游戏中发展到 极致后极具潜在推动力的一个技术。传统认为游戏AI与学术AI有较大差别。但近来的发展表明，学术人-T智能技术将为游 戏产业的发展注入新的活力。本文从游戏设计元素出发，讨论它们与人工智能之间的关系。然后介绍游戏中相对成熟技术， 如A奉算法、有限状态机、群体寻径，再讨论近来游戏开发者关注的问题，如适应与学习。\n\n关键词；人工智能有限状态机群体智能神经网络遗传算法\n\nAn Overview ofArtificial Intelligence in Computer Games\n\nRuifan LI Shenzheng ZUO Wei LI\n\nSchool ofInformation，Beijing Universiyt ofPosts and Telecommunications,mailbox 310,100876",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 10,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 540,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "计算机游戏中的智能技术_s0_c1",
    "source_id": "计算机游戏中的智能技术",
    "text": "E-mail：Liruifan@tom．corn Zuoshenzheng@tom．tom liw@nlu．caai．gn\n\nAbstract：Computer game iS all active field in computer science．Compaled with computer graphics，artificial intelligenee(AI)\n\niS taken as potentiaIIocomotion for computer games．In general，academic AI iS thou曲t as dilyerent from game AI．However，recent\n\nprogress has shown that academic A1 will spark new elements in computer games．In this paper,we discussed the relationship between elements in computer games and artificial intelligence．Then some techniques，including A‘algorithm，finite state machine。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 10,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 562,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "计算机游戏中的智能技术_s0_c2",
    "source_id": "计算机游戏中的智能技术",
    "text": "path-finding based on swarm,and some problems attracting game·developer's interest,such as adaptivity and learning ale discussed．\n\nKey Words：Artificial Intelligence Finite Smte Machine Swarm Intelligence Neurai Network Genetic Algorithms\n\n1．引言 游戏具有相当长的发展史。广义上，可以认为 游戏的产生是伴随着人类的产生、发展而不断进步\n\n的。古人类之间的嬉戏就是一种朴素的游戏活动。 据史料记载，早在公元前4000年时，上古的尧发\n\n明了一种流传至今的游戏一围棋【l】。而计算机游 戏是现代产物，它是在电子技术、信息技术的发展\n\n中诞生的。1950年，信息论之父Shannon就提出用\n\n里。J下如在物理学发展史上出现的“两朵乌云”一样， 计算机游戏也j下面l瞄一次新的考验。也就是，业界 普遍认为，在图形技术给游戏玩家带来的感官极致 之后，人工智能技术将取而代之成为下一个推动整\n\n个游戏产业的支柱技术f5，6】。\n\n2．游戏元素与人工智能\n\n日前，巾．面上的游戏不计其数，种类繁多，形 式多样。但考察不同的游戏可以发现它们具有一般",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 10,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 587,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "计算机游戏中的智能技术_s0_c3",
    "source_id": "计算机游戏中的智能技术",
    "text": "个游戏产业的支柱技术f5，6】。\n\n2．游戏元素与人工智能\n\n日前，巾．面上的游戏不计其数，种类繁多，形 式多样。但考察不同的游戏可以发现它们具有一般\n\n论文发表在：中国人工智能学会2007年全国学术年会(CAM’12)\n\n计算机编程方式实现人机对弈的构想，并发起了棋\n\n牌游戏的研究[2】。这是计算机游戏的发端。1984 年，Cris Crawford出版了名为《The Art ofComputer Game Design))的第一本关于计算机游戏的书籍[3】， 标志着计算机游戏时代的来临，是计算机游戏史上\n\n的～个重要里程碑。\n\n目前，在芯片技术、网络技术和图形技术的催 化下，计算机游戏已经在全球范围内发展成为一个 具有每年数十亿美元的产业。据(2003年度中国游\n\n戏产业报告》，中国网络游戏的市场预计在2007年\n\n将达到67亿元人民币【4】。计算机游戏已成为一种 普遍的娱乐方式。但是，游戏的发展也并非晴空万\n\n化的形式元素。Crawford的建议如下【3】：1)表 现：游戏是设计者根据现实而主观抽象出的一个虚 拟世界；2) 交互：游戏是玩家与计算机互动的过 程；3) 冲突：玩家为完成游戏设定的目标而努\n\n力，同时也受到来自游戏设计的阻力；4)安全：危\n\n险源于游戏冲突所产生的心灵体验；游戏是体验现 实世界的～种安伞方式。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 10,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 572,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "计算机游戏中的智能技术_s0_c4",
    "source_id": "计算机游戏中的智能技术",
    "text": "力，同时也受到来自游戏设计的阻力；4)安全：危\n\n险源于游戏冲突所产生的心灵体验；游戏是体验现 实世界的～种安伞方式。\n\n这些形式元素表达了游戏的特点，当然也可以 有其他的方式表现游戏的特征，它们都是殊途同归\n\n的。形式元素是设计游戏时候必须考虑的。不论游 戏的形式如何，本质上是人参与的一种活动，其最 终目的是为人娱乐，或者是教育目的。\n\n图1三段路径体系\n\n关f游戏的娱乐目的是显然的，而游戏曲教育 目的可能有些迷惑性。事实上．除了棋牌类菩游戏 之外，游戏的教育功能可能是蒯接的、潜移默化的。\n\n比如，H奉光荣公司出品的“大航海时代”系列游戏\n\n171就是一款以16世纪的航海为背景的寓教于乐的 好游戏。\n\n2 1游戏 I与学术 J Crawford认为计算机游戏是可以与电影并列的 一种新的艺术自忡}形式，这种观点也逐渐为认可。 而游戏中人工智能的水平很大程度上挟定了游戏 的是否能够被玩家和市场所接受。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 10,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 405,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "计算机游戏中的智能技术_s0_c5",
    "source_id": "计算机游戏中的智能技术",
    "text": "学术人工智能的一个普遍定义如下【8，9】：即一 种能有效地通过图灵测试的系统。这意味着被测试 的计算机能够就相l司问题提供与人矧样的答案。而 游戏开发者则将用于控制非玩家角色的代码认为 是人工智能，即游戏Al[10]。它确定了游戏对于玩 家输入动作的反应。这种反应可能是符合逻辑的， 也可能是随机的，甚垒是人为的愚蠢，但它们都是 游戏Al。造成游戏Al与学术AI差异的根本原困在 于前者是为游戏的根奉日的——螟乐服务的，而后 者的目的在于探讨智能本身。\n\n蝣戏AI的设计与实现是游戏设计巾的一个重 要环节。事实上，如果没有游戏AI，游戏也就失去 对抗性而不会成为令人愉悦的产品。游戏Al的工 作包含设计与实现两个过程；首先依据游戏奉身的 主题和目标确定游戏AI完成利么工作；然后钊对\n\n完成的工作确定实现的算法、程序。\n\n小同的辩戏类型所要求的AI设计太相径庭。 但可以大体将游戏Al的日标划分如卜110】 I) 挑战玩家：给玩家提供恰到盘『_处的挑战是游戏 AI设计的最基本目标：21建立基本真实感：强俗\n\n地说．就是避免一些显而易见的错误，如避免NPC 绕树行走的明显错误；31增加不确定性：夸玩家 无法准确预测游戏中NPC的行为：4) 辅助情 节的展开：让游戏AI变化游戏故事情节的发展；",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 5,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 10,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 547,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "计算机游戏中的智能技术_s0_c6",
    "source_id": "计算机游戏中的智能技术",
    "text": "5) 刨造逼真的t【|=界：游戏世界虽然是虚拟1H界， 世某些情竹上还需要相对的真实性，如天空中翱翔\n\ne2％ ⋯．“”盘2=\n\n。，=二2。 ‰W” !! ～⋯”” =。一 帮 ¨ 一 ⋯2’⋯～⋯\n\n在处理复杂变量和环境交互的情形下，可以通过模拟自然界的适者生存进化过程，探索出不同寻常的策略，有助于游戏中NPC的设计。N-gram模型通常用于语言建模，简而言之，它是在给定前n个词的条件下，预测第n+1个词的概率分布。该模型被文献【371】用于预测玩家行为，基于假设最近的过去对未来的行为具有最强的预测能力。\n\nN-gram模型的局部结构包括游戏限制、玩家风格、游戏奖励和游戏控制等，这些结构化的元素可以通过训练被N-gram模型识别，进而控制NPC的行为。\n\n不同的学习方法各有优势和不足，因此需要智能信息处理技术在计算机游戏中的应用。以下文献和作者提及了相关技术和方法：",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 6,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 10,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 390,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "计算机游戏中的智能技术_s0_c7",
    "source_id": "计算机游戏中的智能技术",
    "text": "- 【8】Rouse, Game Design: Theory and Practice\n- 【9】W. Steven, \"Game AI: The state of the Industry 2000-2001\"\n- 【10】B. Stout, \"The Basics of AI for Path Planning\"\n- 【11】D. Higgins, \"Generic AI Pathfinding\"\n- 【12】D. Higgins, \"Pathfinding Design Architecture\"\n- 【13】S. Rabin, \"AI Speed Optimization\"\n- 【14】D. Higgins, \"How to Achieve Lightning-Fast AI\"\n- 【15】T. Cain, \"Practical Optimization for AI Path Generation\"\n- 【16】S. Rabin, \"AI Aesthetic Optimization\"\n- 【17】M. Mika and C. Charla, \"Simple, Cheap Pathfinding\"\n- 【18】E. Dybsand, \"A Finite-State Machine Class\"",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 7,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 10,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 561,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "计算机游戏中的智能技术_s0_c8",
    "source_id": "计算机游戏中的智能技术",
    "text": "- 【18】E. Dybsand, \"A Finite-State Machine Class\"\n- 【19】S. Rabin, \"Implementing a State Machine Language\"\n- 【20】S. Rabin, \"Enhancing a State Machine Language through Messaging\"\n- 【21】M. McCuskey, \"Fuzzy Logic for Video Games\"\n- 【22】E. Dybsand, \"A Generic Fuzzy State Machine in C++\"\n- 【23】M. Zarozinski, \"Explosion in a Fuzzy System\"",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 8,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 10,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 332,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "计算机游戏中的智能技术_s0_c9",
    "source_id": "计算机游戏中的智能技术",
    "text": "此外，还有关于遗传算法、神经网络、粒子群优化等在游戏AI中的应用研究。\n\n- 【24】J. Robinson and Y. Rahmat-Samii, \"Particle Swarm Optimization in Electromagnetics\"\n- 【25】M. Dorigo, V Maniezzo, and A. Colomi, \"The Ant System\"\n- 【26】C. W. Reynolds, \"Flocks, Herds, and Schools: A Distributed Behavioral Model\"\n- 【27】S. Woodcock, \"Flocking: A Simple Technique for Simulating Group Behavior\"\n- 【28】T. Scutt, \"Simple Swarms as an Alternative to Flocking\"\n\n以上内容涉及了游戏AI设计的多个方面，展示了该领域的技术发展和研究动态。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 9,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 10,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 449,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向_智能科学与技术_专业的C语言教学探讨_s0_c0",
    "source_id": "面向_智能科学与技术_专业的C语言教学探讨",
    "text": "面向“智能科学与技术”专业的C语言教学探讨\n\n李睿凡，李蕾\n\n（北京邮电大学计算机学院智能科学技术中心，北京100876）\n\n摘要：本文探讨了“智能科学与技术”专业C语言教学的变革，针对新专业要求和学时压缩的问题，提出教学内容和方法改变的具体措施。引入机器智能前沿问题作为实践项目，旨在培养学生的专业兴趣和能力。\n\n关键词：C语言；智能科学与技术；教学\n\n“C语言程序设计”是众多工科专业的基础课程，现已扩展至非计算机专业。本文针对智能科学与技术专业的特点，研究C语言教学内容的整合设计、前沿研究导向的项目实践，并对教学相关因素进行讨论。\n\n传统教学强调语法规则，易忽视语言应用和程序设计思想。本文提出将课程分为语言学习和项目实践两大部分，各占一半学时。具体教学安排以《C程序设计》一书为参考，见表1和表2。\n\n表1教学环节内容说明\n\n表2实验环节内容说明\n\n此外，本文讨论了教材选择、双语教学、学生差异性和教学细节等因素对教学的影响。\n\n基金项目：国家自然科学基金项目\n\n作者简介：李睿凡，男，2006年7月在北京邮电大学获信号与信息处理专业博士学位，后留校任教，从事智能科学与技术方面的研究与教学工作。\n\n本文对“智能科学与技术”专业C语言教学进行了探讨，分析了教学内容变革与教学进程影响因素，为相关教学提供参考。\n\n参考文献：",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 3,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 566,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向_智能科学与技术_专业的C语言教学探讨_s0_c1",
    "source_id": "面向_智能科学与技术_专业的C语言教学探讨",
    "text": "本文对“智能科学与技术”专业C语言教学进行了探讨，分析了教学内容变革与教学进程影响因素，为相关教学提供参考。\n\n参考文献：\n\n[1]谭浩强．C高级语言程序设计[m]．2版．北京：清华大学出版社，2006．\n[2]陈良银，游洪跃，李旭伟．C语言程序设计[m]．C1999．北京：清华大学出版社，2006．\n[3]Brian W．K．，Denni S M．R．The C Programming Language．2版．北京：机械工业出版社，2006．\n[4]TopCoder．ht tp：／／wwW．topcoder．com／．\n[5]蔡自兴，徐光秸．人工智能及其应用(研究生用书)[m]．3版．北京：清华大学出版社，2007．\n[6]钟义信．机器知行学原理[m]．北京：科学出版社，2006．\n[7]Russell S．，Norvig P．Artificial Intelligence：A Modern Approach．2版．北京：人民邮电出版社，2004．\n\nATentative Discussion on C Course Teaching for Science and Technology of Artificial Intelligence\n\nLI Rui-fan，LI Lei",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 548,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向_智能科学与技术_专业的C语言教学探讨_s0_c2",
    "source_id": "面向_智能科学与技术_专业的C语言教学探讨",
    "text": "LI Rui-fan，LI Lei\n\n(School ofComputer Science and Techonology,Beijing University ofPosts and Telecommunications，Beijing 100876，China)\n\nAbstract：This paper discusses the innovation in C course teaching for the major of Science and Technology of Artificial Intelligence, addressing problems faced in this context. From an application-oriented perspective, the paper provides details on content and method innovations.\n\nKey words：C language；Intelligence Science and Technology；teaching",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 3,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 482,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "鲁棒局部保持投影的表情识别_s0_c0",
    "source_id": "鲁棒局部保持投影的表情识别",
    "text": "鲁棒局部保持投影的表情识别\n\n李睿凡1，朱强生1，郭燕慧1，刘海涛2\n\n(1.北京邮电大学信息工程学院，北京100876；2.中国民航大学通信工程系，天津300300)\n\n摘要：针对局部保持投影的流形学习算法对于噪声与异常值的敏感性，提出了一种鲁棒的局部保持投影算法。其基本出发点是首先对所有数据点进行评估，以获得它们可能成为异常值的信息，然后再将这种信息用于邻域选择与低维嵌套中。采用鲁棒局部保持投影进行人脸的表示方法，对JAFFE表情数据库进行了实验，结果表明，该方法有效。\n\n关键词：局部保持投影；鲁棒性；表情识别\n\n1. 局部保持投影\n\n局部保持投影算法是Laplace Beltrami算子特征函数的一个线性估计，其目标是保持数据之间的相似关系，即原始空间上相邻的数据点在投影空间上也保持相应的邻近关系。\n\n2. 鲁棒局部保持投影\n\n鲁棒局部保持投影首先在样本数据集X上执行局部鲁棒主成分分析。而鲁棒主成分分析则利用了带权值的主成分分析算法。这样，可以得到来自潜在流形上的样本数据的可能性度量。而这种可能性度量可以被看成是每个数据点是异常值的度量。一旦识别出可能的异常值，就可以在随后的局部保持投影算法中削弱它们对于整个算法的影响。\n\n3. 表情识别实验",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 5,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 532,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "鲁棒局部保持投影的表情识别_s0_c1",
    "source_id": "鲁棒局部保持投影的表情识别",
    "text": "3. 表情识别实验\n\n将鲁棒局部保持投影算法用于人脸表情识别问题，以表情识别中常用的Gabor小波特征提取方法作为参照，比较局部保持投影算法和鲁棒局部保持投影算法性能上的差异，分类器采用最近邻分类器。JAFFE数据库的表情识别实验中，以Gabor小波为基准测试了算法性能。实验结果表明鲁棒改进算法的有效性。\n\n计方法，以及翻译记忆库和术语库技术，避免了人工翻译中重复劳动和术语不一致现象，有效提高了翻译效率和翻译质量。在具体翻译过程中，人机分工协作，由人完成创造性的工作，而机器随时协助人翻译处理，自动提供已有的翻译记录和术语。同时，机器不断学习人的翻译，作为以后翻译的基础。通过辅助写作翻译系统，可以确保相同句子不被翻译两次，类似句子和短语可以提供范例，节省了大量人工翻译中用于术语和短语查询的时间以及相同句子的重复翻译时间。\n\n作为计算机语言辅助工具的计算机辅助写作翻译系统，很大程度上改变了目前翻译软件的现状，为翻译软件的开发提供了一个全新的思路，这一思想已在国外同类软件中得到体现。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 5,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 445,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "鲁棒局部保持投影的表情识别_s0_c2",
    "source_id": "鲁棒局部保持投影的表情识别",
    "text": "[1] Hutchins W J. Latest Developments in MT technology: beginning a new era in MT research. MT Summit-IV, 1993: 11-34.\n[2] Hutchins W J. Machine translations: past, present and future. England Ellis Horwood Ltd, 1986.\n[3] 冯志伟. 机器翻译——从梦想到现实. 中国翻译, 1999, 19(4): 37—40.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 5,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 266,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "鲁棒局部保持投影的表情识别_s0_c3",
    "source_id": "鲁棒局部保持投影的表情识别",
    "text": "[4] Belkin M, Niyogi P. Laplacian eigenmaps and spectral techniques for embedding and clustering. Advances in Neural Information Processing Systems, 2002: 788–795.\n[5] Weinberger K Q, Packer B D, Saul L K. Nonlinear dimensionality reduction by semidefinite programming and kernel matrix factorization. Proceedings of the Tenth International Workshop on AI and Statistics, 2005: 381—388.\n[6] He X, Niyogi P. Locality preserving projections. Advances in Neural Information Processing Systems, 2004: 327—334.\n[7] Seung H S, Lee D D. The manifold ways of perception. Science, 2000, 290(22): 2268—2269.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 5,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 597,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "鲁棒局部保持投影的表情识别_s0_c4",
    "source_id": "鲁棒局部保持投影的表情识别",
    "text": "[8] de Ridder D, Franc V. Robust manifold learning.\n[9] Chang H, Yeung D Y. Robust locally linear embedding.\n[10] Choi H, Choi S. Kernel isomap on noisy manifold. IEEE International Conference on Development and Learning, 2005: 19—21.\n[11] Chung FR K. Spectral graph theory. NY: American Mathematical Society, 1997.\n[12] Street J O, Carroll R J, Ruppert D. A note on computing robust regression estimates via iteratively reweighted least squares. American Statistician, 1988, 42(2): 152—154.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 5,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 491,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s0_c0",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "低郁密度条件下果园轮式机器人行间运行控制方法研究\n\n目前，我国农业装备自动化程度低，导致农业生产人工需求量和劳动强度较大。为提升我国农业竞争力，研究符合我国实际需求的农业自动化装备至关重要。本文针对果园环境，对轮式差速转向机器人控制及行间自主运行问题进行分析。果园机器人主要在户外环境工作，受天气变化和路面起伏影响，如何在此条件下实现自主运行并保持控制精度成为关键问题。\n\n本文梳理了滑动轮式转向机器人的基础理论，进行了三维建模，并搭建了多体动力学仿真环境。提出了非铺装路面滑动转向轮式机器人轮胎垂直载荷实时估计方法，以及轮胎驱动力实时估计与优化分配算法。垂直载荷估计精度达90%以上，侧向力实时估计准确率92%以上，优化后的轮胎利用率从96.25%降至93.75%。此外，提出了基于激光雷达传感器的户外自主导航方法，并分析了三种轨迹跟踪器在不平路面的跟踪效果，模糊控制器表现出最优适应性。\n\n关键词：户外机器人，差速转向，轮胎动力学，参数估计，轨迹跟踪",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 427,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c0",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "This study addresses the control and autonomous inter-row navigation of wheeled differential steering robots in orchards under low planting density conditions. Given the current low level of automation in China's agricultural equipment, which leads to high labor demand and intensity in agricultural production, research on automation equipment tailored to China's needs is essential to enhance the c",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 1,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 111,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 400,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9900000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c1",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "red to China's needs is essential to enhance the country's agricultural competitiveness. The paper analyzes the challenges of outdoor working conditions, affected by weather and road undulations, focusing on the robot's ability to operate autonomously with precision.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 2,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 267,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c2",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "The research reviews the fundamental theories of sliding wheel steering robots, conducts 3D modeling, and establishes a multi-body dynamics simulation environment. It proposes real-time estimation methods for the vertical load of robot tires on non-paved roads and an algorithm for real-time estimation and optimal distribution of tire driving forces. The accuracy of vertical load estimation exceed",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 3,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 399,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c3",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "s. The accuracy of vertical load estimation exceeds 90%, with over 92% accuracy in lateral force estimation. The optimized tire utilization rate decreases from 96.25% to 93.75%. An outdoor autonomous navigation method based on LiDAR sensors is introduced, and the tracking performance of three trajectory trackers on uneven roads is analyzed, with the fuzzy controller showing the best adaptability.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 4,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 399,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c4",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "Keywords: Outdoor robotics, differential steering, tire dynamics, parameter estimation, trajectory tracking\n\n第一章绪论\n1.1 研究背景及意义\n1.2 国内外果园滑动轮式机器人自主运行研究现状\n   1.2.1 滑动转向机器人运行控制研究现状\n   1.2.2 滑动转向机器人运行参数估计研究现状\n   1.2.3 滑动转向机器人自主运行研究现状\n1.3 本文主要研究内容与组织结构\n   1.3.1 主要研究内容\n   1.3.2 组织结构\n1.4 本章小结",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 5,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 285,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c5",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "第二章轮式滑动转向机器人相关原理\n2.1 差速转向机器人运行学\n   2.1.1 差速转向理想运动学分析\n   2.1.2 差速转向考虑滑移的运动学分析\n2.2 轮胎动力学\n   2.2.1 轮胎动力学模型\n   2.2.2 轮胎参数分析\n2.3 差速转向动力学\n2.4 本章小结\n\n第三章轮式滑动机器人机械设计与仿真搭建\n3.1 机器人机械结构设计\n3.2 仿真搭建\n   3.2.1 多体动力学模型\n   3.2.2 电机仿真模型\n3.3 本章小结\n\n第四章垂直载荷估计与驱动力优化分配方法\n4.1 意义与方法\n4.2 机器人垂直载荷估计\n4.3 驱动力优化分配\n4.4 仿真验证\n   4.4.1 垂直载荷估计方法验证\n   4.4.2 驱动力优化分配方法验证\n4.5 本章小结\n\n第五章机器人行间自主运行\n5.1 点云处理与导航线生成\n5.2 果园环境下轮式滑动转向机器人轨迹跟踪方法研究",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 6,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 398,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c6",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "第二章 轮式滑动转向机器人相关原理\n\n轮式滑动转向机器人(后文简称机器人)转向过程中伴随着大量滑移状态，其运动学与动力学相较其他底盘形式的机器人有较大的区别。为了后文机器人运行控制方法与自主运行方法的研究，本章对机器人的运动学与动力学原理进行介绍，讨论了滑移状态对机器人的影响。\n\n2.1 差速转向机器人运行学\n\n机器人运行学包括两种形式，其一为不考虑滑移影响的理想运动学，其二为考虑滑移影响的运动学。理想运动学多应用于机器人驱动命令解算，当已知机器人目标纵向速度以及目标横摆角速度后根据理想运动学解算两侧轮速。考虑滑移状态的运动学多应用于机器人运动以及动力参数估计。下面对两种运动学进行分别讨论分析。\n\n2.1.1 差速转向理想运动学分析",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 7,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 320,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c7",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "2.1.1 差速转向理想运动学分析\n\n滑动转向因为在转向过程中伴随着大量滑移过程，因此被称为滑移转向机器人。机器人的工作原理与履带车辆相似，当直线行驶时内外侧驱动速度保持一致。当出现转向需求时，依靠内外侧轮速差产生横摆角速度使机器人航向角发生变化。通过调整内外侧轮胎不同速度可以达到不同的转向半径。设机器人左侧轮线速度为VL,右侧车轮线速度为VR,机器人整体纵向速度为VG横摆角速度为WZ,机器人轴距为L,轮距为B。假设机器人质心位于其几何中心，理想状态下滑动转向运动学如图(2-1)所示。图中，机器人绕转向中心O做半径为R的逆时针转向。\n\n下面直接给出机器人在理想状态下的运动学方程，如下式(2-1)所示。\n\n(2-1)",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 8,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 311,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c8",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "下面直接给出机器人在理想状态下的运动学方程，如下式(2-1)所示。\n\n(2-1)\n\n以上即为理想状态下的机器人运动学方程。设左前轮、左后轮、右前轮、右后轮转动角速度分别为W1、W2、W3、W4,左侧旋转角速度为WL右侧旋转角速度为WR,轮胎滚动半径为r。由于不考虑滑移影响，滑动转向速度满足以下条件。\n\n(2-2)\n\n由于滑动转向依靠左右两侧速度差完成，所以可以将左右两侧转速按式(2-3)表示。\n\n(2-3)\n\n式中k为差速因子，表示了两侧车轮速度差。将公式(2-1)(2-2)(2-3)组合，得到机器人两侧速度差与转弯半径之间的关系，如式(2-4)所示。\n\n(2-4)\n\n令=+−，代表滑移平台非直线运动形式函数，图像如图(2-2)，和的转向半径大小相同，方向相反，前者朝右，后者朝左。\n\n图2-2 非直线运动函数图\n\n2.1.2 考虑滑移的运动学分析",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 9,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 378,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c9",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "图2-2 非直线运动函数图\n\n2.1.2 考虑滑移的运动学分析\n\n考虑滑移状态的运动学方程如下式(2-5)所示。\n\n(2-5)\n\n式中，β为滑移角，表示轮胎与地面接触点速度方向与轮胎速度方向之间的夹角。当β=0时，表示没有滑移发生，即理想运动学状态。当β≠0时，表示存在滑移，即实际运动学状态。滑移角β与轮胎速度、地面摩擦系数等因素有关。\n\n2.2 差速转向机器人动力学\n\n机器人动力学包括两种形式，其一为不考虑滑移影响的理想动力学，其二为考虑滑移影响的动力学。理想动力学多应用于机器人驱动转矩解算，当已知机器人目标纵向速度以及目标横摆角速度后根据理想动力学解算两侧轮转矩。考虑滑移状态的动力学多应用于机器人运动以及动力参数估计。下面对两种动力学进行分别讨论分析。\n\n2.2.1 差速转向理想动力学分析\n\n理想动力学方程如下式(2-6)所示。\n\n(2-6)",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 10,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 378,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c10",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "2.2.1 差速转向理想动力学分析\n\n理想动力学方程如下式(2-6)所示。\n\n(2-6)\n\n式中，Fxi、Fyi(i=1,2,3,4)分别为四个车轮的纵向力与侧向力，τi(i=1,2,3,4)分别为四个车轮的转矩，I为机器人转动惯量，m为机器人质量，g为重力加速度，θ为机器人倾角。\n\n2.2.2 考虑滑移的动力学分析\n\n考虑滑移状态的动力学方程如下式(2-7)所示。\n\n(2-7)\n\n式中，Ff、Fr分别为前轮与后轮的纵向力，Ffl、Ffr分别为左前轮与右前轮的侧向力，Frl、Frr分别为左后轮与右后轮的侧向力，Fsl、Fsr分别为左前轮与右前轮的滑移力，Ft、Fb分别为前轮与后轮的滑移力，Ff、Fr、Ffl、Ffr、Frl、Frr、Fsl、Fsr、Ft、Fb与轮胎速度、地面摩擦系数等因素有关。\n\n2.3 轮胎动力学",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 11,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 362,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c11",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "2.3 轮胎动力学\n\n轮胎动力学是研究轮胎与地面之间相互作用力的学科。轮胎动力学模型通常分为纵向动力学模型与侧向动力学模型。纵向动力学模型描述轮胎纵向力与纵向滑移率之间的关系，侧向动力学模型描述轮胎侧向力与侧向滑移率之间的关系。轮胎动力学模型通常采用经验公式或物理模型进行描述。轮胎动力学模型对于机器人运动控制与参数估计具有重要意义。\n\n2.4 坡道稳态动力学\n\n坡道稳态动力学是研究机器人在坡道条件下稳态运行性能的学科。坡道稳态动力学方程如下式(2-8)所示。\n\n(2-8)",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 12,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 238,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c12",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "(2-8)\n\n式中，Fg为重力分量，Ff为前轮驱动力，Fr为后轮驱动力，Ffl为左前轮侧向力，Ffr为右前轮侧向力，Frl为左后轮侧向力，Frr为右后轮侧向力，Fsl为左前轮滑移力，Fsr为右前轮滑移力，Ft为前轮滑移力，Fb为后轮滑移力，Fg、Ff、Fr、Ffl、Ffr、Frl、Frr、Fsl、Fsr、Ft、Fb与轮胎速度、地面摩擦系数、坡道角度等因素有关。\n\n2.5 本章小结\n\n本章介绍了轮式滑动转向机器人的运动学与动力学原理，讨论了滑移状态对机器人的影响。运动学包括理想运动学与考虑滑移的运动学，动力学包括理想动力学与考虑滑移的动力学。轮胎动力学描述轮胎与地面之间的相互作用力，坡道稳态动力学研究机器人在坡道条件下的稳态运行性能。这些原理对于机器人运动控制与参数估计具有重要意义。\n\n第3部分内容：\n\n2.1.2 差速转向考虑滑移的运动学分析",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 13,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 376,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c13",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "第3部分内容：\n\n2.1.2 差速转向考虑滑移的运动学分析\n\n以上分析基于无滑移的运动状态。下面讨论考虑滑移状态下机器人的运动学，分析如图(2-3)所示。\n\n当只考虑纵向滑移，不考虑侧向滑移时，机器人满足以下条件，如式(2-5)。\n\n式中S1、S2、S3、S4 分别为左前轮、左后轮、右前轮、右后轮滑移率。这里关于纵向滑移率和侧向滑移率的定义与文献UniTire 轮胎模型中的滑移率定义一致。假设同侧两轮纵向滑移率相等，左侧滑移率为SL,右侧滑移率为SR。根据公式可以的到纵向滑移率与实际回转半径之间的关系，如式所(2-6)所示。\n\n当同时考虑机器人纵滑与侧滑时机器人运动产生侧滑因子S0，其为转动中心O 与质心CM 沿X 轴方向的距离。\n\n图2-3 滑移状态运动学",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 14,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 333,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c14",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "图2-3 滑移状态运动学\n\n如图(2-3)，R1 为机器人实际转向半径，R0 滑移回转半径，R1、R0 与S0三者共同构成直角三角形，按照勾股定理，三者具有如下关系如式(2-7)所示。\n\n2.2 轮胎动力学",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 15,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 103,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c15",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "2.2 轮胎动力学\n\n在机器人运行过程中，作用于机器人的所有主动力与被动力(除空气阻力外)均来源于轮胎与地面的相互作用。因此车轮与地面之间的作用力与方向直接影响了机器人如何转向、加速以及减速[41]。为深入理解机器人的运行原理，需对轮胎特性进行一定分析。目前，为了能够对轮胎性能进行建模分析，学者们提出了不同的轮胎动力学模型架构。轮胎动力学模型是用来描述轮胎在车辆行驶过程中力学特性的数学模型。它通常由多个子模型组成，包括侧偏角侧偏力模型、侧偏刚度模型、纵向力模型、横向力模型等。这些子模型可以分别描述轮胎在不同方向上的受力特性，从而帮助分析轮胎在车辆运动过程中的行为。轮胎动力学模型在车辆动力学、车辆控制等领域中都有重要应用。轮胎动力学模型的应用示意图如(2-4)所示。\n\n图2-4 轮胎模型",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 16,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 347,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c16",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "图2-4 轮胎模型\n\n轮胎动力学依靠滑移率、侧偏角、外倾角、垂直载荷等参数输入计算得出当前轮胎承受的纵向力、侧向力以及回正力矩等重要轮胎力学特性。在轮胎动力学模型中，侧偏角侧向力模型是其中最重要的子模型之一。它描述了轮胎在侧向力作用下产生的侧偏现象，以及侧偏角和侧偏力之间的关系。侧偏刚度模型则描述了轮胎在侧向偏移时产生的回复力，以及侧偏角和侧偏力的变化关系。纵向力模型和横向力模型则分别描述了轮胎在纵向和横向方向上产生的力，以及这些力和轮胎负载、路面摩擦等因素之间的关系。为进一步说明轮胎动力学模型，下面对应用最为广泛的魔术公式模型以及Fiala 模型进行分析，由于本文机器人不涉及回正力矩，下面只对纵向力以及侧向力进行分析。\n\n2.2.1 轮胎动力学模型\n\n(1)魔术公式模型",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 17,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 340,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c17",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "2.2.1 轮胎动力学模型\n\n(1)魔术公式模型\n\n目前在车辆动力学中应用最为广泛的轮胎动力学模型就是魔术公式模型。魔术公式采用特殊的基于拟合的正弦函数建立轮胎的纵向力、横向力和回正力矩模型。因为三个计算模块只需一组公式即可解出固称为魔术公式。魔术公式纵向力计算模型如图(2-5)所示。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 18,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 143,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c18",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "上图中，Fl 为待求解纵向力，S 为纵向滑移率，Sh 为曲线的水平方向漂移, Sv 为曲线的垂直方向漂移。C 为曲线的形状因子，D 为曲线因子，B 为刚度因子， E 为曲线的曲率因子。其中B0 至B10 为待拟合参数。对于魔术公式侧向力求解，公式中输入参数与纵向力基本相同，只将带待拟合参数由B0 至B10 变为A0 至A13 以及将纵向力模型中输入的滑移率替换为侧偏角，从上图分析可以看出，魔术公式的应用需要大量的参数拟合工作,因此该轮胎动力学模型属于经验模型[42]。同时可以看到，对魔术公式影响因素最大的变量为垂直载荷FZ,其数值直接影响了形状因子，曲线因子等关键参数。\n\n(2)Fiala 模型\n\nFiala 模型属于物理模型，在纵向方向Fiala 模型将轮胎视为与地面接触的弹性刷子，这些刷子可以在与地面平行的方向上发生形变，如图(2-5)所示。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 19,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 378,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c19",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "在侧向方向，Fiala 理论所提出的轮胎模型是解释轮胎侧偏现象时被广泛接受的模型理论。该模型通过将胎体带束视为建立在弹性基础上可以产生侧向变形的弹性梁，从而达到描述侧向变形的目的。侧向模型示意图如(2-5)右侧所示。\n\n该模型认为A 轮辋为刚体，B 为等效弹簧，C 为台面基部胶，D 为胎面胶。根据该模型的发展理论，所有的轮胎受力都由物理变形导致，所以模型内部计算过程复杂，但对于应用者只需提供基本轮胎参数即可求解轮胎六分力，相较魔术公式使用更加方便灵活。\n\n2.2.2 轮胎参数分析\n\n本文机器人不涉及内倾角，固不考虑回正力矩。为进一步说明纵向力侧向力与滑移率和侧偏角的关系以及其他因素对数值大小影响，下面单独对纵向力以及侧向力展开分析。\n\n(1)纵向力\n\n轮胎纵向力即车辆驱动力。根据轮胎动力学模型，纵向力由轮胎纵向变形导致，而这种变形的大小由滑移率进行表示，滑移率公式如(2-11)所示。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 20,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 397,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c20",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "上式中VW 为轮胎纵向速度，VC 为车辆纵向速度。滑移率的大小为一百分比值，数值范围从-100%到+100%对应车辆减速时完全抱死状态以及加速时车轮完全打滑空转的状态。在小滑移率情况下(一般情况下视-5%至+5%范围内为小滑移率阶段)该阶段轮胎输出与滑移率呈线性关系，其公式如(2-12)所示。\n\n式中FX 为轮胎纵向力，S 为滑移率，Cl 为轮胎纵向刚度。轮胎纵向刚度与花纹，材料内部结构等有关，是轮胎固有属性。从上式中可以看出，当轮胎刚度变大时，同等轮胎变形条件下，轮胎输出纵向力更高。当滑移率超过线性区间，纵向力的输出进入非线性阶段，其具体数值大小与轮胎模型有关。通过MATLAB编写魔术公式轮胎模型，仿真轮胎纵向力模型下纵向力输出曲线。分析滑移率、纵向力、纵向刚度之间关系，如图(2-6)所示。\n\n图2-6 轮胎纵向力刚度影响曲线",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 21,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 369,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c21",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "图2-6 轮胎纵向力刚度影响曲线\n\n仿真开始时，当滑移率在线性范围内逐渐增加，其纵向力随之增加。当滑移率增加到一定数值(图中大约为3%)后曲线进入非线性区域，纵向力输出随滑移率继续增长。继续加大滑移率，纵向力将到达输出峰值，此峰值受轮胎与路面摩擦系数以及垂直载荷影响。当继续增大滑移率，输出将逐渐降低。对比刚度不同轮胎，在相同滑移率时，高刚度轮胎纵向力输出明显大于小刚度轮胎。当纵向力输出最大值时，其纵向力输出大小与垂直载荷比值即为当前轮胎与路面最大附着系数[43]。公式如(2-13)所示。\n\n式中，FXmax 为当前轮胎最大纵向力输出，μ为当前最大附着系数。当垂直载荷不变，而最大纵向力更大时，表明当前轮胎与路面附着能力更高。同样当最大附着系数不变时，更高的垂直载荷可以提供更高的纵向力。不同垂直载荷下纵向力与垂直载荷之间关系如图(2-7)所示。\n\n图2-7 轮胎纵向力垂直载荷影响曲线",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 22,
    "chunk_index_in_section": 21,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 395,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c22",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "图2-7 轮胎纵向力垂直载荷影响曲线\n\n上图中大垂直载荷为2000N，小垂直载荷为1000N。可以看出大垂直载荷下轮胎纵向力输出能力明显增大，垂直载荷对于轮胎动力输出起到了重要作用。因此在实际机器人控制中，为了更好地控制驱动力大小，需要对垂直载荷进行分析\n\n(2)侧向力\n\n在汽车行驶过程中，由于路面的倾斜、曲线行驶时的离心力等作用，车轮中心沿Y 轴方向将作用有侧向力Fy，相应地在地面上产生地面侧向反作用力，也称为侧偏力。侧向力是转向阻力矩的重要来源，为了分析机器人运动情况，必须对侧向力进行分析。根据轮胎模型理论，侧向力由轮胎侧向变形导致，而这种变形的大小由侧偏角进行表示，侧偏角如下图(2-8)所示。\n\n图2-8 侧偏角",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 23,
    "chunk_index_in_section": 22,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 313,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c23",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "图2-8 侧偏角\n\n上图为轮胎俯视图，车轮平面与地平面的交线取为X 轴，规定向前为正。Y轴在地平面上，规定垂直于车轮前进方向时指向左边为正。u 为轮胎实际运行方向。由于滑动转向车辆不存在转向角，固X 轴与轮胎实际运行方向u 之间的夹角即为侧偏角A。Fy 为轮胎所受侧向力，侧向力方向与侧偏角相反。在侧偏角较小时侧偏角与侧向力同样存在线性关系，如式(2-13)所示。\n\n式中Ca 为侧偏刚度，侧向刚度与纵向刚度同样为轮胎固有属性。同样应用MATLAB 编写魔术公式侧向力模块。经过仿真，侧向力侧偏角与侧向刚度之间关系如图(2-9)所示。\n\n图2-9 轮胎侧向力侧偏刚度影响曲线",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 24,
    "chunk_index_in_section": 23,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 287,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c24",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "图2-9 轮胎侧向力侧偏刚度影响曲线\n\n与滑移率类似，当侧偏角较小时侧偏力与侧偏角呈线性关系，侧偏刚度Ca为线性段斜率。当侧偏角继续增大时曲线进入非线性区，侧偏角继续增大至侧向力峰值后，侧向力逐渐减小。对比不同侧偏刚度对侧向力的输出影响可以发现，其与纵向刚度对纵向力的影响有所区别。对于纵向刚度与纵向力之间的作用关系，高纵向刚度轮胎输出随滑移率增长快，同时滑移率趋于饱和时输出也明显高于低纵向刚度轮胎。而对于侧向刚度对侧向力输出的影响，高侧偏刚度轮胎侧向力输出随侧偏角增长快，但当侧偏角趋于饱和时，高侧偏刚度轮胎的侧向力输出下降明显，且最终输出小于低刚度轮胎。下面同样对垂直载荷对侧向力的影响进行分析，曲线如图(2-10)所示。\n\n图2-10 轮胎侧向力垂直载荷影响曲线",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 25,
    "chunk_index_in_section": 24,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 335,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c25",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "图2-10 轮胎侧向力垂直载荷影响曲线\n\n图(2-10)中大垂直载荷为2000N，小垂直载荷为1000N。可以看出与垂直载荷对纵向力影响一致，垂直载荷在纵向与侧向力中有重要影响[44]。\n\n2.3 差速转向动力学",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 26,
    "chunk_index_in_section": 25,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 106,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c26",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "动力学旨在研究机器人受力与运动情况之间关系。在2.1 节中我们研究了差速转向机器人运动特征，其不考虑受力情况，在2.2 节中我们研究了轮胎动力学，轮胎动力学是轮式滑动转向机器人运动学与动力学之间的桥梁。有了前文的运动学以及轮胎动力学的分析我们就可以对机器人整体进行动力学分析。从机器人整体运行过程来看可以将动力学可分为两部分进行分析，分别为稳态动力学以及瞬态动力学。当机器人两侧动力输出不一致时机器人整体产生横摆力矩，机器人将绕自身Z 轴进行横摆运动，此时横摆角加速度不为0，其为瞬态动力学。当横摆阻力矩与机器人主动转向力矩平衡后，机器人将进入稳态动力学过程。本文涉及的机器人工作环境主要在果园行间，为了满足运行平稳及作业精准等要求，机器人大多处于稳态工况下",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 27,
    "chunk_index_in_section": 26,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 329,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c27",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "。本文涉及的机器人工作环境主要在果园行间，为了满足运行平稳及作业精准等要求，机器人大多处于稳态工况下。因此，本节首先对机器人瞬态动力学进行简要分析后重点给出机器人稳态动力学方程，该动力学方程考虑了机器人处于坡道状态下的动首先对瞬态动力学分析，将左前轮、左后轮、右前轮、右后轮依次编号为1、2、3、4。设机器人质量为M，机器人纵向速度为v 横向速度为u 横摆角速度为w。瞬态动力学分析如下图(2-11)所示。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 28,
    "chunk_index_in_section": 27,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 203,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c28",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "图2-11 瞬态动力学\n\n上图中Fxi为每个轮胎所受驱动力，该驱动力由电机产生，是需要通过稳态方程解算出的系统输入参数。Fyi为转弯时轮胎由侧偏角产生的侧向力。Fzi为每个轮胎所受到的滚动阻力。在车身坐标下，动力学方程如式(2-15)所示。\n\n上式中B为机器人轮距，L为机器人轴距，Iz为机器人Z轴转动惯量。瞬态动力学主要应用于机器人加减速等情况下，根据目标加速度计算驱动力。下面针对果园行间自主运行情况，分析机器人处于坡道情况下的稳态动力学方程。整体分析如下图(2-12)所示。\n\n图2-12 稳态动力学\n\n上图中，机器人运行于坡道n上，坡道角度为α，机器人运行方向与坡道平面梯度方向夹角为β。根据纵向力横向力及横摆力矩平衡列取平衡方程，其动力学方程如(2-16)所示。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 29,
    "chunk_index_in_section": 28,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 336,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c29",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "其中Ra为四轮滚动阻力之和，Mr为侧向力以及滚动阻力构成的机器人横摆阻力矩，滚动阻力之和与横摆阻力矩的表达如公式(2-17)以及(2-18)所示。\n\n四轮滑动转向可以将驱动力写成左右侧之和的形式,如式(2-19)所示。\n\n根据上式分析即可计算得坡道稳态转向时左右两侧驱动力需求，如式(2-20)所示。\n\n式中FL与FR分别为机器人左侧与右侧驱动力需求。\n\n2.4 本章小结\n\n本章首先对机器人运动学进行了分析。分别讨论了理想状态下即不考虑滑移的运动学以及考虑了滑移影响的运动学。从分析中可以发现机器人质心与几何中心偏移距离影响了差速转向机器人前后轴侧滑速度，距离偏差越大，侧滑速度偏差越大。因此在对机器人进行机械设计时，应尽可能保证机器人质心位于几何中心位置。本小节最后，通过几何规律推导给出了各轮侧偏角与侧滑因子之间关系。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 30,
    "chunk_index_in_section": 29,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 362,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c30",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "其次，本章第二节分析了轮胎动力学的作用并给出了目前应用较多的魔术公式模型，以及Fiala 模型的基本原理并对其应用进行了分析。再次，为说明纵向力侧向力与滑移率和侧偏角的关系以及其他因素对轮胎动力学数值大小的影响，单独对纵向力以及侧向力展开了分析。仿真实验表明，轮胎输出与轮胎各向刚度与垂直载荷有较大的关系，对机器人进行建模时，必须考虑垂直载荷对动力输出的影响。最后，通过滑动转向运动学以及轮胎动力学的铺垫，在本章最后一节中提出了轮式滑动转向机器人的瞬态动力学以及针对于果园环境的坡道稳态动力学方程。根据坡道稳态动力学方程即可求解机器人当前运行状态下所需动力输出，为后续动力输出优化做了准备工作。\n\n3.1 机器人机械结构设计\n\n将机器人几何中心在地面的投影点作为原点建立坐标系，机器人整体形式如图3-1所示。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 31,
    "chunk_index_in_section": 30,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 354,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c31",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "将机器人几何中心在地面的投影点作为原点建立坐标系，机器人整体形式如图3-1所示。\n\n本文设计的机器人主要应用于标准化种植果园进行植保、运输、采摘等工作。标准化果园成行种植，行与行之间趋于平行，两行间有3米左右可通行区域，行头空地空间较大。为了在行间实现灵活移动，机器人设计长宽不超过1.5米。同时滑动转向机器人轮距应大于轴距，并且根据路面条件不同，轮距与轴距之比数值不同。最终设计后机器人整体尺寸如表(3-1)所示。\n\n表3-1 机器人结构尺寸表\n\n长/m 宽/m 高/m 轮距/m 轴距/m 轮距轴距比\n\n1.27 0.928 0.677 0.778 0.565 1.38",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 32,
    "chunk_index_in_section": 31,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 287,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c32",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "1.27 0.928 0.677 0.778 0.565 1.38\n\n为了提高底盘通过性，在设计中创新的使用了工业减速器，该减速器既承担了放大扭矩降低车速的作用又作为底盘重要支撑件从而省去单独设计支撑结构的成本，由于该减速器90度传动的原因，提高了机器人离地间隙，将易受损的电机放置在底盘上部，因此增加了机器人的户外通过能力。底盘轮组装配图如(3-2)左侧所示。\n\n图3-2 轮组与整车装配图\n\n为将机器人质心位置与几何中心重合，在设计时将机器人设计为纵向横向基本完全对称的形式。底盘部分四个行走轮组镜像对称，零件互通互用。经过设计，轮组部分非标零件一共只有32个，增强了设备的互换性。为了满足户外防护的要求，减速器输入与输出端均增加了防水密封胶圈。机器人底盘采用刚性悬挂的设计，既降低了机器人成本也降低了机器人建模复杂程度。机器人底盘部分整体如(3-2)右侧所示。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 33,
    "chunk_index_in_section": 32,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 383,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c33",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "根据最终设计，机器人接近角离去角达到64度最小离地间隙282毫米，满足了户外起伏路面的通行需求。考虑到滑动转向过程伴随的滑移率及侧偏角较大，使得转向时轮胎承受侧向力较大，需提供的轮胎纵向力也相应增大，因此对轮胎磨损较为严重。根据轮胎动力学一节侧偏刚度对侧向力的影响分析可知，高侧偏刚度轮胎在侧偏角饱和情况下产生的侧向力小而更适用于滑动转向形式，考虑到实心轮胎的高侧向刚度以及耐磨性本文选用实心轮胎替代充气轮胎。图(3-3)为充气轮胎、实心胎、铁轮在不同侧偏角下侧向力输出对比图。从图中可以看出，轮胎刚度越大轮胎饱和侧向力越小，轮胎更容易进入滑移状态，从而机器人完成转向所需驱动力越小。\n\n图3-3 不同轮胎侧向力对比\n\n清洗后的内容如下：",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 34,
    "chunk_index_in_section": 33,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 319,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c34",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "图3-3 不同轮胎侧向力对比\n\n清洗后的内容如下：\n\n受机器人特殊的滑动转向形式影响，机器人动力选型较为复杂。本文为了更加精确得到机器人运行中所需驱动力，对机器人整体进行动力学仿真。根据机械设计，机器人自重300千克，考虑后期负载200千克，总重共计500千克。机器人根据作业要求在果园行间运行线速度恒定为0.3米每秒。综合以上要求，设置实验条件为机器人在2度斜坡上以0.3米每秒的线速度做半径为5米的匀速圆周运动，从该运动过程中提取驱动力峰值作为驱动力要求。根据仿真实验结果，在上述条件下该机器人单轮最大驱动力为630N。机器人选用的实心胎直径为0.425米，将轮胎所受纵向力换算为减速器轴上输出扭矩为133.87牛米，将轮胎线速度转换到转速为13转每分钟。依据以上参数指标对动力系统进行选型，最终结果如下表（3-2）。\n\n表3-2 电机参数表",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 35,
    "chunk_index_in_section": 34,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 373,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c35",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "表3-2 电机参数表\n\n电机功率/w 额定扭矩/nm 额定转速/rpm 电机转动惯量/Ns 2m\n\n750 2.8 3000 0.0008\n\n第三章 轮式滑动机器人机械设计与仿真搭建\n\n表3-3 减速器参数表\n\n减速比 额定输出扭矩/nm 额定输出转速/rpm\n\n80 395 18\n\n根据以上选型，减速器最大输出扭矩为224牛米，最大转速18转每分，均满足设计需求。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 36,
    "chunk_index_in_section": 35,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 184,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c36",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "根据动力系统输出力矩以及机器人负载大小对机器人重要零件进行仿真校核。在该机器人中，转接筒是连接轮胎轮毂和减速器的关键零件，其承担传递减速器输出扭矩至轮胎以及在旋转过程中承受机器人重力引起的交变载荷，总体受力情况较为复杂。根据GB/T 33582-2017《机械产品结构有限元力学分析通用规则》通过有限元方法对零件设计强度进行校核。转接筒材料为304不锈钢，其弹性模量为1.9e11N/m2,泊松比为0.29，屈服强度为206MPa。使用Hypermesh软件对该零件进行网格划分并进行局部优化，最终结果如图（3-4）左图所示，99.3%的网格雅可比（Jacobian）大于0.7。网格质量较低的部分位于在中部圆角的边缘处，由于该处不是应力集中区域，可以忽略其对计算产生的影响。92%的网格长宽比小于5，长宽比大于5的网格只出现在中部圆角的边缘处\n。92%的网格长宽比小于5，长宽比大于5的网格只出现在中部圆角的边缘处。整体网格的最大翘曲度为0.00157，远小于一般5的要求，综上网格划分满足求解精度要求。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 37,
    "chunk_index_in_section": 36,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 453,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c37",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "图3-4 网格划分与载荷预设\n\n依据圣维南原理，将边界条件做如下设置。将转接筒与车体的螺栓连接孔面设置为固定；在转接筒与轮毂的螺栓孔面施加垂直向下的、大小为1000N的载荷；在转接筒与轮毂连接面施加大小为150NM的扭矩。零件固定位置与载荷作用位置如图（3-4）右图所示。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 38,
    "chunk_index_in_section": 37,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 136,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c38",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "设置边界条件后，通过ANSYS的Mechanical求解器进行计算，仿真结果如下。根据应力云图信息，如图（3-5a）所示，出现应力最大数值的具体位置为螺栓孔边缘，应力大小为19.131MPa,其明显小于304材料屈服强度，且具有3倍以上安全系数，可以满足抗冲击、抗疲劳的要求。零件应变云图如图（3-5b）所示，零件最大应变值为9.58e-5m,其云图状态和应力云图基本一致。变形云图如图（3-5c）所示，最大变形量为0.09588mm，零件整体变形较小。根据以上分析，机器人整体承受载荷最大，受力情况最复杂零件的强度性能以及抗冲击性能均能满足使用安全要求。\n\n（a）应力云图 (b) 应变云图 (c) 变形云图\n\n3.2 仿真搭建",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 39,
    "chunk_index_in_section": 38,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 315,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c39",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "（a）应力云图 (b) 应变云图 (c) 变形云图\n\n3.2 仿真搭建\n\n多体动力学仿真模型的建立是机器人设计和控制的重要基础，通过建立机器人的多体动力学模型可以对机器人的运动学、动力学和控制系统进行分析以及优化设计。为了后续机器人参数估计、动力优化分配以及自主导航方法的研究以及验证，本节首先对机器人进行多体动力学仿真模型搭建。其次，为了在机器人运动过程中了解电机输出响应，避免电机过载等情况发生，利用SIMULINK搭建了基于磁场矢量控制技术的永磁同步电机仿真模型。将电机模型的输出转速接入多体动力学模型，将多体动力学模型的输出负载接入电机模型的输入实现了完整的机器人仿真系统闭环，从而可以更加真实全面的反应机器人实际运行情况。\n\n3.2.1 多体动力学模型",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 40,
    "chunk_index_in_section": 39,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 331,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c40",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "3.2.1 多体动力学模型\n\nADAMS软件是美国机械动力公司开发的虚拟样机分析软件。软件使用交互式图形环境和零件库、约束库、力库，创建完全参数化的机械系统模型，其求解器采用多刚体系统动力学理论中的拉格朗日方程方法，建立系统动力学方程，对虚拟机械系统进行静力学、运动学和动力学分析，输出位移、速度、加速度和反作用力曲线。ADAMS软件的仿真可用于预测机械系统的性能、运动范围、碰撞检测、峰值载荷以及计算有限元的输入载荷等。本文建立的机器人动力学仿真模型如图(3-6)所示。\n\n图3-6 机器人动力学仿真模型\n\n对于本文设计的差速转向轮式机器人，动力学建模主要包括三个部分第一部分是机器人本体建模，第二部分为轮胎模型建模，第三部分为路面建模。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 41,
    "chunk_index_in_section": 40,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 320,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c41",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "对于机器人本体建模，由于本文中的机器人主要用于分析运动状态下的轮胎受力情况，车身其他部件不参与分析，因此将机器人本体视为完整刚体。通过SOLIDWORKS软件生成x_t格式文件后直接导入ADAMS VIEW软件中。导入仿真软件中，设置重力、质心位置、机器人质量等参数即可。\n\n第二部，轮胎模型搭建。ADAMS中轮胎模型以轮胎文件形式存在其中，ADAMS自带丰富的轮胎模型库，包括魔术公式、FIALA模型、摩托专用轮胎模型等，轮胎文件如图(3-7)所示。\n\n图3-7 轮胎仿真文件",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 42,
    "chunk_index_in_section": 41,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 239,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c42",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "图3-7 轮胎仿真文件\n\n在轮胎动力学一节中分析了当前应用较多的FIALA轮胎动力学模型以及魔术公式模型，魔术公式由于需要提供较多的轮胎参数，导致其拟合工作复杂，有一定实际应用难度。而FIALA轮胎模型基于物理模型，模型应用只需要输入纵向刚度，侧向刚度，垂直刚度与阻尼等较少几个基本轮胎参数，模型内部通过物理变形机理求解受力，因此本文选用该轮胎模型作为后续算法开法的轮胎仿真模\n\n第三部分为路面建模，路面模型同样以txt文件格式存在于仿真模型中。ADAMS中自带了平整路面，典型障碍路面以及3D路面文件。由于本文机器人处于果园环境下，路面存在不规则起伏。固本文利用MATLAB编写了随机3D路面生成代码，其根据对路面不平度的要求生成路面节点参数，并导入ADAMS标准三维路谱模板文本中。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 43,
    "chunk_index_in_section": 42,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 343,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c43",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "根据随机路面统计学规律，随机路面的路谱为平稳、遍历并且均值为零的高斯过程，因此可用三角级数进行模拟。谐波叠加法采用以离散谱逼近目标随机过程的模型，是一种离散化数值模拟路面的方法。我国沿用了国际标准协会文件ISO/TC108/XC2N67制定的路面不平的等级分类方法。该方法规定在空间频率[-∞,∞]内，依照道路等级给出了功率谱密度，路面功率谱密度如下式所示。\n\nS(ω)=Gq(n0)ω-(2/w) （3-1）\n\n上式中ω0为参考空间频率，通常取值为-1，为空间频率为ω时的路面功率谱密度，称其为路面不平度系数。w为频率指数，决定了路面功率谱频率，通常取值为2，n为空间频率范围[-∞,∞]某一频率。依据标准规定，路面不平度系数依据路面分为八级，如下表（3-4）所示。\n\n表3-4 路面等级表\n\n路面等级 路面不平度系数几何平均值\n\nD 1024\n\nE 4096\n\nF 16384",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 44,
    "chunk_index_in_section": 43,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 390,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c44",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "路面等级 路面不平度系数几何平均值\n\nD 1024\n\nE 4096\n\nF 16384\n\nG 65536\n\nH 262144\n\n上表中，A级路面最平坦，H级路面最崎岖。根据学者对乡间土路、耕地路面、草地路面的不平度调查，发现其符合国家F级路面情况。故本文利用正弦叠加法依据F级路面标准生成ADAMS可用路谱文件。生成的路面如图(3-8)所示。\n\n图3-8 路面效果图\n\n上图中，设置路面长度为20米，宽度5米，路面不平度为国家标准F级。\n\n同时考虑到路面可能存在的障碍情况，在长度方向10米以及15米处分别设置高度为0.3米的凸起障碍，以及0.3米的坑洼障碍。将该路谱坐标信息导入ADAMS标准路谱文档中即可生成仿真路面。\n\n至此，多体动力学模型全部搭建完成，为后续机器人仿真与控制算法开发提\n\n3.2.2 电机仿真模型",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 45,
    "chunk_index_in_section": 44,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 359,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c45",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "至此，多体动力学模型全部搭建完成，为后续机器人仿真与控制算法开发提\n\n3.2.2 电机仿真模型\n\n电机是机器人运行的基本动力原件，其响应基本能够反映出机器人行驶时的性能与效率。本文中机器人采用750W直流无刷伺服电机并配有一体式伺服器，通过伺服器可以读取当前电机电流、力矩、转速等基本参数信息。随着电机应用的兴起，电机控制手段也得到较好发展，本文所选用伺服电机为使电机输出力矩更加精准稳定，在伺服器内包含了磁场矢量控制算法，后文简称FOC算法。为后续进行机器人仿真与控制方法验证，本节利用SIMULINK仿真软件，搭建了基于FOC控制方法的永磁同步电机仿真模型，示意图如（3-9）所示。\n\n图3-9 FOC控制方法",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 46,
    "chunk_index_in_section": 45,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 307,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c46",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "FOC控制方法与其他电机控制方法最大不同在于增加了对电机电流进行采样环节，同时需要精确估计转子角度。该控制方法具体流程如下，首先通过传感器或者观测算法，采集得到三相电流Ia、Ib、Ic，电角度θ，电机转速v。接着将采集的三相电流Ia、Ib、Ic进行坐标转换，通过clark变换从三相静止坐标系转换两相静止直角坐标系，再通过park变换从两相静止直角坐标系转换到两相旋转直角坐标系，得到Id、Iq电流。同时，利用反馈的电机转速与设定的目标速度进行比较，得到控制偏差，将偏差经过比例、积分、微分处理得到控制量，输出目标q轴电流。然后对d轴与q轴分别做PID计算，利用当前的Iq电流与速度环PID输出值进行比较，得到控制偏差，将偏差处理得到控制量，输出q轴的目标电压，同理利用当前的Iq电流与0进行比较，输出d轴的目标电压",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 47,
    "chunk_index_in_section": 46,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 358,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c47",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "。把电流环PID输出的d轴与q轴目标电压，先后经过park反变换、clark反变换，从两相旋转直角坐标系转换到三相静止坐标系，得到三相目标电压Ua、Ub、Uc。最终通过转换坐标系后得到的三相电压，计算得到三相电压所需的占空比，并控制逆变器输出三相电流，实现电机的精确控制。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 48,
    "chunk_index_in_section": 47,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 136,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c48",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "本文根据上述原理,基于SIMULINK搭建了仿真模型。FOC算法仿真中除了基本的模型搭建任务外，PID参数整定尤为重要，其决定了电机的响应特性以及控制精度。经过仿真搭建以及参数调试后对该模型进行仿真测试，仿真设置以阶梯形式对电机模型输入目标转速及负载，测试时长10秒。仿真过程中包含低速低负载、低速高负载、高速高负载、高速低负载以及静止低负载这五类典型情况。电机目标转速与实际转速情况如图（3-11）所示，图（3-10）为电机负载，图（3-12）及图（3-13）分别为d轴,q轴电流响应曲线。\n\n图3-10 电机实时负载图 图3-11 转速控制效果\n\n图3-12 d轴电流控制效果 图3-13 q轴电流控制效果",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 49,
    "chunk_index_in_section": 48,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 305,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c49",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "图3-12 d轴电流控制效果 图3-13 q轴电流控制效果\n\n观察上图中仿真实验结果。在速度响应中，无论是在低转速低扭矩还是在高转速低扭矩的各种工况中都不存在大过冲情况，超调量始终低于0.75%。在FOC算法中为将电机输出电流控制在合理范围内避免电机损坏，过热等情况，对电机加速度设定了阈值，因此在本实验中电机最大加速度约为16.7转每平方每秒。当电机负载达到额定扭矩3牛米时，电机的最大加速度只有5转每平方秒，且与目标速度存在稳态误差。观察实验结果中d,q轴电流情况，发现无论电机处于何种工况下d,q轴均能较好跟踪给定目标，且当负载变化时d轴电流始终保持在0安附近震荡，满足了FOC算法控制要求。根据实验结果，仿真算法可以满足后续控制方法开发需求。\n\n3.3 本章小结",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 50,
    "chunk_index_in_section": 49,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 335,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c50",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "3.3 本章小结\n\n本章首节，根据果园实际需求进行机器人机械设计，通过模块化设计使车体结构对称，利于互换性以及运行性能。之后对动力需求进行仿真分析，根据仿真结果完成了电机与减速器选型。同时给出了滑移转向机器人选择实心轮胎的依据。本节最后，对机器人中受力最复杂的零件利用有限元仿真工具进行检验，检验结果表明该零件具有三倍以上安全系数，满足适用需求。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 51,
    "chunk_index_in_section": 50,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 173,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c51",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "为了后续控制方法研究，本章第二节对机器人仿真模型进行搭建。机器人动力学模型基于ADAMS多体动力学分析软件进行搭建。首先将SOLIDWORKS生成的机器人三维模型导入仿真软件中，添加约束，质量位置等信息几何。其次配置了基于FIALA模型的轮胎模型。再次，基于国家路面不平度标准利用正弦波叠加法建立了符合果园环境的F等级路面。最后利用SIMULINK搭建了基于FOC控制方法的永磁同步电机控制模型，并通过仿真实验，验证了电机在不同工况下的响应能力，结果满足后续控制仿真的要求。\n\n至此，本章完成了对果园轮式滑动转向机器人的基础结构设计以及机器人控制方法仿真环境搭建，为后续机器人参数估计、驱动力分配、轨迹跟踪等研究做好了准备工作。\n\n第四章 垂直载荷估计与驱动力优化分配方法\n\n第四章 垂直载荷估计与驱动力优化分配方法",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 52,
    "chunk_index_in_section": 51,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 358,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c52",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "第四章 垂直载荷估计与驱动力优化分配方法\n\n第四章 垂直载荷估计与驱动力优化分配方法\n\n上一章完成了机器人的机械设计与仿真准备工作，本章依靠理论分析与仿真验证对果园环境下滑动转向机器人控制问题展开分析与研究。本章主要旨在解决果园环境下机器人轮胎垂直载荷及侧向力估计，并且根据第二章第三节中提出的机器人坡道稳态动力学方程进行驱动力需求解算，最后通过优化手段将驱动力进行合理分配。\n\n4.1 意义与方法",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 53,
    "chunk_index_in_section": 52,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 199,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c53",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "4.1 意义与方法\n\n轮式滑动转向机器人的转向主要依靠轮胎与路面之间的滑移运动，但滑移过程中机器人的动力学参数变化范围较大。目前，滑动转向在室内环境下应用广泛，相关控制的方法研究也较为全面。但当涉及到户外情况时，由于轮胎受路面起伏影响以及坡道路面影响，驱动力控制与分配就要考虑到环境以及机器人运动状态对轮胎的影响。此时如果缺少垂直载荷、侧向力等重要轮胎动力学，便难以实现机器人运动与姿态的高精度控制要求。因此轮胎动力学参数的实时估计对提高机器人的运行精度具有重要意义。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 54,
    "chunk_index_in_section": 53,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 234,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c54",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "因此本章重点研究了果园环境下的滑动转向机器人轮胎实时垂直载荷以及侧向力实时估计方法。同时提出了基于第二章第三节所分析的坡道稳态动力学及以上估计参数的驱动力优化分配方法。首先，本章提出了一种基于陀螺仪的四轮垂直载荷实时估计方法，通过该方法可实时计算各轮胎垂直载荷。其次，为解算动力学参数，本文引入了轮胎动力学模型并将该模型进行了简化使其适用于刚性悬架的轮式滑动转向机器人。该轮胎动力学模型可根据当前机器人运行情况实时计算用于轮胎驱动力解算的轮胎侧向力。再次，提出了轮式滑动转向机器人坡道稳态转向动力学模型，通过坡道稳态动力学模型可以估计当前姿态下同侧轮胎驱动力需求总和。最后，提出了同侧轮胎最优驱动力估计方法，从而降低轮胎利用率并提高轮胎附着裕量，增加运行稳定性",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 55,
    "chunk_index_in_section": 54,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 330,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c55",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "。最后，提出了同侧轮胎最优驱动力估计方法，从而降低轮胎利用率并提高轮胎附着裕量，增加运行稳定性。本章提出的基于垂直载荷的轮胎动力输出优化方法主要通过垂直载荷计算、侧向力及滚动阻力计算、驱动力需求计算、驱动力优化五步实现，整体技术路线如下图（4-1）所示。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 56,
    "chunk_index_in_section": 55,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 127,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c56",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "图4-1 整体方法示意图\n\n如图（4-1）所示，首先通过机器人的质量、质心位置、轮距与轴距参数以及机器人当前姿态计算机器人各轮垂直载荷。然后，通过滚动阻力估计模型与侧向力估计模型计算轮胎侧向力以及滚动阻力，从而估计所需轮胎纵向力。对于侧向力的估计本文引入了基于物理描述的Fiala轮胎动力学模型，该轮胎模型需要的参数较少，在大半径转弯条件下准确性高，适用于本文研究对象。得到各个轮胎的侧向力及滚动阻力后，根据本文提出的坡道稳态转向动力学对驱动力需求进行估计。驱动力优化部分，以单侧驱动力需求作为约束条件，以单侧两轮胎的利用率之和构造代价函数，通过拉格朗日乘数法评估当前同侧车轮的最优驱动力分配，以提高机器人控制精度。\n\n4.2 机器人垂直载荷估计\n\n本章研究使用的机器人整体形式即为第二章所设计的样式，结构简图如图（4-2）所示。\n\n图4-2 机器人结构示意图",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 57,
    "chunk_index_in_section": 56,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 380,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c57",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "图4-2 机器人结构示意图\n\n图（4-2）中D为机器人轴距，B为机器人轮距。机器人采用几何中心对称设计，经过实测其重心与几何中心偏差较小。差速转向过程滑移率及侧偏角较大，使得轮胎承受侧向力较大，需提供更大驱动力完成转向动作。由于实心胎在侧偏角饱和情况下产生的侧向力小而更适用于滑动转向形式，固本文选用实心轮胎替代充气轮胎。因实心轮胎刚性大且机器人采用低自由度的刚性悬架设计，当路面有激励输入轮胎时，轮胎与车身无相对运动，可将机器人整体作为刚体分析。因此，机器人动力学建模过程可忽略悬架对车身运动的影响，从而降低了机器人运动过程的模型复杂度。机器人为独立四驱运动模式，可根据电机反馈的转速、扭矩评估轮胎附着情况，给出各轮最优驱动力估计。\n\n垂直载荷越高可获得更高轮胎附着力，该参数是本文方法的基础。当机器人处于平整路面时，机器人四轮垂直载荷恒定；而果园",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 58,
    "chunk_index_in_section": 57,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 375,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c58",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "根据提供的指示，以下是清洗后的学术论文第5部分内容：\n\n---\n\n四轮空间坐标已知后，通过求解平面方程系数便得到了完整的理想坡面方程，坡面方程如公式(4-6)所示。\n\n= + + (4-6)\n\n式中a、b、c——待求解的平面方程系数\n\n通过对理想坡面方程求取偏导得到平面的梯度方向设为向量p，梯度向量在平地的投影向量为，车头朝向向量为q。通过余弦定理可以分别求出车头与梯度方向之间夹角β(坡道偏航角)以及坡道与地面之间夹角α(坡道角)。由此，依靠该理想平面可对机器人进行静力学分析，从而进行垂直载荷计算。考虑轮式差速机器人在坡道运行过程，可将机器人与坡道的关系分为四种状态，如下表(4-1)所示\n\n表4-1 机器人坡道状态表\n\n状态一 状态二 状态三 状态四\n\n状态位姿 上坡偏左 上坡偏右 下坡偏左 下坡偏右\n\n坡道角/° 大于0 大于0 小于0 小于0",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 59,
    "chunk_index_in_section": 58,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 378,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c59",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "状态位姿 上坡偏左 上坡偏右 下坡偏左 下坡偏右\n\n坡道角/° 大于0 大于0 小于0 小于0\n\n坡道偏航角/° 大于0 小于0 大于0 小于0\n\n先以状态一为例进行分析。将机器人整体看作质点考虑在坡面下的受力情况，假设坡道角为α坡道偏航角为β并且α与β均大于0 度。机器人在坡面下受到的重力可分解为垂直坡道平面n向内的分力以及沿坡道梯度相反方向的分力F'，如图(4-4)所示。\n\n图4-4 机器人整体坡道受力分解示意图",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 60,
    "chunk_index_in_section": 59,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 210,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c60",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "图4-4 机器人整体坡道受力分解示意图\n\n上图中为重力分解后垂直坡面向内的应力，其大小为α，其产生的反作用力是垂直载荷的主要组成部分称其主应力。F'称其为分应力，大小为α。单独对主应力进行分析，主应力F在机器人四个轮胎之间的分配关系取决于机器人自身的重心位置。因本文假设了重心位于几何中心且机器人几何中心对称，固四轮所受主应力产生的垂直载荷相等。设左前轮、左后轮、右前轮、右后轮受主应力影响所产生垂直载荷为Flf、Flb、Frf、Frb其数值大小如式(4-7)所示。从公式中可以看出Flf、Flb、Frf、Frb只与坡道角有关与坡道偏航角无关。\n\nα = = = = (4-7)\n\n式中Flf——左前轮基础垂直载荷\n\nFlb ——左后轮基础垂直载荷\n\nFrf ——右前轮基础垂直载荷\n\nFrb——右后轮基础垂直载荷\n\nm——机器人质量",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 61,
    "chunk_index_in_section": 60,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 367,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c61",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "Frf ——右前轮基础垂直载荷\n\nFrb——右后轮基础垂直载荷\n\nm——机器人质量\n\n从式(4-7)可知，Flf、Flb、Frf、Frb只与坡道角有关而与坡道偏航角无关。对重力沿斜面分力对四轮垂直载荷的影响进行分析如图(4-5)所示。\n\n图4-5 状态一各轮受力分析示意图\n\n首先正视于坡道平面n，设机器人质心距坡道平面高度为h设左前轮、左后轮、右前轮、右后轮的接地点分别为A、B、C、D。连接对角轮做直线AD,CB。过B点做AD垂线，过C点做AD垂线，两线段长度均为L。AD与CB之间夹角为对角轮连线夹角,设为δ。\n\n首先对状态一的左后轮与右前轮进行受力分析。将分解为沿AD方向的分力垂直于AD连线向下的分力称为主分力为辅分力。设与之间夹角为σ。同理可对左前轮与右后轮进行分析。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 62,
    "chunk_index_in_section": 61,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 340,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c62",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "由图(4-5)左侧受力分解可知，主分力将会对轴AD作用产生力矩MBC，B、C两点将会产生等大反向的作用力，绝对值大小为FBC。FBC与力矩MBC使AD轴力矩平衡。而辅分力不对B、C两点产生影响。力矩MBC具体计算如下。\n\nπ δ σ β = − − (4-8)\n\nσ = (4-9)\n\n= (4-10)\n\n= (4-11)\n\n根据公式(12)可以得到反作用力FBC。\n\n= (4-12)\n\n将基础应力与该反作用力进行叠加便可得到左后轮和右前轮的垂直载荷，设左后轮和右前轮的垂直载荷分别为FLB与FRF。计算公式如(4-13)(4-14)所示。\n\n= + (4-13)\n\n= − (4-14)",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 63,
    "chunk_index_in_section": 62,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 295,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c63",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "= + (4-13)\n\n= − (4-14)\n\n计算得到左后轮以及右前轮垂直载荷后，对左前轮以及右后轮进行分析。其受力分解如图(4-5)右侧所示。其计算过程与左后轮及右前轮一致，对AD轴进行力矩平衡计算时只需重新解算主分力与分应力之间的夹角σ。其计算公式如式(4-15)所示。\n\nπ δ σ β = − + (4-15)\n\n下面给出在状态一下差速轮式机器人四轮垂直载荷计算公式。\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 64,
    "chunk_index_in_section": 63,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 398,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c64",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "α π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 65,
    "chunk_index_in_section": 64,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 394,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c65",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "α π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 66,
    "chunk_index_in_section": 65,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 394,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c66",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "α π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 67,
    "chunk_index_in_section": 66,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 394,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c67",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "α π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 68,
    "chunk_index_in_section": 67,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 394,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c68",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "α π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 69,
    "chunk_index_in_section": 68,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 394,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c69",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "α π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 70,
    "chunk_index_in_section": 69,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 394,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c70",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "α π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 71,
    "chunk_index_in_section": 70,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 394,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c71",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "α π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 72,
    "chunk_index_in_section": 71,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 394,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c72",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "α π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 73,
    "chunk_index_in_section": 72,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 394,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c73",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "α π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 74,
    "chunk_index_in_section": 73,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 394,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c74",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "α π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 75,
    "chunk_index_in_section": 74,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 394,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c75",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "α π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 76,
    "chunk_index_in_section": 75,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 394,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c76",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "α π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 77,
    "chunk_index_in_section": 76,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 394,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c77",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "α π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 78,
    "chunk_index_in_section": 77,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 394,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c78",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "α π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 79,
    "chunk_index_in_section": 78,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 163,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c79",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "α π δ α β\n\nα π δ α β\n\nα π δ α β\n\nα π δ α β\n\n根据前文对平整坡道下垂直载荷研究，当四轮附着条件良好时，四轮机器人对角轮的垂直载荷之和应当一致。观察表6中的对角轮之和数据可以发现，5组数据的对角轮载荷之和均不相等，其中最第二组中产生了最大的1103N对角轮载荷差值，五组仿真中最小的差值也达到了50N。当四轮中其中一轮处于半接触状态时，机器人姿态可能不会改变，但其它轮胎的负载加大以使得机器人处于平衡状态。下面给出垂直载荷计算方法在不平路面下得到的计算数据。结果如表(4-7)所示，其中各组实验机器人的空间姿态与表(4-6)一一对应。\n\n表4-7 E级路面下垂直载荷实时估计结果\n\n左前右后 载荷之和\n\n左后右前 载荷之和\n\n1 1135.44 672.44 996.4 533.4 1668.84 1668.84 3338.68",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 80,
    "chunk_index_in_section": 79,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 385,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c80",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "2 690.9 1090.44 592.27 992.27 1683.17 1683.71 3365.88\n\n3 764.16 956.81 751.56 944.21 1708.37 1708.37 3416.74\n\n4 1064.4 583.9 1058.7 584.2 1648.6 1648.6 3291.2\n\n5 824.09 911.95 801.51 889.37 1713.46 1713.46\n\n观察表(4-6)与表(4-7)的结果，当仿真中对角轮载荷之和偏差较大时，可以判断出当前机器人附着情况较差，因此按照理想接触条件下的计算结果偏差较大。当机器人整体附着情况较好时，计算结果与仿真数值偏差较小，如第一组实验。根据以上对非铺装路面的仿真与分析，可以看出当机器人行驶于非铺装路面时垂直载荷算法在附着良好情况下可以反应出各轮载荷实际情况以及变化趋势。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 81,
    "chunk_index_in_section": 80,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 384,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c81",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "对垂直载荷估计方法进行实际验证。经实测机器人总重343千克且质心与几何中心重合。实验如图11所示。各轮压力拉压力传感器进行测量。传感器输出为模拟量，经变送器转换为数字量发送到显示端。传感器测量精度为0.01%，变送器精度为0.02%。同时，由于机械结构存在一定制造及装配误差，经过测试垂直载荷测量装置综合精度为0.1%。\n\n图4-8 垂直载荷估计实验\n\n将机器人置于2度斜坡，取坡道偏航角为20度姿态并在各轮胎下方放入称重装置并读取机器人陀螺仪与四个称重装置数值。实测与计算结果如表8所示。\n\n根据实验结果，垂直载荷估计方法准确率达到90%以上。误差来源主要包括称重装置存在微小高度差以及机器人存在一定的装配误差等。\n\n表4-8 垂直载荷实测结果\n\n左前轮 左后轮 右前轮 右后轮",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 82,
    "chunk_index_in_section": 81,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 340,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c82",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "表4-8 垂直载荷实测结果\n\n左前轮 左后轮 右前轮 右后轮\n\n垂直载荷实测结果/N 840.2 930.4 770.4 880.3 垂直载荷计算结果/N 868.5 904.5 809.5 845.5 误差/N 28.3 25.9 39.1 34.8 计算准确率 96.6% 97.2% 95.2% 96%\n\n4.4.2 驱动力优化分配方法验证",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 83,
    "chunk_index_in_section": 82,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 173,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c83",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "4.4.2 驱动力优化分配方法验证\n\n驱动力优化算法首先要根据上文提出的差速轮式机器人坡道稳态转向动力学方程计算得出左右侧驱动力需求，之后根据各轮垂直载荷灵活调整单侧驱动力分配。为计算驱动力需求，需对轮胎所受侧向力进行估计。按照上文提出基于FIALA轮胎动力学模型的方法，计算侧向力时要求已知轮胎基本参数、路面摩擦系数、垂直载荷、滑移率、侧偏角等信息。为了通过仿真验证侧向力估计的准确性，在计算中轮胎刚度参数与ADAMS中设定参数保持一致，滑移率与侧偏角与垂直载荷按照仿真中给出的结果带入计算。仿真设定机器人以0.2米每秒的纵向速度在2度斜坡进行等半径转向。设计实验转向半径为10米，转向方向为沿机器人Z轴正向旋转。本文选取机器人运动方向沿坡道梯度方向时进行分析。下面首先给出机器人以半径为10m进行稳态转向的仿真结果，如表(4-8)所示。\n\n表4-8 半径10米稳态转向仿真数据",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 84,
    "chunk_index_in_section": 83,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 390,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c84",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "表4-8 半径10米稳态转向仿真数据\n\n滑移率 侧偏角/rad 垂直载荷/N 纵向力/N 侧向力/N\n\n左前轮 -0.0246 0.0414 808.4 -372.7 -555.7\n\n左后轮 -0.0248 -0.0391 873.3 -376.6 554.1\n\n右前轮 0.0297 0.0413 808.4 430.2 -552.6\n\n右后轮 0.0294 -0.0391 873.3 436.4 550.1\n\n根据以上仿真结果将各轮滑移率、垂直载荷、侧偏角带入在SIMULINK中搭建好的轮胎动力学模型中求解当前轮胎侧向力，计算结果如表(4-9)所示。\n\n表4-9 侧向力计算结果\n\n左前轮 左后轮 右前轮 右后轮\n\n侧向力仿真结果/N -555.7 554.1 -552.6 550.1\n\n侧向力计算结果/N -528.7 546.1 -535.3 589.5",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 85,
    "chunk_index_in_section": 84,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 384,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c85",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "侧向力计算结果/N -528.7 546.1 -535.3 589.5\n\n误差/N 27 8 17.3 39.4\n\n计算准确率 95.1% 98.6% 96.9% 92.8%\n\n根据计算结果可以发现，侧向力计算结果准确率达到92%以上准确率较高，对于果园坡度路面的以满足应用要求。\n\n对实际机器人系统进行侧向力估计实验验证。侧向力检测装置如图(4-9)所示,经过减速器以及机械结构的力分解，测量轴只受动力输出轴沿轴向的拉压力影响。侧向力测量装置装配零件多，装配精度低于垂直载荷传感器。综合传感器误差及机械结构误差，测量装置的总体测量精度为0.3%，精度条件满足验证侧向力估计方法的需求。\n\n图4-9 侧向力实验装置\n\n设定机器人以0.2m/s纵向速度进行半径10m匀速转向。设定路面最大附着系数0.6最小附着系数0.5。实验环境如图(4-10)所示。\n\n图4-10 侧向力实验环境",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 86,
    "chunk_index_in_section": 85,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 390,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c86",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "图4-10 侧向力实验环境\n\n机器人进行匀速等半径转向，因此侧偏角保持不变。分析不同垂直载荷下的侧向力仿真值、估计值、测量值。侧向力实验结果如表(4-10)所示。\n\n表4-10 侧向力实验结果统计\n\n垂直载荷/N 仿真-侧向力/N 估计-侧向力/N 实测-侧向力/N\n\n655.7 277.9 266.3 310\n\n741.2 335.4 310.7 333.1\n\n833.1 374.2 355.1 406.4\n\n910.9 410.6 399.5 442.8\n\n根据实验数据，侧向力估计方法准确率达到85%以上。误差来源主要包括机器人运行过程中的低频振动、路面阶跃激励输入、传感器精度等。\n\n为了说明驱动力优化对于机器人稳定性的提升，下面利用表(4-8)的仿真结果对不进行驱动力分配的轮胎使用情况进行分析，设当前路面摩擦系数为0.9与ADAMS中设置保持一致。分析结果如表(4-11)所示。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 87,
    "chunk_index_in_section": 86,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 397,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c87",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "表4-11 轮胎使用情况统计\n\n左前轮 左后轮 右前轮 右后轮\n\n轮胎力/N 669.1 669.9 700.3 702.17\n\n最大附着能力/N 727.56 785.97 727.56 787.98\n\n附着裕量/N 58.46 116.07 27.26 85.81\n\n轮胎利用率 91% 85.23% 96.25% 89%",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 88,
    "chunk_index_in_section": 87,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 163,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c88",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "从表10可以看出，机器人在大半径转向下的轮胎利用率已经较高，这是由于差速轮式机器人转向过程中伴随着较大的侧偏角导致侧向力较大。大侧向力则要求更高的驱动力，总体呈现出轮胎力高于其他转向的形式的情况。表10中右前轮的利用率已经达到96.25%而附着裕量仅为27.26N，这是由于该轮垂直载荷较低，而所承受的轮胎力又由于侧偏角与滑移率的影响高于左前轮与左后轮。当该轮胎以当前状态驶入湿滑泥泞路面时容易造成侧滑及轮胎空转等情况。因此根据当前轮胎附着能力实时调整动力分配可将驱动力更多的赋予到附着条件更优的轮胎，使机器人整体稳定性得到提升。下面给出按照拉格朗日乘数法分配的轮胎动力输出结果。两侧轮胎驱动力要求按照表(4-8)中的仿真结果给定，其中左侧两轮驱动力要求为749.3N右侧两轮驱动力需求为866.6N。重新分配后各轮驱动力以及轮胎使用情况如表(4-12)所示。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 89,
    "chunk_index_in_section": 88,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 380,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c89",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "表4-12 驱动力优化后的轮胎适用情况统计\n\n左前轮 左后轮 右前轮 右后轮\n\n优化纵向力/N 345.77 403.52 399.9 466.69\n\n轮胎力/N 654.49 685.46 682.12 721.39\n\n最大附着能力/N 727.56 785.97 727.56 787.98\n\n附着裕量/N 73.07 100.51 45.44 66.59\n\n轮胎利用率 89.96% 87.21% 93.75% 91.55%\n\n对比表(4-11)与表(4-12)可发现，当根据垂直载荷动态分配驱动力后，机器人运行于坡道时将更多的驱动力分配给附着能力更高的轮胎。轮胎最低附着裕量由先前的27.26提高至45.44。综合以上结果，通过轮胎侧向力估计后根据利用率分配的驱动力设定方式提高了滑动轮式机器人在果园环境下运行稳定性。\n\n清洗后的内容如下：\n\n5.2.2 模糊控制方法",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 90,
    "chunk_index_in_section": 89,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 388,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c90",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "清洗后的内容如下：\n\n5.2.2 模糊控制方法\n\n当系统的复杂性越高，人们按照数学模型等方法精确控制的能力会下降。这意味着人们无法完全掌握一些十分复杂的系统。但在实际生活中，人们在遇到复杂问题时，往往依靠丰富的经验解决，而不是了解事物本质。因此可以看出“人类经验”才是最完美的控制器。而模糊控制器就是一种最简单的基于人工经验的人脑模拟器。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 91,
    "chunk_index_in_section": 90,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 169,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c91",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "根据系统输入输出划分，本文属于双输入单输出控制器。与PID方法前几步相同，首先计算车机器人与目标点线段的实际偏差角，其次计算与最近点的横向偏差。模糊控制器以实际偏差角与横向偏差作为输入参数，为使轨迹跟踪稳态误差更小，将两个输入参数分别划分为8个论域：负大、负中、负小、零负、零正、正小、正中、正大。隶属函数部分选择了成熟的高斯分布钟形模型，设定模糊控制器输出为机器人横摆角速度，论域数量与隶属度函数与输入相同。不断实验后，经过参数调整，模糊规则表最终如表(5-1)所示。\n\n表5-1 模糊规则表\n\n横向偏差 偏航角 NB NM NS N0 P0 PS PM PB\n\nNB N0 N0 NM NB NB NB NB NB\n\nNM N0 NS NM NB NB NB NB NB\n\nNS PM PS P0 P0 NS NM NB NB\n\nN0 PB PB PS P0 N0 NS NM NB",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 92,
    "chunk_index_in_section": 91,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 393,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c92",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "N0 PB PB PS P0 N0 NS NM NB\n\nP0 PB PM PS P0 N0 NS NB NB\n\nPS PB PB PM PS N0 N0 NS NM\n\nPM PB PB PB PB PB PM PS P0\n\nPB PB PB PB PB PB PM P0 P0\n\n当输入变量进入模糊控制器后得到输出变量论域后，经过解模糊即可获得输出变量大小，本文选用重心法对输出值进行解模糊操作，经过解模糊后的输出值即为机器人横摆角速度。该模糊控制器基本执行逻辑如下，当机器人横向偏差较大且行驶方向更加偏离目标时，控制器输出较大横摆角速度使机器人以最快速度指向目标直线。当机器人接近目标时，还以较大角度向目标逼近时，为了防止超调，控制器输出最大反向角速度。其他情况以此类推。\n\n5.2.3 纯追踪方法",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 93,
    "chunk_index_in_section": 92,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 350,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c93",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "5.2.3 纯追踪方法\n\n纯追踪模型目前已广泛应用于车辆自动驾驶领域。纯追踪理论以单车模型为基础，基于几何模型推导出前轮所需转向角，因此纯追踪方法适用于阿克曼转向形式。纯追踪模型的优势在于其可以通过前视距离这一参数灵活调整机器人跟踪反应程度，当前视距离越大，机器人“看的越远”其跟踪曲线就越平缓。基于单车模型的纯追踪方法示意图如(5-8)所示。\n\n图5-8 纯追踪方法\n\n图中(gx,gy)为机器人下一个要追踪的目标点，其位于生成的导航线上，表示机器人后轴到目标点的距离，角α为机器人纵向方向与目标点之间夹角。根据正弦定理我们可以推导出以下关系。\n\nπ α α = \n\nα α α = \n\n= \n\n从式可以看出，当机器人距离下一个目标点距离越远，机器人转向半径越大跟踪曲线即越平稳。式中ld为机器人前视距离，前视距离为纯追踪要设定的参数，通常前视距离可以按式表示。\n\n= +",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 94,
    "chunk_index_in_section": 93,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 387,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c94",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "= + \n\n式中k为一比例系数，v为机器人运行线速度，lf为机器人基础前视距离。\n\n比例系数k根据不同工作环境以及机器人设定运行速度选取。当k值选取越大，机器人的前视距离随速度变化越大。基础前视距离为确保机器人在低速状态下仍具有一定前视距离。根据公式我们可以基于前视距离以及偏差角度推导出机器人所需转弯半径，对于阿克曼转向形式可以根据转向半径推导出前轮转向角度，而滑动转向方式不存在前轮转向角，因此根据滑动转向机器人运动学对纯追踪算法进行一定修正。机器人具有如下运动学方程，如式所示。\n\n= \n\n式中，w为机器人横摆角速度。联立式与可以解得基于差速转向机器人的纯跟踪方法，如下式所示。\n\n+ \n\n如上式所示，像机器人输入与目标点偏差角度以及机器人线速度，即可求解当前机器人所需横摆角速度，从而完成轨迹跟踪任务。\n\n5.2.4 轨迹跟踪方法仿真实验",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 95,
    "chunk_index_in_section": 94,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 373,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c95",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "5.2.4 轨迹跟踪方法仿真实验\n\n为了对比PID、模糊控制、纯跟踪三种轨迹跟踪方法对于轮式差速转向机器人在果园环境下的控制效果，本小节对三种方法进行仿真验证。实验基于在第三章中搭建好的多体动力学仿真模型进行，路面同样采用第三章中创建的国家标准F级路面。仿真初始位置为(0,0)点车头朝向为x轴正向，给定目标曲线为y=1，机器人初始线速度为0.3米每秒，角速度为0。\n\n(a) PID (b) 模糊控制\n\n(c)纯追踪\n\n图5-9 三种控制器效果对比\n\n图(5-9)分别为PID、模糊控制以及纯追踪控制效果。下面对三种方法的控制效果进行分析",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 96,
    "chunk_index_in_section": 95,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 270,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c96",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "图(5-9)分别为PID、模糊控制以及纯追踪控制效果。下面对三种方法的控制效果进行分析\n\n基于PID的控制方法下，机器人行驶1.8米后首次到达目标值。继续行进到沿X方向2米时，产生了0.08米的超调，之后机器人跟踪曲线时不断波动。当机器人行进到8米时，机器人横向误差收敛在0.05米范围内。仿真中，机器人共沿X方向行进了18米最后时刻偏差为0.004米。\n\n基于模糊控制的轨迹跟踪方法下，机器人沿X方向行驶2.6米后首次到达目标位置，之后沿X方向行进3米时产生了0.07米的超调量。当机器人继续运行时，曲线逐渐向目标靠近，不再出现波动，仿真中，机器人运行到15米时误差为0.04米。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 97,
    "chunk_index_in_section": 96,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 291,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c97",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "基于纯追踪控制的轨迹跟踪方法下，机器人沿X方向行驶1.6米后首次到达目标位置，之后行进到沿x方向1.9米时产生了0.27米的超调，超调量相较于其他两种方法较大。机器人继续运行时曲线不断波动。当机器人行进到沿X方向5米时，偏差收敛在0.05米的范围内，仿真共进行了18米最后偏差为0.0035米。\n\n综合实验结果进行对比。模糊控制具有跟踪平滑，超调量小，后期无反复波动等特点。稳态误差问题也可以通过输入及输出端论域划分得到进一步解决。\n\n为了进一步验证模糊控制器追踪效果，利用自动驾驶工具箱生成运行轨迹并提取路径坐标。生成的导航轨迹点中，每两点之间，间距及偏角均不等。提取坐标后对个别坐标进一步修改，增加偏离点干扰。以此生成更为复杂的曲线路径。\n\n生成的路径如图(5-10)所示。\n\n图5-10 复杂路径设计",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 98,
    "chunk_index_in_section": 97,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 353,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c98",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "生成的路径如图(5-10)所示。\n\n图5-10 复杂路径设计\n\n仿真过程中设计了离群点以及大半径掉头等情况，机器人在运行过程中最大偏差为0.04米，误差在可接受范围内。根据以上仿真结果，模糊控制方法在起伏不平的果园路况下具有较好的适应性。\n\n5.3 本章小结\n\n本章首节，介绍了适用于户外果园环境下的激光雷达LM111相关参数。在某标准化种植的葡萄园内进行了激光雷达点云数据收集工作。通过分析果园实际点云数据后发现，在原始点云数据中存在激光雷达近点干扰以及临近行干扰的情况。为此，本文利用ROI方法设置点云筛选区域，再利用动态半径滤波方法对部分离群点进行剔除。经过处理后的点云数据干扰点数量明显减少，在点云图像中不再存在临近行干扰。将处理后的点云利用随机采样一致方法进行两侧果树排架拟合，结果表明，不论在机器人的航向角与可通行区域前进方向偏差较小或较大，拟合结果均能满足使用要求。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 99,
    "chunk_index_in_section": 98,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 390,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c99",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "本章第二节主要研究了三种控制方法对果园条件下轮式滑动转向机器人轨迹跟踪的控制效果。本文选用的PID以及模糊控制均为无模型控制手段，由于不需要运动过程的精确模型，使用较为方便。本文对基于单车模型的纯追踪控制方法做出了调整，使其原本的输出前轮转向角改为输出机器人横摆角速度。本节末尾，基于第三章搭建的仿真环境对三种方法控制效果进行验证。实验结果表明，三种方法均能在复杂路况下跟踪给定目标。综合比对结果，模糊控制方法的跟踪轨迹更加平滑，并且无反复波动。通过对模糊控制进一步仿真，设置了离群点以及大半径转向等情况后模糊控制器跟踪效果依旧稳定。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 100,
    "chunk_index_in_section": 99,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 267,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c100",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "Liu Fangxu et al. Slip-Aware Motion Estimation for Off-Road Mobile Robots via Multi-Innovation Unscented Kalman Filter. IEEE Access, 2020, 8: 43482-43496.\n\nLu Hao. Research on the Prediction Method of the Motion Trajectory of Sliding Steering Vehicles Based on Real-Time Estimation of the Instantaneous Steering Center. Beijing Institute of Technology, 2016. DOI:10.26948/d.cnki.gbjlu.2016.000768.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 101,
    "chunk_index_in_section": 100,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 397,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c101",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "Li Tianhua, Wu Zenghao, Lian Xiankun, et al. Visual Navigation Research of Large Arch Greenhouse Transport Vehicle Based on Directional Camera. Journal of Agricultural Machinery, 2018, 49(S1): 8-13.\n\nZhang Xiongchu, Chen Bingqi, Li Jingbin, et al. Visual Navigation Path Detection of Jujube Harvester. Transactions of the Chinese Society of Agricultural Engineering, 2020, 36(13): 133-140.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 102,
    "chunk_index_in_section": 101,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 389,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c102",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "Bi Song, Wang Yuhao. Visual Navigation Path Estimation and Target Localization Method of Orchard Robots. Journal of Agricultural Machinery, 2021, 52(08): 16-26+39.\n\nLI Y, WANG X, LIU D. 3D Autonomous Navigation Line Extraction for Field Roads Based on Binocular Vision. Journal of Sensors, 2019, 2019(8): 1–16.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 103,
    "chunk_index_in_section": 102,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 310,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c103",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "Li Qiujie, Ding Xudong, Deng Xian. Path Extraction and Navigation in Orchard Rows Based on Lidar. Journal of Agricultural Machinery, 2020, 51(S02): 7.\n\nNi Jiangnan. Research on Automatic Control System of Harvester Based on Laser Navigation Facilities. Journal of Agricultural Mechanization Research, 43(2): 4.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 104,
    "chunk_index_in_section": 103,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 310,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c104",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "Liu Xingxing, Zhang Chao, Zhang Hao, et al. Autonomous Navigation Method in Forest Rows with Least Squares and SVM Combination. Transactions of the Chinese Society of Agricultural Engineering, 2021, 37(9): 157-164.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 105,
    "chunk_index_in_section": 104,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 214,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c105",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "Ai Changsheng, Lin Hongchuan, Wu Delin, Feng Zhiquan. Path Planning Algorithm of Vineyard Plant Protection Robot. Transactions of the Chinese Society of Agricultural Engineering, 2018, 34(13): 77-85.\n\nBLOK P M, BOHEEMEN K V, EVERT F V, et al. Robot navigation in orchards with localization based on Particle filter and Kalman filter. Computers and Electronics in Agriculture, 2019, 157: 261–269.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 106,
    "chunk_index_in_section": 105,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 395,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c106",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "Haidong Wu, K. Guo, Haibin Xu. Dynamic tire model used in advanced chassis control. 2011 International Conference on Electric Information and Control Engineering, Wuhan, China, 2011, pp. 4957-4961, doi: 10.1109/ICEICE.2011.5776824.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 107,
    "chunk_index_in_section": 106,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 231,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c107",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "Liang Changfei, Li Yuguang, Li Ziyang, et al. Dynamic Performance Study of Magic Formula Tire Model. Automobile Practical Technology, 2020, 45(16): 116-119. DOI:10.16638/j.cnki.1671-7988.2020.16.039.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 108,
    "chunk_index_in_section": 107,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 199,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c108",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "M. Bian, L. Chen, Y. Luo, K. Li. Estimation of Maximum Tire-Road Friction Based on Dynamic Model Reconstruction. 2013 International Conference on Mechanical and Automation Engineering, Jiujang, China, 2013, pp. 224-228, doi: 10.1109/MAEE.2013.63.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 109,
    "chunk_index_in_section": 108,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 246,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c109",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "D. Jeong, S. Kim, J. Lee, S. B. Choi, M. Kim, H. Lee. Estimation of Tire Load and Vehicle Parameters Using Intelligent Tires Combined With Vehicle Dynamics. IEEE Transactions on Instrumentation and Measurement, 2021, 70: 1-12, Art no. 9502712, doi: 10.1109/TIM.2020.3031124.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 110,
    "chunk_index_in_section": 109,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 274,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非_s1_c110",
    "source_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
    "text": "J. Kim, C. D. Crane, J. Kim. Development of the Autonomous Navigation Algorithm based on the Geometric Method for Skid Steering Vehicles: Convergence of Skid Steering and Pure Pursuit Methods Using Compensation Coefficients. 2022 22nd International Conference on Control, Automation and Systems (ICCAS), Jeju, Korea, Republic of, 2022, pp. 1996-2001, doi: 10.23919/ICCAS55662.2022.10003669.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 111,
    "chunk_index_in_section": 110,
    "total_chunks_in_section": 111,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 390,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9450000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕_s0_c0",
    "source_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕",
    "text": "基于优先级的时间敏感网络流量调度算法研究\n\n近年来，科技进步使得工业自动化等领域对低时延、高可靠性需求不断提升。传统以太网难以满足新技术传输要求，时间敏感网络（TSN）作为一种新型确定性网络，通过流量整形与调度策略增强带宽能力，改善数据传输确定性，具有良好的互联互通性。研究主要分为时钟同步、流量调度等四个方向，其中流量调度是实现确定性传输的核心。\n\n本文针对现有调度方法计算复杂度高和路由规划问题，提出以下工作：\n\n1. 研究TSN的完全集中式配置模型，结合软件定义网络（SDN）构建系统模型，形式化表达网络模型与流量模型。\n2. 针对时间触发流，改进调度对象为时间窗口，设计整数线性规划（ILP）调度方法，优化端到端时延性能。\n3. 面向时间触发流的路由优化问题，提出基于绝对优先级的实时调度路由算法，提高流量的可调度性。\n\n仿真实验验证了调度与路由优化算法的有效性。\n\n关键词：时间敏感网络，软件定义网络，流量调度，整数线性规划，路由优化，绝对优先级",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 428,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕_s1_c0",
    "source_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕",
    "text": "In recent years, advancements in technology have led to increasing demands for low latency and high reliability in fields such as industrial automation. Traditional Ethernet struggles to meet the transmission requirements of new technologies. Time-sensitive networking (TSN), as a new type of deterministic network, enhances bandwidth capacity and improves transmission determinism through traffic sh",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 1,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 7,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 400,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9900000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕_s1_c1",
    "source_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕",
    "text": "proves transmission determinism through traffic shaping and scheduling strategies, offering good interconnectivity. Research focuses on four main directions, including clock synchronization and traffic scheduling, with the latter being crucial for achieving deterministic transmission.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 2,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 7,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 285,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕_s1_c2",
    "source_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕",
    "text": "This thesis addresses the high computational complexity of existing scheduling methods and routing planning issues, and presents the following contributions:",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 3,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 7,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 157,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕_s1_c3",
    "source_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕",
    "text": "1. A fully centralized configuration model for TSN is studied, and a system model is constructed combining software-defined networking (SDN), with formalized network and traffic models.\n2. For time-triggered flows, the scheduling object is improved to time windows, and an integer linear programming (ILP) scheduling method is designed to optimize end-to-end delay performance.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 4,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 7,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 377,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕_s1_c4",
    "source_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕",
    "text": "3. A routing optimization algorithm based on strict priority is proposed for time-triggered flows, enhancing traffic schedulability.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 5,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 7,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 132,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕_s1_c5",
    "source_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕",
    "text": "Simulation experiments verify the effectiveness of the proposed scheduling and routing optimization algorithms.\n\nKeywords: Time-sensitive networking, Software-defined networking, Traffic scheduling, Integer linear programming, Routing optimization, Strict priority\n\n---\n3.2.1 调度变量定义\n3.2.2 调度约束表达\n3.2.3 目标函数定义\n3.2.4 实验仿真与验证\n3.3 改进时间窗口偏移量\n   3.3.1 时间窗口偏移量优化方法\n   3.3.2 实验仿真与验证",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 6,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 7,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 374,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕_s1_c6",
    "source_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕",
    "text": "第四章基于绝对优先级的TT流路由规划方法\n4.1 TT流路由问题分析\n4.2 基于绝对优先级的实时调度路由算法\n   4.2.1 优先级选择\n   4.2.2 算法设计\n4.3 实验仿真与验证\n   4.3.1 仿真环境介绍\n   4.3.2 仿真平台搭建\n   4.3.3 仿真实验结果分析\n4.4 本章小结\n\n第五章结论与展望\n5.1 主要结论\n5.2 研究展望",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 7,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 7,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 183,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9450000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕_s2_c0",
    "source_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕",
    "text": "---\n\n根据您的要求，以下是清洗后的内容：\n\n---\n\nYu Q等人[38]提出了一种自适应组路由算法，通过预处理、调度合成和后处理三个阶段对路由进行自适应分组，简化整体拓扑结构并寻找最优路由路径。Bülbül S等人[39]重点关注路由问题的求解效率，提出了一种强化学习算法，通过不断与网络环境交互来学习最佳路由策略，得到接近最优的路由规划（接近1.5%）。Feng Z等人[40]关注在线调度过程中TT流路由冗余问题，基于离线路由和调度算法提出了一种更有效的启发式路由算法，有效提升了路由冗余度。Yu Q等人[41]同样面向在线调度过程，设计了一种启发式广度优先搜索路由算法。离线阶段通过引入最小距离树增加可重用调度结果，在线阶段通过所提路由算法尽可能的重用离线阶段的结果，有效解决了TSN面临动态传输需求变化引起的挑战。\n\n不同业务流在TSN交换机的相同出端口转发时导致的相互冲突也是调度过程中需要考虑的要素，这需要对帧传输时隙的求解和路由路径的优化进行综合考虑。一些学者已经展开了对路由与调度联合方案的研究。Atallah A等人[42]基于迭代整数线性规划调度(IIS)技术，采用一种流分区机制降低IIS迭代之间的冲突度(Degree of Conflic,DoC)，提高IIS的成功率；并由DoC感知的多路径路由技术保证容错性，进一步提高调度成功率。杨思锦等人",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 8,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 24,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 587,
    "chunk_method": "hierarchical",
    "importance_weight": 0.44000000000000006,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕_s2_c1",
    "source_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕",
    "text": "[43]基于深度强化学习设计了联合路由和缓存队列的混合资源调度方案，通过路由与调度联合方案实现了调度过程的迭代更新，有效提高了TT流可调度性。Yang L等人[44]基于图卷积网络解决实际通信场景中的联合优化问题，提出深度强化学习解决方案，采用优先级经验重放来加快模型训练过程的收敛速度，提高了联合优化任务的可行性。Xu L等人[45]同样基于强化学习，设计了一种可扩展调度路由协同设计体系结构。基于探索的领域知识和实际需求的特征流数据集建立了一种流划分方法，并构造了调度与路由协同设计约束，提高了TT流的可调度性。Li J等人",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 9,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 264,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕_s2_c2",
    "source_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕",
    "text": "[46]解决了需要冗余和安全情况下的时间敏感网络路由和调度联合问题，提出了一种基于模拟退火的元启发式算法，通过求解路由与调度联合规划的约束条件找到成本函数的最优解。\n\n综上，目前国内外对于TT流路由规划方法的研究主要基于最短路径优先算法，通过随机自适应搜索、周期感知、负载均衡等思想对最短路径进行优化，同时基于强化学习、图卷积网络等思想进行路由与调度联合方案的研究。但在进行路由规划时大多数研究主要考虑链路利用率，对于实时调度场景下流量的可调度性问题考虑不足，这将导致新增流量与现有节点中流量发生冲突从而产生调度失败。\n\n---\n\n以上内容已去除页眉、页脚、页码、重复信息和乱码，保留了核心学术内容、技术术语和数据、原有逻辑结构、公式和图表说明。\n\n清洗后的内容如下：\n\n---\n\n的带宽和优先级，确保TT流的传输质量。\n\n(3)多流量控制模块",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 10,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 372,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕_s2_c3",
    "source_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕",
    "text": "清洗后的内容如下：\n\n---\n\n的带宽和优先级，确保TT流的传输质量。\n\n(3)多流量控制模块\n\n此模块部署于交换机的出端口，将已经通过流量过滤与分类之后的各种类型业务流量进行优先级排队、流量调度和整形等操作，提高对流量的处理效率和可靠性，模块实现过程示意如图2-5所示。流量通过交换机入端口的逐流过滤和监管模块后会被分为TT流、AVB流和BE流三种业务类型，然后进入多流量控制模块中进行优先级排队，根据流量业务类型与优先级选择对应算法进行流量调度与整形，最后在交换机出端口按调度后顺序将各种流量进行转发传输。由于本章设计的TSSDN系统模型主要为了方便TT流调度方法的设计，因此流量调度过程主要建立在IEEE\n802.1 Qbv",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 11,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 315,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕_s2_c4",
    "source_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕",
    "text": "[52]标准所定义的门控调度机制基础之上，通过控制各优先级队列的开关门时刻来隔离其他业务流对TT流的传输影响，门控机制的实现过程将在后文进行详细介绍。\n\n图2-5 多流量控制模块实现过程\n\nAVB流和BE流属于事件触发类流量，其中BE流是无时延要求的流量，只需要在其他流量无传输任务时进行传输即可，无需进行额外的流量整形操作。而AVB流主要承载时延要求比TT流低的软实时应用类流量，此类流量多是突发性的媒体数据，因此需要对其进行流量整形控制突发数据流的速率，保证数据流的稳定传输。对于AVB流的整形是通过IEEE 802.1Qav",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 12,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 265,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕_s2_c5",
    "source_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕",
    "text": "[53]标准中所定义的基于信用的整形机制(Credit Based Shaper，CBS)来实现的。通过CBS机制，可以在不影响TT流优先级的情况下，尽可能保证AVB流的传输质量，使得两者同时有效传输。在CBS机制中，数据只有在队列的信用值(credit)大于等于0时才被允许传输，CBS机制通过4个关键参数进行配置：发送速率sendSlope、空闲速率idleSlope、信用值上限hiCredit、信用值下限loCredit。CBS机制的实现步骤如下：\n\n1)设置credit：为每个队列设置一个信用值，初始时可以将信用值置为0。\n\n2)数据传输准备：当一个队列中的AVB流准备传输数据时，需要检查该队列的credit是否大于等于0。\n\n3)数据传输：\n\ncredit下降：当AVB流开始传输数据时，队列的信用值以发送速率sendSlope为斜率线性下降。也就是说随着数据的传输，队列的信用值会逐渐减小。\n\n停止传输：如果其他队列正在传输数据，或者当前队列的信用值降至0以下，则当前队列必须停止传输数据。\n\ncredit增加：一旦停止传输，队列的信用值以空闲速率idleSlope为斜率线性增加。这意味着在数据传输暂停期间，队列的信用值会慢慢增加。\n\n如果队列的信用值credit累积到hiCredit或者降低到loCredit，那么当前credit保持不变。",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 13,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 582,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕_s2_c6",
    "source_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕",
    "text": "如果队列的信用值credit累积到hiCredit或者降低到loCredit，那么当前credit保持不变。\n\n4)数据等待：如果队列中的AVB流没有数据等待传输，那么CBS机制会自动将该队列的credit调整为0。\n\n5)重复步骤：以上步骤会循环执行，以确保在数据传输和数据等待之间保持平衡，同时根据credit的变化来控制数据的传输。\n\n通过以上步骤，CBS机制可以动态调整传输AVB流队列的信用值，保证AVB流的服务质量。CBS机制信用值变化过程如图2-6所示。\n\n图2-6 CBS机制信用值变化过程示意图\n\n(4)TT流调度模块\n\n此模块是为了对控制层下发的TT流调度方法进行配置而设计的，调度方法中通过调度表配置TT流的传输时间。所有网络设备都需要按照调度表中的规定进行流量调度，以确保TT流的及时传输。调度方法的设计将在后文详细展开。\n\n(5)SRP模块\n\n此模块封装了上文所介绍的SRP机制，通过为TT流预留大部分带宽优先保证TT流传输，同时协调AVB流和BE流的带宽资源，保证它们能够达到各自的业务流量要求。\n\n---\n\n以上内容已去除页眉、页脚、页码、重复信息、乱码、PDF解析错误、多余符号和格式问题，保留了核心学术内容、技术术语和数据、原有逻辑结构、公式和图表说明。\n\n清洗后的内容如下：\n\n---",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 14,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 560,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕_s2_c7",
    "source_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕",
    "text": "清洗后的内容如下：\n\n---\n\n帧到时间窗口赋值约束定义了时间窗口赋值变量，表示帧实例是否被赋值给时间窗口，其值限制为0或1。每一帧都必须分配给唯一为该链路定义的时间窗口，因此要求某一特定帧的所有时间窗口赋值变量的和为1。\n\n时间窗口大小约束定义了时间窗口大小为分配给某一特定时间窗口的所有帧传输时长之和。另外，时间窗口打开和关闭事件的时间间隔必须等于时间窗口大小。\n\n传输约束规定了帧通过路径上每条链路的时序，即同一帧在前驱链路上的结束时刻必须小于等于后继链路上的开始时刻。\n\n端到端时延约束要求流的最后一帧位于最后一条链路的时间窗口关闭时刻与第一帧位于第一条链路的时间窗口打开时刻之间的差值小于等于流所能容忍的最大端到端时延。\n\n抖动约束要求流的时延变化小于等于流所能容忍的最大抖动。\n\n隔离约束规定了一条链路同一时刻只能存储一条流的帧。\n\n目标函数定义了以时间窗口为调度对象的流量调度问题可以抽象成多维背包问题。目标是确定各个时间窗口的开闭时刻和时间窗口赋值变量，使得所有被调度流的总端到端时延最小化。\n\n实验仿真与验证部分描述了仿真实验环境，实验方法及结果分析。结果显示，本文提出的基于时间窗口的ILP调度方法求解时间减少了41%，但总时延提高了7%。\n\n---\n\n以上内容为清洗后的片段内容，保留了核心学术内容，去除了页眉、页脚、页码等非核心内容。\n\n清洗后的内容如下：",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 15,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 590,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕_s2_c8",
    "source_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕",
    "text": "1. 在上节的实例中，交换机1 SW出端口有TT-1和TT-2两条流通过链路1 1 [ , ] ES SW进入交换机中，两条流的周期均为100μs，因此可进行空闲空间判断。在100μs周期内两条流共计传输2个帧，单个帧的帧长为1500B，根据链路速率可得单个帧的传输时长为12μs。因此1 SW的流量周期百分比为24%，空闲空间为76%，空闲空间较大。而在基于时间窗口的ILP方法设计时，并没有考虑到空闲空间因素，因此可从缩短时间窗口的时间间隔入手解决此问题。\n\n2. 在同一个交换机1 SW出端口存在来自不同链路1 1 [ , ] ES SW和2 1 [ , ] ES SW的时间窗口争用问题，这同样会影响端到端时延性能。基于上述的原因分析，本节引入时间窗口偏移量[ , ] a b v v k Offset来对调度方法进行改进。时间窗口偏移量指的是时间窗口相对于超周期的起始位置，改进时间窗口偏移量约束如式(3-21)所示。设链路[ , ] a b v v存在同周期的不同流量传输，且链路[ , ] a b v v共有n个时间窗口：\n\n3. 将新的约束加入上节的调度方法中，重新得到时间窗口赋值变量与各时间窗口的开闭时刻如表3-4和表3-5所示。",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 16,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 525,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕_s2_c9",
    "source_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕",
    "text": "3. 将新的约束加入上节的调度方法中，重新得到时间窗口赋值变量与各时间窗口的开闭时刻如表3-4和表3-5所示。\n\n4. 改进后的调度方案如图3-9所示，此时可以观察到在1 2 [ , ] SW SW链路上共有5个时间窗口，这是因为经过时间窗口偏移量的改进之后会将之前时间窗口之间多余的空闲空间进行合并消除。\n\n5. 将改进时间窗口偏移量的ILP调度方法与基于帧的ILP调度方法、基于时间窗口的OMT调度方法以及改进之前的调度方法进行对比，如图3-10~图3-11所示，改进时间窗口偏移量的ILP调度方法总时延为135μs。分别与基于帧的ILP调度方法和基于时间窗口的OMT调度方法相比，求解时间分别减少了36%和61%，有效降低了计算复杂度；总时延分别降低了47%和40%，有效改善了时延性能。",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 17,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 347,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕_s2_c10",
    "source_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕",
    "text": "6. 本章研究了时间敏感网络中的流量调度技术，针对目前门控调度方法中计算复杂度过高的问题，提出了基于时间窗口的ILP调度方法。将调度变量由求解所有帧实例的传输时刻改进为求解时间裕度，并根据调度变量设计了约束条件与目标函数，同时提出了时间窗口偏移量优化方法改善时延性能。基于Python搭建仿真实验平台并使用Gurobi优化器得出调度结果，通过仿真实验验证了本章提出的调度方法的有效性，所提方法在满足时间触发流的流量特性基础上有效降低了计算复杂度。\n\n清洗后的内容如下：\n\n---\n\n成传输。然后将符合要求的路由进行基于SP的传输选择算法计算，确保新增加的流优先级较高时不会导致与其他流发生帧抢占，避免影响其他流的可调度性。\n\n算法伪代码如表4-1所示。\n\n图4-1 WCED与WCLL各值对应关系\n\n表4-1 RS-SP算法伪代码\n\nReal-time Scheduling Strict Priority Routing Algorithm (RS-SP)\n\n1） Input: Time-triggered (TT) streams S; The current network topology (, ) Gn l.\n\n2） Output: The optimal transmission path ip in the network topology (, ) Gn l.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 18,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 591,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕_s2_c11",
    "source_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕",
    "text": "2） Output: The optimal transmission path ip in the network topology (, ) Gn l.\n\n3） Initiate: Divide TT streams S into small partition is according to the period i.\n\n4） for each stream is S ∈ do\n\n5） Set temporary variable (, ) G G n l ̃= ;",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 19,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 238,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕_s2_c12",
    "source_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕",
    "text": "4.3 实验仿真与验证\n\n4.3.1 仿真环境介绍\n\n本节使用OMNET++搭建TSN仿真平台，仿真实验环境配置在4GB RAM和35GB SCSI的Ubuntu虚拟机上。基于IEEE 802.1Qbv协议栈对TSN网络进行建模，并安置所提出的RS-SP算法进行仿真实验，通过丢包率和端到端时延两个性能指标验证所提算法的有效性。\n\n4.3.2 仿真平台搭建\n\n本章使用OMNET++搭建仿真平台，建立网络拓扑模拟真实通信场景下的发送端、接收端、交换机，通过NeSTiNg组件将TSN功能配置到交换机上，实现路由转发的模拟与验证。TSN网络拓扑示例如图4-3所示，此拓扑共包含7个节点与2个TSN交换机，节点的数量与位置均可以通过参数进行设置，本文共进行了3节点、5节点、7节点、9节点、11节点的仿真实验，将在后续参数设置表中进行呈现。\n\n4.3.3 仿真实验结果分析\n\n本文将所提出的RS-SP算法与SP-TSA算法\n[64]、CBS-TSA算法[64]、SPF算法",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 20,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 434,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕_s2_c13",
    "source_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕",
    "text": "[65]在不同节点规模的路由场景下进行对比仿真实验，同时结合CBR连接数对仿真实验结果进行分析。RS-SP算法致力于改善TT流的可调度性，这项指标可以通过丢包率和平均端到端时延进行综合判断，仿真实验所得结果如下所示。\n\n(1) 丢包率\n\n仿真实验结果显示本文提出的RS-SP算法整体丢包率较低，即调度成功率较高，尤其在CBR连接数较大时更为明显。\n\n(2) 平均端到端时延\n\n如图4-9所示为不同CBR连接数场景下的各算法平均端到端时延结果对比，整体来看本文所提出的RS-SP算法的平均端到端时延最低，性能最好。但当节点数低于7个时，SP-TSA算法的平均端到端时延性能略优于RS-SP，这是由于拓扑规模较小时路由经过的链路也较少，因此会导致RS-SP将最坏链路时延作为路由决策时往往并不能选择到最优路径。这也说明了RS-SP算法更加适合拓扑规模较大的路由优化场景。",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 21,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 383,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕_s2_c14",
    "source_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕",
    "text": "4.4 本章小结\n\n本章研究了时间敏感网络中的路由优化技术，针对目前实时调度场景下可调度性不足的问题，提出了基于绝对优先级的实时调度路由算法。基于端到端时延的四个组成部分建立路由决策，并将最坏情况下端到端时延与绝对优先级传输选择机制相结合设计路由算法。基于OMNET++下的NeSTiNg模型搭建仿真实验平台，通过仿真实验验证了本章提出的路由优化算法的有效性，所提算法可以有效改善TT流的丢包率与平均端到端时延，有效提高了实时调度场景下TT流的可调度性。\n\n---\n\n以上内容已去除页眉、页脚、页码、重复信息、乱码、PDF解析错误、多余符号和格式问题，保留了核心学术内容、技术术语和数据、原有逻辑结构、公式和图表说明。\n\n在动态场景下，考虑到时间触发流的流量特性进行调度方法设计是一项挑战性的研究方向。本文聚焦于时间触发流的优先调度，这是时间敏感网络中的核心业务流。尽管如此，事件触发流量，其特点为非周期性随机传输，对时延要求不高但承载了大部分应用数据传输，也是重要的。未来研究可以在确保时间触发流性能的基础上，进一步优化事件触发流量的传输性能。\n\n参考文献：",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 22,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 480,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕_s2_c15",
    "source_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕",
    "text": "[1] Wollschlaeger M, Sauter T, Jasperneite J. The Future of Industrial Communication: Automation Networks in the Era of the Internet of Things and Industry 4.0 [J]. IEEE Industrial Electronics Magazine, 2017, 11(1): 17-27.\n[2] Ge X. Ultra-Reliable Low-Latency Communications in Autonomous Vehicular Networks [J]. IEEE Transactions on Vehicular Technology, 2019, 68(5): 5005-5016.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 23,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 379,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕_s2_c16",
    "source_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕",
    "text": "[3] FINZI A, MIFDAOUI A, FRANCES F, et al. Network Calculus-based Timing Analysis of AFDX networks with Strict Priority and TSN/BLS Shapers [C]. IEEE 13th International Symposium on Industrial Embedded Systems (SIES), 2018: 1-10.\n... [44] Yang L, Wei Y, Yu R, et al. Joint Routing and Scheduling Optimization in Time-Sensitive Networks Using Graph-Convolutional-Network-Based Deep Reinforcement Learning [J]. IEEE Internet of Things Journal, 2022, 9(23): 23981-23994.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 24,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 467,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕_s2_c17",
    "source_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕",
    "text": "[45] Xu L. Learning-Based Scalable Scheduling and Routing Co-Design With Stream Similarity Partitioning for Time-Sensitive Networking [J]. IEEE Internet of Things Journal, 2022, 9(15): 13353-13363.\n[46] Li J, Li Q, Xiong H. Enhancing low‐priority traffic reconfiguration designs in mixed ‐ critical avionics networks [J]. IET Communications, 2023, 17(13): 1524-1540.\n[47] IEEE Standard for Local and metropolitan area networks--Virtual Bridged Local Area Networks Amendment 14: Stream Reservation Protocol (SRP) [S]. IEEE Std 802.1Qat-2010 (Revision of IEEE Std 802.1Q-2005), 2010: 1-119.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 25,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 588,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕_s2_c18",
    "source_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕",
    "text": "[48] 张维杰, 周志勇, 任涛林, 等. 时间敏感网络核心机制及标准化进展研究 [J].仪器仪表标准化与计量, 2021, (03): 4-7.\n\n---\n\n[49] IEEE Standard for Local and Metropolitan Area Networks--Bridges and Bridged Networks -- Amendment 31: Stream Reservation Protocol (SRP) Enhancements and Performance Improvements. IEEE Std 802.1Qcc-2018, 2018.\n\n[50] Kreutz D, Ramos V, Veríssimo P, et al. Software-Defined Networking: A Comprehensive Survey. Proceedings of the IEEE, 2015, 103(1): 14-76.\n\n[51] IEEE Standard for Local and Metropolitan Area Networks--Timing and Synchronization for Time-Sensitive Applications. IEEE Std 802.1AS-2020, 2020.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 26,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 589,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕_s2_c19",
    "source_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕",
    "text": "[52] IEEE Standard for Local and metropolitan area networks -- Bridges and Bridged Networks - Amendment 25: Enhancements for Scheduled Traffic. IEEE Std 802.1Qbv-2015, 2016.\n\n[53] IEEE Standard for Local and Metropolitan Area Networks - Virtual Bridged Local Area Networks Amendment 12: Forwarding and Queuing Enhancements for Time-Sensitive Streams. IEEE Std 802.1Qav-2009, 2010.\n\n[54] IEEE Standard for Local and metropolitan area networks--Frame Replication and Elimination for Reliability. IEEE Std 802.1CB-2017, 2017.\n\n[55] 平莉. 时间敏感网络流量调度算法研究. 西安电子科技大学, 2022.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 27,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 564,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕_s2_c20",
    "source_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕",
    "text": "[55] 平莉. 时间敏感网络流量调度算法研究. 西安电子科技大学, 2022.\n\n[56] 于珊珊. 时间敏感网络门控调度机制研究. 西安电子科技大学, 2022.\n\n[57] 喻学才, 张田文. 多维背包问题的一个蚁群优化算法. 计算机学报, 2008.\n\n[58] Huang K, Wu J, Jiang X, et al. A Period-Aware Routing Method for IEEE 802.1Qbv TSN Networks. Electronics, 2021, 10(1): 58.\n\n[59] Feng T, Yang H. SMT-based Task- and Network-level Static Schedule for Time Sensitive Network. 2021 International Conference on Communications, Information System and Computer Engineering (CISCE), 2021.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 28,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 466,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕_s2_c21",
    "source_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕",
    "text": "[60] Serna Oliver, Craciunas S, Steiner W. IEEE 802.1Qbv Gate Control List Synthesis Using Array Theory Encoding. 2018 IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS), 2018.\n\n[61] Liu L, James W. Scheduling Algorithms for Multiprogramming in a Hard-Real-Time Environment. ACM, 1973, 20(1): 46–61.\n\n[62] Nakayama Y, Hisano D, Kubo T, et al. Low-latency routing scheme for a fronthaul bridged network. Journal of Optical Communications and Networking, 2018.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 29,
    "chunk_index_in_section": 21,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 480,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕_s2_c22",
    "source_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕",
    "text": "[63] Shrivastava L, Tomar G, Bhadoria S, et al. Effect of Number of CBR Connections on the Performance of a Load Balanced Congestion Adaptive Routing for MANET. 2012 Fourth International Conference on Computational Intelligence and Communication Networks, 2012.\n\n[64] Niklas Reusch, Mohammadreza Barzegaran, Luxi Zhao, et al. Configuration optimization for heterogeneous time-sensitive networks. Real-Time Syst., 2023, 59(4): 705–747.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 30,
    "chunk_index_in_section": 22,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 434,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕_s2_c23",
    "source_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕",
    "text": "[65] Do Y, Oh S, Lim S, et al. Performance Analysis of Traffic Shaping Approaches in Time-Sensitive Networking(TSN). 2024 18th International Conference on Ubiquitous Information Management and Communication (IMCOM), 2024.\n\n---",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 31,
    "chunk_index_in_section": 23,
    "total_chunks_in_section": 24,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 226,
    "chunk_method": "hierarchical",
    "importance_weight": 0.42000000000000004,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c0",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "信息技术的快速发展带来了互联网文本资源的爆炸式增长。信息抽取技术能够从冗余的信息中获取关键信息。关系抽取旨在从自然语言文本中获取实体对之间的关系，是信息抽取领域的一个重要课题，也是知识图谱构建的重要步骤。传统的有监督关系抽取任务需要大量且高质量的人工标注数据，而远程监督关系抽取能够通过启发式对齐非结构化文本和知识库获取大量的训练数据。本文提出了基于包结构的层级图神经网络框架，进一步提升远程监督关系抽取的性能。\n\n具体研究内容和贡献如下：\n\n1) 在远程监督的关系抽取领域，提出了一种从局部到全局进行学习的层级图卷积神经网络框架(L2G-GCN)。该框架在文本编码过程中先通过局部地学习单个实例中的句法知识，然后整体地聚合包结构内实例的语义关联信息，进而提升远程监督关系抽取的性能。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 75,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 342,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9900000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c1",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "2) 提出了一种基于预训练模型的异质图卷积神经网络模型(BH-GCN)。为了加强实体信息对关系抽取任务的指导作用，在L2G-GCN框架的基础上增加对实体信息的处理，设计了BH-GCN。\n\n3) 设计并实现了远程监督关系抽取演示系统。该系统涵盖了用户管理模块、数据管理模块、关系抽取模块、前端展示模块。支持用户自定义输入文本，进行包级别的关系抽取，并对相关结果可视化。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 183,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c2",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "关系抽取在自然语言处理中具有重要意义，它旨在识别文本中实体对之间的关系，并将这些关系以结构化形式存储，以便于知识图谱构建、问答系统完善等应用。然而，传统的关系抽取方法需要大量人工标注数据，限制了其应用范围。远程监督方法通过将大规模知识库与非结构化文本进行对齐，快速生成训练数据，有效缓解了数据匮乏问题。近年来，深度学习方法在关系抽取中取得了显著进展，如卷积神经网络、循环神经网络、注意力机制等。远程监督关系抽取也受到关注，研究者提出了多种基于深度学习的远程监督关系抽取模型，如分段式卷积神经网络、图神经网络等。本文旨在研究基于远程监督的关系抽取，以有效处理远程监督带来的噪声和长尾问题，实现基于远程监督的包级关系抽取演示系统。\n\n论文分为七个章节，组织结构如下：\n\n第一章：介绍研究主题背景和意义，国内外研究现状，以及本文主要研究内容。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 368,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c3",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "第一章：介绍研究主题背景和意义，国内外研究现状，以及本文主要研究内容。\n\n第二章：详细介绍关系抽取和远程监督领域相关术语和背景知识，包括任务定义、远程监督原理和流程、任务定义和形式，以及远程监督关系抽取的优势。\n\n第三章：介绍引入实体信息的异质图神经网络结构，包括预训练语言模型介绍、如何引入预训练语言模型、层级图神经网络结构改进，以及如何构建异质图神经网络。\n\n第四章：主要介绍实验内容，包括数据集统计和特点、评测指标、实验环境和实现细节、实验结果对比，以及对实验结果的具体分析。\n\n第五章：主要阐述远程监督关系抽取系统设计与实现，包括系统框架设计、模块设计，以及系统实现结果展示。\n\n第六章：总结本文主要研究内容和创新点，指出工作不完善之处，提出进一步研究方案。\n\n第二章背景知识与相关技术：介绍关系抽取任务，引出远程监督关系抽取涉及的基础知识，任务定义以及相关技术。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 386,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c4",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "长短期记忆网络结构\nLSTM的核心思想是通过门控机制来决定保留或者忘记细胞状态中的信息。三个门控机制的介绍如下：\n1. 遗忘门的作用是选择性遗忘细胞状态中保留的信息，如公式2-1所示：\n   ft = σ(Wf[ht-1] + Xt) + bf\n   其中σ代表激活函数Sigmoid，输出一个在0到1之间的数值来决定保留或忘记细胞状态Ct-1中的信息。1表示“完全保留”，0表示“完全丢弃”。\n2. 输入门，将新的信息选择性的记录到细胞状态中，具体如下：\n   it = σ(Wi[ht-1] + Xt) + bi\n   Ct~ = tanh(Wc[ht-1] + Xt) + bc\n   Ct = Ct-1 + it * Ct~",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 316,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c5",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "Ct = Ct-1 + it * Ct~\n   其中输入门可分为两个部分，it为在0到1之间的数值来决定更新什么数值，计算如公式2-2，Ct~为候选细胞状态，根据公式2-3计算，用来加入到新的细胞状态中，最后根据公式2-4更新细胞状态为新的细胞状态Ct。\n3. 输出层过Sigmoid函数来确定细胞状态的哪个部分将输出，如公式2-5所示。最后根据公式2-6来确定最后隐藏状态输出的部分。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 5,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 193,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c6",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "预训练语言模型\n近年来，预训练语言模型(PLMs)在多项NLP任务中频频刷新多项纪录。因此也成为NLP领域中深度学习范畴内的中流砥柱。语言模型(LM)根据上下文的文本信息来预测目标词的向量表示，生成相应文本的概率分布。这个过程为无监督的学习过程，无需人工标注数据，因此便于模型从大规模的数据中学习语言特征。预训练语言模型即依据特定的语言任务对语言模型进行训练，然后获得该语料上的语言特征，学习到的参数可以作为模型的初始化。最后使用预训练好的语言模型，当作最基本的编码模型来对接不同的模型，以处理不同的NLP任务。\n图神经网络",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 6,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 262,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c7",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "深度学习在图像视频处理、语音识别、自然语言理解等任务已经大展拳脚。通常这些任务中所处理的数据在欧几里得空间中可以表示。CNN等神经网络结构可以有效的处理这种规则化的矩阵结构数据，即每个节点的周围节点个数是确定的。然而，对于不规则的或非欧几里德空间上的数据，如知识图谱、基因数据、论文引用、社交网络等，传统的神经网络结构如CNN、RNN等都难以处理。这类数据常常可以使用图(Graph)的形式来表示，从而更好地描述其中富含对象之间的复杂关系。图在计算机科学中是一种数据结构，其由顶点和边两部分组成，可以用G={V,E}表示。其中V表示图中节点的集合，E表示图中边的集合，表示了图中从节点i到节点的边。图中节点的连接关系可以用一个矩阵进行量化表示，称为邻接矩阵。对于此类型数据，如果使用图表示学习，可以同时编码图中的节点特征以及图中所含有的结构信息",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 7,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 372,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c8",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "。对于此类型数据，如果使用图表示学习，可以同时编码图中的节点特征以及图中所含有的结构信息。图神经网络(GNN)是一种可以直接作用在图数据结构上的神经网络模型结构。图卷积神经网络(GCN)是其中的一种。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 8,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 100,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c9",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "本文提出了一个简洁的L2G-GCN模型框架，用于学习具有鲁棒性的包表示，以缓解基于远程监督的关系抽取中的噪声问题。模型中，词级别GCN用于编码句子的局部句法信息，句子级GCN用于更好地利用句子间的相关性，将句子间有关实体关系的全局结构信息聚合到一个包表示中。整个自底向上的模型结构采用两级分层的方式进行训练，生成具有多粒度信息的包表示。此外，本文将互信息最大化MIM引入模型，作为正则化器，以提高模型的鲁棒性。在Riedel提出的NYT-10基准数据集上进行了大量实验，结果表明本文提出的分层L2G-GCN模型和层级训练策略是有效的。本文的L2G-GCN实现了卓越的性能。\n\n清洗后的内容如下：\n\n---\n\n征，互信息的计算公式如下：\n\nI(X;Y) = H(Y) - H(Y|X)\n\n其中F为模型预测的结果，A为模型的输入特征。\n\n相应的正则化函数定义如下：",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 9,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 380,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c10",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "其中F为模型预测的结果，A为模型的输入特征。\n\n相应的正则化函数定义如下：\n\nL = E_y[log p(y)] - E_x p(y|x) log p(y|x)\n\n其中，p(f)表示预测的分布，p_0(y|x)表示预测输出的概率。最大化互信息，根据公式3-5可以看作两部分，即最大化H(Y)和最小化H(Y|X)。通过该方案，最大化H(Y)，提升Y的信息熵，即不确定度，预测的不确定性越高，预测的分布越均衡，以此来防止模型倾向于某些类别。最小化H(Y|X)，最小化条件熵，维持模型预测的平衡性，同时也提升了模型在预测时的置信度。受Li等人[61]启发，本文将p(y)作为p(y|x)平均值的近似值。\n\n3.5两阶段层级的训练方式",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 10,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 313,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c11",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "3.5两阶段层级的训练方式\n\n受到人类由局部到整体的认知方式的启发，本文将模型训练分为两个阶段的层级方式。第一阶段，本文先训练词级别的图神经网络直至收敛，从而先获得个良好的局部结构信息。依据公式3-3获得实例的隐藏状态表示后，将个实例向量平均，映射到输出向量即包表不。而后本文通过函数，定义了包正确分类到第k个关系的条件概率：\n\nP(B|0) = 1 if c=i else 0\n\n第二阶段，本文训练完整的模型结构(包含词级别和语句级别的图神经网络结构)直至收敛。具体而言，通过语句级图神经网络结构得到增强的句子表示后，仍用公式3-7得到第二阶段的包表示。而后的分类过程同公式3-8，3-9。该阶段中，模型以端到端的训练方式利用已经学习过的局部结构信息来更好地聚合包内全局的结构信息。整体训练流程图如图3-6所示，损失函数如下所示：\n\nLc = -ΣB∈D log P(B|0)",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 11,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 389,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c12",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "Lc = -ΣB∈D log P(B|0)\n\nLmi = E_y[log p(y)] - E_x p(y|x) log p(y|x)\n\nL = Lc + λLmi\n\n其中|D|表示训练数据中包的个数，B表示包的标签，θ表示模型所有的参数。最终，本文的模型通过随机选取mini-batch来在训练数据集上训练直至拟合，采用随机梯度下降(SGD)优化算法来最小化损失函数。\n\n---\n\n以上内容为清洗后的片段。\n\n第四章基于预训练模型的异质图神经网络远程监督关系抽取模型\n\n本章主要介绍了一种基于预训练模型的异质图神经网络远程监督关系抽取模型。该模型主要是在第三章的层级图神经网络结构上进行改进，包括验证预训练模型在远程监督关系抽取任务上的有效性，引入实体信息以更有效地进行关系分类，以及设计信息融合门控机制以更好地融合不同来源得到的信息。模型整体训练流程如算法4所示。\n\n第五章实验设置与结果分析",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 12,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 397,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c13",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "第五章实验设置与结果分析\n\n本章主要介绍本文实验的实验环境、实验数据集、评估指标，以及L2G-GCN模型和BH-GCN模型的实验结果和分析。本文设计了多组不同的对比实验来验证设计模型的有效性。实验环境包括服务器配置、操作系统、编程语言等。实验数据集采用了NYT-10和GDS两个公开数据集。评估指标包括PR曲线、AUC值、Precision@N和Hits@K。实验细节包括模型参数设置、数据预处理等。\n\n清洗后的内容如下：",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 13,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 211,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c14",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "清洗后的内容如下：\n\n为了防止过度拟合，本文对单词嵌入应用dropout=0.1，对Word-GCN和-GCN模块应用dropout=0.3，对所有线性层应用dropout=0.5。本文使用PyTorch框架构建L2G-GCN模型。所有模型参数均采用统一的分布U[0,1]进行初始化。L2G-GCN模型在批大小为128的100个epochs内分两个阶段进行训练。本文采用经典的SGD优化器对L2G-GCN模型进行优化，学习率为0.3。本文将互信息最大化正则化添加到总损失中，系数为0.01。此外，对于异质图模型框架BH-GCN，其采用AdamW优化器进行优化训练，其BERT模型和异质图网络等参数设置如表5-6所示。\n\n5.5实验结果分析\n\n5.5.1对比模型\n\n本文将所设计的模型与之前研究工作提出的基线方法进行比较，包括前三个非神经网络模型方法和其他神经网络方法。以下简要说明了这些方法：",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 14,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 396,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c15",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "1) Mintz等人首次将远程监督应用于关系抽取。这是一种新的关系抽取范式，不需要标注语料，并且适用于任何大小的语料库。他们使用了多类别的逻辑回归模型处理远程监督关系抽取，完成了一项开创性的工作。\n\n2) MultiR: Hofinan等人采用了概率图模型来处理远程监督关系抽取中的关系重叠问题。MultiR模型不仅可以用于包级关系抽取，还可以很好地处理句子级抽取和语料库级抽取。\n\n3) MIMLR: Surdeanu等人使用了一个结合MEL和多标签学习的模型。一种使用隐藏变量联合建模一个实体对和多个标签的方法。\n\n此处介绍的前三种方法为非神经网络方法。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 15,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 279,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c16",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "此处介绍的前三种方法为非神经网络方法。\n\n4) PCNN: Zeng等人采用了基于CNN的模型，设计了分段式的最大池化来向量化实例，并在一个包里选择最有可能符合该关系标签的实例。PCNN方法首次将MIL框架融入了针对远程监督关系抽取的深度学习研究中，并且没有使用复杂的人工设计特征。这对远程监督关系抽取来说是一项开创性的工作。\n\n5) PCNN+ATT: Lin等人设计了PCNN模型的一个变体，该模型在包内多个实例上使用选择性注意力机制来加权实例表示获取包的表示。通过采用句子级别的注意力模型，该研究期望减少噪声实例在训练中的权重，从而减少噪音数据对模型分类的影响。\n\n6) PCNN+HATT: Han等人没有孤立地处理某个关系类型，而是更好地利用了关系的层级信息。在关系间关联信息的基础上，提出了一种新的层级的注意力机制以识别包内的有效实例。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 16,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 374,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c17",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "7) RESIDE: Vashishth等人使用额外的知识信息，包括实体类型和知识库中关系标签的别名，用于对关系预测施加约束。此外，该方法在远程监督关系抽取中首次使用了GCN对文本进行编码。\n\n8) PCNN+KATT: Zhang等人融入了从图嵌入表示中获取的关系信息。此外，利用GCN对显性的知识进行建模。同时，它采用了从粗粒度到细粒度的知识感知的注意力机制来表示包。\n\n9) DISTRE: Alt等人观察到之前的模型倾向于预测数据量多的关系类别。为了解决这个问题，他们通过使用生成式预训练语言模型来引入语义、句法、语用知识。这些特征被认为是识别一些不同关系类别的关键。\n\n10) DOPEN: Gou等人利用实体类型与关系类型之间的的联系，构建了个具有动态生成参数能力的神经网络。此外，还提出了种关系感知的注意力机制来聚合实体类型信息。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 17,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 372,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c18",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "11) DCRE: Shang等人应用了一个无监督的深度聚类方法来检测包内的噪声实例，并且为这些噪声实例生成一个置信度较高的标签。\n\n12) PA-TRP: Cao等人构造了一个共现图来学习文本嵌入，并且从未标记的数据中学习关系原型。该模型通过迁移富含有效训练实例的关系类别知识到长尾数据上，以此提高长尾关系提取的性能。\n\n13) PSAN: Shang等人设计了一种模式感知的自注意力网络结构来识别不同种类的短关系模版。然后将这些模版信息输入预训练模型，来辅助预训练模型捕捉局部的依赖关系和短语结构。\n\n5.5.2结果分析\n\n在这节中本文依照上述指标依次报告主要的实验结果，并且同时分析所提出模型的性能。此外，通过将实验结果与基线方法进行比较，更深入地分析本文的工作和实验结果。\n\n1) PR曲线",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 18,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 349,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c19",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "1) PR曲线\n\n在图5-1中，本文将提出的L2G-GCN模型框架的PR曲线与上述基线方法的PR曲线进行比较，并得到以下观察结果：(1)所有非神经网络基线模型，包括Mintz、MultiR和MIMLR都逊于基于神经网络的基线模型。主要原因是非神经网络方法不能准确地捕捉句子的语义信息，并且实验结果很大程度上取决于人工设计的机器学习特征，这些特征会导致错误传播的问题。(2)从PR曲线看，本论文设计的L2G-GCN模型相比其他基线方法有着更好的性能。特别是当召回率超过0.15时，本文设计的方法都在一定程度上领先于其他方法，这说明本文设计的模型在不同种类的关系类别的预测上有一定的优势。(3)此外，与基于GCN的方法相比，本文的模型不仅利用GCN建模句子中词与词之间的依赖信息，还利用GCN建模了句子之间的相关性来获取句子之间全局的结构信息，因此可以获得一个鲁棒性更强的包表示，以避免噪声的影响。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 19,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 397,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c20",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "2) P@N评价\n\n本文使用包中所有的实例进行模型评估，P@N的实验结果如表5-7所示。观察发现，本文设计的L2G-GCN模型在大部分指标上显著优于所有基线方法。总体而言，本文此处列举了L2G-GCN模型在Top 100、200、300、500、1000和2000的精度表现以及以上六个值平均的精度值，其在P@100、300、500、1000上分别提高了2.4%、1.3%、4.7%和3.8%的模型效果。结果表明本文的模型在整个分布上有着惊人的表现。此外，本论文模型与所有的基线模型相比，在六个指标项的平均值P@MEAN上最高，对比所有基于选择性注意的方法和基于先验知识的方法，其提升了3.7%。结果表明，该模型结合句子中的局部信息和全局结构信息来处理含噪实例时具有较强的鲁棒性。\n\n3) AUC值",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 20,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 349,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c21",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "3) AUC值\n\n本文将L2G-GCN模型与其余基线方法在AUC值上进行比较，如表5-8所示。本文注意到AUC值与上述两个指标PR曲线和P@N值保持趋势一致，即L2G-GCN模型实现了最佳性能，AUC值达到了最高，其提高了2.3%的性能。通过观察PR曲线和P@N表格，可以发现虽然基于选择性的注意力的方法，例如PCNN+HATT，在低召回水平下产生了更高的结果，基于先验知识的方法，如DISTRE在高召回水平下实现了高置信度，但是它们在AUC度量下都具有相似的结果。结果表明，这些方法在数据集的总体分布上具有相似的性能。与上述方法相比，本文的模型通过整合局部结构信息和全局的结构信息以及互信息最大化正则器突破了瓶颈。\n\n4) Hits@K",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 21,
    "chunk_index_in_section": 21,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 319,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c22",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "为进一步证明本文模型的优势，并验证该模型在长尾关系类别的性能，本文报告了Hits@K宏观平均值(MacroAverage)，如表5-9所示。本文从NYT-10中提一个测试数据集的子集，均为训练实例数据数量少于100/200的关系类别数据。每个实体对，本文选择模型给出的前K个候选关系并和正确的关系类别比较。K从{10,15,20}中选择。从表5-8中，本文有以下观察结果。(1)从Hits@K指标而言，之前使用最广泛的方法PCNN+ATT性能相对较差。尤其是在Hits@10指标，只有不到5%的长尾类别实例会进入前10名推荐候选关系。这表明长尾问题仍然是远程监督关系抽取面临的严重问题。(2)先验知识的引入可以提高长尾关系抽取的性能。基于先验知识的方法，如受益于知识图谱中实体描述信息、实体类型信息和关系原型的模型方法如PCNN+KATT、DOPEN、PA-TRP通常比基于注意力的模式更好",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 22,
    "chunk_index_in_section": 22,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 395,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c23",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "。这一改进表明，引入外部知识是一个重要而有效的方法来缓解长尾问题。(3)从总体上看，本文的模型表现出了惊人的性能，在很大程度上改善了模型在长尾数据上的性能。具体来说，对于少于100个训练实例的长尾类别而言，本文的模型比之前最好的模型的性能在Hits@15高出了2.7%，而在Hits@20高出了23.1%。而且本文的模型性能在少于200个训练实例的长尾类别一致的有效性，在Hits@10、Hits@15和Hits@20上都有大幅度增长，即4.5%、6.9%和23.2%。从效果来看，无论是基于注意力的模型还是基于先验知识的方法，本文的模型都优于这些以往的模型，这证明了互信息最大化正则器的有效性。增加了信息最大化后，该模型倾向于预测关系的多样性分布，不会偏向于某些特定的关系标签。换句话说，本文的模型将以更高的概率预测尾部关系类别\n。换句话说，本文的模型将以更高的概率预测尾部关系类别。此外，本文的模型探索了层次结构中句子的局部信息和句子之间全局关系的有效性，其增强了DSRE的包表示。实验结果验证了模型的鲁棒性。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 23,
    "chunk_index_in_section": 23,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 457,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c24",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "为了展示L2G-GCN模型中有关模块的有效性，以及验证第四章提出的异构图网络结构，即BH-GCN的有效性，本文进行了如下对比实验。具体而言，本文移除了L2G-GCN中的一些模块，从而产生了四种变体3PL2G-GCNw/oWord-GCN、L2G-GCNw/oSen-GCN、L2G-GCNw/oHierarchical-training和L2G-GCNw/oMIM。同时，在预训练语言模型BERT的基础上，增加了BERT,+SenGCN,-GCN和BH-GCN四种模型的对比实验。具体地，PR曲线如图5-2所示，P@N值的对比情况如表5-9所示，AUC值的对比情况如表5-10。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 24,
    "chunk_index_in_section": 24,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 289,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c25",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "L2G-GCNw/oSen-GCN表示移除了Sen-GCN模块。该模型Word-GCN对实例中的局部语法信息进行编码。本文观察到，在这种情况下，模型的性能在P@MEAN和AUC下降了3.8%和5.2%。这表明Sen-GCN模块能更加关注包中实例之间的全局语义关联信息，全局信息确实增强了模型的能力。此外，与另一种基于GCN的方法RESIDE相比，可以观察到本文的L2G-GCN模型在没有Sen-GCN模块时，仍具有可比较的性能。换而言之，本文的模型在没有先验知识的情况下性能略好于RESIDE模型。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 25,
    "chunk_index_in_section": 25,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 249,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c26",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "L2G-GCNw/oWord-GCN表示移除Word-GCN模块。其中Sen-GCN模块从一开始就伴随模型训练。本文观察到在这种情况下，P@MEAN和AUC性能下降了4.1%和6.9%。这表明应该首先使用Word-GCN模块编码单实例信息，即在本设计的框架中，从局部到全局的编码是一个必要的过程。此外，还可以观察到L2G-GCNw/oWord-GCN的性能略好于L2G-GCNw/oSen-GCN。它表明，Sen-GCN模块通过捕获与噪声问题更直接相关的包结构中的全局信息，为本文的框架提供了更多帮助。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 26,
    "chunk_index_in_section": 26,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 251,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c27",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "L2G-GCNw/oHierarchical-training表示从L2G-GCN模型中移除分层训练策略。换言之，直接以端到端的方式训练整个模型。与L2G-GCNw/oWord-GCN和L2G-GCNw/oSen-GCN相比，性能有所下降。这表明，两个模块的简单组合不能很好地训练模型，得到较好的效果。Sen-GCN模块可能会干扰Word-GCN模块的学习，导致Word-GCN模块不能很好地编码语法信息。也就证明了从局部到全局的逐步编码过程有利于模型更好地表示包，验证了本文学习策略的有效性。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 27,
    "chunk_index_in_section": 27,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 247,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c28",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "L2G-GCNw/oMIM表示移除了MIM正则化器。其结果显示，模型在P@MEAN还有AUC指标分别下降了2.9%和4.7%。这验证了模型与MIM正则化器相结合可以获得比较好的性能。因此本文采用MIM正则化器来缓解长尾问题。如表5-9所示，在这个指标Hits@K中本文的模型性能大幅提升，尤其是在Hits@20指标下。这表明MIM正则化器通过鼓励模型预测长尾关系类别，对于缓解长尾问题是有效的。此外，AUC值的度量结果表明，完整的层次结构的性能略优于其他变体，充分证明了本文的分层图神经网络结构的有效性。\n\nBERT模型表示使用预训练模型编码文本后，直接使用分类器分类。实验结果可以看出引入预训练语言模型，对远程监督关系抽取任务确实有改善作用。相比词级图神经网络结构，BERT所包含的语义语法知识更丰富，有更强的编码效果。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 28,
    "chunk_index_in_section": 28,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 361,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c29",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "模型+Sen-GCN表示组合BERT和第三章所述的句级图神经网络模块。模型+Hete-GCN表示增加了异质图网络结构，但仅仅使用加和平均的方法融合实体和实例的信息表示。从以上实验结果可以看出增加了Sen-GCN模块后，模型因为学习到实例之间的相关性，效果有了一定的提升。在使用Hete-GCN模块后，因为增强了实体信息对关系抽取的指导作用，实验效果得到了进一步提升。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 29,
    "chunk_index_in_section": 29,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 183,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c30",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "模型+Hete-GCN表示第四章所述的基于预训练模型的异质图神经网络远程监督关系抽取模型。从以上实验结果可以看出，BH-GCN模型在P@N,AUC值，以及PR曲线三个指标上，对比L2G-GCN模型、+Sen-GCN、+Hete-GCN均有所提升。相比于+Sen-GCN模型，BH-GCN在P@MEAN上提升了4.7%,AUC值提升了3.2%。实验结果表明，异质图结构增强的实体指导作用，有效的改善了模型的效果。同时相比于+Hete-GCN模型，完整的BH-GCN模型进一步改善了实验效果，这表明相比于简单直接的融合实例和实体的信息表示，通过异质信息融合模块建模后的实例、实体表达更加完备。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 30,
    "chunk_index_in_section": 30,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 294,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c31",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "在本章中，主要介绍了实验的主要环境、实验所使用的数据集、评估指标，并总结了本论文相关的基线模型方法，与本论文设计的模型实验结果进行比较，对本文模型实验结果进行了展示和分析。同时，本论文做了相关的对比实验，对模型的几种变体进行相关分析，对两个图卷积模块的层数对模型的影响进行分析，最后选取了数据集中的案例进行可视化分析。总体而言，实验结果体现了本文提出的两个模型L2GCN和BH-GCN均能有效提升远程监督关系抽取的性能。\n\n第六章远程监督关系抽取平台设计与实现",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 31,
    "chunk_index_in_section": 31,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 230,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c32",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "第六章远程监督关系抽取平台设计与实现\n\n本论文基于上述远程监督关系抽取的研究，结合上文第三章提出的引入互信信息的最大化层级图神经网络结构和第四章提出的异质图网络结构，搭建了基于包级别的关系抽取演示平台。本章将详细介绍搭建的演示系统的实现原理和相关技术，并对系统的组成模块进行介绍。针对本论文的主要研究内容，系统需要完成的模块主要有：数据管理模块，用户管理模块，关系抽取模块，前端展示模块。\n\n6.1系统需求分析\n\n根据研究内容，远程监督关系抽取系统主要需要以下四个功能模块，功能需求分析如下，用户用例图如图6-1：\n\n1)用户管理模块：该系统面向众多用户群体，为了区分用户数据，需要提供用户管理服务，保证拥有访问权限的用户才能进入系统。该模块具体包括用户注册、登录、登出功能。系统根据某一ID确定用户，用数据库记录用户的相关信息。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 32,
    "chunk_index_in_section": 32,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 365,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c33",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "2)数据管理模块：用户与系统的交互时刻伴随着数据，对数据进行记录并一个系统必不可少的服务。对于用户来说，系统需要对用户的相关信息记录到数据库中，此外本系统支持用户上传文件，进行批量分析，并下载分析结果。\n\n3)关系抽取模块：本系统基于本论文提出的两个算法模型，进行包级别的关系抽取系统。用户可根据需求指定模型，根据需求输入自定数量的语句，对文本进行关系抽取。\n\n4)前端展示模块：本系统根据用户的需求，用户可自定义输入实例数量，根据抽取结果，实时展现抽取三元组，并对抽取结果进行知识图谱可视化。此外，本系统还同时考虑了系统非功能性需求。\n\n6.2系统总体设计框架",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 33,
    "chunk_index_in_section": 33,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 281,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c34",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "6.2系统总体设计框架\n\n针对以上所述功能需求，本工作构建的演示平台整体框架如图6-2所示。为了方便用户交互，维修方便，本工作基于浏览器和服务器结构进行系统搭建，前端负责与用户交互，以可视化的方式展示关系抽取结果；后端进行数据处理，模型管理，关系预测。在该关系抽取系统中，用户通过浏览器输入或上传文本数据，向服务器发送请求，服务器接受用户数据和请求，并完成相应的操作，将抽取出关系三元组以及相关信息返回浏览器，并做出相应的可视化展示。\n\n6.3系统详细功能设计与实现\n\n6.3.1用户管理模块\n\n当新用户使用本系统时，需要先填写图6-4中的相关信息来注册账号。本系统以用户id作为数据库主键，即唯一ID。\n\n6.3.2关系抽取模块",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 34,
    "chunk_index_in_section": 34,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 315,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c35",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "6.3.2关系抽取模块\n\n关系抽取模块是本系统的核心模块。主要实现了模型切换、实体指定、数据输入、模型加载，抽取结果，结果可视化功能。用户通过指定实体，输入文本信息，通过配置关系抽取模型，便可对数据打包，进行包级关系抽取。\n\n6.3.3数据管理模块\n\n数据管理模块主要负责数据的存储、读取、预处理等功能。本系统主要使用SQLite进行用户信息的存储，数据表设计如下表6-1：\n\n6.4系统测试\n\n6.4.1功能测试\n\n测试模块 测试功能 测试内容 测试结果 结论\n\n输入非法格式的提示相应信息格 校验功能正常 测试数据式错误 注册\n\n注册功能有有效 正常注册 注册功能正常\n\n输入错误密码 无法登录 校验功能正常 登录\n\n登录功能有效性 正常进入系统 登录功能正常\n\n注销账户 正常退出系统 登出功能正常\n\n多种格式上传成 数据上传 文件上传 文件上传正常",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 35,
    "chunk_index_in_section": 35,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 378,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c36",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "注销账户 正常退出系统 登出功能正常\n\n多种格式上传成 数据上传 文件上传 文件上传正常\n\n数据管理模块 数据下载 预测结果下载 下载成功 数据下载正常\n\n完成系统开发后，对系统含有功能逐一测试，保证系统的正常运行。系统的测试结果如表6-2所示。\n\n6.5本章小结\n\n本章主要介绍了基于远程监督的关系抽取演示系统的设计与实现。首先介绍了系统的功能需求，明确了系统功能。接着介绍了系统的整体框架，本文使用B/S结构搭建系统，使用Bootstrap和Flask的框架组合，结合Python编程语言进行系统开发。然后依次介绍了系统各个模块的实现方法，并演示了相应功能模块的使用。\n\n6.6参考文献",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 36,
    "chunk_index_in_section": 36,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 295,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c37",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "6.6参考文献\n\n[1] Veyseh A P B, Van Nguuyen M, Trung N N, et al. Modeling Document-Level Context for Event Detection via Important Context Selection [A]. // Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing [C]. 2021: 5403-5413.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 37,
    "chunk_index_in_section": 37,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 260,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c38",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "[2] Ding H, Luo X. AttentionRank: Unsupervised Keyword Extraction using Self-and Cross Attentions [A]. // Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing [C]. 2021: 1919-1928.\n\n[3] 孙茂松，李莉，刘知远. 面向中英平行专利的双语术语自动抽取 [J]. 清华大学学报(自然科学版)，2014，54(10): 1339-1343.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 38,
    "chunk_index_in_section": 38,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 292,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c39",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "[4] Agichtein E, Gravano L. Snowball: Extracting Relations from Large Plain-Text Collections [A]. // Proceedings of the fifth ACM conference on Digital libraries [C]. 2000: 85-94.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 39,
    "chunk_index_in_section": 39,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 179,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c40",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "[5] Mintz M, Bills S, Snow R, et al. Distant supervision for relation extraction without labeled data [A]. // Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP [C]. 2009: 1003-1011.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 40,
    "chunk_index_in_section": 40,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 292,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c41",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "[6] Liu C, Sun W, Chao W, et al. Convolutional neural network for relation extraction [A]. // International conference on advanced data mining and applications [C]. Springer, 2013: 231-242.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 41,
    "chunk_index_in_section": 41,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 189,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c42",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "[7] Zeng D, Liu K, Lai S, et al. Relation classification via convolutional deep neural network [A]. // Proceedings of COLING 2014, the 25th international conference on computational linguistics: technical papers [C]. 2014: 2335-2344.\n\n[8] Zhang D, Wang D. Relation classification via recurrent neural network [J]. arXiv preprint arXiv:150801006, 2015.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 42,
    "chunk_index_in_section": 42,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 351,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c43",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "[9] Bahdanau D, Cho K, Bengio Y. Neural Machine Translation by Jointly Learning to Align and Translate [J]. Computer Science, 2014.\n\n[10] Zhou P, Shi W, Tian J, et al. Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification [A]. // Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) [C]. 2016.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 43,
    "chunk_index_in_section": 43,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 389,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c44",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "[11] Hochreiter S, Schmidhuber J. Long short-term memory [J]. Neural computation, 1997, 9(8): 1735-1780.\n\n[12] Kipf T N, Wellinger M. Semi-supervised classification with graph convolutional networks [J]. arXiv preprint arXiv:1609029707, 2016.\n\n[13] Guo Z, Zhang Y, Lu W. Attention guided graph convolutional networks for relation extraction [J]. arXiv preprint arXiv:190607510, 2019.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 44,
    "chunk_index_in_section": 44,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 383,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c45",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "[14] Wei Z, Su J, Wang Y, et al. A novel hierarchical binary tagging framework for joint extraction of entities and relations [J]. arXiv preprint arXiv:190903227, 2019.\n\n[15] Zheng H, Wen R, Chen X, et al. PRGC: Potential Relation and Global Correspondence Based Joint Relation Triples Extraction [A]. // Online: Association for Computational Linguistics. 2021: 6225-6235.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 45,
    "chunk_index_in_section": 45,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 372,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c46",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "[16] Riedel S, Yao L, McCallum A. Modeling relations and their mentions without labeled text [A]. // Joint European Conference on Machine Learning and Knowledge Discovery in Databases [C]. Springer, 2010: 148, 163.\n\n[17] Dietterich T G, Lathrop R H, Lozano-Pérez T. Solving the multiple instance problem with axis-parallel rectangles [J]. Artificial intelligence, 1997, 89(1-2): 31-.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 46,
    "chunk_index_in_section": 46,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 383,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c47",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "[18] Zeng D, Liu K, Chen Y, et al. Distant supervision for relation extraction via piecewise convolutional neural networks [A]. // Proceedings of the 2015 conference on empirical methods in natural language processing [C]. 2015: 1753-1762.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 47,
    "chunk_index_in_section": 47,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 239,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c48",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "[19] Lin Y, Shen S, Liu Z, et al. Neural relation extraction with selective attention over instances [A]. // Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) [C]. 2016: 2124-2133.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 48,
    "chunk_index_in_section": 48,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 242,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c49",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "[20] Vashishth S, JoshI R, Pray E S, et al. Reside: Improving distantly supervised neural relation extraction using side information [J]. arXiv preprint arXiv:181204361, 2018.\n\n[21] Cho K, Van Merrienboer B, Gulcehre C, et al. Learning phrase representations using RNN encoder-decoder for statistical machine translation [J]. arXiv preprint arXiv:14061078, 2014.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 49,
    "chunk_index_in_section": 49,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 362,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c50",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "[22] Ye Z, Ling Z. Distant supervision relation extraction with intra-and inter-bag attentions [J]. arXiv preprint arXiv:1904000143, 2019.\n\n[23] Xing R, Luo J. Distant supervised relation extraction with separate head-tail CNN [A]. // Proceedings of the 5th Workshop on Noisy User-Generated Text (W-NUT) [C]. 2019: 249-258.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 50,
    "chunk_index_in_section": 50,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 323,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c51",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "[24] Shang Y, Huang H, Sun X, et al. A pattern-aware self-attention network for distant supervised relation extraction [J]. Information Sciences, 2022, 584: 269-279.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 51,
    "chunk_index_in_section": 51,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 165,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c52",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "[25] Jiang X, Wang Q, Li P, et al. Relation extraction with multi-instance multi-label convolutional neural networks [A]. // Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers [C]. 2016: 1471-1480.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 52,
    "chunk_index_in_section": 52,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 255,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c53",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "[26] Liu T, Wang K, Chang B, et al. A soft-label method for noise-tolerant distant supervised relation extraction [A]. // Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing [C]. 2017: 1790-1795.\n\n[27] Huang Y, Wang W. Deep residual learning for weakly supervised relation extraction [J].\n\n参考文献：",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 53,
    "chunk_index_in_section": 53,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 330,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c54",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "1. Fenng X, Guo J, Qin B, et al. Effective Deep Memory Networks for Distance Supervised Relation Extraction [C]//IJCAI, 2017.\n2. Fenng J, Huang M, Zhao L, et al. Reinforcement learning for relation classification from noisy data [C]//Proceedings of the AAAI Conference on Artificial Intelligence, 2018.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 54,
    "chunk_index_in_section": 54,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 302,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c55",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "3. Qin P, Xu W, Wang W, et al. Robust distant supervision relation extraction via deep reinforcement learning [J]. arXiv preprint arXiv:1805099927, 2018.\n4. Zeng D, Dai Y, Li F, et al. Adversarial learning for distant supervised relation extraction [J]. Computers, Materials & Continua, 2018, 55(1):121-136.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 55,
    "chunk_index_in_section": 55,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 307,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c56",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "5. Goodfellow I. Nips 2016 tutorial: Generative adversarial networks [J]. arXiv preprint arXiv:170100160, 2016.\n6. Qin P, Xu W, Wang W, et al. DSGAN: Generative adversarial training for distant supervision relation extraction [J]. arXiv preprint arXiv:1805099927, 2018.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 56,
    "chunk_index_in_section": 56,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 269,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c57",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "7. Ji G, Liu K, He S, et al. Distance supervision for relation extraction with sentence level attention and entity descriptions [C]//Proceedings of the AAAI Conference on Artificial Intelligence, 2017.\n8. Bordes A, Usunier N, Garcia-Duran A, et al. Translating embeddings for modeling multi-relational data [J]. Advances in neural information processing systems, 2013, 26: 1-9.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 57,
    "chunk_index_in_section": 57,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 377,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c58",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "9. Han X, Yu P, Liu Z, et al. Hierarchical relation extraction with coarse-grained attention [C]//Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 2018: 2236-2245.\n10. Zhang N, Deng S, Sun Z, et al. Long-tail relation extraction via knowledge graph embeddings and graph convolutional networks [J]. arXiv preprint arXiv:190301306, 2019.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 58,
    "chunk_index_in_section": 58,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 374,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c59",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "11. Alt C, Hieber M, Henning L. Fine-tuning pretrained transformer language models to distantly supervised relation extraction [J]. arXiv preprint arXiv:190608646, 2019.\n12. Gou Y, Lei Y, Liu L, et al. A dynamic parameter enhanced network for distant supervised relation extraction [J]. Knowledge-Based Systems, 2020, 197:105912.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 59,
    "chunk_index_in_section": 59,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 329,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c60",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "13. Cao Y, Kuang J, Gao M, et al. Learning relation prototype from unlabeled texts for long-tail relation extraction [J]. IEEE Transactions on Knowledge and Data Engineering, 2021.\n14. Distiawan B, Weikum G, Qi J, et al. Neural relation extraction for knowledge base enrichment [C]//Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019: 229-240.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 60,
    "chunk_index_in_section": 60,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 386,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c61",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "15. Fan T, Wang H. Research of Chinese intangible cultural heritage knowledge graph construction and attribute value extraction with graph attention network [J]. Information Processing & Management, 2022, 59(1):102753.\n16. Shin S, Jin X, Jung J, et al. Predictive constraints based question answering over knowledge graph [J]. Information Processing & Management, 2019, 56(3):445-462.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 61,
    "chunk_index_in_section": 61,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 384,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c62",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "17. Krallinger M, Rodriguez-Penagos C, Tendulkar A, et al. PLAN2L: a web tool for integrated text mining and literature-based bioentity relation extraction [J]. Nucleic Acids Research, 2009, 37(suppl 2):W160-W165.\n18. Zelenko D, Aone C, Richardella A. Kernel methods for relation extraction [J]. Journal of Machine Learning Research, 2003, 3(Feb):1083-1106.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 62,
    "chunk_index_in_section": 62,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 357,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c63",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "19. Kambhatla N. Combining lexical, syntactic, and semantic features with maximum entropy models for information extraction [C]//Proceedings of the ACL Interactive Poster and Demonstration Sessions, 2004:178-181.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 63,
    "chunk_index_in_section": 63,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 212,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c64",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "20. Bunescu R C, Mooney R J. A shortest path dependency kernel for relation extraction [C]//Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, 2005:724-731.\n21. Tang H, Sun X, Jin B, et al. Improving Document Representations by Generating Pseudo Query Embeddings for Dense Retrieval [J]. arXiv preprint arXiv:210503599, 2021.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 64,
    "chunk_index_in_section": 64,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 387,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c65",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "22. Wang Y, Sun C, Wu Y, et al. UniRE: A Unified Label Space for Entity Relation Extraction [J]. arXiv preprint arXiv:210704292, 2021.\n23. Li Z, Zou Y, Zhang C, et al. Learning Implicit Sentiment in Aspect-based Sentiment Analysis with Supervised Contrastive Pre-training [J]. arXiv preprint arXiv:2111102194, 2021.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 65,
    "chunk_index_in_section": 65,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 315,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c66",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "24. Devlin J, Chang M-W, Lee K, et al. Bert: Pre-training of deep bidirectional transformers for language understanding [J]. arXiv preprint arXiv:1810048205, 2018.\n25. Radford A, Narasimhan K, Salimans T, et al. Improving language understanding by generative pre-training [J]. 2018.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 66,
    "chunk_index_in_section": 66,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 282,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c67",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "26. Yang Z, Dai Z, Yang Y, et al. Xlnet: Generalized autoregressive pre-training for language understanding [J]. Advances in Neural Information Processing Systems, 2019, 32: 5-24.\n27. Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need [J]. Advances in Neural Information Processing Systems, 2017, 30: 5998-6008.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 67,
    "chunk_index_in_section": 67,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 325,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c68",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "28. Shang Y, Huang H-Y, Mao X-L, et al. Are noisy sentences useless for distant supervised relation extraction? [C]//Proceedings of the AAAI Conference on Artificial Intelligence, 2020:8799-8806.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 68,
    "chunk_index_in_section": 68,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 195,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c69",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "29. Li Y, Long G, Shen T, et al. Self-attention enhanced selective gate with entity aware embedding for distantly supervised relation extraction [C]//Proceedings of the AAAI Conference on Artificial Intelligence, 2020:8269-8276.\n30. Deng X, Sun H. Leveraging 2-hop distant supervision from table entity pairs for relation extraction [J]. arXiv preprint arXiv:1909060007, 2019.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 69,
    "chunk_index_in_section": 69,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 376,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c70",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "31. Yuan Y, Liu L, Tang S, et al. Cross-relation cross-bag attention for distantly supervised relation extraction [C]//Proceedings of the AAAI Conference on Artificial Intelligence, 2019:419-426.\n32. 黄兆玮，常亮，宾辰忠，等. 基于GRU和注意力机制的远程监督关系抽取 [J]. 计算机应用研究，2019，36(10):2930-2933.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 70,
    "chunk_index_in_section": 70,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 270,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c71",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "33. Mrini K, Demoncourt F, Buys T, et al. Rethinking self-attention: An interpretable self-attentive encoder-decoder parser [J]. 2019.\n34. Li B, Wang Y, Che T, et al. Rethinking distributional matching based domain adaptation [J]. arXiv preprint arXiv:200613352, 2020.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 71,
    "chunk_index_in_section": 71,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 268,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c72",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "35. Jat S, Khandelwal S, Talukdar P. Improving distantly supervised relation extraction using word and entity based attention [J]. arXiv preprint arXiv:180406987, 2018.\n36. Paszke A, Gross S, Massa F, et al. Pytorch: An imperative style, high-performance deep learning library [J]. Advances in Neural Information Processing Systems, 2019, 32: 8024-8035.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 72,
    "chunk_index_in_section": 72,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 353,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c73",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "37. Hof &inann R, Zhang C, Ling X, et al. Knowledge-based weak supervision for information extraction of overlapping relations [C]//Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies, 2011:541-550.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 73,
    "chunk_index_in_section": 73,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 263,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦_s0_c74",
    "source_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
    "text": "38. Surdeanu M, Tibshirani J, Nallapati R, et al. Multi-instance multi-label learning for relation extraction [C]//Proceedings of the die 2012 joint conference on empirical methods in natural language processing and computational natural language learning, 2012:455-465.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 74,
    "chunk_index_in_section": 74,
    "total_chunks_in_section": 75,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 270,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9450000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元_s0_c0",
    "source_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元",
    "text": "关系抽取任务是指根据语义信息判断文本中两个实体的关系，从而将非结构化文本转化成结构化知识。传统的关系抽取模型需要较多的标注数据，并且很难实现新型关系的建模。为了降低标注成本且满足新型关系的建模需求，少样本关系抽取任务逐渐成为研究热点。少样本关系抽取任务通过少量标注数据建模，保证模型的泛化能力是该任务的一大挑战。近期的研宄通过将知识库融入少样本关系抽取模型，在该任务取得了较大的进展。但在现实应用中，特别是存在领域迁移的情况下，知识库的来源和类型可能存在较大的差异，知识库和融入模块的泛化能力很难得到保证。\n\n在上述任务背景下，本文针对少样本关系抽取任务下的知识融入和领域迁移问题上进行研究，具体研究工作如下：\n\n1. 利用实体概念作为融入知识，寻找不同类型和来源的知识库的有效融入方式。针对图类型的知识库，设计了结合语义门控机制和距离打分器的知识融合模块，该知识融合方式能在不同领域间的知识库上实现有效迁移。针对文本类型的知识库，将文本知识以模板的形式输入语言学模型，提高了模型对于知识的感知。此外，针对已知目标领域的情况，设计了领域导向的元训练方法，从训练数据中获取与领域更为相关的知识，并在训练过程中利用样本对维度的正则项来约束样本的特征表示，提高训练的稳定性。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 21,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 534,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元_s0_c1",
    "source_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元",
    "text": "2. 基于上述模型，设计并实现了少样本关系抽取系统。该系统支持用户定义关系，包括用户管理、数据管理、图谱管理等模块；通过上述模块完成训练数据和测试数据的上传，经过离线训练实现少样本关系抽取的自动化流程。系统测试结果表明设计的系统能够高效完成新型关系的快速抽取工作。\n\n关键词：关系抽取少样本学习领域迁移知识库\n\n知识图谱的构建包括实体抽取和关系抽取两部分。实体抽取方法包括基于Pipeline的方法和联合抽取方法。基于Pipeline的方法通过依次抽取实体和关系来完成知识图谱的构建，而基于联合抽取的方法则是直接抽取知识三元组。早期的实体抽取方法依赖人工抽取或构建规则，而近年来，基于统计学习和深度学习的方法逐渐应用于实体抽取，并取得了显著的性能提升。\n\n关系抽取是指根据文本描述判断文本中两个实体之间的关系。传统的关系抽取任务利用高质量的标注数据来训练关系抽取模型。根据模型特点，传统的关系抽取模型分为基于特征、基于核、深度学习和联合抽取等方法。基于特征的关系抽取模型通过特征编码器和分类器进行建模，而基于核函数的关系抽取模型利用核函数计算文本间的实体关系相似度。基于深度学习的关系抽取模型通过词向量技术和深度学习模型进行结合，通过大量数据训练实现关系抽取任务性能的显著提升。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 539,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元_s0_c2",
    "source_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元",
    "text": "少样本关系抽取方法的发展现状可分为三类：基于元学习的少样本关系抽取方法、基于预训练模型的少样本关系抽取方法和基于知识增强的少样本关系抽取方法。基于元学习的少样本关系抽取方法会利用较多的关系类别数据进行元训练，然后在少样本任务上进行微调。基于预训练模型的少样本关系抽取方法通过语言模型学习到的知识来解决样本不足的问题。基于知识增强的少样本关系抽取方法将现有的知识库融入模型，提升少样本任务性能。\n\n本文旨在通过融入知识库信息来提升模型在少量标注数据下的关系抽取能力。选择实体概念作为融入知识，设计泛化能力较强的知识融合模块，实现不同领域知识库的有效融入。同时，基于少样本关系抽取模型构建关系抽取系统，在领域图谱构建和新型关系抽取任务上进行实际应用。\n\n清洗后的内容如下：",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 334,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元_s0_c3",
    "source_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元",
    "text": "清洗后的内容如下：\n\nCBOW模型通过输入词矩阵V和输出词矩阵U构建上下文词与中心词的关系，最后得到矩阵U和矩阵V。ELMO模型采用双向LSTM结构，对词的表示能够抓住上下文的语义。BERT模型采用Transformer的编码器结构，采用两阶段的训练方式来获得语言模型。少样本学习是机器学习的一种类型，主要针对数据量不足的问题。元学习是少样本学习的一种训练策略，通过训练集进行元训练，通过将训练过程中的经验以先验知识的形式存储进模型，在少样本任务上能够实现快速的迁移。度量学习利用度量公式直接计算样本相似度，避免了参数优化的过程。孪生网络和原型网络是两种常见的度量学习方法。Prompt方法通过重新修改下游任务形式，使下游任务与预训练任务更为相似，希望利用预训练期间的模型结构就能完成下游任务。PET方法通过设置模版，将下游任务转化为MASK词预测预训练任务。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 380,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元_s0_c4",
    "source_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元",
    "text": "如图2-8所示，模版函数为“这是[情感]的情感”，映射函数v为“积极—正向，消极—负向”。PET方法构建多个(p,v)映射来解决模版敏感问题，然后通过多个模板的集成提升模型性能，并选取置信度高的标签作为伪标签。具体而言，PET有三个步骤：(1)对于每个模版(p,v)使用小的训练集进行预训练模型的训练；(2)对于未标注的数据集，通过集成上述多个预训练模型得到未标注数据的伪标签；(3)利用数据集的伪标签训练一般的模型。此外，集成的时候可以根据不同的模版映射，设置相应的权重，对集成的分数进行加权。模型的加权分数是采用语言模型计算得到。\n\n2021年Facebook提出EFLL(eatment few-shot learner)，将少样本文本分类任务转换为文本对关系预测的预训练任务。针对每个类别设定相应的模板，通过计算所有类别模版与句子的逻辑关系来计算句子的类别概率。输入文本X，每个类别标签/通过模版函数p构成标签模板p(X)。EFLL利用语言模型M来计算输入文本x和每个模版的逻辑关系。句子对的逻辑关系集合S为{entailment, not_entailment}。通过计算输入句子x与每个标签的p()的逻辑关系，选择逻辑关系为entailment概率最大的标签作为预测值。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 541,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元_s0_c5",
    "source_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元",
    "text": "在少样本任务的训练中，EFLL将所有的标签模版和文本进行拼接，然后根据句子的真实标签标签，得到句子对的逻辑关系标签。当句子与模版的标签一致时，两者的逻辑关系是entailment；当句子与模版的标签不一致时，两者的逻辑关系是not_entailment。通过句子对的逻辑标签和模型的预测值使用交叉熵损失来训练逻辑关系判断模型。如图2-9所示，在新闻分类任务中对每个标签构建了模版，通过计算输入文本与各个模版的关系来进行训练和预测。\n\nEFLL方法通过将文本分类任务转化为句子对的逻辑关系预测任务，由于句子对的逻辑关系集合是较为固定的，因此可以利用公开的句子关系逻辑判断数据集进行中间训练。在EFLL训练模式中，首先通过逻辑关系判断数据集对模型进行中间训练，然后利用少样本数据和标签模板构建句子对的逻辑关系判断任务，最后利用模型和标签模版对文本类别进行预测。\n\n在实际应用层面，利用实体的本体作为概念是一种高效的概念获取方式。本文也采用这种方式获取实体概念信息，其简洁高效，获取成本较低。此外，还有其他渠道可以获取概念知识，包括Microsoft Concept Graph、CN-Phrase等多个大规模实体概念知识库。实体概念知识具有以下特点：概念和实体通常是“多”的映射关系，概念的表示形式不同，概念粒度不固定，这些都是概念知识作为知识库时需要关注的问题。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 5,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 580,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元_s0_c6",
    "source_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元",
    "text": "在概念映射部分，已经有很多工作完成了该部分的数据收集，但由于实体的不断出现，维护实体和概念的映射是较为困难的工作。本文参考了相关工作，利用实体和“isa”关系构建查询语句，将查询得到的节点作为实体的伪映射概念。具体而言，本文利用公有知识图谱Wikidata和医学知识图谱UMLS作为知识库，利用Wikidata的SPARQL查询服务来完成实体和概念的映射。通过上述方式，本文能对FewRel中的绝大多数实体查询到对应的概念。\n\n在概念编码部分，本文使用的知识库Wikidata和UMLS均为开源的知识图谱。在知识融入时，本文选择两种知识表示形式，包括概念的文本标签和概念节点的图编码向量。在对概念图谱进行图嵌入编码时，由于公开知识图谱Wikidata的规模巨大，本文将其划分为概念层级和实体层级，在知识图谱Wikidata上仅用概念层级的图谱进行图嵌入编码。在UMLS知识库中，由于图谱的数量较少，本文直接使用整个图谱进行编码。本文使用到的知识图谱UMLS和Wikidata的规模如表3-2所示，该规模的图谱能够利用图嵌入算法快速得到对应的图编码结果。编码方式本文选择了DistMult技术。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 6,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 496,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元_s0_c7",
    "source_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元",
    "text": "通过概念映射和概念表示模块，能够将文本中的头实体ehead和尾实体etail映射成概念chead和ctail。提取概念节点的文本标签和图嵌入表征得到知识表示，概念知识的链接和表示为知识融入模块的提供了知识输入。\n\n在文本形式的概念融入部分，当概念知识以文本标签形式融入基础模型时，本文直接将实体概念的文本标签拼接到实体后面，通过文本序列位置上的关联让模型关注到实体和概念的联系。此外，实体与概念的映射关系通常是一对多的，当某个实体存在多个概念时，本文直接将所有概念的文本标签拼接起来，将整体的拼接结果放在文本序列中实体后方，并用BERT的特殊字来标记概念知识的开始和结束位置。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 7,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 288,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元_s0_c8",
    "source_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元",
    "text": "在图谱形式的概念融入部分，当概念以图谱编码的形式融入基模型时，由于文本与概念的表示存在较大的模态差异，概念不能直接以输入的形式输入基础模型。针对模态差异，需要设计知识融入模块来实现文本和概念知识的融合。知识融入模块需要考虑文本和知识的模态差异，以及不同来源知识库在的领域差异。模态差异是由于文本语义和概念知识的数据来源和编码方式不一致产生的，文本语义是输入的非结构化文本经过BERT编码器得到的语义向量，概念知识则是概念图谱中的节点通过图编码技术得到的编码向量，数据来源不同和编码方式不同导致两者在模态维度上差异巨大。领域差异是指训练领域上用的知识库和测试用的知识库是不一致的，这会导致图嵌入编码的向量差异巨大，因此如何保证知识融入方式在不同领域间具有一定的泛化能力也是知识融合模块的挑战。\n\n在元学习增强模块部分，本文构建了句子对级别的对比学习优化目标作为正则项，来减小过拟合的风险和训练时的过大波动问题。在领域导向性训练部分，本文构建了两阶段的元学习训练模式，首先通过训练集和验证集进行元学习训练，然后利用打分模型在第二次训练中计算每个样本的真实标签的置信度作为权重，以损失加权的方式进行领域导向性训练。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 8,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 504,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元_s0_c9",
    "source_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元",
    "text": "在实验结果分析部分，本文采用公开数据集FewRel作为实验数据集，利用训练集进行元学习训练，在验证集和测试集上，通过构建Episode来实现少样本任务的测试，通过构建大量Episode并使用其平均准确率作为评判指标。\n\n本文主要探讨了实体概念指导下的少样本关系抽取模型。首先介绍了数据集FewRel 1.0和FewRel 2.0，其中FewRel 2.0包含领域迁移和不包含类别两个任务。接着，利用知识图谱Wikidata和医学领域图谱UMLS构建了训练集和验证集的知识库。在模型方面，采用了BERT-BASE对文本进行编码，并设计了不同的知识融入模块。实验结果显示，文本形式的概念知识融入可以显著提升性能，而图谱形式的概念融入也取得了不错的效果。最后，通过对比实验验证了所设计的模型的有效性。\n\n清洗后的内容如下：\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 9,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 363,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元_s0_c10",
    "source_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元",
    "text": "清洗后的内容如下：\n\n---\n\n本文详细介绍了少样本关系抽取模型的各模块，包括概念知识增强模块和元学习增强模块。在概念知识增强模块中，针对不同形式的概念知识，设计了相应的概念表示和概念融合机制。在元学习增强模块，通过设计对比学习正则项来对模型的语义表征进行限制，针对目标领域已知的情况设计了领域导向性元训练来获取更好的领域迁移性能。在公开数据集FewRel上对上述模型进行实验，验证了实体概念知识在少样本关系抽取上的有效性，并对比了不同知识融入方法的性能，并对融合结果进行可视化等多维度的分析。此外，本章还验证了元学习增强模块的有效性，实验表明对比学习正则项和领域导向性元学习均能取得不错的结果。\n\n---\n\n以上内容已去除页眉、页脚、页码、重复信息和乱码等噪音，保留了核心学术内容。\n\n第8部分内容：\n\n4.2.2 数据库设计\n\n少样本关系抽取系统的存储内容主要包括上传的数据和抽取的图谱。存储的数据和图谱主要是以Json文件格式进行存储。上传文件和图谱的具体格式如表-2所示。\n\n表-2 系统上传数据和抽取图谱格式\n\n数据类型    Key    Value\n上传数据    annotation_data    category    数组\ncategory    [[sl^e^UsI^e^], ...]\ntestdata",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 10,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 565,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元_s0_c11",
    "source_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元",
    "text": "图谱    kg    [[el, e2, rl, score], ...]\n\n在文件描述中，上传文件包括少样本标注数据annotation_data和推理数据testdata，每条数据包括句子si和句子中对应的头实体4和尾实体es；在图谱设置的格式中，el、e2表示头实体和尾实体，rl表示关系，score表示实体关系的模型置信度。\n\n系统利用MySQL来存储用户的相关信息，角色信息，图谱数据的路径等信息。本系统共设定了5张表，具体包括：用户基本信息表(user_info表)，角色权限映射表(role_permission表)，用户权限表(permission表)，数据管理表(upload_data表)，图谱管理表(kg_data表)，具体的功能如表4-3所示。\n\n表4-3 数据表描述信息\n\n数据表    功能\n用户基本信息表    存储用户的基本信息和注册信息\n角色权限映射表    角色对应的权限等级\n用户权限表    用户和权限的映射\n数据管理表    数据的描述信息和存储地址\n图谱管理表    图谱的描述信息和存储地址\n\n通过上述表，能够实现系统功能的实现，并对信息进行管理，具体的数据表字段如图4-6所示。\n\n4.3 功能模块设计\n\n4.3.1 前端展示模块",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 11,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 540,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元_s0_c12",
    "source_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元",
    "text": "通过上述表，能够实现系统功能的实现，并对信息进行管理，具体的数据表字段如图4-6所示。\n\n4.3 功能模块设计\n\n4.3.1 前端展示模块\n\n前端展示模块是用户与系统的交互模块，设计系统时，需要考虑到用户操作的便利性。前端展示模块包括两个功能栏，包括菜单栏和功能栏，菜单栏和功能栏一一对应，功能栏会随着菜单栏的点击而改变。菜单栏主要包括三部分：用户管理、数据列表和图谱列表。如上述描述，具体的前端模式页面布局如图-7所示。\n\n4.3.2 后端模块\n\n后端模块是前端展示模块通过界面和系统的数据进行交互，前端模块通过http请求完成对应的数据传输。在数据层面，通过将http请求将对应的功能转换成SQL语句，将查询结果转换成Json文件格式，然后展示给用户。后端模块的部分接口如表4-4和表4-5所示。\n\n4.3.3 数据上传模块\n\n数据上传模块是指用户通过该模块将本地文件上传至服务器，用户可以对服务器中上传的文件进行操作，包括删除，使用关系抽取模块构建抽取模型等功能。数据上传模块的部分接口如下表所示。\n\n4.3.4 关系抽取模型构建模块",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 12,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 470,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元_s0_c13",
    "source_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元",
    "text": "4.3.4 关系抽取模型构建模块\n\n关系抽取模块会利用上文少样本关系抽取模型实现少样本关系抽取分类，为了避免模型在多个少样本任务生成过多模型，导致系统的存储风险。该系统要求用户在上传训练数据的同时上传测试数据，方便测试后即可删除训练后的模型。在上传数据模块需要上传满足如上述表4-4的数据，然后选择对应的数据建模并对图谱进行管理。该模块的接口如下表4-8所示。\n\n4.3.5 用户管理模块\n\n用户管理模块可以对系统用户的信息进行管理，通过对用户的权限进行判断，来限制用户的部分操作。\n\n4.4 系统测试\n\n4.4.1 测试环境\n\n系统开发结束后，会对系统进行功能性测试和非功能性测试，功能性测试是指对系统的各个模块是否能够正常运行，非功能性测试是指对系统的响应时间，运行流畅度进行测试。在测试过程中，系统的运行环境如表4-10所示，并记录测试结果。\n\n4.4.2 功能性测试\n\n本系统的目的是构建少样本关系抽取模型的落地应用。用户通过上传某类型关系的少量标注数据和测试数据，系统利用少样本关系抽取系统和少量标注数据完成模型的快速训练。在测试数据上进行快速推理，得到对应的三元组知识图谱。此外，需要满足系统的登陆功能、用户管理功能等。如图4-8和图4-9所示是对应的用户登陆界面和用户管理界面。用户管理界面可以对用户的权限进行限制、密码进行重置、以及用户删除等操作。\n\n4.5 系统测试",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 13,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 592,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元_s0_c14",
    "source_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元",
    "text": "4.5 系统测试\n\n4.5.1 测试环境\n\n系统开发结束后，会对系统进行功能性测试和非功能性测试，功能性测试是指对系统的各个模块是否能够正常运行，非功能性测试是指对系统的响应时间，运行流畅度进行测试。在测试过程中，系统的运行环境如表4-10所示，并记录测试结果。\n\n4.5.2 功能性测试\n\n本系统的目的是构建少样本关系抽取模型的落地应用。用户通过上传某类型关系的少量标注数据和测试数据，系统利用少样本关系抽取系统和少量标注数据完成模型的快速训练。在测试数据上进行快速推理，得到对应的三元组知识图谱。此外，需要满足系统的登陆功能、用户管理功能等。如图4-8和图4-9所示是对应的用户登陆界面和用户管理界面。用户管理界面可以对用户的权限进行限制、密码进行重置、以及用户删除等操作。\n\n第9部分内容：\n\n4.4.3 非功能性测试\n\n本节主要对系统的兼容性和流畅性进行测试，以及少样本学习算法的时间消耗进行测试。在流畅性测试中，本文进行了登录界面速度、页面切换速度、接口响应速度的测试。在兼容性中，本文进行了浏览器兼容性的测试，以验证该系统的运行情况。具体测试结果如表4-11所示，可以看到测试结果满足系统需求分析中的要求。\n\n表4-11 系统非功能性测试结果",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 14,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 527,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元_s0_c15",
    "source_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元",
    "text": "表4-11 系统非功能性测试结果\n\n| 测试用例 | 测试结果 |\n| --- | --- |\n| 登录页面速度 | 通过登录界面提交登录信息到系统后台，并在2秒以内返回结果 |\n| 页面切换速度 | 点击不同的菜单栏切换右侧的不同功能页面，加载时间在2秒以内 |\n| 接口响应速度 | 页面提交请求到后端响应，服务器响应时间在5秒之间 |\n| 浏览器兼容性 | 在Chrome和Safari浏览器登录系统界面，页面展示正常，系统运行正常 |\n\n系统在保证稳定流畅运行的基础上，需要利用少样本数据进行关系抽取任务。由于少样本关系抽取系统是基于文本对的形式，即将测试文本与标注数据进行拼接，所以标注数据量的增多，会增加模型推理时间的损耗。在不同少样本设置下，单条样本推理时耗情况如表4-12所示，可见该少样本关系抽取系统在大多数情况下的推理时间在接受范围内。\n\n表4-12 模型推理时耗情况\n\n| 少样本设置 | 平均时耗 | 时耗方差 |\n| --- | --- | --- |\n| 5-way 1-shot | 40ms | 2ms |\n| 5-way 5-shot | 182ms | 31ms |\n| 10-way 1-shot | 174ms | 42ms |\n| 10-way 5-shot | 398ms | 134ms |\n\n4.5 本章小结",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 15,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 578,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元_s0_c16",
    "source_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元",
    "text": "4.5 本章小结\n\n本章介绍了少样本关系抽取系统的设计与实现的具体细节，包括需求分析、系统概要设计、系统的各个功能模块的详细实现方案。整个系统的功能模块包括数据管理模块，用户管理模块，关系抽取模块三个部分，通过这三部分的有效组合，能够完成少样本关系抽取系统的基本功能。在测试阶段对系统的各个功能模块的功能性和非功能性进行了测试，以验证系统的运行流畅度和兼容性，并且对少样本关系抽取模型的性能时耗进行了测试。通过该系统，少样本关系抽取模型能够应对新关系或是特殊领域上的关系建模问题，提升了关系抽取任务的效率。\n\nConnecting language and knowledge bases with embedding models for relation extraction",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 16,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 340,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元_s0_c17",
    "source_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元",
    "text": "Extracting relations between entities in text is a crucial step for many natural language processing tasks. Recent years have seen a growing interest in using embedding models to connect language and knowledge bases for relation extraction. These models can capture semantic information and improve the accuracy of relation extraction.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 17,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 335,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元_s0_c18",
    "source_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元",
    "text": "Several studies have explored the use of feature-rich compositional embedding models for relation extraction. These models can represent the meaning of phrases and sentences by combining the embeddings of their constituent words. For example, Gormley et al. (2015) proposed a model that uses a feature-rich compositional embedding to improve relation extraction.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 18,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 362,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元_s0_c19",
    "source_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元",
    "text": "Other studies have used neural networks for relation extraction. For instance, Liu et al. (2013) proposed a convolutional neural network model for relation extraction. Similarly, Zhang et al. (2015) used a bidirectional long short-term memory network for relation classification.\n\nMore recent approaches have explored the use of pre-trained language models like BERT for relation extraction. For example, Devlin et al. (2018) used BERT for relation extraction by fine-tuning the model on a relation extraction task.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 19,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 515,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元_s0_c20",
    "source_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元",
    "text": "In summary, embedding models and neural networks have shown promise for improving relation extraction by connecting language and knowledge bases. These approaches can capture semantic information and improve the accuracy of relation extraction.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 20,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 21,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 244,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c0",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "随着移动互联网的飞速发展与普及，问答系统已在多个工业领域成功落地并取得了良好的经济收益与社会价值。常识知识作为海量认知信息中的研究重点，在问答系统中的作用也在不断凸显，具有极大的研究前景。常识问答任务（Commonsense Question Answering，CQA）正是研究如何获取相关常识知识，并通过问题解析与知识推理，进而获取精准答案。本文重点聚焦于CQA任务在有监督和无监督场景下的研究。\n\n目前CQA任务存在以下问题亟待解决：在有监督场景下，当前的研究工作集中于优化和改进模型的知识推理策略，而忽视了知识覆盖面不足及知识噪声等问题，这会严重折损模型在知识推理阶段的性能而致使预测偏差。在无监督场景下，当前的多数研究方法着重于设计特定任务的手工规则以提升知识的生成质量，因而导致知识类型受限并且模型框架的自适应迁移能力较弱。\n\n故本文针对以上挑战进行了探索与研究，并将具体工作总结如下：",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 51,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 399,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c1",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "故本文针对以上挑战进行了探索与研究，并将具体工作总结如下：\n\n1. 针对有监督CQA任务中的知识覆盖不足以及噪声问题，本文提出了一种基于知识增强的图对比学习模型（Knowledge Enhanced Graph Contrastive Learning，KE-GCL）。首先，该模型将问答对的实体上下文描述集成到当前知识子图以实现多源知识融合；随后，模型提出一个自适应的带权采样策略以生成当前子图的增强视图，并同时完成正负图例的构建；最后，该模型通过关系边的散射与实体节点的信息聚集以完成知识图谱的更新与推理。\n\n2. 针对无监督CQA任务中的迁移及自适应能力弱的问题，本文提出了一种基于通用提示模版的知识生成模型（Prompt-based Knowledge Generation Network，PKGN）。首先，该模型通过Dropout增强策略进行无监督的对比学习，以捕获问题之间的细微差异并学到更好的问题表征；接着，PKGN模型通过带指令的模版提示以生成问题相关的知识描述；最后，该模型利用文本匹配模型完成知识推理与答案预测。",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 467,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c2",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "本文在CommonsenseQA、OpenbookQA、SociallQA等三个常识问答数据集上开展了大量的实验。通过定量分析和定性对比，验证了KE-GCL和PKGN模型在有监督和无监督方向的可行性与有效性。实验结果表明，本文所提出的常识问答模型在有监督和无监督两个方向均优于当前的基线方法，且具备较好的鲁棒性与泛化能力。\n\n基于知识的自动问答（KBQA）旨在回答用户提出的各类自然语言问题，在具备海量数据的开源互联系统中，知识作为行为与认知的重点，在问答理解与推理上作用不断凸显。谷歌于2012年首次提出了知识图谱的概念，知识图谱通过模拟人类理解客观世界的方式来动态构建知识，同时赋予了机器建模知识与理解语义的可能，高质量的数据由此开始以大规模的开源知识库方式出现，其中蕴涵了大量实体属性关系与拓扑结构信息。由此，基于知识的自动问答（KBQA）应运而生，并迅速成为了问答任务的一个重要分支。图1-1展示了KBQA任务的具体定义，即给定一个自然语言问题，机器需要对问题进行语法解析和语义匹配，并结合相关的外部知识库进行查询和推理，进而获得精确的预测答案。根据知识来源的不同，KBQA任务可主要划分为开放领域（OpenDomain）知识问答，如百科知识、常识的问答；以及特定领域（CertainDomain）知识问答，如金融、医学领域的问题问答。",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 572,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c3",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "从工业落地的角度来看，基于知识的问答系统在现阶段也得到了广泛的应用，典型的落地服务场景包括：智能语音助手、智能客服、搜索引擎、情感类聊天等。在语音助手领域，广为人知的微软小冰、苹果Siri、小米小爱等智能助手产品正是依托了高性能KBQA系统的支撑，才能为用户提供日常化的精准化定制服务。在搜索引擎场景下，百度推出的百度知道，搜狗推出的搜狗问问等引擎服务，正是将用户的自然语言问题作为搜索查询词进行知识查询与检索排序，从而直接为用户提供准确简洁的答案，而不是返回一个和问题最相关的网页供用户自行查找，这大大降低了用户获取信息的成本。在智能客服领域，阿里小蜜通过知识图谱技术，自动解答用户关于支付宝使用时遇到的问题，这不仅降低了使用人工客服带来的人力成本，同时也降低了用户获得服务的响应时间，并提升用户体验。除此之外，基于知识库的自动问答在电信运营商、金融保险、税务管理等行业领域也有着广泛的应用。",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 397,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c4",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "在KBQA任务中，常识作为人类社会对同一事物普遍存在的日常共识，是一种重要表现形式。常识问答任务（Commonsense Question Answering，CQA）考验的正是模型的常识推理能力，这类任务一般聚焦于多项选择式问答形式，需要模型针对给定的问题进行推理，从而选择最为贴切的答案，目前常见的相关数据集有CommonsenseQA、OpenbookQA、SocialIQA、CosmosQA以及NinersenseW等。区别于机器阅读理解和事实性抽取等传统问答任务，常识问答任务通常只给出题干而没有给出相关的背景知识。在CQA任务中，模型不仅要理解问题和选项的语义，还需要具备相关的常识背景知识才能进行合理的推断及预测。在图1-2所示的CQA样例中，模型只有在知道“老板需要员工向他”的知识前提下，才能推断出正确选项B（进行信息传递）。",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 373,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c5",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "如何有效获取高质量的常识知识，并执行有效地知识推理，这也正是常识问答任务的难点所在。目前常见的大型知识库，例如YAGO、OpenIE、NELLMDBpedia、Freebase、WebChild以及ConceptNet，这些结构型数据库以机器可读的方式存储“实体-关系-实体”的三元组知识，并可根据资源描述框架规范对知识进行描述。这些知识库组成了常识信息的大型语义网络，具有极好的知识表达能力和灵活的建模空间，因而能够辅助模型进行结构化的常识知识推理，并输出具备高置信度的答案决策。\n\n本文的工作聚焦于基于知识的自动问答领域中的常识问答任务，并在有监督和无监督的两个方向展开详细研究。通过建立端到端的深度学习网络，并运用知识建模及推理的相关技术解决CQA任务中现存的挑战和困难。",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 5,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 339,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c6",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "本文针对有监督和无监督常识问答任务，结合当前研究的问题要点，阐述了主要研究内容与工作贡献。针对有监督常识问答模型存在的知识覆盖不完备及知识噪声问题，本文设计了基于知识增强型图对比学习的常识问答算法，通过自适应采样原始图的边和节点特征得到增强视图，并采用多源的知识增强，将问答对的实体描述性文本作为补充节点信息插入到当前子图中，以充分完成知识的融合与对齐。针对无监督常识问答模型缺乏易迁移的自适应方法问题，本文设计了基于提示型知识生成的常识问答算法，通过无监督对比学习策略对问题语义进行继续预训练，并采用具备指令及示例的提示模版产生一系列知识描述的集合。在常识问答的有监督和无监督两个方向分别对比了目前主要的基线方法，并通过大量的定性与定量实验充分证明了本文所提出模型的有效性。\n\n在向量空间中，词向量表示方法Word2Vec通过浅层神经网络学习词向量，包括CBOW和Skip-Gram两种模型。CBOW利用上下文预测目标词，而Skip-Gram则相反，利用目标词预测上下文。知识表示方法包括神经网络模型和张量模型，其中TransE模型基于平移不变性假设，将关系视为头尾实体向量之间的平移。深度学习技术中，注意力机制通过为输入向量分配权重，帮助模型关注关键信息。\n\n其中，＜7表示非线性层的激活函数，％ ，撕2为权重矩阵，为偏置项。",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 6,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 565,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c7",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "其中，＜7表示非线性层的激活函数，％ ，撕2为权重矩阵，为偏置项。\n\n（2）权重归一化：此阶段负责将每个输入向量％ eh对应的相似度得分归一化的权重af，一般采用Softmax函数实现，具体公式如下：\n\ngSimi = Softmaxi(simi) = e^simi / Σe^simi (2-12)\n\n（3）带权向量计算：利用权重对所有输入向量进行加权求和，即将输入序列中的每个向量与它对应的权重项相乘，并将结果进行相加以得到最终向量＾；，其公式表示如下：\n\n＾ = Σaifiei (2-13)\n\n2.3.2预训练语言模型",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 7,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 263,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c8",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "＾ = Σaifiei (2-13)\n\n2.3.2预训练语言模型\n\n预训练语言模型通过在大规模文本语料上进行无监督的先行训练，使得模型能掌握丰富的自然语言的语义和语法规律，并可应用到各类下游的文本任务中以提升性能效果。其中，最具代表性预训练语言模型便是由Open AI于2018年发布的大规模生成式的预训练语言模型GPT[73]（Generative Pretrained Transformer）和Google团队于同年发布的双向文本编码模型BERT[74]（Bidirectional Encoder Representations from Transformers），它们都是基于Transformer网络架构，分别在自回归和自编码方向上对模型的训练范式进行了突破性创新，在机器翻译、文本分类、序列标注、问答系统等多个领域上均表现优异并得到了广泛的应用。下面将针对上述模型展开详细介绍。\n\n（1）Transformer架构",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 8,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 415,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c9",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "（1）Transformer架构\n\nTransformer的模型结构如图2-8所示，主要由两个部分组成，即编码器（Encoder）和解码器（Decoder），其中编码器负责将输入序列编码为隐藏状态序列，解码器则负责从隐藏状态序列中生成输出序列，而它们均是由多层组块（Block）堆叠而成，每个组块内部也包含了不同的层次结构[75]。以编码器为例，每个组块内部由三种类型的结构组成，即多头自注意力层、残差及规范化层、全连接前馈层。\n\nOutput\n\nProbabilities\n\n1\n\nSoftmax 1\n\n1\n\nLinear 1\n\n--I\n\nAdd & Norm\n\nF^d\n\nForward\n\n>—^\n\nAdd & Norm\n\nAttention\n\nForward\n\nI\n\n*Nx\n\n>\n\n.\n\nj\n\nAdd & Norm\n\nj\n\nj\n\n?\n\n.\n\n.\n\n“.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 9,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 596,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c10",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": ".\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 10,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c11",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": ".\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 11,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c12",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": ".\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n第6部分内容：\n\n1. GCN模型通过Laplace矩阵进行谱分解，采用傅里叶变换进行特征分解，并定义了在谱域上的卷积核运算。GCN模型利用每个节点及其邻居节点的特征来更新当前节点的信息表示，并在不同层之间共享权重。\n\n2. GAT模型在GCN的基础上引入了注意力机制，能够自适应地学习不同节点之间的关系权重，从而更精细地捕获节点之间的相互作用。",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 12,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 527,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c13",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "2. GAT模型在GCN的基础上引入了注意力机制，能够自适应地学习不同节点之间的关系权重，从而更精细地捕获节点之间的相互作用。\n\n3. GraphSAGE模型通过随机采样邻居节点并聚合邻居节点的特征向量来完成信息传递和特征更新。相比GCN和GAT，GraphSAGE考虑了所有邻居节点信息，并通过随机采样方式极大减少了模型的计算量。\n\n4. 提示学习通过引入具备指向性的提示模版信息，控制和影响模型生成的文本，并使得输出的可解释性和可控性更强。提示学习可以结合大规模的预训练语言模型，在模版中构建领域相关的先验知识来弥补数据稀缺问题，从而在一定程度上提高模型的性能表现和泛化能力。\n\n从ConceptNet结构化知识库中可以检索得到针对当前问答对(g, q)的知识子图仏，这是与g和q中的主题实体相关的两跳范围内节点与边所构成的子图。对于节点嵌入，使用Feng等人提供的实体嵌入参数进行初始化，它将预训练语言模型应用于ConceptNet中的所有三元组，同时为每个实体获得一个池化表示。经过初始化后的节点嵌入表示如下：＝Ki * vi，2，－＞vin]。",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 13,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 477,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c14",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "对于边的嵌入表示，首先将当前边的类型rst以及边所连接的两个节点的类型us，构成的三元组编码为一个独热向量[us？rst十叫]；然后采用两层MLP结构将独热向量映射为了当前边的初始化嵌入表示。经过初始化后的m条有向边的嵌入表示如下：i = {eu，ei，2，…，e_m}。\n\n因此，当前知识子图对应的图谱嵌入可表示为& = {1，财]，具备n个节点和m条有向边。\n\n知识融合部分通过节点插入和注意力机制进行多源的知识融合。具体来说，上下文的语义特征向量被视为一个新节点插入到当前的知识子图中，插入后的节点嵌入更新为K = {1，新插入的上下文节点定义如下，其中M表示个两层的MLP。i = {1}。\n\n对于相关边的构建，这里将与当前问答对(q, q)中的实体有直接拓扑关系的实体与新插入的上下文节点建立连接，由此图谱边数增加到话。边的嵌入表示更新为{eu，ei，2，…，e_m}。\n\n模块采用注意力机制来进一步融合文本知识，上下文表示4被视作查询(Query)作用于所有图谱节点。对于每个节点融合后的表示定义如下，其中映射Q表示MLP模块，%为节点的嵌入维度。i，q = softmax？vUq。\n\n因此，多源知识的融合图可表示为￥ = {1，M]，具备n+1个节点和衍+1条边。",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 14,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 540,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c15",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "因此，多源知识的融合图可表示为￥ = {1，M]，具备n+1个节点和衍+1条边。\n\n自适应图增强模块针对多源知识的融合图￥，通过自适应地进行节点特征掩码和边丢弃以构建对应的增强视图。首先定义子图中的每个节点eR的影响为如下公式所示：Pv_i，q = /r(＾i，a)]。\n\n这里r(＾)和丸(Y)分别代表拓扑连通性和上下文相关性。拓扑连通性是通过PageRank算法进行计算，该算法会从拓扑结构的角度对那些具有更多入度的节点进行加权；而上下文相关性则通过当前节点与上下文语义表示z_f之间的语义相似性衡量，公式定义如下，其中0(Y)表示两个向量之间的佘弦相似度。\n\n一个直观性假设，即那些经常在有影响力的节点中有高频表达的维度应该是更重要的。因此，对于芡中的任何节点，维度d的重要性权重计算公式定义如下，并将权重数值归一化后作为是否对维度进行掩码的概率。\n\n对于S；中的每条边e，其重要性取决于当前边所指向的尾节点珥的重要性权重，如下公式所示。类似节点的操作，权重I在归一化后便作为边e的丢弃概率。\n\n因此，该模块基于归一化的概率进行伯努利采样，并获得￥对应的增强视图￥ = {1}。",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 15,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 492,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c16",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "因此，该模块基于归一化的概率进行伯努利采样，并获得￥对应的增强视图￥ = {1}。\n\n知识图推理模块中，融合知识子图S；及其增强视图￥会并行执行相同的前向操作，故此处以前者S；为例，通过关联边的散射和基于注意力的节点聚合进行知识推理。具体来说，为了利用图谱中带有关系类型的边信息，对于每个节点圬eK，通过对指向节点珥的那些边进行散射操作可获得对应节点的初始隐藏表示。\n\n之后，当前模块使用GAT网络用以传播和聚合节点的知识信息。对于每层图神经网络，节点的隐藏表示的更新公式如下，其中t/是注意力头的数量，M/u是对应的线性投影矩阵，| |是多个注意力头的拼接操作。\n\n在经过L层的图推理之后，模块选择将上下文节点的隐藏状态作为整个知识子图的池化表示，即：zf = Pool(hL)。\n\n答案预测模块对于答案选项q，使用相应的上下文语义特征表示z_c和知识子图表示z_f来计算其成为正确答案的概率，计算公式如下：P_ic_i|q) = gQ[z_c，z_f]。\n\n其中，门控仍用以控制上下文文本特征和图谱特征的重要性权重。对于答案预测，在得到所有候选项的预测概率后，模型将具有最高分数的选项视为最合理的答案。",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 16,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 503,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c17",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "其中，门控仍用以控制上下文文本特征和图谱特征的重要性权重。对于答案预测，在得到所有候选项的预测概率后，模型将具有最高分数的选项视为最合理的答案。\n\n正负图例构造与目标函数中，损失目标函数KE-GCL采用端到端的有监督训练，其总目标损失Xi定义如下，其中和分别表示答案预测分类损失和图对比学习损失，而入则是用于调节图对比学习影响的超参数，同时也反映了对预测分类目标的惩罚程度。\n\n答案预测分类损失使用标准的交叉熵以优化正确答案q的预测概率，其公式定义如下，而图对比损失则将结合正负图例的构造在下文中详细展开。\n\n正负图例的构建及图对比学习损失中，图对比学习损失定义如下公式所示，该损失是基于InfoNCE函数构建，并在此基础上增设了难负例项以增强对比学习的效果。直观来说，对于一个给定的问题，候选选项相关的知识子图及其增强视图通常会具备较多相同的节点和边，这种相似性会导致模型难以分辨。因此，当前子图的难负例存在两个来源，一个是那些带有错误选项的问答对相关的知识子图；另一个则是它们相应的增强视图。\n\n由此，正例、难负例、普通难负例对应的贡献项可表示如下：TP = exp？z_f/r，TN = i exp？z_f/r，T_C = z exp？M)/r。",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 17,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 526,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c18",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "由此，正例、难负例、普通难负例对应的贡献项可表示如下：TP = exp？z_f/r，TN = i exp？z_f/r，T_C = z exp？M)/r。\n\n在OpenBookQA数据集上，基于Huggingface框架部署实现了AristoBERTa。检索知识子图的限制跳数设置为2。文本编码器的上下文特征维度设置为1024，最大序列长度为128；图推理模块中节点与边的嵌入维度设置为200，GAT的推理层数设置为3。为了控制图对比学习中的变量约束影响，将超参数I、P和t分别设置为0.1、2和0.2。文本编码器的学习率设置为1e-5，其他模型组件的学习率为1e-3，dropout数值为0.2。模型在训练过程中采用Adam作为优化器，将Epoch设置为30并采用早停策略进行端到端的有监督训练。一个完整的训练周期大约需要13小时。此外，采用Mini-batchSize=2的梯度累积策略以实现BatchSize=128的等效训练效果。最终的主实验结果是使用不同的随机种子进行五次运行后的平均指标。",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 18,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 449,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c19",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "本章在有监督常识问答方向开展了基于知识增强型的图对比学习模型（KE-GCL）的研究工作。首先，本章从缓解知识噪声问题和增加知识覆盖面的角度给出了KE-GCL模型的设计动机与解决思路，并概述了模型端到端的处理流程。接着，本章详细介绍了KE-GCL的各个组件模块，该框架将上下文描述集成到当前的知识子图中，形成知识增强图；接着，本章提出了一种自适应采样策略来生成当前知识子图所对应的增强视图；随后，本章通过边散射和节点聚合的方式对图谱的信息进行更新与推理；此外，为了进一步增强图对比学习的训练效果，模型将当前样例中错误问答对相关的知识子图及其增强视图视作为难负例。最后，本章在CommonsenseQA和OpenbookQA两个基准数据集上进行了大量的定量与定性实验分析，从模型性能对比、消融实验、案例分析、可视化分析、难负例影响等方面充分验证了所提出的KE-GCL方法的可行性与有效性。\n\n知识生成的提示模版中包含多个常识问题及其相关背景知识作为输入，构成不同的常识模版提示。然后根据问题和提示的对应关系，生成相应的知识描述作为输出。这种基于提示学习的方式可以有效地利用预训练语言模型中的先验知识，并能以生成的方式提高知识的质量和多样性。",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 19,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 518,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c20",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "在每个问题得到其对应的一系列相关知识描述后，知识推理与答案预测模块负责将当前的知识进行有效整合，推理模型将其结合当前的所有候选答案进行置信度相关的可靠预测。具体而言，对于每个问题g生成的S个知识陈述知识推理模型会将知识陈述单独添加到问题前，形成S+1个带有知识描述的完整问题表达。推理模型会对每个候选答案进行匹配与评分操作，即采用最支持当前候选项的知识描述来确定一个置信度分数。因此，推理模型会从知识描述中选择一个给出最高置信度预测的候选答案作为最终输出。\n\n无监督常识问答任务的实验在CommonsenseQA和OpenbookQA数据集的基础上，增设了SociallQA基准数据集。数据集总共包含37,588个问答对，训练集、测试集、验证集的数量分布如表4-4所示。无监督常识问答任务仍然采用答案预测准确率作为模型性能的评估指标。\n\n本节选取了近年在无监督常识问答任务上的多种研究方法作为对比基线，并对各自的研究工作进行简要地概括说明。为了进行合理地对比分析，后续所有的在三个基准数据集上的实验报道均是基于相关验证集开展。值得注意的是，模型在训练过程中严格控制标签不可见，标签仅用于最终的性能评估。",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 20,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 501,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c21",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "本章提出的PKGN模型采用开源深度学习框架Pytorch进行网络搭建与模型训练，并在3个常识问答基准数据集上进行了定性和定量实验。实验环境为搭载了2张48GB NVIDIA RTX A6000显卡的Ubuntu 20.04服务器。基于问题语义的对比学习模块和提示型知识生成模块以生成式预训练语言模型GPT-2作为基础骨架。在知识推理与答案预测模块中，与SEQA方法保持相同的结构设置，即用SROBERTa-Large作为基础架构。\n\n表4-5展示了PKGN模型的无监督实验结果。在这三个基准常识问答数据集上，所提出的PKGN模型的性能均优于当前所有的对比基线。具体来说，以GPT-2-Medium的配置为例，与之前的最优模型SEQA相比，PKGN模型在CommonsenseQA和OpenbookQA数据集上的表现取得了显著的性能提升，分别高出了2.6%和9.2%。而在SociallQA数据集上，PKGN模型相比SEQA的方法则高出1.5%。随着GPT-2架构的参数规模逐渐上升的同时，PKGN模型在这三个基准数据集上的实验性能也在稳步提升。",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 21,
    "chunk_index_in_section": 21,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 472,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c22",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "为了研究PKGN模型中每个模块的有效性，本小节基于CommonsenseQA和OpenbookQA两个基准数据集，在GPT-2-Medium的配置下，对PKGN模型中的各个组件进行了消融实验分析。实验结果如表4-6所示。可以看到，当移除基于问题语义的对比学习模块后，在不进行任何针对问题本身的学习训练后，模型性能在两个数据集上均显著下降了2.6%和3.7%。这也证实了继续预训练对于后续知识生成的重要性。在移除提示型知识生成模块后，PKGN模型退化为了基于SROBERTa架构的自编码器，并直接以问题与候选答案的组合拼接为输入进行打分预测。因此，模型性能在两个数据集上迎来了最大幅度的下降，分别为4.1%和9.5%。这也反映了常识知识的关键性。在移除知识推理与答案预测模块的设置中，PKGN模型直接拼接问题和首条生成知识，并连同候选答案输入到GPT-2中进行预测打分，模型性能在两个数据集上的下降幅度分别为0.9%和1.7%。这证明了该模块采用的问题知识拼接策略和文本匹配模型能较好地理解并筛选对答案预测有助益的有效知识。",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 22,
    "chunk_index_in_section": 22,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 461,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c23",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "本小节对PKGN模型中的预定义模版设置多种扰动策略，从而测试模型在不同攻击力度下的鲁棒性与泛化能力。扰动方式可分为四种类型，分别是：改述模版指令、调换模版指令与示例顺序、同域随机替换模版示例、跨域随机替换模版示例。为避免偶然性与数据偏差，在每个扰动设置下，PKGN基于GPT-2-Medium的配置在CommonsenseQA数据集上进行3次重复实验并取得平均值进行报道，实验结果如表4-7所示。\n\n在本研究中，我们提出了一种基于提示型知识生成的无监督常识问答算法框架（PKGN）。该模型首先利用基于Dropout增强的对比学习策略对常识问题进行继续预训练，以捕捉更精细的问题语义表示。随后，模型利用设计的提示模版生成一系列问题相关的知识描述。最终，通过文本匹配模型对带有知识的问题和候选项之间进行置信度评分与答案预测。实验结果显示，PKGN模型在三个基准常识数据集上的性能指标均优于当前基线方法，并通过组件消融、模版敏感度分析以及案例分析等角度验证了模型的有效性与泛化能力。\n\n清洗后的内容如下：",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 23,
    "chunk_index_in_section": 23,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 449,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c24",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "清洗后的内容如下：\n\n12. Tandon N, De Melo G, Suchanek F, et al. WebChild: Harvesting and organizing commonsense knowledge from the web. In Proceedings of the 7th ACM international conference on Web search and data mining. 2014: 523-532.\n\n13. Liu H, Singh P. ConceptNet - A practical commonsense reasoning toolkit. In BT Technology Journal, 2004, 22(4): 211-226.\n\n14. Lan Y, He G, Jiang J, et al. A survey on complex knowledge base question answering: Methods, challenges and solutions. arXiv preprint arXiv:2105.11644, 2021.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 24,
    "chunk_index_in_section": 24,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 515,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c25",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "15. Ducharme B. Learning SPARQL: Querying and updating with SPARQL 1.1. Really Media, Inc., 2013.\n\n16. Reddy S, Lapa M, Steedman M. Large-scale semantic parsing without question-answer pairs. Transactions of the Association for Computational Linguistics, 2014, 2: 377-392.\n\n17. Santoro A, Raposo D, Barrett D, et al. A simple neural network module for relational reasoning. In Advances in neural information processing systems, 2017, 30.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 25,
    "chunk_index_in_section": 25,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 437,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c26",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "18. Schlichtkrull M, Kipf TN, Bloem P, et al. Modeling relational data with graph convolutional networks. In 15th International Conference on Extended Semantic Web Conference, ESWC 2018. SpringerA/erlag, 2018: 593-607.\n\n19. Lin B, Chen X, Chen J, et al. KagNet: Knowledge-aware Graph Networks for Commonsense Reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019: 2829-2839.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 26,
    "chunk_index_in_section": 26,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 516,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c27",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "20. Wang X, Kapanipathi P, Musa R, et al. Improving natural language inference using external knowledge in science questions domain. In Proceedings of the AAAI Conference on Artificial Intelligence. 2019, 33(01): 7208-7215.\n\n21. Feng Y, Chen X, Lin B, et al. Scalable Multi-Hop Relational Reasoning for Knowledge-aware Question Answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020: 1295-1309.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 27,
    "chunk_index_in_section": 27,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 454,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c28",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "22. Yasunaga M, Ren H, Bosselut A, et al. QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021: 535-546.\n\n23. Gao T, Yao X, Chen D. SimCSE: Simple Contrastive Learning of Sentence Embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021: 6894-6910.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 28,
    "chunk_index_in_section": 28,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 482,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c29",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "24. Unger C, Biemann L, Lehmann J, et al. Template-based question answering over RDF data. In Proceedings of the 21st international conference on World Wide Web. 2012: 639-648.\n\n25. S W, Chang M W, He X, et al. Semantic parsing via staged query graph generation: Question answering with knowledge base. In Proceedings of the Joint Conference of the 53rd Annual Meeting of the ACL and the 7th International Joint Conference on Natural Language Processing of the AFNLP. 2015.\n\n26. Burgers C. From RankNet to LambdaRank to LambdaMART: An overview. In Journal of Machine Learning, 2010.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 29,
    "chunk_index_in_section": 29,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 582,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c30",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "27. Hu S, Zou L, Zhang X. A state-transition framework to answer complex questions over knowledge base. In Proceedings of the 2018 conference on empirical methods in natural language processing. 2018: 2098-2108.\n\n28. Dong L, Lapata M. Language to logical form with neural attention. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2016: 33-43.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 30,
    "chunk_index_in_section": 30,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 411,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c31",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "29. Xu K, Wu L, Wang Z, et al. Exploiting rich syntactic information for semantic parsing with graph-to-sequence model. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 2018: 918-924.\n\n30. Cui W, Xiao Y, Wang H, et al. KBQA: Learning question answering over knowledge base with multi-hop attention. In Proceedings of the VLDB Endowment, 2017, 10(5): 565-576.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 31,
    "chunk_index_in_section": 31,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 400,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c32",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "31. Liao C, Berant J, Le Q, et al. Neural symbolic machines: Learning semantic parsers on freebase with weak supervision. In 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017. Association for Computational Linguistics (ACL), 2017: 23-33.\n\n32. McCarty J. History of Lisp. In History of programming languages, 1978: 173-185.\n\n33. 陈子睿，王鑫，王林等. 开放领域知识图谱问答研究综述. 计算机科学与探索，2021，15(10): 1843-1869.\n\n34. 郑泳智，朱定局，吴惠粦，彭小荣. 知识图谱问答领域综述. 计算机系统应用，2022，31(04): 1-13.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 32,
    "chunk_index_in_section": 32,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 478,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c33",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "34. 郑泳智，朱定局，吴惠粦，彭小荣. 知识图谱问答领域综述. 计算机系统应用，2022，31(04): 1-13.\n\n35. Yao X, Van Durme B. Information extraction over structured data: Question answering with jBases. In Proceedings of the 52nd annual meeting of the association for computational linguistics (volume 1: Long papers). 2014: 956-966.\n\n36. Bordes A, Chopra S, Weston J. Question answering with subgraph embeddings. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2014: 615-620.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 33,
    "chunk_index_in_section": 33,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 486,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c34",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "37. Dong L, Wei F, Zhou M, et al. Question answering over freebase with multi-column convolutional neural networks. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2015: 260-269.\n\n38. Bordes A, Usunier N, Chopra S, et al. Large-scale simple question answering with memory networks. arXiv preprint arXiv:1506.02075, 2015.\n\n39. Sukhbaatar S, Szlam A, Weston J, et al. Weakly Supervised Memory Networks. In Journal, 2015.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 34,
    "chunk_index_in_section": 34,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 560,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c35",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "40. Chen Y, Wu L, Zak M J. Bidirectional Attention Memory Networks for Question Answering over Knowledge Bases. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019: 2913-2923.\n\n41. Saxe A, Tripathi A, Talukdar P. Improving multi-hop question answering over knowledge graphs using knowledge base embeddings. In Proceedings of the 58th annual meeting of the association for computational linguistics. 2020: 4498-4507.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 35,
    "chunk_index_in_section": 35,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 549,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c36",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "42. Lv S, Guo D, Xu J, et al. Graph-based reasoning over heterogeneous external knowledge for commonsense question answering. In Proceedings of the AAAI Conference on Artificial Intelligence. 2020, 34(05): 8449-8456.\n\n43. 赵思洋. 基于联合训练和无监督方法的中文知识图谱问答研究. 哈尔滨工业大学，2020. DOI: 10.27061/d.cnki.ghgdu.2020.000642.\n\n44. Lewis P, Denoyer L, Riedel S. Unsupervised question answering by cloze translation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Florence, Italy: Association for Computational Linguistics, 2019: 4896-4910.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 36,
    "chunk_index_in_section": 36,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 563,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c37",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "45. Reimers N, Gurvich I. Sentence-BERT: Sentence Embeddings using Siamese Networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019: 3982-3992.\n\n46. Bosselut A, Rashkin H, Sap M, et al. COMET: Commonsense Transformers for Knowledge Graph Construction. In Association for Computational Linguistics, 2019.\n\n清洗后的内容：",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 37,
    "chunk_index_in_section": 37,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 450,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c38",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "清洗后的内容：\n\n47. Raffel C 5, Shazeer N, Roberts A, et al. Exploring the limits of transfer learning with a unified text-to-text transformer [J]. The Journal of Machine Learning Research, 2020, 21(1): 5485-5551.\n\n48. Liu P, Yuan W 5, Fu J? et al. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing [J]. ACM Computing Surveys, 2023, 55(9): 1-35.\n\n49. Khashabi D, Min S? Khot T, et al. UNIFIEDQA: Crossing Format Boundaries with a Single QA System [C]//Findings of the Association for Computational Linguistics: EMNLP 2020. 2020: 1896-1907.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 38,
    "chunk_index_in_section": 38,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 586,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c39",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "50. Brown T, Mann B? Ryder N 5, et al. Language models are few-shot learners [J]. Advances in Neural Information Processing Systems, 2020, 33: 1877-190L.\n\n51. Li XL, Liang P. Prefix-Tuning: Optimizing Continuous Prompts for Generation [C]//Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021: 4582-4597.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 39,
    "chunk_index_in_section": 39,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 444,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c40",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "52. Sun Y, Zhang Y, Qi L, et al. TSGP: Two-Stage Generative Prompting for Unsupervised Commonsense Question Answering [J]. arXiv preprint arXiv:2211.3515 [cs] (2022).\n\n53. Chren WA. One-hot residue coding for low dday_power product CMOS design [J]. IEEE Transactions on Circuits and Systems II: Analog and Digital Signal Processing, 1998, 45(3): 303-313.\n\n54. Koppel M. The course of dimensionality [C]//5th online world conference on soft computing in industrial applications (WSC5). 2000: 1-8.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 40,
    "chunk_index_in_section": 40,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 495,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c41",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "55. Beng io Y, Ducharme R, Vincent P. A neural probabilistic language model [J]. Advances in Neural Information Processing Systems, 2000, 13.\n\n56. Vidaurri D? Bielza C, Larrañaga P. A survey of LI regression [J]. International Statistical Review, 2013, 81(3): 361-387.\n\n57. Cortes C, Mohri M, Rostamizadeh A. L 2 regularization for learning kernels [C]//Proceedings of the Twentieth Conference on Uncertainty in Artificial Intelligence. 2009: 109-116.\n\n58. Mikolov T, Chen K, Corrado G, et al. Efficient estimation of word representations in vector space [M]. 2013.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 41,
    "chunk_index_in_section": 41,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 565,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c42",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "59. Shershtinsky A. Fundamentals of recurrent neural network (RNN) and long short-term memory (LSTM) network [J]. Physics D: Nonlinear Phenomena, 2020, 404: 132306.\n\n60. Mikolov T, Sutskever I? Chen K 3, et al. Distributed representations of words and phrases and their compositionality [J]. Advances in Neural Information Processing Systems, 2013, 26.\n\n61. Huffman D A. A method for the construction of minimum-redundancy codes [J]. Proceedings of the IRE, 1952, 40(9): 1098-1101.\n\n62. Kleinbaum D G, Dietz K, Gail M, et al. Logistic regression [M]. New York: Springer-Verlag, 2002.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 42,
    "chunk_index_in_section": 42,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 583,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c43",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "63. Pennington J, Socher R, Manning C D. GloVe: Global vectors for word representation [A]//Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) [C]. 2014: 1532-1543.\n\n64. Socher R, Chen D, Manning C D, et al. Reasoning with neural tensor networks for knowledge base completion [J]. Advances in Neural Information Processing Systems, 2013, 26.\n\n65. Liu Z 5, Li J 5, Shen Z 5, et al. Learning efficient convolutional networks through network slimming [C]//Proceedings of the IEEE international conference on computer vision. 2017: 2736-2744.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 43,
    "chunk_index_in_section": 43,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 582,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c44",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "66. Bordes A, Usunier N 5, Garcia-Duran A? et al. Translating embeddings for modeling multi-relational data [J]. Advances in Neural Information Processing Systems, 2013, 26.\n\n67. Wang Z 5, Zhang J 5, Feng J? et al. Knowledge graph embedding by translating on hyperplanes [C]//Proceedings of the AAAI conference on artificial intelligence. 2014, 28(1).\n\n68. Lin Y, Liu Z, Sun M, et al. Learning entity and relation embeddings for knowledge graph completion [C]//Proceedings of the AAAI conference on artificial intelligence. 2015, 29(1).",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 44,
    "chunk_index_in_section": 44,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 536,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c45",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "69. Ji G 5, He S 5, Xu L, et al. Knowledge graph embedding via dynamic mapping matrix [C]//Proceedings of the 53rd annual meeting of the association for computational linguistics and the 7th international joint conference on natural language processing (volume 1: Long papers). 2015: 687-696.\n\n70. Wang F, Jiang M, Qian C, et al. Residual attention network for image classification [C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 3156-3164.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 45,
    "chunk_index_in_section": 45,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 482,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c46",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "71. Oliveira A, Torralba A, Castelhano M S? et al. Top-down control of visual attention in object detection [C]//2003 IEEE International Conference on Image Processing (Cat. No.03CH37429). 2003, 1: 1-253.\n\n72. Bahdanau D? Cho K, Beng io Y. Neural machine translation by jointly learning to align and translate [J]. arXiv preprint arXiv:1409.0473 [cs] (2014).\n\n73. Radford A, Narasimhan K, Salimans T, et al. Improving language understanding by generative pre-training [J]. 2018.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 46,
    "chunk_index_in_section": 46,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 478,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c47",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "74. Devlin J, Chang M-W, Lee K, et al. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [A]//Minneapolis, Minnesota: Association for Computational Linguistics, 2019: 4171-4186.\n\n75. Vaswani A, Shazeer N 5, Parmar N, et al. Attention is all you need [A]//Proceedings of the 31st International Conference on Neural Information Processing Systems. 2017: 6000-6010.\n\n76. Glorot X 9, Bordes A, Beng io Y. Deep sparse rectifier neural networks [C]//Fourteenth International Conference on Artificial Intelligence and Statistics. 2011: -323.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 47,
    "chunk_index_in_section": 47,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 565,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c48",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "77. Hochreiter S 5, Schmidhuber J. Long short-term memory [J]. Neural Computation, 1997, 9(8): 1735-1780.\n\n78. Cho K, Merrienboer B, Gulcehre C, et al. Learning phrase representations using RNN Encoder-Decoder for Statistical Machine Translation [C]//EMNLP. 2014.\n\n79. Radford A, Wu J? Child R 5, et al. Language models are unsupervised multitask learners [J]. OpenAI blog, 2019, 1(8): 9.\n\n80. Liu Y, Ott M, Goyal N 5, et al. RoBERTa: A robustly optimized BERT pretraining approach [J]. arXiv preprint arXiv:1907.11692 [cs] (2019).",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 48,
    "chunk_index_in_section": 48,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 531,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c49",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "81. Lan Z, Chen M, Goodman S? et al. ALBERT: A lite BERT for self-supervised learning of language representations [J]. arXiv preprint arXiv:1909.11942 [cs]; 2019.\n\n82. Wu Z, Pan S, Chen F? et al. A Comprehensive Survey on Graph Neural Networks [J]. IEEE Transactions on Neural Networks and Learning Systems, 2021, 32(1): 4-24.\n\n83. Kipf TN, Wellinger M. Semi-supervised classification with graph convolutional networks [J]. arXiv preprint arXiv:1609029907 [cs]; 2016.\n\n84. Velickovic P, Cucurull G, Casanova A, et al. Graph attention networks [J]. arXiv preprint arXiv:1710.10903 [cs]; 2017.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 49,
    "chunk_index_in_section": 49,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 591,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s0_c50",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "85. Hamilton WL, Ying R, Leskovec J. Inductive representation learning on large graphs [A]//Proceedings of the 31st International Conference on Neural Information Processing Systems. 2017.",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 50,
    "chunk_index_in_section": 50,
    "total_chunks_in_section": 51,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 188,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s1_c0",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "[87] Zhou D, Zheng L, Han J, et al. A data-driven graph generative model for temporal interaction networks [A]. // Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining [C], 2020: 401-411.\n\n[88] Kipf TN, Wellinger M. Variational graph auto-encoders [J]. arXiv preprint arXiv:161107308, 2016.\n\n[89] Liu P, Yuan W, Fu J, et al. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing [J]. ACM Computing Surveys, 2023, 55(9): 1-35.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 51,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 6,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 519,
    "chunk_method": "hierarchical",
    "importance_weight": 0.44000000000000006,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s1_c1",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "[90] Petroni F, Rocktaschel T, Riedel S, et al. Language Models as Knowledge Bases? [A]. // Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing [C], 2019: 2463-2473.\n\n[91] Schick T, Schütze H. It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners [C]. // Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies [C], 2021: 2339-2352.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 52,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 482,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s1_c2",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "[92] Zhong Z, Friedman D, Chen D. Factual Probing Is [MASK]: Learning vs. Recall [C]. // Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies [C], 2021: 5017-5033.\n\n[93] Liu X, Zheng Y, Du Z, et al. GPT understands, too [J]. arXiv preprint arXiv:2103.10385, 2021.\n\n[94] Xu Y, Zhu C, Xu R, et al. Fusing Context into Knowledge Graph for Commonsense Question Answering [C]. // Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021 [C], 2021: 1201-1207.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 53,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 561,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s1_c3",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "[95] Shwartz V, West P, Le Bras R, et al. Unsupervised Commonsense Question Answering with Self-Talk [C]. // Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing [C], 2020: 4615-4629.\n\n[96] Bosselut A, Le Bras R, Choi Y. Dynamic neuro-symbolic knowledge graph construction for zero-shot Commonsense question answering [C]. // Proceedings of the AAAI conference on Artificial Intelligence [C], 2021: 4923-4931.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 54,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 443,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s1_c4",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "[97] Niu Y, Huang F, Long J, et al. A Semantic-based Method for Unsupervised Commonsense Question Answering [C]. // Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) [C], 2021: 3037-3049.\n\n[98] Liu J, Liu A, Lu X, et al. Generated Knowledge Prompting for Commonsense Reasoning [C]. // Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) [C], 2022: 3154-3169.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 55,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 555,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于常识知识的多选式问答研究_张力翚_s1_c5",
    "source_id": "基于常识知识的多选式问答研究_张力翚",
    "text": "[99] Oord A, Li Y, Vinyals O. Representation learning with contrastive predictive coding [J]. arXiv preprint arXiv:1807.03748, 2018.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 56,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 132,
    "chunk_method": "hierarchical",
    "importance_weight": 0.42000000000000004,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于提示学习的少样本文本分类研究与应用_魏志宇_s0_c0",
    "source_id": "基于提示学习的少样本文本分类研究与应用_魏志宇",
    "text": "近年来，基于深度学习的方法在自然语言处理任务上取得了显著成绩。然而，深度模型通常需要大量高质量的标注语料，而获取这些数据非常困难。文本分类任务作为NLP领域最常见的任务类型之一，对每个新的领域任务都去标注大规模的数据通常是不现实的。少样本学习旨在利用已学习的先验知识，通过有限的带有标注的训练数据，就可以快速地在目标任务上完成模型的优化。通过少样本学习，既可以降低标注数据的获取成本，又可以加速模型的落地部署和缩短迭代周期。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 6,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 212,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9900000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于提示学习的少样本文本分类研究与应用_魏志宇_s0_c1",
    "source_id": "基于提示学习的少样本文本分类研究与应用_魏志宇",
    "text": "随着BERT等预训练语言模型的提出，基于预训练和微调的两阶段训练范式逐渐成为新的趋势，并在多数的自然语言处理任务上取得了空前的成功。但是在微调阶段，模型的性能通常取决于任务和有标注训练数据的数量。然而在大多数情况下，通常难以获取大量相关领域的标注数据，这使得当模型在面对仅含少量训练样本的下游任务时，往往表现不佳。与微调方法不同的是，基于提示学习的方法，通过添加灵活的提示信息，将具体下游任务与预训练任务在形式上相统一，使得在低资源场景下也能够取得良好的效果。提示学习的优势在于既不需要大量相关领域的数据进行提前训练，也无需显著的改变预训练语言模型结构和参数，仅通过对任务形式和输入形式的改变，就可以达到利用预训练语言模型中已经获得的通用领域知识的目的，实现真正的少样本学习。\n\n本文针对文本分类任务，对基于提示学习的方法进行少样本学习研究，开展如下工作：",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 378,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于提示学习的少样本文本分类研究与应用_魏志宇_s0_c2",
    "source_id": "基于提示学习的少样本文本分类研究与应用_魏志宇",
    "text": "本文针对文本分类任务，对基于提示学习的方法进行少样本学习研究，开展如下工作：\n\n1）当前基于提示学习的少样本文本分类方法仅利用了预训练语言模型中的通用知识，而忽略了下游任务中的具体类别表征表示。本文提出一种基于提示学习和三元组损失的少样本文本分类算法。该算法将文本分类任务转换成基于自然语言推理的提示学习形式，通过任务形式的转换，实现在利用预训练语言模型的先验知识的基础上，达到隐式的数据增强，并通过两种不同粒度的损失进行优化。并且，为了捕获下游任务中丰富的类别表征信息，通过三元组损失进行联合优化，同时引入掩码语言模型任务作为正则项，提升模型的泛化能力。此外，本文又在所提出方法的基础上设计了合适的预训练任务进行进一步地预训练。最终在中、英文数据集中验证了本文提出方法的有效性。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 340,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于提示学习的少样本文本分类研究与应用_魏志宇_s0_c3",
    "source_id": "基于提示学习的少样本文本分类研究与应用_魏志宇",
    "text": "2）设计并实现了基于少样本学习的文本分类任务智能标注工具。在本系统中，保留了传统标注工具中通过自定义快捷操作方式进行的人工标注形式，便于用户灵活标注。同时基于上述的算法研究成果，用户仅需提供少量的标注数据，通过简单地参数配置，就可以完成模型的在线训练以及对数据的智能标注工作。此外，用户可通过智能标注结果的反馈，更新训练数据，即通过主动学习策略，持续提升模型性能和数据标注质量。最后通过一系列的系统测试，表明本系统功能能满足设计需求，可稳定运行。\n\n关键词：预训练语言模型，少样本学习，提示学习，文本分类\n\n3.2 基于提示学习和三元组损失的少样本文本分类算法\n\n3.2.1 模型架构\n\n3.2.2 基于自然语言推理的提示学习模块\n\n3.2.3 度量优化模块\n\n3.2.4 基于自然语言推理的继续预训练过程\n\n3.2.5 算法流程\n\n3.3 实验设计和分析\n\n3.3.1 评测数据集介绍",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 393,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于提示学习的少样本文本分类研究与应用_魏志宇_s0_c4",
    "source_id": "基于提示学习的少样本文本分类研究与应用_魏志宇",
    "text": "3.2.5 算法流程\n\n3.3 实验设计和分析\n\n3.3.1 评测数据集介绍\n\n3.3.2 评测指标和基线方法\n\n3.3.3 网络训练参数设置\n\n3.3.4 中文数据集实验结果\n\n3.3.5 英文数据集实验结果\n\n3.3.6 组件有效性分析\n\n3.3.7 提示模版分析\n\n3.3.8 负采样性能分析\n\n3.3.9 可视化分析\n\n3.4 本章小结\n\n第四章 基于少样本学习的文本分类智能标注工具\n\n4.1 需求分析\n\n4.1.1 系统功能性需求\n\n4.1.2 系统非功能性需求\n\n4.2 系统概要设计\n\n4.2.1 总体功能设计\n\n4.3 系统详细设计\n\n4.3.1 人工数据标注模块设计\n\n4.3.2 智能标注模块设计\n\n4.3.3 主动学习模块设计\n\n4.3.4 数据库设计\n\n4.4 系统实现\n\n4.4.1 系统实现环境\n\n4.4.2 标注任务管理模块实现\n\n4.4.3 数据管理模块实现",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 396,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于提示学习的少样本文本分类研究与应用_魏志宇_s0_c5",
    "source_id": "基于提示学习的少样本文本分类研究与应用_魏志宇",
    "text": "4.4.1 系统实现环境\n\n4.4.2 标注任务管理模块实现\n\n4.4.3 数据管理模块实现\n\n4.4.4 标签管理模块实现\n\n4.4.5 智能标注模块实现\n\n4.4.6 数据统计模块实现\n\n4.5 系统测试\n\n4.5.1 测试环境\n\n4.5.2 功能性测试\n\n4.5.3 非功能性测试\n\n4.6 本章小结\n\n第五章 总结与展望\n\n5.1 工作总结\n\n5.2 工作展望",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 5,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 6,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 184,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9450000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于提示学习的少样本文本分类研究与应用_魏志宇_s1_c0",
    "source_id": "基于提示学习的少样本文本分类研究与应用_魏志宇",
    "text": "第一章 绪论\n\n第1章 绪论\n\n1.1 研究背景及意义\n\n1.2 国内外研究现状\n\n1.2.1 基于度量学习的方法\n\n1.2.2 基于元学习的方法\n\n1.2.3 基于提示学习的方法",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 6,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 30,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 90,
    "chunk_method": "hierarchical",
    "importance_weight": 0.44000000000000006,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于提示学习的少样本文本分类研究与应用_魏志宇_s1_c1",
    "source_id": "基于提示学习的少样本文本分类研究与应用_魏志宇",
    "text": "1.3 主要研究内容\n\n本文介绍了少样本学习任务的研究背景和意义，并调研了国内外少样本学习任务的研究现状和发展历程。文章从基于度量学习的方法、基于元学习的方法以及基于提示学习的方法等方面阐述了目前少样本学习任务的研究现状。此外，本文还介绍了主要工作、创新点以及组织结构。\n\n第二章介绍了少样本学习的概念，相关基础技术，以及提示学习方法，包括提示学习的定义、形式和常见训练策略。\n\n第三章详细介绍了基于提示学习的少样本文本分类算法，包括算法实现细节、实验数据集、实验设计和结果分析。\n\n第四章介绍了基于少样本学习的文本分类智能标注工具的设计和实现，包括需求分析、概要设计和详细设计，以及功能模块的实现和系统测试。\n\n第五章总结了少样本学习任务上的主要工作，分析了研究中的问题，并对未来研究方向进行了展望。\n\n一个长度为A的输入序列(bh tw)，需要根据前1个时刻的文本序列，预测第A个时刻h的出现概率。最终整个序列的概率是每个时刻的条件概率的乘积，如公式2-11所示：\n\nP(bh tw) = ΠP(ti+1|ti, bi+1)\n\n其中，i=1,...,A-1。\n\n同理，对于后向语言模型，如公式2-12所示：\n\nP(bh tw) = ΠP(ti|ti+1, tw-i)\n\n其中，i=1,...,A-1。\n\n最终，模型的优化目标是前向模型与后向模型的联合最大似然，如公式2-13所示：",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 7,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 592,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于提示学习的少样本文本分类研究与应用_魏志宇_s1_c2",
    "source_id": "基于提示学习的少样本文本分类研究与应用_魏志宇",
    "text": "其中，i=1,...,A-1。\n\n最终，模型的优化目标是前向模型与后向模型的联合最大似然，如公式2-13所示：\n\nLSTMf + LSTMb\n\n其中，θf表示前向LSTM隐层参数；θb表示后向LSTM隐层参数；θ表示前向与后向LSTM结合后的隐层参数；OS表示Sotmax层参数。\n\n一个多层双向的语言模型结构。假设共有L层，ELMo会输出2L个LSTM最后隐层向量表示，同时再加上词嵌入层中的向量表示，则在一个L层双向语言模型中，对于一个词的向量化表示/？fc，总共含有2L+1个，如公式2-14所示：\n\n/？fc = [E0, h1, ..., hL]\n\n其中，E0表示词嵌入层；hi表示每层LSTM的隐层向量表示。\n\n有两种常用的方法可以将ELMo中学习的词向量表示应用到下游任务。第一种是使用顶层中的隐层输出作为词向量表示，即E@fc() = hL，这是最简单的一种应用方法。第二种是通过加权的方式计算各层中向量的表示，获得最后的词向量表示：\n\nE{Rk) = Σαask * hik\n\n其中，αask表示Sotmax后的规范化权重因子；yask表示缩放因子，可以根据下游任务情况进行调整。",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 8,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 499,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于提示学习的少样本文本分类研究与应用_魏志宇_s1_c3",
    "source_id": "基于提示学习的少样本文本分类研究与应用_魏志宇",
    "text": "E{Rk) = Σαask * hik\n\n其中，αask表示Sotmax后的规范化权重因子；yask表示缩放因子，可以根据下游任务情况进行调整。\n\n基于预训练和微调范式的预训练语言模型对NLP的发展产生了巨大的影响，无需使用带有标注的数据，可以直接从海量的无标注数据中学习到通用的知识表示。随着预训练语言模型的不断发展，根据预训练语言模型目标的不同，进一步可以分为自回归语言模型(Autoregressive LM)和自编码语言模型(Autoencoder LM)。\n\n基于提示学习方法的基本形式化表示如下：\n\n对于输入的文本c，有函数prompt_00可以将输入x转换为基于提示学习的形式Y，即：\n\nY = prompt_00(x)\n\n对于新闻分类任务，假设给定的模板为：\n\n[X]，这是|Z|新闻。\n\n其中，对于原始输入X，需要将其填充到[X]；以及标签的描述性信息Z，同时需要将其填充到[Z]。例如对于原始输入文本X = “因痛失比赛而落泪的职业选手，让人感到心疼。”，通过函数prompt_00函数映射，可以转换为基于提示学习方法的形式，即：\n\nX' = “因痛失比赛而落泪的职业选手，让人感到心疼，这是|MASK|新闻。”\n\n基于提示学习的方法，将下游任务转化为建模语言模型的问题，实现将预训练阶段与下游微调阶段的统一。",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 9,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 565,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于提示学习的少样本文本分类研究与应用_魏志宇_s1_c4",
    "source_id": "基于提示学习的少样本文本分类研究与应用_魏志宇",
    "text": "基于提示学习的方法，将下游任务转化为建模语言模型的问题，实现将预训练阶段与下游微调阶段的统一。\n\n提示学习形式的选择通常取决于任务类型和预训练模型本身。通常来说，对于自然语言生成任务或者使用自回归预训练模型，如GPT系列，通常选择前缀式提示学习形式。而对于自然语言理解任务或者使用基于掩码(MASK)的自编码预训练模型，如BERT、Roberta等，通常选择完型填空形式的提示学习形式。因为基于掩码的自编码预训练任务本质上就是完型填空形式，通过被掩盖住的词的上下文去预测当前词，这样就与预训练语言模型任务形式相一致。\n\n对于提示学习形式的构建方式，通常包含手工构建提示模板方式以及自动化构建提示模板方式。\n\n手工自定义提示模板通常是最简单和最常用的方式。具体地，对于自然语言理解任务，通常选择掩码式的预训练语言模型。同时根据具体的下游任务，手工设计完型填空形式的提示模板，通过利用预训练模型已经学习到的先验知识，实现解决目标下游任务。",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 10,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 417,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于提示学习的少样本文本分类研究与应用_魏志宇_s1_c5",
    "source_id": "基于提示学习的少样本文本分类研究与应用_魏志宇",
    "text": "自动化构建提示模板可以分为离散式提示模板和连续式提示模板。其中，离散式提示模板通常使用目标预训练语言模型或者其他生成式的语言模型，实现生成若干个合适目标任务的自然语言形式的提示模板，也就是说，模板仍然是离散的字符串形式。而对于连续式提示模板，研究人员已经不再关心提示模板的具体形态，此时提示模板可以是非自然语言的表达形式。在该方法中，研究人员希望模型可以自己学习到适合当前任务形式的连续式提示模板形式。\n\n训练策略的选择通常取决于下游任务的样本数量。对于零样本学习问题，通常不需要对语言模型和提示模板参数进行训练更新。对于少量训练实例的情况下，通过仅更新部分模型参数，实现灵活的适应下游任务。对于数据量较大的场景中，通过对预训练语言模型和提示模板参数同时进行更新，可以使模型获得更好的性能。\n\n本文介绍了一种基于提示学习的少样本文本分类算法。该算法通过定义Sentence-Group-Level损失函数，优化一组中的正负样本间关系。具体地，对输入的实例进行数据构造时，会通过输入实例与所对应的正例，以及输入实例与其他的类别进行数据构造，生成n-1个负例，最终总共为每一条输入样本生成n个实例样本。最后，采用交叉熵损失对Sentence-Group-Level进行优化。",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 11,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 535,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于提示学习的少样本文本分类研究与应用_魏志宇_s1_c6",
    "source_id": "基于提示学习的少样本文本分类研究与应用_魏志宇",
    "text": "此外，本文还提出了基于自然语言推理的提示学习模块的损失函数，以及度量优化模块的损失函数。最后，整体的损失函数由提示学习损失、和度量优化损失Xaux的加权构成。\n\n在实验部分，本文在中文公开文本分类数据集和英文公开文本分类数据集上进行实验。通过与基线方法对比，以及使用组件有效性分析、负采样性能分析和可视化分析等方法，验证了本文提出方法的有效性。\n\n在本文中，我们探讨了基于提示学习的少样本文本分类算法。首先，我们介绍了提示学习的概念，并提出了将文本分类任务转换为自然语言推理形式的完型填空任务的方法。接着，我们详细阐述了算法的组成部分，包括度量优化模块和基于自然语言推理的提示学习模块。在实验部分，我们在中文和英文数据集上进行了性能对比实验，结果显示我们的方法在少样本场景下取得了优异的性能。此外，我们还进行了组件有效性分析，验证了度量优化模块和基于自然语言推理的提示学习模块的有效性。最后，我们对提示模版进行了分析，比较了自然语言形式和非自然语言形式的推理词的性能。实验结果表明，我们的方法在少样本文本分类任务中具有强大的潜力。\n\n表 3-13 中文数据集推理词形式性能分析",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 12,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 488,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于提示学习的少样本文本分类研究与应用_魏志宇_s1_c7",
    "source_id": "基于提示学习的少样本文本分类研究与应用_魏志宇",
    "text": "表 3-13 中文数据集推理词形式性能分析\n\n| Method | EPRSTMT | CSLDCP | TNEWS | IFLYTEK | MEAN |\n| ------- | ------- | ------- | ------- | ------- | ------- |\n| 自然语言推理词 |",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 13,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 151,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于提示学习的少样本文本分类研究与应用_魏志宇_s1_c8",
    "source_id": "基于提示学习的少样本文本分类研究与应用_魏志宇",
    "text": "86.0 | 54.3 | 53.5 | 45.4 | 59.8 |\n| 非自然语言推理词 | 85.3 | 55.1 | 54.6 | 46.4 | 60.4 |\n\n表 3-14 英文数据集推理词形式性能分析 (K=8)\n\n| Method | AG | News | Tree | Yelp | Review | MEAN |\n| ------- | ------- | ------- | ------- | ------- | ------- | ------- |\n| 自然语言推理词 | 77.3 | 59.0 | 26.1 | 54.1 |\n| 非自然语言推理词 | 79.5 | 55.8 | 26.1 |",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 14,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 311,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于提示学习的少样本文本分类研究与应用_魏志宇_s1_c9",
    "source_id": "基于提示学习的少样本文本分类研究与应用_魏志宇",
    "text": "54.1 |\n| 非自然语言推理词 | 79.5 | 55.8 | 26.1 | 53.8 |\n\n从中文、英文数据集的实验结果可以看出，非自然语言形式的推理词较为稳定，模型性能较好。具体地，对于形式简单、数据区分度高的任务，如EPRSTMT和Tree等任务，自然语言形式的推理词表现较为出众。而对于类别数较多、复杂的任务，例如TNEWS、IFLYTEK和CSLDCP等任务，非自然语言形式的推理词具备更好的性能，这是由于它可以从具体任务中自主学习到更适合当前模版的推理词形式，而不受自然语言形式的限制。也就是说，非自然语言形式的推理词可以从众多的上下文信息中学习到推理词的连续化表达形式，从而有效避免了一推理词的影响。\n\n(2) 提示模版性能分析\n\n在基于提示学习的方法中，手工设计的提示模版通常会对模型的效果产生定的波动，本小节评估手工设计的模版对最终模型性能产生的影响。实验结果如表 3-15所示。\n\n表 3-15 提示模版对模型准确率的影响\n\n| Prompt | TNEWS Classification | Tree Question Classification (K=8) |\n| ------- | ------------------- | ----------------------------------- |\n| 下面<maskdesc>新闻：<text>。 |",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 15,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 594,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于提示学习的少样本文本分类研究与应用_魏志宇_s1_c10",
    "source_id": "基于提示学习的少样本文本分类研究与应用_魏志宇",
    "text": "54.2 | 50.0 |\n| it<maskdesc>新闻报道：<text>。 | 53.5 | 56.4 |\n| <text>，<desc>新闻？<mask>? | 54.6 | 55.8 |\n| <text>，il<maskdesc>新闻。 | 59.0 | 55.8 |\n\n从实验结果可以看出，不管是中文还是对于英文数据集，模型的性能都会受到提示模版较大的影响。具体地，在中文TNEWS和英文Tree任务上，本小节对模版采用了前缀式与后缀式的形式进行评测。相比之下，在中文数据集上，模型性能差异性相对较小，最大与最小的差值为1.1%。而在英文数据上，模型性能却表现出较大的差异性，最大与最小的差值为6.4%。表明提示模版对模型准确率性能的影响，与具体的下游任务有较大的关系。通过优化模版的形式，可以较大程度提升模型的性能。\n\n3.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 16,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 369,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于提示学习的少样本文本分类研究与应用_魏志宇_s1_c11",
    "source_id": "基于提示学习的少样本文本分类研究与应用_魏志宇",
    "text": "6.4%。表明提示模版对模型准确率性能的影响，与具体的下游任务有较大的关系。通过优化模版的形式，可以较大程度提升模型的性能。\n\n3.3.8 负采样性能分析\n\n在本小节中，本文尝试对中文TNEWS数据集，通过控制不同的负采样数据比例，分析负采样对模型性能的影响。\n\n如图3-6所示，随着负采样的不断变大，模型的整体性能(准确率)也在逐渐提升。通过实验发现，在负采样在等于2、4和8的时候，模型的性能有一个迅速的提升。其中，当负采样在8-13之间的性能相对稳定。当负采样达到14时(TNEWS任务包含15个类别)，模型的准确率有一个急速下降的趋势，表明使用全部类别作为负例，并不会提升模型的性能。\n\n3.3.9 可视化分析\n\n为了评估在引入度量优化模块后，本文所提出算法获得任务类别表征表示的有效性，本小节将通过t-SNE对中文TNEWS数据集通过随机采样进行了可视化分析，结果如图3-7所示。\n\n图3-7 实例向量t-SNE分布可视化\n\n通过对实例类别的分布可视化分析，表明度量优化模块可以为整个模型提供更多额外的类别知识等信息。",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 17,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 464,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于提示学习的少样本文本分类研究与应用_魏志宇_s1_c12",
    "source_id": "基于提示学习的少样本文本分类研究与应用_魏志宇",
    "text": "3.4 本章小结\n\n本章详细介绍了本文提出的基于提示学习和三元组损失的少样本文本分类算法。首先阐述本文提出该算法的动机，接着介绍了模型的整体网络结构，并详细介绍了基于自然语言推理的提示学习模块和度量优化模块。其中，基于自然语言推理的提示学习模块将下游文本分类任务通过添加提示模版转化为掩码式的完型填空形式，使模型能够更好的利用预训练模型中的先验知识，并通过两种不同粒度的损失进行优化。同时，度量优化模块通过三元组损失优化，实现捕获下游具体的任务类别信息。此外，介绍了将基于自然语言推理形式的提示学习融入到继续预训练任务中的方法。接着详细介绍了模型的训练与推理过程。最后通过在中文、英文数据集上进行模型性能的评测，以及对模型的各个组件进行有效性分析等实验，验证了本文提出模型的有效性。\n\n系统需求分析：\n\n1. 易用性需求：系统应简单易用，方便用户快速部署和使用。\n\n2. 可扩展性需求：系统应能灵活接入更多模型进行智能标注，并支持处理一定规模数据的能力。\n\n3. 兼容性需求：前端应适配各种主流浏览器内核，在各种屏幕尺寸和分辨率下正常显示。\n\n4. 安全性需求：用户密码采用加密存储，操作时验证身份和权限，具备日志记录功能。\n\n系统概要设计：\n\n1. 总体功能设计：系统分为用户管理和标注任务管理两部分，标注任务管理细分为数据管理、标签管理、智能标注和标注统计等模块。",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 18,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 584,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于提示学习的少样本文本分类研究与应用_魏志宇_s1_c13",
    "source_id": "基于提示学习的少样本文本分类研究与应用_魏志宇",
    "text": "1. 总体功能设计：系统分为用户管理和标注任务管理两部分，标注任务管理细分为数据管理、标签管理、智能标注和标注统计等模块。\n\n2. 系统详细设计：\n\n- 人工数据标注模块：用户导入数据，添加标签，实现快速人工标注。\n- 智能标注模块：用户配置模型，利用少量训练数据进行智能标注。\n- 主动学习模块：用户根据模型反馈进行数据修正，提升模型效果。\n- 数据库设计：使用SQLite存储用户数据和文本数据，文件存储模型日志。\n\n系统实现：\n\n1. 采用B/S架构，前后端分离技术，前端Vue，后端Django，数据库SQLite，智能标注模块使用PyTorch。\n\n2. 用户登录后可进行标注任务管理，包括创建、管理、删除标注任务。\n\n3. 数据管理模块支持数据导入导出、查看、人工标注。\n\n4. 标签管理模块支持标签的创建、修改、删除和查看。\n\n5. 智能标注模块支持模型训练、评测、推理管理。\n\n6. 数据统计模块统计标注情况、标签分布、数据量、任务评测结果。\n\n系统测试：\n\n1. 测试环境：操作系统、Python版本、前端框架、后端框架、数据库等。\n\n2. 测试分类：功能性测试和非功能性测试。\n\n环境\n操作系统：Ubuntu 16.04LTS\n处理器：Intel(R) Xeon(R) CPU E5-2678 v3 @ 2.50GHz\n远程服务端：—\n内存：47GB\n硬盘：",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 19,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 589,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于提示学习的少样本文本分类研究与应用_魏志宇_s1_c14",
    "source_id": "基于提示学习的少样本文本分类研究与应用_魏志宇",
    "text": "16.04LTS\n处理器：Intel(R) Xeon(R) CPU E5-2678 v3 @ 2.50GHz\n远程服务端：—\n内存：47GB\n硬盘：2.0TB\n\n操作系统：Windows 10系统\n处理器：Intel(R) Core (TM) i5-1035G1 CPU 1.00GHz 1.19GHz\n本地服务端：  \n内存：16GB\n硬盘：500GB\n\n客户端：  \n内存：16GB\n硬盘：500GB\n浏览器：Microsoft Edge 98.0.1108.43；Firefox 94.0.2\n\n第四章基于少样本学习的文本分类智能标注工具\n4.5.2功能测试\n在功能测试中，本小节主要针对4.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 20,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 299,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于提示学习的少样本文本分类研究与应用_魏志宇_s1_c15",
    "source_id": "基于提示学习的少样本文本分类研究与应用_魏志宇",
    "text": "4.1.1节中功能性需求进行测试。具体地，一个功能模块进行了测试。在表4-3中，展示了各个功能模块的测试结果，具体如下：\n\n表4-3功能性测试结果\n功能模块 | 功能 | 测试结果 | 具体信息\n登录 | 通过 | 登录功能正常\n用户管理 | 通过 | 用户名、邮箱、密码校验有效\n数据上传 | 通过 | txt、csv和json格式文件数据上传正常\n数据下载 | 通过 | csv和json格式文件数据下载正常\n数据管理 | 通过 | 数据分页查看功能正常；关键字搜索结果正常且显示正常\n人工数据标注 | 通过 | 通过鼠标点击、快捷键进行人工数据标注正常\n标签定义 | 通过 | 标签创建、修改、删除功能正常\n标签管理 | 通过 | 快捷键定义有效\n标签标识功能 | 通过 | 标签颜色手动、随机切换正常\n模型数据管理 | 通过 | 模据和评测数据的添加、删除功能\n智能任务管理 | 通过 | 智能标注任务创建、删除和查看功能正常\n智能标注结果管理 | 通过 | 结果查看功能正常；结果排序正常；关键字搜索功能正常；标签修正功能正常\n数据标注统计 | 通过 | 数据标注情况统计结果显示正常\n标签分布统计 | 通过 | 标签分布情况统计结果显示正常\n智能标注评测统计 | 通过 | 智能标注评测统计结果显示正常\n数据统计 | 通过 | 数据在不同集合中的统计结果显示正常\n\n4.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 21,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 591,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于提示学习的少样本文本分类研究与应用_魏志宇_s1_c16",
    "source_id": "基于提示学习的少样本文本分类研究与应用_魏志宇",
    "text": "4.5.3非功能测试\n在非功能测试中，本小节主要针对4.1.2节中非功能性需求中提出的易用性和兼容性需求进行测试。其中，易用性中主要测试本系统的在不同环境下部署便捷性；兼容性主要测试不同的浏览器是否可以有效的支持本系统前端界面的正常显示。\n\n表4-4后端部署易用性测试\n部署方式 | 名称 | 是否部署方便\n本地部署 | 后端部署 | 是\n远程部署 | 后端部署 | 是\n\n表4-5浏览器兼容性测试\n浏览器 | 名称 | 是否兼容\nMicrosoft Edge | 前端显示 | 是\nFirefox | 前端显示 | 是\n大于IE9.0的IE浏览器 | 前端显示 | 是\n\n4.6本章小结\n本章详细介绍了基于少样本学习的文本分类智能标注工具的设计与实现工作。首先分析了系统的功能与非功能性需求；然后对系统的概要设计进行介绍。接下来具体介绍智能标注系统中各个核心模块的详细设计与实现过程。最后通过功能性测试与非功能性测试，确保了本系统的正常运行。\n\n参考文献：",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 22,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 429,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于提示学习的少样本文本分类研究与应用_魏志宇_s1_c17",
    "source_id": "基于提示学习的少样本文本分类研究与应用_魏志宇",
    "text": "1. Shot Leamers [A] // Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (C). 2021: 2339-2352.\n\n2. Tam D, Menon R R, Bansal M, et al. Improving and Simplifying Pattern Exploiting Training [A] // Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (C). 2021: 4980-4991.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 23,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 396,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于提示学习的少样本文本分类研究与应用_魏志宇_s1_c18",
    "source_id": "基于提示学习的少样本文本分类研究与应用_魏志宇",
    "text": "4991.\n\n3. Gao T, Fisch A, Chen D. Making Pre-trained Language Models Better Few-shot Learners [A] // Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) (C). 2021: 3816-3830.\n\n4. Raffel C, Shazeer N, Roberts A, et al. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer [J]. Journal of Machine Learning Research, 2020, 21: 1-67.\n\n5. Liu X, Zheng Y, Du Z, et al. GPT understands, too [J]. arXiv preprint arXiv:2103.10385, 2021.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 24,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 591,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于提示学习的少样本文本分类研究与应用_魏志宇_s1_c19",
    "source_id": "基于提示学习的少样本文本分类研究与应用_魏志宇",
    "text": "2103.10385, 2021.\n\n6. Wang S, Fang H, Khabsa M, et al. Entailment as few-shot learner [J]. arXiv preprint arXiv:2104.14690, 2021.\n\n7. Jimg Z, Xu F F, Araki J5, et al. How can we know what language models know? [J]. Transactions of the Association for Computational Linguistics, 2020, 8: 423-438.\n\n8. Sun Y, Zheng Y, Hao C, et al. NSP-BERT: A Prompt-based Zero-shot Learner Through an Original Pre-training Task — Next Sentence Prediction [J]. arXiv preprint arXiv:2109.03564, 2021.\n\n9. Xu L, Lu X, Yuan C, et al. Fewclue: A Chinese few-shot learning evaluation benchmark [J]. arXiv preprint arXiv:",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 25,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 597,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于提示学习的少样本文本分类研究与应用_魏志宇_s1_c20",
    "source_id": "基于提示学习的少样本文本分类研究与应用_魏志宇",
    "text": "2107.07498, 2021.\n\n10. Dagan I, Glickman O. Probabilistic textual entailment: Generic applied modeling of language variability [J]. Learning Methods for Text Understanding and Mining, 2004, 2004: 26-29.\n\n11. Dagan I, Glickman O, Magnini B. The Pascal recognizing textual entailment challenge [A] // Machine Learning Challenges Workshop (C), Berlin, Heidelberg: Springer, 2005: 177-190.\n\n12. Hadsell R, Chopra S, LeCun Y. Dimensionality reduction by learning an invariant mapping [A] // 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (C), 2006, 2: 1735-",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 26,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 596,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于提示学习的少样本文本分类研究与应用_魏志宇_s1_c21",
    "source_id": "基于提示学习的少样本文本分类研究与应用_魏志宇",
    "text": "1742.\n\n13. Mikolov T, Sutskever I, Chen K, et al. Distributed representations of words and phrases and their compositionality [J]. Advances in neural information processing systems, 2013, 26.\n\n14. Peters M, Neumann M, Iyyer M, et al. Deep contextualized word representations [J]. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), 2018.\n\n15. Radford A, Narasimhan K, Salimans T, et al. Improving language understanding by generative pre-training [J]. 2018.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 27,
    "chunk_index_in_section": 21,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 573,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于提示学习的少样本文本分类研究与应用_魏志宇_s1_c22",
    "source_id": "基于提示学习的少样本文本分类研究与应用_魏志宇",
    "text": "2018.\n\n16. Radford A, Wu J, Child R, et al. Language models are unsupervised multitask learners [J]. OpenAI blog, 2019, 1(8): 9.\n\n17. Devlin J, Chang M W, Lee K, et al. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [A] // Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (C), 2019.\n\n18. Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need [J]. Advances in neural information processing systems, 2017, 30.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 28,
    "chunk_index_in_section": 22,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 546,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于提示学习的少样本文本分类研究与应用_魏志宇_s1_c23",
    "source_id": "基于提示学习的少样本文本分类研究与应用_魏志宇",
    "text": "30.\n\n19. Schuster M, Nakajima K. Japanese and Korean voice search [A] // Acoustics, Speech and Signal Processing (ICASSP) (C), IEEE International Conference on, 2012.\n\n20. Liu Y, Ott M, Goyal N, et al. RoBERTa: A robustly optimized BERT pretraining approach [J]. arXiv preprint arXiv:1907.11692, 2019.\n\n21. Sun Y, Wang S, Li Y, et al. ERNIE: Enhanced representation through knowledge integration [J]. arXiv preprint arXiv:1904.09223, 2019.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 29,
    "chunk_index_in_section": 23,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 439,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于提示学习的少样本文本分类研究与应用_魏志宇_s1_c24",
    "source_id": "基于提示学习的少样本文本分类研究与应用_魏志宇",
    "text": "1904.09223, 2019.\n\n22. Cui Y, Che W, Liu T, et al. Revisiting Pre-trained Models for Chinese Natural Language Processing [A] // Findings of the Association for Computational Linguistics: EMNLP 2020 (C), 2020: 657-668.\n\n23. Liu P, Yuan W, Fu J, et al. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing [J]. arXiv preprint arXiv:2107.13586, 2021.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 30,
    "chunk_index_in_section": 24,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 398,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于提示学习的少样本文本分类研究与应用_魏志宇_s1_c25",
    "source_id": "基于提示学习的少样本文本分类研究与应用_魏志宇",
    "text": "2107.13586, 2021.\n\n24. Petrov F, Rocktäschel T, Riedel S, et al. Language models as knowledge bases? [A] // Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) (C), 2019: 2463-2473.\n\n25. Li X L, Liang P. Prefix-tuning: Optimizing continuous prompts for generation [A] // Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) (C), 2021: 4582-",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 31,
    "chunk_index_in_section": 25,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于提示学习的少样本文本分类研究与应用_魏志宇_s1_c26",
    "source_id": "基于提示学习的少样本文本分类研究与应用_魏志宇",
    "text": "4597.\n\n26. Lester B, Al-Rfou R, Constant N. The power of scale for parameter-efficient prompt tuning [A] // Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (C), 2021: 3045-3059.\n\n27. Schroff F, Kalenichenko D, Philbin J. FaceNet: A unified embedding for face recognition and clustering [A] // Proceedings of the IEEE conference on computer vision and pattern recognition (C), 2015: 815-823.\n\n28. Weinberger K Q, Saul L K. Distance metric learning for large margin nearest neighbor classification [J]. Journal of Machine Learning Research, 2009, 10(1): 207-",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 32,
    "chunk_index_in_section": 26,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 595,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于提示学习的少样本文本分类研究与应用_魏志宇_s1_c27",
    "source_id": "基于提示学习的少样本文本分类研究与应用_魏志宇",
    "text": "244.\n\n29. Xu L, Hu H, Zhang X, et al. CLUE: A Chinese Language Understanding Evaluation Benchmark [A] // Proceedings of the 28th International Conference on Computational Linguistics (C), 2020: 4762-4772.\n\n30. Li X, Roth D. Learning question classifiers [A] // COLING 2002: The 19th International Conference on Computational Linguistics (C), 2002.\n\n31. Zhang X, Zhao J, LeCun Y. Character-level convolutional networks for text classification [J]. Advances in neural information processing systems, 2015, 28.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 33,
    "chunk_index_in_section": 27,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 507,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于提示学习的少样本文本分类研究与应用_魏志宇_s1_c28",
    "source_id": "基于提示学习的少样本文本分类研究与应用_魏志宇",
    "text": "28.\n\n32. Paszke A, Gross S, Massa F, et al. PyTorch: An imperative style, high-performance deep learning library [J]. Advances in neural information processing systems, 2019, 32.\n\n33. Cui Y, Che W, Liu T, et al. Pre-training with whole word masking for Chinese BERT [J]. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2021, 29: 3504-3514.\n\n34. Loshchilov I, Hutter F. Decoupled Weight Decay Regularization [A] // International Conference on Learning Representations (C), 2018.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 34,
    "chunk_index_in_section": 28,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 494,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于提示学习的少样本文本分类研究与应用_魏志宇_s1_c29",
    "source_id": "基于提示学习的少样本文本分类研究与应用_魏志宇",
    "text": "2018.\n\n35. van der Maaten L, Hinton G. Visualizing data using t-SNE [J]. Journal of machine learning research, 2008, 9(11).",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 35,
    "chunk_index_in_section": 29,
    "total_chunks_in_section": 30,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 123,
    "chunk_method": "hierarchical",
    "importance_weight": 0.42000000000000004,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c0",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "图像段落描述旨在为给定图像自动生成描述性段落，是对传统的图像单句描述研究的进一步深入，属于多模态人工智能的新兴研究。随着生成目标从单个句子拓展到多句子段落，图像段落描述对模型的视觉线索梳理和文本逻辑建构能力提出了更高的要求。此外，自动生成能够承载更多信息的段落具有更广阔的应用前景。目前，主流研究存在以下问题亟待解决：首先，主流方法缺乏对段落句子结构信息的建模，容易导致生成段落出现内容冗余和上下文不连贯问题。此外，主流方法也缺乏对图像区域结构关系的建模，简单地将图像表示为一个无结构的区域集合，容易导致模型对图像理解不充分，进而导致描述不全面。为此，本文提出利用树结构显式建模段落结构以及图像区域关系，并将树结构引入图像段落描述模型中。具体而言，本文开展的研宄工作如下：针对句子结构缺失问题，本文设计了用于构建段落句子树结构的层次建模方法",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 109,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 370,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9900000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c1",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "。将句子树结构作为监督信息，本文提出了种新颖的树结构段落解码框架S2TD（Splitting to Tree Decoder）。该框架将段落生成过程建模为一棵自顶向下不断扩展的二叉树结构。从图像全局特征开始，父节点特征被逐步划分成左右子节点。最终，叶节点特征被解码成句子并组成段落。二、针对区域关系缺失问题，本文设计了图像区域树结构的启发式构建方法。将区域树结构作为指导信息，本文提出了种新颖的树结构增强的编码器网络TEE（Tree Enhanced Encoder）。该编码器网络利用区域树分组结果逐层约束多头自注意力机制，使模型对图像内容的理解更加全面和准确。本文在图像段落描述基准数据集上开展了实验。通过定量分析和定性对比，验证了所提方法的可行性与有效性。实验结果表明，在图像段落描述模型中引入树结构有助于生成质量更高的描述段落。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 368,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c2",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "关键词：多模态人工智能 深度学习 图像段落描述 树结构\n\n图像段落描述任务相比传统的单句描述任务更具挑战性。单句描述任务的生成目标为一个篇幅在三十词以内的单个句子，而段落描述任务的生成目标为一个包含六个句子左右的段落文本。段落描述要求生成的段落句子内容多样、句子间上下文语义连贯，符合视觉观察和写作逻辑。同时，段落的多样和连贯受到图像内容的指导和约束。这要求图像描述模型的设计和训练推理提出更高的要求。图像段落描述研究具有重要的研究意义和应用价值。\n\n图像单句描述研究主要围绕编码器-解码器框架展开，一方面探索如何在编码器端引入更好的视觉编码网络，提升图像特征的表达能力。另一方面研究在解码器端如何更好地利用视觉上下文信息生成文本，并缓解暴露偏差问题。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 326,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c3",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "图像段落描述研究根据段落文本的不同建模视角，相关方法可以划分为两大类模型：层次结构模型和非层次结构模型。层次结构模型将段落生成建模为多个句子的顺序生成，即逐句生成段落。非层次结构模型将段落的生成建模为单个长句子的生成，即逐词生成段落。层次结构模型通过显式计算用于指导句子生成的特征表示，不同的句子由不同特征解码得到并最终组成段落。非层次结构模型对段落内的句子不做显式划分，直接利用得到的视觉表示解码生成段落中的词，不引入句子级别的特征表示。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 220,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c4",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "图像段落描述研究主要围绕提升段落生成的连贯性和多样性展开。相关研究可分为层次结构模型和非层次结构模型两大类。最近，图文多模态领域的研究者开始关注树结构的建模能力和应用价值。在视觉接地任务中，Hong等人提出利用二叉树结构实现文本节点与视觉内容的对齐。在视觉问答任务中，Cao等人将问题文本剖析为依存句法树，并在遍历过程中利用注意力机制挖掘图像中的线索信息。在图像描述领域，Yao等人将图像剖析为包含全局、区域、实例三层的树结构，递归地融合不同粒度的信息增强句子生成质量。Ma等人提出一种应用于单句描述的树结构解码器。Wang等人在图像到食谱文本的生成过程中引入树结构信息，增强生成的准确性和连贯性。本文的主要研究内容如下：1) 基于文本树结构的图像段落描述算法研究；2) 引入视觉树结构的图像段落描述算法研究",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 354,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c5",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "。本文的主要贡献包括：1) 在图像段落描述任务中，首次提出利用树结构进行图文模态的建模，并结合建模后的树结构进行模型改进；2) 提出了新颖的树结构段落解码框架以及对应的段落句子树层次建模方法；3) 提出了树结构增强的编码器组件以及对应的图像区域树启发式构建方法。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 5,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 131,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c6",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "相比于通过图像分类训练得到卷积神经网络，目标检测模型能够提供更加细粒度的图像信息，有助于图像描述模型解码生成更准确的描述文本。具体而言，在图像描述任务中，经过多轮卷积和池化操作，卷积神经网络抽取得到特征图的分辨率往往是原图像的1/16甚至更低，因此只能对图像中形状较大的、特征明显的对象实现有效的响应并提取信息。而将目标检测模型作为特征提取器，有助于缓解上述问题，获取区域级别、语义更为丰富和准确的图像特征表示。\n\n在图文多模态研究领域，由Aderson等人在Visual Genome数据集上预训练的Faster R-CNN模型被广泛用于图像特征抽取。预训练阶段，该模型在学习目标检测的同时，需要对图像内物体的颜色、状态等属性进行预测。通过对物体属性的监督学习，该模型进一步提升了图像区域特征的编码质量，极大地提升了下游任务的表现。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 6,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 367,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c7",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "此外，Johnson等人提出的密集描述方法模型Densecap也常被应用于特征抽取。Densecap在Faster R-CNN的基础上，将边界框中物体类别预测任务拓展为描述句子生成任务。值得一提的是，图像段落描述研究的一部分工作是基于Densecap检测的区域特征和对应生成的描述句子开展的。\n\n文本解码器负责文本序列的建模以及生成，完成从编码器得到的中间表示到输出文本的转换。文本解码器的理论基础源于计算语言学中对神经语言模型(Neural Language Model, NLM)的研究。随着深度学习的发展，相关方法在词表示学习、机器翻译等任务中得到了广泛应用，成为了建模文本序列的重要范式。\n\n在训练阶段，给定编码器得到的中间表示V，文本解码器的模型参数为0，优化目标为最大化标签文本S={w1,...,wT}的似然函数：\n\n* = argmax logp(S|V;d)",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 7,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 388,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c8",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "* = argmax logp(S|V;d)\n\n利用链式法则，可以将联合概率分布拆分为：\n\nlogp(S|V;d) = logp(w1|V;d) + logp(w2|V,w1;d) + ... + logp(wT|V,w1,...,wT-1;d)\n\n文本解码器需要通过神经网络完成对p(wt|ht-1,w1,...,wT-1;d)的建模和学习。\n\n在测试阶段，文本解码器通过选取概率最大的值作为输出(贪婪解码)或依据预测的概率分布随机选取(随机采样)，从而实现文本的生成。本节主要介绍两种常用于图像描述任务的解码器组件：循环神经网络和变压器网络。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 8,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 273,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c9",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "循环神经网络是一类深度学习模型，因循环利用同一个网络更新隐状态表示(Hidden State)而得名。循环神经网络适用于处理序列形式的输入和输出，包括但不限于文本、语音、时序信号等数据类型。循环神经网络与卷积神经网络类似，也具有参数共享的特点。此外，循环神经网络每个时间步均有输出，且循环神经网络当前时刻的输出与当前时刻的输入以及历史信息有关。\n\n具体而言，设t时刻模型输入为xt，存储历史信息的隐状态表示为ht-1。朴素的循环神经网络的前向过程如下所示：\n\nht = tanh(Wxxt + Whht-1 + b)\n\n其中，Wx和Wh为可学习的参数矩阵，b为可学习的偏置向量。随着序列输入的改变，隐状态表示将被不断更新。循环神经网络通过BPPT(Backpropagation Through Time)方法实现误差反传和模型参数更新。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 9,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 369,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c10",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "然而，朴素的循环神经网络在训练时容易遇到梯度消失和梯度爆炸问题，导致网络参数难以得到有效的更新。此外，朴素的循环神经网络难以利用历史较远的信息，长距离的序列信息捕捉能力有限。\n\n为此，Hochreiter等人提出了长短期记忆网络(Long Short-Term Memory, LSTM)。LSTM利用门控机制，对输入、历史信息和输出进行有选择性的更新。通过有选择的遗忘和更新，LSTM能够更好的处理长序列数据带来的依赖性挑战。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 10,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 214,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c11",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "变压器网络(Transformer)是由Vaswani等人在2017年提出的新一代神经网络架构，最早用于解决机器翻译问题，包含一套完整的编码器和解码器设计。与前馈神经网络、卷积神经网络不同，循环神经网络计算时依赖于前一时刻的输出，因此无法进行并行化计算，训练效率成为了瓶颈。此外，LSTM等循环神经网络结构依然存在梯度消失和爆炸问题。为此，变压器网络提出利用多头注意力机制实现序列上下文间的依赖关系的捕捉和编码，极大的降低了训练复杂度，使得训练阶段能够进行并行计算加速训练，逐渐成为了文本建模的新范式。\n\n本节主要介绍变压器网络的解码器组件：\n\n1)多头注意力机制(Multi-Head Attention)\n\n首先，定义注意力操作Attention如下：\n\nAttention(Q,K,V) = softmax(V)",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 11,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 359,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c12",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "Attention(Q,K,V) = softmax(V)\n\n其中，Q、K和V分别代表查询向量(Query)、键值向量(Key)以及值向量(Value)。L为序列长度，df和dk分别为查询向量和键值向量的维度。将原始的(Q,K,V)先投影到维度dm，分别进行h个独立注意力操作，将最终得到的结果拼接，便得到了多头注意力机制如下：\n\nMultiHead(Q,K,V) = Concat(head1,...,headh)W0\n\n其中，headj = Attention(QWjQ,KWjK,VWjV)，WjQ、WjK和WjV为可学习的参数矩阵。\n\n2)位置编码(Position Encoding)\n\n变压器网络通过在词向量中引入额外的位置编码，解决由于多头注意力无法建模序列数据的顺序信息的缺陷。位置编码定义如下：",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 12,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 356,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c13",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "PE(pos,2i) = sin(pos/10000^(2i/d))\nPE(pos,2i+1) = cos(pos/10000^(2i/d))\n\n其中，pos为词向量在序列中的下标值，i为词向量对应维度的下标值。\n\n3)解码过程\n\n输入为编码器得到的中间表示V和文本序列的词向量矩阵S，变压器网络解码器组件的前向过程如下所示：\n\nS = PE(S)\n\nSself = MaskedMultiHead(S,S,S)\n\nSdec = MultiHead(S,K,V)\n\nS = (ReLU(SdecWO + bO)) + b\n\nP = Softmax(S)",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 13,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 276,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c14",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "S = (ReLU(SdecWO + bO)) + b\n\nP = Softmax(S)\n\n其中，为了清晰展示，每一层操作均采用了残差连接并采用LayerNorm进行归一化，即St = LayerNorm(xt + Sublayer(xt-1))。公式2-14、2-15和2-16一组计算称为一层解码网络，实际使用时会层叠多个参数独立的解码网络，提升模型的深度，从而增强模型的建模能力。\n\n此外，最终输出为Pemk，说明可以同时为序列每个位置预测的词概率分布，实现了训练阶段并行化计算。然而在文本生成阶段，变压器模型的解码器仍需要逐步抽样上一时间步的输出，更新输入矩阵，才能得到当前时间步的输出。\n\n值得一提的是，在图像单句描述的研究中，变压器网络逐渐替代LSTM成为首选的解码器组件。然而在图像段落描述研究中，变压器网络作为解码器的探索仍处于早期阶段，LSTM仍是段落描述生成的主流解码器组件。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 14,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 397,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c15",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "2.4 数据集及评测指标\n\n2.4.1 图像段落描述基准数据集\n\n当前图像段落描述研究的基准数据集为斯坦福图像段落数据集(Stanford Image Paragraph Dataset)，由Kruse等人在2017年收集并开源。段落基准数据集从MSCOCO和Visual Genome数据集中选取了19551张图像，交由数据众包平台进行人工标注，为每张图像标注了对应的描述段落文本。基准数据进一步划分为训练集、验证集和测试集，分别包含14575、2487和2489个图像和段落描述对。平均每个标注段落的长度为67.5个词，包含约5.7个句子，每个句子包含11.9个词。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 15,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 285,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c16",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "对比来看，图像单句描述基准数据集MSCOCO每张图像包含5个描述句子，其句子的平均长度为11.3个词。单句描述和段落描述基准数据集中名词、形容词、动词和代词的比例差异也反映出段落描述更为内容丰富、生成难度更高，具体统计结果展示在图2-1中。\n\n2.4.2 图像段落描述任务评价指标\n\n在图像段落描述任务中，模型的性能由其生成的段落文本质量反映，生成的段落文本越准确、上下文逻辑越连贯以及内容越丰富，则模型的性能越好。人工评价由于不同研究的标准不统一且难以横向比较、耗时费力等客观条件限制，逐渐被自动化评价指标替代。基于“模型生成的文本与人类给出的标准文本越接近，则生成的文本质量越高”的前提假设，自动化评价指标通常遵循如下范式：\n\nMetric = Sim(C,R)",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 16,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 333,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c17",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "Metric = Sim(C,R)\n\n其中，C为模型生成的待评测文本(Candidates)，R为人工标注的、用于参考的标准文本(References或Ground Truth)。本节将对图像段落描述研究中广泛采用的BLEU、METEOR、CIDEr三类自动化评价指标进行说明。\n\n1) BLEU(Bilingual Evaluation Understudy)\n\nBLEU指标是由Papineni等人在2002年提出，最早用于机器翻译模型的自动化评测。BLEU是首个与人工评价具有高相关程度、计算开销低且简单易用的自动化评价指标，被广泛应用于各类与文本生成相关的任务评测中。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 17,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 288,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c18",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "BLEU指标是对文本匹配精确率(Precision Rate)的改进。首先，BLEU在文本匹配统计时引入了不同粒度N元组(n-gram)的匹配，包括对应单个词的一元组(unigram)、对应词组的二元组(bigram)等。同时，BLEU对待评测文本N元组的统计上限进行约束，使其不超过标准文本中对应N元组的最大值。最后，BLEU考虑了长度对评测的影响，引入了过度简短惩罚(Brevity Penalty, BP)。\n\n具体而言，针对待评测的生成文本q，给定对应的、包含M个标准文本的集Rt={rt1,rt2,...,rtM}。首先统计匹配的N元组个数，得到匹配程度pn：\n\npn = min(COUNTn(Q),COUNTn(Rt))\n\n其中，COUNTn(Q)和COUNTn(Rt)是Q和Rt文本中第n个N元组的统计量。\n\n进一步依据生成文本的长度lc和参考文本的有效长度ls，计算惩罚项BP：",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 18,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 398,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c19",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "进一步依据生成文本的长度lc和参考文本的有效长度ls，计算惩罚项BP：\n\nBP = (1,lc>ls)\nBP = (e^(-1*lc/ls),lc<ls)\n\n最终，通过引入几何平均，得到BLEU评测结果：\n\nBLEUn = BP * exp(wn * log(pn))\n\n其中，w为权重值，通常取w=1/n。由于当N大于4时，相应的N元组会非常稀疏，因此通常采用N={1,2,3,4}，对应BLEU{1,2,3,4}指标。\n\n2) METEOR(Metric for Evaluation of Translation with Explicit Ordering)",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 19,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 283,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c20",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "METEOR指标是由Banerjee等人在2005年提出，是对BLEU指标的改进。BLEU指标仅采用了准确率，且要求N元组完整匹配，没有考虑同义词等语义相似的情况。为此，METEOR指标在计算时同时考虑了精确率和召回率(Recall Rate)，并给予召回率更大的权重。METEOR进一步将词干相同、同义词等情况纳入匹配统计，使其与人工评价更为相关。\n\n具体而言，给定待评测的生成文本c以及参考用标准文本r。METEOR首先一元组的匹配，计算相应的精确率和召回率，并采用调和平均进行加权：\n\nP = m/COUNT(c)\nR = m/COUNT(r)",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 20,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 275,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c21",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "P = m/COUNT(c)\nR = m/COUNT(r)\n\n其中，m为待评测文本c和标准文本r最优匹配下的一元组匹配数，COUNT(c)和COUNT(r)分别为待评测文本c和标准文本r中一元组总个数。词干相同、同义词等情况将计入匹配数。如果存在多种一元组匹配的方案，选取匹配连线交叉最少的作为指标计算的依据。\n\n为了对更长的N元组匹配情况进行评估，METEOR引入了惩罚项p，待评测文本和标准文本中不相邻的匹配越多，则惩罚越大。首先要将匹配好的一元组划分到尽可能少的文本块(Chunk)中，文本块越少，则说明待评测文本和参考文本中相邻的一元组匹配越长。如果待评测文本和参考文本完全相同，则相应的惩罚项p为0。\n\n第5部分内容：\n\n文本块数y后，计算惩罚项如下：\n\np = 0.5 * x * (S) * (2 - 25)\n\n最终，得到METEOR指标如下：",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 21,
    "chunk_index_in_section": 21,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 379,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c22",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "p = 0.5 * x * (S) * (2 - 25)\n\n最终，得到METEOR指标如下：\n\nMETEOR = Fmeanx * (l - p) * (2 - 26)\n\n3）CIDEr（Consensus-based Image Description Evaluation）\n\nCIDEr指标是由Vedantam等人于2015年提出，是专门针对图像描述任务设计的自动化评测标准。CIDEr采用TF-IDF加权，即词频-逆文本频率指数（Term Frequency-Inverse Document Frequency），为不同的词赋予不同的权重。直观来看，重复出现在描述句子中的词通常视觉信息量更少，因此在利用标准文本评测生成的文本时，应当赋予这类词更低的权重。由此，在图像描述任务评测中，CIDEr能取得相比BLEU和METEOR与人工评测更为一致的结果。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 22,
    "chunk_index_in_section": 22,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 381,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c23",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "具体而言，给定基于图像生成的文本Ci，以及对应的参考文本集，并且将文本中的词映射到对应的原始词根或词干形式。定义N元组为q，统计其出现在文本和文本Ci中的次数，分别记为和ifc(q)。计算N元组Wfc的TF-IDF权重如下：\n\nIwzen = (s/per n_i (l, hkr pq))j\n\n其中，ft为包含所有N元组的词表，f为数据集中所有图像的集合。随后，基于平均余弦相似度计算N元组对应的CIDEr值如下：\n\n其中，(Ci)和(Cry)为文本和文本%对应的TF-IDF权重向量。通常采用多个不同长度的N元组捕捉不同粒度的信息：\n\nCIDEr = ^AnCIDErnCcf . i? , - )\n\nn = l\n\n默认米用W = 4，加权权重为 = 1/iV。\n\n2.5本章小结",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 23,
    "chunk_index_in_section": 23,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 342,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c24",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "n = l\n\n默认米用W = 4，加权权重为 = 1/iV。\n\n2.5本章小结\n\n本章主要介绍了开展图像段落描述研究所涉及的基础知识。本章以编码器-解码器框架为线索，首先简要介绍了框架的基本范式，进一步介绍了卷积神经网络和目标检测模型两大类视觉编码器，以及循环神经网络和变压器网络两大类文本解码器。最后，本章介绍了图像段落描述基准数据集和相应的指标。\n\n一维在0到1之间，用于控制父节点的信息向左子节点的流动程度。为了鼓励左右子节点显式建模图像特征的不同方面，实现父节点特征的有效划分，左右子节点的计算方式如下：\n\nl = vPQg\n\nlv r = vP0(l^J)",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 24,
    "chunk_index_in_section": 24,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 283,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c25",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "l = vPQg\n\nlv r = vP0(l^J)\n\n其中，⊙代表逐元素乘积。可以看到，当门控向量某一维度的值越接近1，则该维度的信息主要由左子节点获得；值越接近0，则该维度的信息主要由右子节点获得。上述特点使得划分模块能够在监督训练时学习到差异性。同时，满足父节点等于左右子节点的加和，减小了引入噪声的可能性。\n\n实际上，任何由一个特征表示计算得到两个特征表示的方法都可以应用于划分模块，例如直接使用多层感知机或者利用卷积神经网络完成。后续实验发现基于门控的划分机制简单且有效，故S2TD最终采用上述节点划分方式。其他可采用的划分模块设计如下：\n\n1）基于两个独立的门控向量\n\nl = a(Wt⊙LayerNorm(vp)+b)\n\nr = c(Wr⊙LayerNorm(vp)+br)，vPQg\n\n2）基于多层感知机直接计算左右子节点\n\nl = MLPleft(vp)",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 25,
    "chunk_index_in_section": 25,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 385,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c26",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "2）基于多层感知机直接计算左右子节点\n\nl = MLPleft(vp)\n\nr = MLPright(vn⊙vPQg)\n\n3.3.3打分模块\n\n如果模型只包含划分模块，那么树结构将会无休止地扩展下去，无法从图像和段落数据中学习到如何生成合适的、有利于段落生成的句子树结构。因此，需要使模型具备停止节点划分的能力并能够从数据集中学习如何决策。\n\n为此，本文提出利用打分模块对划分结果进行评价，完成对树结构扩展的控制，学习段落句子树的拓扑结构。具体而言，给定一组由划分模块得到的节点划分提议集合{vp,vl,vr}，打分模块基于余弦相似度计算得到决策分数sp：\n\nsp = cos(vl,vr)",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 26,
    "chunk_index_in_section": 26,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 294,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c27",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "sp = cos(vl,vr)\n\n当sp<a时，打分模块将保留当前节点划分结果，否则拒绝划分。其中，决策阈值a为0和1之间的超参数，需要通过验证集来确定。采用余弦相似度进行度量，具有较好的可解释性：决策分数越大，则说明划分得到的左右子节点的向量表示越相似，更有可能导致冗余的描述句子生成，应当拒绝这次划分；决策分数越小，则说明划分的质量越高，应当采用本次划分结果。\n\n为了有效监督打分模块，首先通过3.2节中提出的段落文本树结构建模方法，基于段落P构造出对应的树结构r作为标签。树结构标签的学习，可以转换为对决策分数的学习。为此，本文提出了一种新颖的树结构损失来实现对决策分数的有效监督，具体计算方式如下所示：\n\nLt = {max(0, sp - a), sp>a\n     max(0, a - sp), sp<a",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 27,
    "chunk_index_in_section": 27,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 359,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c28",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "其中，Lc代表所有叶子节点的集合。对于非叶子节点，即打分模块的决策为划分的情况，损失Lt鼓励左右节点的差异越明显越好。对于叶子节点，即打分模块的决策为不划分的情况，损失鼓励左右节点的差异略大于阈值a。\n\n相比直接利用二分类损失，例如二元交叉熵损失，上述树结构损失可以鼓励打分模块学习到更平滑的打分策略，从而实现更好的段落生成效果。另外，训练过程中，模型保持与给定标签相同的划分决策，测试时则无需预先给定标签。\n\n与划分模块相同，打分模块设计亦可采用不同的计算方式和优化目标，例如直接使用多层感知机加上sigmoid激活函数作为打分模块，或者采用其他距离函数。后续实验发现，采用余弦相似度打分机制可解释性更强，简单有效，故S2TD最终采用上述打分设计。其他可采用的打分模块设计如下：\n\n1）基于二分类决策",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 28,
    "chunk_index_in_section": 28,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 351,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c29",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "1）基于二分类决策\n\n打分模块可以基于父节点的表示进行决策，即给定当前父节点，通过一个二分类器决定是否划分该节点，计算公式如下：\n\nsp = ReLU(VV0⊙vp+b0)+bt\n\n如果决策分数小于0.5，即分类结果为0则决策划分该节点，同时说明该节点为非叶子节点。训练时利用二元交叉熵损失训练。二分类器可以进一步基于预划分的子树三元组进行决策，计算公式如下，其中符号[;]为拼接操作：\n\nsp = a(Wx⊙ReLU(VK0⊙[vp;vl;vr])+b0)+bt\n\n2）基于回归拟合节点深度\n\n打分模块也可以通过预测节点的深度实现树结构划分的控制，同样利用一个多层前馈神经网络预测当前父节点vp的深度分数s，计算公式如下：\n\ns(vp) = <t(VKx⊙ReLU(LV0⊙vp+b0)+bx)\n\n一组节点划分提议集{vp,vl,vr}，划分的决策交由以下不等式进行判别：",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 29,
    "chunk_index_in_section": 29,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 387,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c30",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "一组节点划分提议集{vp,vl,vr}，划分的决策交由以下不等式进行判别：\n\ns(vp) + s(vr) < J\n\n即如果满足该条件则接受该划分提议，否则拒绝。其中，决策阈值cr为0和1之间的超参数。a越大，打分模块越倾向于拒绝。该设计的动机是，新增节点的平均分数应当大于已有父节点。训练时，分数的回归目标设计为d/D，其中d为节点到根的跳数，D为预设最大深度。\n\n3.3.4词级别RNN",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 30,
    "chunk_index_in_section": 30,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 194,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c31",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "3.3.4词级别RNN\n\n树结构构造完成后，词级别RNN模块负责将叶节点解码得到描述句子，约定从最左边的叶节点开始解码到最右边的叶节点结束，将依次解码得到的句子组成最后的描述段落。由于利用了划分模块和打分模块，从图像全局特征开始，自顶向下不断对全局信息进行拆解，完成了从图像整体到局部的理解。因此，与主要利用循环神经网络的顺序解码相比，S2TD采用的树结构解码能够更好对图像内容进行理解和规划。值得注意的是，词级别RNN可以对树结构中任意一个节点表示实现解码，故可以通过解码树结构中的不同节点，展示模型生成段落的过",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 31,
    "chunk_index_in_section": 31,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 257,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c32",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "本文介绍了基于文本树结构的图像段落描述算法研究。首先，介绍了相关模型方法，包括ATR模型、SCST模型、CRL模型、OR-ATT模型等。然后，通过定量实验比较了不同模型在非强化学习和强化学习设置下的性能。结果显示，所提出的S2TD方法在多个指标上取得了最优效果。进一步，通过消融实验验证了S2TD模型设计的有效性。最后，进行了定性分析，展示了S2TD模型生成的段落、段落对应的句子树以及划分过程可视化，突出了树结构解码带来的可解释性。\n\n第8部分内容：",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 32,
    "chunk_index_in_section": 32,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 226,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c33",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "第8部分内容：\n\n在本节中，我们首先提出了一个用于构建段落句子树结构的层次建模方法。随后，我们详细介绍了新颖的树结构解码框架S2TD，用于解决图像段落描述任务。该框架将段落生成过程视为一棵自顶向下不断扩展的二叉树。我们进一步介绍了S2TD的三个核心模块：划分模块、打分模块以及词级别RNN，并提出了一个新颖的树结构损失函数，用于监督S2TD学习段落句子树的拓扑结构。最终，我们总结了一个基于S2TD的树结构段落解码生成算法。在实验验证部分，我们通过定量性能比较、消融实验对比验证以及定性生成展示，验证了所提方法的可行性与有效性。\n\n清洗后的内容如下：\n\n一部分，这类方法的编码过程可以抽象如下：\n\nrV,C = Detector(Z)\n\nTij = Softmax(MLP([vf, if IOU(c, Cj) > 0.5] (4-2))\n\nU = Fusion(F, R)",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 33,
    "chunk_index_in_section": 33,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 387,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c34",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "U = Fusion(F, R)\n\n其中，[;]代表向量拼接操作，为图像区域两两预测的关系矩阵，Fusion代表信息融合的操作。IOU为交并比函数(Intersection over Union)，用于计算两个区域框之间的重叠程度，即只有在框重叠比较高时，才进行关系预测。\n\n尽管上述方法在一定程度上提升了编码器对关系信息的利用，但关系预测的准确性依赖于人工预先定义与额外标注数据，缺少或预测错误的关系结果会引入噪声，影响模型的性能表现。\n\n结合上述分析，本章提出采用树结构来建模图像区域特征间的关系，并利用多头自注意力机制融合编码原始视觉特征和对应的图像区域树结构，过程如下：\n\nV,C = Detector(Z)\n\nT = Parser(C) (4-3)\n\nU = MultiHead(F, r)",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 34,
    "chunk_index_in_section": 34,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 350,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c35",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "T = Parser(C) (4-3)\n\nU = MultiHead(F, r)\n\n其中，Parser为本章提出的启发式树结构建模方法，其利用图像区域的坐标信息剖析得到区域树结构T。进一步利用改进后的多头自注意力机制MultiHead，实现区域特征K和树结构T的融合。可以看到，引入树结构的编码方案充分利用了区域坐标信息，在编码区域关系信息的同时，无需额外的标注和训练开销。此外，区域树结构是对人类观察图像时对信息梳理的模拟，相较于直接引入高度抽象的关系类别，更具可解释性。\n\n本章节的后续内容安排如下：\n\n首先介绍如何利用区域坐标信息启发式地构建区域树结构，进一步详细介绍本文提出的树结构增强的段落描述模型，最终通过实验验证所提方法的有效性。\n\n4.2 图像区域树结构建模方法",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 35,
    "chunk_index_in_section": 35,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 339,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c36",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "4.2 图像区域树结构建模方法\n\n一张图像，相比单个句子，想用一个内容丰富的段落进行描述，需要对图像内容进行更深入的观察理解。人类在处理复杂的图像信息时，一种简单有效的规划策略便是对图像内容进行“一分为二”。区别于直接关注到图像中的每一个细节，人类往往将图像先分为“上下”等子区域，逐步地理解子区域中的内容，建立子区域内外的关联。这种“一分为二”的观察过程可以利用二叉树结构进行建模。因此，本文提出在图像段落描述任务中引入区域树结构信息。\n\n具体而言，区域树为一棵完美二叉树(Perfect Binary Tree)，即一个深度为d(d2^0)且有2^(d+1)-1个节点的二叉树。树T中的每一个节点对应一个包含n个图像区域的分组Z。因此，第d层的一共有2^(d-1)个区域分组，记为Z/d = {L0, L1, ..., Ld}，并满足如下条件：\n\nIDA = C (4-4)",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 36,
    "chunk_index_in_section": 36,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 389,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c37",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "IDA = C (4-4)\n\nVGj, Gj E L, Gnj = 0 (4-5)\n\n其中，公式4-4和4-5要求图像中任意区域仅出现在d层的某一个分组中，不同分组间没有交叉。根据定义，区域树的根节点即为未划分的图像区域全集。\n\n由此，区域树的构建过程抽象如下：\n\nC — {c1, c2, ..., cT} = {L0, L1, ..., L}\n\n为了实现上述变换，本章提出了一种基于图像区域的空间坐标，将图像区域集合启发式地划分为“左与右”子部分的区域树结构建模方法。建模步骤详细说明如下：\n\n首先，给定父节点分组C，目标是将其划分为两个新的子节点分组C1和C2。计算父节点分组整体对应的最大区域Cmax如下：\n\nCmax = (x0, y0, x1, y1)\n\n父节点分组区域Cmax给出了“左右”划分的最大范围。进一步基于Cmax计算变化需要的基准框y和d如下：",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 37,
    "chunk_index_in_section": 37,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 385,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c38",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "y = (x0, y0, x1, y1 + αx1(xf - xl), yl)\n\nd = (xl, yl, x1 + 2αx1(xf - xl), yl)\n\n其中，αl为放缩的比例。随后，计算父节点分组中某个图像区域cf属于“上”划分中上方区域的置信度如下：\n\npf = Intersection(cf, c0) / Area(cf)\n\n其中，Intersection函数用于两个坐标框相交面积的大小，Area函数用于计算坐标框的面积。pf的含义为图像区域框cf与基准框y的重合程度，值在0到1之间。如果pf ≥ 0.5则将该框记入新的、代表“上”的分组中，pf < 0.5则记入代表“下”的分组中。至此，完成了一轮放缩比例为αl的“上下”分组划分。\n\n相似的，可以得到区域cf属于“左”划分中左方区域的置信度：\n\npl = Intersection(cf, c0) / Area(cf)",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 38,
    "chunk_index_in_section": 38,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 394,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c39",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "pl = Intersection(cf, c0) / Area(cf)\n\n如果pl ≥ 0.5则将该框记入新的、代表“左”的分组中，pl < 0.5则记入代表“右”的分组中。至此，完成了一轮放缩比例为αl的“左右”分组划分。\n\n采用不同的αl以及不同方向(“上下”、“左右”)可以得到多个不同的新分组，需要对这些新分组进行筛选。为此，本文提出优先选取候选集R中区域数量最为均衡的作为最终结果，即计算分值sy如下：\n\nsy = abs(|GP| - |GP|)",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 39,
    "chunk_index_in_section": 39,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 230,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c40",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "sy = abs(|GP| - |GP|)\n\n其中，abs为绝对值函数，|代表集合元素数量。分值sy越小，则说明对应的新分组更平衡。优先选取5个最小的新分组，如果存在多组分值相同，则选取最接近0.5的新分组。由于最终目标是得到一棵完美二叉树，因此只需要遍历d-1层的父节点，并逐一采用上述的划分方法，即将树结构扩展到d层。实际应用时，采用αl ∈ {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9}与两种方向(“上下”、“左右”)的组合，共计18种情况，原始图像区域总数为K = 36，建模深度为d = 2。\n\n通过引入上述图像区域树结构，能够辅助建模两类区域关系：组内关联，层次关联。组内关联，指位于同一个节点对应的分组下的所有区域是空间相邻或处于一观察逻辑的；层次关联，指父节点对应的分组与子节点对应的分组间具有由整体向局部过度的特点。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 40,
    "chunk_index_in_section": 40,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 390,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c41",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "图4-1和图4-2中，可视化了由所提方法构建出的区域树结构。其中，图像中的红色框为目标检测模型抽取的区域图像特征。\n\n4.3 编码视觉树结构的图像段落描述模型\n\n4.3.1 模型总览\n\n本文将变压器网络架构应用于图像段落描述任务中，并在编码器端引入了区域树结构信息，提出了一种新颖的树结构增强的编码器网络(Tree-Enhanced Encoder, TEE)。此外，本文提出了一种基于概率分布衰减的段落解码策略，延迟终止(Ending Delay)解码策略，用于改善朴素的变压器网络在解决段落描述问题时存在的解码缺陷，使得变压器架构能够获得与最新方法可比较的性能。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 41,
    "chunk_index_in_section": 41,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 283,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c42",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "模型总体框架如图4-3所示，由树结构增强的编码器网络、解码器网络以及基于概率分布衰减的段落解码策略三个重要组件构成。计算流程如下：首先将图像输入到TEE的目标检测模块中，得到区域的原始特征和边界框坐标，并通过TEE的树结构剖析模块得到区域树结构。随后，TEE通过改进的多头自注意力机制编码区域的原始特征和区域树结构，得到最终的视觉特征表示。进一步将视觉特征表示输入到解码器网络中，得到段落中词的概率分布。在推理测试阶段，模型采用由延迟终止和重复性惩罚组成的、基于概率分布衰减的段落解码策略，对词的概率分布进行优化，从而得到最终的描述段落并输出。\n\n4.3.2 树结构增强的编码器网络",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 42,
    "chunk_index_in_section": 42,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 291,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c43",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "4.3.2 树结构增强的编码器网络\n\n树结构增强的编码器网络TEE是对朴素的变压器编码组件的改进。TEE由三个模块组成：目标检测器模块、区域树剖析模块以及树结构增强的注意力编码模块(Tree-Enhanced Attention Module, TEAM)。区域树剖析模块基于4.2节提出的图像区域树结构建模方法实现，利用目标检测的结果构建区域树结构。TEAM在朴素的多头自注意力机制的基础上，融合了剖析得到的区域树结构信息，使得编码器TEE能够更好的利用区域间关系，从而得到更好的视觉表示。\n\n首先回顾朴素的变压器编码组件，简记为AM(Attention Module)。输入的原始图像区域特征为K，其前向过程抽象如下：\n\nvself = MultiHead(K, V, V) (4-13)\n\nKself = ReLU(VselfW0 + b0)Wx + bx (4-14)",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 43,
    "chunk_index_in_section": 43,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 388,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c44",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "Kself = ReLU(VselfW0 + b0)Wx + bx (4-14)\n\n其中，每一层操作额外采用了残差连接并采用了LayerNorm方法进行一化，即x = LayerNorm(x + SubLayer0c)。MultiHead函数即为公式2-11定义的多头自注意力操作。实际使用时会层叠多个参数独立的编码组件。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 44,
    "chunk_index_in_section": 44,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 161,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c45",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "可以看到，AM采用的多头自注意力操作是全连接的，即输入特征可以与所有的特征进行注意力计算。全连接的注意力计算在机器翻译任务中，即用于编码源语言的文本，是合理的。这是因为文本是符号化的信号，其蕴含着规律性的结构，即语法结构。通过监督学习，全连接的注意力计算可以通过文本语义隐式的获取结构。然而，图像与文本不同，图像中物体对象的空间关系是随机的，并不遵循类似于文本语法的约束。因此，在解决图像描述任务，尤其是更为复杂的图像段落描述任务时，需要引入额外的区域信息关系进行指导或约束。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 45,
    "chunk_index_in_section": 45,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 238,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c46",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "为此，本文提出在AM的基础上引入区域树结构信息，对注意力计算过程进行约束。回顾4.2节中对区域树结构的定义，区域树r = 为一棵完美二叉树，其中Ld = 以及定义为图像区域的一个分组集合。即T的每层Ld将所有的图像区域划分为了2^d个子集合，每个集合内的区域是空间相关。基于上述特点，通过将朴素的全连接方式改进为只允许对子集中的区域进行注意力计算，区域树结构可以被用于约束多层注意力机制。由此，得到了TEAM的基础架构，展示在图4-4中，剖析得到的区域树结构将被逐层输入到编码组件中对多头自注意力进行约束。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 46,
    "chunk_index_in_section": 46,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 253,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c47",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "需要说明的是，图4-4中展示的只是一种引入区域树结构的策略。在图4-4中，区域树结构的第d层被优先输入到编码组件TEAM中，最终输入根节点。这种从局部到整体的树结构引入方式，定义为自底向上的约束策略(Bottom-up)。同理，可以优先输入根节点，最终输入第d层节点，这种从整体到局部的树结构引入方式，定义为自顶向下的约束策略(Top-down)。此外，树结构的同一层节点可以重复使用，对连续的多层独立参数的TEAM组件进行约束。\n\n得到由区域树结构提供的分组信息Ld后，需要将利用集合表示的Ld转换成矩阵表示，方便后续计算。定义分组矩阵Md为仅包含0和1的方阵。依据Ld给出的区域特征分组，如果区域特征i和区域特征y属于同一个节点(即同一个分组)，则矩阵Md对应位置的=1，反之则=0。\n\n由此，输入的原始区域特征F，对应组件TEAM的前向过程抽象如下：",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 47,
    "chunk_index_in_section": 47,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 378,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c48",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "由此，输入的原始区域特征F，对应组件TEAM的前向过程抽象如下：\n\nvself = EnhancedMultiHead(K, V, V, Md) (4-15)\n\nKself = ReLU(VselfW0 + b0)Wx + bx (4-16)\n\n与组件AM一致，每一层均采用了x = LayerNorm(x + SubLayer0c)操作。其中，EnhancedMultiHead函数在传统多头自注意力机制的基础上额外引入了编码分组信息的矩阵Md。本文提出并尝试了下述的几种改进方案：\n\n硬掩码方案通过硬性约束注意力对象，不允许非同组的特征进行注意力计算。具体而言，考虑公式2-1中定义的注意力操作，其中未归一化的注意力矩阵与分组矩阵Md的维度大小相同，矩阵下标的物理含义分别对应特征i与特征j的注意力强度以及是否处于同组。因此，改进后的计算公式如下：",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 48,
    "chunk_index_in_section": 48,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 376,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c49",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "AttentionH((Q,K,V,Md)) = Softmax + (1 - Mo)x C - 17)\n\n其中，-∞代表负无穷。公式4-17的含义为将不在同一分组的注意力分值置为负无穷，归一化后即为0，从而对关注区域的约束。多头自注意力的计算沿用公式2-11，将对应的注意力计算从公式2-10替换为4-17即可。\n\n软加权方案通过加权朴素的多头自注意力机制和硬掩码方案，实现对区域分组信息的引入。结合公式2-10和4-17，具体计算过程如下：\n\nAttention5 = ax AttentionH + (1 - a)x SG(A) (4 - 18)",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 49,
    "chunk_index_in_section": 49,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 276,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c50",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "其中，a e (0,1)为预设的超参数常量，用于平衡两类注意的计算结果，如果a = 0则本方案退化为朴素的多头自注意力机制，若a = 1则本方案等价于硬掩码方案。SG函数表示在反向传播时取消梯度反传，实验中发现只保留硬掩码单方向的梯度有助于提高性能表现。\n\n中心偏移方案基于相同分组内注意力分值应当更高的假设，首先计算不同分组的键值向量均值作为键值中心，然后计算键值向量与各组中心的相似度分值。理论上，同组内的分值会更高。最终将该分值与原始的注意力权重相加，从而约束注意力计算更倾向于同组的区域。相应的计算过程如下所示：\n\nKc = -x MdK (4 - 19)\n\nAttentionc(Q,K,V,Md) = Softmax ( -x V2x Kc (4 - 20)\n\n综上，本节介绍了树结构增强的编码组件TEAM，并阐明了引入区域树结构的编码器网络TEE的各个组件以及区域树结构的引入策略。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 50,
    "chunk_index_in_section": 50,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 398,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c51",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "4.3.3基于概率分布衰减的段落解码策略\n\n在所提模型的解码端，本文沿用变压器网络的解码器组件进行段落生成，相关计算过程见2.3.2节。尽管理论上，相较于循环神经网络，变压器网络能够更好地对文本序列上下文进行理解并生成。然而实验发现，在图像段落描述任务中，由于变压器网络的解码组件仍属于非层次结构的模型，无法显式区分和利用段落中句子层次的信息。因此，受到Melaskyriazi等人[461研究工作的启发，需要在解码推理阶段，采用额外的后处理，才能使模型更好地解码生成描述段落。进一步实验发现，已有的段落解码后处理策略都是针对以循环神经网络为组件的解码器网络设计的，同样的方法直接应用于变压器网络反而会导致性能下降。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 51,
    "chunk_index_in_section": 51,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 308,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c52",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "为此，本文提出了一套基于概率分布衰减的段落解码策略用于增强变压器解码组件在段落生成时的性能表现。顾名思义，概率分布衰减的策略是指，在解码网络输出的词概率分布的基础上，有策略地调整部分词概率分布的强度，使解码过程更加充分，从而生成质量更高的描述段落。\n\n具体而言，所提策略包含两个主要步骤，延迟终止和重复惩罚。在解码生成的每个时间步，首先对输出词分布进行延迟终止的调整，再对潜在的重复内容生成进行罚分，得到优化后的词分布进行采样生成，最终组成输出段落。相关技术细节说明如下：\n\n1)延迟终止(EndingDelay)",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 52,
    "chunk_index_in_section": 52,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 257,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c53",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "1)延迟终止(EndingDelay)\n\n针对变压器网络的最新研究工作[79]指出，变压器网络存在序列长度过拟合的问题，即模型的性能表现受到训练集序列长度分布的影响。在本文展开的实验中，观察到了类似的过拟合问题，由于训练集段落长度方差较大加上非层次结构无法显式表征句子，变压器解码组件会过早结束段落生成，导致生成的段落长度显著不足，从而极大地影响段落内容的丰富程度，降低评测指标。近期的图像单[80]也开始对相关问题进行研究，使生成的描述句子长度可控。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 53,
    "chunk_index_in_section": 53,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 226,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c54",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "受到上述研究的启发，本文提出在解码阶段，对模型输出的词概率分布进行调整，通过渐进式的罚分衰减，鼓励模型生成段落。设表示段落终止的特殊词为“＜eos＞”以及表示句子终止的句号为“。”。需要提前设定期望生成的句子数Nv以及惩罚因子5e(0,1]。在段落解码过程中的第t时刻，基于句号对已生成的句子个数进行统计，记为Nt。对“＜eos＞”的概率输出调整如下：\n\nptos = logptos + {Nv - Nt}x log5e (4 - 21)\n\n其中，pteos为调整前的概率值，pteos为进行延迟终止更新后的概率值。5e越小则越倾向于鼓励模型继续生成段落，直到“＜eos＞”被采样生成。公式4-21会随着段落中已生成的句子数量逐渐增多，逐步降低对词“＜eos＞”的概率输出的影响直到满足预设的期望句子生成个数，实现平滑的转换。\n\n2)重复性惩罚(RepetitionPenalty)",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 54,
    "chunk_index_in_section": 54,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 392,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c55",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "2)重复性惩罚(RepetitionPenalty)\n\n重复性惩罚是由Melaskyriazi等人[46]提出的，用于降低非层次结构模型段落生成的冗余度的解码策略。该策略通过在解码过程中统计已生成的段落中的三元组，降低可能导致重复三元组的词的概率，从而增加段落文本内容的多样性。然而该方法是基于循环神经网络解码器设计的，实验发现，该方法不能直接应用于变压器解码组件。进一步研究后，发现变压器解码组件需要先利用所提出的延迟终止策略进行解码增强，随后重复性惩罚策略才能被有效地实施。\n\n重复性惩罚策略对词w概率值的调整如下所示：\n\nlogptw = logptw - Xa^ (4 - 22)\n\n其中，^代表t时刻，以词w结尾的、已生成的三元组个数。a^ [0, +∞)为惩罚强度，a^越大则对重复的惩罚越大。\n综上所述，本节介绍了本文提出的由延迟终止和重复惩罚组成的解码策略。该策略被用于提升变压器网络中的解码组件在段落生成上的性能表现。\n\n4.4实验对比与分析\n\n4.4.1实验参数与设置",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 55,
    "chunk_index_in_section": 55,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 444,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c56",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "针对本章提出的模型方法和解码策略，为了验证其有效性，本文采用开源深度学习框架Pytorch开发和训练模型，并在基准数据集上进行了定性和定量实验。实验环境为搭载12GB NVIDIA GeForce 1080Ti显卡的Ubuntu 16.04服务器。编码器网络中的目标检测器组件同样采用预训练的Faster R-CNN[21]检测并保留目标检测结果前36个框，每个框抽取的特征为2048维。在训练和测试阶段，参数保持固定。随后，利用多层感知机网络将特征投影=512维。区域树剖析模块遵循4.2节，采用Ae{0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9}与两种方向(“上下”、“左右”)，最终得到的区域树共计3层。模型中采用变压器网络实现遵循Rush[81]开源的变压器网络基准实现和超参数数配置。模型采用了6层树结构增强的编码网络TEE和6层变压器解码网络的配置",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 56,
    "chunk_index_in_section": 56,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 393,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c57",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "。模型采用了6层树结构增强的编码网络TEE和6层变压器解码网络的配置。依据不同TEAM组件设计，剖析得到的区域树结构将采用不同的方式引入组件中，将在模型对比部分详细说明。词表采用与3.4.1节中相似的流程处理，但不区分句子而是将段落作为一个长序列，段落的预设生成最长长度为175个词，其中包含标点符号。在监督训练阶段，模型采用Adam优化器数据批量大小为10的配置。并采用如下的公式[27]对学习率Zr进行调整：",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 57,
    "chunk_index_in_section": 57,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 206,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c58",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "lr = d^-5 x min(iNtep, Nstep) x N^-5up) (4 - 23)\n\n其中，Ntep为当前训练步数，MvarmuP = 20000为预热步数。此外，遵循变压器网络训练的惯例，采用强度为0.1的标签平滑策略[82] (Label Smoothing)进行训练，避免多元交叉熵损失导致过于自信的预测，从而提升模型的泛化性能。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 58,
    "chunk_index_in_section": 58,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 175,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c59",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "在测试推理阶段，模型采用的解码策略的超参数设置为：期望生成的最少句子数Nv=7，惩罚因子5e=0.2以及重复性惩罚强度a^=2.1n2。上述超参数是基于模型在验证集上的性能表现选择的。因受限于实验环境的算力，模型没有采用SCST等强化学习技术[83]进行进一步优化。这是由于在每个时间步的强化学习进行的采样，均需要重新计算一次对历史信息的多头注意力机制，因此需要存储的梯度信息远多于循环神经网络，过大的显存和时间开销使得利用SCST技术优化变压器结构的模型极为困难。\n\n4.4.2评测指标对比与分析",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 59,
    "chunk_index_in_section": 59,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 249,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c60",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "4.4.2评测指标对比与分析\n\n本节选取了近年的图像段落描述模型进行定量的性能对比，并将其进一步划分为建模了未建模区域关系组rnnp+rp，DHVP，IMAP[44]以及建模区域关系组VRD[41]，CAVP，OR-ATT，HSGED[45]。为保持一致，通过采用已1^1；3，4p，METEOR，CIDER (C)指标来评价模型的段落生成质量。\n\n对未在3.4.2节介绍的相关模型补充说明如下：\n\n1)RNN+RP模型：该模型基于图像单句描述模型[21]，解码器组件为循环神经网络，通过监督学习训练模型，解码时采用重复性惩罚策略进行优化，属于最简单的基线方法之一。\n\n2)VRD模型：该模型基于经典的段落描述模型RH。其在目标检测模型结果的基础上，对区域关系进行显式的预测，并将有效的区域关系对输入到循环神经网络解码组件中进行段落生成。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 60,
    "chunk_index_in_section": 60,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 369,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c61",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "3)VRREN模型：该模型是对VRD的改进，利用提出的关系对预测模块替换目标检测模型，该模块基于对抗生成网络实现，同样进行显式的关系标签预测。\n\n4)CAVP模型：该模型由多个感知上下文的视觉策略组件构成，通过将已完成的注意力结果作为上下文信息，隐式的完成对图像区域关系的捕捉和利用。\n\n5)HSGED模型：该模型在层次结构模型RH的基础上引入了场景图，需要预先进行场景图的预测和生成学习。该模型显式建模并引入了图像区域关系信息。同时，该模型采用了复杂的训练和采样手段提升性能指标，是当前最先进的方法。\n\n6)Transformer模型：该模型是直接采用目标检测模型和变压器网络实现的图像段落描述模型，未引入本章提出的编码器和解码策略的改进，同样属于最简单的基线方法之一。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 61,
    "chunk_index_in_section": 61,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 336,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c62",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "7)Ours w/o Tree：该模型是基于Transformer的实现，未引入区域树结构和树结构增强的编码器组件的改进，只引入了本文提出的基于概率分布衰减的解码策略，包括终止延迟和重复性惩罚，属于基线方法。\n\n8)Ours：即本章提出的引入视觉树结构的完整模型，包括对编码器组件和解码策略的改进。树结构增强的编码器中的TEAM组件采用了硬掩码策略，即公式4-17。区域树结构引入的策略是前3层TEAM组件采用自底向上的约束策略，后3层采用自顶向下的约束策略。\n\n表4-1展示了相关模型方法的性能结果。由于大部分研究工作模型设计复杂且缺少有效的开源，表中结果沿用论文提供的指标。需要说明的是，为了公平比较，表中尽可能汇报相关模型进行监督学习后的性能表现。其中，RTT-GAN，-VAE和HSGED模型采用了额外的方法进行优化。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 62,
    "chunk_index_in_section": 62,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 363,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c63",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "从表4-1中可以看到，引入区域信息分组内的模型性能与同期的模型相比，都具有性能上的优势：例如早期的方法VRD和VRREN都相较于基线RH模型有提升，近期的方法例如CAVP和HSGED相较于IMAP模型有优势。相关实验结果表明，引入图像区域关系等信息是有助于提升段落生成质量。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 63,
    "chunk_index_in_section": 63,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 137,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c64",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "本文提出的引入视觉树结构信息的方法取得了良好的性能表现，指标结果普遍优于基线模型，并与最先进的方法HSGED可比较，在评测指标上各有优劣。此外，同样是非层次结构模型的RNN+RP和Transformer基线模型，在性能指标上差异不明显，并且在监督训练设置下性能大幅度低于层次结构模型。然而，在采用本文提出的基于概率分布衰减的段落解码策略进行优化后，变压器网络的性能得到了极大的提升，优于利用了重复性惩罚策略的最新OR-ATT+RP模型的性能，甚至好于大部分层次结构模型。这验证了本文提出的延迟终止策略的有效性，即能够有效的增强变压器网络在段落生成任务上的表现。在进一步引入树结构后，所提模型在BLEU和CIDER指标上均得到了提升。其中，在CIDER指标上提升明显，与未引入树结构的基线模型相比提升了约4.6%。相关结果验证了本文提出的树结构增强的编码器TEE组件的可行性和有效性。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 64,
    "chunk_index_in_section": 64,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 391,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c65",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "4.4.3消融实验\n\n为进一步验证本章提出的引入视觉树结构的段落描述模型的有效性，本节从区域树结构剖析方法的设计、区域树结构信息的引入机制以及所提段落解码策略三个方面进行了定量实验和分析对比。\n\n针对区域树结构剖析方法的对比结果展示在表4-2中，对比方法说明如下：\n\n1)w/o Tree为未引入任何树结构信息的基线模型。\n\n2)w/BBBox为在w/o Tree的基础上，将区域边界框信息(坐标，长宽，面积)作为新增维度直接拼接在对应的区域特征上作为输入，交由多头自注意力机制自行完成关系隐式计算。与其他方法相比，该模型引入了额外的参数量。\n\n3)Rand Tree为采用了随机平均划分剖析策略得到的基线模型。具体而言，\n\n清洗后的内容如下：",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 65,
    "chunk_index_in_section": 65,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 322,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c66",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "清洗后的内容如下：\n\n剖析流程与4.2节提出的剖析方法相似，但划分不同分组时不依赖于区域边界框坐标进行划分，而是将当前父节点分组中的区域随机划分成两个大小相等的集合。除此之外，其他配置保持一致。\n\n4) HACTree采用的是自底向上构建视觉树结构的策略，核心流程是计算区域间的距离，然后利用层次聚类方法(Hierarchical Agglomerative Clustering, HAC)完成树结构的构建。区域间距离计算基于图像区域特征表示间的余弦相似度和空间坐标交并比的加权值。在树结构自底向上构建完成后，进行剪枝只保留3层的树结构并转换为对应的矩阵表示，最终引入编码器中。其他配置保持一致。\n\n5) RegionTree即利用4.2节提出的构建方法得到的区域树结构。值得一提的是，编码器端采用的TEAM策略为硬掩码，该方案没有引入额外的参数量。\n\n-2采用区域树结构剖析方法的消融实验结果",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 66,
    "chunk_index_in_section": 66,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 398,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c67",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "-2采用区域树结构剖析方法的消融实验结果\n\nw/o Tree 44.48 27.04 16.52 9.96 17.68 26.55  \nw/BBBox 44.31 27.04 16.58 10.00 17.45 26.76  \nRandTree 42.53 26.16 16.10 9.77 16.95 27.10  \nHACTree 43.28 26.05 15.70 9.36 17.12 25.57  \nRegionTree 44.64 27.50 16.88 10.25 17.65 27.78",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 67,
    "chunk_index_in_section": 67,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 251,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c68",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "从表4-2中可以看到，采用RegionTree的模型取得了最佳的性能表现。对比RandTree和w/o Tree，发现应用随机剖析得到的树结构会损害模型的表现，这说明RegionTree中采用的区域划分策略是有价值的，远好于随机划分。对比HACTree和RandTree，可以看到尽管HACTree采用了语义和空间关系混合打分的方法进行建模，但是其表现甚至差于随机划分。一方面是因为HACTree采用的层次聚类方法会导致剖析得到的树结构不平衡，即不同分组内区域数量差异。另一方面是因为HACTree基于的图像语义距离是基于目标检测模型原始特征得到的，该特征是粗粒的，容易将不同位置相似的物体聚类在一起，引入额外的噪声。而仅基于空间信息构建的RegionTree能够为模型补充额外的信息而非干扰",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 68,
    "chunk_index_in_section": 68,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 346,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c69",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "。而仅基于空间信息构建的RegionTree能够为模型补充额外的信息而非干扰。最后，对比不同的引入区域关系的方案，隐式方案w/BBBox对性能的提升并不明显，RegionTree更好的性能表现验证了引入区域树结构的有效性。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 69,
    "chunk_index_in_section": 69,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 111,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c70",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "进一步考察不同的树结构约束方案和TEAM组件设计对模型性能的影响。相关结果展示在表4-3中。本节实验了三种TEAM设计，硬掩码方案(HM)、软加权方案(SW)以及中心偏移方案(CS)，以及四种引入树结构约束方案：\n\n1) 自顶向下(TD)：区域树结构引入顺序为从根节点层到叶节点层，每1层分组用2层独立的TEAM组件编码。\n\n2) 自底向上(BU)：区域树结构引入顺序为从叶节点层到根节点层，每1层分组用2层独立的TEAM组件编码。\n\n3) 自顶向下再自底向上(TDBU)：区域树结构引入顺序为从根节点层到叶节点层，叶节点层再到根节点层。每1层分组用1层独立的TEAM组件编码。\n\n4) 自底向上再自顶向下(BUTD)：区域树结构引入顺序为从叶节点层到根节点层，根节点层再到叶节点层。每1层分组用1层独立的TEAM组件编码。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 70,
    "chunk_index_in_section": 70,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 362,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c71",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "从表4-3中可以看到，不同的TEAM组件设计方案需要采用合适的树结构约束方案，才能取得比基线w/o Tree更好的性能表现。例如，HM+BU TD，SW+TD以及CS+BU的组合。从HM、SW到CS，相关设计对注意力机制的约束程度是从严格约束逐步放松，到完全交由模型自行学习。\n\n可以发现，HW和SW采用BU TD和TD的效果普遍较好，原因是最后阶段的引入是从整体到局部，能够增强最终输入到解码器组件中视觉表示间的区分度，并且能让图像边缘非核心对象的特征获得增强。而TDBU和BU的约束策略效果普遍较差，是因为最后一层采用根节点层对应的全连接，等价于放弃了约束。不同的是，学习型的CS的设计，采用BU策略更好，这是因为CS基于键值的中心表示对注意力进行调整，故先从叶节点层开始编码有助于获取到更好的中心。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 71,
    "chunk_index_in_section": 71,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 352,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c72",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "总的来说，结合表4-2和表4-3的实验结果，剖析有效的区域树结构并采用合适的方式引入，将有助于提升模型在图像段落描述任务上的性能表现。\n\n表4-4采用不同解码策略的消融实验结果\n\nTransformer 35.27 21.00 12.81 7.83 14.78 22.40  \n+RP 34.70 21.00 12.82 7.75 14.93 25.32  \n+ED 41.21 24.32 14.77 9.03 17.00 22.64  \n+ED +RP 44.48 27.04 16.52 9.96 17.68 26.55",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 72,
    "chunk_index_in_section": 72,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 263,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c73",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "针对所提的基于概率衰减的段落解码策略，本节在基线模型Transformer上进行了实验对比验证。延迟终止策略简记为ED，重复性惩罚策略简记为RP。相关结果展示在表4-4中。可以看到，直接在变压器解码时采用RP并没有同循环神经网络解码那样对性能指标有一致性的提升，甚至会导致部分指标降低。只引入ED策略时，可以观察到一致性的性能提升，在BLEU和METEOR指标上的提升尤为显著。同时引入ED和RP策略后，基线模型取得了最好的生成结果，RP策略也正常发挥了作用，提升了段落生成的多样性。\n\n综上，通过区域树结构的剖析、引入方式和解码策略三个方面的消融实验，进一步验证了所提的引入视觉树结构的图像段落描述模型的有效性。\n\n4.4.4定性分析\n4.4.4定性分析\n\n本节通过展示区域树结构剖析结果以及不同模型生成的段落，进行定性分析和对比。相关结果展示在图4-5中，每张输入图像对应一个剖析得到区域树结构、三个由不同模型生成的段落文本。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 73,
    "chunk_index_in_section": 73,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 415,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c74",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "可以看到，引入区域树结构后，模型能够捕捉图像中更多的细节信息，从而生成质量更好的段落描述。以图4一幅图的生成结果为例，w/o Tree模型仅完成了图像中大象和湖的描述，而在引入区域树后，模型能进一步提及处于图像背景中停留在湖边的飞鸟(“There are small birds sitting on the ground near the small body of water.”)。这得益于区域树结构对于相关子区域的划分，即区域树第二层首个节点，实现了对该区域单独的分组。同样的，在图4-5的第二幅图中，通过引入区域树结构，模型进一步提及了道路中的黄线(“A road has yellow lines.”)和背景处的白色房屋(“There is a white building in the distance.”)。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 74,
    "chunk_index_in_section": 74,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 363,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c75",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "此外，对图像区域边界框的划分是符合人类认知习惯的。以图4-5中第二幅图的区域树结构为例，区域集合首先被拆分为以上方区域为主的背景以及下方区域为主的前景物体。然后上方的背景被进一步划分，右节点主要涉及房子。下方的的前景也被进一步划分为摩托车以及道路。在更为复杂的第三幅图中，区域树结构剖析的表现稍差，但在首次划分时对不同显示器的划分也是比较准确的。剖析质量下滑的主要原因是目标检测器输出的多个边界框过大，互相重叠严重。尽管如此，通过观察生成的段落，当前的树结构剖析质量是可以接受的。\n\n最后，通过对比基线模型Transformer和w/o Tree生成的段落，可以看到所",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 75,
    "chunk_index_in_section": 75,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 284,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c76",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "最后，通过对比基线模型Transformer和w/o Tree生成的段落，可以看到所\n\n提出的延迟终止和重复惩罚的段落解码组合策略能有效提升段落生成质量。段落不仅在句子生成个数上得到了提升，也提及了更多图像中出现的对象。总的来说，通过在编码端引入区域树结构信息，模型能够有效提升描述段落的准确性和丰富程度。生成段落中仍存在局部不连贯和内容冗余的问题，这是由于本节提出的方法仍属于非层次结构模型，需要对解码组件进一步改进。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 76,
    "chunk_index_in_section": 76,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 210,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c77",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "本章开展了引入视觉树结构的图像段落描述算法的研究工作。本章首先提出一种用于构建图像区域树结构的建模方法。随后，基于变压器网络框架，本章详细介绍了所提出的树结构增强的编码器网络，以及基于概率分布衰减的段落解码策略。最终，得到了一种新颖的编码视觉树结构的图像段落描述模型。在实验部分，本章通过定量分析实验、消融实验对比以及定性展示剖析结构和生成段落，验证了引入视觉树结构的可行性与有效性。\n\n清洗后的内容：\n\n参考文献：",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 77,
    "chunk_index_in_section": 77,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 208,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c78",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "清洗后的内容：\n\n参考文献：\n\n[25] Beng, I.O., Vinyals, O., Jaitly, N., et al. Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks [C]. Advances in Neural Information Processing Systems, 2015: 1171-1179.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 78,
    "chunk_index_in_section": 78,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 211,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c79",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "[26] Ren, S.J., Marcheret, E., Mroueh, Y., et al. Self-critical Sequence Training for Image Captioning [A]. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017: 7008-7024.\n\n[27] Vaswani, A., Shazeer, N., Parmar, N., et al. Attention is All You Need [A]. Proceedings of the 31st International Conference on Neural Information Processing Systems, 2017: 6000-6010.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 79,
    "chunk_index_in_section": 79,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 393,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c80",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "[28] Herdade, S., Kappler, A., Boakye, K., et al. Image Captioning: Transforming Objects into Words [A]. Proceedings of the 33rd International Conference on Neural Information Processing Systems, 2019: 11137-11147.\n\n[29] Huang, L., Wang, W., Chen, J., et al. Attention on Attention for Image Captioning [A]. Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019: 4634-4643.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 80,
    "chunk_index_in_section": 80,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 397,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c81",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "[30] Correia, M., Stefanini, M., Baraldi, L., et al. Meshed-memory Transformer for Image Captioning [A]. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020: 10578-10587.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 81,
    "chunk_index_in_section": 81,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 206,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c82",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "[31] Zhou, L., Palangi, H., Zhang, L., et al. Unified Vision-language Pre-training for Image Captioning and VQA [A]. Proceedings of the AAAI Conference on Artificial Intelligence, 2020: 13041-13049.\n\n[32] Deng, C., Ding, N., Tan, M., et al. Length-controllable Image Captioning [A]. European Conference on Computer Vision, 2020: 712-729.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 82,
    "chunk_index_in_section": 82,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 337,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c83",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "[33] Johnson, J., Karpathy, A., Fei-Fei, L. DenseCap: Fully Convolutional Localization Networks for Dense Captioning [A]. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016: 4565-4574.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 83,
    "chunk_index_in_section": 83,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 217,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c84",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "[34] Mao, Y., Zhou, C., Wang, X., et al. Show and Tell More: Topic-oriented Multi-sentence Image Captioning [A]. Proceedings of the Hiie 27th International Joint Conference on Artificial Intelligence, 2018: 4258-4264.\n\n[35] Liang, X., Hu, Z., Zhang, H., et al. Current Topic-Transition GAN for Visual Paragraph Generation [A]. International Conference on Computer Vision, 2017: 3382-3391.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 84,
    "chunk_index_in_section": 84,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 388,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c85",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "[36] Chattopadhyay, M., Schwingshackl, A. G. Diverse and Coherent Paragraph Generation from Images [A]. European Conference on Computer Vision, 2018: 747-763.\n\n[37] Wang, J., Pan, Y., Yao, T., et al. Convolutional Auto-encoding of Sentence Topics for Image Paragraph Generation [A]. International Joint Conference on Artificial Intelligence, 2019: 940-946.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 85,
    "chunk_index_in_section": 85,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 356,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c86",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "[38] Wu, S., Zhang, Z., Wang, Z., et al. Denseley Supervised Hierarchical Policy-value Network for Image Paragraph Generation [A]. Proceedings of the 28th International Joint Conference on Artificial Intelligence, 2019: 975-981.\n\n[39] Li Ruifan, Liang Haoyu, Feng Fangxiang, et al. 全卷积神经结构的段落式图像描述算法 [J]. 北京邮电大学学报, 2019(6): 7.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 86,
    "chunk_index_in_section": 86,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 326,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c87",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "[40] Li, R., Liang, H., Shi, Y., et al. Dual-CNN: A Convolutional Language Decoder for Paragraph Image Captioning [J]. Neurocomputing, 2020, 396: 92-101.\n\n[41] Che, W., Fan, X., Xiong, R., et al. Paragraph Generation Network with Visual Relationship Detection [A]. ACM Multimedia [C], 2018: 1435-1443.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 87,
    "chunk_index_in_section": 87,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 301,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c88",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "[42] Che, W., Fan, X., Xiong, R., et al. Visual Relationship Embedding Network for Image Paragraph Generation [J]. IEEE Transactions on Multimedia, 2019: 1-1.\n\n[43] Zhang, Z., Liu, D., Zhang, H., et al. Context-aware Visual Policy Network for Fine-grained Image Captioning [J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2019: 1-1.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 88,
    "chunk_index_in_section": 88,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 352,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c89",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "[44] Xu, C., Li, Y., Li, C., et al. Interactive Key-Value Memory-augmented Attention for Image Paragraph Captioning [A]. Proceedings of the 28th International Conference on Computational Linguistics, 2020: 3132-3142.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 89,
    "chunk_index_in_section": 89,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 216,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c90",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "[45] Yang, X., Gao, C., Zhang, H., et al. Hierarchical Scene Graph Encoder-Decoder for Image Paragraph Captioning [A]. Proceedings of the 28th ACM International Conference on Multimedia, 2020: 4181-4189.\n\n[46] Melas-Kyriazi, L., Rush, A. M., Han, G., et al. Training for Diversity in Image Paragraph Captioning [A]. Empirical Methods in Natural Language Processing [C], 2018: 1-1.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 90,
    "chunk_index_in_section": 90,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 380,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c91",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "[47] Wang, Z., Luo, Y., Li, Y., et al. Look Deeper See Richer: Depth-aware Image Paragraph Captioning [A]. ACM Multimedia [C], 2018: 672-680.\n\n[48] Luo, Y., Huang, Z., Zhang, Z., et al. Curiosity-driven Reinforcement Learning for Diverse Visual Paragraph Generation [A]. ACM Multimedia [C], 2019: 2341-2350.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 91,
    "chunk_index_in_section": 91,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 307,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c92",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "[49] Yang, L., Cai, Y., Yang, C., Hsu, J. Object Relation Attention for Image Paragraph Captioning [A]. Proceedings of the AAAI Conference on Artificial Intelligence, 2021, 35(4): 3136-3144.\n\n[50] Hong, R., Liu, D., Mo, X., et al. Learning to Compose and Reason with Language Tree Structures for Visual Grounding [J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2019: 1-1.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 92,
    "chunk_index_in_section": 92,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 392,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c93",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "[51] Liu, D., Zhang, H., Wu, F., et al. Learning to Assemble Neural Module Tree Networks for Visual Grounding [A]. Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019: 4673-4682.\n\n[52] Cao, Q., Long, X., Li, B., et al. Visual Question Reasoning on General Dependency Tree [A]. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018: 7249-7257.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 93,
    "chunk_index_in_section": 93,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 398,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c94",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "[53] Tang, K., Zhang, H., Wu, B., et al. Learning to Compose Dynamic Tree Structures for Visual Contexts [A]. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019: 6619-6628.\n\n[54] Yao, T., Pan, Y., Li, Y., et al. Hierarchy Parsing for Image Captioning [A]. Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019: 2621-2629.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 94,
    "chunk_index_in_section": 94,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 382,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c95",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "[55] Ma, Z., Yuan, C., Cheng, Y., et al. Image-to-tree: A Tree-structured Decoder for Image Captioning [A]. 2019 IEEE International Conference on Multimedia and Expo [C], 2019: 1294-1299.\n\n[56] Wang, H., Lin, G., Hoi, S. C., et al. Structure-aware Generation Network for Recipe Generation from Images [A]. European Conference on Computer Vision [C], 2020: 1-1.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 95,
    "chunk_index_in_section": 95,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 360,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c96",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "[57] LeCun, Y., Boser, B., Denker, J. S., et al. Backpropagation Applied to Handwritten Zip Code Recognition [J]. Neural Computation, 1989, 1(4): 541-551.\n\n[58] Deng, J., Dong, W., Socher, R., et al. ImageNet: A Large-scale Hierarchical Image Database [A]. IEEE Conference on Computer Vision and Pattern Recognition [C], 2009: 248-255.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 96,
    "chunk_index_in_section": 96,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 335,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c97",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "[59] Krizhevsky, A., Sutskever, I., Hinton, G. E. ImageNet Classification with Deep Convolutional Neural Networks [A]. Advances in Neural Information Processing Systems [C], 2012: 84-90.\n\n[60] Simonyan, K., Zisserman, A. Very Deep Convolutional Networks for Large-scale Image Recognition [J]. arXiv preprint arXiv:1409.1556, 2014.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 97,
    "chunk_index_in_section": 97,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 330,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c98",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "[61] Szegedy, C., Liu, W., Jia, Y., et al. Going Deeper with Convolutions [A]. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition [C], 2015: 1-9.\n\n[62] He, K., Zhang, X., Ren, S., et al. Deep Residual Learning for Image Recognition [A]. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition [C], 2016: 770-778.\n\n参考文献：",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 98,
    "chunk_index_in_section": 98,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 368,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c99",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "参考文献：\n\n[63] Hu J, Shen L, Sun G. Squeeze-and-excitation Networks [A]. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition [C], 2018: 7132-7141.\n\n[64] Xie S, Girshick R, Dollar P, et al. Aggregated Residual Transformations for Deep Neural Networks [A]. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition [C], 2017: 1492-1500.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 99,
    "chunk_index_in_section": 99,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 377,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c100",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "[65] Tan M, Le Q. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks [A]. International Conference on Machine Learning [C], 2019: 6105-6114.\n\n[66] Goodfellow I, Bengio Y, Courville A. Deep Learning [M]. press, 2016.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 100,
    "chunk_index_in_section": 100,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 237,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c101",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "[67] Redmon J, Divvala S, Girshick R, et al. You Only Look Once: Unified, Real-time Object Detection [A]. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition [C], 2016: 779-788.\n\n[68] He K, Gkioxari G, Dollar P, et al. Mask R-CNN [A]. Proceedings of the IEEE International Conference on Computer Vision [C], 2017: 2961-2969.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 101,
    "chunk_index_in_section": 101,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 350,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c102",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "[69] Krishna R, Zhu Y, Groth O, et al. Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations [J]. International Journal of Computer Vision, 2017, 123(1): 64-73.\n\n[70] Bengio Y, Ducharme R, Vincent P, et al. A Neural Probabilistic Language Model [J]. Journal of Machine Learning Research, 2003, 3: 1137-1155.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 102,
    "chunk_index_in_section": 102,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 342,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c103",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "[71] Hochreiter S, Schmidhuber J. Long Short-term Memory [J]. Neural Computation, 1997, 9(8): 1735-1780.\n\n[72] Ba JL, Kiros JR, Hinton GE. Layer Normalization [J]. arXiv preprint arXiv:1607.06450, 2016.\n\n[73] Lin TY, Maire M, Belongie S, et al. Microsoft COCO: Common Objects in Context [A]. European Conference on Computer Vision [C], 2014: 740-755.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 103,
    "chunk_index_in_section": 103,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 350,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c104",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "[74] Banerjee S, Lavie A. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments [A]. Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization [C], 2005: 65-72.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 104,
    "chunk_index_in_section": 104,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 265,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c105",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "[75] Devlin J, Chang M, Lee K, et al. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [J]. arXiv preprint arXiv:1810.04805, 2018.\n\n[76] Reimers N, Gurevych I, Reimers N, et al. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks [A]. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing [C], 2019: 671-688.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 105,
    "chunk_index_in_section": 105,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 384,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c106",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "[77] Srivastava R, Greff K, Schmidhuber J. Training Very Deep Networks [A]. Proceedings of the 28th International Conference on Neural Information Processing Systems [C], 2015: 2377-2385.\n\n[78] Kingma DP, Ba J. Adam: A Method for Stochastic Optimization [J]. arXiv preprint arXiv:1412.6980, 2014.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 106,
    "chunk_index_in_section": 106,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 296,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c107",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "[79] Van den Bergh D, Bojar O. Sequence Length is a Domain: Length-based Overfitting in Transformer Models [A]. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing [C], 2021: 8246-8257.\n\n[80] Deng C, Ding N, Tan M, et al. Length-controllable Image Captioning [A]. European Conference on Computer Vision [C], 2020: 712-729.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 107,
    "chunk_index_in_section": 107,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 357,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于树结构的图像段落描述研究_石祎晖_s0_c108",
    "source_id": "基于树结构的图像段落描述研究_石祎晖",
    "text": "[81] Rush A. The Annotated Transformer [A]. Proceedings of Workshop for NLP Open Source Software [C], 2018: 52-60.\n\n[82] Szegedy C, Vanhoucke V, Ioffe S, et al. Rethinking the Inception Architecture for Computer Vision [A]. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition [C], 2016: 2818-2826.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 108,
    "chunk_index_in_section": 108,
    "total_chunks_in_section": 109,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 323,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9450000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于深度卷积结构的图像段落描述研究_梁昊雨_s0_c0",
    "source_id": "基于深度卷积结构的图像段落描述研究_梁昊雨",
    "text": "基于深度卷积结构的图像段落描述研究\n\n摘要：\n段落式图像描述任务旨在为给定图像生成描述性的自然语言段落，连接着计算机视觉和自然语言处理两个关键领域，是跨媒体智能的重要研究方向，其研究进展对于打破图像和文本间的语义鸿沟至关重要。近年来，随着深度学习的发展，基于层次性RNN的解码器已被广泛采用于段落式图像描述任务上。然而，RNN结构上的限制使得这类方法存在如下问题。首先，由于捕获长时信息的能力有限，RNN生成段落这类长文本存在困难，生成的段落连贯性不足。此外，RNN的串行结构导致其训练时间复杂度较高，效率低下。受启发于卷积神经网络(CNN)的特点，本文展开以下工作。提出了基于全卷积神经结构的段落解码器。将门控结构融入层次性的CNN解码器中，该解码器具有更强的长时记忆能力，并拥有并行化训练的能力。提出了一种衡量段落连贯性的指标。经在斯坦福像-段落数据集上进行评估指标、连贯性指标、时间复杂度以及主观分析，证明所提解码器提升了生成段落的质质量。提出了融合区域注意力的段落式图像描述模型Dual-CNN，增强了图像理解能力，提升了段落内句子描述的详细度和多样度。提出了一种衡量段落内句子多样度的指标。经评估指标、多样性指标、区域注意力分析、主观分析，Dual-CNN显著提升了段落式图像描述任务性能。\n\n关键词：深度学习，卷积神经网络，段落式图像描述，连贯性",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 33,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 579,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于深度卷积结构的图像段落描述研究_梁昊雨_s0_c1",
    "source_id": "基于深度卷积结构的图像段落描述研究_梁昊雨",
    "text": "关键词：深度学习，卷积神经网络，段落式图像描述，连贯性\n\n信息容量限制，单句式图像描述着重于生成概括性的整体描述，其在视觉侧仅需较粗颗粒度的检测和识别。而段落式图像描述需较完整地对图像信息进行理解，以便能够生成详细的段落，实现图像和段落的较高度语义对应。在语言推理侧，段落式图像描述不仅要将图像内容进行细颗粒度的表达，更要考虑语言层面上的逻辑性和上下文相关性，以生成较为连贯的段落。区别于单句描述仅需保证一句话的流畅性，段落描述更需建模段落内句子间的逻辑性。以下分别介绍单句式图像描述方法和段落式图像描述的研宂进展，并提出目前段落式图像描述方法仍存在的问题。\n\n1.2.1 单句式图像描述",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 294,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于深度卷积结构的图像段落描述研究_梁昊雨_s0_c2",
    "source_id": "基于深度卷积结构的图像段落描述研究_梁昊雨",
    "text": "1.2.1 单句式图像描述\n\n在2015年之前，单句式图像描述方法大多为基于模板的方法（Template-based）和基于检索的方法（Retrieval-based）。基于模板的方法首先检测出图像中的视觉概念，并使用预定义的句子模板和这些概念结合，得到输出句子。基于检索的方法把图像描述任务转换为检索问题，通过将待描述图像和文本映射到同一语义空间上，试图为待描述的图像检索寻找语义最接近的句子，作为输出句子。随着大数据时代的到来和深度学习方法的兴起，数据驱动的深度神经网络方法逐渐成为求解图像描述问题的主流方法。受端到端的编码器-解码器（Encoder-Decoder）结构在机器翻译任务上成功应用的启发，Vinyals等人[6]在2015年介绍了建立在编码器-解码器架构上的单句式图像描述模型NIC（Neural Image Captioning），其中CNN作为编码器首先将输入图像表示为视觉向量，随后基于RNN的解码器将视觉向量解码为自然语言文本。由于该模型的端到端训练特性以及其表现出的明显性能优势，之后的图像描述研究大多根据CNN+RNN进行扩展和改进。\n\n1.2.2 段落式图像描述方法",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 500,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于深度卷积结构的图像段落描述研究_梁昊雨_s0_c3",
    "source_id": "基于深度卷积结构的图像段落描述研究_梁昊雨",
    "text": "1.2.2 段落式图像描述方法\n\nKrause等人在2017年公布了斯坦福图像-段落数据集（Stanford-Paragraph Dataset），数据集中的每个样本包含一幅图像和一段描述该图像的段落。基于该数据集，并根据自然语言中段落、句子及词语的层次性，他们提出一种段落式图像描述模型Hierarchical-RNN，如图1-3所示。该解码器由句子RNN和词RNN组成，句子RNN建模段落内句子间逻辑关系，并生成句子主题，词RNN建模句内关系并根据句子主题生成句子内的所有单词。在图像理解侧，他们釆用了RPN网络结合VGG网络[31]作为图像编码器检测图像中的区域并生成区域特征，然后将所有区域特征做最大池化操作得到全局图像特征。然而，解码器仅以全局特征作为输入，造成了图像信息的损失，生成的段落丢失了图像中的许多信息，解码器输入特征的单一也造成段落内大量重复句子的出现。并且，Hierarchical-RNN对句间逻辑性的监督也较弱，生成的段落连贯性有待提高。\n\n1.2.3 研究现状的总结与分析",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 451,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于深度卷积结构的图像段落描述研究_梁昊雨_s0_c4",
    "source_id": "基于深度卷积结构的图像段落描述研究_梁昊雨",
    "text": "1.2.3 研究现状的总结与分析\n\n前文介绍了解决单句式和段落式图像描述任务的一些方法。简言之，单句式图像描述己在CNN+RNN的编码器-解码器框架下取得了瞩目的阶段性进展，生成句子有较好的准确性、连贯性。从具体方法上来讲，RNN系列网络以其所独有的时间序列结构和隐藏状态（Hidden State）带来的“记忆能力”，被广泛采用为图像描述解码器的基本结构。而段落式图像描述的研究仍处于起步状态，生成段落在准确性、详细程度、连贯性仍有欠缺。段落式图像描述方法也延伸于RNN及其变体网络长短时记忆网络（LSTM，-short Term Memory）、门控循环单元（GRU，Gated Recurrent Units）段落解码器的框架之下，无法避免RNN的若干固有问题。其一，RNN无法记忆长时信息，这限制了其建模语言的性能。尽管LSTM被提出用来捕捉长时信息，但当建模诸如段落的长文本时，随着时间序列的向后推移，隐藏信息的衰减极大地削弱了解码器关注的视野大小，由此带来的长时记忆能力不足同样造成生成段落连贯性的下降。其二，RNN的时间序列特性使得这些段落解码器运行所需的时间复杂度较高、时间消耗大，也无法使用GPU对其进行并行化加速，这给训练和推理过程带来了极大的时间消耗。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 537,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于深度卷积结构的图像段落描述研究_梁昊雨_s0_c5",
    "source_id": "基于深度卷积结构的图像段落描述研究_梁昊雨",
    "text": "综合本节研宄现状分析，当前段落式图像描述研宄存在以下问题亟待解决：(1)在语言生成侧，增强生成段落的连贯性。(2)在图像理解侧，减少图像信息的损失。(3)对于模型而言，降低模型和算法的时间复杂度。本文针对以上若干问题，开展对段落式图像描述的研宄。\n\n1.3 本文的研究工作\n\n针对问题(1)和(3)，本文第一个研究内容为：基于全卷积神经结构的段落式图像描述方法。第一个研究内容的成果是提出了适合于段落式图像描述任务的全卷积解码器。全卷积解码器由双层的门控卷积神经网络构成，其中的门控结构赋予CNN记忆能力。该解码器包括句子CNN解码器和词CNN解码器，句子CNN解码器捕捉段落内的句子之间关系以加强句子间的连贯性，词CNN解码器负责生成段落内的单词。通过将层次结构、卷积结构、门控结构结合，相对于RNN解码器，全卷积解码器具有更大的“视野”，拥有更强的长时记忆能力。我们提出了一种衡量段落连贯性的指标，并在斯坦福图像-段落数据集上经过评测指标、运行时间、段落连贯性指标、主观评价等多种评价方法的验证，证明了所提方法的有效性。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 5,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 462,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于深度卷积结构的图像段落描述研究_梁昊雨_s0_c6",
    "source_id": "基于深度卷积结构的图像段落描述研究_梁昊雨",
    "text": "针对问题(2)，本文第二个研究内容为：融合区域注意力的段落式图像描述模型。第二个研究内容的成果是通过区域提议网络和区域注意力模块结合，实现对图像的高度语义理解，并将区域注意力模块融合到全卷积解码器中，提出了一种新模型Dual-CNN。区域注意力模块加强了图像和段落的语义对齐，生成段落描述更加详细。通过评测指标、段落多样度指标、区域注意力分析、主观评价等多种评价方法的验证，Dual-CNN提升了段落式图像描述任务上的性能。\n\n1.4 本文的组织结构\n\n本论文总共包含五章，每章的重点内容归纳如下，结构示意图如图1-5。\n\n第一章，绪论。首先介绍了段落式图像描述的相关研究背景及意义，并对研究现状进行综述，对目前段落式图像描述方法存在的问题进行了分析，最后概述了本文主要的研宄工作和成果。\n\n第二章，基础知识。介绍了本文涉及到的基础知识。首先介绍了几种深度学习基本模型，接下来介绍图像描述任务的基础编码器-解码器结构和注意力机制，最后概述了段落式图像描述数据集以及评价指标。\n\n第三章，基于全卷积神经结构的段落式图像描述方法。详细介绍了所提出的全卷积解码器，并提出了一种评价段落连贯性的指标。实验表明，全卷积解码器的训练时间复杂度小于传统方法，所生成的段落具有更好的连贯性。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 6,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 537,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于深度卷积结构的图像段落描述研究_梁昊雨_s0_c7",
    "source_id": "基于深度卷积结构的图像段落描述研究_梁昊雨",
    "text": "第四章，融合区域注意力的段落式图像描述模型。为提高图像理解能力，将区域提议网络、区域注意力机制融合到全卷积段落解码器中，提出了Dual-CNN。\n\n第五章，总结。对全文的主要研究内容进行了总结，并对未来研究方向进行了展望。\n\n第二章 基础知识\n\n2.1 深度神经网络模型\n\n2.1.1 卷积神经网络\n\n卷积神经网络(CNN)是一种特殊的前馈神经网络(Feedforward Neural Network, FNN)，因其在计算中使用了数学上的卷积操作而得名。CNN广泛应用于图像分类、图像分割、人脸识别等领域，是当今深度学习应用的前沿。典型的CNN通常由卷积层、池化层和激活层构成。卷积层通过卷积核在图像上以一定的步长滑动实施卷积操作，且卷积层的权重共享减少了计算量。池化层可降低数据的维度，使用广泛的池化方法有最大池化(Maximum Pooling)和平均池化(Mean Pooling)。激活层使用非线性激活函数增强网络的非线性特性，使用广泛的非线性激活函数包括Sigmoid函数、Tanh函数和线性整流单元(ReLU)。\n\n2.1.2 循环神经网络",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 7,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 478,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于深度卷积结构的图像段落描述研究_梁昊雨_s0_c8",
    "source_id": "基于深度卷积结构的图像段落描述研究_梁昊雨",
    "text": "2.1.2 循环神经网络\n\n循环神经网络(RNN)代表了一系列带有信息自传递功能的神经网络家族，能够对时间序列类结构建模，例如文本、语音、信号等。RNN通过隐藏信息的传递进行信息记忆，但其难以捕捉长时信息，且训练存在梯度消失和爆炸问题。在此基础上提出了包含长时记忆单元的LSTM，LSTM能在一定程度上解决梯度消失和爆炸问题，并拥有一定的长时记忆能力。RNN的成功应用有编码器-解码器结构等。\n\n2.2 编码器-解码器结构\n\n2.2.1 概述\n\n近几年，无论是单句式图像描述研究，还是段落式图像描述研究，都围绕着端到端的编码器-解码器结构展开。该思路受启发于机器翻译任务的最新模型。这些模型利用RNN处理可变长度的待翻译文本，将其编码为固定维度的向量，再将该向量用RNN解码为翻译文本输出。而图像描述任务，可理解为将输入图像翻译为文本。因此在给定图像的情况下，很自然地使用类似的方法，利用CNN将图像编码为特征向量，再将该向量解码为文本。NIC是较开创性、有代表性的单句式图像描述模型，接下来根据该模型来介绍图像描述模型的基本框架。\n\n2.2.2 模型详述\n\n2.2.3 单词采样方法\n\n2.3 注意力机制\n\n2.4 数据集及评价指标\n\n2.4.1 段落式图像描述任务数据集",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 8,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 537,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于深度卷积结构的图像段落描述研究_梁昊雨_s0_c9",
    "source_id": "基于深度卷积结构的图像段落描述研究_梁昊雨",
    "text": "2.2.2 模型详述\n\n2.2.3 单词采样方法\n\n2.3 注意力机制\n\n2.4 数据集及评价指标\n\n2.4.1 段落式图像描述任务数据集\n\n本文介绍了段落式图像描述模型的基础知识，包括CNN、RNN和LSTM，以及基于CNN和RNN的图像描述基本结构。文章还介绍了注意力机制在图像描述任务中的实现细节，以及段落式图像描述的相关数据集和评价指标。\n\n在第三章中，文章提出了一种全卷积神经结构的CNN段落解码器，并详细描述了该解码器的结构。与传统的基于RNN的解码器相比，全卷积解码器具有更大的长时视野和记忆能力，更适合生成长文本。文章通过对比实验验证了所提模型的有效性。\n\n总体而言，本文对段落式图像描述模型的基础知识和关键技术进行了全面介绍，并提出了新的全卷积解码器结构，为该领域的研究提供了有益的参考。\n\n清洗后的内容如下：\n\n---\n\n道数量，输出通道数量为2*F，因此输出的向量维度为1X2F，该向量可视作两个维度为五的向量h和f的拼接，用公式表示如下：\n\nhf = Wa * lt + ba (3-6)\n\nt = Wb * lt + bb (3-7)\n\n其中，Wa、Wb为可学习的卷积核权重参数，ba、bb为可学习的卷积核偏置参数，符号*表示一维卷积操作符。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 9,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 534,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于深度卷积结构的图像段落描述研究_梁昊雨_s0_c10",
    "source_id": "基于深度卷积结构的图像段落描述研究_梁昊雨",
    "text": "t = Wb * lt + bb (3-7)\n\n其中，Wa、Wb为可学习的卷积核权重参数，ba、bb为可学习的卷积核偏置参数，符号*表示一维卷积操作符。\n\n此时值得注意的有两点。第一点，为确保卷积层只关注上文信息，需采用掩码机制(Mask Mechanism)，使卷积核中与下文信息进行计算的后半部分权重置为零。第二点，当卷积层深度为1时，卷积核的大小应不小于句子长度，以确保t时刻的视野大小等于t-I。但当I较大时，可采用堆叠多层卷积层的做法，通过卷积层的深度和卷积核的大小综合控制感受野的大小。\n\n接下来，在卷积层之后执行门控操作，得到门控卷积层的输出向量0t\n\not = σc(h*t) (3-8)\n\n其中，符号c表示Sigmoid激活函数，即σ(a) = 1/(1+e^-a)，符号⊙表示按位乘法操作符。向量if蕴含上文信息lt中的语义信息，起门控的作用，有选择地记住hf中的信息。\n\n然后，预测层将ot映射为词表上的概率分布wp\n\nwp = softmax(Mp * ot) (3-9)\n\n其中，Mp为可学习的全连接层权重参数。\n\n最后，根据wp在词表上进行采样，可选择的采样方式有最大概率采样和束搜索两种方法，得到t时刻的单词输出。当生成的单词为句子结束符时，停止生成，将所有生成的单词组合得到解码生成的句子。\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 10,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 566,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于深度卷积结构的图像段落描述研究_梁昊雨_s0_c11",
    "source_id": "基于深度卷积结构的图像段落描述研究_梁昊雨",
    "text": "---\n\n以上内容已去除页眉、页脚、页码、重复信息和乱码，保留了核心学术内容、技术术语和数据、原有的逻辑结构、公式和图表说明。\n\n本节介绍了实验的硬件和软件环境。硬件环境为GeForce GTX 1080Ti显卡。软件环境为采用Python编程语言的开源框架PyTorch。PyTorch是由Facebook公司开发的开源机器学习库，其计算可使用GPU进行加速，且带有自动微分系统，因此广受深度学习研究者青睐。一些参数设置如下。区域检测器所检测的区域个数L设定为50。区域特征向量初始维度U为4096，经全连接层压缩后的区域特征维度D为1024。词嵌入向量的维度E同样设定为1024。通过对比实验，采用集束搜索方法生成段落。其中束的大小设置为2。段落中最大句子数目设定为6，每句话的最大单词数设定为30。通过对卷积核大小的实验，句子CNN解码器中采用一层卷积，即深度为1，卷积核大小为6；词CNN解码器中采用7层卷积，即深度为7，每层的卷积核大小为5。损失函数中的两个系数；1-和4分别设置为5.0和1.0。整个模型使用Adam优化算法进行训练，学习率参数设置为1e-4。实验中依据算法在验证集上的表现确定超参数。\n\n表 3-3 平均Antecedent、平均Anaphora和平均Anaphora率对比",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 11,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 552,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于深度卷积结构的图像段落描述研究_梁昊雨_s0_c12",
    "source_id": "基于深度卷积结构的图像段落描述研究_梁昊雨",
    "text": "表 3-3 平均Antecedent、平均Anaphora和平均Anaphora率对比\n\n| 模型 | 平均Antecedent | 平均Anaphora | 平均Anaphora率(%) |\n| --- | --- | --- | --- |\n| Hierarchical-RNN | 0.89 | 0.09 | 6.3 |\n| 全卷积解码器 | 0.92 | 0.13 | 8.4 |\n| 人类 | 0.90 | 0.18 | 11.4 |\n\n最后在整个数据集的段落上求平均，得到两种模型在数据集上的平均Antecedent、平均Anaphora和平均Anaphora率，同时计算了数据集的标签段落的这三个统计量，作为人类表现。结果如表 3-3所示。\n\n由表 3-3可看出，在人类和两种机器模型的平均Antecedent相差不大的前提下，人类的平均Anaphora和平均Anaphora率都要大于两种机器模型，这说明平均Anaphora和平均Anaphora率能够一定程度上反映出段落的连贯性。\n\n全卷积解码器的平均Anaphora比Hierarchical-RNN高出44.44%，平均Anaphora率高出33.33%，这在一定程度上可说明该解码器生成的段落具有更强的连贯性。\n\n3.3.5 时间复杂度对比与分析",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 12,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 560,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于深度卷积结构的图像段落描述研究_梁昊雨_s0_c13",
    "source_id": "基于深度卷积结构的图像段落描述研究_梁昊雨",
    "text": "3.3.5 时间复杂度对比与分析\n\n本章所提出的全卷积解码器的一大优势是它的两个组成部分：句子CNN解码器和词CNN解码器都能够被并行训练。相比之下，传统的层次-RNN解码器Hierarchical-RNN逐个生成段落中的单词，无法并行化。本小节首先分析训练这两种段落解码器的时间复杂度，再通过实际实验的训练时间对比来证实我们复杂度的分析。\n\n虽然不同段落是非等长的，但为分析方便起见，假设段落由m个句子组成，每个句子有n个单词，特征维度均为d。对Hierarchical-RNN而言，句子RNN和词RNN每层的复杂度分别为O(md^2)和O(mn^2d^2)。对全卷积段落解码器，假设句子CNN解码器和词CNN解码器的卷积核大小均为fc，r是词CNN中卷积层的个数。则句子CNN和词CNN的每层复杂度分别为O(fc^2md^2)和O(fc^2mnd^2)。传统的RNN解码器一次只能生成一个单词或句子的主题，因此句子RNN和词RNN分别需要O(m)和O(mn)的顺序操作。与此相比，全卷积段落解码器中，句子CNN和词CNN分别只需O(1)和O(r)的顺序操作。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 13,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 481,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于深度卷积结构的图像段落描述研究_梁昊雨_s0_c14",
    "source_id": "基于深度卷积结构的图像段落描述研究_梁昊雨",
    "text": "结合每层的复杂度和顺序操作复杂度，句子RNN的总时间复杂度为O(m^2d^2)，词RNN的总时间复杂度为O(m^2n^2d^2，那么Hierarchical-RNN的总时间复杂度也为O(m^2n^2d^2)。句子CNN的总时间复杂度为O(fc^2md^2)，词CNN的总时间复杂度为O(fc^2rmd^2)，那么全卷积段落解码器的总时间复杂度为O(fc^2rmd^2)。由于fc*r<m*n，因此与Hierarchical-RNN相比，我们提出的全卷积段落解码器具有更低的训练时间复杂度。\n\n为进一步验证我们模型的训练效率，首先对比两种模型训练一个轮次所分别花费的时间，然后对比两种模型的收敛时间。表 3-4展示了两种模型训练一个轮次花费的时间。所提模型比Hierarchical-RNN需要更少的轮次训练时间，仅为Hierarchical-RNN所需时间的34.20%。并且，所提模型具有更大规模的参数，为Hierarchical-RNN的4.14倍，这意味着全卷积段落解码器具有更强的学习能力。所提模型和Hierarchical-RNN每单位参数需要的训练时间分别为25.57s和2.00s。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 14,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 498,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于深度卷积结构的图像段落描述研究_梁昊雨_s0_c15",
    "source_id": "基于深度卷积结构的图像段落描述研究_梁昊雨",
    "text": "图3-8记录了两个模型的CIDEr得分与训练时间的关系，并以CIDEr值的收敛作为训练达到收敛的标准。由图可看出，所提模型训练收敛大约需要800秒，而Hierarchical-RNN大约需要3600秒。\n\n综上可以得出结论，相较于传统方法，所提出的全卷积段落解码器在具有更大参数的同时，训练的理论时间复杂度和实际时间消耗都更少。\n\n3.3.6 生成段落定性分析\n\n我们随机挑选测试集中的若干张图片，并将标签段落和两种模型生成的段落进行对比，如图3-9所示。\n\n表 3-4 参数规模和单Epoch训练时间对比\n\n| 模型 | 参数量(M) | 训练时间(s) | 每单位参数训练时间(s/M) |\n| --- | --- | --- | --- |\n| Hierarchical-RNN | 7 | 179 | 25.57 |\n| 全卷积解码器 | 29 | 58 | 2.00 |\n\n图3-8 CIDEr值随训练时间和轮次的变化\n\n3.3.7 卷积层参数探究\n\n卷积层的深度、卷积核大小的设置决定了CNN的感受野，对全卷积段落解码器的性能至关重要。本小节探宄词CNN中卷积层的深度和卷积核大小对指标得分的影响。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 15,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 505,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于深度卷积结构的图像段落描述研究_梁昊雨_s0_c16",
    "source_id": "基于深度卷积结构的图像段落描述研究_梁昊雨",
    "text": "卷积层的深度、卷积核大小的设置决定了CNN的感受野，对全卷积段落解码器的性能至关重要。本小节探宄词CNN中卷积层的深度和卷积核大小对指标得分的影响。\n\n当卷积层深度为1时，卷积核大小取5,10,15,20,25,30,35。由于句子的最大单词数为30，因此当卷积核大小大于30时，解码器的视野大小才可覆盖整个句子。在卷积核为15时，CIDEr值达到最高，之后随着卷积核的增加，CIDEr得分下降。卷积层深度过小，卷积核大小过大造成性能的下降。总体上，卷积层深度为1时，模型效果较为一般。当卷积层深度为3时，卷积核大小取7时CIDEr值最高，和深度取1时类似，此时解码器视野大小仍不足以覆盖整个句子。\n\n当卷积层深度为5、7、9时，模型分别在卷积核大小取7、5、3时达到最佳效果，此时解码器视野大小均在30左右。这说明当取适中的卷积层深度时，通过调整卷积核大小使得解码器视野覆盖整个句子是较合适的选择。\n\n当卷积层深度为1、3、5、7、9时，模型的最高CIDEr得分如表3-5所示。当深度取7，卷积核的大小取5时，CNN的性能最好。\n\n表3-5 CIDEr随卷积层深度变化的最佳值\n\n卷积层深度  CIDEr值\n1  14.8\n3  15.5\n5  15.7\n7  15.9\n9  15.7",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 16,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 544,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于深度卷积结构的图像段落描述研究_梁昊雨_s0_c17",
    "source_id": "基于深度卷积结构的图像段落描述研究_梁昊雨",
    "text": "表3-5 CIDEr随卷积层深度变化的最佳值\n\n卷积层深度  CIDEr值\n1  14.8\n3  15.5\n5  15.7\n7  15.9\n9  15.7\n\n对于图像描述任务，束大小的设置对实验结果有重要的影响，本小节探究不同的束大小设置对全卷积段落解码器的评测指标得分影响。表3-6展示了束大小为1、2、3和4时，生成段落的评测结果。可以看出，当束大小为1时，由于单词采样的搜索空间过小，丢失了大量解码信息，错过了许多较优解，因此生成的段落质量不佳，指标得分较低。当束大小增加到2时，单词的搜索空间增大，更容易获得较优解，指标得分最高。而当束大小继续增大到3和4时，搜索空间的一个段落内的句子之间相似度快速增加，段落的质量快速下降，导致评测得分降低。并且，过大的束大小会导致解码的时间复杂度显著上升。综上分析，束大小取2是平衡训练效率和段落质量的较好选择。\n\n表3-6 评价指标随束大小变化\n\n束大小  CIDEr  BLEU-1  BLEU-2  BLEU-3  BLEU-4\n1  14.8  40.9  23.1  13.6  7.7\n2  15.9  41.3  23.9  14.1  8.2\n3  15.1  41.5  23.7  14.0  7.8\n4  13.7  40.4  22.3  12.8  7.5",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 17,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 564,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于深度卷积结构的图像段落描述研究_梁昊雨_s0_c18",
    "source_id": "基于深度卷积结构的图像段落描述研究_梁昊雨",
    "text": "本章详细介绍了针对段落式图像描述任务的全卷积段落解码器。首先通过CNN解码器和RNN解码器的对比，阐明了本文使用卷积结构的动机。接下来介绍了全卷积解码器的两个组成部分：句子CNN、词CNN的详细实现，以及该模型的训练与推理过程，最后总结出了一个使用全卷积解码器生成段落的段落生成算法。\n\n在实验验证部分，通过评测指标得分、连贯性指标得分、时间复杂度以及主观评价四个方面综合证明了全卷积段落解码器的优势，该解码器在四项上的表现都超越了基线方法。在实验部分的最后，选取了具有代表性的两种超参数：卷积层参数和束大小，通过对比实验确定了参数的设置。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 18,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 270,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于深度卷积结构的图像段落描述研究_梁昊雨_s0_c19",
    "source_id": "基于深度卷积结构的图像段落描述研究_梁昊雨",
    "text": "在本小节，我们提出了种衡量段落内句子间多样度的指标，来验证区域注意力机制的作用。一个段落，其多样度定义为每两个句子对之间的加权BLEU-n分数。使用BLEU的原因是，相对于CIDEr，BLEU-n直接计算n元语法的共现度，当两句话之间的BLEU得分高时，说明这两句话之间的n元词组重复度高，更直观反映出两句话的重复度较高。一个段落有w句话，则句子对数共有m(m-l)/2对。对于一个句子对，其多样度计算方式如下：Sd = Σl-1 ΣrA BLEU-n(rA)。其中，表示BLEU-n值，为对应的权重。权重满足以下基本条件：yn ≥ 0 (n=1,...,l) 且 Σn=1 yn = 1。此外，我们启发式地增加以下两个式子来进一步确定权重：V1 = V4, V2 = 2A。其中，V1=0.15, V2=0.20, V3=0.30, V4=0.35。对段落内所有句子对的Sd值做简单平均，得到该段落的句子多样度。在实验中，我们记录了在第20个至第40个轮次之间，由Dual-CNN和Hierarchical-RNN在测试集上生成的段落的平均多样度，由图4-4所示。经过40个轮次的训练后，Dual-CNN和Hierarchical-RNN分别取得了93.7和90.6的分数。与Hierarchical-RNN相比，Dual-CNN与人类表现之间的差异减少了7.38%\n。与Hierarchical-RNN相比，Dual-CNN与人类表现之间的差异减少了7.38%。因此，我们得出的结论是，我们的模型能在段落中生成更多样的句子，表现也更接近人类水平。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 19,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 675,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于深度卷积结构的图像段落描述研究_梁昊雨_s0_c20",
    "source_id": "基于深度卷积结构的图像段落描述研究_梁昊雨",
    "text": "第10部分内容：\n\n本章将区域提议网络、区域注意力机制和全卷积段落解码器结合，提出了Dual-CNN模型。我们详细介绍了Dual-CNN的结构，主要包括区域提议网络、融合区域注意力的句子CNN。在实验验证部分，通过评测指标得分、多样度指标得分、区域注意力效果分析以及主观评价四个方面综合证明了Dual-CNN的优势。\n\n清洗后的内容如下：",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 20,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 169,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于深度卷积结构的图像段落描述研究_梁昊雨_s0_c21",
    "source_id": "基于深度卷积结构的图像段落描述研究_梁昊雨",
    "text": "Linguistics. Association for Computational Linguistics, 2012: 747-756. [19] Yan Y, Teo CL, Daumé III H, et al. Corpus-guided sentence generation of natural images [C]//Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2011: 444-454. [20] Farhad I, Hejrati M, Sadeghi MA, et al. Every picture tells a story: Generating sentences from images [C]//European conference on computer vision. Springer, Berlin, Heidelberg, 2010: 15-29. [21] Hodosh M, Young P, Hockenmaier J. Framing image description as a ranking task: Data, model",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 21,
    "chunk_index_in_section": 21,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于深度卷积结构的图像段落描述研究_梁昊雨_s0_c22",
    "source_id": "基于深度卷积结构的图像段落描述研究_梁昊雨",
    "text": "Young P, Hockenmaier J. Framing image description as a ranking task: Data, models and evaluation metrics [J]. Journal of Artificial Intelligence Research, 2013, 47: 853-899. [22] Socher R, Karpathy A, Le QV, et al. Grounded compositionality for finding and describing images with sentences [J]. Transactions of the Association for Computational Linguistics, 2014, 2: 207-218. [23] Gong Y, Wang L, Hodosh M, et al. Improving image-sentence embeddings using large weakly annotated photo collections [C]//European conference on computer vision. Springer, Cham, 2014: 529-545. [24] Ordonez V, Kulikarni G",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 22,
    "chunk_index_in_section": 22,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 600,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于深度卷积结构的图像段落描述研究_梁昊雨_s0_c23",
    "source_id": "基于深度卷积结构的图像段落描述研究_梁昊雨",
    "text": "e on computer vision. Springer, Cham, 2014: 529-545. [24] Ordonez V, Kulikarni G, Berg T, et al. Im2text: Describing images using 1 million captioned photographs [C]//Advances in neural information processing systems. 2011: 1143-1151. [25] Ren S, He K, Girshick R, et al. Faster R-CNN: Towards real-time object detection with region proposal networks [C]//Advances in neural information processing systems. 2015: 91-99. [26] Papineni K, Roukos S, Ward T, et al. BLEU: a method for automatic evaluation of machine translation [C]//Proceedings of the 40th annual meeting on association for computationa",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 23,
    "chunk_index_in_section": 23,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 600,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于深度卷积结构的图像段落描述研究_梁昊雨_s0_c24",
    "source_id": "基于深度卷积结构的图像段落描述研究_梁昊雨",
    "text": "tion [C]//Proceedings of the 40th annual meeting on association for computational linguistics. Association for Computational Linguistics, 2002: 311-318. [27] Denkowski M, Lavie A. Meteor universal: Language-specific translation evaluation for any target language [C]//Proceedings of the ninth workshop on statistical machine translation. 2014: 376-380. [28] Vedantam R, Lawrence Zitnick C, Parikh D. Cider: Consensus-based image description evaluation [C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 4566-4575. [29] Johnson J, Karpathy A, Fei-Fei L. Densecap",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 24,
    "chunk_index_in_section": 24,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 600,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于深度卷积结构的图像段落描述研究_梁昊雨_s0_c25",
    "source_id": "基于深度卷积结构的图像段落描述研究_梁昊雨",
    "text": "rn recognition. 2015: 4566-4575. [29] Johnson J, Karpathy A, Fei-Fei L. Densecap: Fully convolutional localization networks for dense captioning [C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 4565-4574. [30] Kruse J, Johnson J, Krishna R, et al. A hierarchical approach for generating descriptive image paragraphs [C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 317-325. [31] Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition [J]. arXiv preprint arXiv:1409.1556, 2014. [32]",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 25,
    "chunk_index_in_section": 25,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 600,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于深度卷积结构的图像段落描述研究_梁昊雨_s0_c26",
    "source_id": "基于深度卷积结构的图像段落描述研究_梁昊雨",
    "text": "or large-scale image recognition [J]. arXiv preprint arXiv:1409.1556, 2014. [32] Li X, Hu Z, Zhang H, et al. Recurrent topic-transition for visual paragraph generation [C]//Proceedings of the IEEE international conference on computer vision. 2017: 3362-3371. [33] Goodfellow I, Pouget-Abadie J, Mirza M, et al. Generative adversarial nets [C]//Advances in neural information processing systems. 2014: 2672-2680. [34] Chatterjee M, Schwinghammer J. Diverse and coherent paragraph generation from images [C]//Proceedings of the European conference on computer vision (ECCV). 2018: 729-744. [35] Wang Z,",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 26,
    "chunk_index_in_section": 26,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 600,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于深度卷积结构的图像段落描述研究_梁昊雨_s0_c27",
    "source_id": "基于深度卷积结构的图像段落描述研究_梁昊雨",
    "text": "f the European conference on computer vision (ECCV). 2018: 729-744. [35] Wang Z, Liao Y, Li Y, et al. Look deeper: Depth-aware image paragraph captioning [C]//Proceedings of the 26th ACM international conference on multimedia. 2018: 672-680. [36] Che W, Fan X, Xiong R, et al. Paragraph generation network with visual relationship detection [C]//Proceedings of the 26th ACM international conference on multimedia. 2018: 1435-1443. [37] Melas-Kyriazi L, Rush A, Han G. Training for diversity in image paragraph captioning [C]//Proceedings of the 2018 conference on empirical methods in natural languag",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 27,
    "chunk_index_in_section": 27,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 600,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于深度卷积结构的图像段落描述研究_梁昊雨_s0_c28",
    "source_id": "基于深度卷积结构的图像段落描述研究_梁昊雨",
    "text": "[C]//Proceedings of the 2018 conference on empirical methods in natural language processing. 2018: 757-761. [38] Hochreiter S, Schmidhuber J. Long short-term memory [J]. Neural Computation, 1997, 9(8): 1735-1780. [39] Cho K, Van Merrienboer B, Gulcehre C, et al. Learning phrase representations using RNN encoder-decoder for statistical machine translation [J]. arXiv preprint arXiv:1406.1078, 2014. [40] LeCun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition [J]. Proceedings of the IEEE, 1998, 86(11): 2278-2324. [41] Krizhevsky A, Sutskever I, Hinton G. Image",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 28,
    "chunk_index_in_section": 28,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于深度卷积结构的图像段落描述研究_梁昊雨_s0_c29",
    "source_id": "基于深度卷积结构的图像段落描述研究_梁昊雨",
    "text": "e IEEE, 1998, 86(11): 2278-2324. [41] Krizhevsky A, Sutskever I, Hinton G. ImageNet classification with deep convolutional neural networks [C]//Advances in neural information processing systems. 2012: 1097-1105. [42] Szegedy C, Liu W, Jia Y, et al. Going deeper with convolutions [C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 1-9. [43] Glorot X, Bordes A, Bengio Y. Deep sparse rectifier neural networks [C]//Proceedings of the fourteenth international conference on artificial intelligence and statistics. 2011: 315-323. [44] Bahdanau D, Cho K, Bengio Y.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 29,
    "chunk_index_in_section": 29,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 599,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于深度卷积结构的图像段落描述研究_梁昊雨_s0_c30",
    "source_id": "基于深度卷积结构的图像段落描述研究_梁昊雨",
    "text": "l intelligence and statistics. 2011: 315-323. [44] Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and translate [J]. arXiv preprint arXiv:1409.0473, 2014. [45] Long MT, Pham H, Manning CD. Effective approaches to attention-based neural machine translation [J]. arXiv preprint arXiv:1508.04025, 2015. [46] Krishna R, Zhu Y, Groth O, et al. Visual genome: Connecting language and vision using crowd-sourced dense image annotations [J]. International Journal of Computer Vision, 2017, 123(1): 32-73. [47] Chen X, Fang H, Lin TY, et al. Microsoft coco captions: Data",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 30,
    "chunk_index_in_section": 30,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 600,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于深度卷积结构的图像段落描述研究_梁昊雨_s0_c31",
    "source_id": "基于深度卷积结构的图像段落描述研究_梁昊雨",
    "text": "123(1): 32-73. [47] Chen X, Fang H, Lin TY, et al. Microsoft coco captions: Data collection and evaluation server [J]. arXiv preprint arXiv:1504.00325, 2015. [48] Anjaria J, Deshpande A, Schwing A. G. Convolutional image captioning [C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 561-5570. [49] Wang Q, Chan AB. Cnn + cnn: Convolutional decoders for image captioning [J]. arXiv preprint arXiv:1805.00909, 2018. [50] Dauphin YN, Fan A, Auli M, et al. Language modeling with gated convolutional networks [C]//Proceedings of the 34th International Conference on",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 31,
    "chunk_index_in_section": 31,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 600,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于深度卷积结构的图像段落描述研究_梁昊雨_s0_c32",
    "source_id": "基于深度卷积结构的图像段落描述研究_梁昊雨",
    "text": "convolutional networks [C]//Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017: 939-948. [51] Paszke A, Gross S, Chintala S, et al. Automatic differentiation in PyTorch [J]. 2017.\n李睿凡，梁昊雨，冯方向，张光卫，王小捷．基于全卷积神经结构的段落式图像描述算法[J]．北京邮电大学学报，2019，42（6）：DOI: 10.13190/2019-057．",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 32,
    "chunk_index_in_section": 32,
    "total_chunks_in_section": 33,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 311,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s0_c0",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "方面级情感分析旨在预测文本中每个方面的情感极性，是自然语言处理领域的前沿研究之一。其中，方面词情感分析和方面类别情感分析是最重要的两个子任务，主要区别在于预测对象是否显式地存在于句子之中。近年来，图神经网络在方面级情感分析领域取得了较优效果。然而，大多数方法的性能提升有限，主要原因在于对外部知识利用不充分，对句中概念词与方面类别关系构造不合理，以及对句法结构和语义关系互补性的建模缺失。\n\n针对以上问题，本文开展了如下工作：",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 4,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 213,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9900000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s0_c1",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "针对以上问题，本文开展了如下工作：\n\n1. 针对方面词情感分析任务，提出一种情感感知的双通道图卷积神经网络模型，其中双通道分别为语法通道和语义通道。在语法通道中，利用依存句法分析方法捕获句子的语法结构，得到原始的句法依存树矩阵，然后利用SenticNet中词的情感数值构建句子的情感值矩阵，以对原始矩阵增强情感知识，另外使用词性知识构建词性知识矩阵，对句法依存树矩阵进一步增强；在语义通道中，将基于ConceptNet知识库训练的词向量与经过BLSTM编码的隐向量融合，对词语进行知识语义增强。最终解决了语法、语义通道对外部知识利用不足的问题。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 4,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 272,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s0_c2",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "2. 针方面类别情感分析任务，提出一种知识增强的多通道图卷积神经网络模型，其中多通道分别为知识通道、语法通道和语义通道。在知识通道中，利用基于WordNet的相似度函数计算方面类别与句子上下文之间的语义相似度，进而得到与方面类别相关的相似度矩阵，解决了方面类别与句中相关概念词关系的捕获和利用不合理的问题；在语法和语义通道中，分别使用句法依存分析和自注意力机制构建对应的邻接矩阵，设计基于注意力的特征融合模块，融合语法和语义通道的特征，解决了模型对句法结构和语义关系互补性的建模缺失问题。\n\n3. 设计并实现了面向评论的方面级情感分析系统，该系统由用户信息管理模块、模型管理模块和情感分析与可视化模块组成。该系统平台无关、用户友好，允许用户进行可视化参数配置、模型训练以及对文本进行方面级情感分析。\n\n关键词：方面级情感分析，方面词情感分析，方面类别情感分析，图卷积神经网络，知识增强",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 4,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 392,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s0_c3",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "关键词：方面级情感分析，方面词情感分析，方面类别情感分析，图卷积神经网络，知识增强\n\n5.4.4 情感分析与可视化模块实现\n5.5 系统测试\n5.5.1 功能件测试\n5.5.2 非功能性测试\n5.6 本章小结\n第六章 总结与展望\n6.1 工作总结\n6.2 工作展望",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 4,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 132,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9450000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s1_c0",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "第一章 绪论\n1.1 研究背景及其意义\n1.2 国内外研究现状\n1.2.1 文本情感分析\n1.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 4,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 38,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 47,
    "chunk_method": "hierarchical",
    "importance_weight": 0.44000000000000006,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s1_c1",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "2.2 文本方面级情感分析\n\n越来越多的研究发现，对于许多自然语言处理任务，仅依靠句子内部信息编码得到的特征无法解决特定问题。因此，一些研究开始利用外部知识库对特定任务，甚至预训练语言模型进行不同方式的知识增强。例如，Zhou等人将SenticNet作为常识知识库引入方面词情感分析任务中，通过图卷积神经网络同时融合语法与知识信息，对句法依存矩阵进行知识增强。Zhang等人将知识图谱的结构化信息引入预训练语言模型Bert，对模型进行知识实体层面的掩盖处理，使模型学到更多的语义知识。对于ATSA任务，Liang等人提出Sentic-GCN模型，在利用句法依存树的基础上引入了SenticNet，利用SenticNet中的情感信息对句法依存树做情感语义的增强。对于ACSA任务中方面类别不显式出现在句子中的情况，通过句子内部的结构和语义挖掘方面类别词和句中相关词之间的语义关系十分困难。Liang等人提出AA-GCN模型，利用外部知识库中先验Beta分布的统计特征来计算其句子中词语对方面类别的重要性，从而达到知识增强的目的。综上所述，对于方面词情感分析和方面类别情感分析两个任务而言，如何同时利用好语法和语义信息，更好地利用外部知识对语法和语义信息进行知识增强仍然面临挑战。",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 5,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 38,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 538,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s1_c2",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "长短期记忆网络(LSTM)是一种带有门控机制的循环神经网络，利用三个不同的门控机制，可以选择输入、存储以及输出的信息。LSTM的结构如图2-4所示。\n\n图2-4 LSTM模型结构\n\n设t时刻输入为xt，隐状态表示为ht，额外引入的细胞状态(Cell State)为Ct，LSTM的前向过程如下所示：",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 6,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 38,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 149,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s1_c3",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "1. ft = σ(Wfxt + Uhft-1 + bf)\n2. it = σ(Wixt + Uhit-1 + bi)\n3. Ct = tanh(Wcxt + Uht-1 + bc)\n4. Ct = ft * Ct-1 + it * Ct",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 7,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 38,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 119,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s1_c4",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "5. ht = ot * tanh(Ct)\n\n其中，W和U为可学习的参数矩阵，b为可学习的偏置向量，σ代表逐元素乘积。ft为遗忘门，it为输入门，ot为输出门。遗忘门为一个在0到1之间的数值，用于选择性遗忘细胞状态中保留的信息，决定保留或遗忘细胞状态中的信息。输入门为一个在0到1之间的数值，用于将新的信息选择性地记录到细胞状态中，决定更新什么数值。输出门为一个在0到1之间的数值，用于确定细胞状态的哪个部分将输出。\n\n图2-5 图卷积神经网络(GCN)\n\n图卷积神经网络(GCN)是将卷积操作迁移使用在图结构的数据上，通过卷积的方式获取每个节点的邻居节点的信息，即每个节点的空间特征，最终来更新图中每个节点或图的向量表示。\n\n在GCN学习过程中，不仅要为模型输入每个节点的特征向量表示，还要输入节点之间的连接关系，即邻接矩阵。如图2-5所示，对于堆叠多层的GCN模型，每层的映射关系表示如下：\n\nHl+1 = f(Hl, A)\n\n其中，Wl为第l层神经网络的权重矩阵，f为非线性的激活函数。\n\n进一步地，通过加入节点度的对角矩阵，对上述模型中邻接矩阵A进行归一化操作，如下所示：\n\nD = diag(d1, d2, ..., dn)\nA' = D-1/2AD-1/2\n\n其中，D也称为图的拉普拉斯矩阵。",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 8,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 38,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 552,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s1_c5",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "D = diag(d1, d2, ..., dn)\nA' = D-1/2AD-1/2\n\n其中，D也称为图的拉普拉斯矩阵。\n\n注意力机制(Attention)机制，指对模型的各个输入项都有关注，即对各个输入项的权重设定在0到1之间。换句话说，该机制关注输入的全局信息，对某些部分关注的多，对某些部分关注的少。注意力机制通过注意力分布来加权求和融合各个输入向量，计算公式如下：\n\nαi = softmax(score(xi, q)) = exp(score(xi, q)) / Σj exp(score(xj, q))\n\n其中，score(xi, q)为打分函数，计算方式主要包括加性算法、点积算法、缩放点积算法和双线性算法。\n\n加性算法计算公式如下：\n\nscore(x, q) = V^T tanh(Wx + Uq)\n\n点积算法计算公式如下：\n\nscore(x, q) = x^T q\n\n缩放点积算法对注意力分布进行平滑处理，计算公式如下：\n\nscore(x, q) = (x^T q) / √d\n\n双线性算法是一种泛化点积模型，在计算时引入了非对称性，计算公式如下：\n\nscore(x, q) = (Ux) (Vq)\n\n在上述公式中，d是输入向量的维度，V、U和K是可学习的参数。",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 9,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 38,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 540,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s1_c6",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "score(x, q) = (Ux) (Vq)\n\n在上述公式中，d是输入向量的维度，V、U和K是可学习的参数。\n\n自注意力(Self-Attention)机制通过计算输入项内部的相关性，经过输入项内部得到不同部分的不同重要程度，从而对模型的各个输入项赋予不同的注意力权重。在自注意力机制中，查询项可以由输入信息本身经过计算后得到。模型读到输入信息后，根据输入信息本身决定当前最重要的信息。\n\n如图2-6所示，自注意力机制往往采用查询项-值项(Quey-Value)的模式，且(x, y)为相同的输入特征，计算公式如下：\n\nAttention(Q, K, V) = softmax(score(Q, K))V\n\n其中，Q、K和V分别代表查询项、键项以及值项。L为序列长度，d为查询向量和键值向量的维度。自注意力机制在处理长序列输入时，具备并行计算的能力。\n\n预训练语言模型(PTMs)成为近年来人工智能以及自然语言处理领域技术发展史上的一个突破。训练语言模型的过程无需人工标注好的数据，是一个无监督的学习过程。预训练语言模型通过特定的语言任务对语言模型进行训练，可以有效地从大量的数据中获取知识，并将学习到的知识存储到大量的参数中。这些参数可以作为下游任务模型的初始化参数，然后在预训练好的大模型之上针对特定下游任务对初始化参数进行微调——这直接催生出了“预训练-微调”范式。",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 10,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 38,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 587,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s1_c7",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "Transformer由编码器和解码器两个部分组成。其中，编码器由6个相同的层构成，每层通过多头自注意力机制模块和全连接前向网络模块进行实现。此外，残差网络(Residual Connection)和层标准化(Layer Normalization)用于每一层。Transformer的编码器重要组件如下：\n1. 多头注意力机制(Multi-Head Attention)\n2. 位置编码(Positional Encoding)",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 11,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 38,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 215,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s1_c8",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "2. 位置编码(Positional Encoding)\n3. 前向神经网络(Feed Forward Network)\n\n解码器和编码器的结构类似，但在文本解码器中，多头注意力需要引入一个掩码矩阵(Mask)，引入掩码的多头注意力机制记为Masked Multi-Head Attention。\n\nBERT由多层双向Transformer编码器堆叠而成。其中，自注意力机制(Self-Attention)是整个编码器的核心部分，通过自注意力机制模型不仅可以捕获自身的信息，也可以关注并捕获到文本中其他词的信息。\n\nBERT在大量无标注文本上通过两个无监督语言任务完成对模型预训练。一个是掩码语言模型(Masked Language Model)，采用了类似完形填空的双向语言建模思想，即通过[MASK]周围的上下文语境词来预测[MASK]位置的单词。另一个是下一个句子预测(Next Sentence Prediction)任务。训练样本中有50%的正样本，它们来自自然文本中相邻的两个句子，构成“相关”关系；另外50%的数据为负样本，即将两个句子中的后句替换为语料库中任意一个其他句子，构成“不相关”关系。\n\nBERT同样采用了预训练-微调的训练范式。首先，在大量的无标注文本语料上进行预训练。接着，在具体的下游任务对预训练模型进行微调。\n\n第5部分内容：\n\n2.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 12,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 38,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 584,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s1_c9",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "2.5.3 GPT\n\nGPT（Generative Pre-Training，GPT）是OpenAI在2018年提出的一种生成式预训练模型。如图2-10所示，GPT模型由12个Transformer中的解码器模块经修改后组成，代替了传统的LSTM网络。GPT在解码器端采用了Masked Multi-Head Attention的方式来避免预测当前词时会看见后面的词，是一个单向语言模型。GPT的整体结构如图所示：\n\n图2-10 GPT模型架构\n\n如图2-11所示，对于不同类型的下游任务，对输入给模型的数据进行适当的改造，就可以利用预训练阶段产出的初始化好的模型参数，实现快速的在下游任务中微调，大大提高了下游任务的训练效率。\n\n图2-11 下游任务数据改造\n\nGPT很适合完成生成式任务。",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 13,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 38,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 346,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s1_c10",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "2.6 知识表示技术\n\n目前，知识以三元组的形式存储在知识库中，其中h表示头实体，r表示联系，t表示尾实体。知识表示学习就是通过基于深度学习的表示学习方法，将知识图谱中的三元组知识编码为低维稠密的分布式表示。通过知识表示学习，知识以低维向量的形式分布在语义向量空间中，方便对知识进行度和语义相似度计算。两个向量在空间中的距离越近，则这两个知识在语义上越相似。\n\n一个知识三元组的可信度有两类方法：平移距离和语义匹配。基于此，衍生出基于平移距离和语义匹配两种打分函数。基于这两种打分函数，知识表示学习方法包括了基于平移的方法和基于语义的方法。常用模型包括TransE、TransH等等都输基于平移的方法。这种方法从三元组的结构出发学习知识图谱中实体和联系，而RESCAL、DistMult等方法是基于语义的方法，该方法利用文本语义来学习知识图谱中的实体和联系。",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 14,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 38,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 379,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s1_c11",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "2.7 本章小节\n\n本章节介绍了方面词情感分析任务和方面类别情感分析任务涉及到的背景知识及相关技术。详细介绍了词向量表示技术、循环神经网络、图神经网络、注意力机制、预训练语言模型和知识表示技术在内的基础理论和关键技术。\n\n本文提出了一种基于词性的矩阵增强方法，用于对带有情感语义信息的语法依存树进行语义增强。首先，根据上文所述，模型需要关注可能含有情感含义的词，即形容词、副词和动词。本文预先设置了一个需要关注的词性表N=[形容词、副词、动词]。对于任意词对<Wi,Wj>，如果Wi和Wj的词性在词性表中，则S(i,j)=1；否则S(i,j)=0。基于情感知识和词性知识增强后的邻接矩阵如图3-4所示，具体按照算法2进行构建，计算表达式如下：Aij=Di,j * (SentiNet(Wi) + SentiNet(Wj) + Py + 1)。其中，Di,j是原始的依存矩阵，SentiNet(Wi)和SentiNet(Wj)是Wi和Wj的情感语义信息，Py是词性增强信息。本文通过算法2将经过知识增强的矩阵转化为图结构，表示为邻接矩阵。知识增强的语法图卷积神经网络将基于邻接矩阵作为图结构，将经过Bi-LSTM编码过的隐藏状态作为图中节点的初始化表示，通过KSyn-GCN对节点表示进行更新，得到基于知识增强的语法感知通道的最终表示。\n\n第三章 情感感知的双通道图卷积神经网络的方面词情感分析研究",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 15,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 38,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s1_c12",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "3.6.5 结果分析\n\n表3-2展示了相关模型方法在不同数据集的性能结果。实验主要结果如表3-2所示，除了使用Bert进行编码的模型Bisyn-GAT，SADC-GCN在三个数据集上准确率和F1值指标超过了其他非Bert的方法，取得了较好的效果。基于Bert编码的SADC-GCN+Bert在三个数据集上取得了与现有最新方法可比较的性能。\n\n3.6.6 消融实验与分析\n\nSADC-GCN模型主要包括语义通道，语法通道。为了进一步分析SADC-GCN模型的各个组件的有效性，本文进行了以下消融实验分析。消融实验结果如表3-3所示，对比方法如下：\n\n1) SADC-GCN w/o KSyn-GCN表示完整模型中不含Syn-GCN模块。\n\n2) SADC-GCN w/o KSem-GCN表示完整模型中不含Sem-GCN模块。\n\n3) SADC-GCN w/o sentic&ps表示完整模型中不含情感增强方法和词性增强方法。\n\n4) SADC-GCN w/o sentic表示完整模型中不含情感增强方法。\n\n5) SADC-GCN w/o ps表示完整模型中不含词性增强方法。\n\n6) SADC-GCN w/o we表示完整模型中不含词向量增强方法。\n\n3.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 16,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 38,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 529,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s1_c13",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "3.6.7 图卷积层数影响\n\n为进一步研究SADC-GCN模型中GCN层数的影响，本文在Restaurant-14数据集上评估了本文提出的SADC-GCN模型，并且将GCN层数设置在{1,2,3,4,5,6,7}层的范围内。实验结果表明，2层的KSyn-GCN和2层的KSem-GCN是最佳的参数组合。\n\n3.6.8 案例分析\n\n表3-4展示了3个从测试集中挑选的例子，positive、negative分别表示正向、负向情感极性。从表中可以看出，本文提出的模型SADC-GCN能很好地对句子中方面词的情感极性做出准确的预测。\n\n3.7 本章小结\n\n本章对ATSA任务及当前模型存在的问题进行了研究，提出一种情感感知的双通道图卷积神经网络模型框架SADC-GCN。随后详细介绍了该模型的核心模块：基于知识增强的语法通道模块、基于知识增强的语义通道模块。实验结果表明，本文提出的模型SADC-GCN在方面词情感分析任务上的有效性。\n\n概念节点在本体中的距离越近，它们之间的相似度越高。基于1C的计算方法通过概念节点自身涵盖的语义信息来计算相似度。1C的大小代表了信息量的多少，每个单词的信息量通过语料库计算得到，单词在语料库中出现的频率越低，则信息量越大。基于特征的方法通过概念之间共享信息，比如两个概念在WordNet中的释义的覆盖率，来计算语义相似度。\n\n4.3编码器\n4.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 17,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 38,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 588,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s1_c14",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "4.3编码器\n4.3.1输入层\n在输入层通过词向量模块，将文本中每个单词映射为一个dw维度的分布式词向量。然后，使用双向长短期记忆神经网络(Bi-LSTM)对每个词向量x进行编码，得到隐状态表示。具体而言，输入单词通过前向网络计算得到前向向量hf，通过后向网络计算得到后向向量h。将两者进行拼接，得到隐状态表示。\n\n4.3.2基于依存句法分析的语法图通道\n针对长文本而言，软注意力机制可以高效地捕捉到词与词之间的关系，并对这种关系进行建模。然而单纯的软注意力机制往往会关注到句子中的每个词，这不可避免地会引入噪声，即将与当前词不太相关的单词特征聚合到当前词表示。特别地，对于方面级情感分析任务而言，会把不相关的情感表达词聚合到方面词，从而造成错误的情感分析。当句子中的方面类别的下位词与情感表达词在句子中相距较远时，本文通过句法依存树分析得到的句子结构帮助建立起方面部分与情感部的直接联系，从而缩短它们之间的距离，降低方面类别相关词与情感词之间冗余长文本对情感极性分类的影响。\n\n4.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 18,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 38,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 442,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s1_c15",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "4.3.3基于自注意力机制的语义图通道\n依存句法解析特别适合符合句法结构的文本，通过依存句法解析工具可以获得较为准确的解析结果。然而，部分网络评论数据是没有严谨语法结构的短文本，使用依存句法分析反而可能会产生错误的解析结果，从而影响后续情感分析结果。为解决上述问题，本文使用通过自注意力机制捕捉不含语法结构的短句中词与词之间的语义关系，如果两个单词表达的语义相关，则这两个单词语义相似度更高，通过自注意力机制得到的分值更高。相反，弱相关的词汇之间注意力分值低。注意力分值在0到1之间，由模型在训练中学习得到。\n\n4.3.4基于知识增强的知识图通道\n在ACSA任务中，预先定义好的方面类别可能不会显式地出现在句子之中。本文引入了Lin相似度来计算与度量方面类别与句中相关词在概念层面上的内在相似性与语义相似度。Lin相似度认为，每个概念既有共性信息，也有自己单独的信息容量，可以通过概念之间的信息共性与信息总量的比值来计算两个概念的相似性。为了更好地利用WordNet建模方面类别与句中词的关系，本文通过Lin相似度根据算法4构建出方面类别相似度矩阵。基于知识增强的知识通道的最终输出如下：",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 19,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 38,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 494,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s1_c16",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "4.4多通道融合模块\n语法特征通道将句法结构信息融入到语句的编码信息中，生成了包含句法信息的特征向量。语义特征通道将句法结构信息融入到语句的编码信息中，生成了包含语义信息的特征向量。多通道特征融合模块需要将两种来自不同通道的句子表示进行融合，得到最终的多个通道特征的句子表示。本文利用采用注意力机制来设计多通道特征融合模块，来学习每个通道的权重，对多通道的特征进行融合。\n\n4.5分类与损失函数\n基于外部知识的Knowledge-GCN将外部常识库中的知识信息融入到语句的编码信息中，生成了包含知识语义信息的特征向量。该特征蕴含着了方面类别的信息，具有外部先验知识信息。本文使用注意力机制，根据式(4-10)将该通道的特征向量表示为并作为query，与另外两个通道的融合后的特征向量进行计算，得到能够感知方面类别情感特征的最终表示。情感分析任务的目标是通过最小化预测值和真实值之间的交叉熵损失来训练情感分类器。\n\n4.6实验设计与分析\n4.6.1实验数据\n实验数据集主要由三个基准数据集组成，分别是Restaurant-15、Laptop-15和MAMS。所有的数据集包含三种情感极性：正向、中性和负向。数据集中的每条句子都被标注出了属性词和相应的情感极性。\n\n4.6.2评价指标介绍\n本文选用准确率Accuracy和F1值作为本模型性能的评价指标。\n\n4.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 20,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 38,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 579,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s1_c17",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "4.6.2评价指标介绍\n本文选用准确率Accuracy和F1值作为本模型性能的评价指标。\n\n4.6.3实验参数及设置\n为了验证KE-MC-GCN模型的有效性，本文采用深度学习框架PyTorch开发和训练模型并在基准数据集上进行了定性和定量实验。文本输入阶段，釆用GloVe词向量来初始化句子中的单词，维度为300维。紧接着，使用BiLSTM作为句子特征编码器，隐状态表示维数为300维。各个通道的GCN层数为2，隐状态表示维数为300维。为了防止过拟合，本文把dropout设置为0.3。模型所有的权重初始化服从均匀分布。在监督训练阶段，采用Adam优化器，学习率为0.001，数据批量大大小为16。对于基于Bert的模型而言，本文使用bert-base-uncased，维数为768维，学习率为0.0002。\n\n4.6.4对比模型\n本节选取了近年的针对ACSA任务的细粒度情感分析模型进行定量的性能对比，包括TC-LSTM模型、ATAE-LSTM模型、GCAE模型、CapsNet模型、AS-Capsules模型、GNN模型、MIMLNN模型、AAGCN模型。\n\n第9部分内容：",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 21,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 38,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 490,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s1_c18",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "2. ATAELSTMW模型：该模型是对TC-LSTM模型的改进，将Aspect的表示和隐状态表示拼接作为输入。同时，该模型最早将注意力机制引入方面级情感分析模型，使用注意力机制对句中不同重要程度的单词给予不同的权重，最后将LSTM的输出与注意力计算得到与给定方面相关的句子加权表示。\n\n3. GCAE模型：该模型基于CNN，并行处理速度较快，通过卷积层不同大小的卷积核捕捉句子的n-gram特征。模型上层接了一个门控机制用于保留与方面类别有关的信息。\n\n4. CapsNet模型：该模型提出了MAMS(Multi-AspectMulti-Sentiment)数据集，以及一种基于胶囊网络的模型来学习方面类别和上下文之间的复杂关系。一个情感类别先验知识矩阵指导路由权重的调整。\n\n5. AS-Capsules模型：该模型使用胶囊网络同时进行方面识别和情感分类两个任务，不同的胶囊对应不同的方面类别。该模型在低层采用共享的_对两个任务进行联合学习。",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 22,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 38,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 423,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s1_c19",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "6. MIMLLN模型：该模型提出了一种多实例多标记学习的网络框架，将句子作为包，单词作为包中实例。具体地，将句子中表示方面类别的单词作为指不方面类别的关键实例，并利用方面类别检测(AspectCategoryDetection)任务辅助训练。该模型首先找到文本里指示方面类别的词，然后对指示词进行情感分类，最终聚合指示词的情感得到方面类别的情感。\n\n7. AAGCN-TW模型：该模型利用外部知识库检索获取与方面类别高度相关的扩展词，并利用统计得到的Beta分布概率来计算对方面类别的重要性，从而构建出句子中词对的邻接矩阵，结合图卷积神经网络来聚合更新节点的特征。\n\n8. SRGN模型：该模型利用基于本体相似度的方法NPMI计算方面类别与句子中词语的语义相似度，构建出相似度单词和方面类别的异质图邻接矩阵，结合边控图卷积神经网络来聚合更新节点的特征。同时，该模型利用ACD任务辅助训练。\n\n4.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 23,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 38,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 399,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s1_c20",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "6.5结果分析\n\n表4-2展示了相关模型方法在不同数据集的性能结果。需要说明的是，由于大部分研究工作模型设计复杂且缺少有效的开源代码，因此，难以在与KEMC-GCN相似的实验环境下重复相关实验并评测，故表中的性能结果均沿用相关研究论文中的指标或其他论文中复现的指标进行汇报展示。\n\n为了评估本文提的KEMC-GCN模型，本文使用Accuracy(Acc.)和F1-score(F1)作为主要评估指标。实验主要结果如表4-2所示，KEMC-GCN在三个数据集上准确率和F1值指标超过了其他的方法，取得了较好的效果。",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 24,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 38,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 256,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s1_c21",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "具体分析来看，1)与基于句法依存树和图卷积神经网络的模型相比，如AAGCN，KEMC-GCN取得了更好的结果，这说明了基于自注意力机制的语义通道模型有效地融合了语义信息，能够很好地捕获到不符合语法结构短文本的语义信息。2)与基于注意力的方法相比，如TC-LSTM、ATAE-LSTM，KEMC-GCN模型利用了句法结构建立了词与词之间的直接关系，可以缩短方面类别相关词与情感表达词的距离，避免其他词带来的噪声。这进一步说明，基于句法依存树的语法通道模型有效地融合了句法结构信息。从整体来看，基于多通道特征的KEMC-GCN模型可以学习到语义通道和语法通道的互补信息，取得更好的结果。3)与基于外部知识的模型相比，比如AAGCN和SRGN，KEMC-GCN模型通过外部知识库，引入基于概念的相似度函数，来更高效地建立方面类别与句子中相关词的关系，取得了更好的效果。4)与多实例框架和胶囊网络模型相比，本模型的结果同样更具有竞争力。\n\n总的来说，通过引入语法通道特征、语义通道特征和知识通道特征，本文所提出的模型与方法取得了更优的指标性能，具有一定的优势和竞争力。",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 25,
    "chunk_index_in_section": 21,
    "total_chunks_in_section": 38,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 480,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s1_c22",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "总的来说，通过引入语法通道特征、语义通道特征和知识通道特征，本文所提出的模型与方法取得了更优的指标性能，具有一定的优势和竞争力。\n\n用户评论是分析用户行为、构建用户画像的关键数据，同时也是构建个性化推荐的基础。对于接入本地生活的商户而言，其用户范围从实体门店的一隅之地变为一个城市。每家商户都面临着机遇与挑战，他们的经营范围变大的同时必然面临着市场相互重叠。要从众多商户激烈的竞争中脱颖而出，必须时刻根据用户的反馈做出策略调整。用户反馈的评论数据蕴含着用户的情感喜恶，具有十分重要的应用价值。对这些数据的分析，可以帮助商家更容易地从菜品、服务、到环境等方方面面进行改善，从而提高顾客黏性，实现长期的销售数据增长。\n\n本系统的目标是设计和实现一个面向评论的方面级情感分析系统，该系统可以同时供模型训练管理员和运营人员使用。\n\n功能性需求：",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 26,
    "chunk_index_in_section": 22,
    "total_chunks_in_section": 38,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 368,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s1_c23",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "1. 用户管理模块：包括注册功能、登录功能、个人信息管理功能。\n2. 模型管理模块：包括数据管理功能、数据预处理功能、模型可视化参数配置与训练功能。\n3. 情感分析与可视化模块：包括数据输入功能、情感分析功能、评论结果可视化功能。\n\n非功能性需求：\n1. 易用性需求：保证用户可以获得最佳的体验效果。\n2. 可扩展性需求：考虑后期数据库服务器、逻辑服务器硬件的扩展。\n3. 兼容性需求：前端页面需要适配各种主流浏览器。\n4. 安全性需求：用户信息需要加密存储，用户操作前需要验证身份信息。\n\n系统概要设计：\n1. 总体功能设计：系统分为用户管理、模型管理和情感分析与可视化三大部分。\n2. 系统架构设计：应用层、模型层和数据层。\n\n系统详细设计：\n1. 用户管理模块：包括用户注册、登录等功能。\n2. 模型训练模块：面向系统管理员，提供可视化配置和模型训练功能。\n3. 情感分析与可视化模块：面向运营人员，提供情感分析和结果可视化功能。\n4. 数据库设计：包括用户信息表、训练任务表、情感分析任务表等。\n\n系统实现：\n1. 系统实现环境：前端使用Vue，后端使用Flask，数据库使用SQLite，算法部分使用PyTorch。\n2. 用户管理模块实现：包括注册和登录页面。\n3. 模型训练模块实现：提供数据上传和模型训练页面。",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 27,
    "chunk_index_in_section": 23,
    "total_chunks_in_section": 38,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 563,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s1_c24",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "2. 用户管理模块实现：包括注册和登录页面。\n3. 模型训练模块实现：提供数据上传和模型训练页面。\n4. 情感分析与可视化模块实现：提供文本输入和结果展示页面。\n\n系统测试：\n1. 功能性测试：对系统的功能、性能及界面等进行冒烟测试、黑盒与白盒测试。\n\n第六章 总结与展望\n\n6.1 工作总结\n\n方面级情感分析旨在以方面粒度为分析单位，预测出文本中各个方面的情感倾向，具有重要的学术价值和广泛的应用价值，在自然语言处理领域方兴未艾。本文主要围绕方面词情感分析(ATS)和方面类别情感分析(ACSA)这两个任务展开，分别针对两个任务及当前遇到的问题进行研宄，具体研究工作如下：\n\n1. 针对方面词情感分析任务，提出一种情感感知的双通道图卷积神经网络模型，其中双通道分别为语法通道和语义通道。在语法通道中，本文首先利用依存句法分析方法捕获句子的语法结构，得到原始的句法依存树矩阵，然后利用SenticNet中词的情感数值构建句子的情感值矩阵，以对原始矩阵增强情感知识，另外使用词性知识构建词性知识矩阵，对句法依存树矩阵进一步增强；在语义通道中，本文将基于ConceptNet知识库训练的词向量与经过模型编码的隐向量融合，对词语进行知识语义增强。最终解决了语法、语义通道对外部知识利用不足的问题。",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 28,
    "chunk_index_in_section": 24,
    "total_chunks_in_section": 38,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 545,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s1_c25",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "2. 针对方面类别情感分析任务，提出一种知识增强的多通道图卷积神经网络模型，其中多通道分别为知识通道、语法通道和语义通道。在知识通道中，本文利用基于WordNet的相似度函数计算方面类别与句子上下文之间的语义相似度，进而得到与方面类别相关的相似度矩阵，解决了方面类别与句中相关概念词关系的捕获和利用不合理的问题；在语法和语义通道中，本文分别使用句法依存分析和自注意力机制构建对应的邻接矩阵，设计基于注意力的特征融合模块，融合语法和语义通道的特征，解决了模型对句法结构和语义关系互补性的建模缺失问题。\n\n3. 根据对方面词情感分析和方面类别情感分析任务的研宄，本工作设计并实现一套面向用户评论的方面级情感分析系统。该系统的搭建过程包括需求分析、功能设计、功能实现等阶段。主要功能包括用户管理功能、模型管理功能以及情感分析及可视化功能。该系统不仅可以用于可视化模型训练，同时也可以用于模型推理和预测，即使用训练好的模型对文本进行方面级情感分析，并对分析结果进行可视化展示。",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 29,
    "chunk_index_in_section": 25,
    "total_chunks_in_section": 38,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 434,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s1_c26",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "6.2 工作展望\n\n本文对方面词情感分析和方面类别情感分析两个任务及当前遇到的问题进行了研宄，提出了相应的SADC-GCN和KEMC-GCN模型，取得了一些进展，但依然存在探索和改进的空间。具体如下：\n\n1) 探索更有效的建立方面类别与句中相关词关系的方法。本文引入外部知识库，利用基于知识库的相似度函数来建模方面类别与句中相关词关系，同时结合多通道特征图捕获句子的语法和语义特征。这种基于外部知识库的单一相似度函数并不能充分地表达方面类别与句中相关词的关系。未来，可以考虑多种相似度函数融合的方法，对单一相似度函数的结果进行完善。\n\n2) 探索隐式情感数据的情感预测方法。评论数据中存在少量的隐式情感样本，即样本中没有出现明显的情感观点词，本文提出的模型不能准确地预测此类样本的情感极性，从而在性能指标上有局限性。未来，可以探索使用预训练模型在针对含有隐式情感表达的数据集上，构造无监督对比学习或其他相关任务，进行领域自适应的预训练，帮助大模型学习到更多包含隐式情感的句子表达方式。然后，使用大模型更高效地进行情感分析。\n\n清洗后的内容如下：",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 30,
    "chunk_index_in_section": 26,
    "total_chunks_in_section": 38,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 472,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s1_c27",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "27. Zhou J, Huang JX, Hu QV, et al. Sk-gcn: Modeling syntax and knowledge via graph convolutional network for aspect-level sentiment classification [J]. Knowledge-Based Systems, 2020, 205:106292.\n\n28. Zhang Z, Han X, Liu Z, et al. ERNIE: Enhanced Language Representation with Informative Entities [J]. 2019.\n\n29. Liang B, Su H, Gu L, et al. Aspect-based sentiment analysis via effective knowledge enhanced graph convolutional networks [J]. Knowledge-Based Systems, 2022, 235:107643.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 31,
    "chunk_index_in_section": 27,
    "total_chunks_in_section": 38,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 482,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s1_c28",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "107643.\n\n30. Liang B, Su H, Yin RS, et al. Beta Distribution Guided Aspect-Aware Graph for Aspect Category Sentiment Analysis with Effective Knowledge [C]. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing.\n\n31. Mikolov T, Chen K, Corrado G, et al. Efficient estimation of word representations in vector space [J]. 2013.\n\n32. Pennington J, Socher R, Manning CD. GloVe: Global vectors for word representation [C]. Proceedings of the 2014 conference on empirical methods in natural language processing.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 32,
    "chunk_index_in_section": 28,
    "total_chunks_in_section": 38,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 537,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s1_c29",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "33. Peters ME, Neumann M, Iyyer M, et al. Deep contextualized word representations [J]. 2018.\n\n34. Elman JL. Finding structure in time [J]. Cognitive Science, 1990, 14(2):211-281.\n\n35. Graves A. Long short-term memory [J]. 2012.\n\n36. Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and translate [J]. arXiv preprint arXiv:1409.0473, 2014.\n\n37. Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need [J]. Advances in neural information processing systems, 2017, 30.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 33,
    "chunk_index_in_section": 29,
    "total_chunks_in_section": 38,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 511,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s1_c30",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "30.\n\n38. Radford A, Narasimhan K, Salimans T, et al. Improving language understanding by generative pre-training [J]. 2018.\n\n39. Devlin J, Chang MW, Lee K, et al. Bert: Pre-training of deep bidirectional transformers for language understanding [J]. arXiv preprint arXiv:1810.04805, 2018.\n\n40. Liu Y, Ott M, Goyal N, et al. RoBERTa: A robustly optimized BERT pretraining approach [J]. arXiv preprint arXiv:1907.11692, 2019.\n\n41. Sun Y, Wang S, Li Y, et al. ERNIE: Enhanced representation through knowledge integration [J]. arXiv preprint arXiv:1904.09223, 2019.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 34,
    "chunk_index_in_section": 30,
    "total_chunks_in_section": 38,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 560,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s1_c31",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "1904.09223, 2019.\n\n42. Ji S, Pan S, Cambria E, et al. A survey on knowledge graphs: Representation, acquisition, and applications [J]. IEEE Transactions on Neural Networks and Learning Systems, 2021, 33(2):494-514.\n\n43. Bordes A, Usunier N, Garcia-Duran A, et al. Translating embeddings for modeling multi-relational data [J]. Advances in Neural Information Processing Systems, 2013, 26.\n\n44. Wang Z, Zhang J5, Feng J. Knowledge graph embedding by translating on hyperplanes [C]. Proceedings of the AAAI conference on artificial intelligence. 2014, 28(1).",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 35,
    "chunk_index_in_section": 31,
    "total_chunks_in_section": 38,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 555,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s1_c32",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "45. Nickel M, Tresp V, Kriegel HP. A three-way model for collective learning on multi-relational data [C]. IJCAI. 2011, 11(10.5555):3104482-3104584.\n\n46. Yang B, Yih W, He X, et al. Embedding Entities and Relations for Learning and Inference in Knowledge Bases [J]. 2015.\n\n47. Speer R, Chin J, Havasi C. ConceptNet 5.5: An open multilingual graph of general knowledge [C]. Thirty-first AAAI conference on artificial intelligence. 2017.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 36,
    "chunk_index_in_section": 32,
    "total_chunks_in_section": 38,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 435,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s1_c33",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "2017.\n\n48. Cambria E, Poria S, Hazarika D, et al. SenticNet 5: Discovering Conceptual Primitives for Sentiment Analysis by Means of Context Embeddings [J]. Proceedings of the AAAI Conference on Artificial Intelligence. 2018.\n\n49. SenticNet 6: Ensemble Application of Symbolic and Subsymbolic Al for Sentiment Analysis | Proceedings of the 29th ACM International Conference on Information & Knowledge Management [EB/OL]. https://dl.acm.org/doi/abs/10.1145/3340531.34112003.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 37,
    "chunk_index_in_section": 33,
    "total_chunks_in_section": 38,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 472,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s1_c34",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "10.1145/3340531.34112003.\n\n50. Graves A, Schmidhuber J. Frame-based phoneme classification with bidirectional LSTM networks [C]. Proceedings of the IEEE International Joint Conference on Neural Networks. 2005.\n\n51. Nivre J. An efficient algorithm for projective dependency parsing [C]. Proceedings of the eighth international conference on parsing technologies. 2003.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 38,
    "chunk_index_in_section": 34,
    "total_chunks_in_section": 38,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 367,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s1_c35",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "2003.\n\n52. Jiang Q, Chen L, Xu R, et al. A Challenge Dataset and Effective Models for Aspect-Based Sentiment Analysis [C]. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP).\n\n53. Miller GA. WordNet: A lexical database for English [J]. Communications of the ACM, 1995, 38(11):39-41.\n\n54. Zhao FQ. 基于词嵌入和WordNet的词汇相似度计算模型 [J]. 2021.\n\n55. Liu HZ, Xu D. 基于本体的语义相似度和相关度计算研究综述 [J]. 计算机科学, 2012, 39(2):1-13.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 39,
    "chunk_index_in_section": 35,
    "total_chunks_in_section": 38,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 526,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s1_c36",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "2021.\n\n55. Liu HZ, Xu D. 基于本体的语义相似度和相关度计算研究综述 [J]. 计算机科学, 2012, 39(2):1-13.\n\n56. Lin D. An information-theoretic definition of similarity [C]. IJCAI. 1998, 98(1998):304-313.\n\n57. Wang X, Zhu M, Bo D, et al. Am-gen: Adaptive multi-channel graph convolutional networks [C]. Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 2020.\n\n58. Kingma DP, Ba J. Adam: A method for stochastic optimization [J]. arXiv preprint arXiv:1412.6980, 2014.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 40,
    "chunk_index_in_section": 36,
    "total_chunks_in_section": 38,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 483,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于知识增强的方面级情感分析研究及应用_杜一帆_s1_c37",
    "source_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
    "text": "1412.6980, 2014.\n\n59. Tang DS, Qin B, Feng X5, et al. Effective LSTMs for target-dependent sentiment classification [J]. arXiv preprint arXiv:1512.01100; 2015.\n\n60. Wang Y, Sun A, Huang M, et al. Aspect level sentiment analysis using as-capsules [C]. The world wide web conference. 2019.\n\n61. Zhou T, Law KY. Semantic Relatedness Enhanced Graph Network for Aspect Category Sentiment Analysis [J]. Expert Systems with Applications. 2022, 195:116560.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 41,
    "chunk_index_in_section": 37,
    "total_chunks_in_section": 38,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 448,
    "chunk_method": "hierarchical",
    "importance_weight": 0.42000000000000004,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c0",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "图像描述（Image Captioning）旨在为给定图片生成人类可理解的描述性文本，具有丰富的应用价值。目前最先进的图像描述模型大多基于自回归的文本生成方法，每次推理基于先前生成的单词一个单词，在效率上存在不足。近年来一些非自回归图像描述工作试图改善图像描述的效率，但是在效果上相比现有的自回归基线仍存在差距。本文聚焦于非自回归的图像描述以提升图像描述的效率和效果，并在此基础上进一步研究了具有现实意义的可控图像描述任务。\n\n本文的主要研究工作如下：\n\n1）为了提升非自回归图像描述方法的效率和效果，本文提出了一种结合人类编辑操作的非自回归图像描述方法。包括一个编辑器和适用于编辑操作的两阶段图像描述训练策略。统一编辑器通过一种创新的位置预测操作同时执行多种编辑任务，提高了基于编辑操作生成图像描述的效率。两阶段训练策略优化了图像描述方法中传统的交叉熵和强化学习训练阶段以更好地适应编辑操作。实验结果表明本文提出的方法取得了先进的速度-性能平衡，验证了方法的有效性。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 58,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 434,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c1",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "2）在可控图像描述任务中，现有的大部分方法都局限于初步地引入控制信号。对可控图像描述的推理速度和数据集的关注相对较少。首先，本文基于非自回归图像描述方法引入控制信号，提出了一种高速的可控非自回归图像描述方法。其次，本文提出了一种基于大语言模型的风格化数据生成方法，可以缓解目前工作中存在的数据集限制。实验结果表明本文提出的可控非自回归图像描述方法可以高效地完成可控图像描述任务，验证了方法的有效性。\n\n3）基于对非自回归图像描述的研究，本文设计并实现了一个非自回归图像描述系统，该系统支持用户的注册和登录以及图像描述的在线生成，有助于图像描述的社区传播。\n\n自回归方法与非自回归方法生成文本的对比：\n\n自回归方法生成五个单词的句子共需要六步（包括终止符），而非自回归方法生成的步数与句子的长度无关。非自回归方法与自回归方法的主要区别在于，自回归方法每次根据先前已经生成的句子生成一个单词，而非自回归方法并行地处理整个句子，与句子的长度无关，因此可以达到O(1)的时间复杂度。因此，非自回归的生成方法相比自回归生成在速度方面有着巨大的优势。\n\n图像描述中存在的模式崩溃、风格单一的问题的例子：",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 495,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c2",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "图像描述中存在的模式崩溃、风格单一的问题的例子：\n\n生成的可控性问题来自生成式任务中广泛存在的“模式崩溃”问题。作为生成式模型，目前最先进的图像描述模型普遍面临不同程度的模式崩溃问题，即在很大的模式空间中，模型收敛到了某几个固定的简单模式。在图像描述中，这个问题体现为生成收敛在某几个固定的句式中，如图1-3所示，生成的结果明显地局限在了几个简单的“there are _ in _”句式中。\n\n自回归图像描述：\n\n图像描述的研究由来已久，早期的图像描述方法主要基于模板或检索生成。最早的基于神经网络的图像描述模型Neural Image Captioning generator（NIC）由Vinyals等人于2014年提出，采用了编解码器架构，以卷积神经网络（CNN）作为图像编码器，长短期记忆网络（LSTM）作为文本解码器，取得了当时最好的表现。随后，随着对模型各方面能力的不断探索，更多的基于神经网络的图像描述模型被提出。以编解码器模型的角度来看图像描述模型，可以将其拆解为三个部分：图像编码器、文本解码器（本节中将专注于自回归的文本解码器）和整体训练策略。本小节中，本文将从这三个部分出发，简述近年来自回归图像描述的相关工作。\n\n非自回归图像描述：",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 529,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c3",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "非自回归图像描述：\n\n近年来，图像描述工作开始转向纯Transformer结构，其强大的学习能力大大提高了图像描述模型的表现。这些工作在训练时通过上三角掩码实现了并行的语言模型训练，而解码时，这些模型仍然使用了串行自回归的解码方式，即每个时间步只预测一个词，虽然这样做得到了很好的效果，但无疑是对并行一种浪费，在效率上有很大的优化空间，因此非自回归的Transformer解码器被提出用于并行解码。非自回归是与自回归相对的概念，表示生成文本方式的改变，自回归生成在每一个时间步都只能利用前t-1步生成的文本生成第t个单词，即串行生成，非自回归生成方法致力于通过不同方式向生成中引入了一定程度的并行性，在图像描述中，目前被提出的非自回归模型可以被分为三类：单步非自回归图像描述、迭代细化（多步）非自回归图像描述、半自回归图像描述。\n\n本文主要研究了基于编辑的非自回归图像描述方法和可控非自回归图像描述方法。\n\n在基于编辑的非自回归图像描述方面，提出了Unified Edit-based Non-autoregressive Image Captioning (UniCap)方法，该方法利用统一的编辑任务简化了编辑操作，并提出了冲突消解算法解决无效输出问题。为了适应编辑操作，提出了多路径训练方法和阈值采样方法。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 556,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c4",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "在可控非自回归图像描述方面，提出了Controllable Unified Edit-based Non-autoregressive Image Captioning (ConUniCap)方法，通过引入可学习的张量提示词作为控制信号，并利用大语言模型生成风格化数据。同时，提出了大语言模型数据校验机制以保证数据质量。\n\n在MSCOCO数据集上进行了实验，结果表明UniCap方法实现了8倍的速度提升，并在非自回归图像描述中达到了最先进的效果。ConUniCap方法也验证了控制图像描述风格的能力，并可以简单地拓展到其他风格。\n\n第二章 相关技术\n\n2.1 深度学习技术\n\n2.1.1 Transformer技术\n\nTransformer技术由Vaswani等人在2017年提出，深刻地改变了深度学习领域的方方面面。先前的循环神经网络在计算过程中需依赖上一时刻的输出，无法实现计算的并行处理，导致训练效率低下。如LSTM等循环神经网络架构仍面临梯度消失或爆炸的问题。针对这些问题，Transformer通过多头注意力机制来捕获序列之间的上下文依赖，有效降低了训练的复杂度，实现了训练过程的并行化，近年来逐步成为了深度学习领域的基础模型。\n\n2.1.2 编解码器结构",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 533,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c5",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "2.1.2 编解码器结构\n\n编解码器架构是一种通用的神经网络架构，由编码器与解码器两大部分组成。编码器主要负责把输入数据转换成一种中间表示形式，而解码器则将此中间表示解码，以产生最终的输出结果。编解码器架构通过更改输入和输出的定义可以拓展出多种多样的功能，例如在自然语言处理领域，编码器-解码器架构可以用于机器翻译，其中编码器一种语言的文本输入转换为中间表示，而解码器则将该表示转换成另一种语言的文本输出。这种架构也被广泛应用于语音识别和文本生成等任务中，如生成式对话系统和文本摘要生成。在图像处理领域，编码器-解码器架构可以用于图像分割任务中，编码器负责提取图像的特征信息，而解码器则基于这些特征生成图像的每个像素所属的类别。这种方法不仅适用于静态图像，也可以扩展到视频内容的处理，如视频分割和动作识别。此外，广泛应用的Transformer架构和在变分自编码器(VAE)也遵循编码器-解码器架构。\n\n2.2 视觉特征编码\n\n视觉特征编码器的目标是提取图像中的特征到一系列向量中，根据提取的特征的种类不同，可以分为初步的全局视觉特征和更细粒度的目标检测视觉特征。根据网络架构不同，近年来主要使用卷积神经网络或者Transformer网络提取图像特征。本节将从这两个角度分类对视觉特征的编码进行讨论。\n\n2.2.1 全局视觉特征\n\n2.2.1.1 基于卷积神经网络的视觉特征编码",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 5,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 589,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c6",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "2.2.1 全局视觉特征\n\n2.2.1.1 基于卷积神经网络的视觉特征编码\n\n卷积神经网络是最具代表性的神经网络模型，主要代表工作包括ResNet，GoogLeNet等，在提取图像特征方面有极大的优势。一个卷积神经网络通常由卷积层、池化层、归一化层和其它部件叠加组成。其中主要发挥特征提取作用的是卷积层和池化层。卷积层通过滑动具有预定步长的卷积核模板对特征图进行分析，以提取新的特征图。这种操作处理的特征图是三维张量，最初来源于经过预处理的图像像素，其中三维分别代表图像的长度、宽度及RGB颜色通道。卷积核的参数共享机制相较于全连接层的矩阵运算，既降低了计算量，使得对更高分辨率图像的处理成为可能，又增强了模型对图像平移不变性特征的捕捉能力，从而提升了模型的泛化性。以二维图像为例，当应用二维时，卷积计算可以形式化为：\n\nS(i,j) = (l * K)(i,j) = Σm,n l(m,n)K(i+m,j+n)\n\n其中，l代表输入特征图，K代表卷积核，S代表输出特征图。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 6,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 436,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c7",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "S(i,j) = (l * K)(i,j) = Σm,n l(m,n)K(i+m,j+n)\n\n其中，l代表输入特征图，K代表卷积核，S代表输出特征图。\n\n池化层也可以称为下采样操作，通过采集临近特征图区域的统计数据作为样本，旨在从经卷积处理的特征图中筛选信息并进行过滤。此过程的一个显著属性是导致特征图维度，即长度和宽度的减少。类似于卷积过程，池化要求预设其作用域及步进长度。主要的池化技术包括最大池化和平均池化。最大池化一个邻近区域(如2X2)内的最大值来更新特征图的相应位置，而平均池化通过计算邻近区域内的平均值来进行更新。\n\n归一化由Sergey等人提出，广泛应用于卷积神经网络中，批归一化的原理是将卷积神经网络中每一层输出的方差和均值归一化。批归一化的目的是减少训练过程中网络层输入分布的变化。通过对每一批数据进行归一化处理，可以使网络更加稳定，加快训练速度，并有助于减轻过拟合。具体来说，批归一化在每个训练批次中，对于给定的特征，在批次的维度上计算均值和方差，然后使用这些统计量来归一化该批次的数据，使其均值接近0且方差接近1。此外，批归一化还引入了两个可训练参数，以恢复在某些情况下可能需要的网络表达能力。归一化层通常置于卷积层(或全连接层)的输出和激活函数的输入之间。这种做法可以有效减少训练过程中梯度消失或梯度爆炸的问题，从而允许使用更高的学习率，加速网络的收敛。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 7,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 590,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c8",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "2.2.1.2 基于Transformer的视觉特征编码\n\nTransformer架构首先在纯文本领域被提出，由于其强大的建模能力迅速被拓展到其它场景，比如图片的特征抽取，早期被提出的纯Transformer图像特征编码器是Dosovitskiy等人提出的ViT模型，ViT虽然不是第一篇将Transformer应用在视觉任务的论文，但是因为其模型简单且效果好，可扩展性强，被广泛应用于各种视觉任务中。\n\nViT将输入图片分为多个块(16x16)，再将每个块投影为固定长度的向量送入模型，后续编码器的操作和原始Transformer中完全相同。在对这些块的处理中，采用了与处理文本类似的方法，这一点也体现了Transformer架构的通用性和灵活性。\n\n在将图像分割成块之后，ViT会将这些图像块展平(即将二维像素矩阵转一维向量)，并通过一个线性层或简单的全连接层将其映射成一个固定长度的向量。这一过程可以看作是对每个图像块进行特征提取。然后，为了使模型能够理解这些图像块之间的相对位置，ViT还会向每个块的特征向量中添加位置编码。这与Transformer在处理文本数据时添加位置编码的做法是一致的，目的是提供序列中不同元素的位置信息。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 8,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 519,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c9",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "ViT是Transformer思想在视觉领域的直接应用，直接对全部的块应用注意力。然而，图像天然具有局部性，卷积神经网络中的卷积操作也天然蕴含了局部性质。因此，兼容局部特征处理的视觉Transformer在研究中被提出，目前最流行的是Liu等人提出的Swin Transformer。Swin Transformer使用了类似卷积神经网络中的多尺度特征，即特征图包括对图像下采样4倍、8倍及16倍的特征。而且Swin Transformer首次提出了窗口多头自注意力(Windows Multi-Head Self-Attention)机制，使得自注意力的范围局限在窗口中，减少了计算量，额外地，为了保证全局的特征传递，作者还提出了移位窗口自注意力(Shifted Windows Multi-Head Self-Attention)的概念，通过应用移位窗口自注意力，可以在得到窗口局部化的优势的同时保留全局特征流动的能力。\n\n2.2.2 目标检测视觉特征\n\n全局的图像特征是粗粒度的，然而真实图像中往往存在复杂、多层次多尺度的细粒度信息，目标检测任务就是对这种信息的一种捕捉过程，为了在图像描述和其它跨模态任务中应用细粒度的信息，目标检测模型经常被用于各种交叉领域任务中以提取细粒度的区域特征。与全局特征同样，目标检测视觉特征也可以通过卷积神经网络或者Transformer网络实现。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 9,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 593,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c10",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "2.2.2.1 基于卷积神经网络的目标检测\n\nGirshick等人提出的R-CNN是最早引入卷积神经网络的目标检测算法，整体的网络结构如图2-2所示，R-CNN由三个模块组成，包括候选框生成，特征提取和分类模块，候选框生成模块首先在图片中找到很多的候选框(Proposal)，然后特征提取模块基于CNN提取每个候选框的图像特征，分类模块则将每个框的图像特征通过全连接层后执行分类任务，判断是否是待检测的目标，后续的大部分基于卷积神经网络的目标检测模型都沿用了R-CNN的架构。\n\nR-CNN中存在很多的不足，其中仍然存在很多传统的机器学习算法，如通过选择性搜索(Selective Search)获得候选区域，选择性搜索是一种在传统图像处理领域中采用的图像层次分割技术，先在图像中创建初始区域，然后基于颜色、纹理等特征将这些区域合并，从而实现图像区域的最终分割。分类任务也是通过传统的支持向量机进行。其次，通过选择性搜索获得的候选区域数量级往往很大，带来了巨大的成本问题。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 10,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 436,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c11",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "为了解决R-CNN的不足，Girshick等人提出了Fast R-CNN，一种端到端的目标检测方法，相比R-CNN，Fast R-CNN首先计算一次全局的卷积特征，然后直接映射到选择性搜索的候选框中，避免重复计算，同时在候选框集合上应用了非极大值抑制(Non-Maximum Suppression)算法以压缩候选框的数量，提升了R-CNN推理的速度。最后，Fast R-CNN还将R-CNN中应用的支持向量机替换为基于神经网络的分类器。实现了端到端的训练。\n\nFast R-CNN实现了端到端的训练和推理，但候选框的提取仍然采用的是选择性搜索，非常耗时，为了支持目标检测的实时应用，替换选择性搜索是必须的，因此Ren等人提出了Faster R-CNN，在其中首次应用了区域提案网络(Region Proposal Network)以基于卷积神经网络生成候选框，为了整体结构的轻量化，区域提案网络的权重是与特征提取使用的卷积神经网络共享的。\n\n目前图像描述领域最常用的目标检测特征均基于Faster R-CNN，如Anderson等人提出了一个用于图像描述的版本，使用在ImageNet上分类预训练的ResNet。\n\n清洗后的内容如下：\n\n1. 使用101作为初始化权重，在Visual Genome数据集上进行训练，加入额外预测属性类别的训练输出，帮助目标检测模型学习更好的特征表达。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 11,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 593,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c12",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "1. 使用101作为初始化权重，在Visual Genome数据集上进行训练，加入额外预测属性类别的训练输出，帮助目标检测模型学习更好的特征表达。\n\n2. 近年来，目标检测领域取得巨大进展，其中之一是以加等人提出的Transformer目标检测器DETR。DETR使用完全不同的编码器-解码器架构实现目标检测任务，不需要传统的候选框机制。\n\n3. DETR的基本流程包括输入图像，通过卷积神经网络获取下采样后的特征图，加入位置编码后展平为一维特征图。后续过程与正常的Transformer编码器-解码器架构基本相同，不同之处在于解码器的输入是一系列可学习的向量，称作查询。\n\n4. DETR将解码器每个位置的输出都通过预测头翻译为框和对象类别，完成目标检测的任务。由于查询的数量往往超过图中对象的数量，使用二部图匹配算法计算损失。\n\n5. DETR的训练通过基于匈牙利算法的匹配和传统目标检测的框回归损失、目标分类损失，保证了无需候选框的高效推理，在推理速度上远高于传统的Faster R-CNN。\n\n6. 在自然语言处理领域，评价两个句子相似度的常见指标包括BLEU、CIDEr、ROUGE、METEOR等。这些评价方法主要通过n元语法分析文本，依据n的大小，把连续的ri个单词视作一个单位，进而把文本理解为由这些ri元词组单元构成的序列。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 12,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 571,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c13",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "7. BLEU在2002年由Papineni等人提出，首次被应用于评估机器翻译模型的性能。该指标因与人类评价之间的高度相关性、低计算成本以及易用性而广泛用于评价各种文本生成任务。\n\n8. ROUGE算法通过比较机器生成的摘要一组人工编写的参考摘要，并计算两者之间重叠的基本单元(n元语法、词序列、词对)的数量给出得分。ROUGE实际上测量的是生成文本的召回率，更全面地评价了文本的质量。\n\n9. METEOR由Banerjee等人提出，旨在对BLEU指标进行改良。与BLEU不同，后者只基于准确度评估并要求严格的n-gram匹配，未涉及语义相近词汇的匹配。METEOR指标在评估过程中同时兼顾了精确率和召回率，并对召回率赋予较高的权重。\n\n10. CIDEr指标由Vedantam等人提出，专为图像描述领域的自动评估而设计。CIDEr通过应用TF-IDF加权机制，对词汇进行差异化权重分配。这意味着，在图像描述中频繁出现的词，由于其较低的视觉信息贡献，会被赋予较低的权重。因此，CIDEr在评价图像描述文本时，能更加贴近人类的评价标准，相较于BLEU和METEOR指标，实现了更高的一致性。\n\n本文提出了一种基于编辑的非自回归图像描述方法，主要内容包括：\n\n1. 提出了统一编辑器，将删除、插入、换位等编辑操作统一到一个解码器中，提高了模型效率。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 13,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 573,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c14",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "本文提出了一种基于编辑的非自回归图像描述方法，主要内容包括：\n\n1. 提出了统一编辑器，将删除、插入、换位等编辑操作统一到一个解码器中，提高了模型效率。\n\n2. 解决了统一编辑器中的预测冲突问题，提出了矩阵版本的冲突消解算法。\n\n3. 提出了多路径交叉熵训练方法，通过加噪生成编辑训练数据，并设计了基于队列的统一编辑标签生成算法。\n\n4. 提出了阈值采样的强化学习训练方法，用于直接优化不可微分评估指标。\n\n5. 通过两阶段训练方法，即交叉熵训练和强化学习训练，有效训练了基于编辑的非自回归图像描述模型。\n\n清洗后的内容如下：\n\n---\n\n强化学习的目标是使J(θ)最大化，因此使用∇θJ(θ)的梯度进行梯度上升。也就是说，损失函数是-L(θ)，即：\n\nL(θ) = -E[logP(a_t|s_t)R(t)]",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 14,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 355,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c15",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "L(θ) = -E[logP(a_t|s_t)R(t)]\n\n其中，P(a_t|s_t)是每一步通过网络计算出的概率，而R(t)表示奖励越大，梯度越大。通过策略梯度算法，可以根据评估指标得到一个对梯度的估计，从而一定程度上解决了不能通过评估指标得到梯度的问题。在图像描述应用中，由于生成空间很大，根据策略梯度算法计算全部的T是不可能的，因此结合了蒙特卡洛采样的REINFORCE算法被提出并用于解决策略梯度的实际计算问题。REINFORCE算法是策略梯度算法的一种变形，将公式中的期望改写成了采样均值形式。REINFORCE算法利用了蒙特卡洛采样的思路。根据蒙特卡洛采样的理论，可以证明，在采样次数足够大时，REINFORCE算法和策略梯度算法拥有同样的期望。REINFORCE算法还额外地引入了一个基线b以降低强化学习训练中由于采样存在的高方差问题。本文提出了阈值采样强化学习策略，即在训练时过滤掉置信单词，只对非置信单词对应位置的结果应用策略梯度算法，计算梯度。提升训练的效率。\n\n---\n\n以上内容已去除页眉、页脚、页码、重复信息和乱码，保留了核心学术内容。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 15,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 481,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c16",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "---\n\n以上内容已去除页眉、页脚、页码、重复信息和乱码，保留了核心学术内容。\n\n表3-1报告了在Karpthy测试集上的离线评估结果，从表中可以看到，与自回归方法相比，本文提出的UniCap在保持可比较的性能的同时，实现了高达8倍的显著加速。UniCap的质量表现甚至好于一些近年来的自回归图像描述方法，如LSTNet。\n\n与非自回归方法相比，UniCap也实现了良好的推理速度，并在大多数指标上优于所有的非自回归基线。为了更直观地展示UniCap在速度-性能平衡上的优势，本文绘制了UniCap的CIDEi分数和加速率SpeedUp的二维可视化图以直观地展示整体性能。可以看出，UniCap在质量和速度之间实现了良好的平衡。\n\n3.3A2在线结果\n\n-2在线测试结果，表示在MSCOCO在线测试集上的性能比较。所有结果均在强化学习阶段后报告。符号t表示预训练方法。此外，c5和c40分别表示使用5个和40个参考字幕。\n\n非自回归图像描述方法用灰色背景表示。\n\nModel   B-3   B-4   METEOR   ROUGE_L   CIDEr   SpeedUp",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 16,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 486,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c17",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "非自回归图像描述方法用灰色背景表示。\n\nModel   B-3   B-4   METEOR   ROUGE_L   CIDEr   SpeedUp\n\nTransformer   81.6   96.0   66.4   90.8   51.8   82.7   39.7   72.8   29.4   39.0   59.2   74.8   129.3   132.1\n\nCMAL   79.5   94.3   63.8   87.2   48.8   77.2   36.8   66.1   27.9   36.4   57.6   72.0   119.3   121.2\n\nX Transformer   81.9   95.7   66.9   90.5   52.4   82.5   40.3   72.4   29.6   39.2   59.6   75.0   131.1   133.5\n\ntVinVL   81.9   96.9   66.9   92.4   52.6   84.7   40.4   74.9   30.6   40.8   60.4   76.8   134.7   138.7",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 17,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 516,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c18",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "DICT   82.4   96.6   67.4   91.7   52.8   83.8   40.6   74.0   29.8   39.6   59.8   75.3   133.4   135.4\n\nPureT   82.8   96.5   68.1   91.8   53.6   83.9   41.4   74.1   30.1   39.9   60.4   75.9   136.0   138.3\n\nPTSN   84.0   97.5   69.2   93.2   64.5   85.7   42.1   76.1   30.5   40.2   60.4   75.6   141.4   143.9\n\nDccCap   80.5   95.1   65.2   89.1   50.3   80.0   38.1   69.5   28.0   37.0   58.4   73.5   121.4   124.4\n\ntGIT   84.3   98.1   70.0   94.4   55.7   87.6   43.2   78.3   31.9   42.1   62.0   78.4   146.4   149.8",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 18,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 531,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c19",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "tOFA   84.5   98.1   70.1   94.4   55.9   87.8   43.6   78.7   32.1   42.7   62.5   79.0   147.2   149.6\n\nUAJC   81.9   96.3   66.5   91.1   51.8   83.0   39.6   72.9   29.2   38.9   59.2   74.7   129.0   132.8\n\nLSTNet   82.6   96.7   67.8   92.0   53.3   84.3   41.1   74.7   29.9   39.6   60.0   75.4   134.0   136.3\n\n-Nei   80.2   95.1   64.9   89.3   50.1   80.1   38.1   69.4   29.0   38.2   58.5   73.5   126.2   129.0\n\nUniCap   84.0   97.2   67.8   91.5   52.6   82.7   40.1   72.2   29.4   38.6   59.6   74.4   131.9   134.1",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 19,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 532,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c20",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "为了更公平地对比，本文也在MSCOCO的测试服务器上提交了在线测试结果，在线测试是在COCO未开放答案的数据集上运行的，可以更直观地反映出模型的性能。在线测试结果如表3-2所示，从中可以看出，UniCap在在线测试上的性能也优于大部分的自回归图像描述模型和全部的非自回归图像描述模型。由于在线服务器测评，无法衡量推理速度，因此推理速度并未在在线测试中报告。\n\n3.3.5消融分析\n\n为进一步检验提出的各种组件的有效性，本节将基于MSCOCO数据集对UniCap进行进一步的消融实验。\n\n3.3.5.1统一编辑操作\n\n为验证统一编辑操作的有效性，本文实现了先前的朴素编辑操作，包括位置的插入、删除以及词的预测三个解码器。并遵循同样设置进行了训练，结果如图3-9所示。根据分析，本文提出的UniCap相比朴素方法，使用两个解码器完成图像描述任务，速度上的收益在理论上应为朴素方法的33%。而根据实验结果，UniCap的加速率为8.02x，朴素方法的加速率为5.62x，则速度的收益约为29.84%，考虑到UniCap需要做一些冲突消解的操作，这个速度收益符合本文的预期。\n\n3.3.5.2训练策略\n\n在本节，本文对提出的两阶段训练策略的改进分别地和统一地进行了实验。\n\n表3-3展示了交叉熵阶段多路径组合实验结果。\n\n在交叉熵阶段，用不同的路径组合训练模型，结果如表3-3所示，可以观察到的结论包括：",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 20,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c21",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "表3-3展示了交叉熵阶段多路径组合实验结果。\n\n在交叉熵阶段，用不同的路径组合训练模型，结果如表3-3所示，可以观察到的结论包括：\n\n(1) 路径2显著地提升了模型的表现，这表明通过掩码预测器MP为模型生成更难的样本有助于提升模型的生成性能。\n\n(2) 路径3提升了模型的加速率，这表明通过加入一个专门的路径让模型学习减少不必要的操作是有效的。\n\n在强化学习阶段，本文进行了各种设置的实验，包括无强化学习(w/o RL)、强化学习(w/ common RL)和带有阈值采样的RL(w/ Threshold RL)。结果如表3-4所示。从设置4、5和6中，可以观察到阈值采样方法与多路径交叉熵训练方法结合在一起取得了最佳性能(相比无强化学习CIDEr+2.2，相比强化学习CIDEr+1.4)。从设置1、2和3中，可以观察到即使没有多路径交叉熵训练方法，本文提出的阈值采样方法在三个RL设置中仍然表现更好(相比无强化学习CIDEr+1.9，相比强化学习CIDEr+1.7)。\n\n表3-4展示了训练策略综合实验结果。\n\n同时，表3-4中展示的两阶段训练所有训练方法组合的结果表明，本文提出的两种方法的组合取得了最佳的表现。\n\n3.3.6案例分析",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 21,
    "chunk_index_in_section": 21,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 520,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c22",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "同时，表3-4中展示的两阶段训练所有训练方法组合的结果表明，本文提出的两种方法的组合取得了最佳的表现。\n\n3.3.6案例分析\n\n图3-10展示了本文从MSCOCO数据集抽样了一些图片并用UniCap推理，将推理过程、目标图像描述和自回归模型给出的结果展示。可以观察到，与自回归方法相比，本文提出的UniCap生成的图像描述质量更好，并且可以动态地修改已经输出的错误单词。例如，在案例1中，“dog”一词是不正确的。在案例2中，两个“street”单词重复。在案例3中，“blue train”和“white train”重复，且“white train”是不正确的。此外，在案例4中，“three group”一词与给定的图片不匹配。\n\n本章介绍了基于编辑的非自回归图像描述模型UniCap，由跨模态特征抽取器和两个文本解码器组成。训练策略包括交叉熵和强化学习阶段。在MSCOCO数据集上进行了大量实验，验证了UniCap的可行性和有效性。\n\n第四章提出了基于视觉提示词的可控非自回归图像描述方法ConUniCap，使用可学习的提示词控制图像风格。模型框架包括图像编码层、提示词机制和数据风格判别器。数据风格判别器包括描述详细度判别器、基于情感词典的正负情感判别器和基于预训练BERT的正负情感判别器。还提出了基于大语言模型和提示词生成可控图像描述数据的策略。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 22,
    "chunk_index_in_section": 22,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 580,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c23",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "模型时被填入模板内组成最终的提示词。对于图像描述数据集，每一个图片都对应有五个描述，在每次填入模板时对五个描述进行抽样，随机选择一个。\n\n4.2.4.1 数据标注样例\n\n为了直观理解本文基于大语言模型提示词的数据标注策略，提供数个标注样例如表4-1和表4-2所示。\n\n表4-1 大语言模型自动数据标注样例 - 正面\n\n| 大语言模型交互过程 |\n|------------------|\n| Q: Edit the sentence to make it positive without adding any additional objects or details not present in the original sentence: 'Two kids with helmets on sitting in the snow.' |\n| A: Two kids joyfully sitting on the snow with skis next to them. |\n\n表4-2 大语言模型自动数据标注样例 - 负面",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 23,
    "chunk_index_in_section": 23,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 467,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c24",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "表4-2 大语言模型自动数据标注样例 - 负面\n\n| 大语言模型交互过程 |\n|------------------|\n| Q: Edit the sentence to make it sad without adding any additional objects or details not present in the original sentence: 'A man in a baseball uniform throwing a ball.' |\n| A: A lonely man in a tattered baseball uniform, desperately throwing a ball. |\n\n4.2.4.2 数据验证策略\n\n在利用大语言模型生成图像描述后，由于大语言模型存在指令遵循失败或幻觉等现象，因此适当的验证是必须的。本文通过多种方式对大语言模型的输出进行了验证。验证主要包含幻觉验证和风格验证，整体的验证流程如图4-3所示。\n\n4.3 实验对比和分析\n\n4.3.1 实验配置",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 24,
    "chunk_index_in_section": 24,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 461,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c25",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "4.3 实验对比和分析\n\n4.3.1 实验配置\n\n对于ConUniCap，本文采用与第三章相同的MSCOCO数据集作为评估数据集，同样遵循了Karpthy等人对于COCO数据集划分的训练-验证集以保证成果的可复现性。硬件环境为Ubuntu22.04操作系统、NVIDIA A6000显卡、PyTorch深度学习框架、Python3.8版本。本文采取Dropout机制以缓解模型的过拟合问题，词嵌入层的丢弃率设置为0.9，Transformer块的丢弃率设置为0.7。所有的模型权重在初始化时遵循均匀分布，词嵌入的维度设置为512维，中间层的向量维度设置为512维。采用AdamW作为模型的优化器。学习率设置均与第三章中讨论的UniCap一致。\n\n4.3.2 基线方法\n\n为了对比生成的质量，本实验选取了近年的可控图像描述方法作为基线，这些方法简介如下：\n\n(1) LaBERT提出通过计算文本详略嵌入的方式控制生成文本的详细与否，并提出了基于双向语言模型的迭代细化非自回归图像描述方法，支持对详略程度的控制。\n\n(2) ConCap在自回归方法中首先提出了基于提示词的可控图像描述方法，利用提示词对不同风格的图像进行训练，最终得到可以控制图像描述风格的提示词。\n\n4.3.3 实验结果",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 25,
    "chunk_index_in_section": 25,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 542,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c26",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "如表4-3所示，本实验在Karpthy划分的测试集上评估ConUniCap的性能，对比了任务相似的LaBERT和ConCap两个可控图像描述模型，其中，LaBERT是基于嵌入的，ConCap和ConUniCap是基于提示词的。需要注意的ConCap是基于大规模多模态预训练模型BLIP2的，因此指标相对较高。通过性能的对比，可以得出ConUniCap在生成各种风格的描述时的表现部分好于先前的最先进可控模型LaBERT，与基于大规模预训练模型的ConCap相比达到了可比较的表现。同时，在速度的比较上，由于LaBERT模型是非自回归的，但是论文中没有给出具体的推理速度，为了大致地比较推理速度，本文从迭代步数估算了ConUniCap相对于LaBERT的速度优势：由于ConUniCap和LaBERT使用了相似的Transformer结构，这里首先假设执行一次前向的时间是相同的，设为t。LaBERT每步只执行一次前向传播，ConUniCap则每步执行两次(UE和MP分别需要一次前向传播，详见第三章对UniCap的生成策略的描述)。LaBERT设置了固定25步的迭代步数，因此共耗时25t，ConUniCap实现了生成时的动态退出机制，并设置了4步的步数上限，通过实验统计，平均步数约为2.5，因此ConUniCap的平均耗时约为5t，相比LaBERT可以达到五倍的加速\n。同时，由于在UniCap基础上实现的提示词机制并未对模型的结构进行改变，因此在整体的推理速度上，ConUniCap保持了UniCap的高效性。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 26,
    "chunk_index_in_section": 26,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 658,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c27",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "4.3.4 实验分析\n\n4.3.4.1 控制能力分析\n\n为了研究ConUniCap的控制能力，本文对ConUniCap进行风格控制的成功率进行了分析。ConUniCap共实现了5种风格，对于所有的测试集图片，利用ConUniCap的五种不同风格分别生成描述，并通过数据风格判别器判断输出描述的风格是否与输入的风格一致性通过精度衡量，即一致数量/总数。各个风格对应的精度如表4-4所示。\n\n表4-4 ConUniCap控制能力分析表\n\n| 风格 | 精度 |\n|------|------|\n| 短长度 | 99.66 |\n| 中等长度 | 95.16 |\n| 高长度 | 39.10 |\n| 正面情感 | 70.30 |\n| 负面情感 | 68.46 |\n\n4.3.4.2 生成式风格化数据消融分析\n\n为了证明生成的风格化数据的有效性，本文执行了生成式风格数据的消融实验。训练了一个没有加入额外风格化数据的ConUniCap，并与原始模型进行比较。结果如表4-5所示。实验结果表示额外的风格化数据提升了模型输出情感图像描述时的生成质量。由于成本问题，本文标注的数据仅为小规模的情感风格化数据，因此提升幅度有限，但是本文所提出的生成数据方法完全可以拓展到大规模数据集上，以获得更显著的提升效果。\n\n第11部分内容：",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 27,
    "chunk_index_in_section": 27,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 556,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c28",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "第11部分内容：\n\n4.3.5 案例分析\n本节提供了ConUniCap的一些生成案例，以建立对ConUniCap生成的优势和不足的直观认识。如图4-5所示。首先从详细度进行分析，在第一张图片中，三个不同详细度的描述以不同的详细度描述了图片，短长度的描述仅包括女人坐在卫生间；中等长度的描述则包括了女人具体坐的位置：地板；高长度的描述进一步地注意到女人的坐姿信息，并给出了最详细的描述。在第二张图片中同样可以注意到类似的效果，中等长度的描述额外注意到了叉子，高长度的描述则注意到了更细节的信息，包括蛋糕的种类和奶油。在第三张图片中，中等长度的描述注意到了办公桌，而高长度的描述则注意到了办公桌上的书籍。从情感控制的角度分析，样例展示了ConUniCap具有的控制描述情感能力，ConUniCap可以通过改变用词以保持句子的语义的同时改变句子的风格，如使用sad, lonely, poor等使句子变得偏向消极，使用happily, delicious, neatly等词使句子变得偏向积极。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 28,
    "chunk_index_in_section": 28,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 444,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c29",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "4.4 本章小结\n本章在图像描述方向开展了可控非自回归图像描述模型的研究，在第三章提出的UniCap的基础上提出了ConUniCap。为了实现可控的图像描述模型，ConUniCap中引入了基于提示词的风格控制机制，由于有着UniCap强大的推理速度作为基础，ConUniCap可以满足现实可控图像描述应用中对于推理速度的要求。随后，本章介绍了一种基于大语言模型生成式地训练可控图像描述的方法，并提出了基于双向预训练语言模型的数据风格判别方法，加强了对大语言模型生成数据的校验。最后，本章在MSCOCO图像描述数据集上进行了大量的实验和消融实验，从模型的性能、消融实验、案例分析等方面充分验证了ConUniCap方法的可行性和有效性。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 29,
    "chunk_index_in_section": 29,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 316,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c30",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "5.1 系统需求分析\n经过对提出算法的分析，非自回归图像描述系统主要包含以下需求：(1) 用户管理功能：对于一个系统而言，用户管理功能至关重要。对普通用户而言，系统支持注册和登录操作；而对于管理员，则提供了展示用户信息列表的功能。此外，为了优化使用体验，系统还设计了游客模式，允许用户在不登录的情况下直接访问。(2) 图像描述功能：该功能包含无条件的基于编辑的非自回归图像描述和可控非自回归图像描述，并可以通过用户的选择切换功能。包括切换可控非自回归图像描述中图像描述的风格。(3) 数据管理：在本系统中，会产生各种各样的用户数据，如用户上传的图片和用户的账号密码，因此需要将各种各样的数据存到数据库中，方便查看。(4) 方便部署：对于深度学习系统而言，部署需要环境配置，因此整个系统需要以镜像的方式提供。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 30,
    "chunk_index_in_section": 30,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 353,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c31",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "5.2 系统整体设计\n基于系统的需求分析，本系统的整体架构如图5-1所示，后端包括模型层，框架层，部署层三层封装。模型层主要负责封装基础的模型前向推理过程，供后续流程调用。框架层则负责调用flask和sqlite等库编写业务逻辑代码，实现API。框架层和模型层实现了整个后端的功能。部署层中，首先使用gunicorn包装flask应用，gunicorn是一个实现了WSGI协议的服务，可以提供多进程支持，提升多核服务器的处理性能。然后将整个gunicorn部署的后端程序打包为docker，支持多端的分发，使得整个系统具有可拓展性和可移植性。前端则使用Vue.js框架实现，Vue框架是一种流行的JavaScript框架，用于构建用户界面和单页应用程序。它的设计目标是通过尽可能简单的API实现响应的数据绑定和组合的视图组件。Vue的核心库只关注视图层，易于学习且集成。本系统的前后端采用分离的设计，后端经过包装，提供调用方便的API，然后前后端系统遵循预先设计的通信方式完成用户的请求。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 31,
    "chunk_index_in_section": 31,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 444,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c32",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "5.3 前端设计\n本系统的注册登录界面如图5-2所示，当用户首次进入系统时，可以选择登录或者注册。登录选项涉及到对数据库的查询，以确认用户是否已注册：如果用户已注册且密码匹配，则允许登录；如果用户存在但密码不匹配，则登录尝试将被拒绝；若未找到用户信息，则系统将自动进行用户注册。注册过程同样需要验证数据库中用户的存在性，仅当用户未被记录时，注册才能完成。一开始见到的界面如图5-3所示，左侧是图片预览区，右侧是操作区，下边是结果展示区。用户将会被要求点击选择文件按钮，选择一张本地的图片上传并预览，预览无误后，用户需要点击描述风格选单，选择所需要的风格，额外的，描述风格中有”选项，对应无条件的图像描述生成(UniCap)。最后，点击“请求图像描述”按钮，上传图像到服务端，服务端将处理用户的请求。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 32,
    "chunk_index_in_section": 32,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 349,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c33",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "5.4 后端设计\n后端系统主要实现的是数据的处理，模型前向过程的封装，以及数据的返回。数据处理方面，后端接收到前端发送的POST请求后，会将其中的请求字段取出，拿到用户提交的图片，转换为模型能够识别的形式，用于后续调用模型。模型前向过程的封装主要体现在对模型的输入输出的封装，首先，本文实现了两个模型：UniCap和ConUniCap，因此需要根据用户的选择将调用不同的模型。其次，由于前端支持可视化非自回归模型迭代的每一步，因此需要对每一步的中间结果进行保存，体现在最后的输出结果里，在前端展示每一步的编辑过程。数据返回的过程主要体现在需要将模型的输出转换为对API的响应，具体来说，前端页面期望的响应是json格式的数据，因此，需要将模型的输出首先通过分词器转换为文本形式，再将保留的中间步骤信息堆叠成为列表，最后将这些数据都转化为json格式，返回给前端页面渲染。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 33,
    "chunk_index_in_section": 33,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 385,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c34",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "5.5 系统工作流程\n本系统的工作流程可以总结为图5-5。首先，用户通过注册和登录操作进入系统，在注册阶段，用户的数据会被存入数据库中。在登录阶段，用户的数据从数据库中被取出进行校验。用户登录进入系统后，可以在前端与后端进行交互，当用户确定需求，发一个图像描述请求后，后端将会根据前端所提供的请求参数确定用户的需求类型，并将用户的请求分配给两个不同的模型，在模型底层，需要对用户输入的实际数据进行处理，使其变成模型可以前向推理的格式，随后模型进行前向推理，推理的结果经过封装返回上级框架并返回给前端并展示。完成一次图像描述过程。\n\n最终模型学到的是我们希望学到的内容。\n\n在非自回归的可控图像描述中，目前的实现方法是在一次推理中，不同的步之间保持一个风格提示词，因此也只支持同时接收一个风格信号。然而，一个多步之间接受不同风格信号的可控非自回归图像描述方法在理论上是可行的，但是需要在数据的生成和处理上做进一步的努力。\n\n参考文献：\n\n[1] 沈佳敏，鲍秉坤．基于深度学习的广告布局图片美学属性评价[J]．计算机技术与发展，2021，31(3)：39-44．\n\n[2] 徐守坤，倪楚涵，吉晨晨，等．基于YOLOV3的施工场景安全帽佩戴的图像描述[J]．计算机科学，2020，47(8)：233-240．",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 34,
    "chunk_index_in_section": 34,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 551,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c35",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "[2] 徐守坤，倪楚涵，吉晨晨，等．基于YOLOV3的施工场景安全帽佩戴的图像描述[J]．计算机科学，2020，47(8)：233-240．\n\n[3] 杨润霞，邵洁，罗岩，等．基于编解码器的电力施工场景可控图像字幕生成[J]．电网技术，2022，46(7)：2572-2581．\n\n[4] 陈悦，郭宇，谢圆琰，等．基于图像描述算法的离线盲人视觉辅助系统[J]．电信科学，2022，38(1)：61-72．\n\n[5] Bemardi R, Akici R, Elliot D, et al. Automatic description generation from images: a survey of models, datasets, and evaluation measures[J]. J Artif Intell Res, 2016, 55：409-442．\n\n[6] Xnyals O, Toshhev A, Bengio S, et al. Show and Tell: A neural image caption generator[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015：3156-3164．",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 35,
    "chunk_index_in_section": 35,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 576,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c36",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "[7] Szegedy C, Wei Liu, Yangqing Jia, et al. Going deeper with convolutions[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015：1-9．\n\n[8] Krizhevsky A, Sutskever I, Hinton G E. ImageNet classification with deep convolutional neural networks[J]. Advances in Neural Information Processing Systems, 2012，25：1097-1105．\n\n[9] Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition[J]. arXiv preprint arXiv:1409.1556，2014．",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 36,
    "chunk_index_in_section": 36,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 488,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c37",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "[10] He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016：770-778．\n\n[11] Xu K, Ba J L, Kiro R, et al. Show, attend and tell: neural image caption generation with visual attention[C]//Proceedings of the 32nd International Conference on Machine Learning. 2015：2048-2057．\n\n[12] Sugano Y, Bulling A. Seeing with humans: gaze-assisted neural image captioning[J]. arXiv preprint arXiv:1608.05203，2016．",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 37,
    "chunk_index_in_section": 37,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 504,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c38",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "[13] Anderson P, He X, Buehler C, et al. Bottom-up and top-down attention for image captioning and visual question answering[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018：6077-6086．\n\n[14] Yang X, Tang K, Zhang H, et al. Auto-encoding scene graphs for image captioning[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019：10685-10694．\n\n[15] Yao T, Pan Y, Li Y, et al. Hierarchy Parsing for image captioning[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019：2621-2629．",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 38,
    "chunk_index_in_section": 38,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 585,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c39",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "[16] Lu J, Xiong C, Parikh D, et al. Knowing when to look: adaptive attention via a visual sentinel for image captioning[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017：375-383．\n\n[17] Anel J, Deshpande A, Schwing A G. Convolutional image captioning[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018：5561-5570．\n\n[18] Vazwani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. Advances in Neural Information Processing Systems, 2017，30：5998-6008．",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 39,
    "chunk_index_in_section": 39,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 532,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c40",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "[19] Herdade S, Kappeler A, Booky K, et al. Image captioning: transforming object detection into words[J]. Advances in Neural Information Processing Systems, 2019，32：3231-3240．\n\n[20] Guo L, Liu J, Zhu X, et al. Normalized and geometry-aware self-attention network for image captioning[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020：10327-10336．\n\n[21] Comia M, Stefanini M, Baraldi L, et al. Meshed-Memory Transformer for image captioning[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020：10578-10587．",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 40,
    "chunk_index_in_section": 40,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 587,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c41",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "[22] Wang Z, Yu J, Yu A W, et al. SimvLM: Simple visual language model pretraining with weak supervision[J]. arXiv preprint arXiv:2108.10904，2021．\n\n[23] Wang J, Yang Z, Hu X, et al. Git: A generative image-to-text transformer for vision and language[J]. arXiv preprint arXiv:2205.14100，2022．\n\n[24] Li J, Li D, Savarese S, et al. BLIP: Bootstrapping language-image pre-training with frozen image encoders and large language models[C]//International Conference on Machine Learning. 2023：19730-19742．",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 41,
    "chunk_index_in_section": 41,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 497,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c42",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "[25] Wang P, Yang A, Men R, et al. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework[C]//International Conference on Machine Learning. 2022：23318-23340．\n\n[26] Ranzato M, Chopra S, Auli M, et al. Sequence level training with recurrent neural networks[J]. arXiv preprint arXiv:1606.02361，2016．\n\n[27] Ren S J, Marcheret E, Mroueh Y, et al. Self-Critical Sequence Training for Image Captioning[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017：7008-7024．",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 42,
    "chunk_index_in_section": 42,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 546,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c43",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "[28] Honda U, Watanabe %, Matsumoto Y. Switching to discriminative image captioning by releasing a bottleneck of reinforcement learning[C]//Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2023：1124-1134．\n\n[29] Gu J, Bradbury J, Xiong C, et al. Non-autoregressive neural machine translation[C]//6th International Conference on Learning Representations. 2018．\n\n[30] Fei Z. Fast image caption generation with positional alignment[J]. arXiv preprint arXiv:1912.06365，2019．",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 43,
    "chunk_index_in_section": 43,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 502,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c44",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "[31] Guo L, Liu J, Zhu X, et al. Non-Autoregressive Image Captioning with Counterfactuals - Critical Multi-Agent Learning[C]//Proceedings of the Twentieth International Joint Conference on Artificial Intelligence. 2021：767-773．\n\n[32] Gao J, Meng X, Wang S, et al. Masked non-autoregressive image captioning[J]. arXiv preprint arXiv:1906.00717，2019．\n\n[33] Fei Z. Iterative back modification for faster image captioning[C]//Proceedings of the 28th ACM International Conference on Multimedia. 2020：3182-3190．",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 44,
    "chunk_index_in_section": 44,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 505,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c45",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "[34] Fei Z, Fan M, Zhu L, et al. Uncertainty-aware image captioning[C]//AAAI Conference on Artificial Intelligence. 2023，37(1)：614-622．\n\n[35] Ho J, Jain A, Abbeel P. Denoising diffusion probabilistic models[J]. Advances in Neural Information Processing Systems, 2020，33：6840-6851．\n\n[36] Li Y, Zhou K, Zhao W X, et al. Diffusion models for non-autoregressive text generation: a survey[J]. arXiv preprint arXiv:2303.06574，2023．",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 45,
    "chunk_index_in_section": 45,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 425,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c46",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "[37] Chen T, Zhang R, Hinton G. Analog Bits: Generating discrete data using diffusion models with self-conditioning[C]//The Eleventh International Conference on Learning Representations. 2022．\n\n[38] Luoj J, Li Y, Pan Y, et al. Semantic-Conditional diffusion networks for image captioning[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2023：23359-23368．\n\n[39] Mathews A, Xie L, He X. Senticap: Generating image descriptions with sentiments[C]//Proceedings of the AAAI conference on artificial intelligence. 2016，30(1)．",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 46,
    "chunk_index_in_section": 46,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 553,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c47",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "[40] Comia M, Baraldi L, Cucchiarra S. Show, Control and Tell: A framework for generating controllable and grounded captions[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016：1-10．\n\nVision and Pattern Recognition. 2019: 8307-8316.\n\n[41] Deng C, Ding N, Tan M, et al. Length-controllable image capturing [C]//European Conference on Computer Vision. 2020: 712-729.\n\n[42] Wang N, Xie J, Wu J, et al. Controllable image capturing via prompting [C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2023: 2617-2625.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 47,
    "chunk_index_in_section": 47,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 562,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c48",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "[43] Zhang W, Shi H, Guo J, et al. Magic: Multimodal relational graph adversarial inference for diverse and unpaired text-based image capturing [C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2022: 3335-3343.\n\n[44] Hironaka Y, Nakashima Y, Garcia N. Model-agnostic gender debiased image capturing [C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2023: 15191-15200.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 48,
    "chunk_index_in_section": 48,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 419,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c49",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "[45] Qiu H, Dou ZY, Wang T, et al. Gender biases in automatic evaluation metrics for image capturing [C]//The 2023 Conference on Empirical Methods in Natural Language Processing. 2023.\n\n[46] Zhao D, Wang A, Russakovsky O. Understanding and evaluating racial biases in image capturing [C]//Proceedings of the IEEE International Conference on Computer Vision. 2021: 14830-14840.\n\n[47] Abdelrahman E, Sun P, Li LE, et al. ImageCap: Image capturing bias amplification assessment [C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2024, 38(19): 20902-20911.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 49,
    "chunk_index_in_section": 49,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 569,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c50",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "[48] Gu J, Wang C, Zhao J. Levenshtein transformer [J]. Advances in Neural Information Processing Systems, 2019, 32.\n\n[49] Xu W, Carpuat M. Editor: An edit-based transformer with repositioning for neural machine translation with soft lexical constraints [J]. Transactions of the Association for Computational Linguistics, 2021, 9: 311-328.\n\n[50] Glorot X, Bordes A, Bengio Y. Deep sparse rectifier neural networks [C]//Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics. 2011.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 50,
    "chunk_index_in_section": 50,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 522,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c51",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "[51] LeCun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition [J]. Proceedings of the IEEE, 1998, 86(11): 2278-2324.\n\n[52] Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift [C]//International Conference on Machine Learning. 2015: 448-456.\n\n[53] Dosovitskiy A, Beyer L, Kolesnikov A, et al. An image is worth 16x16 words: Transformers for image recognition at scale [J]. arXiv preprint arXiv:2010.11929, 2020.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 51,
    "chunk_index_in_section": 51,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 506,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c52",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "[54] Liu Z, Lin Y, Cao Y, et al. Swin transformer: Hierarchical vision transformer using shifted windows [C]//Proceedings of the IEEE International Conference on Computer Vision. 2021: 10012-10022.\n\n[55] Girshick R, Donahue J, Darrell T, et al. Rich feature hierarchies for accurate object detection and semantic segmentation [C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2014: 580-587.\n\n[56] Girshick R. Fast R-CNN [C]//Proceedings of the IEEE International Conference on Computer Vision. 2015: 1440-1448.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 52,
    "chunk_index_in_section": 52,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 544,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c53",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "[57] Ren S, He K, Girshick R, et al. Faster R-CNN: Towards real-time object detection with region proposal networks [J]. Advances in Neural Information Processing Systems, 2015, 28.\n\n[58] Krishna R, Zhu Y, Groth O, et al. Visual genome: Connecting language and vision using crowd-sourced dense image annotations [J]. International Journal of Computer Vision, 2017, 123: 32-73.\n\n[59] Carion N, Massa F, Synnaeve G, et al. End-to-end object detection with transformers [C]//European Conference on Computer Vision. 2020: 213-229.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 53,
    "chunk_index_in_section": 53,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 526,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c54",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "[60] Papineni K, Roukos S, Ward T, et al. BLEU: A method for automatic evaluation of machine translation [C]//Proceedings of the 40th annual meeting of the Association for Computational Linguistics. 2002: 311-318.\n\n[61] Lin CY. Rouge: A package for automatic evaluation of summaries [C]//Text Summarization Branches Out. 2004: 74-81.\n\n[62] Banerjee S, Lavie A. Meteor: An automatic metric for evaluation with improved correlation with human judgments [C]//Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. 2005: 65-72.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 54,
    "chunk_index_in_section": 54,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 593,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c55",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "[63] Vedantam R, Zitnick CL, Parikh D. CIDEr: Consensus-based image description evaluation [C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015: 4566-4575.\n\n[64] Sutton RS, McAllister D, Singh S, et al. Policy gradient methods for reinforcement learning with function approximation [J]. Advances in Neural Information Processing Systems, 1999, 12.\n\n[65] Williams RJ. Simple statistical gradient-following algorithms for connectionist reinforcement learning [J]. Machine Learning, 1992, 8(3): 229-256.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 55,
    "chunk_index_in_section": 55,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 536,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c56",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "[66] Lin TY, Maire M, Belongie S, et al. Microsoft COCO: Common objects in context [C]//European Conference on Computer Vision. 2014: 740-755.\n\n[67] Karpathy A, Fei-Fei L. Deep visual-semantic alignments for generating image descriptions [C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015: 3128-3137.\n\n[68] Sumanuma M, Okatani T. GRI\n\nImage captioning [C] // CCF International Conference on Natural Language Processing and Chinese Computing. 2023: 469-481.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 56,
    "chunk_index_in_section": 56,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 494,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "基于非自回归方法的图像描述研究及应用_张煜松_s0_c57",
    "source_id": "基于非自回归方法的图像描述研究及应用_张煜松",
    "text": "[80] Devlin J, Chang MW, Lee K, et al. BERT: Pre-training of deep bidirectional transformers for language understanding [C] // Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics. 2019: 4171-4186.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 57,
    "chunk_index_in_section": 57,
    "total_chunks_in_section": 58,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 258,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c0",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "互联网上众多平台包含大量的文本，其蕴含的情感信息具有重要的商业价值以及参考价值。与传统的粗粒度情感分析不同，细粒度情感分析可以提取文本中方面词、意见词等更具体的信息。本文聚焦于两大前沿多元组细粒度情感分析任务：方面级三元组抽取(Aspect Sentiment Triplet Extraction, ASTE)以及结构化情感分析(Structured Sentiment Analysis, SSA)。本文主要研究内容如下：",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 91,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 213,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9900000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c1",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "一、在方面级三元组抽取任务中，最近相关研究工作采用机器阅读理解架构通过多轮询问得到方面词以及对应的意见词和情感，并取得了令人印象深刻的结果。然而这些方法在一句话包含多个方面词时会遇到其他方面词造成干扰这一问题，从而不能准确地鉴别各个方面词对应的情感信息。为克服上述挑战，本文提出一种基于掩码上下文的机器阅读理解(Context-based Masking Machine Reading Comprehension, COM-MRC)框架。该方法包含三个部分：掩码式数据增强、交互式判别模型以及阶段式推理方法。三部分协同工作，实验结果表明，COM-MRC取得了先进的性能，验证了方法的有效性。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 296,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c2",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "二、在结构化情感分析任务中，最近相关的研究工作通常将其转化为双词汇依赖解析问题。然而因为该任务存在重叠实体以及非连续实体问题，故这些转化方法都是不等价的。为克服上述挑战，本文介绍一种处理重叠实体以及非连续实体的双词汇依赖解析方法。该方法分为两种类型的解析边：关系预测以及单词提取，分别对应解决重叠实体以及非连续实体问题。并且将双词汇依赖解析方法转化为统一的2D表格填充机制，称为USSA机制。该机制将表格划分为下半三角以及上半三角，分别对应关系预测以及单词提取。最后设计一个模型以适配USSA机制，该模型中的双轴向注意力机制可以捕捉表格中关系类型的行列关联信息，以进行更准确的识别。实验结果表明，USSA取得了先进的性能，验证了方法的有效性。\n\n三、基于对方面级三元组抽取和结构化情感分析任务的研究，本文开发了一个细粒度情感分析系统。该系统支持注册登录、用户管理、在线处理等多个功能，简单易用。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 397,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c3",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "关键词：深度学习，方面级情感分析，结构化情感分析，注意力机制\n\n情感分析从粒度上可分为粗粒度与细粒度。粗粒度情感分析包括篇章级和句子级情感分析，旨在判断整体情感极性。细粒度情感分析则关注方面词、意见词和情感等多个元素，常见任务有方面意见词联合抽取、方面情感对抽取、方面级三元组抽取、意见词挖掘和结构化情感分析。本文选取方面级三元组抽取和结构化情感分析进行研究。方面级三元组抽取旨在抽取方面词、意见词和情感的三元组，而结构化情感分析则包含情感持有者抽取、情感目标抽取、情感表达抽取和情感极性判定等子任务。本文主要研究了机器阅读理解框架和端到端框架在这两个任务上的应用。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 283,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c4",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "尽管前沿的机器阅读理解方法表现令人印象深刻，但当一句话同时存在多个方面词时，这些方法会面临严重的其他方面词带来的干扰问题。首先从直觉来看，一句话包含的方面词数量越多，句子中包含的情感信息就越丰富，方面词对应的意见词和情感也通常就越难判别。如图1.2所示，当句子中同时存在两个“ambience place”时，模型可能会错误地将“overrated”视为“ambience”对应的意见词。而且，训练后的模型因为其内部的transformer架构自带注意力机制，模型会潜在地捕捉方面词与意见词的对应关系。如果模型注意到其他的方面词，那么也就会关注到这些方面词对应的意见词，进而对目前方面词的意见词推理造成干扰。如图1.2所示，对于方面词“ambience”，如果“place”仍存在，模型可能会关注到错误的意见词“overrated”上",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 367,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c5",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "。需要注意的是表1-1显示多方面词句子在整体中占比很多，这些句子包含的三元组数量更是占比接近一半，因此如何从多方面词句子中排除其他方面词的干扰以更准确地鉴别情感信息是非常有意义的。基于以上的观察，本文提出了掩码一核心思想。掩码方面词不仅可以直接去除对无关方面词的注意力，而且基于此可以进行简单有效的数据增强，这恰好又符合“一句话包含方面词数量越多，所以情感信息量越大，所以应该将这句话增强为多个训练样本”这个直觉。为此，本文提出了一种基于掩码上下文的机器阅读理解(Context-Masked machine reading comprehension, COM-MRC)框架，该方法打破了传统机器阅读“不同的查询、相同的上下文”理念，转而采用“相同的查询，不同”并实现了先进的效果。该框架包括掩码式数据增强、交互式判别模型以及阶段式推理方法三个部分",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 5,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 375,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c6",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "。该框架包括掩码式数据增强、交互式判别模型以及阶段式推理方法三个部分。在掩码式数据增强中，本文针对原始输入句子中每个方面词都会考虑是否掩码，故假设一句话包含t个方面词，数据增强后变为^个句子。将这些增强后的句子作为上下文，并设置固定的一句话作为查询，目的是找到该句中首个方面词以及对应的意见词和情感，因此模型会减少无关方面词带来的干扰，更好地分辨来自不同方面词的情感信息。在交互式判别模型中，为了更加充分地利用三元组个元素的关联关系，本文设计了四个模块，并且允许各模块之间交流信息。这四个模块分别是方面词提取模块、意见词提取模块、情感判别模型以及方面词探测模块，其中最后一个模块是为了探测掩码上下文中是否存在方面词。在阶段式推理方法中，本文设计了两个阶段，分别为方面词推理阶段以及方面词附属物推理阶段",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 6,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 350,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c7",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "。在阶段式推理方法中，本文设计了两个阶段，分别为方面词推理阶段以及方面词附属物推理阶段。在方面词推理阶段中，通过逐步掩码方面词将所有的方面词提取出来；在方面词附属物推理阶段中，所有无关的方面词都会被掩码以排除它们带来的干扰，由此可以针对每个提取出来的方面词识别对应的意见词和情感；最后综合两个阶段形成三元组。在基准数据集上的大量实验结果证明了本文提出的COM-MRC框架的有效性。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 7,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 190,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c8",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "本文在方面级情感分析任务中的研究内容总结如下：\n\n1）首次提出利用掩码方面词的思想进行简单有效的数据增强，提供了一种与传统机器阅读理解思路不同的、用于解决多方面词干扰问题的研究新视角。\n\n2）提出完整的基于掩码上下文的机器阅读理解框架，该框架包括掩码式数据增强、交互式判别模型以及阶段式推理方法三个部分。\n\n3）在基准数据集上做了大量实验，结果表明了COM-MRC方法的有效性。\n\n门控循环单元（Gated Recurrent Unit，GRU）\n\nGRU是一种类似于LSTM的循环神经网络结构，由两个门控单元和一个更新门组成，可以有效地解决长期依赖问题，并且具有比LSTM更少的参数和计算量，是对LSTM的一种简化和改进。GRU只使用一个记忆单元来存储过去的状态信息，并使用两个门控单元，即Reset Gate和Update Gate来控制记忆单元的更新和信息的流动。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 8,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 385,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c9",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "卷积神经网络（Convolutional Neural Network，CNN）\n\nCNN是一种可以自动提取图像、音频、文本等数据的特征的神经网络结构，具有优秀的特征提取和分类能力，是计算机视觉和自然语言处理领域最常用的模型之一。CNN的基本组成部分包括卷积层、池化层和全连接层。卷积层通过卷积操作提取输入数据的特征，池化层用于降低卷积层的输出数据的空间尺寸，减少网络参数，全连接层用于将卷积层和池化层的输出特征映射到分类结果。\n\n注意力机制（Attention）",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 9,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 232,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c10",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "注意力机制（Attention）\n\n注意力机制是一种可以根据输入的序列自适应地给不同部分分配不同的权重的机制，常用于序列到序列（Seq2Seq）模型和自然语言处理任务中，能够有效提升模型的表现。注意力机制可以视为一种加权求和操作，它会为输入序列中的每个元素分配一个权重，然后利用这些权重对输入序列中的不同部分进行加权求和。常见的注意力机制包括点积注意力（Dot-Product Attention）、加性注意力（Additive Attention）、乘性注意力（Multiplicative Attention）等。\n\nTransformer",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 10,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 272,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c11",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "Transformer\n\nTransformer是一种基于自注意力机制的神经网络模型，由Google在2017年提出，用于处理自然语言处理任务，如机器翻译、文本生成等。其核心思想是将自注意力机制和前馈神经网络结合在一起。自注意力机制用于学习输入序列中不同位置的依赖关系，前馈神经网络用于学习特征之间的非线性关系。Transformer模型中还采用了残差连接和层归一化等技术，进一步提高模型的表现。\n\n一种算法。常见的优化器有随机梯度下降(SGD)、Adam(Adaptive Moment Estimation)、AdamW等。本节主要介绍实验中涉及到的Adam以及AdamW优化器。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 11,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 292,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c12",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "Adam优化器是一种自适应学习率的优化算法，可以根据梯度的一阶矩估计和二阶矩估计来自适应地调整学习率。它在处理稀疏梯度时表现良好，同时也适用于大规模数据集和深层网络。其是通过计算梯度的一阶矩估计和二阶矩估计来更新模型参数，定义为：\n\nmt = β1 * mt-1 + (1 - β1) * gt  \nvt = β2 * vt-1 + (1 - β2) * gt^2  \nθt+1 = θt - α * mt / (sqrt(vt) + ε)\n\n其中，gt表示梯度，mt和vt分别表示梯度的一阶矩估计和二阶矩估计，β1和β2分别是一阶矩和二阶矩的衰减系数，ε是一个很小的常数，用于避免分母为零。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 12,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 296,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c13",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "在实际应用中，Adam优化器可以自适应地调整学习率，同时也适用于大规模数据集和深层网络，因此得到了广泛的应用。需要注意的是，在使用Adam优化器时，需要选择合适的学习率和衰减系数，以达到最好的训练效果。\n\nAdamW是Adam优化器的一种变体，相较于Adam优化器，它添加了权重衰减(WeightDecay)的正则化项，用于避免模型过度拟合训练集。在Adam优化器中，权重衰减是通过在损失函数中添加正则化项来实现的，但是这种方式会使得学习率对于不同的参数具有不同的影响，导致训练结果不稳定。所以AdamW优化器在Adam优化器的基础上，将权重衰减移到了更新步骤中，以使学习率对于所有参数都有相同的影响。AdamW优化器定义为：\n\nθt+1 = θt - α * mt / (sqrt(vt) + ε) - α * λ * θt",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 13,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 363,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c14",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "θt+1 = θt - α * mt / (sqrt(vt) + ε) - α * λ * θt\n\n其中，λ表示权重衰减系数，它可以控制正则化的强度。相比于Adam优化器，AdamW优化器能够更好地避免模型过度拟合，尤其是在大规模数据集和深层网络中表现更加优异。因此，AdamW优化器在深度学习的实践中得到了广泛的应用。\n\n交互式判别模型分为四个模块：方面词提取模块、意见词提取模块、情感判别模块和方面词探测模块。方面词提取模块使用交叉熵损失函数，意见词提取模块同样使用交叉熵损失函数。情感判别模块使用多头注意力机制融合信息，方面词探测模块使用二元交叉熵损失。整体优化目标是四个模块损失的最小化。阶段式推理方法包含方面词推理和方面词附属物推理两个阶段，以缓解其他方面词的干扰。实验在两组ABSA数据集上进行，评估指标包括精确率、召回率和F1。基线方法包括阶段式、端到端式和机器阅读理解式模型。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 14,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 396,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c15",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "第7部分内容：\n\n4.9) 是一种基于双仿射评分器的多任务学习框架，以缓和之前方法在提取方面-意见词对时缺乏情感极性为参照的问题，其可以联合抽取方面词、意见词以及情感极性。\n\n9) JET-BERT [4?1]提出具有位置感知标记方案的端到端模型以联合提取三元组，其中具有位置感知的标记方案是一种融入位置信息的序列标注方法。\n\n3.5) 与GTS相近，同样利用二维网格标注的模式来端到端的抽取三元组，其还利用图神经网络编码词与词之间的语法或语义关系。\n\n12) BMRc [53]一种机器阅读理解方法，其包括两种顺序的提问，先询问方面词再询问意见词，或先询问意见词再询问方面词，综合考虑两个方向的询问结果最后生成三元组。\n\n13) Unified [51＾]ASTE任务转化为生成式任务，并利用BART生成式模型作为基底模型进行实验。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 15,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 367,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c16",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "14) SPAN-ASTE [923]是一种基于span标注的方法，其会枚举所有的单词作为开始或结束位置形成span，然后对span进行预测与配对。\n\n15) EMC-GCN [93]一种加强多通道的图神经网络，其首先设计一种二维的网格标注方案，然后利用该图神经网络对词与词之间的关系进行预测，并利用到词性、语法等语言学信息进行特征加强。\n\n3.3.5实验结果\n\n表3-3和表3-4展示了在仏数据集上的评测结果，表3-5和表3-6展示了在?2数据集上的评测结果。\n\n表3-3 在仏数据集中Rest14以及Lap14的评测结果对比\n\n表3-4 在仏数据集中Rest15以及Rest16的评测结果对比\n\n表3-5 在?2数据集中Rest14以及Lap14的评测结果对比\n\n表3-6 在?2数据集中Rest15以及Rest16的评测结果对比",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 16,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 367,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c17",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "表3-6 在?2数据集中Rest15以及Rest16的评测结果对比\n\n可以看到在：01和：02数据集，基于FI指标，本文的COM-MRC全面超越了所有阶段式、端到端式以及MRC式的方法。值得注意的是在：02数据集上，本文框架同样超越了现有最好的端到端式方法SPAN-ASTE。可以看出相对于阶段式方法，端到端式方法和MRC式方法是更具竞争力的，这是因为他们减缓了错误传播问题并且构建了子任务之间的关联性。更进一步来说，对比另外一个强有力的MRC式方法，即BMRc，本文所提出的COM-MRC方法在仏和?2数据集上分别超越了其3.59%以及4.18%的F1指标。该提升归功于本文的COM-MRC可以通过上下文增强策略、判别式模型以及推理方法有效地缓解千扰问题。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 17,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 329,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c18",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "此外，为了体现本研究实验结果的可信性，本文针对F1指标做了t检验，该检验是在%和2）2数据集上对比COM-MRC于BMRc以及EMC-GCN而完成，所有的p值均小于0.05，证明了对应实验结果具有统计学显著性。\n\n3.6实验分析\n\n为了进一步验证COM-MRC框架的有效性，本节分别从上下文增强策略、判别式模型、推理算法、查询的设置、注意力可视化和案例研究进行了定性或定量分析。\n\n3.3.6.1对上下文增强策略的定量分析",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 18,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 211,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c19",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "3.3.6.1对上下文增强策略的定量分析\n\n本节将应用于COM-MRC的指数级数据增强策略与另外两个策略，即线性级策略和空策略进行了对比。在COM-MRC框架中的模型与推理方法不变的情况下，分别应用以上三种上下文增强策略进行对比实验，指标F1结果以及样本数量如表3-7所示。可以看出对比线性级策略以及空策略，指数级策略实现了显著的性能提升。总的来说，样本数量越多，模型表现越好，但指数级策略对比空策略平均增加约2.5倍的样本量，并不会带来过多的计算负担。\n\n3.3.6.2对判别式模型的定量分析\n\n为验证判别式模型各模块的有效性，本研宄基于D2数据集对各模块进行了消融研究。结果显示所有模块对于该模型在ASTE任务的性能表现均有贡献。\n\n3.3.6.3对推理算法的分析",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 19,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 334,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c20",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "3.3.6.3对推理算法的分析\n\n为验证COM-MRC框架中推理方法的有效性，本文展现了两种不同的推理方法，这两种推理方法包含了相同的方面词推理阶段和不同的方面词附属物推理阶段，并将这两种不同的方面词附属物推理阶段命名为AAI1和AAI2。结果显示AAI2可以有效地缓解方面词之间的干扰问题。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 20,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 146,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c21",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "基于掩码上下文机器阅读理解框架的方面级三元组抽取方法研究工作在本章中展开。首先介绍了掩码上下文机器阅读理解框架，包括掩码式数据增强、交互式判别模型和阶段式推理方法。随后，介绍了实验的基准数据集、实验参数与设置、评估指标、基线方法和实验结果，验证了方法的可行性及有效性。最后，对上下文增强方法、判别式模型模块、推理算法以及查询进行了定量分析，并进行了注意力可视化和案例研究，从多方面验证了方法中各部分的有效性，证明了其他方面词会带来干扰问题，而本章提出的方法有效地缓解了该问题，进一步验证了方法的完备性及可行性。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 21,
    "chunk_index_in_section": 21,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 255,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c22",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "对于结构化情感分析任务，现有方法无法识别重叠和非连续的实体。为解决这一问题，提出了一种新颖的双词汇依赖解析图，包含关系预测(RP)和单词提取(TE)两种有向边。RP边用于处理实体边界和实体间关系，解决重叠问题；TE边用于提取给定边界内的所有token，解决非连续问题。将依赖解析图转化为2D表格填充机制，命名为统一结构化情感分析(UNSA)。基于UNSA，提出了一种模型架构，包括编码层、词对表示层、细化策略和预测层。编码层使用多语言BERT和BiLSTM作为语义编码器。词对表示层使用条件层归一化建模词对表示。细化策略中，提出双轴注意力模块捕捉横纵坐标上的关联信息。最后，预测层用于决定词对关系类型。实验证明，该方法在多个基准数据集上取得了先进性能。\n\n双轴注意力模块用于捕获关系之间的关联性并确保全局连接。首先定义单向注意力如下：",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 22,
    "chunk_index_in_section": 22,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 367,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c23",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "双轴注意力模块用于捕获关系之间的关联性并确保全局连接。首先定义单向注意力如下：\n\na_Uj = MultiHead{nj, row f, row j} + (4 - 5)\n\n然后利用对称的轴向注意力及词对本身的表示构造语义表示C：\n\ncu = aU_nj / , i (4 - 6)\n\n为加强表示，引入相对距离特征，得到最终表示V：\n\nvi,j = ci,j * di-j (4 - 7)\n\n预测层将细化表示V输入至前馈神经网络(FFN)和双仿射预测头(biaffine predictor)。\n\nFFN预测头得到关系预测分值：\n\ns{j = FFN(Vy) (4 - 8)\n\nBiaffine预测头利用Biaffine模块得到关系分值：\n\nhj = FNNa\n\nhj = FNNb(hj) (4 - 9)\n\n最后的关系标签概率分布由两部分计算得到：",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 23,
    "chunk_index_in_section": 23,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 375,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c24",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "hj = FNNb(hj) (4 - 9)\n\n最后的关系标签概率分布由两部分计算得到：\n\nPij = softmax(as + (1 - a)(4 - 10))\n\n损失函数是最小化交叉熵损失：\n\nLoss = -ΣNΣN KVij log(Pij) (4 - 11)\n\n其中W是句子中token的数量，R是USSA预定义的关系集合。\n\n实验对比和分析部分介绍了数据集、实验参数设置、评估指标、基线方法等。实验结果显示，本文提出的USSA方法在多个指标上优于其他基线模型，尤其在处理重叠实体和非连续实体方面表现更好。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 24,
    "chunk_index_in_section": 24,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 257,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c25",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "本文介绍了基于掩码上下文机器阅读理解框架的方面级三元组抽取和基于统一表格填充机制端到端框架的结构化情感分析方法。方面级三元组抽取旨在提取句子中的(方面词、意见词、情感极性)三元组，而结构化情感分析旨在提取句子中的(持有者、目标、表达、极性)四元组。实验结果表明，所提出的方法在多个数据集上取得了较好的性能。此外，还设计了一个文本细粒度情感分析系统，该系统包含方面级三元组抽取和结构化情感分析两个功能，并采用前后端分离的技术实现。前端使用Flutter框架，后端使用Flask框架，并采用SQLite数据库存储用户信息和操作历史。整体而言，本文在方面级三元组抽取和结构化情感分析两个任务上取得了较好的研究成果，并设计了一个实用的文本细粒度情感分析系统。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 25,
    "chunk_index_in_section": 25,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 325,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c26",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "分别设计了掩码式数据增强、交互式判别模型以及阶段式推理方法来分别解决以上问题，统称为基于掩码上下文的机器阅读理解（Contexted Masked Machine Reading Comprehension，COM-MRC）框架。掩码式数据增强是通过设置固定的查询以及掩码方面词后的上下文进行简单有效的数据增强；交互式判别模型包括方面词提取模块、意见词提取模块、情感判别模块以及方面词探测模块，并允许各模块之间交互信息；阶段式推理方法首先推理方面词，然后推理方面词对应的意见词和情感，推理时均通过掩码方面词进行，从而可以减少无关方面词的干扰。实验在两组ABSA数据集上进行，实验结果证明了COM-MRC框架的有效性。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 26,
    "chunk_index_in_section": 26,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 308,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c27",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "结构化情感分析任务旨在提取句子中的（持有者、目标、表达、极性）四元组，现有的深度学习方法主要是将该任务视为双词汇依赖解析问题，然而这些方法并不能同时处理重叠以及非连续问题。本文首先构造了一种双词汇依赖解析图，该图包含两类有向边，即“关系预测”以及“单词提取”，它们分别对应解决了重叠和非连续问题。然后本文将双词汇依赖解析转化为统一的表格填充机制，命名为USSA（Unified Table Filling Scheme for Structured Sentiment Analysis）。在该表格填充机制里，RP与TE分别对应表格的左下三角部分以及右上三角部分。最后，为了很好地适配USSA，本文构造了一个端到端模型进行训练和预测。在该模型中，本文提出了一种双轴注意力模块去有效地捕捉表格中的行与列的关联信息",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 27,
    "chunk_index_in_section": 27,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 355,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c28",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "。在该模型中，本文提出了一种双轴注意力模块去有效地捕捉表格中的行与列的关联信息。实验在NoReCFine、MultiBEU、MultiBcA、MPQA以及08触五个基准测试集上进行，实验结果证明了该方法的有效性。",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 28,
    "chunk_index_in_section": 28,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 106,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c29",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "－ １８ １ ．Ａｓｓｏｃｉａｔｉｏｎｆｏｒ  \n\nＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ，１９９７ ．  \n\n［２４］Ｌｉｕ ，Ｂ ．ＳｅｎｔｉｍｅｎｔＡｎａｌｙｓｉｓａｎｄＯｐ ｉｎｉｏｎＭｉｎｉｎｇ\n\n．ＳｙｎｔｈｅｓｉｓＬｅｃｔｕｒｅｓｏｎＨｕｍａｎ  \n\nＬａｎｇｕａｇｅＴｅｃｈｎｏｌｏｇ ｉｅｓ ．Ｍｏｒｇａｎ＆ＣｌａｙｐｏｏｌＰｕｂｌｉｓｈｅｒｓ，２０１２ ．  \n\n［２５］Ｌｉｕ，Ｂ ．Ｓｅｎｔｉｍｅｎｔａｎａｌｙｓｉｓａｎｄｓｕｂｊｅｃｔｉｖｉｔｙ\n\n．ＩｎＩｎｄｕｒｋｈｙａ，Ｎ ．ａｎｄＤａｍｅｒａｎ，Ｆ．Ｊ ．  \n\n（ｅｄｓ ．）５ＨａｎｄｂｏｏｋｏｆＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ，ＳｅｃｏｎｄＥｄｉｔｉｏｎ ，ｐｐ",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 29,
    "chunk_index_in_section": 29,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 391,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c30",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "．６２７ －６６６．  \n\nＣｈａｐｍａｎａｎｄＨａｌｌ／ＣＲＣ ５２０１０．  \n\n［２６ ］Ｐａｎｇ，Ｂ ．ａｎｄＬｅｅ ，Ｌ ．Ａｓｅｎｔｉｍｅｎｔａｌｅｄｕｃａｔｉｏｎ：Ｓｅｎｔｉｍｅｎｔａｎａｌｙｓｉｓｕｓｉｎｇｓｕｂ ｊｅｃｔｉｖｉｔ ｙ  \n\nｓｕｍｍａｒｉｚａｔｉｏｎｂａｓｅｄｏｎｍｉｎｉｍｕｍｃｕｔｓ ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ４２ｎｄａｎｎｕａｌｍｅｅｔｉｎｇ  \n\nｏｎＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ，ｐｐ\n\n．２７１ ．Ａｓｓｏｃｉａｔｉｏｎｆｏｒ  \n\nＣｏｍｐｕｔａｔ ｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ，２００４ ．  ",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 30,
    "chunk_index_in_section": 30,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 354,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c31",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "Ｃｏｍｐｕｔａｔ ｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ，２００４ ．  \n\n［２７ ］Ｇｌｏｒｏｔ ，Ｘ ． ，Ｂｏｒｄｅｓ，Ａ ． ，ａｎｄＢｅｎｇ ｉｏ ，Ｙ．Ｄｏｍａｉｎａｄａｐｔａｔｉｏｎｆｏｒｌａｒｇｅ －ｓｃａｌｅｓｅｎｔｉｍｅｎｔ  \n\nｃｌａｓｓｉｆｉｃａｔｉｏｎ；Ａｄｅｅｐ ｌｅａｒｎｉｎｇａｐｐｒｏａｃｈ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２８ｔ ｆｉｉｎｔｅｒ ｎａｔｉｏｎａｌ  \n\nｃｏｎｆｅｒｅｎｃｅｏｎｍａｃｈｉｎｅｌｅａｒｎｉｎｇ（ＩＣＭＬ －１ １ ），ｐｐ\n\n．５ １３ －５２０ ，２０ １１ ．  \n\n［２８ ］Ｍｏｒａｅｓ，Ｒ． ，Ｖａｌｉａｔｉ ，Ｊ．Ｆ． ５ａｎｄＮｅｔｏ，Ｗ．Ｐ．Ｇ ．Ｄｏｃｕｍｅｎｔ －ｌｅｖｅｌｓｅｎｔｉｍｅｎｔ  ",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 31,
    "chunk_index_in_section": 31,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 394,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c32",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "ｃｌａｓｓｉｆｉｃａｔ ｉｏｎ：Ａｎｅｍｐ ｉｒ ｉｃａｌｃｏｍｐａｒ ｉｓｏｎｂｅｔｗｅｅｎｓｖｍａｎｄａｎｎ ．Ｅｘｐｅｒ ｔＳｙｓｔｅｍｓ  \n\nｗｉｔｈＡｐｐ ｌｉｃａｔｉｏｎｓ，４０（２）\n\n：６２１ －６３３ ？２０１３ｂ ．  \n\n［２９ ］Ｐａｎｇ，Ｂ ． ５Ｌｅｅ ，Ｌ ． ，ａｎｄＶａｉｔｈｙａｎａｔｈａｎ ，Ｓ ．Ｔｈｕｍｂｓｕｐ？ ：ｓｅｎｔｉｍｅｎｔｃｌａｓｓｉｆｉｃａｔｉｏｎ  \n\nｕｓｉｎｇｍａｃｈｉｎｅｌｅａｒｎｉｎｇ ｔｅｃｈｎｉｑｕｅｓ ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔ ｉｉｅＡＣＬ －０２ｃｏｎｆｅｒｅｎｃｅｏｎ  \n\nＥｍｐ ｉｒｉｃａｌｍｅｔｈｏｄｓｉｎｎａｔｕｒａｌｌａｎｇｕ＾ｅｐｒｏｃｅｓｓｉｎｇ\n\n－Ｖｏｌｕｍｅ１０ ， ｐｐ\n\n－８６．  ",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 32,
    "chunk_index_in_section": 32,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 398,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c33",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "－Ｖｏｌｕｍｅ１０ ， ｐｐ\n\n－８６．  \n\nＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ，２００２ ．  \n\n［３０］Ｐａｎｇ？Ｂ ． ｓＬｅｅ ，Ｌ ． ？ａｎｄＶａｉｔｈｙａｎａｔｈａｎ ，Ｓ ．Ｔｈｕｍｂｓｕｐ？：ｓｅｎｔｉｍｅｎｔｃｌａｓｓｉｆｉｃａｔｉｏｎ  \n\nｕｓｉｎｇｍａｃｈｉｎｅｌｅａｒｎｉｎｇｔｅｃｈｎｉｑｕｅｓ ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＡＣＬ －０２ｃｏｎｆｅｒｅｎｃｅｏｎ  \n\n７５  \n\n北京邮电大学电子信息硕士学位论文  \n\nＥｍｐ ｉｒｉｃａｌｍｅｔｈｏｄｓｉｎｎａｔｕｒａｌｌａｎｇｕａｇｅｐｒｏｃｅｓｓｉｎｇ\n\n－Ｖｏｌｕｍｅ１０ ， ｐｐ\n\n－８６．  ",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 33,
    "chunk_index_in_section": 33,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 370,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c34",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "－Ｖｏｌｕｍｅ１０ ， ｐｐ\n\n－８６．  \n\nＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ，２００２ ．  \n\n［３１ ］Ｌｌｏｒｅｔ ，Ｅ ． ？Ｂａｌａｈｕｒ ，Ａ ． ５Ｐａｌｏｍａｒ ，Ｍ． ？ａｎｄＭｏｎｔｏｙｏ ，Ａ ．Ｔｏｗａｒｄｓｂｕｉｌｄｉｎｇａ  \n\nｃｏｍｐｅｔｉｔｉｖｅｏｐ ｉｎｉｏｎｓｕｍｍａｒ ｉｚａｔｉｏｎｓｙｓｔｅｍ ：Ｃｈａｌｌｅｎｇｅｓａｎｄｋｅｙｓ．ＩｎＨｕｍａｎ  \n\nＬａｎｇｕａｇｅＴｅｃｈｎｏｌｏｇ ｉｅｓ：ＣｏｎｆｅｒｅｎｃｅｏｆｔｈｅＮｏｒｔｈＡｍｅｒ ｉｃａｎＣｈａｐｔｅｒｏｆｔｈｅ  ",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 34,
    "chunk_index_in_section": 34,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 332,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c35",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "ＡｓｓｏｃｉａｔｉｏｎｏｆＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ，Ｐｒｏｃｅｅｄｉｎｇｓ ，Ｍａｙ３１ －Ｊｕｎｅ５ ，２００９ ，  \n\nＢｏｕｌｄｅｒ ，Ｃｏｌｏｒａｄｏ ，ＵＳＡ ，ＳｔｕｄｅｎｔＲｅｓｅａｒｃｈＷｏｒｋｓｈｏｐａｎｄＤｏｃｔｏｒａｌＣｏｎｓｏｒｔｉｕｍ，  \n\n．７２ －７７ ．ＴｈｅＡｓｓｏｃｉａｔ ｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ，２００９ ．  \n\n［３２ ］Ｙｕ ，Ｈ ．ａｎｄＨａｔｚｉｖａｓｓｉｌｏｇ ｌｏｕ ，Ｖ．Ｔｏｗａｒｄｓａｎｓｗｅｒｉｎｇｏｐ ｉｎｉｏｎ ｑｕｅｓｔｉｏｎｓ ：Ｓｅｐａｒａｔｉｎｇ  ",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 35,
    "chunk_index_in_section": 35,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 338,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c36",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "ｆａｃｔｓｆ ｒｏｍｏｐ ｉｎｉｏｎｓａｎｄｉｄｅｎｔｉｆ ｙ ｉｎｇｔｈｅｐｏｌａｒｉｔｙｏｆｏｐ ｉｎｉｏｎｓｅｎｔｅｎｃｅｓ．Ｉｎ  \n\nＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２００３ｃｏｎｆｅｒｅｎｃｅｏｎＥｍｐ ｉｒｉｃａｌｍｅｔｈｏｄｓｉｎｎａｔｕｒａｌｌａｎｇｕａｇｅ  \n\nｐｒｏｃｅｓｓｉｎｇ，ｐｐ\n\n－ １３６ ．ＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ，２００３ ．  \n\n［３３ ］Ｋｉｍ ，Ｓ ．ｍｉｄＨｏｖｙ５Ｅ ．Ｈ．Ｄｅｔｅｒｍｉｎｉｎｇｔｈｅｓｅｎｔｉｍｅｎｔｏｆ ｏｐ ｉｎｉｏｎｓ ．ＩｎＣＯＬＩＮＧ２００４ ，  ",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 36,
    "chunk_index_in_section": 36,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 340,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c37",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "２０ｔｈＩｎｔｅｒ ｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔ ｅｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ，Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ  \n\nＣｏｎｆｅｒｅｎｃｅ，２３ －２７Ａｕｇｕｓｔ２００４ ，Ｇｅｎｅｖａ，Ｓｗｉｔｚｅｒｌａｎｄ ，２００４ ．  \n\n［３４ ］Ｋｏｕｌｏｕｍｐ ｉｓ ，Ｅ” Ｗｉｌｓｏｎ ，Ｔ” ａｎｄＭｏｏｒｅ ， Ｊ ．Ｄ ．Ｔｗｉｔｅｒｓｅｎｔｉｍｅｎｔａｎａｌｙｓｉｓ ：Ｔｈｅ ｇｏｏｄ  \n\nｔｈｅｂａｄａｎｄｔｈｅｏｍｇ\n\n！Ｉｃｗｓｍ ，１１（５３８\n\n－５４１） ：１６４，２０１１ ．  ",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 37,
    "chunk_index_in_section": 37,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 313,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c38",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "！Ｉｃｗｓｍ ，１１（５３８\n\n－５４１） ：１６４，２０１１ ．  \n\n［３５ ］Ｈｕ ？Ｍ ．ａｎｄＬｉｕ ５Ｂ ，Ｍｉｎｉｎｇａｎｄｓｕｍｍａｒ ｉｚｉｎｇｃｕｓｔｏｍｅｒｒｅｖｉｅｗｓ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆ  \n\nｔｈｅｔｅｎｔｈＡＣＭＳＩＧＫＤＤｉｎｔｅｒ ｎａｔｉｏｎａｌｃｏｎｆｅｒｅｎｃｅｏｎＫｎｏｗｌｅｄｇｅｄｉｓｃｏｖｅｒｙａｎｄ  \n\nｄａｔａｍｉｎｉｎｇ，ｐｐ\n\n－ １７７ ．ＡＣＭ ５２００４ｂ ．  \n\nＰ ６ ］ＪｅｒｅｍｙＢａｒｎｅｓ ，ＲｏｂｉｎＫｕｒｔｚ ，ＳｔｅｐｈａｎＯｅｐｅｎ ，Ｌｉｌ ｊ ａ０ｖｒｅｌｉｄ ，ａｎｄＥｒｉｋＶｅｌｌｄａＬ２０２１？  ",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 38,
    "chunk_index_in_section": 38,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 353,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c39",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "Ｓｔｒｕｃｔｕｒｅｄｓｅｎｔｉｍｅｎｔａｎａｌｙｓｉｓａｓｄｅｐｅｎｄｅｎｃｙｇｒａｐｈ ｐａｒｓｉｎｇ\n\n．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ  \n\n５９ｔｈ ＡｎｎｕａｌＭｅｅｔｉｎｇｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓａｎｄｔｈｅ１ １ｔｈ  \n\nＩｎｔｅｒ ｎａｔ ｉｏｎａｌＪｏｉｎｔＣｏｎｆｅｒｅｎｃｅｏｎＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ（Ｖｏｌｕｍｅ１ ：Ｌｏｎｇ  \n\nＰａｐｅｒｓ），ｐａｇｅｓ３３８７ －３４０２ ，Ｏｎｌｉｎｅ ．Ａｓｓｏｃｉａｔ ｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ．  ",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 39,
    "chunk_index_in_section": 39,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 348,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c40",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "［３７ ］ＷａｎｇＷ ｓＰａｎＳＪ ９ＤａｈｌｍｅｉｅｒＤ ，ｅｔａｌ ．ＲｅｃｕｒｓｉｖｅＮｅｕｒａｌＣｏｎｄｉｔｉｏｎａｌＲａｎｄｏｍＦｉｅｌｄｓ  \n\nｆｏｒＡｓｐｅｃｔ －ｂａｓｅｄＳｅｎｔｉｍｅｎｔＡｎａｌｙｓｉｓ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２０１６ｃｏｎｆｅｒｅｎｃｅｏｎ  \n\nｅｍｐ ｉｒ ｉｃａｌｍｅｔｈｏｄｓｉｎｎａｔｕｒ ａｌｌａｎｇｕａｇｅ ｐｒｏｃｅｓｓｉｎｇ，Ａｕｓｔ ｉｎ ，Ｔｅｘａｓ ：Ａｓｓｏｃｉａｔｉｏｎｆｏｒ  \n\nＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ，２０１６：６１６ －６２６ ．  ",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 40,
    "chunk_index_in_section": 40,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 320,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c41",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "ＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ，２０１６：６１６ －６２６ ．  \n\n［３ ８ ］ＷａｎｇＷ ５ＰａｎＳＪ ？ＤａｈｌｍｅｉｅｒＤ ，ｅｔａｌ ．Ｃｏｕｐ ｌｅｄｍｕｌｔｉ － ｌａｙｅｒａｔｅｎｔｉｏｎｓｆｏｒｃｏ ？  \n\nｅｘｔｒａｃｔ ｉｏｎｏｆａｓｐｅｃｔａｎｄｏｐ ｉｎｉｏｎｔｅｒｍｓ ．ＩｎＴｈｉｒｔｙ\n\n－ＦｉｒｓｔＡＡＡＩＣｏｎｆｅｒｅｎｃｅｏｎ  \n\nＡｒｔｉｆｉｃｉａｌＩｎｔｅｌｌｉｇｅｎｃｅ ，２０１７ ．  \n\n７６  \n\n参考文献  \n\n［３ ９ ］ＤａｉＨ ｓＳｏｎｇＹ．ＮｅｕｒａｌＡｓｐｅｃｔａｎｄＯｐ ｉｎｉｏｎＴｅｒｍＥｘｔｒａｃｔｉｏｎｗｉｔｈＭｉｎｅｄＲｕｌｅｓａｓ  ",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 41,
    "chunk_index_in_section": 41,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 378,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c42",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "ＷｅａｋＳｕｐｅｒｖｉｓｉｏｎ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２０１９ｃｏｎｆｅｒｅｎｃｅｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒ  \n\nＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ，Ｆｌｏｒｅｎｃｅ，Ｉｔａｌｙ，２０１９ ：５２６８ －５２７７ ．  \n\n［４０］ＷａｎｇＷ ，ＰａｎＳＪ 〇Ｔｒａｎｓｆｅｒａｂｌｅｉｎｔｅｒａｃｔｉｖｅｍｅｍｏｒｙｎｅｔｗｏｒｋｆｏｒｄｏｍａｉｎａｄａｐ ｔａｔｉｏｎ  \n\nｉｎｆｉｎｅ － ｇｒａｉｎｅｄｏｐ ｉｎｉｏｎｅｘｔｒａｃｔｉｏｎ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＡＡＡＩＣｏｎｆｅｒｅｎｃｅｏｎ  \n\nＡｒｔｉｆｉｃｉａｌＩｎｔｅｌｌｉｇｅｎｃｅ ，２０１９：７１９２ －７１９９ ．  ",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 42,
    "chunk_index_in_section": 42,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 377,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c43",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "ＡｒｔｉｆｉｃｉａｌＩｎｔｅｌｌｉｇｅｎｃｅ ，２０１９：７１９２ －７１９９ ．  \n\n［４１ ］ＣｈｅｎＳ ５ＬｉｕＪ ，ＷａｎｇＹ５ｅｔａｌ ．ＳｙｎｃｈｒｏｎｏｕｓＤｏｕｂｌｅ －ｃｈａｎｎｅｌＲｅｃｕｒｒｅｎｔＮｅｔｗｏｒｋｆｏｒ  \n\nＡｓｐｅｃｔ －Ｏｐ ｉｎｉｏｎＰａｉｒＥｘｔｒａｃｔ ｉｏｎ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆ ｔｈｅ５８ｔｈＡｎｎｕａｌＭｅｅｔｉｎｇｏｆ ｔｈｅ  \n\nＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ，２０２０：６５１５ －６５２４ ．  \n\n［４２ ］ＭａＤ ，ＬｉＳ ，ＷａｎｇＨ ．Ｊｏｉｎｔｌｅａｒｎｉｎｇｆｏｒｔａｒｇｅｔｅｄｓｅｎｔｉｍｅｎｔａｎａｌｙｓｉｓ ．ＩｎＰｒｏｃｅｅｄｉｎｇｓ  ",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 43,
    "chunk_index_in_section": 43,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 396,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c44",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "ｏｆｔｈｅ２０１８ＣｏｎｆｅｒｅｎｃｅｏｎＥｍｐ ｉｒｉｃａｌＭｅｔｈｏｄｓｉｎＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ，  \n\n２０１８ ：４７３７ －４７４２．  \n\n［４３］ ＨｅＲ ，ＬｅｅＷＳ ，ＮｇＨＴ ３ｅｔａｌ ．ＡｎＩｎｔｅｒａｃｔｉｖｅＭｕｌｔｉ －ＴａｓｋＬｅａｒｎｉｎｇＮｅｔｗｏｒｋｆｏｒ  \n\nＥｎｄ －ｔｏ\n\n－ＥｎｄＡｓｐｅｃｔ －ＢａｓｅｄＳｅｎｔｉｍｅｎｔＡｎａｌｙｓｉｓ ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２０１９  \n\nｃｏｎｆｅｒｅｎｃｅｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ，Ｆｌｏｒｅｎｃｅ ，Ｉｔａｌｙ，  \n\n２０１９ ：５０４ －５１５ ．  ",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 44,
    "chunk_index_in_section": 44,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 383,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c45",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "２０１９ ：５０４ －５１５ ．  \n\n［４４ ］ＬｉＸ ，ＢｉｎｇＬ ，ＬｉＰ ？ｅｔａｌ ．Ａｕｎｉｆｉｅｄｍｏｄｅｌｆｏｒｏｐ ｉｎｉｏｎｔａｒｇｅｔｅｘｔｒａｃｔｉｏｎａｎｄｔａｒｇｅｔ  \n\nｓｅｎｔｉｍｅｎｔｐｒｅｄｉｃｔｉｏｎ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＡＡＡＩＣｏｎｆｅｒｅｎｃｅｏｎＡｒｔ ｉｆｉｃｉａｌ  \n\nＩｎｔｅｌｌｉｇｅｎｃｅ，２０１９ ：６７１４ －６７２１ ．  \n\n［４５］ＬｉＸ ，ＢｉｎｇＬ ，ＺｈａｎｇＷ ？ｅｔａｌ ．Ｅｘｐ ｌｏｉｔｉｎｇＢＥＲＴｆｏｒＥｎｄ －ｔｏ\n\n－ＥｎｄＡｓｐｅｃｔ\n\n－ｂａｓｅｄ  ",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 45,
    "chunk_index_in_section": 45,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 332,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c46",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "－ＥｎｄＡｓｐｅｃｔ\n\n－ｂａｓｅｄ  \n\nＳｅｎｔｉｍｅｎｔＡｎａｌｙｓｉｓ ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２０１９ｃｏｎｆｅｒｅｎｃｅｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒ  \n\nＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ，ＨｏｎｇＫｏｎｇ，Ｃｈｉｎａ ，２０１９ ：３４ －４１ ．  \n\n［４６ ］ＰｅｎｇＨ ？ＸｕＬ ，ＢｉｎｇＬ ？ｅｔａｌ ．Ｋｎｏｗｉｎｇｗｈａｔ ，ｈｏｗａｎｄｗｈｙ\n\n：Ａｎｅａｒｃｏｍｐ ｌｅｔｅ  \n\nｓｏｌｕｔｉｏｎｆｏｒａｓｐｅｃｔ －ｂａｓｅｄｓｅｎｔｉｍｅｎｔａｎａｌｙｓｉｓ，ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＡＡＡＩ  ",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 46,
    "chunk_index_in_section": 46,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 347,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c47",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "ＣｏｎｆｅｒｅｎｃｅｏｎＡｒｔｉｆｉｃｉａｌＩｎｔｅｌｌｉｇｅｎｃｅ ，２０２０ ：８６００ －８６０７ ．  \n\n［４７ ］ＬｕＸｕ ５ＨａｏＬｉ ，ＷｅｉＬｕ ，ａｎｄＬｉｄｏｎｇＢｉｎｇ\n\n，２０２０ ．Ｐｏｓｉｔｉｏｎ －ａｗａｒｅｔａｇｇ ｉｎｇｆｏｒａｓｐｅｃｔ  \n\nｓｅｎｔｉｍｅｎｔｔｒ ｉｐ ｌｅｔｅｘｔｒａｃｔｉｏｎ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２０２０ＣｏｎｆｅｒｅｎｃｅｏｎＥｍｐ ｉｒｉｃａｌ  \n\nＭｅｔｈｏｄｓｉｎＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ（ＥＭＮＬＰ ），ｐ＾ｅｓ２３３９ －２３４９ ，Ｏｎｌｉｎｅ．  \n\nＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ．  \n\n［４８ ］ＺｈｅｎＷｉｌ",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 47,
    "chunk_index_in_section": 47,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 394,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c48",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "［４８ ］ＺｈｅｎＷｉｌ\n\n，ＣｈｅｎｇｃａｎＹｉｎｇ，ＦｅｉＺｈａｏ，ＺｈｉｆａｎｇＦａｎ，ＸｉｎｙｕＤａｉ ，ａｎｄＲｕｉＸｉａ．２０２０ ．  \n\nＧｒ ｉｄｔａｇｇ ｉｎｇｓｃｈｅｍｅｆｏｒａｓｐｅｃｔ －ｏｒ ｉｅｎｔｅｄｆｉｎｅ － ｇｒａｉｎｅｄｏｐ ｉｎｉｏｎｅｘｔｒａｃｔｉｏｎ．Ｉｎ  \n\nＦｉｎｄｉｎｇｓｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ：ＥＭＮＬＰ２０２０ ，ｐａｇｅｓ  \n\n２５７６ －２５８５ ，Ｏｎｌｉｎｅ ．ＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ．  \n\n７７  \n\n北京邮电大学电子信息硕士学位论文  ",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 48,
    "chunk_index_in_section": 48,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 365,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c49",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "７７  \n\n北京邮电大学电子信息硕士学位论文  \n\n［４９ ］ＣｈｅｎＺｈａｎｇ，ＱｉｕｃｈｉＬｉ ，ＤａｗｅｉＳｏｎｇ？ａｎｄＢｅｎｙｏｕＷａｎｇ\n\n．２０２０ ．Ａｍｕｌｔｉ －ｔａｓｋ  \n\nｌｅａｒｎｉｎｇｆ ｒａｍｅｗｏｒｋｆｏｒｏｐ ｉｎｉｏｎｔｒ ｉｐ ｌｅｔｅｘｔｒａｃｔｉｏｎ．ＩｎＦｉｎｄｉｎｇｓｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎ  \n\nｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ：ＥＭＮＬＰ２０２０ ，ｐａｇｅｓ８１９ －８２８ ，Ｏｎｌｉｎｅ ．Ａｓｓｏｃｉａｔｉｏｎ  \n\nｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ．  ",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 49,
    "chunk_index_in_section": 49,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 332,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c50",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "ｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ．  \n\n［５０］ＺｈｅｘｕｅＣｈｅｎ，ＨｏｎｇＨｕａｎｇ，ＢａｎｇＬｉｕ，ＸｕａｎｈｉｘａＳｈｉ ，ａｎｄＨａｉＪｉｎ．２０２１ｂ，Ｓｅｍａｎｔｉｃ  \n\nａｎｄｓｙｎｔａｃｔｉｃｅｎｈａｎｃｅｄａｓｐｅｃｔｓｅｎｔｉｍｅｎｔｔｒｉｐ ｌｅｔｅｘｔｒａｃｔｉｏｎ．ＩｎＦｉｎｄｉｎｇｓｏｆｔｈｅ  \n\nＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ：ＡＣＬ －ＩＪＣＮＬＰ２０２１ ，ｐａｇｅｓ１４７４— １４８３ ，  \n\nＯｎｌｉｎｅ．ＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ．  ",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 50,
    "chunk_index_in_section": 50,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 352,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c51",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "［５１ ］ ＨａｎｇＹａｎ ，Ｊｕｎｑ ｉＤａｉ ，ＴｕｏＪｉ ？ＸｉｐｅｎｇＱｉｕ ，ａｎｄＺｈｅｎｇＺｈａｎｇ\n\n．２０２ＬＡｕｎｉｆ ｉｅｄ  \n\nｇｅｎｅｒａｔｉｖｅｆ ｒａｍｅｗｏｒｋｆｏｒａｓｐｅｃｔ －ｂａｓｅｄｓｅｎｔｉｍｅｎｔａｎａｌｙｓｉｓ ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ  \n\n５９ｔｈＡｎｎｕａｌＭｅｅｔｉｎｇｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓａｎｄｔｈｅ１ １ｔｈ  \n\nＩｎｔｅｒ ｎａｔｉｏｎａｌＪｏｉｎｔＣｏｎｆｅｒｅｎｃｅｏｎＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ（Ｖｏｌｕｍｅ１ ：Ｌｏｎｇ  \n\n以下是清洗后的参考文献内容：",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 51,
    "chunk_index_in_section": 51,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 367,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c52",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "以下是清洗后的参考文献内容：\n\n1. Yue Mao, Yi Shen, Chao Yu, and Longjun Cai. 2021. A joint training dual-mrc framework for aspect-based sentiment analysis. Proceedings of the AAAI Conference on Artificial Intelligence, 35(15): 13543-13551.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 52,
    "chunk_index_in_section": 52,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 225,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c53",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "2. Showei Chen, Yu Wang, Jie Liu, and Yuelin Wang. 2021a. Bidirectional machine reading comprehension for aspect sentiment triplet extraction. Proceedings of the AAAI Conference on Artificial Intelligence, 35(14): 12666-12674.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 53,
    "chunk_index_in_section": 53,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 226,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c54",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "3. Andre Ely, Fabrizio Sebastiani, and Iaria Urzìuoli. 2008. Annotating expressions of opinion and emotion in the Italian content annotation bank. In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC 2008), Marrakech, Morocco. European Language Resources Association (ELRA).",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 54,
    "chunk_index_in_section": 54,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 317,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c55",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "4. Arzoo Katiyar and Claire Cardie. 2016. Investigating LSTMs for joint extraction of opinion entities and relations. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 919-929, Berlin, Germany. Association for Computational Linguistics.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 55,
    "chunk_index_in_section": 55,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 308,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c56",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "5. Wei Quan, Jinli Zhang, and Xiaohua Tony Hu. 2019. End-to-end joint opinion role labeling with BERT. In 2019 IEEE International Conference on Big Data (Big Data), pages 2438-2446.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 56,
    "chunk_index_in_section": 56,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 181,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c57",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "6. Bo Zhang, Yue Zhang, Rui Wang, Zhenghua Li, and Min Zhang. 2020a. Syntax-aware opinion role labeling with dependency graph convolutional networks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3249-3258, Online. Association for Computational Linguistics.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 57,
    "chunk_index_in_section": 57,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 309,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c58",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "7. Qingrong Xia, Bo Zhang, Rui Wang, Zhenghua Li, Yue Zhang, Fei Huang, Si Luo, and Min Zhang. 2021. A unified span-based approach for opinion mining with syntactic constituents. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1795-1804, Online. Association for Computational Linguistics.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 58,
    "chunk_index_in_section": 58,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 393,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c59",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "8. Wenxuan Shi, Fei Li, Jingye Li, Hao Fei, and Donghong Ji. 2022. Effective token graph modeling using a novel labeling strategy for structured sentiment analysis. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4232-4241, Dublin, Ireland. Association for Computational Linguistics.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 59,
    "chunk_index_in_section": 59,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 357,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c60",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "9. David Samuel, Jeremy Bames, Robin Kurtz, Stephan Oepen, Lijia Ovrelid, and Erik Velldal. 2022. Direct parsing to sentiment graphs. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 470-478, Dublin, Ireland. Association for Computational Linguistics.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 60,
    "chunk_index_in_section": 60,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 325,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c61",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "10. Shershtinsky A. Fundamentals of recurrent neural network (RNN) and long short-term memory (LSTM) network [J]. Physics D: Nonlinear Phenomena, 2020, 404: 132306.\n\n11. Chiu CC, Sainath TN, Wu Y, et al. State-of-the-art speech recognition with sequence-to-sequence models [C]//2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018: 4774-4778.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 61,
    "chunk_index_in_section": 61,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 388,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c62",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "12. Wang S, Jiang J. Learning natural language inference with LSTM [J]. arXiv preprint arXiv:1512.08849, 2015.\n\n13. Wang C, Yang H, Bartz C, et al. Image captioning with deep bidirectional LSTMs [C]//Proceedings of the 24th ACM International Conference on Multimedia. ACM, 2016: 988-997.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 62,
    "chunk_index_in_section": 62,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 287,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c63",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "14. Schuster M, Paliwal K K. Bidirectional recurrent neural networks [J]. IEEE Transactions on Signal Processing, 1997, 45(11): 2673-2681.\n\n15. Xu G, Meng Y, Qiu X, et al. Sentiment analysis of comment texts based on BiLSTM [J]. IEEE Access, 2019, 7: 51522-51532.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 63,
    "chunk_index_in_section": 63,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 263,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c64",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "16. Panchevdradjajan R, Amarsaen A. Bidirectional LSTM-CRF for named entity recognition [C]//Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation. 2018.\n\n17. Wu T W, Chen I F, Gandhe A. Learning to rank with BERT-based confidence models in ASR rescoring [J]. 2022.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 64,
    "chunk_index_in_section": 64,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 300,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c65",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "18. Cho K, Van Merriënboer B, Gulcehre C, et al. Learning phrase representations using RNN encoder-decoder for statistical machine translation [J]. arXiv preprint arXiv:1406.10785, 2014.\n\n19. O'Shea K, Nash R. An introduction to convolutional neural networks [J]. arXiv preprint arXiv:1511.08458, 2015.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 65,
    "chunk_index_in_section": 65,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 302,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c66",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "20. Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need [J]. Advances in neural information processing systems, 2017, 30.\n\n21. Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and translate [J]. arXiv preprint arXiv:1409.0473, 2014.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 66,
    "chunk_index_in_section": 66,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 281,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c67",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "22. Nallapati K, Zhou B, Gulcehre C, et al. Abstractive text summarization using sequence-to-sequence models and beyond [J]. arXiv preprint arXiv:1602.06023, 2016.\n\n23. Xing C, Wu W, Wu Y, et al. Topic aware neural response generation [C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2017, 31(1).",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 67,
    "chunk_index_in_section": 67,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 315,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c68",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "24. Xu K, Ba J, Kiros R, et al. Show, attend and tell: New image caption generation with visual attention [C]//International Conference on Machine Learning. PMLR, 2015: 2048-2057.\n\n25. Chorowski J K, Bahdanau D, Serdyuk D, et al. Attention-based models for speech recognition [J]. Advances in neural information processing systems, 2015, 28.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 68,
    "chunk_index_in_section": 68,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 341,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c69",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "26. An Empirical Comparison of Sequence-to-Sequence Models for Chinese-to-English Machine Translation.\n\n27. Devlin J, Chang M W, Lee K, et al. Bert: Pre-training of deep bidirectional transformers for language understanding [J]. arXiv preprint arXiv:1810.04805, 2018.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 69,
    "chunk_index_in_section": 69,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 267,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c70",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "28. Srivastava N, Hinton G, Krizhevsky A, et al. Dropout: a simple way to prevent neural networks from overfitting [J]. The Journal of Machine Learning Research, 2014, 15(1): 1929-1958.\n\n29. Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift [C]//International Conference on Machine Learning. PMLR, 2015: 448-456.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 70,
    "chunk_index_in_section": 70,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 375,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c71",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "30. Ba J L, Kiros J R, Hinton G, et al. Layer normalization [J]. arXiv preprint arXiv:1607.06450, 2016.\n\n31. Wu Y, He K. Group normalization [C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018: 3-19.\n\n32. Ulyanov D, Vedaldi A, Lempitsky V. Instance normalization: The missing ingredient for fast stylization [J]. arXiv preprint arXiv:1607.08022, 2016.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 71,
    "chunk_index_in_section": 71,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 375,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c72",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "33. Wang Z, Bovik A C. Mean squared error: Love it or leave it? A new look at signal fidelity measures [J]. IEEE Signal Processing Magazine, 2009, 26(1): 98-117.\n\n34. Feng L, Shu S, Lin Z, et al. Can cross entropy loss be robust to label noise? [C]//Proceedings of the Twenty-Ninth International Joint Conferences on Artificial Intelligence. 2021: 2206-2212.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 72,
    "chunk_index_in_section": 72,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 358,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c73",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "35. Kim T, Oh J, Kim N Y, et al. Comparing kl divergence and mean squared error loss in knowledge distillation [J]. arXiv preprint arXiv:2105.08919, 2021.\n\n36. Ruder S. An overview of gradient descent optimization algorithms [J]. arXiv preprint arXiv:1609.04747, 2016.\n\n37. Klingner D, Pfeiffer J, Ba J, Adam: A method for stochastic optimization [J]. arXiv preprint arXiv:1607.06450, 2016.\n\n参考文献：",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 73,
    "chunk_index_in_section": 73,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 397,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c74",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "参考文献：\n\n[89] Loshchilov I, Hutter F. Fixing weight decay regularization in Adam[J]. arXiv preprint arXiv:1412.6980, 2014.\n\n[90] Duchi J, Hazan E, Singer Y. Adaptive subgradient methods for online learning and stochastic optimization[J]. Journal of machine learning research, 2011, 12(7).",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 74,
    "chunk_index_in_section": 74,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 286,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c75",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "[91] He R, Lee WS, Ng HT, et al. An Interactive Multi-Task Learning Network for End-to-End Aspect-Based Sentiment Analysis[A]. Florence, Italy: Association for Computational Linguistics, 2019: 504-515.\n\n[92] Xu L, Chia YK, Bing L. Learning span-level interactions for aspect sentiment triplet extraction[J]. arXiv preprint arXiv:2107.12214, 2021.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 75,
    "chunk_index_in_section": 75,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 346,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c76",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "[93] Chen H, Zhai Z, Feng F, et al. Enhanced multi-channel graph convolutional network for aspect sentiment triplet extraction[C]//Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022: 2974-2985.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 76,
    "chunk_index_in_section": 76,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 260,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c77",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "[94] Yucheng Wang, Bowen Yu, Hongsheng Zhu, Tingwen Liu, Nan Yu, and Limin Sun. Discontinuous named entity recognition as maximal clique discovery[A]. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 764-774, Online. Association for Computational Linguis\n774, Online. Association for Computational Linguistics, 2021.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 77,
    "chunk_index_in_section": 77,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 461,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c78",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "[95] Jonathan Ho, Nal Kalchbrenner, Diederik P. Kingma, and Tim Salimans. Axial attention in multidimensional transformers[J]. arXiv preprint arXiv:1912.12180, 2019.\n\n[96] Huiyu Wang, Yukun Zhu, Bradley Green, Haftwig Adam, Alan Loddon Yullie, and Chi-eh Chen. Axial-deepLab: Stand-alone axial-attention for panoptic segmentation[A]. European Conference on Computer Vision, 2020.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 78,
    "chunk_index_in_section": 78,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 379,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c79",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "[97] Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, Humphrey Shi, and Wenyu Liu. Ccenet: Cross-attention for semantic segmentation[A]. IEEE/CVF International Conference on Computer Vision (ICCV), pages 603-612, 2019.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 79,
    "chunk_index_in_section": 79,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 238,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c80",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "[98] Jingye Li, Hao Fei, Jiang Lin, Shengqiong Wu, Meishan Zhang, Chongteng Ji, and Fei Li. Unified named entity recognition as word-word relation classification[A]. Proceedings of the AAAI Conference on Artificial Intelligence, 36(10):10965-10973, 2022.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 80,
    "chunk_index_in_section": 80,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 254,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c81",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "[99] Jingye Li, Kang Xu, Fei Li, Hao Fei, Yafeng Ren, and Donghong Ji. MRN: A locally and globally mention-based reasoning network for document-level relation extraction[A]. Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1359-1370, Online. Association for Computational Linguistics, 2021.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 81,
    "chunk_index_in_section": 81,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 323,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c82",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "[100] Zhiyang Chen and Tieyun Qian. Relation-aware collaborative learning for unified aspect-based sentiment analysis[A]. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3685-3694, Online. Association for Computational Linguistics, 2020.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 82,
    "chunk_index_in_section": 82,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 284,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c83",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "[101] David Samuel and Milan Straka. UFAL at MRP 2020: Permutation-invariant semantic parsing in PERIN[A]. Proceedings of the CONLL 2020 Shared Task: Cross-Framework Meaning Representation Parsing, pages 53-64, Online. Association for Computational Linguistics, 2020.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 83,
    "chunk_index_in_section": 83,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 267,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c84",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "[102] He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778.\n\n[103] Szegedy C, Liu W, Jia Y, et al. Going deeper with convolutions[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 1-9.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 84,
    "chunk_index_in_section": 84,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 341,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c85",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "[104] Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition[J]. arXiv preprint arXiv:1409.1556, 2014.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 85,
    "chunk_index_in_section": 85,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 139,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c86",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "[105] Minghao Hu, Yuxing Peng, Zhen Huang, Dongsheng Li, and Yiwei Lv. Open-domain targeted sentiment analysis via span-based extraction and classification[A]. Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 537-546, Florence, Italy. Association for Computational Linguistics, 2019.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 86,
    "chunk_index_in_section": 86,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 329,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c87",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "[106] Lili Javrelidze, Peter Maschler, Jeremy Bames, and Erik Velldal. A fine-grained sentiment dataset for Norwegian[A]. Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 5025-5033, Marseille, France. European Language Resources Association, 2020.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 87,
    "chunk_index_in_section": 87,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 277,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c88",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "[107] Jeremy Bames, Toni Badia, and Patrik Lambert. MultiBooked: A corpus of Basque and Catalan hotel reviews annotated for aspect-level sentiment classification[A]. Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA), 2018.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 88,
    "chunk_index_in_section": 88,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 339,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c89",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "[108] Janyce Wiebe, Theresa Wilson, and Clare Cardie. Annotating expressions of opinions and emotions in language[J]. Language resources and evaluation, 39(3):165-210, 2005.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 89,
    "chunk_index_in_section": 89,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 173,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "多元组细粒度情感分析研究及应用_翟泽鹏_s0_c90",
    "source_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
    "text": "[109] Cigdem Toprak, Niklas Jakob, and Iryna Gurevych. Sentence and expression level annotation of opinions in user-generated discourse[A]. Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 575-584, Uppsala, Sweden. Association for Computational Linguistics, 2010.",
    "section_title": "摘要",
    "section_type": "abstract",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 90,
    "chunk_index_in_section": 90,
    "total_chunks_in_section": 91,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 309,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9450000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航_s0_c0",
    "source_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航",
    "text": "户外种植园行间可通行区域识别与路径生成方法研究\n\n王远航\n\n工程硕士-控制工程领域\n\n模式识别与智能控制\n\n毕松 导师\n\n李睿凡 校外导师\n\n2022年5月20日\n\n论文密级：公开\n\n农业是国民经济的重要组成部分，农业生产稳定性是社会发展和基础保障的关键。本研究围绕轮式移动机器人在户外种植园行间的自主行驶技术，通过理论分析和地面地形检测方法，实现了行间地形的可通行区域识别。主要工作包括：分析轮式移动机器人不可通行路面的特点及参数，获得可通行地面的边界条件；设计适用于户外种植园行间场景的道路通行性评价指标和基于图像分割的行间道路区域分割方法；生成可通行区域约束下的导航路径。\n\n关键词：户外种植园行间，轮式移动机器人平台，车辆通过性检测，地形可通行区域识别，图像语义分割与路径生成",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 3,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 342,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航_s1_c0",
    "source_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航",
    "text": "Agriculture is vital to the national economy, and the stability of agricultural production is fundamental to social development. This study focuses on the autonomous driving technology of wheeled mobile robots in outdoor plantation rows. It theoretically analyzes the passability of mobile robots and develops ground terrain detection methods for identifying passable areas between rows. Key contribu",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 1,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 4,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 400,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9900000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航_s1_c1",
    "source_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航",
    "text": "ntifying passable areas between rows. Key contributions include: characterizing impassable surfaces and obtaining boundary conditions for passable terrain; designing indices for road passability in outdoor plantation rows and an image segmentation method for inter-row road area; and generating navigation paths under constraints of passable areas.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 2,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 4,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 348,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航_s1_c2",
    "source_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航",
    "text": "Keywords: outdoor plantation inter-row, wheeled mobile robot platform, vehicle passability detection, terrain passable area recognition, image semantic segmentation, path generation\n\n---\n\n4.2.2 越障能力分析\n\n4.3 地形特征提取分析\n   4.3.1 地形相对不变性证明及其相关假设\n   4.3.2 坡度计算\n\n4.4 可通行区域识别综合分析与决策\n   4.4.1 深度数据处理\n   4.4.2 负障碍通行性判别\n   4.4.3 可通行区域识别\n\n4.5 本章小结",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 3,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 4,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 334,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航_s1_c3",
    "source_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航",
    "text": "4.5 本章小结\n\n第五章 行间道路识别与路径生成\n   5.1 模型组网的构建\n   5.2 模型训练\n       5.2.1 目标数据来源与获取\n       5.2.2 训练过程\n       5.2.3 训练结果\n       5.2.4 模型测试\n   5.3 路径生成\n   5.4 本章小结\n\n第六章 总结与展望\n   6.1 工作总结\n   6.2 研究展望",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 3,
    "chunk_index": 4,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 4,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 188,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9450000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航_s2_c0",
    "source_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航",
    "text": "在学期间的研究成果\n致谢\n\n第一章 绪论\n   1.1 课题背景及意义\n   1.2 国内外户外种植园行间自主行驶的研究现状\n       1.2.1 地形负障碍检测研究现状\n       1.2.2 可通行道路识别研究现状\n       1.2.3 路径生成研究现状\n   1.3 本文的主要研究内容与组织结构\n\n---\n\n根据论文内容，以下是清洗后的片段：\n\n第四章研究了轮式移动机器人平台行间地形感知及可通行区域的识别方法。分析了移动机器人完整的自相关限定阈值集合，研究了地形相对不变特征分析及提取方法，实现了负障碍可通行区域的识别。\n\n第五章设计了行间道路识别与路径生成系统。本章基于图像分割方法设计了户外行间地面识别方法，研究了道路中可通行轨迹生成方法。完成了户外种植园行间地面分割模型的训练，测试结果表明所设计的模型具有良好的道路识别能力，并根据移动机器人行驶方向生成可通行路径。\n\n第六章为总结与展望。本章针对户外种植园行间可通行区域识别与路径生成方法研究进行了分析和总结，指出本文系统设计最终达到的预期效果及评价，同样指出目前研究中存在的不足，并对这些不足的解决方法研究进行了展望。\n\n第二章 可通行区域识别及路径生成相关理论\n\n2.1 地形感知及可通行区域识别相关理论\n\n2.1.1 车辆质心求解原理\n\n2.1.2 深度数据处理原理\n\n2.1.3 地形数据高程描述及坡度求解原理",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 5,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 28,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 598,
    "chunk_method": "hierarchical",
    "importance_weight": 0.44000000000000006,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航_s2_c1",
    "source_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航",
    "text": "2.1 地形感知及可通行区域识别相关理论\n\n2.1.1 车辆质心求解原理\n\n2.1.2 深度数据处理原理\n\n2.1.3 地形数据高程描述及坡度求解原理\n\n2.1.4 顶起失效判定原理\n\n2.2 道路分割与路径生成相关理论\n\n2.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 6,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 28,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 114,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航_s2_c2",
    "source_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航",
    "text": "2.1 基于深度学习的图像语义分割原理\n\n以上内容已去除页眉、页脚、页码等非核心学术内容，保留了技术术语、数据、逻辑结构、公式和图表说明。\n\n清洗后的内容如下：\n\n语义分割系统的评价指标中最重要的是分割精度(Accuracy)，主要的精度指标有精确率、召回率和F1分数。精确率是用分类准确的正类数量与全部预测为正类的数量做比值，代表预测正类中实际正类的比例。召回率是用分类准确的正类数量与实际正类的数量做比值，代表实际正类中，被预测准确的正类的比例。F1分数可以平衡精确率和召回率两个指标，可以有效衡量模型整体分割性能。\n\n路径生成原理：由于道路在相机水平拍摄的形状是远小近大的，所以对于路面来讲，图片中道路的信息呈现的是梯形的形状且符合大部分路面信息的形状规则，所以本文采用对道路信息的最小外接梯形提取导航线的方式进行路径生成。通过查找目标特征区域的最小外接梯形，依据梯形的位置及方向来定位目标物体的位置与姿态。通过道格拉斯-普克算法能够得到道路信息的最小外接梯形。该算法将曲线离散化由离散的点进行表示，并根据大体轮廓进行滤波处理。",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 7,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 28,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 467,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航_s2_c3",
    "source_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航",
    "text": "户外种植园行间可通行区域识别与路径生成技术需求分析及方案设计：户外种植园是典型的非结构化环境，存在光照变化大、路面起伏等特点。标准的种植园对果树定植，形成具有一定规律性的种植模式。户外种植园定植后，形成的行间区域是种植园中可通行区域，为土质松软路面，因此行间地面存在坑洼不平的情况，对于轮式移动机器人通过性提出了挑战。本文主要解决的三个问题为：(1)轮式移动机器人的可通行路面的参数和参数范围。(2)符合行间实际环境的负障碍参数检测和提取方法。(3)户外环境下行间地面识别及可通行区域约束下的路径生成方法。\n\n行间地形感知及可通行区域识别方案设计：本文首先对行间可通行区域识别与路径生成系统进行了行间地形感知及可通行区域识别的研究。其次将地形感知及可通行区域识别的研究分为整车通过性分析和地形识别两个部分进行研究。地形感知模块安装在车体的前部\n0.85m处，距离地面",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 8,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 28,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 383,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航_s2_c4",
    "source_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航",
    "text": "1.05m处，进行对地形深度数据的感知；陀螺仪安装在相机所在刚性平面，进行位姿校正。通过坐标系畸变校正和陀螺仪位姿校正得到真实的深度数据。进而对地形图像中的每个像素点所对应的距离数据进行坡度卷积计算得到坡度值，实现对地形的坡度特征提取。最后结合平台通过性综合限制参数，识别出相机视野内的可通行区域。\n\n行间道路识别与路径生成方案设计：本文首先对户外种植园环境进行数据量为1060张的种植园行间图像数据采集。进而通过人眼对图像数据进行人工标注，标注出天空、树木、背景、道路四类特征，然后将所标注出的数据通过Labelme进行裁剪最终得到1060张图像数据的数据集。通过模型构建模块的设计，达到对这1060张人工标注的数据集方式的学习方式。根据户外种植园行间图像的特点构建图像语义分割模型并选择具有较高精度与环境抗干扰能力的深度卷积神经网络分割模型。模型训练过程包括下采样和上采样两个过程，共包含九个模块，最终使用分类函数完成了图像多分类任务，实现了将输入模型的图像分割成树木、天空、地面、背景四部分后输出的效果。最后根据图像处理原理，将四分类中的道路部分提取，并根据环境道路近大远小的特征确定道路最小外接梯形，最终确定沿行驶方向上的行驶路径。\n\n清洗后的内容如下：",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 9,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 28,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 531,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航_s2_c5",
    "source_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航",
    "text": "清洗后的内容如下：\n\n公式4-4表明，匀速行驶时的侧翻角度大于直线行驶中的侧翻角，因此当平台直线行驶或曲线匀速行驶时，平台向内侧翻的概率减小。公式4-5为车辆整体所受外力对车辆与斜坡的上侧轮胎接触点C的动力矩守恒分析下，规定顺时针为正。\n\n在公式4-5中，1ZyF为车辆与斜坡的上侧轮胎接触点A处所受的正向力，根据对4-4式的推理过程，可进一步根据公式4-5得到式4-6所示的向心加速度与重力加速度的比值，即道路坡度角与车辆倾翻角之间的关系为：\n\n1 0 1 Z y F B a b cos sin g h mg φ φ φ = − + \n\n在公式4-6中，弯道行驶发生侧翻的临界条件为1 0 Z F = 0且当φ角较小时，sinφ = cosφ，将此临界条件代入式4-6中可得式4-7的简化判别公式。\n\ny 0 a b g h φ = + \n\n公式4-7中，0bh为侧翻阈值，可评估车辆抵抗侧翻的能力。\n\n由于轮式移动机器人的结构较为复杂，因此需要在Adams系统中建立车辆的实际模型，根据表3-1可知前轴载重",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 10,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 28,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 456,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航_s2_c6",
    "source_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航",
    "text": "116.1kg，后轴载重108.8kg，利用Adams中view模块并结合第二章式2-9、2-10的求解，计算出平台满载和空载时的质心的切向距离与垂向高度。\n\n车辆前翻模型与侧翻模型分析过程一致如图4-2(b)所示，同侧翻原理可得其侧翻阈值为0lh，其中0l为质心到车辆前轴方向的距离，L为车辆轴距。车辆前翻、侧翻角度如表4-1所示。\n\n表4-1 车辆前翻、侧翻角度对应表\n\n— 质心切向距离(mm) 质心垂向高度(mm) 倾翻角(°)\n\n侧翻满载 0 329.38 l = 0 653.6 h = 26.7\n\n侧翻空载 0 332.95 l = 0 459.09 h = 35.9\n\n前翻满载 0 314.49 b = 0 653.6 h = 25.7\n\n前翻空载 0 283.39 l = 0 459.09 h = 31.7\n\n将表4-1得到的在满载和空载时的质心位置信息与极限倾翻角度参数和车辆自身的通过性限制参数的综合分析中可知，在纵向角度通过性方面，车辆自身的接近角和离去角构成的角度阈值为47.6°~90.0°，车辆前翻角度阈值为25.7°~",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 11,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 28,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 477,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航_s2_c7",
    "source_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航",
    "text": "47.6°~90.0°，车辆前翻角度阈值为25.7°~31.7°，前翻角度阈值远小于接近角和离去角构成的角度阈值。所以车辆先发生前翻事故，后发生触头、拖尾事故，在现实情况下，当发生其中一个事故时，车辆立即停止行驶，因此在此纵向角度通过性判别中，首要考虑倾翻角度阈值作为车辆通过性检测的纵向角度阈值约束。侧向角度阈值约束选取本节的侧翻阈值26.7°~35.9°。\n\n最终得到轮式移动机器人在户外种植园行间环境下行驶的纵向、侧向角度阈值约为25.7°~31.7°和26.7°~35.9°。\n\n本文通过使用Basler ToF 3D相机采集地形深度信息，结合轮式移动机器人平台的速度和相机视野，计算了纵向误差占比约为",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 12,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 28,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 305,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航_s2_c8",
    "source_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航",
    "text": "26.7°~35.9°。\n\n本文通过使用Basler ToF 3D相机采集地形深度信息，结合轮式移动机器人平台的速度和相机视野，计算了纵向误差占比约为1.77%，符合可接受误差范围，表明数据准确性较高。\n\n由于户外种植园行间场景存在径向、切向畸变和位姿偏转，本文通过相机畸变校正和位姿校正过程，校正了深度数据误差。畸变校正采用棋盘格标定求取相机内参和畸变系数，坐标系转换实现了像素坐标系到相机坐标系的转换，位姿校正则通过陀螺仪数据实现了相机坐标系到世界坐标系的转换。\n\n负障碍分割通过设定地面高度阈值，将低于该阈值的区域标记为负障碍区域。直行区域负障碍通行性判别流程中，首先计算负障碍区域的最大宽度，并与轮距比较，以判断车辆是否可通过。\n\n最后，综合车辆通过性约束和地形分析，实现了地形可通行性识别。通过计算地形坡度，并与车辆倾翻角度阈值比较，识别出可通行区域。\n\n第四章 行间地形感知及可通行区域识别\n\n图4-26(a)(b)(c)(d)中，位姿角在负障碍边缘的近似值为20°，经统计X方向最大位姿角为28.4725°，平均位姿角为13.7273°；Y方向最大位姿角为27.5256°，平均位姿角为13.4971°。通过与移动机器人平台的侧翻角度阈值26.74°~35.69°，前翻角度阈值25.69°~",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 13,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 28,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 553,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航_s2_c9",
    "source_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航",
    "text": "27.5256°，平均位姿角为13.4971°。通过与移动机器人平台的侧翻角度阈值26.74°~35.69°，前翻角度阈值25.69°~31.68°对比，可知，此地形整体通过性良好，但存在使车辆发生倾翻事故的区域。\n\n对于地形坡度特征，并结合国家车辆通过性评价标准",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 14,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 28,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 132,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航_s2_c10",
    "source_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航",
    "text": "[39]，本文规定在轮式移动机器人平台道路通过性评价的标准中倾翻角度的占比指标σ为70%，平均坡度指标τ为30%，最终指标大于0.2的道路为不可通过；大于0.1小于0.2的道路为通行困难道路。小于0.1的道路为通行性良好道路。根据4.1得出的车辆倾翻通过性限制角度，对地形坡度特征及其通过性评价如表4-4所示。\n\n表4-4 地形坡度特征及其通过性评价\n\nx向平均坡度|y向平均坡度|通过性评价|σ|τ\n23.93°|14.14%|困难|0.1904|0.0406\n30.81%|良好|0.0247|0.1141\n22.57°|1.98%|良好|0.0247|",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 15,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 28,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 279,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航_s2_c11",
    "source_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航",
    "text": "23.93°|14.14%|困难|0.1904|0.0406\n30.81%|良好|0.0247|0.1141\n22.57°|1.98%|良好|0.0247|0.1141\n\n同样规定通行良好道路为当平均坡度不大于其阈值极限且倾翻角度占比低于20%的道路；通行困难道路为平均坡度大于其阈值极限30%且倾翻角度占比高于20%低于50%的道路；不可通行道路为平均坡度大于其阈值极限50%且倾翻角度占比高于50%的道路，道路状态评价，如表4-5所示。\n\n表4-5 道路状态评价\n\n—|良好道路|通行困难|不可通行\n20%~50%|≤30%|20%~50%|>50%\n\n当车辆满载或空载并检测到多负障碍的地形情况时，地形中存在远大于车辆发生倾翻的角度，但此地形平均坡度较小。从表4-4的数据上可以说明车辆倾斜角度相较于整体坡度而言占比较小，地形整体通过坡度分析，整体可通行，所以此多负障碍存在继续通行的可能，所以应进一步根据通过性检测中的越障能力约束条件对地形参数进行识别工作。\n\n(2)结合倾翻角度阈值和通过性尺寸约束的可通行区域识别及结果对比\n\n在4.1关于对轮式移动机器人平台顶起失效和跨越负障碍能力的研究中，得出本课题组的轮式移动机器人平台的侧翻阈值26.7°~35.7°，前翻角度阈值25.7°~31.7°，纵向的越障宽度¤¤限制阈值为357.0mm~424.7mm，最大深度ℎ¤的限制阈值为",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 16,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 28,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 595,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航_s2_c12",
    "source_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航",
    "text": "26.7°~35.7°，前翻角度阈值25.7°~31.7°，纵向的越障宽度¤¤限制阈值为357.0mm~424.7mm，最大深度ℎ¤的限制阈值为100.7mm~204.6mm。通过这两个纵向约束条件即可对整体坡度进行进一步的优化选取得到发生倾翻阈值角度时刻的深度极限。在侧向方向，通过对以相机为原点的坐标系内固定位置出车身底盘动点和地形动点的深度判断，进而得到侧向深度约束。如图4-27所示为通过坡度阈值识别并加以深度约束后并进行图像处理面积估计的可通行区域识别结果。\n\n图4-27 综合约束下可通行区域识别效果\n\n图4-27中蓝色部分为经角度阈值判别后的不可通行区域，绿色部分为经尺寸约束判别后的不可通行区域，红字为对应负障碍区域的编号及像素尺寸面积。未被绿色框圈起和颜色勾画的部分即为行间可通行区域。\n\n图4-27中可以看出，对于此时刻的负障碍地形其坡度、尺寸均不满足通行的条件，从实际拍摄场景，每一个负障碍的边缘均有近90°的坡度变化，且负障碍内部坡度近似为0，负障碍的深度约为220mm。\n\n与融合地面高差、地面坡度、地表粗糙度3种地形因素的2.5D栅格条件判别的可通行区域识别方法",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 17,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 28,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 495,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航_s2_c13",
    "source_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航",
    "text": "[11]相比，综合车辆通过性约束和地形特征结合预测的方法省略了高差和地表粗糙度的算法逻辑过程，提高了可通行区域的识别效率，得到了更精确的车辆实际运动需求的通行区域，充分发挥车辆良好的越障能力；与视觉、激光雷达、IMU融合的可通行区域识别方法相比[9][10][12]\n[13]，本文方法逻辑更为简单，功能更易实现，对于农业成本需求上更为经济耐用且具有较高实用性。",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 18,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 28,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 181,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航_s2_c14",
    "source_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航",
    "text": "4.5 本章小结\n\n本章为解决轮式移动机器人平台在户外种植园行间行驶时，通过对平台通过性分析与地形特征提取的研究，得到了车辆在不同情况下的倾翻角度阈值约束，也得到了基于负障碍地形深度数据的地形坡度计算方法，将高程上各点的坡度值进行纵向、侧向方向的车辆位姿角转化，进而通过对纵向、侧向上的位姿角和倾翻角度阈值限制参数结合识别，得出识别后的可通行区域。并根据能够跨越的最大负障碍宽度、深度参数，得到关于最大越障尺寸的不可通行区域，实现轮式移动机器人平台缓慢行驶速度条件下的安全行驶。\n\n第五章 行间道路识别与路径生成\n\n第五章 行间道路识别与路径生成\n\n针对户外种植园行间场景的道路识别与路径生成系统设计，本文通过构建一个针对户外种植园行间图像的图像语义分割模型，对行间环境信息进行道路识别，并设计了道路提取后的路径生成方式，实现对第四章处理后的可通行区域信号进行道路识别及其路径生成。\n\n在模型选择上，深度卷积神经网络分割模型相比于传统分割模型具有更好的精度与环境抗干扰能力，所以本文选择深度卷积网络模型。在对各个深度卷积神经网络模型调研中，本文最终选择了U-Net网络作为户外种植园场景下的语义分割模型，U-Net网络更适用于小尺度数据集，能在较小数据集的情况下完成端到端训练，获得较好的分割效果。",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 19,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 28,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 549,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航_s2_c15",
    "source_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航",
    "text": "常见的深度学习包有TensorFlow、Caffe和Theano。TensorFlow是有针对张量的命令格式，可安装在Linux、MacOS和Windows系统上。Caffe速度快，适合工业部署，提供了C++、MATLAB、Python接口，但其不够灵活，占用内存较多。Theano支持高性能的符号运算，可将操作指令快速转换成底层代码，提升了执行速度，同时，Theano提供了GPU的透明使用，大大提高了模型组网速度，具有API简单易懂、运行稳定等优点。",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 20,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 28,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 227,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航_s2_c16",
    "source_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航",
    "text": "经过对各深度学习框架在户外种植园环境适应性方面的比较，最终选择TensorFlow深度学习框架作为开发工具，原因有以下几点：首先，TensorFlow支持各种CPU和GPU，可以保证计算的高效。其次，TensorFlow封装了很多高级API，为实验提供了有效的支撑，因为使用这些高级API可以通过几行代码实现复杂的模型构建和训练，并且该工具具备丰富详实的文档辅助，有助于快速上手学习，实现功能。此外，TensorFlow提供了多语言接口，具有较强的可扩展性，支持移动端、服务器等各种设备，具有较强灵活性。最后，TensorFlow集成了Keras，Keras是一个使用Python编写的专精于深度学习领域的框架，具有简洁优美、可移植性强的特点，目前在深度学习领域，Keras提供了最方便易行的模块和接口，在性能没有损耗的条件下，实现了程序的简化。通过TensorFlow，可以调用Keras代码，大大方便了模型组网和训练。\n\n在道路路径生成上，本文采用对经过图像语义分割后的道路图像进行行驶方向和可通行区域双重约束的导航线拟合，实现行间可通行区域的路径生成。本文的整体设计方案如图5-1所示：\n\n图5-1 道路识别与路径生成总体框架",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 21,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 28,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 516,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航_s2_c17",
    "source_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航",
    "text": "5.1 模型组网的构建\n\n本文对U-Net网络基于编码器-解码器结构，模型可分为九个模块，其中左侧四个模块为编码器下采样过程，右侧四个模块为解码器上采样过程。编码器利用下采样的特征图做融合，依靠不断的卷积和池化操作完成对图像深层特征的提取，解码器则依靠转置卷积操作来恢复原图像分辨率，实现最终的图像分割。U-Net网络模型结构如图5-2所示。\n\n图5-2 U-Net网络模型结构\n\n本文使用了U-Net网络结构组成，除经过四次下采样提取特征，四次上采样实现分辨率还原，本文在所有模块的卷积计算后加入了填充，保持了图像卷积后尺寸不变，这样做可以不会丢失图像边缘特征信息，提升分割效果",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 22,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 28,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 290,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航_s2_c18",
    "source_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航",
    "text": "[40]。\n\n在下采样过程中，需要不断进行卷积和池化的操作，在卷积过程中，使用尺寸为3×3的卷积核，使用ReLU作为激活函数，在池化过程中，使用了2×2的池化窗口做最大池化。输入图像尺寸为224×224、深度为3，下采样过程中的网络模型见表5-1。\n\n表5-1 下采样网络模型\n\n输入层 输出尺寸 承接\ninput (224,224,3) —\nconv2d1_1 (224,224,64) input\nconv2d1_2 (224,224,64) conv2d1_1\nmax_pooling2d1 (112,112,64) conv2d1_2\nconv2d2_1 (112,112,128) max_pooling2d1\nconv2d2_2 (112,112,128) conv2d2_1\nmax_pooling2d2 (56,56,128) conv2d2_2\nconv2d3_1 (56,56,256) max_pooling2d2\nconv2d3_2 (56,56,256) conv2d3_1\nmax_pooling2d3 (28,28,256) conv2d3_2\nconv2d4_1 (28,28,512) max_pooling2d3\nconv2d4_2 (28,28,512) conv2d4_1\nmax_pooling2d4 (14,14,512) conv2d4_2",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 23,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 28,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 593,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航_s2_c19",
    "source_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航",
    "text": "下采样结束后，进入中间的第五模块，第五模块是下采样和上采样的中间结合部分，只有两个卷积层。第五模块见表5-2。\n\n表5-2 第五模块网络模型\n\n输入层 输出尺寸 承接\nconv2d5_1 (14,14,1024) max_pooling2d4\nconv2d5_2 (14,14,1024) conv2d5_1\n\n第五模块结束后，进行上采样操作，上采样网络构建见表5-3。\n\n表5-3 上采样网络模型",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 24,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 28,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 199,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航_s2_c20",
    "source_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航",
    "text": "输入层 输出尺寸 承接\nconv2d_transpose_1 (28,28,512) conv2d5_2\nconcat1 (28,28,1024) conv2d_transpose_1\nconv2d6_1 (28,28,512) concat1\nconv2d6_2 (28,28,512) conv2d6_1\nconv2d_transpose_2 (56,56,512) conv2d6_2\nconcat2 (56,56,768) conv2d_transpose_2\nconv2d7_1 (56,56,256) concat2\nconv2d7_2 (56,56,256) conv2d7_1\nconv2d_transpose_3 (112,112,128) conv2d7_2\nconcat3 (112,112,256) conv2d_transpose_3\nconv2d8_1 (112,112,128) concat3\nconv2d8_2 (112,112,128) conv2d8_1\nconv2d_transpose_4 (224,224,64) conv2d8_2\nconcat4 (224,224,128) conv2d_transpose_4\nconv2d9_1 (224,224,64) concat4\nconv2d9_2 (224,224,64) conv2d9_1\nconv2d9_1 (224,224,64) concat4\nconv2d9_2 (224,224,64) conv2d9_1\nconv2d9_3 (224,224,4) conv2d9_2",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 25,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 28,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 689,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航_s2_c21",
    "source_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航",
    "text": "下采样和上采样过程完成后，需要实现最终的图像输出，输出是对每个像素进行分类，需要得出每个像素属于各类别的概率，此时需要使用Softmax函数。ReLU函数是单节点激活函数，除此之外还有多节点激活函数，最常用的是Softmax函数，在U-Net网络中被用来完成多分类任务，Softmax的计算如公式5-1所示。\n\ni V e S je = ∑\n\n其中iS是一个向量，针对¤点，代表该点的回归概率，向量的各个分量代表各个类别的概率，各分量中最大的分量所对应的类别被视为该像素所属类别，j代表标注类别。可以看出Softmax工作的原理是先将输入值进行处理，将其当成幂指数求值，而后通过正则化转化为(0,1)区间上的概率，使各类别的概率总和为1，针对U-Net网络来说，就是每个像素会被给出属于各类别的概率，概率值均为正数，且总和为1，在各类别的概率中的最大值对应的类别将被视作该像素的类别，从此实现像素级分类。\n5.2 模型训练\n\n5.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 26,
    "chunk_index_in_section": 21,
    "total_chunks_in_section": 28,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 415,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航_s2_c22",
    "source_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航",
    "text": "2.1 目标数据来源与获取\n\n由于本文针对户外种植园行间场景，所以采集的图像都来自户外行间场地，包括种植园、公园和野外。通过差异化比对，筛选出其中的53张图像作为原始数据。确定原始数据后需要对其进行标注，标注使用的工具是图像标注软件labelme，见图5-3。标注首先要定义需要划分的类别，标注使用绳索的方式，将目标区域用绳索围成一个封闭区域，然后将此区域划入定义好的类别中，标注过程中要保证各区域之间不能有重叠，因为图像语义分割是像素级分类，每一个像素只能从属于一个固定类别，标注完成后保存至.json文件。\n\n图5-3 图像标注操作界面\n\n通过对图像类别的标注，可以自行设置相关类别的区域，通过后续训练、测试，对个人的感兴趣区域进行学习，为风格多异的图像类别的识别工作提供了便利，帮助我们标注图像，不需要我们在电脑中安装或复制大量的数据集。\n\n模型训练的图像标签对格式有特定的要求，需要将.json文件进行转换，转换成模型可用的标签文件，数据集中数据共分成4类，0代表背景，1代表树木，2代表地面，3代表天空。标签制作完成后，包含原始图像和标注图像数据集初步构建完成，其中的一组数据见图5-4。\n\n图5-4 标注过程图",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 27,
    "chunk_index_in_section": 22,
    "total_chunks_in_section": 28,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 511,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航_s2_c23",
    "source_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航",
    "text": "图5-4 标注过程图\n\n图5-4(a)为原始图像，图5-4(b)中红色部分代表树木，黄色部分代表天空，绿色部分代表地面，未标注部分代表背景。其他样本数据库中的样本图像也都标记如图5-4(b)所示。图像采集过程中，由于图像采集设备不同，导致图像尺寸不一，U-Net网络要求输入的图片是正方形，所以要对数据集图像和标签进行裁剪操作，保证图像和模型的匹配。本文将图像裁剪成了224×224大小的图像，首先导入待裁剪的图像和标注好的标签，为防止裁剪超出图像范围，在图像和标签左上角、距图像右边和下边224像素的矩形里选择随机点，然后向右边和下边扩展224个像素，完成一次裁剪，重复20次完成对1张图像的裁剪，依次对53张图像重复操作。\n\n裁剪的方式是对图像进行随机裁剪，裁剪数量是20，即对53张图像中的每张图像裁剪20次，共获得53×20=1060张224×224的图像作为预处理后的数据集，同时，对应的标签也执行相同操作，裁剪后的原始图像见图5-5(a)，预测图像见图5-5(b)。\n\n图5-5 裁剪后原始图像和预测图像",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 28,
    "chunk_index_in_section": 23,
    "total_chunks_in_section": 28,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 458,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航_s2_c24",
    "source_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航",
    "text": "5.2.2 训练过程\n\n模型组网接下来是模型训练，训练的目的是获得针对本模型的数据集的模型参数，未来用这些参数进行预测。模型的输入图像尺寸为224×224，深度为3，为RGB三通道，共计1060张，其中训练集与测试集的比例为8:2，即训练集有848张，测试集有212张，图像场景包含了种植园、公园和野外。\n\n模型训练时，每次训练的样本数设置为2，训练代数设置为50，训练时需要明确的五个参数，分别是模型指定训练数据、训练代数、训练数据组的数量、验证数据、验证数据组的数量。\n\n模型在训练过程中需要使用损失函数，目的是更新模型参数让模型不断拟合实验数据，TensorFlow框架中有封装好的优化器可以完成梯度下降，本文选择使用Adam优化器进行梯度下降，本文将学习率设置为0.0001，模型配置如下：\n\nmodel.compile(tf.keras.optimizers.Adam(learning_rate=0.0001),loss='sparse_categorical_crossentropy',metrics=['acc'])\n\nAdam可以针对不同的模型参数使用动量和自适应学习率来加快收敛速度，具有计算效率高、内存需求小等优点，非常适合数据量较大或参数较多的场景。\n\n5.",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 29,
    "chunk_index_in_section": 24,
    "total_chunks_in_section": 28,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 542,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航_s2_c25",
    "source_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航",
    "text": "2.3 训练结果\n\n定义好网络和各相关参数后开始进行模型训练，由于训练集数量为1060×80%=848，由于每次训练的样本数设置为2，所以训练数据组的数量为848×20％=424。训练结束后绘制损失函数的变化和正确率的变化，损失函数的变化见图5-6。\n\n图5-6 损失函数的变化\n\n损失函数用来估量模型的预测值( )f x与真实值Y的不一致程度，它是一个非负实值函数，损失函数越小，模型的鲁棒性就越好。随着训练代数的增加，训练集和测试集的损失函数值不断降低，前期梯度下降较快，且训练集快于测试集，训练集在27代基本保持稳定，测试集在22代基本保持稳定。损失函数的下降趋势稳定于33附近\n\n图5-8(a)、(c)、(e)展示了原始图像，而5-8(b)、(d)、(f)则是对应的预测图像。模型在户外种植园行间场景的图像语义分割上表现出较高的准确性，尤其是在树木和地面的分割上。然而，某些区域的分割仍存在不准确的情况。这些不准确的原因包括：图像在裁剪过程中信息的丢失，以及实验数据集数量较少且标注精度较低。\n\n在路径生成方面，通过上采样和反卷积方法减少了图像信息丢失，使用Softmax函数完成像素多分类任务。道路信息提取后，采用道格拉斯-普克算法计算道路轮廓，并根据车辆行驶方向和可通行区域信号确定路径。",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 30,
    "chunk_index_in_section": 25,
    "total_chunks_in_section": 28,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 550,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航_s2_c26",
    "source_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航",
    "text": "本章小结指出，基于U-Net网络的模型在户外种植园行间图像分割上具有较好的性能，尽管面对非结构化和恶劣的道路环境，系统的通过性检测和地形感知能力仍有待提升。研究展望中提到，需要进一步研究车辆通过性理论，并对自制数据集进行扩展和优化。\n\n在总结与展望部分，文章强调了户外种植园行间道路识别与路径生成技术的重要性，并指出当前技术面临的挑战，如缺乏成熟的数据集和复杂的户外环境。文章提出，未来的研究应关注车辆通过性理论的深入探讨，以及融合更多传感器数据以提高系统的鲁棒性和准确性。\n\n---\n\nPeucker Algorithm, Monotonic Chains and Dichotomy[J]. ISPRS International Journal of Geo-Information, 2020, 9(4): 251-",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 31,
    "chunk_index_in_section": 26,
    "total_chunks_in_section": 28,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 362,
    "chunk_method": "hierarchical",
    "importance_weight": 0.4,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航_s2_c27",
    "source_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航",
    "text": "251.\n\n研究成果：\n\n一、发表论文\n\n1. 毕松, 王远航, 李嘉伟, 刘蕾. 户外种植园行间可通行区域识别[J]. 计算机仿真\n\n二、研究成果转化或应用\n\n1. “绿茵先锋”全自动足球识别、抓取与投掷机器人团队设计—2020.10.30—ROBOCON 全国大学生机器人大赛三等奖\n2. “投壶行觞”全自动箭矢抓取、精准投掷机器人团队设计—2020.07.27—ROBOCON 全国大学生机器人大赛三等奖\n\n致谢：\n\n感谢导师毕松教授及校外导师李睿凡先生，IFR 实验室的工作学习经历，对我的耐心培养，拓宽视野，夯实专业基础，丰富科研生活，树立严谨科研态度及正确三观。\n\n感谢实验室同学们，大师兄张潞先生，二师兄王宇豪先生，以及师弟张国轩、韩奕非、隗朋峻、张东航、余鑫和李东先生的帮助与陪伴。\n\n感谢父母及岳父岳母的养育与支持。\n\n感谢未婚妻张静怡女士在研究生期间的鼓励和支持。\n\n经过三年努力，从空白文档到四万余字的论文完成。感谢所有帮助和支持我的人，愿疫情早日退散，一切安好。\n\n---",
    "section_title": "参考文献",
    "section_type": "reference",
    "section_index": 2,
    "total_sections": 3,
    "chunk_index": 32,
    "chunk_index_in_section": 27,
    "total_chunks_in_section": 28,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 448,
    "chunk_method": "hierarchical",
    "importance_weight": 0.42000000000000004,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s0_c0",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "果园行间自主导航关键技术研究\n\n我国水果产量自1990年至2019年居世界首位，果品产业成为农村经济的重要支柱。为提升生产效率和水果品质，农业机械装备的智能化是发展趋势。智能农业装备通过传感器感知环境信息和自身状态，在果园环境中执行任务。果园行间自主导航技术能够降低劳动强度，提高作业效率。本文围绕单目视觉导航和激光雷达导航进行研究，主要工作和创新点如下：\n\n(1) 提出自适应半径滤波方法，动态计算滤波半径以抑制激光点云密度影响，实现多尺度噪声去除。实验结果显示，DBSCAN聚类精确率和召回率提升，计算时间为43ms。\n\n(2) 提出基于交叉像素的可通行区域提取方法，适应性和稳定性强，能准确提取可通行区域、偏航角和横向位移。\n\n(3) 研究基于改进的Mask R-CNN网络提取场景语义信息，分割道路与树干，通过霍夫变换计算边界方程和消失点坐标，为单目位姿估计和视觉导航提供有效参考。\n\n(4) 建立果园行间道路几何成像模型，实现偏航角、横向偏移与道路宽度的定量计算，具有高测量精度和计算效率。\n\n(5) 利用树行共线性和边界平行性，基于单目相机计算果树相对位置，实验结果表明定位精度高，计算时间短。\n\n关键词：自主导航，位姿测量，相机模型，深度神经网络，自适应半径滤波",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 2,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 538,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s0_c1",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "关键词：自主导航，位姿测量，相机模型，深度神经网络，自适应半径滤波\n\nResearch on Key Technologies of Autonomous Navigation Between Rows in Orchard",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 2,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 112,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8400000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c0",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "Chinese fruit production has ranked first worldwide from 1990 to 2019, with the fruit industry becoming a significant pillar of the rural economy. The intelligentization of agricultural machinery is a trend to enhance production efficiency and fruit quality. Intelligent agricultural equipment uses sensors to perceive environmental information and its state for task execution in orchards. Autonomou",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 2,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 59,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 400,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9900000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c1",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "ts state for task execution in orchards. Autonomous navigation between orchard rows reduces labor intensity and improves efficiency. This paper focuses on monocular vision and lidar navigation, with main work and innovations as follows:",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 3,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 236,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c2",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "(1) An adaptive radius filtering method is proposed to dynamically calculate the filter radius, suppressing the impact of LiDAR point cloud density for multi-scale noise removal. Experimental results show increased DBSCAN precision and recall rates with an average calculation time of 43ms.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 4,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 290,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c3",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "(2) A passable area extraction method based on intersecting pixels is proposed, demonstrating high adaptability and stability in accurately extracting passable areas, yaw angles, and lateral displacements.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 5,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 205,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c4",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "(3) An improved Mask R-CNN network is used for scene semantic information extraction, segmenting roads and tree trunks, and calculating boundary equations and vanishing point coordinates using the Hough transform, providing an effective reference for monocular pose estimation and visual navigation.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 6,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 299,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c5",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "(4) A geometric imaging model of orchard roads is established for quantitative calculations of yaw angle, lateral offset, and road width, with high measurement accuracy and computational efficiency.\n\n(5) Relative fruit tree positioning is calculated using monocular camera based on tree row collinearity and boundary parallelism, showing high positioning accuracy and short calculation time.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 7,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 391,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c6",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "Key words: autonomous navigation, pose measurement, camera model, deep neural network, adaptive radius filter.\n\n---\n\n4.2.3 密集种植场景实验与分析\n4.2.4 误差来源分析\n4.3 本章总结\n第五章 基于单目视觉的行间自主导航方法\n5.1 道路与消失点信息提取\n   5.1.1 果园场景实例分割\n   5.1.2 消失点和边界方程提取\n   5.1.3 实验与分析\n5.2 位姿估计方法\n   5.2.1 道路几何成像模型\n   5.2.3 偏航角与横向偏移估计\n   5.2.4 实验与分析\n5.3 果树定位方法\n   5.3.1 果树相对位置估计\n   5.3.2 实验与分析\n5.5 本章总结\n第六章 结论与展望\n   6.1 主要结论\n   6.2 研究展望",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 8,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 395,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c7",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "第一章 绪论\n1.1 研究背景和意义\n1.2 国内外研究现状及存在的问题\n   1.2.1 基于激光雷达的行间自主导航技术\n   1.2.2 基于视觉的行间自主导航技术\n1.3 研究内容\n1.4 组织结构\n\n第二章 行间自主导航理论基础\n2.1 惯性导航理论\n\n---\n\n捷联式惯导技术通过数字计算平台实现导航，提高了结构的可靠性和易用性。其姿态估计方法包括将加速度计和陀螺仪安装在载体上，计算载体坐标系下的角速度，并利用该信息计算姿态矩阵。加速度计和陀螺仪分别用于估计姿态角和角速度，但存在动态性能和累计误差问题。目前，基于概率滤波器的融合导航方法可在一定程度上抑制误差，但长时间工作仍存在累计误差。因此，本文研究了激光雷达导航和单目视觉导航方法。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 9,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 325,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c8",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "激光雷达导航理论包括点云滤波方法和点云密度聚类方法。点云滤波方法包括统计滤波和半径滤波，用于抑制噪声点。统计滤波通过计算点到近邻的平均距离，与阈值比较判断噪声点。半径滤波通过考察点云中指定半径内的相邻点数量判断孤立点。点云密度聚类方法包括K-Means聚类和基于密度的聚类方法，用于对点云进行聚类。\n\n单目视觉导航理论包括单目相机成像原理和目标信息提取方法。单目相机成像原理描述了三维世界坐标点到二维像平面的映射关系。目标信息提取方法包括基于特征分类器的目标检测方法和基于深度神经网络的目标检测方法。基于深度学习的实例分割模型如Mask R-CNN，可同时完成目标识别和实例分割，具有较高的精度和适应性。\n\n综上所述，本章分析了惯性导航、激光雷达导航和单目视觉导航的理论基础，为后续研究提供了理论支持。\n\n清洗后的内容如下：",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 10,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 362,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c9",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "清洗后的内容如下：\n\n反射的表面或者探测超出量程的目标，失落信息存在于无目标处或目标距离较远处。距离反常主要存在于目标边缘，枝叶相对密集和细小，激光点存在发射至枝叶边缘的可能，进而导致距离反常。\n\n3.1.2 激光点云密度特性\n\n单位矩形内的点云平均间距是衡量点云密度的重要指标，平均距离越大，点云密度越低；点云间的平均间距越小，点云密度越高。单位矩形中，点云数量为激光雷达光束与单位平面交点的数量，计算方法如公式(3-1)所示，数据点间距为面积的几何平均数，即单位面积除以点云总数的几何平均数，计算方法如公式(3-2)所示，点云密度与点云平均间距的关系呈负线性相关，计算方法如公式(3-3)所示，计算方法对应示意图如图3-1所示。\n\n图3-1 点云平均间距计算方法示意图\n\n1 1 2 2 2arctan 2arctan\n\nl l m r r = ×\n\nh w s m = × (3-2)",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 11,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 395,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c10",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "l l m r r = ×\n\nh w s m = × (3-2)\n\ns c ρ=−+ (3-3)\n\n式中 m——单位矩形的点云数量\n\ns——点云的几何平均间距\n\nρ——点云密度\n\nh——单位矩形高\n\nw——单位矩形宽\n\nl——激光雷达到单位矩形的距离\n\nrh——水平方向激光雷达的角分辨率\n\nrv——垂直方向激光雷达的角分辨率\n\nc——点云单位间距，几何意义为单位面积内仅有单个点云时的平均间距\n\n基于以上公式推导，研究变量rh、rv、l和点云间距s、点云密度ρ之间的关系，文中采用等边单位矩形，即h、w相同。单位矩形垂直于激光雷达中心o与y轴的延长线上，且矩形中心o'与oy共线，四者关系如图3-2、图3-3和图3-4所示。\n\n图3-2 s-l关系曲线\n\n图3-3 ρ-l关系曲线\n\n0 0.5 1 1.5 激光雷达与单位矩形的距离",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 12,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 367,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c11",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "图3-3 ρ-l关系曲线\n\n0 0.5 1 1.5 激光雷达与单位矩形的距离\n\n水平角分辨率0.1度，垂直角分辨率0.1度 水平角分辨率0.1度，垂直角分辨率0.2度 水平角分辨率0.1度，垂直角分辨率0.3度 水平角分辨率0.2度，垂直角分辨率0.2度 水平角分辨率0.2度，垂直角分辨率0.3度 水平角分辨率0.3度，垂直角分辨率0.3度\n\n图3-4 ρ-l关系曲线(近距离区域)\n\n由图3-2、图3-3和图3-3可知，平均间距s随距离l增加而增加，点云密度ρ随之降低，在近距离区域，点云密度ρ加速降低。其他区域的点云密度ρ与距离l呈近似线性负相关，表明此范围中的点云密度随探测距离的增加而线性减小。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 13,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 302,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c12",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "经理论分析可知：理想条件下，当单位矩形与激光雷达的距离为0时，点云密度达到最大值，即ρ=1；当点云密度等于0时，单位矩形内的点云处于消失的距离l相同的条件下，激光线越密集，分辨率越高，角分辨率rv、rh数值越小，平均间距s越小，表明具有更高的点云密度ρ。由于等边矩形的h、w相同，根据公式(3-1)、公式(3-2)可知，此条件下的rv、rh具有对称性，例如，rv=0.1，rh=0.2与rv=0.2，rh=0.1两种条件下的点云密度ρ一致。为进一步研究点云密度与距离的关系，本文对公式(3-3)进行求导，以点云密度的变化率，如公式(3-4)和图3-5所示。\n\n2 2 2 2 2 1 1 2 1 1\n\n8 tan tan 1 8 tan t\n\nan 1 2 2 4 2 2 4 '\n\n4tan tan 2 2\n\nh v h v\n\nw w w l l l l l l l l",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 14,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 386,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c13",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "4tan tan 2 2\n\nh v h v\n\nw w w l l l l l l l l\n\nh r r hw r r\n\nh r r ρ\n\n− − − −\n\n[ ρ(l) ] [ ρ(l) ]\n\n图3-5 ρ’(l)-l关系曲线\n\n经分析图3-4和图3-5可知，在激光雷达附近区域(1.5米内)的点云密度变化率ρ’(l)逐渐减小，点云密度平缓加速降低(如图3-4所示)；在1.5米以外区域，点云密度变化率ρ’(l)无明显变化，近似为常数，点云密度ρ(l)与距离l可近似为线性负相关。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 15,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 240,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c14",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "综合以上分析，除激光雷达附近的点云外，其他区域的点云密度近似线性减小。激光雷达常用于较远距离点云信息的获取，点云密度非线性变化的区域集中在激光雷达附近，所占比重较低，而且本文所用激光雷达的探测范围为0.4m-28m，在此区域点云密度近似线性变化。激光雷达输出点云密度具有随目标点距离增大而减小的特性，其点云密度分布示意图如图3-7所示。\n\n图3-7 激光雷达点云密度分布示意图\n\n由图3-7可知，近距离区域和远距离区域的点云密度差异较大，点云密度与探测距离近似线性负相关，点云噪声密度分布同样符合以上特性，A1、A2、A3以r为半径的邻域内分别具有1、4、15个近邻，其数量差异明显，由于半径滤波参数固定，在场景中，容易出现过度去噪或噪声残留问题，实际效果并不理想。\n\n3.2 自适应半径滤波原理",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 16,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 348,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c15",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "3.2 自适应半径滤波原理\n\n含有噪声的激光点云果园机器人工作场景可分为密集种植环境和非密集种植环境，水果种植园行间纵深较大、光线复杂多变，导致激光点云密度具有较大的差异性，噪声具有较高的随机性。\n\n在密集种植场景中，激光点云呈连续立面状；在非密集种植场景中，激光数据表现为若干点云簇。由于不同种植模式中的点云簇形状和大小均存在差异性，为更有效地抑制噪声，需要根据种植模式调整滤波器参数。为使滤波器参数对种植模式有更高的适应性，也为避免在运行过程中整定方法参数，本文构建了基于卷积神经网络的种植模式判定器，并支持预置滤波器参数，实现了根据点云数据特征自动选取相应的去噪参数。\n\n为使滤波器参数与点云密度匹配，适应密度差异大且随机性强[37-38]的激光点云噪声，提出动态半径滤波器，该方法的滤波半径可根据点云密度动态调整，适应不同密度的点云噪声，从而实现对多尺度激光点云噪声的有效抑制。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 17,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 393,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c16",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "本文提出的自适应半径滤波器由点云图像转换器、种植模式判定器和动态半径滤波器组成。由于激光点云通常呈无序排列，其处理速度相对较慢，而图像数据通常为多维矩阵，处理方法更加多样且高效，更适合用于场景分类。滤波器结构如图3-8所示。\n\n图3-8 自适应半径滤波器结构图\n\n由图3-8所示，自适应半径滤波器首先利用点云图像转换器将从激光雷达中获取原始点云数据转换为二值图像。其次，场景判定层根据二值图像判断当前种植模式，并将场景类别信息传递至下一层。最后，点云去噪层将根据点云密度去噪，当处理完所有信息点，输出去噪结果。自适应半径滤波器周期运行，从而使该模型具有对种植模式变化的场景具有良好的适应性。\n\n3.2.1 种植模式判定器",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 18,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 310,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c17",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "3.2.1 种植模式判定器\n\n果园机器人运行过程中，数据采集角度和位置均具有较大的不确定性，且各果园的株距行距不尽相同，导致激光雷达可探测的范围不同。机器人在上述条件作业时，激光点云及其二值图的特征变化明显，难以用单一特征判别种植模式。\n\n目前，大部分传统的特征提取方法提取的特征相对较浅，如HOG特征、SIFT特征、颜色特征、局部二值特征等，基于传统特征的分类器适用于某些特定场景。在复杂多变的场景中，表现并不尽如人意，难以满足种植模式的分类需求。\n\n深度卷积神经网络具有模型层次深、特征表达能力强的特点，能自适应地从大规模数据集中学习当前任务所需要的特征表达，广泛应用于图像分类问题。因此，本文采用深度卷积神经网络作为种植模式判别器。卷积神经网络以转换后的二值图像为输入，判断并输出其种植模式。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 19,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 349,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c18",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "由于该方法应用于实时性较高的场景，所以，在轻量化模型MobileNetV2[39]的基础上，本文重新调整并精简了瓶颈层(Bottleneck)结构和卷积核数量，网络结构如图3-9所示。\n\n图3-9 网络模型示意图",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 20,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 106,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c19",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "图3-9 网络模型示意图\n\n图3-9中，Conv3BN为3×3卷积正则化层，Conv1BN为1×1卷积正则化层，DWConv3BN为3×3深度可分离卷积正则化层，深度可分离卷积能显著降低参数量和运算量，在轻量化网络中广泛使用[40]。首先，二值图经过卷积正则化层提取3张112×112特征图，之后经过瓶颈层和瓶颈残差层[39]提取更高维特征。为使网络更加高效，在网络中主要使用深度可分离卷积提取特征。此外，在网络中加入残差模块，此方法可有效抑制梯度消失和梯度爆炸问题。在网络末端，将64张7×7特征图进行全局池化，转换为特征向量，由于共有密集种植和非密集种植两个类别，因此，经全连接层变换为含有两个元素的一维向量。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 21,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 307,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c20",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "本文在训练阶段利用Softmax分类器将预测类别与标签类别比较得到其交叉熵损失，进而使用Adam算法[41](Adaptive moment estimation algorithm，Adam)优化损失函数使其收敛。在推理阶段，神经网络输出当前图像所属标签，即当前种植模式，读取预置的去噪参数后，将两类信息传递至动态半径滤波算法，从而有针对性地抑制不同种植模式下的点云噪声。\n\n3.2.2 动态半径滤波\n\n半径滤波方法的效果与选取的半径参数相关，而半径参数与区域点云密度相关。若探测范围内点云密度相对均匀，则根据点云密度可选取适当的半径参数；若点云密度差异较大，则应根据点云密度动态改变去噪操作的半径。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 22,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 301,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c21",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "降低点云密度变化的影响是提高去噪准确性的关键方法之一。点云距离激光雷达越远，其密度越稀疏，滤波半径应增大。本文根据目标点到原点的欧式距离设计该点的滤波半径，从而有效地避免了密度对滤波过程的影响，为降低算法时间和空间复杂度，将滤波半径与探测距离视作线性相关，如公式(3-7)所示。\n\ni i r K d = × (3-7)\n\n2 2 2 i i i i d y x z = + + (3-8)\n\n式中 ri——滤波半径\n\ndi——目标点与原点的欧式距离，本文中激光雷达所在位置为原点\n\nK——滤波半径相关系数\n\nxi,yi,zi——目标点的欧式坐标\n\n动态半径滤波的具体方法如下：",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 23,
    "chunk_index_in_section": 21,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 289,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c22",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "K——滤波半径相关系数\n\nxi,yi,zi——目标点的欧式坐标\n\n动态半径滤波的具体方法如下：\n\n①根据场景类别初始化滤波参数：滤波半径相关系数K和近邻数量N，其中，K为目标点欧式距离的缩小比例，K值越大，则滤波半径越大，K值越小，则滤波随之减小；N为近邻数量，当邻域内的点云数目小于N时，删除该点；反之，则保留数据点。\n\n②依据公式(3-7)、公式(3-8)计算某目标点去噪参数：欧式距离di和滤波半径ri。\n\n③统计其滤波半径ri内的近邻点ni，若ni小于近邻数量N，则该点为离群点，删除该点；否则予以保留。当按上述步骤遍历所有目标点后，完成当前数据帧处理。\n\n本文提出的噪声去除方法的伪代码3-1如下：\n\n伪代码3-1 自是半径滤波伪代码\n\nAlgorithm 自适应半径滤波方法 Input: 原始点云 Output: 去噪后的点云\n\nwhile 接收到完整点云数据帧 do",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 24,
    "chunk_index_in_section": 22,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 391,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c23",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "while 接收到完整点云数据帧 do\n\n转换原始点云为二值图；场景类别 = 改进MobileNetV2(二值图)；if 场景类别 == “密集种植” then\n\n动态半径滤波器(K1,N1,原始点云)；else\n\n动态半径滤波器(K2,N2,原始点云)；End function 动态半径滤波器(K,N,原始点云)\n\nfor i = 1 to 点云数量 do\n\n2 2 2 i i i i d y x z = + + (3-8)\n\nri = K di if 以pi为中心,ri为半径的邻域内的点云数量小于N then\n\n删除该点；else\n\n保留该点；end end\n\n3.3 实验与分析",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 25,
    "chunk_index_in_section": 23,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 295,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c24",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "删除该点；else\n\n保留该点；end end\n\n3.3 实验与分析\n\n为保证数据集能够较好地反映自然条件下树林的真实特点，分别在苹果种植园、白杨树林和旱柳树林进行实验，点云采集设备为北醒光子CE30-D固态面阵激光雷达，其固有参数为探测范围0.4m-28m，视场角60°×4°，分辨率320×20。\n\n此类场景中，激光雷达主要受到两类噪声的干扰，一是天空、树行尽头等无目标处产生的失落信息，二是树干、树枝等目标边缘产生的逸出值。实验场景及其噪声来源具有较强的代表性，且包含的信息复杂程度与农业自动导航机器人的一般工作环境相似。\n\n同时，为验证算法有效性，将本文去噪方法与点云库PCL中的统计滤波、半径滤波进行对比测试，实验平台为Intel(R) Core(TM) i7-6850K CPU@3.60GHz，NVIDIA 1080Ti×4 GPU，32GB RAM，500GB SSD。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 26,
    "chunk_index_in_section": 24,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 393,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c25",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "3.3.1 种植模式判定器性能与分析\n\n本文共收集1083张点云数据，其中，477来自采用密集矮化种植的苹果园，在胸高位置，密集种植的植株间不存在明显间隙；285张来自非密集种植的白杨树林，321张来自非密集种植的旱柳树林，在此种植条件下，植株间在胸高位置存在明显间隙。\n\n训练神经网络时，先将点云数据转换成俯视视角的二值图像，点云转二值图平均耗时为每张0.312 ms。数据集的60%作为训练集，20%作为验证集，20%作为测试集。配置网络超参数如下：批处理大小为128张，学习率为0.05，Adam优化器，同时，模型训练过程中，使用随机旋转的方法增强数据。采用边训练边评估的方式，共训练1000周期，过程的平均损失值、训练精度和验证集精度如图3-10、图3-11、图3-12所示。\n\n0 200 400 600 800 1000 0.0\n\n图3-10 训练过程的损失值",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 27,
    "chunk_index_in_section": 25,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 386,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c26",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "0 200 400 600 800 1000 0.0\n\n图3-10 训练过程的损失值\n\n0 200 400 600 800 1000 0.7\n\n训练集精确度\n\n图3-11 训练集精确度\n\n0 200 400 600 800 1000 0.0\n\n验证集精确率\n\n图3-12 验证集精度\n\n由于训练集和验证集存在差异性，且训练初期的模型参数变化明显，因此，验证集精度呈波动上升趋势，如图3-12所示。本文为抑制模型过拟合现象，在训练时随机旋转数据集角度，增加了训练数据的复杂性和多样性，加剧了训练集和验证集的差异，导致验证集精度曲线波动更加明显。随着训练周期的增加，模型参数逐渐收敛，分类能力趋于稳定。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 28,
    "chunk_index_in_section": 26,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 299,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c27",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "训练至702周期时，分类器在训练集的精确率为0.99，验证集的精确率为0.98，综合效果较好，仅使用CPU的条件下，推理速度平均为每帧7.51ms；如果使用GPU，推理速度为每帧7.04ms。测试结果如图3-13所示。\n\n(a)密集种植 (b)非密集种植\n\n图3-13 模型测试结果\n\n由图3-13可知，该模型可准确分类不同拍摄角度和位置的场景类别，数据中包含失落信息、逸出值等类型噪声，模型仍可准确分类，且计算时间较低，具有强的鲁棒性和高效性。\n\n3.3.2 去噪结果分析\n\n点云去噪不仅应准确去除远处、近处和物体边缘的噪声，而且目标点云簇的细节和特征应被充分保留，为聚类、识别和融合等操作奠定基础。由于点云去噪为不可逆操作，因此本文在整定去噪参数时，以保留充足的信息量为原则，兼顾去噪效果和目标细节。经过实验分析，三种方法去噪的参数如表3-1所示：\n\n表3-1 去噪参数",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 29,
    "chunk_index_in_section": 27,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 387,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c28",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "表3-1 去噪参数\n\n滤波方法 密集种植场景 非密集种植场景\n\n半径滤波 近邻数量 6 30 滤波半径 0.3 0.8\n\n统计滤波 近邻数量 30 30 标准差倍数 2 0.5\n\n本文方法 近邻数量 11 30 滤波半径相关系数 0.072 0.066\n\n1.苹果果园实验结果\n\n实验地点为北京市昌平区苹果果园，该果园采用矮化密植的种植模式，株行距为1×3米，如图3-14所示。\n\n图3-14 苹果果园实验场景\n\n在图3-14所示的场景中，点云噪声主要来自两个方面：一是枝叶边缘带来的逸出值，如图3-15(a)的虚线框所示；二是部分激光束穿过枝叶间隙到达无目标处，带来失落信息，表现为近距离密集噪声，如图3-15(a)的实线框所示。去噪方法应有效抑制目标边缘的逸出值和近距离密集噪声，且保留丰富的点云信息。\n\n图3-15 苹果果园实验结果",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 30,
    "chunk_index_in_section": 28,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 369,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c29",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "图3-15 苹果果园实验结果\n\n图3-15(b)为统计滤波结果，近距离区域存在大量密集噪声；在远\n\n清洗后的内容如下：\n\n抑制离群噪声和拖点，但近距离区域内存在大量密集噪声，半径滤波在远距离区域目标数量和细节相对更少。图3-19(d)为本文提出的自适应半径滤波实验结果，稀疏离群点、密集噪声和目标周围的拖点被有效抑制，目标数量和细节损失更少。\n\n4.计算效率分析\n\n综合比较三种去噪方法，由于半径滤波和统计滤波参数固定，容易出现细节损失和噪声残留的问题，本文方法根据点云密度动态调整去噪参数，可有效抑制不同点云密度中的噪声，且目标特征保留相对完整。为比较方法计算效率，本文统计了三种滤波器的去噪时间，如表3-2所示。\n\n表3-2 去噪计算时间分析\n\n统计滤波 半径滤波 本文方法\n\n去噪耗时/ms",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 31,
    "chunk_index_in_section": 29,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 348,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c30",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "表3-2 去噪计算时间分析\n\n统计滤波 半径滤波 本文方法\n\n去噪耗时/ms\n\n苹果果园 36.1 35.4 43.1 白杨树林 52.1 51.9 45.6 旱柳树林 48.5 47.4 40.8 平均值 45.6 44.9 43.2\n\n由表3-2可知，与半径滤波和统计滤波相比，本文方法的去噪时间(已包含神经网络分类时间)并未明显增加。一方面，由于以上算法均基于领域分析，去除邻域中点云的数量越多，算法迭代次数越少，耗时越短，本文方法的噪声去除率更高，因此，去噪耗时更短。另一方面，本文优化了神经网络模型，使网络模型更加符合使用条件，通过合理地裁剪卷积层、广泛使用深度可分离卷积、减少池化层等方式，不仅取得了较高的算法效率，而且保证了网络的分类精度。\n\n第三章 基于自适应半径滤波的激光点云去噪方法\n\n5.密度聚类相关分析",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 32,
    "chunk_index_in_section": 30,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 363,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c31",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "第三章 基于自适应半径滤波的激光点云去噪方法\n\n5.密度聚类相关分析\n\n本文聚类精确率、聚类召回率和F1分数四个维度评价算法性能。本文将原始点云、去噪后的点云进行DBSCAN聚类，聚类结果包含3类信息：噪声点云簇、目标点云簇和未被聚类点云。本文把聚类结果的精确率(Precision)、召回率(Recall)和F1分数的计算方法为：\n\nTPA=(3-9)\n\nTRG=(3-10)\n\n1 2 PRFPR×=×+ (3-11)\n\n式中P——聚类精确率\n\nR——聚类召回率\n\nF1——F1分数\n\nT——聚类结果中，目标点云簇数量\n\nA——聚类结果中，目标点云簇和噪声点云簇数量之和\n\nG——实际目标点云簇数量",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 33,
    "chunk_index_in_section": 31,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 301,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c32",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "A——聚类结果中，目标点云簇和噪声点云簇数量之和\n\nG——实际目标点云簇数量\n\n点云的噪声数量直接影响聚类或识别的精度，经过去噪的点云有利于提升聚类的精确率和召回率。首先，对原始数据依次用统计滤波、半径滤波和本文提出自适应半径滤波进行去噪，并统计去噪时间，本文方法的去噪时间由三部分组成：点云转换图像、神经网络推理和自适应半径滤波；其次，针对白杨树林和旱柳树林原始数据和滤波结果，依次进行DBSCAN聚类，计算其精确率、召回率和F1分数。由于苹果种植园采用密集种植方法，点云呈连续的立面状，不适合做聚类分析。白杨树林和旱柳树林的聚类真值如图3-20(b)和图3-21(b)所示。\n\n(a)采集场景 (b)聚类真值\n\n图3-20 白杨树林采集场景及聚类结果\n\n第三章 基于自适应半径滤波的激光点云去噪方法\n\n(a)采集场景 (b)聚类真值\n\n图3-21 旱柳树林采集场景及聚类结果",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 34,
    "chunk_index_in_section": 32,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 389,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c33",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "(a)采集场景 (b)聚类真值\n\n图3-21 旱柳树林采集场景及聚类结果\n\n图3-20(b)与图3-21(b)中的矩形框中为目标点云簇，白杨树林和旱柳树林的聚类真值分别为20和22，其余部分为点云噪声或信息量很少的点云。由图3-17、图3-19可知，DBSCAN算法能够在含有较多噪声的激光点云数据中，较准确地完成聚类分割，对明显的离群噪声有较强的抑制效果。但由于果园场景较大且复杂多变，点云数据包含较多的失落信息和逸出值，基于原始数据的聚类结果精确度较低，而基于去噪点云的聚类效果有较明显的提升。结合图3-17与图3-19中的聚类信息，计算精确率、召回率和F1分数，结果如表3-3所示。\n\n表3-3 DBSCAN聚类结果\n\n去噪方法 原始数据 统计滤波 半径滤波 本文方法\n\n白杨树林 0.500 0.875 0.933 0.944\n\n旱柳树林 0.579 0.938 0.923 0.941",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 35,
    "chunk_index_in_section": 33,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 397,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c34",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "旱柳树林 0.579 0.938 0.923 0.941\n\n平均值 0.539 0.906 0.928 0.943\n\n白杨树林 0.400 0.700 0.700 0.850\n\n旱柳树林 0.500 0.682 0.545 0.727\n\n平均值 0.450 0.691 0.623 0.789\n\n白杨树林 0.444 0.778 0.800 0.895\n\n旱柳树林 0.537 0.789 0.686 0.821\n\n平均值 0.491 0.784 0.743 0.858",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 36,
    "chunk_index_in_section": 34,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 235,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c35",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "平均值 0.491 0.784 0.743 0.858\n\n根据表3-3可知，聚类原始数据的平均F1分数为0.491，而统计滤波、半径滤波和自适应半径滤波的平均F1分数分别为0.784、0.743、0.858，相比提升29.3%，25.2%、36.7%。经本文提出的自适应半径滤波方法去噪后，DBSCAN聚类的平均精确率和平均召回率的提升40.4%和33.9%，高于统计滤波和半径滤波，也表明该方法的有效性。综上所述，基于自适应半径滤波方法和DBSCAN聚类方法，能够实现准确、稳定地聚类分割激光点云数据，为点云后续处理提供更可靠的数据。\n\n6.综合分析\n\n在诸如苹果果园、白杨树林和旱柳树林等大场景中，点云密度相差较大，本文中的数据点间平均距离相差约70倍，而半径滤波、统计滤波的参数固定，不能根据点云密度动态调整，去噪效果容易受到点云密度、数据点间距的影响，实际效果并不理想。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 37,
    "chunk_index_in_section": 35,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 389,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c36",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "经分析，在密集种植和非密集种植场景中，激光点云的形态特点差异较大，通常需要采用多组去噪参数，本文通过提前预置参数并使用神经网络分类器的方式，有效地提升了方法的适应性。\n\n本文提出动态半径滤波器，利用点云密度与探测距离近似负相关的特性，根据目标点到原点的欧式距离，有针对性地设计该点的滤波半径，从而有效地避免了点云密度差异对去噪的影响，实现有效抑制噪声的同时，保留更加丰富的场景信息。\n\n该方法共有近邻数量和滤波半径相关系数两个参数：近邻数量相同时，增大滤波半径相关系数，噪声去除能力有所减弱；在滤波半径相关系数一定时，适当增大最小近邻数可提升滤波效果。经实验：在密集种植场景，点云呈连续里面状，选择较多的近邻数量和较大的滤波半径相关系数有利于保持点云特征并提高去噪效果；而在非密集种植场景，激光数据表现为若干点云簇，近邻数量较多且滤波半径相关系数较大时，目标信息损失较少，噪声抑制效果更加显著。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 38,
    "chunk_index_in_section": 36,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 397,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c37",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "综上所述，本文提出的自适应半径滤波不仅能有效去除稀疏离群噪声，而且对密集噪声、目标周围的噪声同样有较明显的抑制效果。此外，与半径滤波和统计滤波相比，耗时基本一致，具有较高的实时性，可适用于密集种植、非密集种植场景。自适应半径滤波保留了更加丰富的目标信息，较明显地提升了聚类精确率和召回率，有利于提升后续点云处理的效果。\n\n3.4 本章总结\n\n本章主要研究内容由两部分组成：一是激光雷达噪声来源及密度特性，二是激光点云去噪方法。在果园条件中，激光雷达噪声主要来源无目标处的失落信息和微小目标处的逸出值，激光点云密度差异较大，在工作范围内，点云密度对探测距离的导数值约为常数，表明点云密度呈线性减小。基于点云密度特性，提出自适应半径滤波方法，该方法由种植模式判定器和动态半径滤波组成，判定器用于识别当前种植模式并确定去噪参数，动态半径滤波器基于数据点的欧式距离设计滤波半径，实现与点云密度相适应。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 39,
    "chunk_index_in_section": 37,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 396,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c38",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "实验结果表明：本文提出的自适应半径滤波方法可明显抑制多尺度噪声，对密集噪声和稀疏离群点均可有效去除，基于优化设计的卷积神经网络和较高的噪声去除率，该方法的去噪时间未明显增加，具有较高的效率，为后续点云处理提供可靠的点云数据。\n\n第四章 基于激光雷达的行间自主导航方法\n\n第四章 基于激光雷达的行间自主导航方法\n\n从激光雷达获取的数据通常为无序点云，即由数据点位置构成的列表，与矩阵排列的数据相比，此类数据处理方法相对复杂且计算量大，而自主导航技术需要算法具有高效性、稳定性和鲁棒性，农业机械装备自主运行需要感知周围目标类别和方位，因此，本文将点云转化为距离二值图后，进行形态学处理提取感兴趣区域，提出交叉像素的可通行区域提取与位姿估计方法，在密集种植和非密集种植条件下，具有良好的鲁棒性和稳定性。\n\n4.1 算法原理及步骤",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 40,
    "chunk_index_in_section": 38,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 361,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c39",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "4.1 算法原理及步骤\n\n准确提取农业机械装备的可通行区域并估计其位姿是实现自主导航的关键技术，果园场景中，可通行区域一般由左右两行植株构成的空间组成，左右两行植株在激光雷达数据中表现为高密度、大数量的点云簇，而其他区域中的点云密度和数量均较低，激光雷达数据通常为无序点云，且处理三维数据的时间和难度均大于二值图。基于以上分析，本文提出基于交叉像素的激光雷达导航方法，其流程如图4-1所示。\n\n连通域分析 椭圆离心率分析\n\n直线簇与ROI交叉像素数量\n\n图4-1 基于激光雷达的导航方法流程图\n\n由图4-1可知，本文方法由两部分构成：由数据转化、图像预处理、感兴趣区域提取等方法构成图像处理模块，由直线簇生成、交叉像素统计、位姿估计等方法构成导航信息提取模块，具体流程与原理如下：\n\n1.参数初始化",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 41,
    "chunk_index_in_section": 39,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 349,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c40",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "1.参数初始化\n\n由于激光雷达探测范围和导航精度需求不尽相同，本文通过细腻度参数调整二值图分辨率。细腻度RW、RH越高，二值图尺寸W、H越大，点云转换至二值图的量化误差越低，有利于提升导航信息的准确性，但也增加了数据计算量。图像尺寸由公式(4-1)和公式(4-2)确定。\n\nLWR=(4-1)\n\nLHR=(4-2)\n\n式中LW——激光雷达探测宽度\n\nLH——激光雷达探测长度\n\nRW——宽度细腻度，物理意义为单个像素代表距离值\n\nRH——高度细腻度，物理意义为单个像素代表距离值\n\n2.转换点云为二值图",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 42,
    "chunk_index_in_section": 40,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 251,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c41",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "RH——高度细腻度，物理意义为单个像素代表距离值\n\n2.转换点云为二值图\n\n为最大程度地保留信息，本文二值图包含距离信息，细腻度参数为单颗像素大小，以原点为基准，可进一步确定每颗像素对应的距离和位置。根据初始化中的图像尺寸生成矩阵I(1)，遍历所有点云数据pi(xi,yi,zi)，i=1,2,...,n，根据信息点坐标判断所属像素，根据公式(4-3)转换为二值图，并对图像I(1)依次采用孔洞填充、腐蚀、膨胀处理方法完成离群点去除与预处理。\n\n;1yi/xiRWRH=I(4-3)\n\n3.感兴趣区域提取",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 43,
    "chunk_index_in_section": 41,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 252,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c42",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": ";1yi/xiRWRH=I(4-3)\n\n3.感兴趣区域提取\n\n针对非密集种植场景的二值图数据，树干区域呈椭圆状，可根据其形态特点确定树干感兴趣区域。本文分析了图像连通域并标记每个独立区域，计算每个区域的离心率ξ与面积σ。若同时满足离心率阈值和面积阈值，则标记为感兴趣区域，生成对应图像I(2)，通过计算I(2)中感兴趣区域质心像素坐标可进一步估计相应树干的位置。\n\n4.计算交叉像素\n\n根据搜索角度和横向偏移确定斜率集合K={k1,k2,...,kp}与截距集合C={c1,c2,...,cq}，并计算直线矩阵Di,j，该矩阵由为横轴向量a和纵轴向量b组成，相应计算方法如公式(4-4)所示。创建用于存放直线像素点的图像矩阵I(3)，统计I(2)、I(3)交叉像素数量，即I(2)、I(3)矩阵点积运算后所有元素的和，计算方法如公式(4-5)所示。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 44,
    "chunk_index_in_section": 42,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 374,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c43",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "[[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 45,
    "chunk_index_in_section": 43,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 398,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c44",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]]",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 46,
    "chunk_index_in_section": 44,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 400,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c45",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 47,
    "chunk_index_in_section": 45,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 400,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c46",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "[[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 48,
    "chunk_index_in_section": 46,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 398,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c47",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]]",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 49,
    "chunk_index_in_section": 47,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 400,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c48",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 50,
    "chunk_index_in_section": 48,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 400,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c49",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "[[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 51,
    "chunk_index_in_section": 49,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 398,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c50",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]]",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 52,
    "chunk_index_in_section": 50,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 400,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c51",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 53,
    "chunk_index_in_section": 51,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 400,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c52",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "[[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 54,
    "chunk_index_in_section": 52,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 398,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c53",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[ ]] [[",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 55,
    "chunk_index_in_section": 53,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 139,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c54",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "第五章 基于单目视觉的行间自主导航方法\n\n与城市道路条件相比，农业果园存在密集的遮挡现象，通常也不具备可用于导航的地图，因此，自主导航逐渐成为果园机器人自主运行的关键技术。本文提出单目视觉的位姿估计和果树定位方法，方法流程如下：首先，基于Mask R-CNN深度神经网络提取道路和果树掩码；其次，根据掩码信息进一步提取消失点坐标和道路边界方程；最后，基于建立的道路成像几何模型，估计当前的位置和姿态，并计算果树的相对位置。此外，本文基于三次B样条曲线，选取起始点、消失点和中心关键点拟合导航基准线。\n\n5.1 道路与消失点信息提取\n\n深度神经网络具有强大的特征拟合和学习能力，可在较复杂的场景中完成目标检测、图像分割等任务。本文采用Mask R-CNN深度神经网络模型完成实例分割任务。其次，基于霍夫变换和角度阈值计算左右边界方程，联合边界方程所得的交点坐标为道路消失点。\n\n5.2 位姿估计方法",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 56,
    "chunk_index_in_section": 54,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 398,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c55",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "5.2 位姿估计方法\n\n本文利用果园边界的平行性，建立道路几何成像模型，可通过消失点坐标与边界线方程实现偏航角、横向偏移与果树位置的定量计算。首先，求解相机像平面方程；其次，计算边界线在像平面中的映射方程；最后，分析方程斜率和消失点表达式，逆解航向信息和横向位移。\n\n第五章 基于单目视觉的行间自主导航方法\n\n5.2.1 位姿估计方法\n\n首先，根据边界关键点与焦点构成的直线参数方程，联立像平面方程与直线方程，解得交点坐标。然后，将交点的世界坐标转换至图像坐标系，计算其在向量上的投影。接着，确定直线在图像坐标系内的斜率、截距和方程。最后，根据透视投影原理，利用消失点坐标估计偏航角，再由像平面中的边界线方程估计横向偏移。\n\n5.2.2 偏航角与横向偏移估计\n\n首先，计算消失点坐标。然后，推导偏航角计算方法。再次，计算与边界S1和S2的距离。最后，计算道路宽度和横向偏移。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 57,
    "chunk_index_in_section": 55,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 387,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c56",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "5.2.3 实验与分析\n\n实验结果表明，偏航角平均误差为2.91%，横向偏移平均误差为4.82%，道路宽度平均误差为4.89%，精度可满足农业装备导航需求。\n\n5.3 果树定位方法\n\n5.3.1 果树相对位置估计\n\n根据边界线像素-基本坐标系的相互映射关系，利用树干的像素坐标估计其相对位置。\n\n5.3.2 实验与分析\n\n实验结果表明，横向位置平均误差为3.80%，纵向位置平均误差为2.65%，株距测量误差小于10%，具有较高的准确性和稳定性。\n\n5.4 本章总结\n\n本章研究了基于单目视觉的位姿估计与果树定位方法，实验结果表明该方法具有较高的准确性和稳定性。\n\n第八部分内容：",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 58,
    "chunk_index_in_section": 56,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 290,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c57",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "第八部分内容：\n\n与可通行区域的识别和像素级分割密切相关。在不同拍摄位置与角度下，所提方法能稳定准确地计算消失点与边界线方程，具有高鲁棒性和适应性。对于激光雷达导航方法，提出的自适应半径滤波方法和基于深度神经网络的判定器，以及动态半径滤波，有效实现了多尺度噪声抑制。基于交叉像素的导航方法，通过图像形态学处理和直线簇交叉像素数量，实现了可通行区域的提取与位姿估计。单目视觉导航方法中，基于Mask R-CNN的场景语义信息提取方法，可准确提取不同位姿条件的消失点与道路边界。基于道路几何成像模型的位姿估算方法，利用单目相机完成偏航角估计、横向偏移计算与果树定位。实验结果表明，所提方法具有较高的实用性、稳定性和准确性。\n\n第六章 结论与展望\n\n6.1 主要结论",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 59,
    "chunk_index_in_section": 57,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 330,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "果园行间自主导航关键技术研究_王宇豪_s1_c58",
    "source_id": "果园行间自主导航关键技术研究_王宇豪",
    "text": "第六章 结论与展望\n\n6.1 主要结论\n\n本文研究了基于激光雷达和单目视觉的导航方法。激光雷达导航方法在去噪和可通行区域提取方面表现出较强能力，而单目视觉导航方法在场景语义信息提取和位姿估算方面准确度高。\n\n6.2 研究展望\n\n未来的研究将聚焦于果园地图建立、场景语义信息挖掘和高精度位姿估计方法的融合。这些研究将为自主导航技术和农业装备智能化提供更坚实的理论基础。\n\n[参考文献列表省略]\n\n在学期间的研究成果\n\n一、发表论文\n\n1. 毕松, 王宇豪. 基于自适应半径滤波的激光点云去噪方法[J]. 农业机械学报, 2021.02\n2. 毕松, 王宇豪. 果园机器人行间位姿估计与果树目标定位方法[J]. 农业机械学报, 2021.05\n\n[个人感言与致谢省略]\n\n---\n\n值此临近毕业之际，对所受帮助和支持表达深深的感激！\n\n---",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 60,
    "chunk_index_in_section": 58,
    "total_chunks_in_section": 59,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 369,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9450000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s0_c0",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "---\n\n关键技术研究\n\n Wei Pengjun\n\n电子信息（控制工程）\n\n模式识别与智能控制\n\n导师：刘仁学\n\n校外导师：李睿凡\n\nNorth China University of Technology\n\n2023年5月28日\n\nKey Technology Research on Strawberry Picking Robot in Greenhouse Environment under Elevated Cultivation",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 222,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c0",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "In the past decade, China has become the world's largest producer and consumer of strawberries. The existing manual-based planting and management modes, constrained by rising labor costs, are limiting the development of the strawberry industry. Automation of strawberry picking can significantly reduce labor requirements and enhance the industry's sustainability. This paper focuses on key technolog",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 1,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 81,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 400,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9900000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c1",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "ustainability. This paper focuses on key technologies for the identification and localization of strawberry fruits in a greenhouse environment, including target recognition and segmentation, ripeness and occlusion judgment, and pose estimation for picking points.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 2,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 263,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c2",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "1. Target Recognition and Localization System: A strawberry target recognition and fine segmentation technology based on image target prepositioning, convolutional neural network, and U-net network is proposed for high-accuracy localization and point cloud segmentation.\n\n2. Ripeness and Occlusion Judgment: Using convex hull features and support vector classifiers to assess草莓目标的成熟性和遮挡性.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 3,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 388,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c3",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "3. Pose Estimation and Picking Point Estimation: Utilizing the geometric characteristics of the target point cloud and the strawberry surface point cloud for precise estimation.\n\nKeywords: Strawberry picking, fruit occlusion, point cloud segmentation, pose estimation, target localization",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 4,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 288,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c4",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "Chapter 1 Introduction...............................................................................................1\n1.1 Background and Significance...............................................................................1\n1.2 Research Status of Strawberry Picking Robot Technology at Home and Abroad......................................1",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 5,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 347,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c5",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "1.2.1 Strawberry Target Identification.....................................................................2\n1.2.2 Strawberry Target Localization......................................................................4\n1.3 Main Research Contents and Chapter Structure......................................................5",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 6,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 320,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c6",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "1.3.1 Main Research Content................................................................................5\n1.3.2 Chapter Structure........................................................................................6\n1.4 Summary...........................................................................................................7",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 7,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 341,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c7",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "Chapter 2 Theories Related to Target Identification and Localization..........................................8\n2.1 Target Identification and Segmentation Algorithm Theory..............................................8\n2.2 Algorithm Theories for Strawberry Harvestability Discrimination...................................12",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 8,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 323,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c8",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "2.3 Target Localization and Point Cloud Processing Algorithm Theory...................................16\n2.4 Basic Theories of Picking Action Generation System...................................................22\n2.5 Summary...........................................................................................................28",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 9,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 333,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c9",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "Chapter 3 Overall Scheme Design of Strawberry Automatic Picking System for Elevated Cultivation...29\n3.1 Demand Analysis of Strawberry Automatic Picking System...........................................29",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 10,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 204,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c10",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "3.2 Scheme Design for Strawberry Pose Estimation and Picking Point Localization in Natural Environment........................................................................................................29\n3.3 Summary...........................................................................................................32",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 11,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 329,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c11",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "Chapter 4 Design of Strawberry Image Identification and Regional Segmentation Methods..............33\n4.1 Strawberry Target Pre-localization Module Based on Target Color and Saturation..............33\n4.2 Strawberry Target Identification Module Based on Convolutional Neural Network..............34\n4.3 Strawberry Target Fine Localization Module Based on U-net Network...........................39",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 12,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 397,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c12",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "4.4 Test Analysis of Target Area and Point Cloud Segmentation System..............................45\n4.5 Summary...........................................................................................................46",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 13,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 221,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c13",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "Chapter 5 Design of Strawberry Fruit Spatial Pose and Picking Point Identification Methods........47\n5.1 Strawberry Fruit Spatial Pose Estimation and Picking Point Localization System..............47\n5.2 Software Design for Action Generation System........................................................53\n\n---\n\n第二章目标识别与定位方法相关理论\n\n2.1 目标识别与分割算法理论",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 14,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 346,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c14",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "---\n\n第二章目标识别与定位方法相关理论\n\n2.1 目标识别与分割算法理论\n\n目标识别和目标分割是计算机视觉领域中两个重要的任务，它们可以帮助计算机自动地理解和分析图像或视频中的目标，从而实现智能化的应用。\n\n随着计算机视觉技术的不断发展，目标识别技术也在不断的改进和完善。传统的目标识别算法主要是基于目标有限特征分析，适用于简单的目标识别任务。基于卷积神经网络(CNN)的目标识别模型通过对目标图像数据集的训练学习，获得目标的深层特征，使基于卷积神经网络的识别算法能够在复杂的非结构化环境中更加稳定准确地识别目标。在应对复杂环境情况下的目标识别任务时，相比于传统的目标识别算法，基于CNN的目标识别算法具有更高的鲁棒性。\n\n2.1.1 图像的色彩空间分析",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 15,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 329,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c15",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "2.1.1 图像的色彩空间分析\n\n图像的色彩空间是用于描述图像颜色属性的一组坐标系。常见的色彩空间包括RGB、CMYK、HSV等。目前大多数数字图像均使用RGB色彩空间，由红色、绿色和蓝色三个颜色通道组成。本文所获取到的草莓原始图像颜色空间也为RGB模式，所拍摄的温室环境草莓种植场景图像如图2-1所示。\n\n图2-1 自然环境下草莓的RGB图像\n\n图2-1为自然环境下的草莓图像，图中包含成熟草莓，半成熟草莓和未成熟草莓。由于成熟草莓的生物特点，其颜色大多为红色，成熟草莓目标区域的R通道值较高。但是，自然场景中除了成熟草莓，还有部分枯萎的枝叶、强光照等区域的R通道值也明显较高。因此，在RGB颜色空间中无法找到一个显著的数值区间将成熟草莓成功分割。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 16,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 325,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c16",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "若要解决草莓目标与背景颜色差异不显著的问题，这里引入HSV色彩空间。HSV是一种常用的颜色模型，由A.R.Smith在1978年提出。其中H代表色调(Hue),S代表饱和度(Saturation),V代表明度(Value)。相对于RGB颜色模型，HSV颜色模型更符合人类视觉对颜色的感知，其与RGB颜色模型之间的转化关系为：\n\nH = 60° * ((max(R,G,B) - min(R,G,B)) / (max(R,G,B) - min(R,G,B))) + 120° * ((max(R,G,B) - min(R,G,B)) / (max(R,G,B) - min(R,G,B))) + 240° * ((max(R,G,B) - min(R,G,B)) / (max(R,G,B) - min(R,G,B)))",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 17,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 359,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c17",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "S = (max(R,G,B) - min(R,G,B)) / max(R,G,B)\n\nV = max(R,G,B)\n\n式中：H、S、V为色相、饱和度、明度；R、G、B为红、绿、蓝三通道数值；max(R,G,B)与min(R,G,B)为图像同一像素点RGB三通道中最大值和最小值。\n\n将图2-1转换到HSV颜色空间后，其H、S、V通道灰度图像如图2-2所示。\n\n图2-2 自然环境下草莓图像HSV三通道灰度图\n\n图2-2(a)、2-2(b)、2-2(c)为图2-1转换到HSV颜色空间后H通道、S通道、V通道灰度图。根据图2-2可直观的观察到，相比于图2-2(a)和图2-2(c)成熟草莓于背景区域并无显著差异，图2-2(b)中成熟草莓的饱和度较高，在S通道灰度图像上，成熟草莓区域的灰度值明显高于背景同时轮廓较为清晰。因此，在S通道上做阈值分割处理有利于完成对成熟草莓的预定位处理。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 18,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 392,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c18",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "卷积神经网络(Convolutional Neural Network,CNN)是一种广泛应用于图像处理和计算机视觉等领域的深度学习模型，其可以很好的完成图像分类或物体识别等任务。卷积神经网络主要由输入层、卷积层、激活层、池化层、批归一化层、随机失活层以及全连接层构成。\n\n图像输入层会接收到数据输入，通常是图像或者其他形式的数据。在输入到卷积神经网络之前，通常需要对原始图像数据进行一些预处理操作，例如将像素值进行归一化、对图像进行裁剪或缩放等。在卷积神经网络中，图像数据通常被表示为张量的形式。\n\n卷积层是卷积神经网络的核心。在卷积层中，输入数据和多个卷积核进行卷积运算，以提取输入数据的特征。卷积运算可以看作是一个滑动窗口在输入数据上滑动并取得各个位置上的局部区域与卷积核进行乘积和求和的过程。其运算过程如图2-3所示。\n\n图2-3 卷积运算示意图",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 19,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 377,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c19",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "图2-3 卷积运算示意图\n\n如图2-3所示，输入一个3x3的图像，使用2x2的卷积核，步长为1，通过图2-3(a)、(b)、(c)、(d)对其进行滑动卷积运算，得到原始图像的特征输出。\n\n激活层是对卷积层输出进行非线性变换的层。在卷积神经网络中，每个卷积层后面通常都会紧跟一个激活层。激活层的输入是卷积层的输出，它对每个元素进行非线性的变换，并输出给下一层。常见的激活函数包括ReLU、Sigmoid、tanh等，通过激活函数的非线性变换，可以使神经网络具有更强的表达能力。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 20,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 237,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c20",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "池化层的主要作用是对输入数据进行下采样，从而减少特征图的尺寸，并且可以减少网络的参数量，防止过拟合。池化层将输入特征图划分为不重叠的小块，然后对每个小块进行操作，通常选择最大值(Max Pooling)或平均值(Average Pooling),生成一个新的、尺寸更小的特征图。这个新的特征图通常保留了输入特征图中的主要特征，同时减少了数据的维度和参数量，使得网络更加高效。最大值池化层(Max Pooling)通常能够保留输入特征图中的主要特征，而平均值池化层(Average Pooling)则可以平滑噪声，减少过拟合。\n\n批归一化层可以加速神经网络的训练收敛，提高模型的泛化能力，通过对每个小批量输入数据进行归一化处理，使得网络更易于训练和调整。同时，由于批归一化层可以减少网络对特定批次输入数据的依赖，因此可以提高网络的泛化能力，减少过拟合的风险。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 21,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 378,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c21",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "随机失活层是一种正则化技术，旨在减少过拟合的风险。在训练过程中随机选择一些神经元并将它们的输出设为0，从而使得网络不依赖于任何一个单独的神经元来进行预测，而是依赖于多个神经元的组合，增加了网络的泛化能力。\n\n全连接层通常是网络的最后一层，它将前面的卷积层、池化层、激活层等的输出特征进行展开并转化为一个一维向量，该向量包含了输入图像中的各种特征，如纹理、形状和颜色等，它们将被用于下一步的分类或回归任务。全连接层中的每个神经元都与前一层中的所有神经元相连，因此它们需要学习大量的权重和偏置参数，以便对输入特征进行有效的组合和转换。这些参数的学习通常是通过反向传播算法来实现的，从而优化损失函数并使网络的预测结果更加准确。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 22,
    "chunk_index_in_section": 21,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 310,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c22",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "全卷积神经网络(Fully Convolutional Neural Network,FCN)区别于卷积神经网络的一点是，FCN完全由卷积层和反卷积层组成，没有全连接层。FCN在最后一层使用反卷积层(上采样)代替了全连接层，从而将原始图像的每一个像素都映射到输出结果中，将特征图像信息还原为与原图大小一样的语义分割图像。因此，使用全卷积神经网络更有利于精确的找到自然环境下草莓目标所属的像素。\n\n反卷积的过程可以看作是对卷积操作的逆向操作。在卷积操作中，图像经过卷积核的卷积操作得到下采样的特征图。而在反卷积中，特征图被上采样，再经过反卷积核的反卷积操作，从而得到更高分辨率的图像。图像反卷积的运算过程如图2-4所示。\n\n图2-4 反卷积运算示意图",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 23,
    "chunk_index_in_section": 22,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 324,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c23",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "图2-4 反卷积运算示意图\n\nU型全卷积神经网络(Unet)在全卷积神经网络的基础上引入了跳跃连接，将编码器中的特征图与解码器中的特征图进行连接，从而使网络能够同时利用浅层次和深层次的信息，提高了网络对于物体边缘细节的识别能力。与FCN相比，Unet可以更好的保留特征图中的细节信息。其网络结构如图2-5所示。\n\n图2-5 Unet网络结构示意图",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 24,
    "chunk_index_in_section": 23,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 173,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c24",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "图2-5 Unet网络结构示意图\n\n如图2-5所示，Unet网络结构呈U型对称。左半部分结构为图像的编码器部分，编码器将输入的原始图像通过多个卷积层和池化层进行特征提取和抽象，将图像中的低级特征转化为高级特征，使得模型能够更好地理解图像的语义信息。其中卷积层用于提取特征，而池化层则用于对特征进行下采样，将特征图的尺寸缩小一半，从而使得模型能够更好地处理大尺寸的图像。右半部分结构为图像的解码器部分，解码器将编码器部分生成的特征图通过多个反卷积层和卷积层进行上采样和合并，以生成最终的分割结果。其中反卷积层用于对特征图进行上采样，将特征图的尺寸放大一倍，而卷积层则用于进一步提取和融合特征信息。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 25,
    "chunk_index_in_section": 24,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 297,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c25",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "草莓的质量和产量与草莓的成熟度息息相关。由于果实的种植轮次和收获周期等原因，同一张图象中的所有草莓并非都可以采摘。如果草莓未成熟就被采摘下来，不仅草莓本身质量不佳，而且还会对植株生长和产量产生不良影响。因此，为了准确的识别所有草莓，并获取其中可以采摘的草莓。本文基于草莓目标区域的凸包属性，使用统计阈值和SVM分类器实现了草莓果实的成熟性判别和遮挡类型判别，为后续的点云处理方法提供指引。\n\n图像目标区域凸包是指将目标区域中的所有点连接起来所形成的最小凸多边形。计算目标区域凸包的方法有多种，其中比较常用的是Graham扫描法。\n\nGraham扫描法是一种基于极角排序的凸包计算方法，设目标区域的边缘点集为{(x1,y1),(x2,y2)...(xn,yn)}，其基本实现步骤如下：",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 26,
    "chunk_index_in_section": 25,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 341,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c26",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "1)找到图像中的最低点P0，也就是点集P中y值最小的点，若有多个y值相等且最小，则选择其对应的x值最小的点作为P0。\n\n2)对于P中的其他点Pi，计算Pi与P0的极角并按照极角从小到大的顺序对Pi进行排序。若有多个点极角相同，则按照距离P0的距离从小到大排序。\n\n3)创建一个栈，初始时将P0和排序后的第二个点P1入栈。对于排序后的每个点Pi，判断Pi是否在栈顶前一个元素T2和栈顶元素T1连线形成射线的左边。如果是，则将Pi入栈；否则，弹出栈顶元素T1，直到Pi在T2T1连线的左边。依次进行如上操作直至最后一个Pi点，最终栈内的点即为凸包的点集。\n\n将上述过程动态展示，如图2-5所示。\n\n图2-5 Graham法计算凸包算法动态展示",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 27,
    "chunk_index_in_section": 26,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 319,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c27",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "将上述过程动态展示，如图2-5所示。\n\n图2-5 Graham法计算凸包算法动态展示\n\n图2-5(a)-(e)中的坐标系为以P0为原点建立的xPy坐标系，逆时针旋转分别为按照极角从小到大的顺序对Pi。图2.5(a)中，先将最低点P0和排序后的第二个点P1存入栈中，由于极角最大的点也一定在凸包上，所以将P5与P0的红色连线也一并在图中绘制。图2-5(b)中，点P2在射线P1P0的左边，因此将P2压入栈中，此时P2为栈顶元素。图2-5(c)中，点P3在射线P2P1的右边，说明P2在P1P3的内部，P2不是凸包上的点，因此将P2从栈中弹出。图2-5(d)中，P3入栈。依此类推，P4、P5均符合算法规则，依次入栈。图2-5(f)中，找到目标点集的完整凸包。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 28,
    "chunk_index_in_section": 27,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 328,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c28",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "支持向量机(Support Vector Machine,SVM)是一种常用的机器学习算法，主要用于二分类问题。其基本思想是在特征空间中找到一个能够最大化不同类别之间间隔(Margin)的超平面，从而将不同类别的数据分开。\n\n其公式推导如下所示，给定训练样本集：\n\n{(x1,y1),(x2,y2),...,(xl,yl)}，其中xi∈RN，为N维向量；对于二分类问题yi∈{-1,1}；通过学习训练寻求分类器M(x)，使其不但对于训练样本集满足yi=M(xi)，而且对于预测数据\n\n同样能得到对应输出值yi。线性划分的理想情况是训练样本集可以完全线性分离。当训练样本集不能线性分离时，可以通过引入松弛变量而转化为可线性分离的情况。对于式(2-4)给出的训练样本集的线性二分类问题，就是寻求函数：\n\nf(x) = Sgn(w·x+b)\n\n使对于i=1,2,...,l满足条件：\n\nyi=f(xi)",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 29,
    "chunk_index_in_section": 28,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 398,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c29",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "f(x) = Sgn(w·x+b)\n\n使对于i=1,2,...,l满足条件：\n\nyi=f(xi)\n\n其中w、b∈R为待确定的参数，Sgn为符号函数。显然w·x+b=0为划分超平面，w为其法向量。式(2-7)又可写成等价形式：\n\nyi(w·xi+b)≥1\n\n对于线性可分离的问题，满足上述条件形如式(2-6)的线性决策函数是不唯一的。图2-6(a)给出二维情况下满足条件的划分直线的分布区域图。落在中间区域内的任意直线都可作为决策函数。\n\n图2-6 SVM二分类问题数据示意图\n\nVapnik提出一个最大边际化(maximal-margin)原则，所谓边际又称间隔，是指训练样本集到划分超平面的距离，它是所有训练样本点到划分超平面的(垂直)距离中的最小：\n\nMin(w) = (1/||w||) * Min(1,2,...,l) {xi·w+b}",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 30,
    "chunk_index_in_section": 29,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 372,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c30",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "Min(w) = (1/||w||) * Min(1,2,...,l) {xi·w+b}\n\n所谓最大边际化原则是指寻求使间隔达到最大的划分为最优，即是对w,寻求最优，求得最大间隔：\n\nMax(Min(w)) = (1/||w||) * Max(Min(1,2,...,l) {xi·w+b})\n\n对应最大间隔的划分超平面称为最优划分超平面，简称为最优超平面，如图2-6(b)中的L。图2-6(b)中两条平行虚线1L、2L(称为边界)距离之半就是最大间隔。\n\n可以证明最大间隔是唯一的，但达到最大间隔的最优超平面可能不唯一。\n\n最大间隔和最优超平面只由落在边界上的样本点完全确定，我们称这样的样本点为支持向量，如图2.6(b)中的1x、2x、3x样本点。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 31,
    "chunk_index_in_section": 30,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 326,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c31",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "只由少数训练样本点(支持向量)就把最大间隔和最优超平面完全确定，其余非支持向量的样本点均不起作用，说明间隔最大化原则下的最优划分不是依赖于所有点，而只是由支持向量决定。求最优超平面和最大间隔等同于确定各个样本点是否支持向量，说明该方法具有好的鲁棒性和好的算法复杂性。\n\n由支持向量确定的线性分类机称为线性支持向量机。\n\n对于式(2-4)给定的训练样本集，如果样本是线性可分的，则图2.6(b)中的划分超平面L的方程为：\n\nw·x+b=0\n\n两条边界1L、2L的方程(经过恒等变形后)为：\n\nw·x+b=±1\n\n设x1在1L上，x2在2L上，即：\n\nw·x1+b=1\n\nw·x2+b=-1\n\n两式相减有：\n\nw·(x2-x1)=2\n\n||w||=2/||x2-x1||",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 32,
    "chunk_index_in_section": 31,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 333,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c32",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "两式相减有：\n\nw·(x2-x1)=2\n\n||w||=2/||x2-x1||\n\n式(2-9)左边恰好就是连接x1、x2的向量在划分超平面法方向上的投影，它是最大间隔的二倍。最大间隔等价于求||w||或2||w||或0.5||w||的最小值。考虑到要使所有训练样本点分类正确，应成立：\n\nyi(w·xi+b)≥1\n\n这样，建立线性支持向量机的问题转化为求解二次凸规划问题：\n\nmin(1/2||w||^2+C∑ξi)\n\ns.t. yi(w·xi+b)≥1-ξi\n\nξi≥0\n\n其中C为惩罚系数，ξi为松弛变量。由于目标函数和约束条件都是凸的，根据最优化理论存在全局最小解。应用Lagrange乘子法并考虑满足KKT条件(Karush-Kuhn-Tucker)：\n\nyi(w·xi+b)-1+ξi=0\n\n可求得最优超平面决策函数为：\n\nM(x) = Sgn(∑iαi*yi(xi·x)+b)",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 33,
    "chunk_index_in_section": 32,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 393,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c33",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "可求得最优超平面决策函数为：\n\nM(x) = Sgn(∑iαi*yi(xi·x)+b)\n\n*，iα、b为确定最优划分超平面的参数，(xi·x)为两个向量的点积。\n\n对于线性不可分的情况，通过引入松弛变量ξi≥0，修改目标函数和约束条件，应用完全类似的方法可以求解。与式(2-10)类似的新的凸规划问题为：\n\nmin(1/2||w||^2+C∑ξi)\n\ns.t. yi(w·xi+b)≥1-ξi\n\nξi≥0",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 34,
    "chunk_index_in_section": 33,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 202,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c34",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "min(1/2||w||^2+C∑ξi)\n\ns.t. yi(w·xi+b)≥1-ξi\n\nξi≥0\n\n图像可以提供物体表面的纹理和颜色信息，但是却难以准确描述物体的几何结构和形状。我们可以通过单张图像找到草莓在二维平面内的位置，但是由于深度信息的缺失，我们无法获取其空间位置，从而无法进行草莓目标的位姿估计和采摘点的空间定位。点云数据的补充了图像数据缺少空间三维信息的这一不足，点云数据是离散的三维点，每个点包含其在三维空间中的坐标以及其他属性信息，可以更加准确地描述物体的形状和结构。因此，为了完成草莓目标在三维空间中的位姿估计和分析，引入点云数据是必不可少的环节。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 35,
    "chunk_index_in_section": 34,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 283,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c35",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "深度相机是一种能够获取场景深度信息的相机，其原理通常是利用红外光或激光束来探测场景中物体的深度。深度相机的深度获取原理主要有光飞行时间(TOF)、结构光编码、以及双目立体视觉等。光飞行时间原理的深度相机，属于主动式测距，相机通过发射一束激光束或红外光脉冲，并测量该光脉冲从发射到反射回来的时间来计算物体表面的深度信息。这种方法需要一个内置的激光器和时间测量传感器。该方式获取的深度不受光照变化和物体纹理的影响，最高可达到厘米级精度，一般使用来测量在一百米之内的较远距离；结构光原理的深度相机，属于主动式测距，相机通过主动投射已知编码图案来照射场景，然后计算这些光线在物体表面上的变形来计算物体表面的深度信息。此方法需要一个内置红外光源和一个红外摄像头",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 36,
    "chunk_index_in_section": 35,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 325,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c36",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "。此方法需要一个内置红外光源和一个红外摄像头。该方式获取的深度同样也不受光照变化和物体纹理的影响，近距离可达到0.01mm-1mm的精度，测量距离一般在10m以内；双目立体视觉的深度相机，属于被动式测距，深度相机使用两个摄像头同时拍摄场景，然后利用特征点匹配算法计算场景中物体的深度信息。该方法需要两个内置摄像头。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 37,
    "chunk_index_in_section": 36,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 157,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c37",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "清洗后的内容如下：\n\n双目立体视觉深度相机受光照变化和物体纹理变化的影响很大，夜晚无法使用，但在近距离情况下可达到毫米级精度。由于基线限制，一般只能测量较近的距离，范围大多在两米内。本文方法面对的环境为高架栽培草莓种植温室大棚，光照变化范围较大，不适于使用双目立体视觉深度相机。温室内草莓种植高架的行间距在两米左右，需要传感器识别深度的范围也在0.5-2米之间。在此范围内，主动式测距的结构光原理深度相机能达到1mm的高精度，符合采摘应用需求。因此，本文选用使用主动编码原理的深度相机RealSense D415。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 38,
    "chunk_index_in_section": 37,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 257,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c38",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "RealSense D415可以在获取到RGB图像的同时得到每个像素点的深度信息。其深度感知水平视场角为69.4°、垂直视场角为42.5°、对角线视场角为77°，彩色相机视场角与深度相机视场角一致，分辨率为1920 x 1080，可选自动或手动白平衡和曝光，最大帧率为30fps。\n\n深度数据转换为点云数据本质上是图像坐标系到世界坐标系的变换，变换的约束条件是相机内参。深度相机采集深度信息时，会受到噪声干扰，导致获得的深度信息不准确，进而使点云数据产生误差。点云双边滤波是一种常用的点云滤波方法，利用点云中每个点的领域信息来对该点进行滤波，不仅考虑了点与点之间的距离关系，还考虑了它们在特征空间中的相似度。\n\n点云的法向特征是描述点云表面形状和结构的重要特征之一。计算点云法向量有几种算法，如主成分分析法、平面拟合法等，本文方法对平面拟合法进行展开和推导。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 39,
    "chunk_index_in_section": 38,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 379,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c39",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "空间射线交点求解是计算机图形学中常用的技术，用于确定一条射线与三维空间中的物体的交点。其原理基于空间向量的运算，可以通过求解一元线性方程组来计算交点。\n\n获得到草莓目标在相机坐标系下的采摘坐标点位置后，需要将其转换到机器人坐标系下，才能引导机器人末端运动到采摘位置，完成采摘动作。这其中会涉及到相机内参标定和机器人手眼标定等步骤。由于同一幅图像中可能会同时采集到多个成熟草莓目标，所以对多个采摘点的排序和路径规划问题，也是值得关注的。\n\n相机内参标定是指确定相机内部参数的过程，其基本步骤如下：准备标定板，拍摄标定板，提取角点，计算内部参数。\n\n采摘机器人手眼标定需要建立起采摘点在相机传感器下坐标与在草莓采摘机器人基坐标系下的映射关系，获得相机与采摘机器人末端坐标系的相对位置关系，从而实现采摘点从相机坐标系到机器人坐标系的转换。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 40,
    "chunk_index_in_section": 39,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 366,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c40",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "多目标连续采摘的旅行商路径规划问题可以看成是三维空间中的旅行商路径规划问题。旅行商问题的求解主要有精确算法包括分枝定界法、线性规划法、动态规划法等，但是随着问题规模的增大以及城市数量的增加，精确算法便显得无能为力。因此便出现了使用近似算法或启发式算法来解决这一问题的思路，遗传算法将问题的求解过程转换成类似生物进化过程中的染色体基因的交叉、变异等过程。\n\n清洗后的内容如下：\n\n---\n\n个体适应度定义为总距离的倒数。\n\n选择: 将种群中所有个体的适应度值进行累加然后归一化，最终通过随机数对随机数落在区域所对应的个体进行选取。\n\n交叉: 把两个父代个体的部分结构替换重组而形成新的个体。\n\n变异: 对个体上的某些基因值作变动，使能随机搜索到解可能存在的整个空间，一定程度上求得全局最优解。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 41,
    "chunk_index_in_section": 40,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 345,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c41",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "假设一共有N个采摘点，用P[N,3]表示所有采摘点的空间坐标，D[i,j]来记录每两个采摘点之间的空间距离，迭代优化的采摘点路径被记录在Route[gen]中，M代表种群数量，T代表遗传运算的终止进化代数。则算法伪代码如下：\n\n---\n\n草莓遮挡识别方法\n\nInput: 采摘点空间坐标数组P[N,3],果实数量N Output：Route[i]机器人采摘顺序列表\n\nfunction Route = TSP_GA(Points,N) 初始化种群规模; 初始化基因交叉概率; 初始化基因变异概率; 随机初始化数量大小为M的种群数组pop; while gen<T\n\nfor i=1:M\n\n依次计算种群中个体的评价函数存入fitness数组中; end 将fitness数组累加并归一化,基于概率选择出优秀父代; 选择优秀父代作为后续交叉变异的基础，更新种群; for i=1:M",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 42,
    "chunk_index_in_section": 41,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 389,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c42",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "if rand < 基因交叉概率\n\n执行交叉操作，将相邻优秀父代的路径点中间片段截取交换; end end 更新种群; for i=1:M if rand < 基因变异概率\n\n执行变异操作，随机对种群中优秀父代的内部路径点 end end\n\n保留路径长度最短最好的一代; Route[gen] = 1/min(fitness); gen = gen+1; end\n\nTSP_GA函数输入量为采摘点空间数组，输出为排序后采摘点序列。如上代码所示，首先随机初始化种群，计算出种群中每个个体的适应度函数，根据适应度函数选择优秀个体，并进行交叉变异操作来更新种群，直至迭代进化完成，获取最优路径序列。\n\n---\n\n第二章目标识别与定位方法相关理论\n\n2.5 本章总结",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 43,
    "chunk_index_in_section": 42,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 329,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c43",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "---\n\n第二章目标识别与定位方法相关理论\n\n2.5 本章总结\n\n本章首先介绍了目标识别与分割相关的算法理论基础，介绍了图像的色彩空间分析、卷积神经网络和全卷积神经网络主要层级结构以及二者之间的区别和各自的实用场景。其次，介绍了本文方法中需要用到的对草莓目标进行判别分析需要用到的基础理论方法，包括需要用到的图像凸包特征计算方法和支撑向量机分类原理。接着本章介绍了目标定位和点云处理算法相关的内容，包括深度相机的深度信息获取原理和选型依据、深度数据与点云数据的转换、点云的滤波方法、点云的法相特征求解原理以及空间射线交点的求解。最后，本章介绍了采摘动作生成系统的基础理论，包括相机的内参标定原理和机器人手眼标定原理以及多目标连续采摘会涉及到的旅行商路径规划问题。\n\n---\n\n第三章高架栽培草莓自动采摘系统总体方案设计\n\n3.1 草莓自动采摘系统需求分析",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 44,
    "chunk_index_in_section": 43,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 377,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c44",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "---\n\n第三章高架栽培草莓自动采摘系统总体方案设计\n\n3.1 草莓自动采摘系统需求分析\n\n草莓自动采摘系统的架构基于场景需求可以分为两个关键部分，即感知系统和执行系统。感知系统负责获取草莓果实的位置信息，而执行系统则根据感知获得的信息实现自动采摘精准动作。在采摘过程中，系统首先通过感知系统观测到果实的位置，然后将手臂移动到果实所在的位置，并使用剪刀等工具将果实剪下。这个过程类似于人工采摘，但自动采摘系统使用机器视觉技术来代替人类的视觉感知，从而实现高效、精准的采摘。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 45,
    "chunk_index_in_section": 44,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 236,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c45",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "感知系统在获取到目标果实的图像后，将数据转化为执行系统可以接收理解的采摘点信息并反馈给执行系统。执行系统采用逆运动学方法，根据机器人的关节结构和机械特性，计算出每个关节电机所需的位置信息后，机器人就能根据关节位置信息准确地移动手臂和工具，将果实剪下。通过精确的控制和运动规划，执行系统能够高效地实现采摘目标果实的动作。通过循环执行上述工作流程，实现对草莓目标的自动采摘。\n\n3.2 面向自然环境的草莓位姿估计与采摘点定位方案设计",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 46,
    "chunk_index_in_section": 45,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 214,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c46",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "3.2 面向自然环境的草莓位姿估计与采摘点定位方案设计\n\n在采摘场景中，光照条件、不同植株的种植需求以及温室大棚在不同时节的影响导致色温、光照强度和果实生长程度存在较大差异。传统图像方法在颜色分割和简单场景下的目标识别上具有优势，但难以处理如此复杂的场景，并且很难通过一套固定参数适应不确定的影响因素。相比之下，深度神经网络具有强大的特征提取能力、环境适应性和鲁棒性，能够准确稳定地识别复杂场景下的语义信息。因此，本文采用传统图像方法与卷积神经网络相结合的方式来解决这一问题。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 47,
    "chunk_index_in_section": 46,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 238,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c47",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "首先，传统图像方法用于目标识别和初步草莓掩码提取。该方法能够利用颜色分割等技术，在一定程度上处理光照和色温等因素的影响，识别出草莓目标并提取初步的掩码信息。然后，引入卷积神经网络来进一步提取草莓目标的特征和生成精确的掩码。卷积神经网络能够学习复杂的图像特征表示，并具有对环境变化的鲁棒性。通过训练网络模型，系统能够准确地识别草莓目标并生成高质量的掩码，进而基于掩码获得草莓目标曲面的点云数据。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 48,
    "chunk_index_in_section": 47,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 196,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c48",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "在采摘过程中，由于果实的种植轮次和收获周期等因素的影响，同时识别到的所有草莓并非都适合采摘。为了准确地识别所有草莓，并确定其中可以采摘的草莓，本文提出了基于草莓目标区域边缘和凸包形态以及支持向量机的方法。该方法能够实现草莓果实的可否采摘判别以及遮挡类型判别，并为后续的点云处理工作提供支持。首先，利用机器视觉技术提取草莓目标区域的边缘信息。通过对图像进行处理和分析，可以获得草莓目标的轮廓形状。接下来，通过计算凸包，系统能够进一步理解草莓目标的几何特征和形态信息。同时考虑边缘和凸包形态的方法，能够提供更全面的草莓特征描述。然后，本文采用支持向量机(SVM)算法进行可否采摘判别和遮挡类型判别。通过训练样本的分类特征，建立一个能够判别完整草莓和遮挡草莓的SVM模型",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 49,
    "chunk_index_in_section": 48,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 332,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c49",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "。通过训练样本的分类特征，建立一个能够判别完整草莓和遮挡草莓的SVM模型。该模型能够利用草莓目标的特征向量进行预测，并确定草莓是否满足采摘条件和草莓是否被遮挡，此部分可使系统可以进一步优化草莓的识别和采摘策略，提高整体采摘效率。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 50,
    "chunk_index_in_section": 49,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 114,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c50",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "为了让草莓识别不再仅限于研究且真正的能为采摘机器人所用，本文提出了在温室大棚条件下，面向高架栽培草莓的位姿估计和采摘点计算方法。草莓可以近似为一个倒圆锥形的曲面，而在同一个水平面上，草莓点的分布可近似为一个圆形，且圆上的每一条弦的垂线都交于圆心点。本文利用圆锥体的几何性质，将空间果轴比作圆锥体中心的轴线，建立草莓-位姿几何模型。通过计算草莓目标点云数据的法向量，计算法向交点来定量分析草莓曲面、位姿二者之间的关系，推导草莓位姿的计算和采摘点的提取方法。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 51,
    "chunk_index_in_section": 50,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 227,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c51",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "为了提高采摘机器人的效率，本文研究了一种基于遗传算法的采摘序列路径最短规划方法，用于识别当前视野下的所有可采摘草莓的位姿和采摘点，并规划最优的采摘顺序。首先，采摘机器人通过感知系统获取当前视野下的所有可采摘草莓的位姿和采摘点。然而，如果将这些采摘点无序地发送给采摘机构并执行采摘动作，可能会导致采摘动作的时间和流畅程度受到影响。本文采用遗传算法来进行采摘序列路径的最短规划。将采摘机器人的采摘路径距离作为优化目标，将可采摘草莓的位姿和采摘点作为候选解的基因编码。通过迭代优化的方式不断寻找最短路径，使其能够按照最优顺序采摘草莓，提高采摘效率。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 52,
    "chunk_index_in_section": 51,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 271,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c52",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "为了让规划生成的采摘序列能够为采摘机器人所用，需要将估计出的草莓位姿和采摘点从相机坐标系转化到机器人坐标系下。为此，本文研究了使用张正友棋盘格标定法进行手眼标定的方法，完成了相机的内参标定，并计算了手眼矩阵。首先，为了实现相机的内参标定，本文采用了张正友棋盘格标定法。通过在场景中放置具有已知尺寸的棋盘格，利用相机拍摄棋盘格的多个图像，可以计算出相机的内参矩阵，包括相机的焦距、主点位置等参数。接下来，利用内参标定得到的相机参数，本文完成了手眼矩阵的计算。手眼矩阵描述了相机坐标系和机器人坐标系之间的转换关系。通过采集相机在不同位置和姿态下的图像，并记录机器人的运动轨迹，可以利用手眼标定方法计算出手眼矩阵，从而将相机坐标系和机器人坐标系进行关联。\n\n综上所述，本文方法主要由四部分组成：自然图像草莓区域分割、草莓目标可采摘性识别、草莓位姿估计与采摘点定位、采摘动作生成，方法结构如图3.1所示。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 53,
    "chunk_index_in_section": 52,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 398,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c53",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "由图3-1可知，本文方法的流程如下：首先，在自然图像草莓区域分割模块中，以优化的对草莓颜色适应性较高阈值对图像进行阈值分隔获得草莓图像区域预定位；基于预定位结果利用训练好的图像目标识别网络对图像中草莓区域进行快速定位；利用训练好的图像分割网络提取精确的草莓目标区域并提取图像目标区域形状特征。其次，在草莓目标可采摘性识别模块中，使用提取出的图像区域特征对草莓的可采摘性进行识别。然后，在草莓位姿估计模块中，基于草莓区域的点云数据和草莓的可采摘性结果估计草莓位姿和采摘点空间位置坐标；最后，在采摘动作生成模块中，基于计算得到的采摘点序列计算最短采摘路径并控制机器人完成采摘作业。\n\n3.3 本章总结",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 54,
    "chunk_index_in_section": 53,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 298,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c54",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "3.3 本章总结\n\n本章将自动采摘系统的工作过程和人工采摘过程进行类比，将其需求分为了感知模块与执行模块并简单介绍了其所要承担的功能。本文通过结合传统图像方法和卷积神经网络，解决采摘场景中光照条件和复杂环境对草莓识别的挑战。传统图像方法能够处理一定程度上的光照和色温差异，提取初步的草莓目标掩码，而卷积神经网络则进一步提取草莓目标的特征和生成精确的掩码。这种组合方法能够有效地识别复杂场景下的草莓目标，并获取其曲面的点云数据。在采摘过程中，针对果实的种植需求和不同生长程度，本文提出了基于边缘和凸包形态以及支持向量机的可采摘性判别方法，通过机器视觉技术提取草莓目标的边缘信息和凸包形态，结合支持向量机进行分类判别，进一步优化草莓的识别和采摘策略。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 55,
    "chunk_index_in_section": 54,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 323,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c55",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "此外，针对温室大棚环境下的高架栽培草莓，提出了基于几何模型的位姿估计和采摘点计算方法，利用圆锥体的几何性质建立草莓-位姿模型，并通过计算法向交点实现位姿的计算和采摘点的提取。最后，为提高采摘机器人的效率，本文利用遗传算法进行采摘序列路径最短规划，将可采摘草莓的位姿和采摘点作为候选解进行优化，实现按照最优顺序采摘草莓。此外，通过手眼标定方法，将估计的草莓位姿和采摘点从相机坐标系转化到机器人坐标系，为采摘机器人的实际应用提供支持。综上所述，本文的方案在采摘场景中具有可行性，通过综合应用多种技术手段，能够实现高效、精确的草莓识别和自动采摘，为草莓种植和采摘系统的应用提供了有效的解决方案。\n\n---\n\n第四章草莓图像识别与区域分割方法设计\n\n本文设计了自然环境下草莓目标区域点云的精确分割方法，精确分割的草莓目标点云可为后续的草莓位姿估计提供数据基础，其方法流程如图4-1所示。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 56,
    "chunk_index_in_section": 55,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 389,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c56",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "首先，在HSV空间对草莓目标进行预定位确定图像中草莓目标潜在区域；其次，利用图像目标识别模型在预定位结果基础上检测草莓位置区域；再次，利用语义分割网络在草莓位置区域图像基础上对草莓区域进行像素分割；使用像素分割后的草莓目标区域图像提取特征进行草莓的成熟性判断；最后，使用成熟草莓的草莓目标区域图像在深度数据中匹配获得草莓目标点云。\n\n4.1 基于目标颜色和饱和度的草莓目标预定位模块",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 57,
    "chunk_index_in_section": 56,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 191,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c57",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "在传统计算机视觉算法中，特征选择是其核心部分，包括纹理特征、形状特征以及颜色特征等等。高架栽培草莓种植温室大棚场景中，草莓的颜色与背景之间存在明显的差异，因此，为了实现准确的目标检测，需要基于合适的颜色空间来进行草莓目标与背景的分离。不同的颜色空间具有不同的通道，如RGB、HSV、YUV等，通过不同颜色和系数的组合，适用于各种不同的应用场景。正如在2.1.1中所分析，成熟草莓目标的红色，在HSV色彩空间中色域较宽、范围较大，草莓前景在S通道下相对于背景也较为显著，较容易提取。又因为温室环境下，光照变化等影响因素较为复杂，无法设定统一参数来适应环境的变化。因此在HSV色彩空间中使用较大的范围的S通道值和红色的颜色提取可较好的完成草莓目标的预定位。利用区域形态学操作完成孔洞填充，去除图像噪声后，获得草莓图像预定位\n。利用区域形态学操作完成孔洞填充，去除图像噪声后，获得草莓图像预定位。预定位过程主要由色彩空间转换、颜色提取、饱和度阈值、区域形态学操作、二值图像求交集等过程组成，方法流程如图4-2所示。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 58,
    "chunk_index_in_section": 57,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 454,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c58",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "其中，在HSV颜色空间中提取的颜色范围为：\n\n[0,10] [156,180]\n\n[43,255]\n\n[46,255]\n\n(4 1) −\n\nHSV空间中提取红色后的二值化图像中，满足式(4-1)条件的像素值为255(白色)，不满足式(4-1)条件的像素值为0(黑色)；S通道分割阈值为140，S通道图中高于此阈值的像素值变为255，低于此阈值的像素值置0。\n\n4.2 基于卷积神经网络的草莓目标识别模块",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 59,
    "chunk_index_in_section": 58,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 201,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c59",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "4.2 基于卷积神经网络的草莓目标识别模块\n\n基于卷积神经网络的模型结构具有深层次、特征表达能力强等特点，能够根据标签自主学习当前任务中所需要的多维特征，比传统方法具有更强的鲁棒性和更大范围的实用性。因此，完成草莓目标的预定位处理后，需要对草莓目标进行目标定位和检测。首先，计算每个预定位目标区域的质心，以区域质心作为中心基于Yolo网络构造识别模块从原始图像中搜索草莓目标。对于预定位过程中正确找到的草莓目标区域，网络会输出草莓目标的预测位置以及置信度；对于预定位过程中误识别的区域，网络无输出，同时也代表着此区域无草莓，不再参与后续流程的计算。\n\n4.2.1 草莓目标识别模块",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 60,
    "chunk_index_in_section": 59,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 290,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c60",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "4.2.1 草莓目标识别模块\n\nYOLO算法采用回归的方式进行目标检测，将输入图片划分为7x7的网格，根据每个网格中心点预测周围的目标边框，从而得到目标预测区域的位置信息并预测目标的类别概率。然后将预测区域和类别概率进行对比，得到目标的有效区域。但是，每个网格最多只能预测一个物体，这限制了模型对多目标的识别能力。此外，由于特征网格为7x7，模型对于较小的物体识别能力有限。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 61,
    "chunk_index_in_section": 60,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 187,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c61",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "YOLOv2是对YOLO算法的优化，其引入了批标准化层提高了网络的收敛性。此外，将输入图片划分为13x13的网格，提高了模型对于较小的物体的支持。YOLOv2还采用了Faster-RCNN中的Anchor boxes，用于预测候选框使得其识别效率高于Faster-RCNN。但是，YOLOv2网络设计中较大的步幅和较少的卷积层还是限制了其对于小目标的检测性能，采用Anchor Boxes的方式虽然提高了检测的精度，但是对于目标形状比较奇异的情况还是会有误检情况。此外，YOLOv2对于长宽比较大的物体的检测也存在一定的问题，容易出现尺度失配的情况。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 62,
    "chunk_index_in_section": 61,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 275,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c62",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "相对于YOLOv2，YOLOv3在多个方面进行了改进和优化。首先，YOLOv3采用了多尺度检测的策略，可以检测大小不同、宽高比不同的目标。其次，YOLOv3使用残差连接(Residual connections)和特征金字塔(Feature Pyramid)结构，可以在保证高精度的同时大大提高检测速度。此外，YOLOv3使用多尺度训练策略，可以使得模型对于不同大小的目标具有更好的鲁棒性和泛化能力。\n\n因此本文设计了基于YOLOv3的草莓果实识别模块，其流程如图4-3所示：",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 63,
    "chunk_index_in_section": 62,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 238,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c63",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "因此本文设计了基于YOLOv3的草莓果实识别模块，其流程如图4-3所示：\n\n目标预定位模块输出草莓目标的潜在区域后，以预定位草莓区域的质心为中心并以416x416的尺寸截取原图像，将截取后的图像放入识别网络的输入端。特征提取模块使用Darknet-53来提取图像特征，共52个3x3卷积层和一个1x1卷积层和5个2x2的最大池化层。Yolov3使用Anchor Boxes来检测物体，Anchor Boxes是一组预先定义好的框，可以覆盖各种不同大小和形状的物体。对于每个Anchor Box，YOLOv3会预测其包含物体的置信度和物体的类别，并调整预测框的位置和大小，使其更好地匹配实际目标，在检测到多个Anchor Box都包含同一物体时，网络使用非极大值抑制算法来剔除重复的检测结果，只保留置信度最高的一个结果。\n\n4.2.2 识别网络数据集构建",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 64,
    "chunk_index_in_section": 63,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 376,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c64",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "4.2.2 识别网络数据集构建\n\n本文主要针对温室高价栽培种植环境下的草莓果实进行识别，为了保证能够涵盖自然环境下普遍存在的天然干扰信息，选择了不同时间段和不同天气进行数据采集。数据采集地点为北京市昌平区小汤山镇万德草莓庄园，拍摄时间为2022.2.24晴天中午11：00-12：10、2022.2.24晴天下午14：50-15：30和2022.3.6阴天下午16：00-16：45。所使用的数据采集设备型号为Intel的RealSense D415深度相机，相机的图像分辨率和深度信息分辨率都为720x1280像素。\n\n对所有采集图像进行人工挑选，选出包含不同强度光照、不同遮挡情况的图像共2000张，按照8：2的比例划分训练集和测试集，并将1600张训练集图像进行人工标记。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 65,
    "chunk_index_in_section": 64,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 339,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c65",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "在对草莓目标进行数据标注时，尽量保证标注框与草莓实际大小一致，本文采用Matlab 2021b环境下的Image Labeler软件对训练集图像进行标注，形式如图4-4所示。\n\n4.2.3 识别网络模型训练分析\n\n使用构建好的草莓数据集对目标识别网络进行训练，将预测框与标记框进行比较得出模型的损失函数计算方法为：\n\n---\n\n以上为清洗后的内容。\n\n清洗后的内容如下：\n\n在网络模型训练过程中，使用惯性随机梯度下降法(SGDM)对损失函数进行优化。SGDM在随机梯度下降(SGD)的基础上加入了动量(momentum)的概念。动量可以理解为在梯度下降时加入一定的惯性，使得参数更新不会因为梯度变化太快而波动较大，从而使得收敛更快、更稳定。SGDM的更新公式如下：\n\n1 1 ( ) t t t v v J θ α μ θ − − ∇ = + (4 3) −",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 66,
    "chunk_index_in_section": 65,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 379,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c66",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "1 1 ( ) t t t v v J θ α μ θ − − ∇ = + (4 3) −\n\n1 t t tv θ θ − = − (4 4) −\n\n其中，tv表示第t次迭代时的动量，μ表示动量因子，α表示学习率，1 ( ) t J θ θ − ∇表示损失函数对参数θ的梯度。\n\n草莓目标识别网络模型的训练参数如下表所示：\n\n表4-1 识别网络模型训练参数\n\n训练参数 数值 初始学习率 0.001 训练次数 600 小批次样本数量 16 学习动量 0.9\n\n如表4-1所示，草莓识别网络模型共训练600次，每次训练16张图像，学习率设置为0.001，学习动量设置为0.9，模型训练过程中的损失率与迭代次数关系如图4.5所示：\n\n图4-5 草莓目标识别模型训练损失曲线\n\n根据图4-5可知，在训练过程中，损失率随着迭代次数的增加而下降，到迭代500次后平均损失趋于稳定不再减小。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 67,
    "chunk_index_in_section": 66,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 389,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c67",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "模型的部分识别结果如图4-6所示：\n\n图4-6 草莓目标识别模块部分识别结果\n\n图4-6中的草莓目标存在光照强、光斑反射、背光和枝叶遮挡等复杂情况。图4-6(a)中三个草莓虽然均存在光斑反射和亮点的情况，但都成功被网络识别，且框选位置与草莓实际大小基本一致；图4-6(b)中的草莓目标光线环境较差，背景背光且十分昏暗，模型仍找到了草莓的目标位置，右上角两个草莓簇拥在一起，导致模型对单个草莓的识别框位置精确度稍差；图4-6(c)中，环境光照程度良好，但草莓果实存在较严重遮挡，也被模型有效识别，且准确率较高。\n\n根据对以上几种较极端情况识别结果的分析，可以看出本节设计的草莓目标识别网络模型对于天气情况变化频繁且存在较多情况干扰的复杂自然环境下，具有较好的准确性与实用性，能够有效完成草莓目标的识别任务。\n\n4.3 基于U-net网络的草莓目标精细定位模块",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 68,
    "chunk_index_in_section": 67,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 378,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c68",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "4.3 基于U-net网络的草莓目标精细定位模块\n\n语义分割技术可以对图像中的目标像素区域进行分割，获得更加精准的草莓有效像素区进而通过三维点云有效分析采摘点信息，卷积神经网络输出草莓目标位置之后，根据草莓识别框的参数对原图像进行剪裁，这样能够去除背景区域对语义分割带来的干扰。由于果实相连、叶片遮挡等原因，识别网络预测的草莓边界框可能会存在未完全包含整个草莓的情况，所以会宽高等比例放大边界框，以保证整个草莓都在图像中。根据放大后边界框的宽高，使用黑色将图像填补为正方形，最后将图像调整至224x224分辨率，输入到草莓语义分割网络中，以获取草莓目标图像。\n\n4.3.1 识别结果图像分割前处理",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 69,
    "chunk_index_in_section": 68,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 298,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c69",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "4.3.1 识别结果图像分割前处理\n\n在进行草莓果实区域的像素分割之前，需要先对原始图像进行分割预处理。为此，设计了图像分割前处理模块，该模块通过草莓识别网络模块输出的果实预测框在图像中的位置、大小和置信度信息，对原始图像进行分割前处理以便进行进一步的像素分割。草莓图像分割前处理模块包括图像剪裁、图像拼接和图像缩放过程，具体流程如图4-7所示：\n\n图4-7 识别结果图像分割前处理流程\n\n如图4-7所示，在草莓识别网络输出结果后，根据草莓识别框的参数对识别框放大后，裁剪图像，以去背景区域的干扰。然而，草莓目标识别网络有可能受到枝叶遮挡、果实遮挡等因素的影响，导致识别框形状各异，无法输入精细分割网络模型。因此，为解决这一问题，通过图像填补算法，根据识别框的宽度和高度差值，使用黑色对其短边方向进行补全。最终，将图像调整至分辨率为224x224，并传入草莓果实分割网络进行下一步操作。\n4.3.2 草莓精细分割模块\n\n基于Unet的草莓果实精确分割网络结构，如图4-8所示：\n\n图4-8 识别结果图像分割前处理流程",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 70,
    "chunk_index_in_section": 69,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 458,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c70",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "图4-8 识别结果图像分割前处理流程\n\n经过分割预处理模块处理后，得到了224x224像素的草莓识别框图片，该图片将被输入到U-net草莓分割网络模型中。该模型使用3x3的卷积层和2x2的池化层进行了4次下采样，每次下采样前后的图片大小相同。这四次下采样将草莓图像特征图缩小了16倍。接下来，模型使用3x3的卷积层和2x2的反卷积层对高维特征图进行上采样计算。反卷积层使用了补零的步长为2的卷积核构造的2x2反卷积结构，以确保上下采样前后的对称性。在四次下采样前的卷积层输出都被拼接融合到了对称的上采样反卷积后的卷积输入位置。最后，将224x224x32的上一层卷积层的输出通过一个1x1的卷积层输出为一张特征图，每个像素点对应草莓类的得分，越接近1表示该像素点为草莓，接近0表示为背景。将该特征图进行Softmax二值化，得到草莓目标区域分割特征图。\n\n4.3.3 分割网络数据集构建",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 71,
    "chunk_index_in_section": 70,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 393,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c71",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "4.3.3 分割网络数据集构建\n\n草莓分割网络数据集来自4.3.1中获得的草莓分割前处理结果图像，为了使数据集具有更好的抗干扰性，标注时包含了光照亮斑、遮挡阴影、枝条枝叶遮挡等果实像素点。数据集总共有5000张图像，标注形式如图4-9(a)所示，使用ImageLabeler标注工具通过人工标记的方式对草莓图像进行标注，标注文件格式为.mat，其描述了草莓图像中的像素标注区域位置信息，将其可视化后，如图4.9(b)所示：\n\n图4-9 分割网络数据集标注示意\n\n4.3.4 分割网络模型训练分析\n\n在草莓目标语义分割网络的训练中，损失函数是通过比较网络输出的分割特征图和标注图像来计算的。该损失用来度量实际值与预测值之间的概率距离，具体的计算方法如下：\n\n[ log( ) (1 )log(1 )]\n\nx loss y p y p\n\n= = + − − ∑ (4 5) −",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 72,
    "chunk_index_in_section": 71,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 386,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c72",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "x loss y p y p\n\n= = + − − ∑ (4 5) −\n\n其中n为BatchSize，即一次循环训练用到的图片数量。p为网络预测值，y为输出期望值。损失函数通过AdaGrad的改进方法Adadelta方法进行优化，其在底t次的参数tθ如下式：\n\n1 [ ] [ ]\n\nt t t t\n\nRMS g RMS g\n\nθ θ − ∆ = ∆ − (4 6) −\n\n1 t t t θ θ θ + = + ∆ (4 7) −\n\n其中tg为梯度参数，其均方根(RMS)为：\n\n2 [ ] [ ] t t RMS E θ θ ε ∆ = ∆ + (4 8) −\n\nAdadelta优化方法的优势在于不需要手动设置学习速率，从而减少了调整学习速率的训练成本，并且相较于AdaGrad鲁棒性更强。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 73,
    "chunk_index_in_section": 72,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 348,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c73",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "在草莓分割任务中，将数据按照9:1的比例划分为训练集和验证集，经过对草莓分割网络的100个批次的训练，得到了损失曲线如图4-10所示。\n\n图4-10 草莓分割模型训练损失曲线\n\n根据图4-10可知，30轮训练后，网络的泛化能力较强，30轮后面损失率缓慢下降并逐渐稳定。\n\n测试集图像标注和分割预测结果对比如图4-11所示，根据图4-11(a)、(b)、(c)所示，模型成功分割出了图像标注的除果梗外的草莓像素区域，且未收草莓表面光斑的影响。根据图4-11(d)、(e)、(f)可知，模型对遮挡草莓也能给出合理的分割结果。\n\n图4-11 测试集标注图像与分割预测结果图对比\n\n从上述对比结果可知，在光照不均，存在阴影、光强暗淡以及枝叶遮挡情况下，草莓目标分割网络模块均能成功预测草莓目标区域像素位置，具有较好的鲁棒性。\n\n4.3.5 草莓目标的成熟性分析",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 74,
    "chunk_index_in_section": 73,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 376,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c74",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "4.3.5 草莓目标的成熟性分析\n\n获得草莓目标的区域分割图像后，应对草莓的成熟性做出分析，以完成后续对成熟草莓目标点云的分割。成熟草莓与未成熟草莓的区域目标图像，如图4.12所示：\n\n图4-12 成熟草莓与未成熟草莓展示\n\n草莓的成熟与否在成熟区域度特征上有明显差异。如图4-12所示，成熟草莓图像的凸包面积，在尺度一致的区域目标图像中，占比较大且较为集中；未成熟草莓图像的凸包面积，在区域目标图像中，占比较小。因此，设1S为草莓目标区域凸包像素面积,2S为目标图像像素面积，以1/2/F为草莓的成熟区域度特征。通过对草莓样本图像的分析，获取其目标区域并计算区域度特征，将特征数据1F生成了箱型图如图4-13所示：\n\n图4-13 草莓区域度特征分布箱型图",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 75,
    "chunk_index_in_section": 74,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 328,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c75",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "图4-13 草莓区域度特征分布箱型图\n\n图4-13中，纵轴为区域度特征1F的数值，从图中可以观察到未成熟草莓的区域度特征分布集中度较高，均分布在0.08左右。而成熟草莓的区域度较为分散，从最小值0.18到最大值0.72均有分布，相对集中在0.35-0.5区间。因草莓样本的多样性及其品质的随机性，所得数据中有个别离群点(红色十字)，但图中数据分布仍能反应出可通过区域度特征阈值方法成功区分成熟草莓与未成熟草莓。当把阈值设定在0.2时，对成熟草莓分类的准确率为96.03%，对未成熟草莓分类的准确率为91.67%，满足实际场景的应用需求。\n\n4.3.6 草莓目标点云的精细分割及去噪\n\n成熟草莓中，以草莓目标精确分割区域像素的坐标为索引，结合配准的深度信息矩阵，结合2.3.2中的式(2-14)计算草莓区域的点云数据，获得精确分割的草莓目标区域点云数据。如图4.14所示：",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 76,
    "chunk_index_in_section": 75,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 385,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c76",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "图4-14 草莓目标点云数据获取\n\n此时得到的点云曲面数据，是不够平滑的，不足以拟合草莓曲面的平滑程度。因此，为了去除点云数据在获取阶段受到环境干扰导致的误差，需要对点云数据进行预处理。本文使用双边滤波方法对草莓目标点云区域进行滤波去噪，双边滤波算法通过使用当前数据点与临近点的距离权重和临近点沿当前点法向量的投影权重来修正当前采样中心点的位置，从而达到保持点云数据原特征的目的。根据2.3.3的分析和推导，使用此方法平滑后的数据对比如图4-15所示：\n\n图4-15 草莓曲面点云数据经双边滤波后效果对比图\n\n根据图4-15(a)和图4-15(a1)可知，由于传感器精度和环境光学噪声干扰等原因，草莓目标原始点云呈现阶梯状，且点云边缘存在毛边现象。由图4-15(b)和图4-15(b1)所示，对原始点云数据进行双边滤波可有效的消减点云的噪声，是点云数据表面变得平滑，增强数据的可用性。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 77,
    "chunk_index_in_section": 76,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 392,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c77",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "4.4 目标区域及点云分割系统测试分析\n\n在测试数据中，选用了不同环境条件下，以及不同成熟程度，不同遮挡情况下的草莓图像来评估草莓目标区域分割模块的性能，测试结果如图4-16所示：\n\n图4-16 不同条件下草莓区域分割测试结果\n\n图4-16(a)为弱光照条件下的草莓目标区域点云分割测试结果，得到了正确的草莓目标区域并精准分割目标点云。图4-16(b)为有枝叶遮挡的情况，本文设计的点云分割模块获得了合理的预测结果，还原了图像中草莓所属的像素区域，并成功获得遮挡草莓的目标点云。图4-16(c)为强光照射情况，模块也表现出了较好的分割能力，没有受到光斑的影响。图4-16(d)为强光照射且有枝叶遮挡的条件下，模块的分割结果仍较为准确。从以上测试结果看，本文设计的方法对温室环境下图像的草莓目标点云分割具有较好的适应性。\n\n4.5 本章总结",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 78,
    "chunk_index_in_section": 77,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 369,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c78",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "4.5 本章总结\n\n本章将草莓目标区域精细分割分成了三大步，分别是基于颜色分割与饱和度阈值的草莓目标预定位、基于卷积神经网络的草莓目标识别模块以及基于全卷积神经网络的草莓目标区域像素精细分割模块。完成草莓目标区域图像的精细分割后，对草莓目标区域在点云空间中进行分割和滤波，其中，还涉及到了对分割出的目标区域的草莓进行成熟性判别，以更好的完成草莓目标点云的提取和分割。最后对整个草莓目标区域精细分割系统进行了测试，测试结果表明，本文设计的草莓目标区域精细分割系统对自然环境具有较高的适用性和鲁棒性，可成功完成草莓目标区域的精细分割和点云分割任务。\n\n第五章草莓果实空间姿态与采摘点识别方法设计\n\n5.1 草莓果实空间姿态估计\n\n5.1.1 点云预处理\n\n5.1.2 草莓目标识别与分割\n\n5.1.3 草莓位姿估计\n\n5.1.4 遮挡草莓位姿估计\n\n5.1.5 采摘点获取",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 79,
    "chunk_index_in_section": 78,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 384,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c79",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "5.1.3 草莓位姿估计\n\n5.1.4 遮挡草莓位姿估计\n\n5.1.5 采摘点获取\n\n5.2 动作生成系统软件设计\n\n5.2.1 坐标空间转换\n\n5.2.2 采摘顺序分析\n\n5.3 位姿估计与采摘点定位测试分析\n\n5.4 真实采摘测试\n\n5.5 本章总结\n\n第六章总结与展望\n\n6.1 已有工作总结\n\n6.2 未来改进方向\n\n本文方法框架流程连续，各个环节紧密配合，成功实现草莓采摘点的空间定位，完成采摘动作。未来需增强算法的通用性与模块间独立性，提高稳定性和鲁棒性。研究指出，为实现草莓完全自主采摘，需对果园机器人移动平台技术进行深入研究，确保移动平台能在果园畅行无阻，承载采摘平台进行作业。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 80,
    "chunk_index_in_section": 79,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 297,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻_s1_c80",
    "source_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
    "text": "参考文献：\n[1]蓝峰,苏子昊,黎子明,谢舒.果园采摘机械的现状及发展趋势[J].农机化研究,2010,32(11):249-252.\n[2]徐明东,邱立涛,曲文浩,冀年源,于杨青.草莓采摘机器人的系统研究设计[J].机械研究与应用,2022,35(01):136-138+144.\n[3]冯青春,郑文刚,姜凯,邱权,郭瑞.高架栽培草莓采摘机器人系统设计[J].农机化研究,2012,34(07):122-126.\n[4]赵世达.基于机器视觉的草莓识别技术研究[D].武汉轻工大学,2018.\n[5]谢志勇,张铁中,赵金英.基于Hough变换的成熟草莓识别技术[J].农业机械学报,2007(03):106-109.\n…（以下省略其他参考文献编号，以“…”代替）\n多项研究涉及草莓识别与定位方法，包括机器视觉、Hough变换、卷积神经网络等技术的应用。为实现自动采摘，还需关注果园机器人移动平台技术，确保其能在复杂环境中准确进行果实定位与采摘。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 81,
    "chunk_index_in_section": 80,
    "total_chunks_in_section": 81,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 422,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9450000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c0",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "随着电力需求的快速增长，电力运检领域积累了大量数据资料，其中包括大量非结构化文本，造成专职人员在信息阅读和整理方面的不便。开展电力运检领域的自动化、智能化工作，是现代电网的必然需求。知识图谱以图的方式表达了人类对世界万物的认知方式，使用知识图谱技术可以发现知识、有效的组织知识并高效的检索知识，从而帮助电力运检领域实现业务智能化。目前，电力运检领域知识图谱中知识抽取面临两个问题：一是缺乏针对电力运检领域的知识抽取算法模型，二是没有公开可用的标注语料。为此，本文研究并参考知识抽取算法在其他领域范围的研究材料和具体应用状况，最后结合本文所处的实际情况，设计并实现了面向电力运检的知识抽取算法。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 44,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 296,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c1",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "为实现电力检修领域的自动化和智能化，提高检修效率，减少工作疏漏，利用非结构化文本文档资料中的电力运检知识成为关键。首先，需要将海量文本数据转化为计算机易识别和处理的数据格式，然后挖掘电力运检领域的知识点并构建知识库，最后利用信息管理能力实现知识数据的管理，方便索引和查询相关知识点。构建电力运检知识图谱是智能搜索的基础，通过对运行检修规范文件和日志数据的智能分析和深度挖掘，实现数据关联分析，建立语义化关联关系，实现数据的可视化，提高数据检索的准确性和智能性。构建知识图谱使用的数据包括指导手册、相关百度百科数据、运检日志等非结构化数据。知识图谱以图的方式表达概念、实体及其关系，通过使用知识图谱的方式使互联网中信息数据的表达方式更符合人类思考认知世界的形式。知识图谱技术在语义化搜索和智能问答应用方面表现不俗，已成为互联网新一代以知识驱动为基础的智能服务应用的重要基础设施。知识图谱技术包含知识图谱构建技术和知识图谱应用技术，是一个集知识表示推理与计算、自然语言处理、信息抽取与检索、语义网、机器学习与深度学习等众多方向于一身的交叉研究。从大数据中可以半自动化或自动化获取高质量知识，根据知识建立智能化系统并最终为互联网提供智能化的知识服务是如今大数据时代的必然需求。\n\n本文针对电力运检知识图谱的知识抽取算法及应用进行研究，主要研究内容包括：",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 573,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c2",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "本文针对电力运检知识图谱的知识抽取算法及应用进行研究，主要研究内容包括：\n\n1. 针对电力运检实体抽取任务，生成伪标注数据集，并研究在伪标注有噪音的情况下的电力运检命名实体识别算法。\n\n2. 针对数据不平衡问题，提出通过层次编码来缓解标签类别不平衡的方法。\n\n3. 针对电力运检文本结构复杂专业，关系隐含在句子中的问题，提出基于依存句法分析的电力运检关系抽取方法。\n\n4. 将知识抽取算法抽取到的三元组存入数据库，封装成可调用的服务供后续电力运检知识图谱管理系统调用。\n\n5. 设计并实现电力运检知识图谱管理系统，包括需求分析、概要设计、详细设计、具体实现与系统测试。\n\n6. 总结与展望，回顾课题在电力运检知识图谱知识抽取算法设计与应用的主要工作，并展望未来可能的进一步工作。\n\n清洗后的内容如下：\n\n---\n\nTransformer模型是一种基于多层编码器-多层解码器结构的网络模型。最初该结构是为了提升机器翻译任务的效率，由于循环神经网络是顺序执行的，t时刻的任务没有完成，就没办法进行t+1时刻的解码工作，因此训练过程中难以并行化，该网络提出的自注意力机制不仅在功能上可以替代循环神经网络，并且可以并行运算。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 508,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c3",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "自注意力机制如图2-5所示，对于每个输入向量都会生成三个新变量<2、尺、V，分别表示query向量、key向量和value向量。query向量在编码当前词的时候可以去注意到其他词，key向量和value向量是一对键值对，key向量是索引，value向量是该词真正内容，其表达式为公式2-12所示，自注意力机制在Transformer模型中被升级为由多个自注意力机制组成的多头注意力机制，多头注意力机制的结构图如图2-6所示其表达式为公式2-13和公式2-14。多头注意力机制在计算一个词向量时会从多个维度进行自注意力机制的计算从而得到不同的角度的结果并将结果进行拼接。\n\nBERT模型是由Google于2018年提出的面向自然语言处理任务的通用预训练模型架构且该架构在多项自然语言处理任务中表现优秀。BERT模型通过利用海量无标注的互联网文本数据进行无监督训练来获得包含字、词、句子以及句间关系等丰富的语义信息的文本表征并通过将BERT模型预训练得到富含丰富语义的文本表征在特定自然语言处理任务中进行轻微调整或特征集成来最终应用于特定领域的特定场景下的自然语言处理任务。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 485,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c4",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "BERT模型的预训练阶段将会进行两个任务的训练：随机遮盖文章部分词并训练模型预测遮盖词任务和判断两句话是否连贯任务。随机遮盖文章部分词并训练模型预测遮盖词任务是让模型根据文章的上下文像做完形填空一样来预测文章中被随机遮盖的词汇；判断两句话是否连贯任务主要判断B句是否紧跟在A句之后。模型可以通过两个任务的无监督预训练后学到通用的语言表征。微调是指通过BERT预训练阶段后获得BERT模型及对应网络结构(Transformer)后，微调训练阶段仍采用与预训练过程结构相同的网络结构进行模型训练，并将特定任务的特定数据直接用在该网络上进行模型训练使得预训练阶段获得的网络参数得到特定领域数据的修正。BERT模型在命名实体识别任务上的微调如图2-8所示。\n\n---\n\n以上内容已去除页眉、页脚、页码、重复信息和乱码，保留了核心学术内容、技术术语和数据、原有逻辑结构、公式和图表说明。\n\n清洗后的内容如下：\n\n---\n\n电气设备，如压力表、高低压开关柜、温度计等\n\n电力领域相关机构名称，如国家电网公司、国家电网公司国家电力调度中心、省公司运检部等\n\n参数 人为传进去的变量，如运行电压、持续时间等\n\n环境变量 非人为变量，如相对湿度、环境温度等\n\n不可单独使用的设备部件，如气体收集袋、聚四氟乙烯垫片、聚氯乙烯管等\n\n可以单独使用的不属于一次和二次设备的设备，如真空泵、吹风机、吸湿器、排水泵等",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 594,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c5",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "不可单独使用的设备部件，如气体收集袋、聚四氟乙烯垫片、聚氯乙烯管等\n\n可以单独使用的不属于一次和二次设备的设备，如真空泵、吹风机、吸湿器、排水泵等\n\n实验相关实体 电力检测实验中相关实体，如带电检测记录、档案分析判断法、波形图、趋势图等\n\n非1-8的其他可以作为实体的电力名词，如PMS系统、放电信号、杂质微粒等\n\n---\n\n远程监督的思想最早由文献[59]引入到关系抽取任务中。训练一个特定领域的关系抽取模型，需明确待识别关系的两个目标实体和出现的文本上下文，且待识别的关系是预先明确定义的。这是由于传统的监督限定域关系抽取模型是无法自主地完成对关系的命名。远程监督是一种利用外界海量知识来指导标注数据集的弱监督方法。将其应用于关系抽取任务即假设存在两个实体且这两个实体能够和知识库中已存在的实体对匹配，则这两个实体同时包含了在知识库中匹配的实体对关系。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 5,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 378,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c6",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "将远程监督的思想应用于命名实体识别的数据标注任务，需要自行构建一个知识库，本文通过人工标注的方法得到了一个带有电力运检实体类别标注的文本。再对该文本进行去重及提取操作处理，可以得到一个包含电力运检的实体二元组<实体类型，实体名称>的知识库。本文把所有的《五通一措》文本数据按照一定的顺序进行遍历，当在某位置的实体与知识库中存储的实体名匹配时，则按照BEMO的方式将该命名实体机器标注为电力运检二元知识库中存储的相应的电力运检命名实体类型，机器标注完成后即可得到一个伪标注的电力运检命名实体的文本数据集。\n\n---\n\n数据标注流程：首先，从整个《五通一措》数据中抽出部分(总数据的1/4)未标注数据，然后根据上文中的标注方法对其进行人工标注得到部分标注数据，进一步从这部分标注数据中提取标注的电力运检实体形成电力运检实体二元组<实体类型，实体名称>并将其存入知识库，然后使用该电力运检二元实体知识库对整个待标注的《五通一措》语料进行机器程序伪人工标注，最后代替人工标注使用机器生成的伪人工标注数据语料对模型进行训练。本文中对《五通一措》语料进行数据伪标注流程如图3-3所示。其中，本文把所有的数据重新标注的原因是可以避免一些人工标注不一致所带来的错误。\n\n---\n\n标注1/4 人工标注 数据提取 伪人工回标\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 6,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 558,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c7",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "---\n\n标注1/4 人工标注 数据提取 伪人工回标\n\n---\n\n数据标注结果：根据3.2.3小节介绍的数据标注流程，本文得到了一个伪标注数据集，本文“BPT-Power”数据集。“BPT-Power”伪标注数据集的实体统计信息和标注统计信息分别如表3-2、表3-3所示。\n\n---\n\n表3-2 “BPT-Power”数据集实体信息统计 类别编号 类别含义 实体数量 实体标注结果示例\n\n1 ITT 电力变压器、电压互感器、隔离开关、并联电容器 420\n\nAQ 1 真空压力表、高低压开关柜、低压配电屏、电解式湿度计 69\n\n3 机触 量准确度、放电频次、取样总量、气体流量 179\n\n5 环境变量 环境温湿度、气样湿度、风速 17\n\n6 赚 气体收集袋、聚四氟乙烯垫片、聚氯乙烯管 882\n\n7 其他设备 真空泵、吹风机、吸湿器、排水泵 206\n\n8 走萨铂龙壶彼 带电检测记录、档案分析判断法、波形图、趋势图 830\n\n9 其他实体 PMS系统、放电信号、杂质微粒 0\n\n---\n\n表3-3 “BPT-Power”数据集标记统计信息 数据集名称 标签数 句子数 实体数 文件大小/MB\n\nBPT-Power 426 767 19659 28847 3.2\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 7,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 534,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c8",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "BPT-Power 426 767 19659 28847 3.2\n\n---\n\n为了对比程序伪标注结果与人工标注的差别，我们从全部数据集中随机抽取50个句子通过再次人工标注的方法进行检查，并以指标P进行结果统计，如表3-4所示。\n\n---\n\n表3-4 程序伪标注检查结果 抽取方法 抽取句数 P(%) \n\n随机抽取 50 92\n\n---\n\n基于层次编码的实体抽取算法：实体抽取任务即命名实体识别是自然语言处理领域中一项重要任务且经过几十年研究发展已取得了显著成果。但在一些特定领域，难以建立足够多针对特定领域的标注语料且数据大多存在数据标记类别数量不平衡问题。大多数现有的实体抽取方法基于数据驱动来实现，即数据样本量越大则模型的学习效果越好。而数据量不够大则会导致模型的效果大打折扣。数据类别标记不平衡是指不同类别的数据样本量差距较大。而部分小样本量类别呈现出的信息可能更具有价值。数据类别标记样本数量的不平衡问题会使模型更关注多数类别的数据并忽略少数更有价值类别数据，从而影响统计学习模型的效果。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 8,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 449,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c9",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "针对已有方法需要大量数据驱动的问题，本节基于BERT预训练模型学习语言模型并通过改造数据标签编码方式提出了两种基于变长编码的数据类别平衡方法，本文将其分别命名为BERT-Huffman算法和BERT-Balanced算法。本节方法没有改变原始数据，而是根据数据类别标签数量构建哈夫曼编码与平衡编码，通过分层处理的方法平衡数据标签在每一层的差距。实验结果表明本节提出的两种命名实体识别方法能够有效缓解数据集中类别标记样本数量不平衡的问题并提高电力运检领域命名实体实体识别模型的性能。\n\n---\n\n标注数据分析：在命名实体识别任务中标签样本数量不平衡是常见的问题，首先造成这种问题的原因是由于句子中大多数词不属于实体引起的。如图3-4所示，在BPTpower数据集上“0”标签数量占比达到了75.6%，而电力运检领域命名实体只占到总数据的24.4%。\n\n---\n\n实体抽取模型总体结构：实体抽取模型即命名实体识别模型总体结构如图3-6所示。本文提出的电力运检命名实体识别模型分为两个部分其分别是BERT层、编码解码层(图中树结构部分)。本文使用BERT模型[％的微调方式获得单词的特征表示。编码解码层，本文采用了两种方式来进行层次编码替代传统的独热编码，这种编码本文分别命名为哈夫曼编码和平衡编码。\n\n---\n\n层次编码：(1)哈夫曼层次编码 (2)平衡层次编码\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 9,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 584,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c10",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "---\n\n层次编码：(1)哈夫曼层次编码 (2)平衡层次编码\n\n---\n\n实验对比与分析：本小节将在“BPT-Power”数据集以及CLUNER数据集[57]对模型进行训练并与基线方法进行对比，通过实验结果对模型进行分析来验证模型的有效性。\n\n---\n\n实验实现细节：“BPT-Power”数据集划分为两部分分为训练集和测试集其中实验数据集划分统计信息如表3-7所示。\n\n---\n\n表3-7 实验数据集划分 统计类别 训练集 测试集\n\n文件大小/MB 2.75 1 0.260\n\n数据量/实体数 26003 1 2816\n\n数据量/句子数 8828 1 1005\n\n1:5037, 1:596, \n\n2:3768, 2:418, \n\n3:3256, 3:419, \n\n实体分布 4=791 4z88\n\n5:2594, 5:366, \n\n(未去重) 6:802, 6:95, \n\n7:4634, 7:568, \n\n8:635, 8:77, \n\n0:1590 0:174\n\n---\n\n本文实验使用的服务器是配置了两张FeForce GTX 1080Ti显卡并搭载Linux操作系统。实验通过PyTorch深度学习框架编写所使用得深度学习相关代码。本文模型的训练主要过程描述如下1-6个步骤所示。\n\n---\n\n1) 从维基百科、百度百科爬取海量文本，构成海量文本语料库；",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 10,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 582,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c11",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "---\n\n1) 从维基百科、百度百科爬取海量文本，构成海量文本语料库；\n\n2)对海量本文语料库进行数据预处理并形成语言模型训练语料；\n\n3)BERT语言模型通过语言模型训练语料进行大规模预训练；\n\n4)根据电力运检实体标签在电力运检实体识别语料中的数量构建电力运检实体标签的哈夫曼编码或平衡编码；\n\n5)在预训练得到BERT语言模型后增加分类层构成BERT电力实体识别模型，通过电力运检实体识别语料对BERT电力运检实体识别模型进行再次训练，得到BERT电力运检实体识别模型；\n\n6)在预训练得到BERT语言模型后增加分类层构成BERT电力运检实体识别模型。输入文本经过BERT电力运检实体识别模型时，先经过嵌入层变成文本向量序列，然后经过多层编码器得到BERT语言模型的输出，最后经过分类得到实体标签。分类层包括依次连接的全连接层和Sigmoid激活函数，分类层的输入为BERT语言模型的输出，分类层的输出为预测的电力实体标签的哈夫曼编码或平衡编码，通过编码映射得到对应的电力运检实体标签。采用交叉熵损失计算电力\n\n清洗后的内容如下：\n\n实体识别语料上的真实标签和BERT电力运检实体识别模型输出标签的差异，并通过AdamW优化器训练BERT电力运检实体识别模型。当电力运检实体识别语料验证集上模型的损失不再下降时，模型停止训练，保存模型参数。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 11,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 573,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c12",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "将待识别语料输入至BERT电力运检实体识别模型中，得到电力运检实体标签的哈夫曼编码或平衡编码，通过编码映射得到实体标签，进而得到语料中预先定义9个类别的实体。\n\n实验分析首先对比两种本文提出的层次编码效果，两种标签编码方式，在不同的标签数量分布下各具有特点。其次为对比独热编码与层次编码的差别，本文在表中统计了在BUPower数据集上采用独热编码方式的不平衡标签数量。\n\n本文提出的层次编码产生的标签类别不平衡数量更少，在编码上取得了更平衡的效果。为对比模型效果，本文选取了4个最具代表性的命名实体识别模型进行对比。\n\n本文方法没有使用电力运检领域的本身特征，模型具有很好的泛化能力。为验证本文提出模型的泛化能力，本文在CLUNER数据集上对其进行相同模型的验证。\n\n本文通过对实体标签进行哈夫曼编码来缓解类别不平衡造成的问题。平衡编码与哈夫曼编码类似，更注重实体标签类别数量的平衡，在构建编码路径时，每次选择数量相差最小的两个实体标签构成子树，则其0、1分布相较于哈夫曼编码更为平衡，对于实体标签预测存在的标签类别数量不平衡问题的处理效果也会更好。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 12,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 475,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c13",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "基于依存句法分析的关系抽取算法由于缺乏电力运检的实体关系的监督信息，本文实现电力运检实体关系抽取将基于对关系抽取算法的调研以及对电力领域现状的分析，本文采用了现有的成熟工具包HanLP使用基于依存句法分析的方法通过无监督的方式挖掘文本中的电力运检实体关系。\n\n本文使用的电力运检实体关系抽取方法首先对待识别的句子使用本文提出的电力运检命名实体识别算法进行命名实体识别，得到正确的电力运检实体并将其加入到HanLP的分词规则中来保障电力运检实体在分词阶段没有被错误切分，然后通过HanLP对待识别句子再次进行中文词性标注和中文依存句法分析操作得到待识别句子的句法依存树，最后通过对得到的句法依存树进行分析来抽取句中关系。在此基础上通过分析谓词与论元(电力运检实体)的句间关系来抽取初步的三元组，然后通过分析挖掘到的关系表述进一步制定电力运检的关系转换规则来生成可用的电力运检三元组，最后通过人工筛选进一步提高电力运检三元组的质量。\n\n本文3.4节中使用的数据是数据预处理模块对《五通一措》的原数据进行数据清洗、去重、切割成适合依存句法分析算法后的文本数据。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 13,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 477,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c14",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "本文3.4节中使用的数据是数据预处理模块对《五通一措》的原数据进行数据清洗、去重、切割成适合依存句法分析算法后的文本数据。\n\n基于依存句法分析的关系抽取在依存句法算法中\"是描述句子含义的主要信息，而句子中的其他成分例如主宾补等都与之有间接或直接的联系。所以在使用依存句法算法抽取得到的关系将是谓语动词。通过分析句中成分间的关系，我们以关系表述为核心寻找前后电力运检实体论元。由于文本中涉及到的句间关系种类繁多，本文主要实现了如表所示的部分句间关系。\n\n关系表述分为三个部分介绍，首先分析依存句法分析算法适合使用的句子长度，再介绍电力运检关系表述的含义分析，最后再介绍电力关系表述的转换规则。\n\n通过上述方法，本文从23064个句子中一共获取到了6108个三元组，2067个关系表述且其中三元组部分示例如表所示。\n\n由于抽取的文本是电力指导手册，部分抽取结果中不能保证两个论元都是电力实体，这样导致我们抽取三元组不是客观事物间联系的表述，但从结果分析来看，抽取的关系表述也表达了电力运检指导意见的关注重点，隐含了电网公司对于运行检查电力设备的要求、操作等关系，本文对得到的2067个关系表述中三元组统计数量前20的关系表述进行详细统计如表所示。\n\n第7部分内容：\n\n满足某种要求采用方式\n\n清洁现象\n\n完好表现\n\n进行表述过程相关操作良好表现\n\n电力设备的正常状态\n\n指不表现\n\n正吊具体现象表现",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 14,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 597,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c15",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "第7部分内容：\n\n满足某种要求采用方式\n\n清洁现象\n\n完好表现\n\n进行表述过程相关操作良好表现\n\n电力设备的正常状态\n\n指不表现\n\n正吊具体现象表现\n\n使用相关部件大于細細\n\n包括表ｓＳｆ包含牢固牢固的现象\n\n超过范围范围接地接地现象\n\n安装安装操作安装操作应\n\n表３－１８三元组转换结果部分示例\n\n关系表述关系转换三元组原句\n\n金属法兰，注意现象，锈金属法兰无锈蚀、无外伤\n\nＴ社音善Ｍ或铸造砂眼\n\n７Ｃ部位表面，注意现象，过套管接线端子等连接部位\n\n热现象表面应无氧化或过热现象\n\n出厂试验电容量值与设备\n\n标志电容，范围，±５％铭牌标志电容量相比超过\n\n超过范围士５％\n\n实测电感值与额定值偏差\n\n差，范围，土５％不应超过±５％\n\n非全相保护继电器，包\n\n带有试验按钮的非全相保护继电器应有警示标志\n\n钢管构架，包含，排水孔钢管构架应有排水？\n\n可以通过规则转换的方式将关系表述转换为更符合知识图谱构建的关系表\n\n述，从而得到更可用更易于人类理解的三元组，例如谓语\n\n，在《五通一措》中，有这样的表达\n\n“ ＭＯＶ接线板表面无氧化、划痕、脏污，接触良好。”\n\n“ＳＦ６密度继电器指示正常，表计防震液无渗漏”\n\n“油断路器本体油位正常，无渗漏”\n\n等，这些句子表达了电力公司想要运检人员关注电力设备这些现象，并要求无这些损坏现象，那么这个谓语关系表述就可以转化为“要求无该类”。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 15,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 591,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c16",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "等，这些句子表达了电力公司想要运检人员关注电力设备这些现象，并要求无这些损坏现象，那么这个谓语关系表述就可以转化为“要求无该类”。\n\n本文的详细电力运检关系表述转换规则及分析如表３－１７所示，表述该项被弃用。通过关系表述规则转换后部分三元组结果如表３－１８所示。\n\n３．４３实验结果\n\n通过这种方式我们得到了１１７７个三元组，为了验证本文提出的电力运检三元组抽取方法的准确率，我们通过人工标注了５０个句子，通过Ｐ、Ｒ和Ｆ指标来验证，验证结果如表３－１９所示。\n\n表３－１９三元组验证结果\n\n抽取方法抽取句数Ｐ（％）Ｒ（％）Ｆ（％）\n\n依存句法分析抽取５０３９．２８３３．９２３６．４０\n\n本文中关系抽取的指标效果较低，主要原因有两个：一个是本身使用的数据是电力运检的指导手册，句子蕴含的电力运检关系数量较少且关系隐含导致关系抽取困难其最终指标较低；二是关系抽取使用的方法是依存句法分析，只使用到实体边界信息，没有使用到实体本身信息导致的关系抽取算法本身具有局限性。\n\n一的关系更为重要，领域关系抽取难度较大，如何增加更多与电力运检领域相关数据也是未来提升模型能力的一个研究方向。\n\n３．５本章小结",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 16,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 498,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c17",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "一的关系更为重要，领域关系抽取难度较大，如何增加更多与电力运检领域相关数据也是未来提升模型能力的一个研究方向。\n\n３．５本章小结\n\n数据类别标记失衡是命名实体识别任务中普遍存在的问题。在电力领域也存在着命名实体识别研究较少、标注数据稀缺、数据类别标记失衡的问题，本文在３．３小节针对这一现状创造性地改造了原有的独热编码，提出了以分层平衡数据的哈夫曼编码与平衡编码。实验结果表明，本文提出的基于层次编码的哈夫曼编码与平衡编码的命名实体识别方法能有效缓解数据集标记类别数量失衡问题并有效改善了模型的精确率和召回率。本文方法以分层处理的方式来分级处理标签不平衡，但是哈夫曼编码和平衡编码都是从不同的角度启发式构建，在一些特殊的数据标签分布或下可能会失效，如文中编码的首个位置的标签数量比是３：１，未来可以考虑在该编码上加入平衡因子等数据平衡方法。\n\n本章在３．４小节详细介绍了基于依存句法分析的电力运检关系抽取算法的设计与实现。首先对基于依存句法分析的电力运检关系抽取算法的整体流程进行了简单介绍，然后对基于依存句法分析的关系表述抽取方法进行了介绍，接着分析了电力运检文本数据的关系表述的特点，介绍了电力运检关系抽取关系表述的转换规则后介绍了本文提出的基于依存句法分析的电力运检关系抽取算法的实验结果。\n\n清洗后的内容如下：",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 17,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 559,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c18",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "清洗后的内容如下：\n\n本系统采用B/S结构，前后端分离实现，后端采用基于EGG的Web框架技术栈，使用MySQL数据库持久化用户相关数据，并使用Redis作为数据缓存以提高系统查询及存储性能。后端通过Nginx进行多机器负载均衡来提高系统吞吐量。前端采用基于React的单页应用Web应用框架技术栈来实现。前后端通过基于HTTP的Restful API交互数据，使用Nginx做反向代理实现跨域通信。使用JWT技术来实现无状态token，用于验证用户的登录状态和实现权限控制，以保证系统的安全性能。\n\n综合算法研究部分的数据预处理、知识抽取和通过爬虫收集实体属性三个部分，整个系统可以归总为六个部分：数据处理模块、知识图谱构建模块、前端展示模块、后端处理模块、自然语言处理模块以及爬虫模块。整个系统的数据流动如图所示，由数据处理模块对文本进行预处理后将数据送进知识图谱构建模块进行知识抽取与知识的存储，用户通过前端界面发送请求给到后端处理模块，后端处理模块根据请求类型来调用数据库服务或调用自然语言处理服务，爬虫模块通过爬取网页数据并进行解析后存入数据库。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 18,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 478,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c19",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "前端页面结构设计如图所示，结合系统模块功能需求分析，为了方便用户使用，本系统的Web端页面结构如图所示。本系统将使用者分为三种角色，分别是：超级管理员、图谱管理员以及图谱查询用户，不同角色登录本系统会依据角色不同展示不同的功能面板。从总体上来说，Web端将分为四个模块，分别是：用户权限、知识图谱、自然语言处理以及日志监控。\n\n系统数据存储分为三个部分：实体及实体关系的知识库数据的存储，用户信息、角色信息以及实体属性信息的存储以及日志监控信息的存储，分别用HugeGraph、MySQL以及文件进行存储。\n\n后端模块即Web后端模块，通过对来自前端的http请求根据请求类型分别转化为SQL语句对MySQL中实体属性信息(实体详情)做查询并生成JSON格式数据返回给前端用于展示、调用HugeGraph图数据库进行查询、存储用户及角色信息。Web后端模块还会通过中间件拦截请求以对用户登录状态及其权限进行管理。后端模块连接了系统的其他模块，为前端模块提供可靠服务。后端的整体处理流程如图所示。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 19,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 447,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c20",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "表 5-3 登录请求接口\n- 请求路径: api/user/login\n- 请求方法: POST\n- 接口功能说明: 用户登录\n- 请求参数示例: \n  - username: iW\n  - password: 1\n  - data: { role: '' }\n- 返回参数示例: \n  - user_id\n  - username\n  - nickname\n\n表 5-4 查询用户信息请求接口\n- 请求路径: api/user/detail/{userld}\n- 请求方法: GET\n- 接口功能说明: 查询用户信息\n- 请求参数示例: 无\n- 返回参数示例: \n  - username\n  - email\n  - birthday\n  - role\n\n表 5-5 查询实体属性信息请求接口\n- 请求路径: api/kgdetail/{entityld}\n- 请求方法: GET\n- 接口功能说明: 查询实体属性信息\n- 请求参数示例: 无\n- 返回参数示例: \n  - entity: { name, type }\n\n数据处理模块流程图如图 5-4 所示。\n\n爬虫模块流程图如图 5-5 所示。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 20,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 500,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c21",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "数据处理模块流程图如图 5-4 所示。\n\n爬虫模块流程图如图 5-5 所示。\n\n知识图谱模块分为两部分，一部分是建立知识图谱，使用预处理模块持久化的数据，进行知识抽取和三元组筛选，将筛选后的三元组 CSV 文件以及实体 CSV 文件使用 HugeGraph 数据导入工具导入 HugeGraph 图数据库。另一部分是提供对知识图谱进行管理的功能，包含知识图谱查询以及创建修改能力，使用 HugeGraph 语句，对图数据库中数据进行查询、修改以及创建。\n\n自然语言处理模块分为两个部分，一个是调用 HanLP 进行分词、词性标注和依存句法分析，另一个是封装了本文的电力运检命名实体识别模型，可以为知识图谱模块提供命名实体识别的数据处理能力。\n\n日志监控模块分为两部分，分别是登录及日常操作日志 access.log 和系统自定义日志（本系统为报错信息 error.log 和常用信息 info.log 等）。使用 koa-logger 对日志进行格式化，使用 koa-morgan 库使用流的方式提高文件读写性能来记录日志。日志文件会按照时间划分，以天为拆分单位。\n\n系统测试环境如表 5-12 所示。系统开发后需对系统进行详细的功能性测试和非功能性测试。功能性测试旨在满足基本的系统运行条件下，系统功能可以正常运行。非功能性测试旨在测试系统的运行响应速度和页面的兼容性。\n\n清洗后的内容如下：",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 21,
    "chunk_index_in_section": 21,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 597,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c22",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "清洗后的内容如下：\n\n本文开展了电力运检知识图谱知识抽取算法设计与应用的研究。首先，通过分析电力运检数据资料，确定了研究模型的方向，并确立了相关核心技术。接着，设计了数据处理、实体抽取、关系抽取和爬虫四个模块，实现了电力运检实体和关系的抽取。最后，建立了电力运检知识图谱管理系统，实现了知识图谱可视化、查询和管理功能。本文为电力运检知识图谱知识抽取算法实现和可视化知识图谱管理系统提供了可行性的方案并验证了算法模型和系统的可行性。未来的工作主要包括改进算法性能、提升知识图谱应用、获取更多数据提升模型性能，以及基于知识图谱开发智能问答系统。\n\n参考文献：\n\n[3] Quillian, M. R. (1968). Semantic networks. In M. Minsky (Ed.), Semantic networks: approaches to knowledge representation and research studies (pp. 1-50). Cambridge, MA: MIT Press.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 22,
    "chunk_index_in_section": 22,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 462,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c23",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "[4] Arrigo, S., Bruin, J. D., & Ding, Y. (1970). Ontology mapping and aligning. In Proceedings of the 1970 conference on artificial intelligence (pp. 1-10). New York: ACM.\n\n[5] Feigenbaum, E. A. (1981). Expert systems in the 1980s. In State of the art report on machine intelligence (pp. 1-50). Maidenhead: Pergamon-Infotech.\n\n[6] Lenat, D. B., Prakash, M., & Shepherd, M. (1985). CYC: Using common sense knowledge to overcome brittleness and knowledge acquisition bottlenecks. AI Magazine, 6(4), 65-75.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 23,
    "chunk_index_in_section": 23,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 503,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c24",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "[7] Bemers-Lee, T. (1989). Information management: A proposal. http://www.w3.org/History/1989/proposal.html\n\n[8] Bemers-Lee, T. (1998). What the Semantic Web can represent. http://www.w3.org/DesignIssues/RDFnot\n\n[9] Bemers-Lee, T. (1998). Semantic Web roadmap. Internal note. World Wide Web Consortium.\n\n[10] Bemers-Lee, T. (2006). Linked data - design issues. http://www.w3.org/DesignIssues/Link\n\n[11] Bemers-Lee, T. (2009). The next web. TED.com.\n\n[12] Auer, S., Bizer, C., Kobilarov, G., et al. (2007). Dbpedia: A nucleus for a web of open data. The Semantic Web, 722-735.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 24,
    "chunk_index_in_section": 24,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 575,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c25",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "[13] Zhou, K. (2010). 基于规则的命名实体识别研究 [Research on rule-based named entity recognition]. Hefei: Hefei University of Technology.\n\n[14] Schuster-Boeckler, B., & Bateman, A. (2007). An introduction to hidden Markov models and current protocols in bioinformatics. Current protocols in bioinformatics, 18(1), A3A.1-A3.9.\n\n[15] McCallum, A., Dayne Freitag, & Fernando Pereira, C. N. (2000). Maximum entropy Markov models for information extraction and segmentation. Icml, 17, 2000.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 25,
    "chunk_index_in_section": 25,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 473,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c26",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "[16] Lafferty, J., McCallum, A., & Pereira, F. C. N. (2001). Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the 18th International Conference on Machine Learning (pp. 67-72). Morgan Kaufmann.\n\n[17] Huang, Z., Xu, W., & Yu, K. (2015). Bidirectional LSTM-CRF models for sequence tagging. arXiv preprint arXiv:1508.01991.\n\n[18] Zhu, Y., Wang, G., & Karlsson, B. (2019). CAN-NER: Convolutional attention network for Chinese named entity recognition. arXiv preprint arXiv:1905.05583.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 26,
    "chunk_index_in_section": 26,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 540,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c27",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "[19] Rei, M., Crichton, G. K. O., & Pyysalo, S. (2016). Attending to characters in neural sequence labeling models. arXiv preprint arXiv:1603.01354.\n\n[20] Zukov-Gregoric, A., Bachrach, Y., Minkovskiy, P., et al. (2017). Neural named entity recognition using a self-attention mechanism. In 2017 IEEE 29th International Conference on Tools with Artificial Intelligence (ICTAI) (pp. 1-8). IEEE.\n\n[21] Yan, H., Deng, B., Li, X., Xiao, N., & Qiu, X. (2019). TENER: Adapting transformer encoder for name entity recognition. arXiv preprint arXiv:1911.04474.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 27,
    "chunk_index_in_section": 27,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 550,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c28",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "[22] Peng, M., Xin, X., & Zhang, Q. (2019). Distantly supervised named entity recognition using positive-unlabeled learning. arXiv preprint arXiv:1905.05583.\n\n[23] Shang, J., Liu, L., Ren, X., et al. (2018). Learning named entity tagger using domain-specific dictionary. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (pp. 1-10).\n\n[24] Liu, Z., Zhu, C., Zheng, D., & Zhao, T. (2019). 面向特定标注数据稀缺领域的命名实体识别 [Named entity recognition for specific domains with scarce annotation data]. Command Information System and Technology, 1-5.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 28,
    "chunk_index_in_section": 28,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 566,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c29",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "[25] Wang, S., Tang, K., & Yao, X. (2009). Diversity exploration and negative correlation learning on imbalanced data sets. In 2009 International Joint Conference on Neural Networks (pp. 3259-3266). IEEE.\n\n[26] Guo, X., Xinjian, Y., & Yin, G. (2008). On the class imbalance problem. In Proceedings of the 4th International Conference on Natural Computation (pp. 1-4). IEEE.\n\n[27] Douzas, G., Georgios, B., Bacao, F., et al. (2018). Effective data generation for imbalanced learning using conditional generative adversarial networks. Expert Systems with Applications, 114, 322-335.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 29,
    "chunk_index_in_section": 29,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 580,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c30",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "[28] Akkas, A., & Abbasi, A. (2017). Balanced undersampling: A novel sentence-based undersampling method to improve recognition of named entities in chemical and biomedical text. Applied Intelligence, 48(4), 1-14.\n\n[29] Lin, T. Y., Goyal, P., Girshick, R., et al. (2017). Focal loss for dense object detection. In Proceedings of the IEEE International Conference on Computer Vision (pp. 2999-3007). IEEE.\n\n[30] Nguyen, T., Nguyen, D., & Rao, P. (2020). Adaptive name entity recognition under highly unbalanced data. arXiv preprint arXiv:2003.10296.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 30,
    "chunk_index_in_section": 30,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 548,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c31",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "[31] Xu, L., Liu, J., & He, X. (2020). 一种解决命名实体识别数据集类别标记失衡的方法 [A method to solve the class label imbalance problem in named entity recognition datasets]. Journal of Sichuan University (Natural Science Edition), 57(1), 1-8.\n\n[32] Li, M., & Yang, J. (2016). 基于依存分析的开放式中文实体关系抽取方法 [Open domain Chinese entity relation extraction method based on dependency analysis]. Computer Engineering, 6, 201-207.\n\n[33] Li, M. (2018). 医学文献中疾病与病症关系抽取研究与应用 [Research and application of disease and symptom relationship extraction in medical literature]. Unpublished doctoral dissertation.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 31,
    "chunk_index_in_section": 31,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 569,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c32",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "[34] He, L. (2019). 基于依存句法分析的企业税法实体关系抽取方法研究 [Research on enterprise tax law entity relationship extraction method based on dependency syntax analysis]. Unpublished doctoral dissertation.\n\n[35] Kambhatla, N. (2004). Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations. In Proceedings of the ACL 2004 Interactive Poster and Demonstration Sessions (pp. 22-29).\n\n[36] Yang, Y., Dai, Q., Jia, Z., et al. (2014). 基于弱监督的属性关系抽取方法 [Weakly supervised attribute relation extraction method]. Computer Applications, 34(1), 64-68.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 32,
    "chunk_index_in_section": 32,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 569,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c33",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "[37] Gan, L., Wan, C., Liu, D., et al. (2016). 基于句法语义特征的中文实体关系抽取 [Chinese entity relation extraction based on syntactic and semantic features]. Journal of Computer Research and Development, 53(2), 284-302.\n\n[38] Zhao, S., & Grishman, R. (2005). Extracting relations with integrated information using kernel methods. In Proceedings of the ACL 2005 Annual Meeting of the Association for Computational Linguistics (pp. 25-30). University of Michigan, USA.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 33,
    "chunk_index_in_section": 33,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 452,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c34",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "[39] Bunescu, R. C., & Mooney, R. J. (2005). A shortest path dependency kernel for relation extraction. In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (pp. 6-8). Association for Computational Linguistics.\n\n[40] Zhang, Z. (2008). 无监督关系抽取方法研究 [Research on unsupervised relation extraction methods]. Unpublished doctoral dissertation. Harbin Institute of Technology.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 34,
    "chunk_index_in_section": 34,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 442,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c35",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "[41] Sun, Y. (2014). 开放领域的中文实体无监督关系抽取 [Unsupervised relation extraction of Chinese entities in the open domain]. Unpublished doctoral dissertation. East China Normal University.\n\n[42] Hashimoto, K., Kazuma, K., et al. (2013). Simple customization of recurrent neural networks for semantic relation classification. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (pp. 1372-1376).",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 35,
    "chunk_index_in_section": 35,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 415,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c36",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "[43] Liu, C., Chunyang, L., et al. (2013). Convolutional neural network for relation extraction. In Proceedings of the International Conference on Advanced Data Mining and Applications (pp. 1-10). Springer.\n\n[44] Zheng, D., Daojir, N., et al. (2014). Relation classification via convolutional deep neural network. In Proceedings of the 25th International Conference on Computational Linguistics: Technical Papers (pp. 2335-2344).",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 36,
    "chunk_index_in_section": 36,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 429,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c37",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "[45] Jiang, X., Xiaotian, W., et al. (2016). Relation extraction with multi-instance multi-label convolutional neural networks. In Proceedings of the 26th International Conference on Computational Linguistics: Technical Papers (pp. 1471-1480).\n\n[46] Zheng, S., Hao, Y., Lu, D., Bao, H., Xu, J., Hao, H., & Xu, B. (2017). Joint entity and relation extraction based on a hybrid neural network. Neurocomputing, 27, 59-66.\n\n[47] Miao, M., & Bansal, M. (2016). End-end relation extraction using lstms on sequences and trees. arXiv preprint arXiv:1601.00770.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 37,
    "chunk_index_in_section": 37,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 552,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c38",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "[48] Zheng, S., Wang, F., Bao, H., et al. (2017). Joint extraction of entities and relations based on a novel tagging scheme. arXiv preprint arXiv:1706.05075.\n\n[49] Wu, C. (2020). 电力调度知识图谱中知识抽取系统的设计与实现 [Design and implementation of knowledge extraction system in power dispatching knowledge graph]. Unpublished doctoral dissertation. University of Chinese Academy of Sciences (Shenyang Institute of Computing Technology).",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 38,
    "chunk_index_in_section": 38,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 421,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c39",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "[50] Wang, Q. (2020). 电力领域实体关系抽取及知识图谱构建研究 [Research on entity relation extraction and knowledge graph construction in power domain]. Unpublished doctoral dissertation. China University of Geosciences (Beijing).\n\n[51] Zhang, M., Xu, N., Hu, J., Wang, Y., Li, C., Xu, J., & Zhang, S. (2020). 面向变压器智能运检的知识图谱构建和智能问答技术研宄 [Research on knowledge graph construction and intelligent question answering technology for transformer intelligent operation and maintenance]. Global Energy Internet, 3(6), 607-617.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 39,
    "chunk_index_in_section": 39,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 498,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c40",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "[52] Liu, Z., & Wang, H. (2018). 基于知识图谱技术的电力设备缺陷记录检索方法 [Power equipment defect record retrieval method based on knowledge graph technology]. Electric Power System Automation, 42(14), 158-164.\n\n[53] Ding, Y., Shang, X., & Mi, W. (2019). 基于深度学习的电网调控文本知识抽取方法 [Power grid control text knowledge extraction method based on deep learning]. Electric Power Automation Equipment, 39(11), 1-8.\n\n---\n\n力系统自动化, 2020, 44(24): 161-168.\n\n参考文献:",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 40,
    "chunk_index_in_section": 40,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 427,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c41",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "---\n\n力系统自动化, 2020, 44(24): 161-168.\n\n参考文献:\n\n[54] Manning CD, Surdeanu M, Bauer J, et al. The Stanford CoreNLP Natural Language Processing Toolkit. Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations. 2014.\n\n[55] Che WC, Li Z, Liu TL. LTP: A Chinese Language Technology Platform. The 23rd International Conference on Computational Linguistics. 2010.\n\n[56] He HH, Choi JD. Establishing Strong Baselines for the New Decade: Sequence Tagging, Syntactic and Semantic Parsing with BERT. 2019.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 41,
    "chunk_index_in_section": 41,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 545,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c42",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "[57] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need. Advances in Neural Information Processing Systems. 2017: 5998-6008.\n\n[58] Devlin J, Chang MW, Lee K, et al. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. 2018.\n\n[59] Mintz M, Bills S, Snow R, et al. Distant supervision for relation extraction without labeled data. International Joint Conference on Artificial Intelligence. Association for Computational Linguistics. 2009.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 42,
    "chunk_index_in_section": 42,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 479,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷_s0_c43",
    "source_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
    "text": "[60] Xu L, Dong Q, Qianqian L, Cong Y, Tian Y, Liu W, Li L, Zhang X. CLUENER2020: Fine-grained Named Entity Recognition for Chinese. arXiv preprint arXiv:2001.04351. 2020.\n\n[61] Zhu TT, Du YF, Li WF, Xiong YP. Research on Power Text Specialized Vocabulary Recognition Based on Unsupervised Methods. Electric Power Engineering Technology, 2020, 39(06): 159-165.\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 43,
    "chunk_index_in_section": 43,
    "total_chunks_in_section": 44,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 365,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s0_c0",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "网络化多智能体系统的主动容错预测控制方法研究\n\n近年来，网络化多智能体系统在控制领域中的研究热度持续增加。网络化多智能体系统是由多个能够通过网络交互的智能体组成的分布式自治系统。与单智能体和网络化控制系统相比，多智能体系统具有与环境交互、信息处理能力和解决复杂问题的能力，被广泛应用于实际场景。本研究针对网络化多智能体系统中随机网络时延、数据包丢失及智能体内部物理组件故障问题，设计了状态观测器和相应的控制律。\n\n本文的主要工作分为以下三个方面：\n\n1. 针对理想通信条件下的多智能体系统，设计了基于状态观测器的主动容错控制方法，实现对执行器故障和传感器故障的主动补偿。\n\n2. 研究了具有执行器故障和双通道随机通信约束的网络化多智能体系统的输出跟踪控制问题，提出了主动容错预测控制方法，并推导了闭环系统稳定性的充分必要条件。\n\n3. 针对具有执行器故障、传感器故障和双通道随机通信约束的网络化多智能体系统，提出了一种基于云的主动容错预测控制方法，通过设计观测器、云节点中的容错预测控制器和时延补偿器，实现了系统的输出跟踪控制。\n\n关键词：网络化多智能体系统，主动容错预测控制，执行器故障，传感器故障，随机通信约束\n\nActive Fault-Tolerant Predictive Control Methods for Networked Multi-Agent Systems",
    "section_title": "前言",
    "section_type": "introduction",
    "section_index": 0,
    "total_sections": 2,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 1,
    "has_previous": false,
    "has_next": false,
    "is_section_start": true,
    "is_section_end": true,
    "chunk_length": 591,
    "chunk_method": "hierarchical",
    "importance_weight": 0.8800000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c0",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "The research on networked multi-agent systems (NMASs) has been increasing in the control field. NMASs are distributed autonomous systems consisting of multiple agents that interact through the network. Compared with single-agent and networked control systems, MASs have the ability to interact with the environment, process information, and solve complex problems, and are widely used in practical sc",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 1,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 67,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 400,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9900000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c1",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "plex problems, and are widely used in practical scenarios. This study designs state observers and corresponding control laws for addressing random network delays, data packet loss, and internal physical component faults in NMASs.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 2,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 229,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c2",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "The main contributions of this paper are divided into three aspects:\n\n1. An active fault-tolerant control method based on state observer is designed for MASs under ideal communication conditions, achieving active compensation for actuator and sensor faults.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 3,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 257,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c3",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "2. The output tracking control problem of NMASs with actuator faults and two-channel random communication constraints is studied, and an active fault-tolerant predictive control method is proposed. The necessary and sufficient conditions for the stability of the closed-loop system are derived.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 4,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 294,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c4",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "3. A cloud-based active fault-tolerant predictive control method is proposed for NMASs with actuator, sensor faults, and two-channel random communication constraints. By designing observers, fault-tolerant predictive controllers in cloud nodes, and delay compensators, the output tracking control of the system is realized.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 5,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 323,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c5",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "Key words: Networked multi-agent systems, active fault-tolerant predictive control, actuator fault, sensor fault, random communication constraints\n\n第二章 双故障多智能体系统主动容错跟踪控制\n\n本章针对没有随机通信约束干扰的多智能体系统，提出了一种基于观测器的主动容错跟踪控制方法，用于主动补偿执行器故障和传感器故障。设计了状态观测器，可同时估计智能体状态、执行器故障和传感器故障，并根据估计值设计容错控制策略。理论分析表明，闭环系统的稳定性与执行器故障和传感器故障无关。数值仿真验证了该方法的有效性。\n\n第二章 双故障多智能体系统主动容错跟踪控制\n\n本节旨在设计一种主动补偿执行器故障和传感器故障的控制方法，以使每个智能体的输出跟踪误差趋于0。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 6,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 397,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c6",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "本节旨在设计一种主动补偿执行器故障和传感器故障的控制方法，以使每个智能体的输出跟踪误差趋于0。\n\n考虑一个多智能体系统，其中每个智能体i的状态、输入、控制输出和测量输出分别表示为x_i、u_i、y_i和z_i。执行器故障和传感器故障分别表示为f_i和g_i。系统矩阵为A_i、B_i、C_i、D_i和E_i。系统噪声和测量噪声分别表示为w_i和v_i。主智能体跟踪参考信号r并发送控制输出到各跟随者，以实现输出跟踪控制。\n\n通过构建误差增广系统，将输出跟踪控制问题转化为渐近稳定问题。设计了状态观测器来估计智能体状态、执行器故障和传感器故障。利用估计的跟踪误差代替实际跟踪误差，设计了增量式控制律来实现输出跟踪控制。\n\n稳定性分析表明，闭环系统的稳定性与执行器故障和传感器故障无关。数值仿真验证了该方法的有效性。\n\n第三章 执行器故障网络化多智能体系统主动容错预测控制",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 7,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 384,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c7",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "第三章 执行器故障网络化多智能体系统主动容错预测控制\n\n本章考虑网络化多智能体系统，提出了一种主动容错预测控制方法，以实现对执行器故障和随机通信约束的主动补偿。\n\n设计了状态观测器来估计执行器故障和智能体状态。利用状态观测器的估计值，设计了容错预测控制器来预测智能体的未来控制输入。同时设计了时延补偿器来补偿随机通信约束。\n\n理论分析表明，闭环系统的稳定性仅与系统矩阵和观测器增益有关。数值仿真和实验结果验证了该方法的有效性。\n\n清洗后的内容如下：",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 8,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 224,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c8",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "在本节中，重点分析了具有执行器故障的网络化多智能体系统的稳定性。在不损失一般性的情况下，使参考信号。定义智能体i故障增广系统的状态估计误差可以由下式表达：。根据式(3-6)、式(3-7)和式(3-16)，可以推导出故障增广系统的估计误差有如。由式(3-8)和式(3-9)，可得：。其中，由于执行器故障是慢时变的，因此有。由式(3-4)和式(3-11)进行迭代计算，可以得到领导者在k时刻的误差增广状态和误差增广状态的预测值：。由式(3-19)减去式(3-20)可以得到：。其中，。将式(3-4)减去式(3-12)，再结合式(3-21)，可以得到：。利用类似于式(3-21)和式(3-22)的推导，可以得到跟随者实际的误差增广状态和误差增广状态的预测值：。其中，。由式(3-23)可知，跟随者的输出增量预测有如下关系：",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 9,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 358,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c9",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "。其中，。由式(3-23)可知，跟随者的输出增量预测有如下关系：。结合式(3-4)、式(3-21)、式(3-22)、式(3-23)和式(3-24)将领导者的增量式控制律和跟随者的增量式控制律式(3-13)改写成如下所示：。其中，。将式(3-25)代入到式(3-4)中，可得领导者在时刻的实际误差增广状态和增量输出如下：。结合式(3-26)和式(3-28)，跟随者在时刻的实际误差增广状态式(3-4)可改写为：。结合式(3-17)、式(3-27)和式(3-29)，具有执行器故障的闭环网络化多智能体系统可由下式表示：。其中，矩阵中，存在。值得注意的是，闭环网络化多智能体系统式(3-30)中，系统矩阵是一个与无关的上三角矩阵。其中，与系统(3-30)的稳定性无关，故在本章中进行了省略。由此得出如下定理：定理3-1：当且仅当矩阵和的所有特征值均在单位圆内时，闭环网络化多智能体系统是稳定的\n。不难看出，矩阵和仅与系统矩阵和控制器参数有关，因此闭环网络化多智能体系统的稳定性与执行器故障和随机通信约束无关。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 10,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 451,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c10",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "第四章 双故障网络化多智能体系统云主动容错预测控制\n\n本章节针对具有双通道随机通信约束、执行器故障和传感器故障的网络化多智能体系统，提出了一种基于云的主动容错预测控制方法。该方法将各智能体的容错预测控制器置于云平台中，保证了智能体间的信息交换，并减少了智能体的计算负担。通过MATLAB仿真软件对网络化多智能体系统的主动容错预测控制、网络化预测控制和主动容错控制等3种不同控制方法进行了数值仿真。仿真结果表明，本章中所设计的基于云的主动容错预测控制方法可以实现与无随机通信约束影响的主动容错跟踪控制方法近似的效果。\n\n第五章 结论与展望",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 11,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 268,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c11",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "第五章 结论与展望\n\n本文针对一类线性离散异构网络化多智能体系统的输出跟踪控制问题进行了研究。考虑了具有双通道随机通信约束、执行器故障或传感器故障等不同情形下的控制方法。在不考虑随机通信约束影响的理想通信条件下，针对具有执行器故障和传感器故障的多智能体系统，提出了一种基于观测器的主动容错控制方法；其次，考虑智能体控制回路中具有前向通道随机通信约束和反馈通道随机通信约束，以及执行器故障的情况下，结合主动容错控制和网络化预测控制提出了一种网络化主动容错预测控制方法；最后，考虑系统可能同时具有双通道随机通信约束、执行器故障和传感器故障的情况下，设计了一种基于云的主动容错预测控制方法。本文的主要研究工作总结如下：",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 12,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 306,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c12",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "(1) 针对一类具有理想通信条件下的线性离散异构多智能体系统，实现了具有执行器故障和传感器故障情况下的输出跟踪控制。为了补偿执行器故障和传感器故障对系统的不利影响，提出了一种基于状态观测器的主动容错控制方法。最后，通过理论分析给出了闭环系统稳定性的充分必要条件，数值仿真证明了所设计控制方法的有效性。\n\n(2) 针对具有双通道随机通信约束和执行器故障的网络化多智能体系统，结合主动容错控制和网络化预测控制提出了一种网络化主动容错预测控制方法，实现了系统的输出跟踪控制。通过3组不同的数值仿真和实验结果对比验证了该方法的有效性。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 13,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 263,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c13",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "(3) 针对具有双通道随机通信约束、执行器故障和传感器故障的网络化多智能体系统。在第三章的基础上，考虑可能发生的传感器故障，结合云平台的优点，提出了一种基于云的网络化多智能体系统主动容错预测控制方法。在智能体的控制回路中设计了数据缓存器、容错预测控制器和时延补偿器。其中，各智能体的容错预测控制器均置于云平台中，保证了智能体间的信息传递，并减少计算负担。推导了闭环系统稳定性的充分必要条件，并通过3组不同的数值仿真结果进行对比，表明了所设计的控制方法可以减少双通道随机通信约束、执行器故障和传感器故障对网络化多智能体系统的不利影响，实现系统的输出跟踪控制。\n\n在未来的研究中，需要关注多输入多输出模型预测控制方法的实验验证，考虑多种类型的执行器故障或传感器故障，以及引入事件触发机制或量化控制输入减轻通信负担等方面。",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 14,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 358,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c14",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "Yu Z, Zhang Y, Jiang B, et al. Decentralized fractional-order backstepping fault-tolerant control of multi-UAVs against actuator faults and wind effects. Aerospace Science and Technology, 2020, 104: 105939.\n\nYan D H, Zhang W G, Chen H, et al. Robust control strategy for multi-UAVs system using MPC combined with Kalman-consensus filter and disturbance observer. ISA Transactions, 2023, 135: 35-51.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 15,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 398,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c15",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "Pang Z H, Xia C G, Sun J, et al. Active fault-tolerant predictive control of networked systems subject to actuator faults and random communication constraints. International Journal of Control, 2022, 95(9): 2357-2363.\n\nLiu G P. Networked predictive control for nonlinear information physical systems with time-varying communication constraints. Control Theory and Application, 2022, 39(1): 145-153.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 16,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 398,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c16",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "Pang Z H, Luo W C. Networked multi-agent predictive control based on observers. Control and Decision, 2021, 36(9): 2290-2296.\n\nYan Y M, Chen Z Y. Cooperative output regulation of linear discrete-time time-delay multi-agent systems by adaptive distributed observers. Neurocomputing, 2019, 331: 33-39.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 17,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 299,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c17",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "Wang H, You Y, Li W Q. Distributed output-feedback tracking for stochastic nonlinear multi-agent systems with time-varying delays. IEEE Access, 2022, 10: 69323-69332.\n\nGuo S, You R, Ahn C K. Adaptive consensus for multi-agent systems with switched nonlinear dynamics and switching directed topologies. IEEE Transactions on Control Systems Technology, 2023, 111: 1285-1299.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 18,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 372,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c18",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "Sakthivel R, Manickavalli S, Parivallal A, et al. Observer-based bipartite consensus for uncertain Markovian-jumping multi-agent systems with actuator saturation. European Journal of Control, 2021, 61: 13-23.\n\nCao W J, Zhang J H, Ren W. Leader-follower consensus of linear multi-agent systems with unknown external disturbances. Systems & Control Letters, 2015, 82: 64-70.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 19,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 372,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c19",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "Zhang D, Xu Z H, Srinivasan D, et al. Leader-follower consensus of multi-agent systems with energy constraints: A Markovian system approach. IEEE Transactions on Systems, 2017, 47(7): 1727-1736.\n\nWang H, Li H Z. Event-driven nonlinear multi-agent iterative learning control under data packet loss. Control Theory and Application, 2022, 39(9): 1688-1698.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 20,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 353,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c20",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "Lin N, Ling Q. Dynamic periodic event-triggered consensus protocols for linear multi-agent systems with network delay. IEEE Systems Journal, 2023, 17(1): 1204-1215.\n\nSu H, Wang Z, Song Z, et al. Event-triggered consensus of non-linear multi-agent systems with sampling data and time delay. IET Control Theory & Applications, 2017, 11(11): 1715-1725.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 21,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 349,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c21",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "Chen J, Chen B, Zeng Z. Synchronization in multiple neural networks with delay and disconnected switching topology via event-triggered impulsive control strategy. IEEE Transactions on Industrial Electronics, 2020, 68(3): 2491-2500.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 22,
    "chunk_index_in_section": 21,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 231,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c22",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "Chen X, Zhou Q X. Dynamic event-triggered consensus for multi-agent systems with time delay and switching topology. Fire Control and Command Control, 2022, 47(1): 43-49.\n\nLiu G P, Mu J X, Rees D, et al. Design and stability analysis of networked control systems with random communication time delay using the modified MPC. International Journal of Control, 2006, 79(4): 288-297.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 23,
    "chunk_index_in_section": 22,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 378,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c23",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "Pang Z H, Luo W C, Liu G P, et al. Observer-based incremental predictive control of networked multi-agent systems with random delays and packet dropouts. IEEE Transactions on Circuits and Systems II: Express Briefs, 2021, 68(1): 426-430.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 24,
    "chunk_index_in_section": 23,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 237,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c24",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "Pang Z, Du T, Zheng C, et al. Event-triggered cooperative predictive control for networked multi-agent systems with random delays and packet dropouts. Symmetry, 2022, 14(3): 541.\n\nYang R, Yu Y, Sun J, et al. Event-based networked predictive control for networked control systems subject to two-channel delays. Information Sciences, 2020, 524: 136-147.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 25,
    "chunk_index_in_section": 24,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 351,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c25",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "Pang Z H, Zhao X Y, Sun J, et al. Comparison of three data-driven networked predictive control methods for a class of nonlinear systems. IEEE/CAA Journal of Automatica Sinica, 2022, 9(9): 1714-1716.\n\nWang B, Shen Y, Zhang Y. Active fault-tolerant control for a quadrotor helicopter against actuator faults and model uncertainties. Aerospace Science and Technology, 2020, 99: 105745.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 26,
    "chunk_index_in_section": 25,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 382,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c26",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "Ding R, Cheng M, Jiang L, et al. Active fault-tolerant control for electro-hydraulic systems with an independent metering valve against valve faults. IEEE Transactions on Industrial Electronics, 2021, 68(8): 7221-7232.\n\nWang X. Active fault tolerant control for unmanned underwater vehicle with sensor faults. IEEE Transactions on Instrumentation and Measurement, 2020, 69(12): 9485-9495.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 27,
    "chunk_index_in_section": 26,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 388,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c27",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "Jiang J, Yu X. Fault-tolerant control systems: A comparative study between active and passive approaches. Annual Reviews in Control, 2012, 36(1): 60-72.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 28,
    "chunk_index_in_section": 27,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 152,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c28",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "Bahreini M, Zarei J, Razavi-Far R, et al. Robust finite–time stochastic stabilization and fault–tolerant control for uncertain networked control systems considering random delays and probabilistic actuator faults. Transactions of the Institute of Measurement and Control, 2019, 41(12): 3550-3561.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 29,
    "chunk_index_in_section": 28,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 296,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c29",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "Murugesan S, Liu Y C. Resilient memory event-triggered finite-time bounded for networked control systems with multiple cyber-attacks. American Control Conference, 2021, 2713-2719.\n\nLi H, Pan J, Zhang X, et al. Integral-based event-triggered fault estimation and impulsive fault-tolerant control for networked control systems applied to underwater vehicles. Neurocomputing, 2021, 442: 36-47.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 30,
    "chunk_index_in_section": 29,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 390,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c30",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "Lu H, Guo C, Hu Y, et al. Event-triggered stability analysis of Semi-Markovian jump networked control system with actuator faults and time-varying delay via Bessel–Legendre inequalities. Complexity, 2019, 2019: 1-16.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 31,
    "chunk_index_in_section": 30,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 216,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c31",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "Zhang Z, Liang H, Wu C, et al. Adaptive event-triggered output feedback fuzzy control for nonlinear networked systems with packet dropouts and actuator failure. IEEE Transactions on Fuzzy Systems, 2019, 27(9): 1793-1806.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 32,
    "chunk_index_in_section": 31,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 220,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c32",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "Zhang L, Chen M, Li T, et al. Adaptive event-triggered control for discrete-time networked control systems with actuator faults and nonlinearity. International Journal of Control, Automation and Systems, 2020, 18(11): 2842-2856.\n\nLi Y X, Yang G H. Adaptive asymptotic tracking control of uncertain nonlinear systems with input quantization and actuator faults. Automatica, 2016, 72: 177-185.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 33,
    "chunk_index_in_section": 32,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 391,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c33",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "Aslam M S, Qaisar I, Saleem M A. Quantized event-triggered feedback control under fuzzy system with time-varying delay and Actuator fault. Nonlinear Analysis: Hybrid Systems, 2020, 35: 100823.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 34,
    "chunk_index_in_section": 33,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 192,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c34",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "Chen J, Hua C, Guan X. Iterative learning model-free control for networked systems with dual-direction data dropouts and actuator faults. IEEE Transactions on Neural Networks and Learning Systems, 2021, 32(11): 5232-5240.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 35,
    "chunk_index_in_section": 34,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 221,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c35",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "Lu R, Peng H, Liu S, et al. Reliable filtering for fuzzy Markov stochastic systems with sensor failures and packet dropouts. IET Control Theory & Applications, 2017, 11(14): 2195-2203.\n\nWang H, Xie S, Zhou B, et al. Non-fragile robust filtering of Takagi-Sugeno fuzzy networked control systems with sensor failures. Sensors, 2019, 20(1):",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 36,
    "chunk_index_in_section": 35,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 337,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c36",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "Mathiyalagan K, Park J H, Sakthivel R. Robust reliable dissipative filtering for networked control systems with sensor failure. IET Signal Processing, 2014, 8(8): 809-822.\n\nHe X, Zhao L, Fei M, et al. Event-triggered robust H control for networked control system with sensor faults. Chinese Control and Decision Conference, 2016, 957-961.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 37,
    "chunk_index_in_section": 36,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 338,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c37",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "Li Z, Zhou C, Che W, et al. Data-based security fault tolerant iterative learning control under denial-of-service attacks. Actuators, 2022, 11(7): 178.\n\nTian E, Peng C, Yue D. Reliable control for networked control systems with probabilistic sensors and actuators faults. IET Control Theory & Applications, 2010, 4(8): 1478-88.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 38,
    "chunk_index_in_section": 37,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 327,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c38",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "Mahmoud G, Chen Y, Zhang L, et al. FHOSM tolerant control for networked control systems with disturbances and faults. International Journal of Control, Automation and Systems, 2021, 19(9): 3049-3061.\n\nYan B, Wu C, Shi P. Formation consensus for discrete-time heterogeneous multi-agent systems.\n\n---",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 39,
    "chunk_index_in_section": 38,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 298,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c39",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "---\n\n[47] Pang Z H, Xia C G, Zhai W F, et al. Networked active fault-tolerant predictive control for systems with random communication constraints and actuator/sensor faults [J]. IEEE Transactions on Circuits and Systems II: Express Briefs, 2022, 69(4): 2166-2170.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 40,
    "chunk_index_in_section": 39,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 264,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c40",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "[48] Fragkoulis D, Roux G, Dahhou B. Detection, isolation and identification of multiple actuator and sensor faults in nonlinear dynamic systems: Application to a waste water treatment process [J]. Applied Mathematical Modelling, 2011, 35(1): 522-543.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 41,
    "chunk_index_in_section": 40,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 251,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c41",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "[49] Hua Y, Dong X, Wang J, et al. Time-varying output formation tracking of heterogeneous linear multi-agent systems with multiple leaders and switching topologies [J]. Journal of the Franklin Institute, 2019, 356(1): 539-560.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 42,
    "chunk_index_in_section": 41,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 227,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c42",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "[50] Wang C, Wen C, Wang W, et al. Output-feedback adaptive consensus tracking control for a class of high-order nonlinear multi-agent systems [J]. International Journal of Robust and Nonlinear Control, 2017, 27(18): 4931-4948.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 43,
    "chunk_index_in_section": 42,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 227,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c43",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "[51] Li P, Xu S, Ma Q, et al. Leader-following rendezvous for uncertain Euler–Lagrange multi-agent systems by output feedback [J]. Journal of the Franklin Institute, 2017, 354(10): 4215-4230.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 44,
    "chunk_index_in_section": 43,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 191,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c44",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "[52] Li W, Zhang H, Ming Z, et al. Fully distributed event-triggered bipartite formation tracking control for heterogeneous multi-agent systems on signed digraph [J]. IEEE Transactions on Circuits and Systems II: Express Briefs, 2022, 69(4): 2181-2185.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 45,
    "chunk_index_in_section": 44,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 252,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c45",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "[53] Hu J, Sun X, He L. Formation tracking for nonlinear multi-agent systems with input and output quantization via adaptive output feedback control [J]. Journal of Systems Science and Complexity, 2019, 33(2): 401-425.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 46,
    "chunk_index_in_section": 45,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 218,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c46",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "[54] Zou A M, Li W. Fixed-time output-feedback consensus tracking control for second-order multiagent systems [J]. International Journal of Robust and Nonlinear Control, 2019, 29(13): 4419-4434.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 47,
    "chunk_index_in_section": 46,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 194,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c47",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "[55] Li Y, Hua C, Guan X. Distributed output feedback leader-following control for high-order nonlinear multiagent system using dynamic gain method [J]. IEEE Transactions on Cybernetics, 2020, 50(2): 640-649.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 48,
    "chunk_index_in_section": 47,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 208,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c48",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "[56] Chen G, Xiang H, Dai J. Distributed output-feedback finite-time tracking control of nonaffine nonlinear leader-follower multiagent systems [J]. International Journal of Robust and Nonlinear Control, 2020, 30(7): 2977-2998.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 49,
    "chunk_index_in_section": 48,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 227,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c49",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "[57] Wang R. Distributed time-varying output formation tracking control for general linear multi-Agent systems with multiple leaders and relative output-feedback [J]. IEEE Access, 2021, 9: 59586-59596.\n\n[58] Xu X, Li W, Wang M. Distributed output tracking of nonlinear multi-agent systems by linear sampled-data control [J]. Neurocomputing, 2021, 462: 238-246.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 50,
    "chunk_index_in_section": 49,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 360,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c50",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "[59] Li H, Li W, Gu J. Distributed output tracking control of nonlinear multi-agent systems with unknown time-varying powers [J]. International Journal of Control, 2021, 95(11): 2960-2971.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 51,
    "chunk_index_in_section": 50,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 188,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c51",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "[60] Cai X, Wang C, Wang G, et al. Distributed low-complexity output feedback tracking control for nonlinear multi-agent systems with unmodeled dynamics and prescribed performance [J]. International Journal of Systems Science, 2019, 50(6): 1229-1243.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 52,
    "chunk_index_in_section": 51,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 250,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c52",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "[61] Lu Y, Liao F, Liu H, et al. Cooperative preview tracking problem of discrete-time linear multi-agent systems: A distributed output regulation approach [J]. ISA Transactions, 2019, 85: 33-48.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 53,
    "chunk_index_in_section": 52,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 195,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c53",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "[62] Fu J, Wang J. Adaptive consensus tracking of high-order nonlinear multi-agent systems with directed communication graphs [J]. International Journal of Control, Automation and Systems, 2014, 12(5): 919-929.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 54,
    "chunk_index_in_section": 53,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 210,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c54",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "[63] Cai Y, Zhang H, Wang Y, et al. Adaptive bipartite fixed-time time-varying output formation-containment tracking of heterogeneous linear multiagent systems [J]. IEEE Transactions on Neural Networks Learning Systems, 2022, 33(9): 4688-4698.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 55,
    "chunk_index_in_section": 54,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 243,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c55",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "[64] Hua Y, Dong X, Wang J, et al. Time-varying output formation tracking of heterogeneous linear multi-agent systems with multiple leaders and switching topologies [J]. Journal of the Franklin Institute, 2019, 356(1): 539-560.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 56,
    "chunk_index_in_section": 55,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 227,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c56",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "[65] Luo W, Lu P, Du C, et al. Cooperative output tracking control of heterogeneous multi-agent systems with random communication constraints: an observer-based predictive control approach [J]. IEEE Transactions on Circuits and Systems II: Express Briefs, 2022, 69(3): 1139-1143.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 57,
    "chunk_index_in_section": 56,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 279,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c57",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "[66] Guo Z, Xue H, Pan Y. Neural networks-based adaptive tracking control of multi-agent systems with output-constrained and unknown hysteresis [J]. Neurocomputing, 2021, 458: 24-32.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 58,
    "chunk_index_in_section": 57,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 182,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c58",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "[67] Cui B, Xia Y, Liu K, et al. Cooperative tracking control for general linear multiagent systems with directed intermittent communications: An artificial delay approach [J]. International Journal of Robust and Nonlinear Control, 2019, 29(10): 3063-3077.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 59,
    "chunk_index_in_section": 58,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 256,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c59",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "[68] Duan Z, Zhang X, Kong L, et al. Tracking control for nonlinear time‐delay multiagent systems with input saturation [J]. International Journal of Adaptive Control and Signal Processing, 2019, 34(2): 127-140.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 60,
    "chunk_index_in_section": 59,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 211,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c60",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "[69] Wang Z, Liu Q, Wang D, et al. Event-triggered tracking control of heterogeneous multiagent systems based on two kinds of observers with asymmetric delay [J]. International Journal of Robust and Nonlinear Control, 2019, 29(10): 2862-2876.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 61,
    "chunk_index_in_section": 60,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 242,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c61",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "[70] Cui B, Xia Y, Liu K, et al. Cooperative tracking control for general linear multiagent systems with directed intermittent communications: An artificial delay approach [J]. International Journal of Robust and Nonlinear Control, 2019, 29(10): 3063-3077.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 62,
    "chunk_index_in_section": 61,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 256,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c62",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "[71] Li K, Hua C-C, You X, et al. Output feedback-based consensus control for nonlinear time delay multiagent systems [J]. Automatica, 2020, 111: 108669.\n\n[72] Fienego G, Lui D G, Petrillo A, et al. Distributed robust output consensus for linear multi-agent systems with input time-varying delays and parameter uncertainties [J]. IET Control Theory & Applications, 2019, 13(2): 203-212.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 63,
    "chunk_index_in_section": 62,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 386,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c63",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "[73] Jiang Y L, Liu J C, Wang S Q. Cooperative output feedback tracking control for multi-agent consensus with time-varying delays and switching topology[J]. Transactions of the Institute of Measurement and Control, 2015, 37(4): 550-559.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 64,
    "chunk_index_in_section": 63,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 237,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c64",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "[74] Liu G-P, Xia Y, Rees D, et al. Design and stability criteria of networked predictive control systems with random network delay in the feedback channel [J]. IEEE Transactions on Systems, Man and Cybernetics, Part C (Applications and Reviews), 2007, 37(2): 173-184.\n\n[75] 张天勇. 网络化多智能体跟踪和编队预测控制的设计与实现[D]. 哈尔滨工业大学, 2017.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 65,
    "chunk_index_in_section": 64,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 321,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c65",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "[76] Hinrichsen, D. and Pritchard, A. J. Mathematical systems theory I: modelling, state space analysis, stability and robustness[M]. Singapore: Springer, 2011.\n\n[77] 卢行远, 侯忠生. 基于改进卡尔曼滤波器的扰动抑制无模型自适应控制方案[J]. 控制理论与应用, 2022, 39(7): 1211-1218.\n\n[78] Liu G-P. Predictive control of networked multiagent systems via cloud computing[J]. IEEE Transactions on Cybernetics, 2017: 47(8): 1852-1859.",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 66,
    "chunk_index_in_section": 65,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 387,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通_s1_c66",
    "source_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
    "text": "[79] Liu G-P. Coordinated control of networked multiagent systems via distributed cloud computing using multistep state predictors[J]. IEEE Transactions on Cybernetics, 2022, 52(2): 810-820.\n\n---",
    "section_title": "Abstract",
    "section_type": "abstract",
    "section_index": 1,
    "total_sections": 2,
    "chunk_index": 67,
    "chunk_index_in_section": 66,
    "total_chunks_in_section": 67,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 195,
    "chunk_method": "hierarchical",
    "importance_weight": 0.9450000000000001,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "采用音质特征和VLAD编码的新冠肺炎检测算法_张昊然_s0_c0",
    "source_id": "采用音质特征和VLAD编码的新冠肺炎检测算法_张昊然",
    "text": "---\n\n采用音质特征和VLAD编码的新冠肺炎检测算法\n\n张昊然 韩易辰 谭咏梅 李雅\n\n（北京邮电大学，人工智能学院，北京100876）\n\n摘要：2020年，世界卫生组织宣布COVID-19疫情为大流行病。本研究通过语音信号分析技术寻找感染COVID-19的语音信号特征，利用咳嗽声片段和语音片段进行自动判断。在INTEＲSPEECH 2021 ComParE竞赛数据集和baseline基础上，首先使用语音端点检测技术增广数据集，其次加入语音质量特征，有效提升了baseline结果，证明了语音质量特征在COVID-19自动语音检测任务上的有效性。引入局部聚合描述子向量（VLAD）对低级别特征进行编码，在小字典规模下有效提升了系统分类性能。最后，通过多算法融合进一步提升分类效果，在两个子任务验证集上UAＲ分别达到73.9%和77.2%。\n\n关键词：COVID-19自动检测；语音切分；语音质量特征；局部聚合描述子向量；情感识别\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 12,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 422,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "采用音质特征和VLAD编码的新冠肺炎检测算法_张昊然_s0_c1",
    "source_id": "采用音质特征和VLAD编码的新冠肺炎检测算法_张昊然",
    "text": "关键词：COVID-19自动检测；语音切分；语音质量特征；局部聚合描述子向量；情感识别\n\n---\n\n2020年3月11日，世界卫生组织宣布COVID-19疫情为大流行病。疫情快速国际化，改变生活多方面。医生和科学家寻找COVID-19线索/指标，希望快速检测控制疫情。除肺部损伤识别外，常见症状包括发烧、咳嗽、肺炎、喉咙痛。研究发现发烧、咳嗽、疲劳、喉咙痛和呼吸短促是重要感染特征，咳嗽和喉咙痛普遍存在。这促使使用语音信号处理技术寻找此类特征，提供可靠快速的COVID-19检测方法。\n\n基于语音的COVID-19识别研究已取得进展。AI4COVID-19项目通过智能手机应用程序收集咳嗽声进行感染筛查。剑桥COVID-19声音数据库收集朗读语音和咳嗽声。学者们提出实时机器人，集成语音识别、温度测量、关键词检测、咳嗽检测等功能。INTEＲSPEECH 2021计算语言学挑战赛组织了公开挑战赛，加速研究进展。\n\n本研究对挑战赛的贡献有三方面：首先，使用语音端点检测进行数据增广；其次，引入语音质量特征；第三，对基线提取的低水平特征进行VLAD编码。本文组织结构如下：第2节介绍方法和系统框架；第3节提出实验对比和分析；第4节总结发现。\n\n2 系统框架",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 526,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "采用音质特征和VLAD编码的新冠肺炎检测算法_张昊然_s0_c2",
    "source_id": "采用音质特征和VLAD编码的新冠肺炎检测算法_张昊然",
    "text": "2 系统框架\n\n图1展示了算法流程框架。输入原始音频文件，进行数据增广，提取特征，包括VQ特征和VLAD编码。将特征编码输入线性SVM分类器，得到分类结果，通过投票机制融合，得到最终分类结果。\n\n2.1 数据增广\n\nCCS和CSS任务训练集和测试集数据分布不均，采用等时间间隔和基于静音片段的切分方法进行数据增广。\n\n2.2 baseline算法\n\nBaseline使用openSMILE、openXBOW、DeepSpectrum、auDeep和End2You工具进行特征提取，输入到线性SVM中进行分类。\n\n2.3 音质特征\n\n引入音质特征描述发声器官变化，包括基频抖动、振幅抖动、开商、准开商、H1-H2、AQ、NAQ和HＲF等。\n\n---\n\n[注：由于原文中包含图表和公式，这些内容在文本清洗中无法保留，建议在实际论文中查看相关图表和公式。]",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 375,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "采用音质特征和VLAD编码的新冠肺炎检测算法_张昊然_s0_c3",
    "source_id": "采用音质特征和VLAD编码的新冠肺炎检测算法_张昊然",
    "text": "---\n\n[注：由于原文中包含图表和公式，这些内容在文本清洗中无法保留，建议在实际论文中查看相关图表和公式。]\n\n本文提出了基于语音内容分析的新冠肺炎自动识别方法，并在INTEＲSPEECH 2021 ComParE竞赛提供的数据集上进行了验证。针对新冠肺炎数据量小的问题，本文通过语音端点检测方法对数据进行切分增广，提升了小数据集上的分类效果。此外，本文引入语音质量特征，用于对发声器官例如喉、声带等器官的变化进行建模，补充openSMILE所提的LLDs对音质特征提取的不足。受到ComParE竞赛基线系统中的BoAW编码的启发，本文还引入VLAD编码对低水平特征进行更深层次的描述，使得在字典规模较小时，就能获得更好得分类效果，证实了VLAD编码在语音特征编码中的有效性。引入VLAD编码能够在获得更快系统响应速度的同时占用相对较小的内存空间，便于检测系统在各种功能移动终端上部署，并能快速计算得到新冠检测结果。但是字典的具体大小并不能提前确定，需要进行多次尝试确定最佳取值。在未来的研究中，可以从深度学习的角度引入新的特征提取网络、分类模型对基于语音的新冠肺炎识别检测任务进行提升。\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 500,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "采用音质特征和VLAD编码的新冠肺炎检测算法_张昊然_s0_c4",
    "source_id": "采用音质特征和VLAD编码的新冠肺炎检测算法_张昊然",
    "text": "---\n\nLAGUAＲTA J, PUIG F H, SUBIＲANA B. Covid-19 artificial intelligence diagnosis using only cough recordings. IEEE Open Journal of Engineering in Medicine and Biology, 2020: 275-281.\n\nSCHULLEＲB W, BATLINEＲA, BEＲGLEＲC, et al. The INTEＲSPEECH 2021 computational paralinguistics challenge: COVID-19 cough, COVID-19 speech, escalation & primates. arXiv Preprint arXiv:2102.13468, 2021.\n\nFLOＲIAN E, MAＲTIN W, BJＲN S. Opensmile: the Munich versatile and fast open-source audio feature extractor. Proceedings of the 18th ACM International Conference on Multimedia, 2010: 1459-1462.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 576,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "采用音质特征和VLAD编码的新冠肺炎检测算法_张昊然_s0_c5",
    "source_id": "采用音质特征和VLAD编码的新冠肺炎检测算法_张昊然",
    "text": "MAXIMILIAN S, BJＲN S. openXBOW-Introducing the Passau open-source crossmodal bag-of-words toolkit. The Journal of Machine Learning Ｒesearch, October 2017, 18(96): 1-5.\n\nAMIＲIPAＲIAN S, GEＲCZUK M, OTTL S, et al. Snore sound classification using image-based deep spectrum features. Proceedings INTEＲSPEECH 2017, 18th Annual Conference of the International Speech Communication Association, (Stockholm, Sweden), ISCA, August 2017: 3512-3516.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 5,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 438,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "采用音质特征和VLAD编码的新冠肺炎检测算法_张昊然_s0_c6",
    "source_id": "采用音质特征和VLAD编码的新冠肺炎检测算法_张昊然",
    "text": "AMIＲIPAＲIAN S, FＲEITAG M, CUMMINS N, et al. Sequence to sequence autoencoders for unsupervised representation learning from audio. Proceedings of the Detection and Classification of Acoustic Scenes and E-events 2017 Workshop, 2017: 17-21.\n\nTZIＲAKIS P, ZAFEIＲIOU S, SCHULLEＲB W. End2You—the Imperial toolkit for multimodal profiling by end-to-end learning. arXiv Preprint arXiv: 1802.01115, 2018.\n\nMAIDMENT J A. The phonetic description of voice quality. Journal of the International Phonetic Association, 1981, 11(2): 78-84.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 6,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 524,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "采用音质特征和VLAD编码的新冠肺炎检测算法_张昊然_s0_c7",
    "source_id": "采用音质特征和VLAD编码的新冠肺炎检测算法_张昊然",
    "text": "ZHU Jianqing, LIN Luxin, SHEN Fei, et al. Fabric retrieval algorithm using SIFT and VLAD feature coding. Journal of Signal Processing, 2019, 35(10): 1725-1731.\n\nBALAJI B, OＲUGANTI V ＲM. Multi-level feature fusion for group-level emotion recognition. Proceedings of the 19th ACM International Conference on Multimodal Interaction, 2017: 583-586.\n\nTZIＲAKIS P, ZHANG Jiehao, SCHULLEＲB W. End-to-end speech emotion recognition using deep neural networks. 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018: 5089-5093.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 7,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 561,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "采用音质特征和VLAD编码的新冠肺炎检测算法_张昊然_s0_c8",
    "source_id": "采用音质特征和VLAD编码的新冠肺炎检测算法_张昊然",
    "text": "MATTHEW G, PETEＲL. Phonation types: a cross-linguistic overview. Journal of Phonetics, 2001, 29(4): 383-406.\n\nALKU P, BCKSTＲM T, VILKMAN E. Normalized amplitude quotient for parametrization of the glottal flow. The Journal of the Acoustical Society of America, 2002, 112(2): 701-710.\n\nCHILDEＲS D G, LEE C K. Vocal quality factors: analysis, synthesis, and perception. The Journal of the Acoustical Society of America, 1991, 90(5): 2394-2410.\n\nKANE J. Tools for analysing the voice: developments in glottal source and quality analysis. Dublin, Ireland, Trinity College, 2012.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 8,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 576,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "采用音质特征和VLAD编码的新冠肺炎检测算法_张昊然_s0_c9",
    "source_id": "采用音质特征和VLAD编码的新冠肺炎检测算法_张昊然",
    "text": "CHＲISTEＲG, AILBHE N C. The role of voice quality in communicating emotion, mood and attitude. Speech Communication, 2003, 40(1): 189-212.\n\nLI Ya, NICK C, TAO Jianhua. Voice quality: not only about “you” but also about “your interlocutor”. 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015: 4739-4743.\n\nCAMPBELL N. Listening between the lines: a study of paralinguistic information carried by tone-of-voice. International Symposium on Tonal Aspects of Languages: With Emphasis on Tone Languages, 2004.\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 9,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 548,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "采用音质特征和VLAD编码的新冠肺炎检测算法_张昊然_s0_c10",
    "source_id": "采用音质特征和VLAD编码的新冠肺炎检测算法_张昊然",
    "text": "Authors' information:\n- Zhang Haoran, male, born in 1999, Nantong, Jiangsu. Graduate student at Beijing University of Posts and Telecommunications, majoring in speech language processing and machine learning. E-mail: zhanghaoran@bupt.edu.cn\n- Han Yichen, male, born in 1997, Baotou, Inner Mongolia. Graduate student at Beijing University of Posts and Telecommunications, majoring in speech synthesis and affective computing. E-mail: adelacvgaoiro@bupt.edu.cn",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 10,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 458,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "采用音质特征和VLAD编码的新冠肺炎检测算法_张昊然_s0_c11",
    "source_id": "采用音质特征和VLAD编码的新冠肺炎检测算法_张昊然",
    "text": "- Tan Yongmei, female, born in 1975, Lijiang, Yunnan. Associate Professor at Beijing University of Posts and Telecommunications, Ph.D., majoring in natural language processing and machine learning. E-mail: ymtan@bupt.edu.cn\n- Li Ya (corresponding author), female, born in 1984, Xi'an, Shaanxi. Associate Professor at Beijing University of Posts and Telecommunications, Ph.D., majoring in speech interaction and multimodal affective computing. E-mail: yli01@bupt.edu.cn\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 11,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 12,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 472,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c0",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "指称表达理解(Refering Expression Comprehension, REC)旨在根据自然语言描述定位图像中的目标物体，是图文多模态领域的重要研究方向。指称表达通常包含物体的外观、属性、位置、与其他物体的关系、上下文场景知识等关键信息，因此文本具有多样性和复杂性。本文聚焦于复杂文本场景下的REC研究。\n\n当前主流方法在跨模态推理时通常简单拼接视觉和文本特征，导致文本信息被视觉信息淹没，难以有效提取关键线索。同时，当前方法在处理复杂场景知识时效果较差。为解决这些问题，本文提出：\n\n1. 语言引导的REC推理网络(LGRNet)，通过分离文本和图像模态，引入跨注意力机制，逐步引导图像模态的对齐，提高跨模态推理效果。\n\n2. 基于大语言模型的场景知识简化方法，设计指令模板过滤无关描述，简化场景知识。\n\n3. 场景知识推理网络(SKRNet)，分别提取指称表达和场景知识特征，进行联合推理。\n\n在多个公开数据集上验证了所提方法的可行性和有效性。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 0,
    "chunk_index_in_section": 0,
    "total_chunks_in_section": 54,
    "has_previous": false,
    "has_next": true,
    "is_section_start": true,
    "is_section_end": false,
    "chunk_length": 430,
    "chunk_method": "hierarchical",
    "importance_weight": 0.77,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c1",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "3. 场景知识推理网络(SKRNet)，分别提取指称表达和场景知识特征，进行联合推理。\n\n在多个公开数据集上验证了所提方法的可行性和有效性。\n\n物体的图片中识别和定位出文本指定的目标物体，比目标检测这一传统计算机视觉任务更具挑战性。指称表达是指描述场景中特定物体的自然语言表达，人们经常使用指称表达区分场景中的多个物体，如“穿着蓝色衣服的人”、“左边的那只狗”。指称表达理解任务要求模型根据自由多变的指称表达定位图片中的物体。此外，还会包含丰富的场景知识，进一步提高指称表达的文本长度。这种理解能力对于人工智能在现实世界中的应用至关重要，因为它模拟了人类如何通过语言和视觉信息进行交流和理解。从应用角度来看，在VR领域，能够理解自然语言描述并将其与虚拟环境中的对象关联起来，对于提升用户体验和交互效率至关重要，REC技术可以帮助用户更自然地与虚拟世界互动。在机器人导航中，理解周围环境的自然语言描述对于任务执行是有必要的，REC技术可以帮助此类系统更好地理解人类指令和环境信息。另外，对于视障人士，可以根据指称表达理解技术开发出辅助工具，帮助他们通过语言描述识别和理解周围环境，提高生活质量。因此，开展面向复杂文本的指称表达理解的研究不仅能推动人工智能领域的技术进步，也能为多个行业和日常生活带来实际的应用价值。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 1,
    "chunk_index_in_section": 1,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 556,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c2",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "在指称表达理解任务中，当前基于Transformer的REC模型存在文本信息淹没问题，导致对文本特征利用不足。此外，REC模型缺乏有效的针对复杂场景知识的推理方案。为解决这些问题，本文提出了语言引导的指称表达理解网络LGNET，通过跨注意力机制分离图文模态，避免文本信息淹没，并从多个角度引入不同抽象层次的文本特征用于引导推理过程。同时，针对基于场景知识的REC任务，本文从数据和模型两个角度提出了简化场景知识的方案，并提出了面向场景知识的指称表达理解模型。通过在多个基准数据集上的实验分析，证明了本文提出方案的有效性。\n\n在本文中，我们主要介绍了Transformer架构及其在自然语言处理和计算机视觉中的应用。首先，我们详细阐述了Transformer的编码器和解码器结构，包括多头注意力机制、位置编码和前馈网络等组成部分。接着，我们讨论了基于Transformer的预训练语言模型BERT和GPT，它们通过在大规模语料上进行预训练，学习到丰富的语言表示，并在下游任务中进行微调，显著提升了自然语言处理的效果。此外，我们还介绍了视觉Transformer(ViT)和改进的Swin Transformer，它们利用Transformer架构提取图像特征，并在某些视觉任务上取得了优异的性能。最后，我们还讨论了Transformer在目标检测和实例分割等密集型视觉任务中的应用。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 2,
    "chunk_index_in_section": 2,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 590,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c3",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "根据指示，以下是清洗后的学术论文第5部分内容：\n\n---\n\n总体复杂度为4/iwC^2 + 2(iw)^2C。而在W-MSA中，窗口大小为MxM，则窗口数为4。窗口内部复杂度可根据上述MSA复杂度计算，得到最终结果为4hwC^2 + 2M^2。可以发现MSA复杂度为/iw的平方级别，而W-MSA为hv的线性级别(M为常数)。因此使用Swin Transformer可以处理更高分辨率的图片，提高了此类视觉特征提取器的适用范围。\n\n此外，为了实现窗口间的注意力，Swin Transformer通过移动窗口注意力(Shifted window MSA, SW-MSA)实现跨窗口交互。如图2-7所示，通过对原来的窗口划分进行左右、上下的循环移位实现窗口间的信息交互，同时设计了高效的掩码机制避免因移位导致原来不相邻的图片区域之间计算注意力。同时参考CNN的方案，Swin Transformer中的Patch Merging操作逐步对图片特征进行降采样，进一步提高模型的全局建模能力并降低计算复杂度。\n\n后续在Swin Transformer的基础上发展的Swin V2在跨窗口连接，多尺度特征融合等多个角度进行了改进，进一步提升了模型的性能。综上所述，相比传统的卷积神经网络，如今Transformer架构在图像特征提取上也具备强大的竞争力，并逐渐成为视觉表征的一个更佳的方案。\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 3,
    "chunk_index_in_section": 3,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 595,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c4",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "---\n\n以上内容已去除页眉、页脚、页码、重复信息和乱码，保留了核心学术内容、技术术语和数据、原有逻辑结构、公式和图表说明。\n\n文本特征扩展器（Textual Feature Extender, TFE）从三个方面扩展了文本特征。首先，模型设计了一种基于文本特征的新型坐标嵌入（Coordinate Embedding），并将其整合到预测标记中，以促进其捕获与语言相关的视觉特征。其次，使用提取的文本特征交替进行文本引导的跨模态对齐（Text-guided Cross-modal Alignment, TCA）和融合（Fusion, TCF）。第三，设计了一种新型的跨模态损失函数，以增强指称表达与可学习预测标记之间的跨模态对齐。\n\nLG-R-NET的整体模型框架主要包括文本提取模块、视觉提取模块、文本特征扩展模块、跨模态推理与定位框预测模块，以及模型的损失函数设计。文本提取模块使用BERT作为文本特征提取器。视觉提取模块使用SwimTransformer提取图像特征。文本特征扩展模块生成坐标向量、词向量和句子向量。跨模态推理与定位框预测模块包括TCA和TCF组件，用于跨模态对齐和融合。最后，损失函数包括预测框的回归损失和跨模态对齐损失。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 4,
    "chunk_index_in_section": 4,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 524,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c5",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "实验部分，首先使用Visual Genome数据集对LG-R-NET模型进行预训练，然后在基准数据集上微调。实验结果显示，LG-R-NET在多个基准数据集上取得了state-of-the-art的性能。\n\n一个多模态双流模型，分别处理视觉和文本输入，并通过协同注意力变换器层进行交互。UNITER提出了通用的图像-文本表示学习，通过在四个图像-文本数据集和四个预训练任务上的大规模预训练学习。MDERT是基于DETR的端到端调制多模态检测器，通过基于查询的变换器解码器来检测所有对象。OFA将不同的视觉-语言任务统一到一个Sequence-to-sequence框架中，并收集大量数据来预训练他们的框架。Shikra使多模态大型语言模型能够处理空间坐标的输入和输出，并将其应用于指代对话。ONE-PEACE构建了一个面向任意模态的通用表示模型，可以无缝地在视觉、音频和语言模态之间对齐和集成。mPLUG引入了一种高效的视觉-语言架构，具有新颖的跨模态跳跃连接，并在大规模图像-文本对上进行了端到端的预训练，同时具有区分性和生成性目标。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 5,
    "chunk_index_in_section": 5,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 467,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c6",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "表3-1中展示了LG-R-NET在三个基准数据集(RefCOCO、RefCOCO+和RefCOCOg)上的实验结果。可以发现LG-R-NET在三个数据集上均取得了优越的性能。与最佳的基于提议的方法Ref-NMS和基于锚框的方法LBLYL-Net相比，LG-R-NET在各方面都取得了显著的优势。对于RefCOCO和RefCOCOg数据集，test B上的改进尤为显著。这表明LG-R-NET在涉及多样化的指称对象时实现了更好的效果。对于RefCOCOg，指称表达更为复杂，LG-R-NET在val-g上仍然超出LBLYL-Net 12.78%，在val-u、test-u上平均超越Ref-NMS 6.34%。这表明模型在面对复杂的引用表达时能够进行更准确的推理。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 6,
    "chunk_index_in_section": 6,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 331,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c7",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "此外，可以观察到LG-R-NET在表3-1中仍然优于大部分最近提出的基于Transformer的方法。与TransVGP相比，LG-R-NET在使用resnet-101作为backbone时在RefCOCO、RefCOCO+和RefCOCOg上分别实现了高达3.70%、8.08%和6.50%的绝对提升。这展示了LG-R-NET相较于拼接注意力的跨模态推理方法的优越性。与表现最好的VLTVG相比，LG-R-NET在相同的骨干网络上获得了可比较的结果，进一步，当双方都使用更好的视觉骨干网络swin-S时，本文的方法全面超越了VLTVG，这意味着LG-R-NET对更强骨干网络适应性更好。此外，与那些具有更强视觉骨干网络的基线模型，如ViT-B和CLIP-B相比，LG-R-NET取得了有竞争力的结果。尤其当面对指称表达更长、更复杂的RefCOCOg数据集时，LG-R-NET取得了显著的性能优势。这进一步证明了LG-R-NET在面对复杂文本时的推理性能。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 7,
    "chunk_index_in_section": 7,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 428,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c8",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "表3-2中展示了LG-R-NET在ReferrltGame和Flickr30K Entities上的实验结果。可以发现LG-R-NET明显优于基于提议和基于锚框的方法。与最佳的基于Transformer的方法相比，LG-R-NET同样展现出有竞争力的结果。值得注意的是，ReferrltGame和Flickr30K Entities中的指称表达主要是简单的名词性短语，不适合展示LG-R-NET面对复杂文本的推理能力。因此，在这两个数据集上获得的提升小于在RefCOCOg上的改进。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 8,
    "chunk_index_in_section": 8,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 241,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c9",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "表3-3中展示了LG-R-NET模型在预训练设置下的性能对比。一类包括MDERT和RefTR，它们的网络架构是专为检测或分割任务设计的，并只在这些任务的数据集如VG、MS-COCO和Flickr 30K上进行了预训练。由于模型框架的限制，可用的预训练数据集和下游任务相对较少。其他基线属于第二类，如OFA和ONE-PEACE。这些方法将不同类型的多模态任务统一为序列到序列任务，并应用统一的框架来处理它们。这样，它们可以从VQA、图像描述和REC等多种不同的多模态任务中收集数据集。至于REC数据集，它们会将边界框标签转换为类似的。因此，这些方法可以利用更多的数据集进行预训练，如表3-3所示。本文的预训练LG-R-NET属于第一类，因为它仅适用于REC。与RefTR和MDERT相比，预训练LG-R-NET在相同甚至更少的预训练数据上实现了更好的性能，这展示了它的可扩展性。与第二类方法如ONE-PEACE或mPLUG相比，本文的方法性能略差。可见，由多种数据集预训练的通用跨模态框架拥有更高的性能上限。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 9,
    "chunk_index_in_section": 9,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 453,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c10",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "综上所述，上述实验结果表明了LG-R-NET相比之前的基线方法的优势，尤其是面对具备复杂指称表达的RefCOCOg数据集，LG-R-NET展现出显著的性能优势，说明其面对复杂指称表达时优越的跨模态推理效果。在预训练设置下，LG-R-NET超越了相同设置下的基线，证明了LG-R-NET的可扩展性。\n\n在三个数据集中，空间位置词的指称表达比例存在明显差异，这有助于观察CE的消融效果。结果显示，在RefCOCO+中，由于空间指称表达比例不高，CE对LGR-NET的提升不明显。然而，在RefCOCO和RefCOCOg上，CE对LGR-NET的性能提升显著，在RefCOCO的testB中，CE带来了1.77%的性能提升。这充分说明了CE在捕捉指称表达中关于物体空间位置描述的有效性。\n\n此外，对坐标编码CE融入预测token的方式也进行了消融实验，探索了不同方式的优劣。包括加和(add)、乘积(multiply)、拼接(concatenation)三种方式，其中拼接操作之后会接一个线性层对齐维度。结果显示，使用加和操作向预测token中融入坐标编码的效果最佳。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 10,
    "chunk_index_in_section": 10,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 481,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c11",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "跨模态损失CL采用的是对比学习的形式，由于对比学习效果受训练过程中的批次大小(batch size, bs)影响明显，对此进行了消融实验。结果显示，当增大batch size时，CL带来的提升更大。当batch size为16时，CL在两个划分下只带来0.12%的平均提升，而当batch size提升至128时，CL带来的性能提升达到1.19%。\n\n跨模态推理模块中的TCA和TCF模块组合起来可以视作一个拥有自定义输入的，标准的TransformerDecoder模块。然而，正是因为这样的自定义输入方式，本文将其分为TCA和TCF模块，即文本模态以TCF中的cross attention的key和value的形式输入跨模态推理模块中，而图像模态输入TCA中。这样在堆叠模块时，文本特征能反复输入，起到引导跨模态推理的效果，同时这个过程中经过TCA中的自注意力机制能逐渐调整预测token与图像特征之间的注意力分布，与指称表达在语义层面上实现对齐，逐渐注意到指称物体。\n\n本模块通过交换输入的图文模态进行关于TCA和TCF的消融实验，即交换跨模态推理模块输入的图像文本，称之为LGR-NET(text-img)。结果显示，交换输入的图文模态后，模型在两个划分上的性能都明显下降，说明了TCA和TCF的有效性。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 11,
    "chunk_index_in_section": 11,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 557,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c12",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "对跨模态推理模块中TCA和TCF堆叠层数进行消融。结果显示，从1层到6层的过程中，模型在两个数据划分上都得到一致且明显的提升，当堆叠至8层时，一方面在RefCOCOg test-u上提升不够明显，一方面在RefCOCOg val-u上不升反降。考虑到堆叠层数的同时会提高模型的复杂度，因此LGR-NET最终选择堆叠6层TCA和TCF组件。\n\nLGR-NET的损失不仅包含之前方法常用的框回归损失，同时还有跨模态损失，通过权重因子A进行平衡。对权重因子进行消融，结果显示，当权重A=2时模型性能达到最佳。一方面，当A过小时，跨模态损失权重太小，对齐约束不足。另一方面，当A过大时，跨模态损失权重过大，导致模型对预测框的监督约束不充分。两者都会降低模型的收敛性能。消融结果说明了合理的权重因子可以促进跨模态损失的效果。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 12,
    "chunk_index_in_section": 12,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 357,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c13",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "本章开展了基于语言引导的指称表达理解研究工作并提出了语言引导的推理网络（LGR-NET）。首先，本章从解决先前方法出现的文本信息淹没问题和跨模态推理过程中语言特征利用不足的问题出发，给出了LGR-NET模型的设计动机和解决思路。接下来，本章详细介绍了LGR-NET各个组件的设计细节，该模型核心在于扩展语言特征并充分利用其进行跨模态推理的引导。包括建模指称表达中存在的关于目标物体在图片中的空间位置信息；结合跨注意机制避免文本信息的淹没；优化训练模型的损失函数，增强图文对齐约束。最后在实验验证部分，本章通过全面的定量性能对比、详尽的组件有效性消融分析、关键组件的可视化展示、定性分析全方位地验证了所提方法的可行性和有效性。\n\n清洗后的内容：\n\n高质量的场景数据可以有效降低模型的推理难度，提高模型性能。\n\n表4-2对生成数据的评估分类和说明：\n\n评分：\n1. 简化的场景知识完全不包含原始场景知识中对目标物体的描述。\n2. 简化的场景知识只包含原始场景知识对目标物体的部分描述。\n3. 简化的场景知识包含原始场景知识对目标物体的所有描述，同时也包含其他无关的描述。\n4. 简化的场景知识包含原始场景知识对目标物体所有描述，并且不包含其他无关的描述。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 13,
    "chunk_index_in_section": 13,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 525,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c14",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "经过GPT4评估后的数据，其中评分为1和2的数据被视作“低质数据”，对其重新调用4.2.1小节的标注过程进行重标，在这个过程中结合人工抽样核验确保数据质量。综合考虑API调用过程中的时间成本和GPT4的API价格，这个过程经历了两轮，流程如图4-5所示。\n\n最终结合少量的人工核验和标注，评分为3和4的数据比例分别为12.03%和87.97%，部分例子如表4-3所示。\n\n表4-3 生成的简化场景知识及其评分样例：\n\n评分3的样例：\nThe man wearing a pinkuit and that in the middle of the image is Jonah. He is grabbed by Logan, a girl wearing a black hat with a closed face on his right. Vince, a boy who wearing a hat with the same closed face on Jonah's left, is standing on Jonah's left and staring closely at Jonah. The three of them are performing on the street, attracting many passers-by.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 14,
    "chunk_index_in_section": 14,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 580,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c15",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "评分4的样例：\nThe bald white-haired man Rex is pulling his daughter Anna into the room, completely ignoring his barefoot wife Cathy behind him. Anna wears a whiteuit jacket and is waving her hand to cool down, seeming to feel very hot.\n\n最终在原始SK-VG数据集的基础上得到了新标注的Condensed SK-VG (CSK-VG)数据集。统计两者中场景知识长度结果对比如表44所示，可以发现，本章提出的数据简化方案有效地降低了场景知识的复杂度。\n\n表44 SK-VG和CSK-VG数据集中场景知识复杂度对比：\n\n数据集    最大长度    平均长度\nSK-VG    109    58.28\nCSK-VG    82    47.00\n\n4.3面向场景知识的指称表达理解推理网络\n\n本小节全面介绍SKRN的整体模型框架和设计思路，以及各组件的设计细节。\n\n4.3.1模型总览",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 15,
    "chunk_index_in_section": 15,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 503,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c16",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "4.3面向场景知识的指称表达理解推理网络\n\n本小节全面介绍SKRN的整体模型框架和设计思路，以及各组件的设计细节。\n\n4.3.1模型总览\n\nSKRN的整体模型框架如图4-6所示，主要包括语言编码器，图像编码器，以及场景知识推理模块，定位框预测模块，以及模型的损失函数设计。其中语言和图像编码器的具体实现、定位框预测模块的设计跟第三章LG-RNET相应部分相同，在此不再赘述。模型首先通过各自的编码器提取指称查询、场景知识、图像的特征并得到特征序列。然后，本节将重点介绍场景知识推理模块，通过三个不共享的多头注意力分别对进行注意力计算并提取特征，在此过程中预测token逐渐捕获相应模态中用于定位目标物体的特征，并在每一层都输出预测框坐标。\n\n4.3.2结合场景知识的指称表达推理模块\n\n为充分利用指称查询、场景知识和图片三方面的特征进行联合的跨模态推理，本节设计了面向场景知识的推理模块。首先参考LG-RNET设置一个预测token PER用于收集来自上述三方面的特征并预测定位框，c为隐向量维度。在每一层的推理过程中依次对指称查询、场景知识、图片进行多头注意力运算。最后，经过一个全连接层和残差连接得到当前层更新后的预测token表示。\n\n4.3.3模型损失函数\n\n为训练SKRN，本小节设计损失函数如下：\n\ntotal_loss = Σgiou_loss + Σl1_loss",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 16,
    "chunk_index_in_section": 16,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 590,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c17",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "4.3.3模型损失函数\n\n为训练SKRN，本小节设计损失函数如下：\n\ntotal_loss = Σgiou_loss + Σl1_loss\n\n其中U和分别为GIoU损失和L1损失，α和β为平衡两者的超参数。此外，SKRN模型框架更侧重结合指称查询和场景知识的联合，一方面由于SK-REC任务的场景知识中存在一定的噪声，因此SKRN的损失函数没有沿用LG-RNET中包含的跨模态对齐损失，而是仅设置框回归损失来监督模型的训练。\n\n4.4实验对比与分析\n\n4.4.1实验参数与设置\n\n关于SKRN训练过程中的具体设置跟LG-RNET大体相同。主要差异在于损失函数的超参数设置，参考之前工作[28]的经验，设置α和β分别为2和5，此外在模型训练的前10个epoch中冻结视觉和文本编码器的参数以稳定训练过程。由于SKRN对输入的指称表达和场景知识分别进行特征提取，因此结合表4-3的数据集文本长度统计，SKRN设置指称表达最大文本长度为40，SK-VG和CSK-VG数据集下的场景知识最大文本长度分别为120和100。\n\n4.4.2评测指标对比与分析",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 17,
    "chunk_index_in_section": 17,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 472,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c18",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "4.4.2评测指标对比与分析\n\nSK-REC任务以及SK-VG数据集比较新，可用于对比的baseline不如传统REC任务丰富。在此列出所有在该数据集评估过的工作作为对比基线，它们分别是NM-Erase[1(3)], RESC-Large_[2G], FAOA[18], MatNet[14], KEVILI[40]。其中NM-Tree、RESC-Large、FAOA、以及MatNet在3.3.2节中已经介绍过，在此不再赘述。首先简要介绍一下CM-At-Erase和KEVILI：\n\nCM-At-Erase以MatNet作为基础模型，通过移除数据样本中最关键的图文关联信息强迫模型学习到更细节，容易被忽视的图文关联，进而提升模型注意力层的性能。\n\nKEVILI利用场景知识特征与图像特征进行跨注意力计算先对图像特征进行增强，提高对场景知识提及物体的注意权重，再结合3.3.1小节提及的拼接注意力方案对指称表达特征和增强后的图像特征进行跨模态融合。",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 18,
    "chunk_index_in_section": 18,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 424,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c19",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "实验结果对比如表4-5所示，可以发现SKRN取得了最佳的性能表现，说明模型面对复杂场景知识的推理能力。此外，本节还对第三章提出的LG-RNET在SK-VG数据集上进行评估，通过模板“Query: referring expression. Knowledge: scene knowledge.”将SK-VG数据样本中的指称表达和场景知识拼接为的文本序列，并将最大句子长度设置为140，其他设置与3.3.1小节一致。可以发现LG-RNET取得了次优的结果，相比于KEVILI的拼接注意力方案，LG-RNET超越了10个百分点，进一步说明LG-RNET中跨模态推理方案相比拼接注意力方案的优势。另一方面LG-RNET性能表现落后于SKRN模型，说明仅仅通过上述模板对场景知识进行简单的拼接尚不足以应对如此复杂的跨模态推理要求。\n\n表4-5 SKRN模型在SK-VG下的性能对比：\n\nMethod    Acc    Method    Acc\n[83]    25.24    MatNet[14]    25.28\nCM-At-Erase[103]    26.08    KEVILI[40]    30.01\n-RESC-Large[2G]    36.68    LG-RNET    40.41\n[18]    16.30    SKRN    43.04",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 19,
    "chunk_index_in_section": 19,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 582,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c20",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "此外，SK-VG数据集中的测试集包含推理难度的标注，分为简单(Easy, E)，中等(Medium, M)和困难(Hard, H)。核心的标注依据为与场景知识相关程度越高，视觉可分辨性越差的指称表达难度越高。其中“E”的评判标准为指称表达中包含目标物体明显的外观信息，与其他物体的关系或者其他视觉线索；“M”的评判标准为指称表达中仅仅提及不明显的视觉信息；“H”的评判标准为答案要求完全来源于场景知识，没有视觉偏向。示例如图4-8所示，面对同一张图片和相同的场景知识，由于不同指称表达指向物体对场景知识的依赖程度不同，导致不同的难度。\n\n根据您提供的指示，以下是清洗后的学术论文第11部分内容：\n\n---\n\n图 4-8 展示了不同难度的指称表达样例。表 4-6 统计了 VG 测试集中不同难度样本的比例。表 4-7 展示了模型在不同难度样本上的表现。结果显示，LG-R-Net 在简单和中等难度样本上获得了明显的性能提升，但在困难样本上的提升相对不够显著。这表明 LG-R-Net 在充分利用语言特征引导跨模态推理方面是正确而有效的，但也需要改进模型架构以处理更复杂的场景知识。SKRN 在不同难度样本上的性能提升更显著且均衡，尤其在困难样本上也有明显改进，说明其推理框架对场景知识的有效推理。\n\n---",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 20,
    "chunk_index_in_section": 20,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 552,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c21",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "---\n\n以上内容已去除页眉、页脚、页码等非核心学术内容，保留了原文中的学术内容、技术术语、数据、逻辑结构和公式图表说明。\n\n根据您的要求，以下是清洗后的参考文献内容：\n\n1. Ren S, He K, Girshick R, et al. Faster R-CNN: Towards real-time object detection with region proposal networks [J]. Advances in Neural Information Processing Systems, 2015, 28:91-99.\n\n2. Uijlings J R R, Van De Sande K E A, Gevers T, et al. Selective search for object recognition [J]. International Journal of Computer Vision, 2013, 104:154-171.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 21,
    "chunk_index_in_section": 21,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 434,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c22",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "3. Rohrbach A, Rohrbach M, Hu R, et al. Grounding of textual phrases in images by reconstruction [C]//European Conference on Computer Vision. Springer, 2016:817-834.\n\n4. Nagaraj V K, Morariu V I, Davis L S. Modeling context between objects for referring expression understanding [C]//Em\n\n5. Yu L, Poirson P, Yang S, et al. Modeling context in referring expressions [C]//European Conference on Computer Vision. Springer, 2016:69-85.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 22,
    "chunk_index_in_section": 22,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 431,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c23",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "6. Hu R, Rohrbach M, Andriluka M, et al. Modeling relationships in referring expressions with compositional modular networks [C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017:4418-4427.\n\n7. Yu L, Lin Z, Shen X, et al. Mattnet: Modular attention network for referring expression comprehension [C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018:1307-1315.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 23,
    "chunk_index_in_section": 23,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 430,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c24",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "8. Wang P, Wu Q, Cao J, et al. Neighbouthood watch: Referring expression comprehension via language-guided graph attention networks [C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019:1960-1968.\n\n9. Yang S, Li G, Yu Y. Dynamic graph attention for referring expression comprehension [C]//Proceedings of the IEEE International Conference on Computer Vision. 2019:4644-4653.\n\n10. Redmon J, Farhadi A. YOLOv3: An incremental improvement [J]. arXiv preprint arXiv:1804.02767, 2018.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 24,
    "chunk_index_in_section": 24,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 513,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c25",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "11. Yang Z, Gong B, Wang L, et al. A fast and accurate one-stage approach to visual grounding [C]//Proceedings of the IEEE International Conference on Computer Vision. 2019:4683-4693.\n\n12. Liao Y, Liu S, Li G, et al. A real-time cross-modality correlation filtering method for referring expression comprehension [C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2020:10880-10889.\n\n13. Yang Z, Chen T, Wang L, et al. Improving one-stage visual grounding by recursive sub-query construction [C]//European Conference on Computer Vision. Springer, 2020:387-404.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 25,
    "chunk_index_in_section": 25,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 591,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c26",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "14. Sun M, Xiao J, Lim E G. Iterative shrinking for referring expression grounding using deep reinforcement learning [C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2021:14060-14069.\n\n15. Dosovitskiy A, Beyer L, Kolesnikov A, et al. An image is worth 16x16 words: Transformers for image recognition at scale [J]. arXiv preprint arXiv:2010.11929, 2020.\n\n16. Liu Z, Lin Y, Cao Y, et al. Swin transformer: Hierarchical vision transformer using shifted windows [C]//Proceedings of the IEEE International Conference on Computer Vision. 2021:10012-10022.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 26,
    "chunk_index_in_section": 26,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 584,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c27",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "17. Canon N, Massa F, Synnaeve G, et al. End-to-end object detection with transformers [C]//European Conference on Computer Vision. Springer, 2020:213-229.\n\n18. Deng J, Yang Z, Chen T, et al. TransVG: End-to-end visual grounding with transformers [C]//Proceedings of the IEEE International Conference on Computer Vision. 2021:1769-1779.\n\n19. Deng J, Yang Z, Liu D, et al. TransVG: End-to-end multi-modal grounding with language conditioned vision transformer [J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023, 45(11):13636-13652.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 27,
    "chunk_index_in_section": 27,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 553,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c28",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "20. Ye J, Tian J, Yan M, et al. Shifting more attention to visual backbone: Query-modulated refinement networks for end-to-end visual grounding [C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2022:15502-15512.\n\n21. Yang L, Xu Y, Yuan C, et al. Improving visual grounding with visual-linguistic verification and iterative reasoning [C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2022:9499-9508.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 28,
    "chunk_index_in_section": 28,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 466,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c29",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "22. Li M, Sigal L. Referring transformer: A one-step approach to multi-task visual grounding [J]. Advances in Neural Information Processing Systems, 2021, 34:19652-19664.\n\n23. Su W, Miao P, Dou H, et al. Language adaptive weight generation for multi-task visual grounding [C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2023:10857-10866.\n\n24. Kamath A, Singh M, LeCun Y, et al. Mdetr-modulated detection for end-to-end multi-modal understanding [C]//Proceedings of the IEEE International Conference on Computer Vision. 2021:1780-1790.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 29,
    "chunk_index_in_section": 29,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 570,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c30",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "25. Wang P, Yang A, Men R, et al. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-sequence learning framework [C]//International Conference on Machine Learning. PMLR, 2022:23318-23340.\n\n26. Su W, Zhu X, Cao Y, et al. Vl-bert: Pre-training of generic visual-linguistic representations [C]//International Conference on Learning Representations. 2019.\n\n27. Li L H, Yatskar M, D, et al. Visualbert: A simple and performant baseline for vision and language [J]. arXiv preprint arXiv:1908.03557, 2019.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 30,
    "chunk_index_in_section": 30,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 524,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c31",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "28. Radford A, Kim J W, Hallacy C, et al. Learning transferable visual models from natural language supervision [C]//International Conference on Machine Learning. PMLR, 2021:8748-8763.\n\n29. Li J, Li D, Xiong C, et al. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation [C]//International Conference on Machine Learning. PMLR, 2022:12888-12900.\n\n30. Li J, Li D, Savarese S, et al. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models [J]. arXiv preprint arXiv:2301.12597, 2023.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 31,
    "chunk_index_in_section": 31,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 580,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c32",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "31. Zhu D, Chen J, Shen X, et al. Minigpt-4: Enhancing vision-language understanding with advanced large language models [J]. arXiv preprint arXiv:2304.10592, 2023.\n\n32. Kazerouni S, Ordonez V, Matten M, et al. Referitgame: Referring to objects in photographs of natural scenes [C]//Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. 2014:787-798.\n\n33. Song Y, Zhang R, Chen Z, et al. Advancing visual grounding with scene knowledge: Benchmark and method [C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2023:15039-15049.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 32,
    "chunk_index_in_section": 32,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 593,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c33",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "34. He K, Zhang X, Ren S, et al. Deep residual learning for image recognition [C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016:770-778.\n\n35. Chren W A. One-hot residual coding for low delay-power product CMOS design [J]. IEEE Transactions on Circuits and Systems II: Analog and Digital Signal Processing, 1998, 45(3):303-313.\n\n36. Mikolov T, Chen K, Corrado G, et al. Efficient estimation of word representations in vector space [J]. arXiv preprint arXiv:1301.3781, 2013.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 33,
    "chunk_index_in_section": 33,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 511,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c34",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "37. Mikolov T, Sutskever I, Chen K, et al. Distributed representations of words and phrases and their compositionality [J].\n\nAdvances in Neural Information Processing Systems, 2013, 26(2): 3111-3119.\n\n[45] Huffman D A. A method for the construction of minimum-redundancy codes [J]. Proceedings of the IRE, 1952, 40(9): 1098-1101.\n\n[46] Pennington J, Socher R, Manning C D. GloVe: Global vectors for word representation [C]//Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. 2014: 1532-1543.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 34,
    "chunk_index_in_section": 34,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 528,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c35",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "[47] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need [J]. Advances in Neural Information Processing Systems, 2017, 30: 5998-6008.\n\n[48] Hochreiter S, Schmidhuber J. Long short-term memory [J]. Neural Computation, 1997, 9(8): 1735-1780.\n\n[49] Devlin J, Chang M W, Lee K, et al. Bert: Pre-training of deep bidirectional transformers for language understanding [J]. arXiv preprint arXiv:1810.04805, 2018.\n\n[50] Liu Y, Ott M, Goyal N, et al. RoBERTa: A robustly optimized BERT pretraining approach [J]. arXiv preprint arXiv:1907.11692, 2019.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 35,
    "chunk_index_in_section": 35,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 554,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c36",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "[51] Lan Z, Chen M, Goodman S, et al. ALBERT: A lite BERT for self-supervised learning of language representations [J]. arXiv preprint arXiv:1909.11942, 2019.\n\n[52] Radford A, Narasimhan K, Salimans T, et al. Improving language understanding by generative pre-training [J]. OpenAI blog, 2018, 1(8): 9.\n\n[53] Radford A, Wu J, Child R, et al. Language models are unsupervised multitask learners [J]. OpenAI blog, 2019, 1(8): 9.\n\n[54] Brown T, Mann B, Ryder N, et al. Language models are few-shot learners [J]. Advances in Neural Information Processing Systems, 2020, 33: 1877-1901.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 36,
    "chunk_index_in_section": 36,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 579,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c37",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "[55] Ouyang L, Wu J, Jiang X, et al. Training language models to follow instructions with human feedback [J]. Advances in Neural Information Processing Systems, 2022, 35: 27730-27744.\n\n[56] Touvron H, Lavril T, Izacard G, et al. LLaMA: Open and efficient foundation language models [J]. arXiv preprint arXiv:2302.13971, 2023.\n\n[57] Touvron H, Martin L, Stone K, et al. LLaMA 2: Open foundation and fine-tuned chat models [J]. arXiv preprint arXiv:2307.09288, 2023.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 37,
    "chunk_index_in_section": 37,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 464,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c38",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "[58] Du Z, Qian Y, Liu X, et al. GLM: General language model pretraining with autoregressive blank infilling [J]. arXiv preprint arXiv:2103.10360, 2021.\n\n[59] Lowe D G. Distinctive image features from scale-invariant keypoints [J]. International Journal of Computer Vision, 2004, 60: 91-110.\n\n[60] Harris C, Stephens M. A combined comer and edge detector [C]//Alvey Vision Conference. 1988: 10-5244.\n\n[61] LeCun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition [J]. Proceedings of the IEEE, 1998, 86(11): 2278-2324.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 38,
    "chunk_index_in_section": 38,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 552,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c39",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "[62] Simonyan Zisserman A. Very deep convolutional networks for large-scale image recognition [J]. arXiv preprint arXiv:1409.1556, 2014.\n\n[63] Szegedy C, Liu W, Jia Y, et al. Going deeper with convolutions [C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015: 1-9.\n\n[64] Huang G, Liu Z, van der Maaten L, et al. Densely connected convolutional networks [C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017: 4700-4708.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 39,
    "chunk_index_in_section": 39,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 489,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c40",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "[65] Liu Z, Hu H, Lin Y, et al. Swin Transformer v2: Scaling up capacity and resolution [C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2022: 12009-12019.\n\n[66] He K, Gkioxari G, Dollár P, et al. Mask R-CNN [C]//Proceedings of the IEEE International Conference on Computer Vision. 2017: 2961-2969.\n\n[67] Bochkovskiy A, Wang C Y, Liao H Y M. YOLOv4: Optimal speed and accuracy of object detection [J]. arXiv preprint arXiv:2004.10934, 2020.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 40,
    "chunk_index_in_section": 40,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 475,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c41",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "[68] Kuhn H W. The Hungarian method for the assignment problem [J]. Naval Research Logistics Quarterly, 1955, 2(1): 83-97.\n\n[69] Meng D, Chen X, Fan Z, et al. Conditional detr for fast training convergence [C]//Proceedings of the IEEE International Conference on Computer Vision. 2021: 3651-3660.\n\n[70] Liu S, Li F, Zhang H, et al. DaB-detr: Dynamic anchor boxes are better queries for detr [J]. arXiv preprint arXiv:2201.12329, 2022.\n\n[71] Zhu X, Su W, Lu L, et al. Deformable detr: Deformable transformers for end-to-end object detection [J]. arXiv preprint arXiv:2010.04159, 2020.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 41,
    "chunk_index_in_section": 41,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 583,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c42",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "[72] Mao J, Huang J, Toshev A, et al. Generation and comprehension of unambiguous object descriptions [C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016: 1-20.\n\n[73] Plummer B A, Wang L, Cerban C M, et al. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models [C]//Proceedings of the IEEE International Conference on Computer Vision. 2015: 2641-2649.\n\n[74] Lin T Y, Maire M, Belongie S, et al. Microsoft COCO: Common objects in context [C]//European Conference on Computer Vision. Springer, 2014: 740-755.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 42,
    "chunk_index_in_section": 42,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 589,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c43",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "[75] Escolano H, Herandez C A, Gozalez J A, et al. The segmented and annotated LAPR TC benchmark [J]. Computer Vision and Image Understanding, 2010, 114(4): 421-428.\n\n[76] Young P, Lai A, Hodosh M, et al. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions [J]. Transactions of the Association for Computational Linguistics, 2014, 2: 67-78.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 43,
    "chunk_index_in_section": 43,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 405,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c44",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "[77] Rezatofighi H, Tsoi N, Gwak J Y, et al. Generalized intersection over union: A metric and a loss for bounding box regression [C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019: 658-666.\n\n[78] Krishna R, Zhu Y, Groth O, et al. Visual genome: Connecting language and vision using crowd-sourced dense image annotations [J]. International Journal of Computer Vision, 2017, 123: 32-73.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 44,
    "chunk_index_in_section": 44,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 423,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c45",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "[79] Zhang H, Niu Y, Chang S F. Grounding referring expressions in images via textual context [C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 4158-4166.\n\n[80] Plummer B A, Kordas P, Kiapour M H, et al. Conditional image-text embedding networks [C]//European Conference on Computer Vision. Springer, 2018: 249-264.\n\n[81] Yu Z, Yu J, Xiang C, et al. Rethinking diversified and discriminative proposal generation for visual grounding [J]. arXiv preprint arXiv:1805.03508, 2018.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 45,
    "chunk_index_in_section": 45,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 516,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c46",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "[82] Wang L, Li Y, Huang J, et al. Learning two-branch neural networks for image-text matching tasks [J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, 41(2): 394-407.\n\n[83] Liu D, Zhang H, Wu F, et al. Learning to assemble neural module tree networks for visual grounding [C]//Proceedings of the IEEE International Conference on Computer Vision. 2019: 4673-4682.\n\n[84] Chen L, Ma W, Xiao J, et al. Ref-nms: Breaking proposal bottlenecks in two-stage referring expression grounding [C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2021, 35(2): 1036-1044.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 46,
    "chunk_index_in_section": 46,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 597,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c47",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "[85] Mu Z, Tang S, Tan J, et al. Disentangled led motif-aware graph learning for phrase grounding [C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2021, 35(15): 13594.\n\n[86] Hong R, Liu D, Mo X, et al. Learning to compose and reason with language tree structures for visual grounding [J].\n\n清洗后的内容如下：\n\n参考文献：\n\n[87] Ye J, Lin X, He L, et al. One-stage visual grounding via semantic-aware feature filter [C]//Proceedings of the 29th ACM International Conference on Multimedia. 2021: 1702-1711.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 47,
    "chunk_index_in_section": 47,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 508,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c48",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "[88] Huang B, Lian D, Luo W, et al. Look before you leap: Learning landmark features for one-stage visual grounding [C]//Proceedings of the 3EEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 1688-1697.\n\n[89] Zhao H, Zhou J T, Ong Y S. Word2Pix: Word to pixel cross-attention Transformer in visual grounding [J]. IEEE Transactions on Neural Networks and Learning Systems, 2024, 35(2): 1523-1533.\n\n[90] Ho C H, Appalaraju S, Jasan B, et al. YOLO-lightweightened end-to-end visual grounding [C]//European Conference on Computer Vision. Springer, 2022: 3-23.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 48,
    "chunk_index_in_section": 48,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 573,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c49",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "[91] Zhu C, Zhou Y, Shen Y, et al. SeqTR: A simple yet universal network for visual grounding [C]//European Conference on Computer Vision. Springer, 2022: 598-615.\n\n[92] Shi F, Gao R, Huang W, et al. Dynamic MDETR: A dynamic multimodal transformer decoder for visual grounding [J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024, 46(2): 1181-1198.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 49,
    "chunk_index_in_section": 49,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 369,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c50",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "[93] Lu J, Batra D, Parikh D, et al. VILBERT: Pretraining task-agnostic visual and linguistic representations for vision-and-language tasks [C]//Proceedings of the 33rd International Conference on Neural Information Processing Systems. 2019: 13-23.\n\n[94] Chen Y C, Li L, Yu L, et al. Uniter: Universal image-text representation learning [C]//European Conference on Computer Vision. Springer, 2020: 104-120.\n\n[95] Chen K, Zhang Z, Zeng W, et al. Shikra: Unleashing Multimodal LLMs Referential Dialogue Magic [J]. arXiv preprint arXiv:2306.15195, 2023.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 50,
    "chunk_index_in_section": 50,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 550,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c51",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "[96] Li C, Xu H, Tian J, et al. mplug: Effective and efficient vision-language learning by cross-modal skip-connections [J]. arXiv preprint arXiv:2205.12005, 2022.\n\n[97] Wang P, Wang S, Lin J, et al. ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities [J]. arXiv preprint arXiv:2305.11172, 2023.\n\n[98] Zhang R, Li Y, Ma Y, et al. Llmaaa: Making large language models as active annotators [J].\n\n[99] Dong Q, Li L, Dai D, et al. A survey for in-context learning [J]. arXiv preprint arXiv:2301.00234, 2022.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 51,
    "chunk_index_in_section": 51,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 533,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c52",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "[100] Wei J, Wang X, Schuurmans D, et al. Chain-of-thought prompting elicits reasoning in large language models [J]. Advances in Neural Information Processing Systems, 2022, 35: 24824-24837.\n\n[101] Kojima T, Gu S S, Reid M, et al. Large language models are zero-shot reasoners [J]. Advances in Neural Information Processing Systems, 2022, 35: 22199-22213.\n\n[102] Achan J, Adler S, Agarwal S, et al. GPT-4 technical report [J]. arXiv preprint arXiv:2303.08774, 2023.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 52,
    "chunk_index_in_section": 52,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": true,
    "is_section_start": false,
    "is_section_end": false,
    "chunk_length": 465,
    "chunk_method": "hierarchical",
    "importance_weight": 0.7,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  },
  {
    "chunk_id": "面向复杂文本的指称表达理解研究_陆明聪_s0_c53",
    "source_id": "面向复杂文本的指称表达理解研究_陆明聪",
    "text": "[103] Liu X, Wang Z, Shao J, et al. Improving referring Expression Grounding with Cross-Modal Attention-Guided Reasoning [C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019: 1950-1959.",
    "section_title": "全文",
    "section_type": "default",
    "section_index": 0,
    "total_sections": 1,
    "chunk_index": 53,
    "chunk_index_in_section": 53,
    "total_chunks_in_section": 54,
    "has_previous": true,
    "has_next": false,
    "is_section_start": false,
    "is_section_end": true,
    "chunk_length": 221,
    "chunk_method": "hierarchical",
    "importance_weight": 0.735,
    "document_metadata": {
      "text_source": "cleaned",
      "original_status": "text_extracted",
      "document_type": "academic_paper"
    }
  }
]