{
  "0_387_29295_0_82": {
    "document_metadata": {
      "document_type": "原文无此信息",
      "title": "SKFD-ISOMAP FOR FACE RECOGNITION",
      "authors_or_creators": [
        "Ruifan Li",
        "Cong Wang",
        "Xuyan Tu"
      ]
    },
    "main_content": {
      "stated_purpose": "提出SKFD-Isomap方法，利用类别信息和核Fisher判别（KFD）进行非线性嵌入，以改善人脸识别中的模式分类。",
      "key_concepts": [
        "人脸识别",
        "流形",
        "Isomap",
        "KFD"
      ],
      "main_methods": [
        "SKFD-Isomap",
        "核Fisher判别",
        "多维标度",
        "最近邻分类器"
      ]
    },
    "relationships": {
      "references_to": [
        "Zhao et al. (2003)",
        "Turk and Pentland (1991)",
        "Belhumeur et al. (1997)",
        "Martinez and Kak (2001)",
        "He et al. (2003)",
        "Yang (2002)",
        "Liu et al. (2002)",
        "Seung and Lee (2000)",
        "Silva et al. (2000)",
        "Roweis and Saul (2000)",
        "Belkin and Niyogi (2001)",
        "Mika et al. (1999)",
        "Scholkopf et al. (1998)",
        "Specht (1991)"
      ],
      "builds_on": [
        "Eigenfaces",
        "Fisherfaces",
        "kernel Eigenfaces",
        "kernel Fisherfaces",
        "Isomap",
        "kernel-based nonlinear discriminant analysis",
        "Laplacian eigenmaps",
        "Fisher discriminant analysis with kernels",
        "nonlinear component analysis as a kernel eigenvalue problem"
      ]
    },
    "note": "文档类型不明确，仅提取最基础信息",
    "extraction_metadata": {
      "file_id": "0_387_29295_0_82",
      "detected_doc_type": "unknown",
      "extraction_time": "2025-08-02 12:33:53",
      "validation": {
        "warnings": [],
        "suspicious_count": 0,
        "validation_score": 100,
        "quality_level": "high"
      },
      "text_length": 5058,
      "processed_text_length": 5058
    }
  },
  "0592": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "Show and Tell More: Topic-Oriented Multi-Sentence Image Captioning",
      "authors": [
        "Yuzhao Mao",
        "Chang Zhou",
        "Xiaojie Wang",
        "Ruifan Li"
      ],
      "institutions": [
        "Center for Intelligence Science and Technology, School of Computer Science, Beijing University of Posts and Telecommunications"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "Single-sentence (SS) captioning models",
          "relationship_type": "基于",
          "evidence_text": "Single-sentence (SS) captioning models have been developed using templates and neural network approaches, but they often provide incomplete descriptions."
        },
        {
          "method_name": "MS captioning methods",
          "relationship_type": "基于",
          "evidence_text": "MS captioning methods generate multiple sentences for a complete depiction, with some focusing on regions of interest."
        }
      ],
      "compared_methods": [
        {
          "method_name": "NIC",
          "comparison_result": "Our TOMS demonstrates improved performance, especially in terms of IC.",
          "evidence_text": "Our proposed Topic-Oriented Multi-Sentence (TOMS) captioning model incorporates topic embedding to guide the generation process... Our TOMS demonstrates improved performance, especially in terms of IC."
        },
        {
          "method_name": "ATT-FCN",
          "comparison_result": "原文无此信息",
          "evidence_text": "We compare our TOMS model with NIC and ATT-FCN for sentence level MS captioning"
        },
        {
          "method_name": "RTT-GAN",
          "comparison_result": "原文无此信息",
          "evidence_text": "We compare our TOMS model with NIC and ATT-FCN for sentence level MS captioning and with RTT-GAN, Regions-Hierarchical, and Sentence-Concat for paragraph level MS captioning."
        },
        {
          "method_name": "Regions-Hierarchical",
          "comparison_result": "原文无此信息",
          "evidence_text": "We compare our TOMS model with NIC and ATT-FCN for sentence level MS captioning and with RTT-GAN, Regions-Hierarchical, and Sentence-Concat for paragraph level MS captioning."
        },
        {
          "method_name": "Sentence-Concat",
          "comparison_result": "原文无此信息",
          "evidence_text": "We compare our TOMS model with NIC and ATT-FCN for sentence level MS captioning and with RTT-GAN, Regions-Hierarchical, and Sentence-Concat for paragraph level MS captioning."
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "Flickr8k",
          "dataset_description": "原文无此信息",
          "evidence_text": "We evaluate our model on standard datasets like Flickr8k, Flickr30k, COCO for sentence level MS captioning"
        },
        {
          "dataset_name": "Flickr30k",
          "dataset_description": "原文无此信息",
          "evidence_text": "We evaluate our model on standard datasets like Flickr8k, Flickr30k, COCO for sentence level MS captioning"
        },
        {
          "dataset_name": "COCO",
          "dataset_description": "原文无此信息",
          "evidence_text": "We evaluate our model on standard datasets like Flickr8k, Flickr30k, COCO for sentence level MS captioning"
        },
        {
          "dataset_name": "paragraph dataset from Krause et al. (2017)",
          "dataset_description": "原文无此信息",
          "evidence_text": "We also introduce Instance Coverage (IC) to measure descriptive completeness on a paragraph dataset from Krause et al. (2017) for paragraph level MS captioning."
        }
      ],
      "evaluation_metrics": [
        "BELU",
        "METEOR",
        "ROUGE L",
        "CIDEr",
        "Instance Coverage (IC)"
      ],
      "baseline_methods": [
        "NIC",
        "ATT-FCN",
        "RTT-GAN",
        "Regions-Hierarchical",
        "Sentence-Concat"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "Instance Coverage (IC)",
          "our_result": "原文无此信息",
          "baseline_result": "原文无此信息",
          "dataset": "原文无此信息",
          "evidence_text": "Our TOMS demonstrates improved performance, especially in terms of IC."
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "a novel topic-oriented captioning model",
        "the FGU design",
        "extensive experimental evaluation"
      ],
      "stated_novelty": [
        "Our TOMS model differs by generating sentences from topics of interest, capturing linguistic distinctions in image descriptions.",
        "The Fusion Gate Unit (FGU) fuses three sources of representations: image, context, and topic."
      ],
      "stated_advantages": [
        "topical consistency and descriptive completeness",
        "arranges multi-sentence generation using topics",
        "captures image details better than single-sentence captions"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文无此信息"
      ],
      "failure_cases": [
        "原文无此信息"
      ]
    },
    "extraction_metadata": {
      "file_id": "0592",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 12:35:23",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑技术关系evidence_text: Our proposed Topic-Oriented Multi-Sentence (TOMS) captioning model incorporates topic embedding to guide the generation process... Our TOMS demonstrates improved performance, especially in terms of IC."
        ],
        "suspicious_count": 2,
        "validation_score": 60,
        "quality_level": "medium"
      },
      "text_length": 9105,
      "processed_text_length": 9105
    }
  },
  "0628": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "Multi-scale Two-way Deep Neural Network for Stock Trend Prediction",
      "authors": [
        "Guang Liu",
        "Yuzhao Mao",
        "Qi Sun",
        "Hailong Huang",
        "Weiguo Gao",
        "Xuan Li",
        "JianPing Shen",
        "Ruifan Li",
        "Xiaojie Wang"
      ],
      "institutions": [
        "原文无此信息"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "Support Vector Machine and Neural Networks",
          "relationship_type": "基于",
          "evidence_text": "STP is a classification task traditionally tackled by Support Vector Machine and Neural Networks."
        },
        {
          "method_name": "Ensemble-based methods like Random Forest and deep learning models",
          "relationship_type": "基于",
          "evidence_text": "Ensemble-based methods like Random Forest and deep learning models have also been explored."
        }
      ],
      "compared_methods": [
        {
          "method_name": "XGBoost",
          "comparison_result": "原文无此信息",
          "evidence_text": "The data is treated as a non-stationary discrete signal and decomposed using DWT to obtain transformed multi-scale components. These are concatenated and input to an XGBoost model to ensemble multi-scale information and output category scores."
        },
        {
          "method_name": "CNN",
          "comparison_result": "原文无此信息",
          "evidence_text": "A key operation concatenates these features, and a GRU unit temporally cascades the information to output categories."
        },
        {
          "method_name": "RNN",
          "comparison_result": "原文无此信息",
          "evidence_text": "A key operation concatenates these features, and a GRU unit temporally cascades the information to output categories."
        },
        {
          "method_name": "RCNN",
          "comparison_result": "RCNN shows the best performance on most indices",
          "evidence_text": "In the multi-scale rows, variations are fed with DWT-based, downsampling-based, and CNN multi-kernel size scale-information. RCNN shows the best performance on most indices, while XGBoost is less effective."
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "FI-2010",
          "dataset_description": "The first publicly available benchmark dataset of high-frequency Limit Order Book (LOB) data",
          "evidence_text": "For FI-2010, the experimental settings are:"
        },
        {
          "dataset_name": "CSI-2016",
          "dataset_description": "A dataset we collected from three one-minute stock index data",
          "evidence_text": "For CSI-2016, the experimental settings are:"
        }
      ],
      "evaluation_metrics": [
        "F1 score",
        "accuracy"
      ],
      "baseline_methods": [
        "Support Vector Machine",
        "Neural Networks",
        "Random Forest",
        "deep learning models"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "F1 score",
          "our_result": "81.05%",
          "baseline_result": "原文无此信息",
          "dataset": "FI-2010",
          "evidence_text": "The results on FI-2010 show that our two-way model achieves state-of-the-art (SOTA) performance with 81.05% F1 score"
        },
        {
          "metric_name": "accuracy",
          "our_result": "81.12%",
          "baseline_result": "原文无此信息",
          "dataset": "FI-2010",
          "evidence_text": "The results on FI-2010 show that our two-way model achieves state-of-the-art (SOTA) performance with 81.12% accuracy"
        },
        {
          "metric_name": "accuracy",
          "our_result": "63.07%",
          "baseline_result": "原文无此信息",
          "dataset": "CSI-2016",
          "evidence_text": "On CSI-2016, our MTDNN model achieves the highest accuracy of 63.07%"
        },
        {
          "metric_name": "F1 score",
          "our_result": "61.65%",
          "baseline_result": "原文无此信息",
          "dataset": "CSI-2016",
          "evidence_text": "On CSI-2016, our MTDNN model achieves the highest F1 score of 61.65%"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "Proposing a Multi-scale Two-way Deep Neural Network (MTDNN) for stock trend prediction"
      ],
      "stated_novelty": [
        "Using wavelet-based and downsampling-based scale information",
        "Achieving state-of-the-art performance on FI-2010 and CSI-2016 datasets"
      ],
      "stated_advantages": [
        "Effectively utilizes multi-scale information in stock data"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文无此信息"
      ],
      "failure_cases": [
        "原文无此信息"
      ]
    },
    "extraction_metadata": {
      "file_id": "0628",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 12:36:06",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑技术关系evidence_text: In the multi-scale rows, variations are fed with DWT-based, downsampling-based, and CNN multi-kernel size scale-information. RCNN shows the best performance on most indices, while XGBoost is less effective."
        ],
        "suspicious_count": 2,
        "validation_score": 60,
        "quality_level": "medium"
      },
      "text_length": 8174,
      "processed_text_length": 8174
    }
  },
  "10.5560_zna.2012_0029": {
    "document_metadata": {
      "document_type": "原文无此信息",
      "title": "An improved quantum secure direct communication protocol based on a four-particle Green–Horne–Zeilinger (GHZ) state",
      "authors_or_creators": [
        "原文无此信息"
      ]
    },
    "main_content": {
      "stated_purpose": "To enhance the efficiency of eavesdropping detection in quantum secure direct communication",
      "key_concepts": [
        "Quantum secure direct communication",
        "Four-particle GHZ state",
        "Eavesdropping detection",
        "Entropy theory",
        "Quantum dense coding"
      ],
      "main_methods": [
        "Utilizing four-particle GHZ state",
        "Quantum dense coding",
        "Entropy theory analysis"
      ]
    },
    "relationships": {
      "references_to": [
        "G. S. Vernam, J. Am. Inst. Electr. Eng. 55, 109 (1926)",
        "C. E. Shannon, Bell Syst. Tech. J. 28, 656 (1949)",
        "C. H. Bennett and G. Brassard, in: Proc. IEEE Int. Conf. Computers, Systems and Signal Processing, Bangalore 1984, p. 175–179",
        "A. K. Ekert, Phys. Rev. Lett. 67, 661 (1991)",
        "C. H. Bennett, Phys. Rev. Lett. 68, 3121 (1992)",
        "N. Gisin, G. Ribordy, W. Tittel, and H. Zbinden, Rev. Mod. Phys. 74, 145 (2002)",
        "K. Boström and T. Felbinger, Phys. Rev. Lett. 89, 187902 (2002)",
        "G. L. Long and X. S. Liu, Phys. Rev. A 65, 032302 (2002)",
        "Q. Y. Cai and B. W. Li, Chin. Phys. Lett. 21, 601 (2004)",
        "F. G. Deng, G. L. Long, and G. L. Long, Phys. Rev. A 69, 052319 (2004)",
        "M. Lucamarini and S. Mancini, Phys. Rev. Lett. 94, 140501 (2005)",
        "F. G. Deng, G. L. Long, and X. S. Liu, Phys. Rev. A 68, 042317 (2003)",
        "Q. Y. Cai and B. W. Li, Phys. Rev. A 69, 054301 (2004)",
        "T. Gao, F. L. Yan, and Z. X. Wang, Chin. Phys. Lett. 22, 2473 (2005)",
        "C. Wang, F. G. Deng, Y. S. Li, X. S. Liu, and G. L. Long, Phys. Rev. A 71, 044305 (2005)",
        "C. Wang, F. G. Deng, and G. L. Long, Opt. Commun. 253, 15 (2005)",
        "X. H. Li, F. G. Deng, and H. Y. Zhou, Phys. Rev. A 74, 054302 (2006)",
        "X. H. Li, C. Y. Li, F. G. Deng, P. Zhou, Y. J. Liang, and H. Y. Zhou, Chin. Phys. Lett. 16, 2149 (2007)",
        "B. A. Nguyen, Phys. Lett. A 328, 6 (2004)",
        "Z. X. Man, Z. J. Zhang, and Y. Li, Chin. Phys. Lett. 22, 22 (2005)",
        "X. Ji and S. Zhang, Chin. Phys. Lett. 15, 1418 (2006)",
        "Z. X. Man, Y. J. Xia, and B. A. Nguyen, J. Phys. B, At. Mol. Opt. Phys. 39, 3855 (2006)",
        "Z. X. Man and Y. J. Xia, Chin. Phys. Lett. 23, 1680 (2006)",
        "Y. Xia, C. B. Fu, S. Zhang, S. K. Hong, K. H. Yeon, and C. I. Um, J. Korean Phys. Soc. 48, 24 (2006)",
        "X. R. Jin, X. Ji, Y. Q. Zhang, S. Zhang, S.-K. Hong, K.-H. Yeon, and C.-I. Um, Phys. Lett. A 354, 67 (2006)",
        "Z. X. Man and Y. J. Xia, Chin. Phys. Lett. 24, 15 (2007)",
        "Y. Chen, Z. X. Man, and Y. J. Xia, Chin. Phys. Lett. 24, 19 (2007)",
        "Y. G. Yang, and Q. Y. Wen, Sci. China Ser. G, Phys. Mech. Astron. 50, 558 (2007)",
        "A. Wojcik, Phys. Rev. Lett. 90, 157901 (2003)",
        "F. G. Deng, X. H. Li, C. Y. Li, P. Zhou, and H. Y. Zhou, Chin. Phys. Lett. 16, 277 (2007)",
        "Q. Y. Cai, Phys. Rev. Lett. 91, 109801 (2003)",
        "Z. J. Zhang and Z. X. Man, Int. J. Quantum Inf. 2, 521 (2004)",
        "H. Hoffmann, K. Boström, and T. Felbinger, Phys. Rev. A 72, 016301 (2005)",
        "F. G. Deng and G. L. Long, Phys. Rev. A 72, 016302 (2005)",
        "F. G. Deng, X. H. Li, C. Y. Li, P. Zhou, and H. Y. Zhou, Phys. Lett. A 359, 359 (2006)",
        "A. Beige, B. G. Englert, C. Kurtsiefer, and H. Weinfurter, Acta Phys. A 101, 357 (2002)",
        "M. Hillery, V. Buzek, and A. Berthiaume, Phys. Rev. A 59, 1829 (1999)",
        "R. Cleve, D. Gottesman, and H. K. Lo, Phys. Rev. Lett. 83, 648 (1999)",
        "C. H. Bennett, G. Brassard, and N. D. Mermin, Phys. Rev. Lett. 68, 557 (1992)",
        "F. Gao, F. Z. Guo, Q. Y. Wen, and F. C. Zhu, Phys. Lett. A 349, 53 (2006)",
        "C. H Bennet and S. J. Wiesner, Phys. Rev. Lett. 69, 2881 (1992)",
        "C. Y. Li, H. Y. Zhou, Y. Wang, and F. G. Deng, Chin. Phys. Lett. 22, 1049 (2005)",
        "C. Y. Li, X. H. Li, F. G. Deng, P. Zhou, Y. J. Liang, and H. Y. Zhou, Chin. Phys. Lett. 23, 2896 (2006)"
      ],
      "builds_on": [
        "Quantum secure direct communication",
        "Four-particle GHZ state",
        "Eavesdropping detection strategies"
      ]
    },
    "note": "Document type not explicitly stated, thus only basic information has been extracted.",
    "extraction_metadata": {
      "file_id": "10.5560_zna.2012_0029",
      "detected_doc_type": "unknown",
      "extraction_time": "2025-08-02 12:37:10",
      "validation": {
        "warnings": [],
        "suspicious_count": 0,
        "validation_score": 100,
        "quality_level": "high"
      },
      "text_length": 8901,
      "processed_text_length": 8901
    }
  },
  "1307.0414v1": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "Challenges in Representation Learning: A report on three machine learning contests",
      "authors": [
        "Ian J. Goodfellow",
        "et al."
      ],
      "institutions": [
        "原文无此信息"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "sparse filtering",
          "relationship_type": "原文无此信息",
          "evidence_text": "The top-performing teams used convolutional neural networks, with the winner employing an SVM primal objective as the loss function."
        },
        {
          "method_name": "entropy regularization",
          "relationship_type": "原文无此信息",
          "evidence_text": "Other top performers used methods such as sparse filtering, entropy regularization, and ensemble voting techniques with denoising autoencoders and maxout networks."
        },
        {
          "method_name": "ensemble voting techniques with denoising autoencoders and maxout networks",
          "relationship_type": "原文无此信息",
          "evidence_text": "Other top performers used methods such as sparse filtering, entropy regularization, and ensemble voting techniques with denoising autoencoders and maxout networks."
        }
      ],
      "compared_methods": [
        {
          "method_name": "null model (convolutional network with no feature learning)",
          "comparison_result": "the best 'null' model achieved an accuracy of 60%",
          "evidence_text": "The contest utilized the Facial Expression Recognition 2013 (FER-2013) dataset... Human performance on a similar dataset was estimated at 68±5%, while the best 'null' model... achieved an accuracy of 60%."
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "BBL-2013",
          "dataset_description": "an obfuscated subset of the Street View House Numbers dataset",
          "evidence_text": "The black box learning challenge had two objectives... The BBL-2013 dataset, created by Dumitru Erhan, was an obfuscated subset of the Street View House Numbers dataset."
        },
        {
          "dataset_name": "Facial Expression Recognition 2013 (FER-2013)",
          "dataset_description": "nearly 36,000 images categorized into seven emotions",
          "evidence_text": "In the facial expression recognition challenge... the contest utilized the Facial Expression Recognition 2013 (FER-2013) dataset, compiled by Pierre Luc Carrier and Aaron Courville."
        }
      ],
      "evaluation_metrics": [
        "accuracy"
      ],
      "baseline_methods": [
        "原文无此信息"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "accuracy",
          "our_result": "70.22%",
          "baseline_result": "原文无此信息",
          "dataset": "BBL-2013",
          "evidence_text": "David Thaler won with an accuracy of 70.22% using a blend of models."
        },
        {
          "metric_name": "accuracy",
          "our_result": "60%",
          "baseline_result": "68±5%",
          "dataset": "Facial Expression Recognition 2013 (FER-2013)",
          "evidence_text": "Human performance on a similar dataset was estimated at 68±5%, while the best 'null' model... achieved an accuracy of 60%."
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "原文无此信息"
      ],
      "stated_novelty": [
        "原文无此信息"
      ],
      "stated_advantages": [
        "原文无此信息"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文无此信息"
      ],
      "failure_cases": [
        "原文无此信息"
      ]
    },
    "extraction_metadata": {
      "file_id": "1307.0414v1",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 12:38:51",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑技术关系method_name: null model (convolutional network with no feature learning)",
          "可疑技术关系comparison_result: the best 'null' model achieved an accuracy of 60%",
          "可疑技术关系evidence_text: The contest utilized the Facial Expression Recognition 2013 (FER-2013) dataset... Human performance on a similar dataset was estimated at 68±5%, while the best 'null' model... achieved an accuracy of 60%."
        ],
        "suspicious_count": 4,
        "validation_score": 20,
        "quality_level": "low"
      },
      "text_length": 7870,
      "processed_text_length": 7870
    }
  },
  "1307.1275v1": {
    "document_metadata": {
      "document_type": "unknown",
      "title": "Constructing Hierarchical Image-tags Bimodal Representations for Word Tags Alternative Choice",
      "authors_or_creators": [
        "Fangxiang Feng",
        "Ruifan Li",
        "Xiaojie Wang"
      ]
    },
    "main_content": {
      "stated_purpose": "This paper presents a solution to the multi-modal learning challenge of ICML, involving the construction of three-level representations in three stages and a data-specific strategy for choosing correct tag words.",
      "key_concepts": [
        "Hierarchical Image-tags Bimodal Representations",
        "Word Tags Alternative Choice",
        "Multi-modal Learning",
        "Restricted Boltzmann Machines",
        "Quasi-Siamese Auto-encoder"
      ],
      "main_methods": [
        "Using MPEG-7, gist descriptors, and additional features for image representation",
        "Bag-of-words model for tags representation",
        "Stacked RBMs for level-2 representations",
        "Quasi-Siamese auto-encoder for level-3 representations"
      ]
    },
    "relationships": {
      "references_to": [
        "Salakhutdinov, R., and Hinton, G. (2007)",
        "Salakhutdinov, R., and Hinton, G. (2009)",
        "Smolensky, P. (1986)",
        "Srivastava, N., and Salakhutdinov, R. (2012)",
        "von Ahn, L., and Dabbish, L. (2004)",
        "Welling, M., Rosen-Zvi, M., and Hinton, G. (2004)",
        "LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., and Huang, F.-J. (2006)"
      ],
      "builds_on": [
        "The multi-modal learning challenge of ICML 2013",
        "The Small ESP Game Dataset",
        "The manually labeled dataset by Ian Goodfellow"
      ]
    },
    "note": "Document type is not explicitly mentioned, only the most basic information has been extracted.",
    "extraction_metadata": {
      "file_id": "1307.1275v1",
      "detected_doc_type": "unknown",
      "extraction_time": "2025-08-02 12:39:11",
      "validation": {
        "warnings": [
          "可疑document_type: unknown"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 7538,
      "processed_text_length": 7538
    }
  },
  "1911.09359v1": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "Multi-Scale RCNN Model for Financial Time-series Classification",
      "authors": [
        "Liu Guang",
        "Wang Xiaojie",
        "Li Ruifan"
      ],
      "institutions": [
        "原文无此信息"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "Multi-Scale (MS) methods, TD-oriented methods",
          "relationship_type": "结合",
          "evidence_text": "Existing FTC research can be categorized into MS-oriented and TD-oriented methods. However, few studies effectively integrate both properties."
        }
      ],
      "compared_methods": [
        {
          "method_name": "Support Vector Machine (SVM), Random Forest (RF), Fuzzy Deep Neural Network (FDNN), TreNet, State-Frequency Memory Recurrent Neural Networks (SFM), Multi-Scale CNN (MS-CNN)",
          "comparison_result": "Our MSTD-RCNN model is compared with six baseline models on three datasets. The results, listed in Table 5, show that our model achieves the best performance in accuracy and F1.",
          "evidence_text": "Our MSTD-RCNN model is compared with six baseline models on three datasets."
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "SH000001, SZ399005, SZ399006",
          "dataset_description": "Data spans from January 1, 2016, to December 30, 2016, with a total of 58,000 data points.",
          "evidence_text": "We describe the datasets from the Chinese stock market: SH000001, SZ399005, and SZ399006."
        }
      ],
      "evaluation_metrics": [
        "accuracy",
        "F-score (F1)",
        "Confusion Matrix (CM)",
        "accumulated profit"
      ],
      "baseline_methods": [
        "Support Vector Machine (SVM)",
        "Random Forest (RF)",
        "Fuzzy Deep Neural Network (FDNN)",
        "TreNet",
        "State-Frequency Memory Recurrent Neural Networks (SFM)",
        "Multi-Scale CNN (MS-CNN)"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "accuracy, F1",
          "our_result": "3.07%, 3.00%, 2.13% improvement",
          "baseline_result": "原文无此信息",
          "dataset": "SH000001, SZ399005, SZ399006",
          "evidence_text": "Our model effectively extracts MS features and captures Temporal Dependency (TD) within financial time-series."
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "A novel method combining both MS and TD properties in financial time-series.",
        "MS feature extraction with convolutional units without predefined parameters.",
        "Fusion of different scale features using a Recurrent Neural Network to capture temporal dependencies."
      ],
      "stated_novelty": [
        "原文无此信息"
      ],
      "stated_advantages": [
        "End-to-end system",
        "State-of-the-art performance in trend classification and simulated trading on Chinese stock market datasets."
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文无此信息"
      ],
      "failure_cases": [
        "原文无此信息"
      ]
    },
    "extraction_metadata": {
      "file_id": "1911.09359v1",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 12:40:51",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑技术关系method_name: Multi-Scale (MS) methods, TD-oriented methods",
          "可疑技术关系evidence_text: Existing FTC research can be categorized into MS-oriented and TD-oriented methods. However, few studies effectively integrate both properties.",
          "可疑技术关系method_name: Support Vector Machine (SVM), Random Forest (RF), Fuzzy Deep Neural Network (FDNN), TreNet, State-Frequency Memory Recurrent Neural Networks (SFM), Multi-Scale CNN (MS-CNN)",
          "可疑数据集: SH000001, SZ399005, SZ399006",
          "可疑性能数据our_result: 3.07%, 3.00%, 2.13% improvement"
        ],
        "suspicious_count": 6,
        "validation_score": 0,
        "quality_level": "low"
      },
      "text_length": 19761,
      "processed_text_length": 14959
    }
  },
  "2021.acl_long.494": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "Dual Graph Convolutional Networks for Aspect-based Sentiment Analysis",
      "authors": [
        "Ruifan Li",
        "Hao Chen",
        "Fangxiang Feng",
        "Zhanyu Ma",
        "Xiaojie Wang",
        "Eduard Hovy"
      ],
      "institutions": [
        "原文无此信息"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "Graph Convolutional Network (GCN)",
          "relationship_type": "改进",
          "evidence_text": "We propose a GCN-based method that combines syntactic and semantic features."
        },
        {
          "method_name": "Attention-based LSTM",
          "relationship_type": "改进",
          "evidence_text": "Attention-based neural networks, including those proposed by Wang et al. (2016) and others, have been introduced to implicitly model the semantic relation between aspects and their context."
        },
        {
          "method_name": "Recursive neural network by Dong et al. (2014)",
          "relationship_type": "改进",
          "evidence_text": "Notable works include the recursive neural network by Dong et al. (2014)"
        }
      ],
      "compared_methods": [
        {
          "method_name": "ATAE-LSTM, IAN, RAM, MGAN, TNet, ASGCN, CDT, BiGCN, kumaGCN, InterGCN, R-GAT, DGEDT, BERT, R-GAT+BERT, DGEDT+BERT",
          "comparison_result": "Our DualGCN model consistently outperforms attention-based and syntax-based methods",
          "evidence_text": "We compare DualGCN with state-of-the-art baselines, including ATAE-LSTM, IAN, RAM, MGAN, TNet, ASGCN, CDT, BiGCN, kumaGCN, InterGCN, R-GAT, DGEDT, BERT, R-GAT+BERT, and DGEDT+BERT."
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "Restaurant, Laptop, Twitter",
          "dataset_description": "三个公开数据集",
          "evidence_text": "Experiments are conducted on three public datasets: Restaurant, Laptop, and Twitter."
        }
      ],
      "evaluation_metrics": [
        "accuracy",
        "macro-averaged F1-score"
      ],
      "baseline_methods": [
        "ATAE-LSTM, IAN, RAM, MGAN, TNet, ASGCN, CDT, BiGCN, kumaGCN, InterGCN, R-GAT, DGEDT, BERT, R-GAT+BERT, DGEDT+BERT"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "accuracy",
          "our_result": "84.27, 78.08, 78.48",
          "baseline_result": "原文无此信息",
          "dataset": "Restaurant, Laptop, Twitter",
          "evidence_text": "DualGCN | 84.27 | 78.08 | 78.48 | 74.74 | 75.92 | 74.29"
        },
        {
          "metric_name": "macro-averaged F1-score",
          "our_result": "74.74, 75.92, 74.29",
          "baseline_result": "原文无此信息",
          "dataset": "Restaurant, Laptop, Twitter",
          "evidence_text": "DualGCN | 84.27 | 78.08 | 78.48 | 74.74 | 75.92 | 74.29"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "提出了一种结合语法和语义特征的DualGCN模型",
        "设计了正交和微分正则化器以增强模型捕获语义关联的能力"
      ],
      "stated_novelty": [
        "DualGCN模型的结构设计",
        "正交和微分正则化器的应用"
      ],
      "stated_advantages": [
        "能够有效整合语法知识和语义信息",
        "适应不同的评论风格"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文未明确提及"
      ],
      "failure_cases": [
        "原文提到的失败案例，如无则为空数组"
      ]
    },
    "extraction_metadata": {
      "file_id": "2021.acl_long.494",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 12:42:47",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑技术关系evidence_text: We propose a GCN-based method that combines syntactic and semantic features.",
          "可疑技术关系method_name: Recursive neural network by Dong et al. (2014)",
          "可疑技术关系method_name: ATAE-LSTM, IAN, RAM, MGAN, TNet, ASGCN, CDT, BiGCN, kumaGCN, InterGCN, R-GAT, DGEDT, BERT, R-GAT+BERT, DGEDT+BERT",
          "可疑数据集: Restaurant, Laptop, Twitter",
          "可疑性能数据our_result: 84.27, 78.08, 78.48",
          "可疑性能数据our_result: 74.74, 75.92, 74.29"
        ],
        "suspicious_count": 7,
        "validation_score": 0,
        "quality_level": "low"
      },
      "text_length": 15277,
      "processed_text_length": 14980
    }
  },
  "2022.acl_long.212": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "Enhanced Multi-Channel Graph Convolutional Network for Aspect Sentiment Triplet Extraction",
      "authors": [
        "Hao Chen",
        "Zepeng Zhai",
        "Fangxiang Feng",
        "Ruifan Li",
        "Xiaojie Wang"
      ],
      "institutions": [
        "School of Artificial Intelligence, Beijing University of Posts and Telecommunications, China"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "Graph Convolutional Network (GCN)",
          "relationship_type": "扩展",
          "evidence_text": "Motivated by CNN, GCN is an efficient variant operating on graphs (Kipf and Welling, 2017)."
        }
      ],
      "compared_methods": [
        {
          "method_name": "GTS-BERT",
          "comparison_result": "Our EMC-GCN significantly surpasses GTS-BERT with an average of 1.96% and 2.61% F1-score on D1 and D2, respectively",
          "evidence_text": "Under the F1 metric, our EMC-GCN model outperforms all pipeline, end-to-end, and MRC-based methods on two groups of datasets."
        },
        {
          "method_name": "BMRC",
          "comparison_result": "The term 'light' is challenging to identify by GTS-BERT and BMRC, yet 'easy' is predicted correctly by all methods",
          "evidence_text": "The term 'light' is challenging to identify by GTS-BERT and BMRC, yet 'easy' is predicted correctly by all methods due to its closer proximity to 'transport' than 'light'."
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "SemEval ABSA Challenges (Pontiki et al., 2014, 2015, 2016)",
          "dataset_description": "two ABSA datasets",
          "evidence_text": "We evaluate our method on two ABSA datasets from the SemEval ABSA Challenges (Pontiki et al., 2014, 2015, 2016)."
        }
      ],
      "evaluation_metrics": [
        "F1"
      ],
      "baseline_methods": [
        "CMLA+",
        "RINANTE+",
        "Li-unified-R",
        "Peng-two-stage",
        "OTE-MTL",
        "JET-BERT",
        "GTS-BERT",
        "BMRC",
        "BART-ABSA"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "F1",
          "our_result": "71.21",
          "baseline_result": "68.09",
          "dataset": "14res",
          "evidence_text": "Our EMC-GCN | 71.21 | 71.78 | 61.54 | 68.33"
        },
        {
          "metric_name": "F1",
          "our_result": "71.78",
          "baseline_result": "68.81",
          "dataset": "14lap",
          "evidence_text": "Our EMC-GCN | 71.21 | 71.78 | 61.54 | 68.33"
        },
        {
          "metric_name": "F1",
          "our_result": "61.54",
          "baseline_result": "59.28",
          "dataset": "15res",
          "evidence_text": "Our EMC-GCN | 71.21 | 71.78 | 61.54 | 68.33"
        },
        {
          "metric_name": "F1",
          "our_result": "68.33",
          "baseline_result": "55.42",
          "dataset": "16res",
          "evidence_text": "Our EMC-GCN | 71.21 | 71.78 | 61.54 | 68.33"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "a novel EMC-GCN model",
        "a comprehensive exploitation of linguistic features",
        "an effective refining strategy"
      ],
      "stated_novelty": [
        "Our proposed framework is EMC-GCN, which addresses Aspect and Opinion Term Co-Extraction (AOTE) and Aspect-Sentiment Pair Extraction (ASPE)",
        "We define ten types of relations between words for the ASTE task"
      ],
      "stated_advantages": [
        "Our EMC-GCN model consistently outperforms all baseline methods on benchmark datasets"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文无此信息"
      ],
      "failure_cases": [
        "The term 'light' is challenging to identify by GTS-BERT and BMRC"
      ]
    },
    "extraction_metadata": {
      "file_id": "2022.acl_long.212",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 12:44:51",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑技术关系method_name: Graph Convolutional Network (GCN)",
          "可疑技术关系comparison_result: The term 'light' is challenging to identify by GTS-BERT and BMRC, yet 'easy' is predicted correctly by all methods",
          "可疑技术关系evidence_text: The term 'light' is challenging to identify by GTS-BERT and BMRC, yet 'easy' is predicted correctly by all methods due to its closer proximity to 'transport' than 'light'."
        ],
        "suspicious_count": 4,
        "validation_score": 20,
        "quality_level": "low"
      },
      "text_length": 17424,
      "processed_text_length": 14966
    }
  },
  "2022.coling_1.234": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "A Simple Model for Distantly Supervised Relation Extraction",
      "authors": [
        "Ziqin Rao",
        "Fangxiang Feng",
        "Ruifan Li",
        "Xiaojie Wang"
      ],
      "institutions": [
        "School of Artificial Intelligence, Beijing University of Posts and Telecommunications, China"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "BERT-based Graph Convolutional network Model (BGM)",
          "relationship_type": "结合",
          "evidence_text": "We introduce BGM, a model that combines a Pretrained Language Model (PLM) and a Graph Convolutional Network (GCN)"
        },
        {
          "method_name": "PCNN-based methods",
          "relationship_type": "基于",
          "evidence_text": "DS-RE methods can be categorized into PCNN-based and PLMs-based approaches"
        },
        {
          "method_name": "PLMs-based methods",
          "relationship_type": "基于",
          "evidence_text": "PCNN-based methods use various techniques to acquire effective bag representations, while PLMs-based methods have shown remarkable performance"
        }
      ],
      "compared_methods": [
        {
          "method_name": "multiple baseline methods",
          "comparison_result": "report better performance",
          "evidence_text": "We compare our BGM with multiple baseline methods and report better performance"
        },
        {
          "method_name": "BGM without GCN",
          "comparison_result": "performance drop",
          "evidence_text": "In the ablation study, we find that removing GCN results in a performance drop"
        },
        {
          "method_name": "BGM without EntCon",
          "comparison_result": "performance drop",
          "evidence_text": "In the ablation study, we find that removing entity connections results in a performance drop"
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "NYT10",
          "dataset_description": "原文无此信息",
          "evidence_text": "Our experiments utilize the BERT-base-uncased model, with a maximum input sequence length of 120 and a hidden size of 768 on the NYT10 and GDS datasets"
        },
        {
          "dataset_name": "GDS",
          "dataset_description": "原文无此信息",
          "evidence_text": "Our experiments utilize the BERT-base-uncased model, with a maximum input sequence length of 120 and a hidden size of 768 on the NYT10 and GDS datasets"
        }
      ],
      "evaluation_metrics": [
        "P@N",
        "AUC",
        "Micro-F1 score"
      ],
      "baseline_methods": [
        "原文无此信息"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "P@N",
          "our_result": "原文无此信息",
          "baseline_result": "原文无此信息",
          "dataset": "NYT10 and GDS",
          "evidence_text": "We compare our BGM with multiple baseline methods and report better performance on the NYT10 and GDS datasets in terms of P@N, AUC, and Micro-F1 score"
        },
        {
          "metric_name": "AUC",
          "our_result": "原文无此信息",
          "baseline_result": "原文无此信息",
          "dataset": "NYT10 and GDS",
          "evidence_text": "We compare our BGM with multiple baseline methods and report better performance on the NYT10 and GDS datasets in terms of P@N, AUC, and Micro-F1 score"
        },
        {
          "metric_name": "Micro-F1 score",
          "our_result": "原文无此信息",
          "baseline_result": "原文无此信息",
          "dataset": "NYT10 and GDS",
          "evidence_text": "We compare our BGM with multiple baseline methods and report better performance on the NYT10 and GDS datasets in terms of P@N, AUC, and Micro-F1 score"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "提出了一种结合预训练语言模型和图卷积网络的简单模型"
      ],
      "stated_novelty": [
        "首次使用GCN直接学习实例之间的包表示"
      ],
      "stated_advantages": [
        "在基准数据集上表现出更好的性能"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "需要进一步研究背后的理论以提供更好的解释性"
      ],
      "failure_cases": [
        "原文无此信息"
      ]
    },
    "extraction_metadata": {
      "file_id": "2022.coling_1.234",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 12:46:45",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑技术关系evidence_text: In the ablation study, we find that removing GCN results in a performance drop",
          "可疑技术关系evidence_text: In the ablation study, we find that removing entity connections results in a performance drop"
        ],
        "suspicious_count": 3,
        "validation_score": 40,
        "quality_level": "low"
      },
      "text_length": 8126,
      "processed_text_length": 8126
    }
  },
  "2022.emnlp_main.212": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "COM-MRC: A Context-Masked Machine Reading Comprehension Framework for Aspect Sentiment Triplet Extraction",
      "authors": [
        "Feifan Fan",
        "Yansong Feng",
        "Dongyan Zhao"
      ],
      "institutions": [
        "School of Artificial Intelligence, Beijing University of Posts and Telecommunications, China",
        "Engineering Research Center of Information Networks, Ministry of Education, China"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "Aspect Sentiment Triplet Extraction (ASTE)",
          "relationship_type": "基于",
          "evidence_text": "Aspect Sentiment Triplet Extraction (ASTE) extracts sentiment triplets from sentences and is formalized as an effective machine reading comprehension (MRC) framework."
        }
      ],
      "compared_methods": [
        {
          "method_name": "BMRC",
          "comparison_result": "Our COM-MRC outperforms other baselines in terms of Precision, Recall, and F1 scores on both D1 and D2 datasets",
          "evidence_text": "Our COM-MRC outperforms other baselines in terms of Precision, Recall, and F1 scores on both D1 and D2 datasets, as shown in Tables 3 and 4."
        },
        {
          "method_name": "EMC-GCN",
          "comparison_result": "Our COM-MRC outperforms EMC-GCN",
          "evidence_text": "Our COM-MRC outperforms EMC-GCN on datasets D1 and D2."
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "D1 (Wu et al., 2020a)",
          "dataset_description": "benchmark datasets from SemEval Challenges",
          "evidence_text": "We use two groups of benchmark datasets from SemEval Challenges (Pontiki et al., 2014, 2015, 2016): D1 (Wu et al., 2020a) and D2 (Xu et al., 2020)."
        },
        {
          "dataset_name": "D2 (Xu et al., 2020)",
          "dataset_description": "benchmark datasets from SemEval Challenges",
          "evidence_text": "We use two groups of benchmark datasets from SemEval Challenges (Pontiki et al., 2014, 2015, 2016): D1 (Wu et al., 2020a) and D2 (Xu et al., 2020)."
        }
      ],
      "evaluation_metrics": [
        "Precision",
        "Recall",
        "F1 scores"
      ],
      "baseline_methods": [
        "pipeline",
        "end-to-end",
        "MRC-based methods"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "F1 scores",
          "our_result": "72.01",
          "baseline_result": "67.99",
          "dataset": "D1",
          "evidence_text": "Our COM-MRC: 75.46, 68.91, 72.01, 62.35, 58.16, 60.17, 68.35, 61.24, 64.53, 71.55, 71.59, 71.57"
        },
        {
          "metric_name": "F1 scores",
          "our_result": "71.57",
          "baseline_result": "65.75",
          "dataset": "D2",
          "evidence_text": "Our COM-MRC: 75.46, 68.91, 72.01, 62.35, 58.16, 60.17, 68.35, 61.24, 64.53, 71.55, 71.59, 71.57"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "propose a COntext-Masked MRC (COM-MRC) framework for ASTE tasks",
        "alleviate interference in ASTE tasks",
        "the context augmentation strategy effectively expanding the training corpus"
      ],
      "stated_novelty": [
        "COM-MRC’s components work collaboratively",
        "the two-stage inference method reduces interference from other aspects"
      ],
      "stated_advantages": [
        "effectiveness of our COM-MRC framework",
        "our inference method involving two stages",
        "context augmentation strategy"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "Our context augmentation strategy may increase training time, preventing COM-MRC from being applied to large-scale data scenarios"
      ],
      "failure_cases": [
        "原文无此信息"
      ]
    },
    "extraction_metadata": {
      "file_id": "2022.emnlp_main.212",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 12:48:53",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑技术关系comparison_result: Our COM-MRC outperforms EMC-GCN",
          "可疑技术关系evidence_text: Our COM-MRC outperforms EMC-GCN on datasets D1 and D2."
        ],
        "suspicious_count": 3,
        "validation_score": 40,
        "quality_level": "low"
      },
      "text_length": 22704,
      "processed_text_length": 14952
    }
  },
  "2022.findings_emnlp.6": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "KE-GCL: Knowledge Enhanced Graph Contrastive Learning for Commonsense Question Answering",
      "authors": [
        "Lihui Zhang",
        "Ruifan Li"
      ],
      "institutions": [
        "School of Artificial Intelligence, Beijing University of Posts and Telecommunications",
        "Engineering Research Center of Information Networks, Ministry of Education"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "Graph Contrastive Learning",
          "relationship_type": "扩展",
          "evidence_text": "Graph contrastive learning extends contrastive learning to graph-structured data, with works focusing on unsupervised representation learning."
        }
      ],
      "compared_methods": [
        {
          "method_name": "RoBERTa-Large (w/o KG)",
          "comparison_result": "KE-GCL consistently outperforms previous methods",
          "evidence_text": "Our KE-GCL model consistently outperforms other baselines on CommonsenseQA and OpenBookQA datasets."
        },
        {
          "method_name": "Relation Network (RN), RGCN, GconAttn, KagNet, MHGRN, QA-GNN",
          "comparison_result": "KE-GCL consistently outperforms previous methods",
          "evidence_text": "We compare our KE-GCL with state-of-the-art baselines: RoBERTa-Large (w/o KG), Relation Network (RN), RGCN, GconAttn, KagNet, MHGRN, and QA-GNN."
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "CommonsenseQA",
          "dataset_description": "原文无此信息",
          "evidence_text": "We evaluate our model on CommonsenseQA and OpenbookQA datasets using Accuracy (Acc) as the metric."
        },
        {
          "dataset_name": "OpenbookQA",
          "dataset_description": "原文无此信息",
          "evidence_text": "We evaluate our model on CommonsenseQA and OpenbookQA datasets using Accuracy (Acc) as the metric."
        }
      ],
      "evaluation_metrics": [
        "Accuracy (Acc)"
      ],
      "baseline_methods": [
        "RoBERTa-Large (w/o KG)",
        "Relation Network (RN)",
        "RGCN",
        "GconAttn",
        "KagNet",
        "MHGRN",
        "QA-GNN"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "Accuracy",
          "our_result": "平均准确度提升1.08% on CommonsenseQA and 0.83% and 0.64% on OpenBookQA",
          "baseline_result": "原文无此信息",
          "dataset": "CommonsenseQA and OpenBookQA",
          "evidence_text": "Our KE-GCL model consistently outperforms other baselines on CommonsenseQA and OpenBookQA datasets. It achieves an average accuracy improvement of 1.08% on CommonsenseQA and 0.83% and 0.64% on OpenBookQA."
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "提出了一种新的KE-GCL模型",
        "在CQA任务中整合了GCL",
        "通过自适应图增强和选择错误的答案来增强图表示学习"
      ],
      "stated_novelty": [
        "原文未明确提及"
      ],
      "stated_advantages": [
        "减少了KG噪声",
        "在两个基准数据集上性能一致提升"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文明确承认的局限性",
        "讨论了未来工作，包括在少样本或无监督场景中应用GCL"
      ],
      "failure_cases": [
        "原文提到的失败案例",
        "分析了正确和错误预测的细微差别"
      ]
    },
    "extraction_metadata": {
      "file_id": "2022.findings_emnlp.6",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 12:50:37",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑技术关系method_name: Relation Network (RN), RGCN, GconAttn, KagNet, MHGRN, QA-GNN",
          "可疑性能数据our_result: 平均准确度提升1.08% on CommonsenseQA and 0.83% and 0.64% on OpenBookQA"
        ],
        "suspicious_count": 3,
        "validation_score": 40,
        "quality_level": "low"
      },
      "text_length": 16697,
      "processed_text_length": 14875
    }
  },
  "2023.acl_long.802": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "USSA: A Unified Table Filling Scheme for Structured Sentiment Analysis",
      "authors": [
        "Zepeng Zhai",
        "Hao Chen",
        "Ruifan Li",
        "Xiaojie Wang"
      ],
      "institutions": [],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "bi-lexical dependency parsing",
          "relationship_type": "改进",
          "evidence_text": "We propose USSA, a unified 2D table-filling scheme that utilizes 13 relation types."
        }
      ],
      "compared_methods": [
        {
          "method_name": "RACL-BERT, Head-first, Head-final, Frozen PERIN, TGLS",
          "comparison_result": "USSA generally outperforms other baselines",
          "evidence_text": "Table 4 shows that USSA generally outperforms other baselines in terms of Span F1."
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "NoReCFine, MultiBEU, MultiBCA, MPQA, DSUnis",
          "dataset_description": "原文无此信息",
          "evidence_text": "We conduct experiments on five benchmark datasets across four languages."
        }
      ],
      "evaluation_metrics": [
        "Sentiment Graph F1 (SF1)",
        "Holder F1",
        "Target F1",
        "Exp. F1",
        "Nonpolarity Sentiment Graph F1 (NSF1)"
      ],
      "baseline_methods": [
        "RACL-BERT",
        "Head-first",
        "Head-final",
        "Frozen PERIN",
        "TGLS"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "Span F1",
          "our_result": "原文无此信息",
          "baseline_result": "原文无此信息",
          "dataset": "MPQA",
          "evidence_text": "It achieves significant improvements, including a 7.2% F1 score increase for target extraction on MPQA."
        },
        {
          "metric_name": "Holder F1",
          "our_result": "原文无此信息",
          "baseline_result": "原文无此信息",
          "dataset": "NoReCFine",
          "evidence_text": "a 5.4% F1 score increase for holder extraction on NoReCFine."
        },
        {
          "metric_name": "NSF1",
          "our_result": "3.48",
          "baseline_result": "TGLS",
          "dataset": "average",
          "evidence_text": "USSA consistently surpasses other methods in NSF1 and SF1 metrics, with average improvements of 3.48 NSF1 and 3.14% SF1 over TGLS."
        },
        {
          "metric_name": "SF1",
          "our_result": "3.14%",
          "baseline_result": "TGLS",
          "dataset": "average",
          "evidence_text": "USSA consistently surpasses other methods in NSF1 and SF1 metrics, with average improvements of 3.48 NSF1 and 3.14% SF1 over TGLS."
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "a bi-lexical dependency parsing graph converted to a unified 2D table filling scheme (USSA)",
        "an effective model that collaborates with USSA, utilizing the bi-axial attention module",
        "extensive experimental validation on benchmark datasets"
      ],
      "stated_novelty": [
        "原文未明确提及"
      ],
      "stated_advantages": [
        "addressing overlap and discontinuity in SSA",
        "effective capture of correlations within the table with bi-axial attention module"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "increased training time and memory usage associated with the table filling method",
        "comparative inefficiency in handling long sentences",
        "computational challenges"
      ],
      "failure_cases": [
        "原文未明确提及"
      ]
    },
    "extraction_metadata": {
      "file_id": "2023.acl_long.802",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 12:52:30",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑技术关系method_name: RACL-BERT, Head-first, Head-final, Frozen PERIN, TGLS",
          "可疑数据集: NoReCFine, MultiBEU, MultiBCA, MPQA, DSUnis"
        ],
        "suspicious_count": 3,
        "validation_score": 40,
        "quality_level": "low"
      },
      "text_length": 14066,
      "processed_text_length": 14066
    }
  },
  "2025.coling_main.22": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "Multimodal Aspect-Based Sentiment Analysis under Conditional Relation",
      "authors": [
        "Xinjing Liu",
        "Ruifan Li",
        "Shuqin Ye",
        "Guangwei Zhang",
        "Xiaojie Wang"
      ],
      "institutions": [
        "原文无此信息"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "Traditional methods",
          "relationship_type": "基于",
          "evidence_text": "Traditional methods assume the image contains objects referred to by the aspects, which is not always true."
        }
      ],
      "compared_methods": [
        {
          "method_name": "ChatGPT 3.5, Llama 2, VisualGLM-6B, Llava 1.5, MMICL, mPLUG-Owl2, GPT4V",
          "comparison_result": "our model outperforms",
          "evidence_text": "Comparative experiments with large language models (LLMs) and multi-modal LLMs (MLLMs) on the JMASA and MASC tasks demonstrate that our model outperforms ChatGPT 3.5, Llama 2, VisualGLM-6B, Llava 1.5, MMICL, mPLUG-Owl2, and GPT4V, showcasing the advantage of multimodal input and the effectiveness of our approach."
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "TWITTER-15 and TWITTER-17",
          "dataset_description": "benchmark datasets for MABSA tasks, specifically C-MABSA with conditional relations",
          "evidence_text": "We construct datasets for MABSA tasks, specifically C-MABSA with conditional relations. We automatically annotate two popular datasets, TWITTER-15 and TWITTER-17, using UNINEXT for conditional relation detection and YOLOv8 for visual object annotation."
        }
      ],
      "evaluation_metrics": [
        "Accuracy",
        "F1 score"
      ],
      "baseline_methods": [
        "原文无此信息"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "F1 score",
          "our_result": "best performance in Twitter-15 and is second to CMMT in Twitter-17",
          "baseline_result": "原文无此信息",
          "dataset": "Twitter-15 and Twitter-17",
          "evidence_text": "On MATE, our model achieves the best performance in Twitter-15 and is second to CMMT in Twitter-17."
        },
        {
          "metric_name": "Accuracy and F1 score",
          "our_result": "best results on both datasets",
          "baseline_result": "原文无此信息",
          "dataset": "Twitter-15 and Twitter-17",
          "evidence_text": "For MASC, our model achieves the best results on both datasets in terms of Accuracy and F1 score."
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "propose the CORSA framework",
        "address the impact of irrelevant images",
        "localize condition-related visual regions"
      ],
      "stated_novelty": [
        "原文未明确提及"
      ],
      "stated_advantages": [
        "the advantage of multimodal input",
        "the effectiveness of our approach"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "use of a pre-trained model (UNINEXT) for automatic data annotation",
        "introduces inaccuracies",
        "affects the CORSA model's performance"
      ],
      "failure_cases": [
        "原文无此信息"
      ]
    },
    "extraction_metadata": {
      "file_id": "2025.coling_main.22",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 12:53:03",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑技术关系method_name: ChatGPT 3.5, Llama 2, VisualGLM-6B, Llava 1.5, MMICL, mPLUG-Owl2, GPT4V"
        ],
        "suspicious_count": 2,
        "validation_score": 60,
        "quality_level": "medium"
      },
      "text_length": 12438,
      "processed_text_length": 12438
    }
  },
  "2408.03632v3": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "Concept Conductor: Orchestrating Multiple Personalized Concepts in Text-to-Image Synthesis",
      "authors": [
        "Zebin Yao",
        "Fangxiang Feng",
        "Ruifan Li",
        "Xiaojie Wang"
      ],
      "institutions": [
        "Beijing University of Posts and Telecommunications"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "Stable Diffusion",
          "relationship_type": "改进",
          "evidence_text": "Our method is implemented on Stable Diffusion v1.5, utilizing layout references from SDXL."
        },
        {
          "method_name": "ED-LoRA",
          "relationship_type": "结合",
          "evidence_text": "ED-LoRA, a method for single-concept customization, is used as our default single-concept model, involving learnable hierarchical text embeddings and low-rank adaptation (LoRA) applied to pre-trained weights."
        }
      ],
      "compared_methods": [
        {
          "method_name": "Custom Diffusion",
          "comparison_result": "原文无此信息",
          "evidence_text": "We compare our method against three baselines: Custom Diffusion, Cones 2, and Mix-of-Show, using their official code implementations."
        },
        {
          "method_name": "Cones 2",
          "comparison_result": "原文无此信息",
          "evidence_text": "We compare our method against three baselines: Custom Diffusion, Cones 2, and Mix-of-Show, using their official code implementations."
        },
        {
          "method_name": "Mix-of-Show",
          "comparison_result": "原文无此信息",
          "evidence_text": "We compare our method against three baselines: Custom Diffusion, Cones 2, and Mix-of-Show, using their official code implementations."
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "Mix-of-show",
          "dataset_description": "原文无此信息",
          "evidence_text": "Our dataset encompasses 30 personalized concepts across various categories, with images sourced from the Mix-of-show, DreamBooth, and CustomConcept101 datasets."
        },
        {
          "dataset_name": "DreamBooth",
          "dataset_description": "原文无此信息",
          "evidence_text": "Our dataset encompasses 30 personalized concepts across various categories, with images sourced from the Mix-of-show, DreamBooth, and CustomConcept101 datasets."
        },
        {
          "dataset_name": "CustomConcept101",
          "dataset_description": "原文无此信息",
          "evidence_text": "Our dataset encompasses 30 personalized concepts across various categories, with images sourced from the Mix-of-show, DreamBooth, and CustomConcept101 datasets."
        }
      ],
      "evaluation_metrics": [
        "CLIP",
        "ImageReward",
        "Segmentation Similarity (SegSim)"
      ],
      "baseline_methods": [
        "Custom Diffusion",
        "Cones 2",
        "Mix-of-Show"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "Segmentation Similarity (SegSim)",
          "our_result": "原文无此信息",
          "baseline_result": "原文无此信息",
          "dataset": "原文无此信息",
          "evidence_text": "Evaluation metrics include text alignment using CLIP and ImageReward, and a new metric called Segmentation Similarity (SegSim) for image alignment."
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "We introduce Concept Conductor, a training-free framework that ensures visual fidelity and correct layout in multi-concept customization."
      ],
      "stated_novelty": [
        "原文未明确提及"
      ],
      "stated_advantages": [
        "It prevents interference between concepts, ensures correct layouts, and maintains visual harmony."
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "Our method experiences quality issues when generating small subjects, such as distorted and deformed small faces.",
        "Our method introduces considerable computational overhead."
      ],
      "failure_cases": [
        "原文未明确提及"
      ]
    },
    "extraction_metadata": {
      "file_id": "2408.03632v3",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 12:53:39",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 16342,
      "processed_text_length": 14871
    }
  },
  "2504.15958v2": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "FreeGraftor: Training-Free Cross-Image Feature Grafting for Subject-Driven Text-to-Image Generation",
      "authors": [
        "Zebin Yao",
        "Lei Ren",
        "Huixing Jiang",
        "Chen Wei",
        "Xiaojie Wang",
        "Ruifan Li",
        "Fangxiang Feng"
      ],
      "institutions": [
        "Beijing University of Posts and Telecommunications",
        "Li Auto Inc."
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "FLUX.1",
          "relationship_type": "基于",
          "evidence_text": "We build upon FLUX.1, where joint attention is computed as:"
        }
      ],
      "compared_methods": [
        {
          "method_name": "FreeCustom",
          "comparison_result": "FreeCustom suffers from severe attribute confusion",
          "evidence_text": "FreeCustom suffers from severe attribute confusion, while MS-Diffusion and OmniGen lack visual faithfulness for small objects."
        },
        {
          "method_name": "MS-Diffusion",
          "comparison_result": "MS-Diffusion and OmniGen lack visual faithfulness for small objects",
          "evidence_text": "FreeCustom suffers from severe attribute confusion, while MS-Diffusion and OmniGen lack visual faithfulness for small objects."
        },
        {
          "method_name": "OmniGen",
          "comparison_result": "OmniGen lacks visual faithfulness for small objects",
          "evidence_text": "FreeCustom suffers from severe attribute confusion, while MS-Diffusion and OmniGen lack visual faithfulness for small objects."
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "DreamBench",
          "dataset_description": "原文无此信息",
          "evidence_text": "We use datasets from DreamBench, CustomConcept101, and Mix-of-Show for qualitative evaluation"
        },
        {
          "dataset_name": "CustomConcept101",
          "dataset_description": "原文无此信息",
          "evidence_text": "We use datasets from DreamBench, CustomConcept101, and Mix-of-Show for qualitative evaluation"
        },
        {
          "dataset_name": "Mix-of-Show",
          "dataset_description": "原文无此信息",
          "evidence_text": "We use datasets from DreamBench, CustomConcept101, and Mix-of-Show for qualitative evaluation"
        }
      ],
      "evaluation_metrics": [
        "CLIP",
        "DINOv2"
      ],
      "baseline_methods": [
        "FreeCustom",
        "MS-Diffusion",
        "OmniGen"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "原文无此信息",
          "our_result": "原文无此信息",
          "baseline_result": "原文无此信息",
          "dataset": "原文无此信息",
          "evidence_text": "Quantitative results show that FreeGraftor outperforms other methods in both image and text alignment."
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "training-free framework",
        "cross-image feature grafting",
        "structure-consistent initialization"
      ],
      "stated_novelty": [
        "原文无此信息"
      ],
      "stated_advantages": [
        "preserves subject identity",
        "pixel-level detail",
        "flexible text guidance",
        "without training or test-time optimization"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "dependency on external models",
        "significant GPU memory requirement"
      ],
      "failure_cases": [
        "原文无此信息"
      ]
    },
    "extraction_metadata": {
      "file_id": "2504.15958v2",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 12:54:12",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑技术关系comparison_result: OmniGen lacks visual faithfulness for small objects"
        ],
        "suspicious_count": 2,
        "validation_score": 60,
        "quality_level": "medium"
      },
      "text_length": 23250,
      "processed_text_length": 14880
    }
  },
  "2647868.2654902": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "Cross-modal Retrieval with Correspondence Autoencoder",
      "authors": [
        "Fangxiang Feng",
        "Xiaojie Wang",
        "Ruifan Li"
      ],
      "institutions": [
        "Beijing University of Posts and Telecommunications, Beijing, China"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "Correspondence LDA, deep autoencoders, deep belief networks, and deep Boltzmann Machines",
          "relationship_type": "基于",
          "evidence_text": "Shared layer models include topic models like Correspondence LDA and deep architectures such as deep autoencoders, deep belief networks, and deep Boltzmann Machines."
        }
      ],
      "compared_methods": [
        {
          "method_name": "CCA-based models and multi-modal models",
          "comparison_result": "Our correspondence autoencoders significantly outperform other models",
          "evidence_text": "Our correspondence autoencoders significantly outperform other models on both retrieval tasks across Wikipedia, Pascal, and NUS-WIDE-10k data sets."
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "Wikipedia, Pascal, NUS-WIDE-10k",
          "dataset_description": "These datasets vary in text modality, size, and category numbers.",
          "evidence_text": "We evaluate our models on three real-world datasets: Wikipedia, Pascal, and NUS-WIDE-10k."
        }
      ],
      "evaluation_metrics": [
        "mean average precision (mAP)",
        "top 20% percentage"
      ],
      "baseline_methods": [
        "CCA-based models",
        "multi-modal deep learning models"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "mAP scores and top 20%",
          "our_result": "significantly better performance",
          "baseline_result": "原文无此信息",
          "dataset": "Wikipedia, Pascal, and NUS-WIDE-10k",
          "evidence_text": "Our correspondence autoencoders significantly outperform other models on both retrieval tasks across Wikipedia, Pascal, and NUS-WIDE-10k data sets."
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "We propose the correspondence autoencoder (Corr-AE) based on two basic unimodal autoencoders.",
        "The Corr-AE integrates representation and correlation learning into a single process.",
        "A novel loss function is designed, incorporating the losses of autoencoders for all modalities and the loss of correlation between modalities."
      ],
      "stated_novelty": [
        "原文无此信息"
      ],
      "stated_advantages": [
        "The advantage of our correspondence models is their integration of these processes, making two-stage methods suboptimal in comparison."
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文未明确提及"
      ],
      "failure_cases": [
        "原文未明确提及"
      ]
    },
    "extraction_metadata": {
      "file_id": "2647868.2654902",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 12:55:38",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑技术关系method_name: Correspondence LDA, deep autoencoders, deep belief networks, and deep Boltzmann Machines",
          "可疑技术关系method_name: CCA-based models and multi-modal models",
          "可疑数据集: Wikipedia, Pascal, NUS-WIDE-10k"
        ],
        "suspicious_count": 4,
        "validation_score": 20,
        "quality_level": "low"
      },
      "text_length": 16428,
      "processed_text_length": 14835
    }
  },
  "2808205": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "Cross-modal retrieval with correspondence autoencoder",
      "authors": [
        "Fangxiang Feng",
        "Xiaojie Wang",
        "Ruifan Li"
      ],
      "institutions": [
        "原文无此信息"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "autoencoder",
          "relationship_type": "改进",
          "evidence_text": "We propose several novel models based on different autoencoders, which correlate hidden representations of a pair of autoencoders."
        },
        {
          "method_name": "Canonical Correlation Analysis (CCA)",
          "relationship_type": "结合",
          "evidence_text": "The models are trained using a novel optimal objective that minimizes a linear combination of representation learning errors and correlation learning error."
        }
      ],
      "compared_methods": [
        {
          "method_name": "CCA-based models",
          "comparison_result": "Our models achieve substantial improvements in mAP scores.",
          "evidence_text": "Compared to CCA-AE, our Corr-AE enhances the average mAP by 53.6%, 81.5%, and 48.3% on the respective datasets."
        },
        {
          "method_name": "Bimodal AE, Bimodal DBN",
          "comparison_result": "Our Corr-AE also achieves notable improvements over Bimodal AE and Bimodal DBN.",
          "evidence_text": "Our Corr-AE also achieves notable improvements over Bimodal AE and Bimodal DBN."
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "Wikipedia dataset",
          "dataset_description": "2,866 image/text pairs from ten semantic categories.",
          "evidence_text": "The Wikipedia dataset [Rasiwasia et al. 2010] consists of 2,866 image/text pairs from ten semantic categories."
        },
        {
          "dataset_name": "Pascal dataset",
          "dataset_description": "1,000 image/text pairs from twenty categories.",
          "evidence_text": "The Pascal dataset [Farhadi et al. 2010] contains 1,000 image/text pairs from twenty categories."
        },
        {
          "dataset_name": "NUS-WIDE-10k dataset",
          "dataset_description": "1,000 image/text pairs per category from ten categories.",
          "evidence_text": "The NUS-WIDE-10k dataset is a subset of NUS-WIDE [Chua et al. 2009], with 1,000 image/text pairs per category from ten categories."
        }
      ],
      "evaluation_metrics": [
        "mean average precision (mAP)",
        "top 20% percentage"
      ],
      "baseline_methods": [
        "CCA-AE",
        "CCA-Cross-AE",
        "CCA-Full-AE",
        "Bimodal AE",
        "Bimodal DBN"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "mAP",
          "our_result": "12.3% improvement",
          "baseline_result": "best baseline results",
          "dataset": "Wikipedia",
          "evidence_text": "Our three multimodal reconstruction Corr-AEs significantly outperform other models on both text and image retrieval tasks across all datasets."
        },
        {
          "metric_name": "mAP",
          "our_result": "32.4% improvement",
          "baseline_result": "best baseline results",
          "dataset": "NUS-WIDE-10k",
          "evidence_text": "For instance, Corr-Full-AE improves mAP scores by 12.3% and 16.6% for text-by-image and image-by-text retrieval on Wikipedia, by 12.4% and 2.2% on Pascal, and by 32.4% and 10.2% on NUS-WIDE-10k."
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "We propose the correspondence autoencoder (Corr-AE), which integrates representation learning and correlation learning into a single process."
      ],
      "stated_novelty": [
        "原文未明确提及"
      ],
      "stated_advantages": [
        "Our models are categorized into two groups: multimodal reconstruction correspondence autoencoder, which reconstructs both modalities, and unimodal reconstruction correspondence autoencoder, which reconstructs a single modality."
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文明确承认的局限性，如无则为空数组"
      ],
      "failure_cases": [
        "原文提到的失败案例，如无则为空数组"
      ]
    },
    "extraction_metadata": {
      "file_id": "2808205",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 12:57:51",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑技术关系comparison_result: Our models achieve substantial improvements in mAP scores.",
          "可疑技术关系method_name: Bimodal AE, Bimodal DBN",
          "可疑性能数据our_result: 12.3% improvement",
          "可疑性能数据our_result: 32.4% improvement"
        ],
        "suspicious_count": 5,
        "validation_score": 0,
        "quality_level": "low"
      },
      "text_length": 22080,
      "processed_text_length": 16856
    }
  },
  "2820400": {
    "document_metadata": {
      "document_type": "unknown",
      "title": "原文无此信息",
      "authors_or_creators": [
        "Hayley Hung",
        "George Toderici"
      ]
    },
    "main_content": {
      "stated_purpose": "This special issue invites the best papers from ACM Multimedia 2014 to extend their work into journal articles.",
      "key_concepts": [
        "Deep Learning for Multimedia",
        "Emotional and Social Signals in Multimedia"
      ],
      "main_methods": [
        "rigorous review process",
        "two-day technical program committee meeting"
      ]
    },
    "relationships": {
      "references_to": [
        "ACM Multimedia 2014",
        "Emotion Recognition During Speech Using Dynamics of Multiple Regions of Face",
        "Correspondence Autoencoders for Cross-Modal Retrieval"
      ],
      "builds_on": [
        "conference versions of the extended papers"
      ]
    },
    "note": "文档类型不明确，仅提取最基础信息",
    "extraction_metadata": {
      "file_id": "2820400",
      "detected_doc_type": "unknown",
      "extraction_time": "2025-08-02 12:58:02",
      "validation": {
        "warnings": [
          "可疑document_type: unknown"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 1368,
      "processed_text_length": 1368
    }
  },
  "3191835.3191989": {
    "document_metadata": {
      "document_type": "原文无此信息",
      "title": "Sentiment Analysis of Microblog Combining Dictionary and Rules",
      "authors_or_creators": [
        "Ding Yuan",
        "Yanquan Zhou",
        "Ruifan Li",
        "Peng Lu"
      ]
    },
    "main_content": {
      "stated_purpose": "Microblogging emotional classification is a key research area based on User-Generated Content (UGC). This paper focuses on distinguishing between positive and negative emotional tendencies in microblogs.",
      "key_concepts": [
        "emotional classification",
        "feature extraction",
        "weight computing",
        "support vector machine"
      ],
      "main_methods": [
        "Support Vector Machine (SVM)",
        "dictionary and rule-based approaches",
        "vector space model",
        "Modified CHI algorithm (MCHI)",
        "Modified TF-IDF (MTF-IDF) algorithm"
      ]
    },
    "relationships": {
      "references_to": [
        "Pang B, Lee L, Vaithyanathan S. Sentiment classification using machine learning techniques. EMNLP '02.",
        "Liu Zhiming, Liu Lu. Empirical study of sentiment classification for Chinese microblog based on machine learning.",
        "Zhu Yanlan, Min Jin, Zhou Yaqian, Huang Xuanjin, Wu Lide. Semantic Orientation Computing Based on HowNet.",
        "Yao Tianfang, Lou Decheng. Research on Semantic Orientation Distinction for Chinese Sentiment Words.",
        "Hu M, Liu B. Mining and summarizing customer reviews. KDD '04.",
        "Turney P D. Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews. ACL '02.",
        "Liu Xiaojuan, You Bin, Zhang Aiyun. Review on the Data Used in Researches of Microblogs.",
        "Zhu Lina. Research of Feature Selection for Chinese Web Page Categorization.",
        "Ma Wenwen, Deng Yigui. New feature weight calculation method for short text.",
        "Ma Ting, Geng Guohua, Zhou Mingquan. An effective approach to calculate the feature weights.",
        "Qin Shian, Li Fayun. Improved TF-IDF Method in Text Classification."
      ],
      "builds_on": [
        "Affective Computing by Picard",
        "traditional machine learning methods like Naive Bayes, Maximum Entropy, K Nearest Neighbor, and SVM",
        "Chinese text processing improvements with machine learning techniques and weight calculation algorithms like TF-IDF"
      ]
    },
    "note": "文档类型不明确，仅提取最基础信息",
    "extraction_metadata": {
      "file_id": "3191835.3191989",
      "detected_doc_type": "unknown",
      "extraction_time": "2025-08-02 12:58:25",
      "validation": {
        "warnings": [],
        "suspicious_count": 0,
        "validation_score": 100,
        "quality_level": "high"
      },
      "text_length": 7288,
      "processed_text_length": 7288
    }
  },
  "3394171.3416296": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "Learning Visual Features from Product Title for Image Retrieval",
      "authors": [
        "Fangxiang Feng",
        "Tianrui Niu",
        "Ruifan Li",
        "Xiaojie Wang",
        "Huixing Jiang"
      ],
      "institutions": [
        "原文无此信息"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "Deep Convolutional Neural Networks (CNNs)",
          "relationship_type": "基于",
          "evidence_text": "Our method, detailed in Figure 2, addresses this by leveraging the titles of product images."
        }
      ],
      "compared_methods": [
        {
          "method_name": "SEResnet-152 and Densenet-201",
          "comparison_result": "The deepest model, Densenet-201, shows the best performance among the ImageNet-1k trained models, while the shallowest model, ResNet-50, performs the worst",
          "evidence_text": "Table 1 presents the results of five deep CNN models with four different pooling methods on the validation set."
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "Perfect-500K",
          "dataset_description": "dataset from the 'AI Meets Beauty' challenge",
          "evidence_text": "For evaluation, we use the Perfect-500K dataset from the 'AI Meets Beauty' challenge."
        }
      ],
      "evaluation_metrics": [
        "Mean Average Precision (MAP)"
      ],
      "baseline_methods": [
        "SEResnet-152",
        "Densenet-201",
        "ResNet-50 trained on ImageNet-1k",
        "ResNet-50 trained on ImageNet-21k"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "MAP@7",
          "our_result": "0.402402",
          "baseline_result": "原文无此信息",
          "dataset": "private test set",
          "evidence_text": "Our team, using the fine-tuned ResNet-50 with the simplest MAC feature, ranks fourth with a MAP@7 of 0.402402."
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "method for learning product visual representations",
        "deep CNNs pre-trained and fine-tuned on this dataset can enhance feature effectiveness for product image retrieval"
      ],
      "stated_novelty": [
        "using product titles to guide the learning of visual features",
        "conversion of product titles into discrete labels for supervised learning"
      ],
      "stated_advantages": [
        "performing well without human annotations or additional processing",
        "can create a high-quality label set without word segmentation for Chinese titled products"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文无此信息"
      ],
      "failure_cases": [
        "原文无此信息"
      ]
    },
    "extraction_metadata": {
      "file_id": "3394171.3416296",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 12:58:51",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑技术关系comparison_result: The deepest model, Densenet-201, shows the best performance among the ImageNet-1k trained models, while the shallowest model, ResNet-50, performs the worst"
        ],
        "suspicious_count": 2,
        "validation_score": 60,
        "quality_level": "medium"
      },
      "text_length": 8300,
      "processed_text_length": 8300
    }
  },
  "3469877.3490585": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "S2TD: A Tree-Structured Decoder for Image Paragraph Captioning",
      "authors": [
        "Yihui Shi",
        "Yun Liu",
        "Fangxiang Feng",
        "Ruifan Li",
        "Zhanyu Ma",
        "Xiaojie Wang"
      ],
      "institutions": [
        "原文无此信息"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "hierarchical and non-hierarchical decoders",
          "relationship_type": "基于",
          "evidence_text": "Existing approaches, such as hierarchical and non-hierarchical decoders, have limitations in managing visual observations and maintaining paragraph coherence."
        }
      ],
      "compared_methods": [
        {
          "method_name": "DAM-ATT, SCST, SCST+RP, CRL, OR-ATT, OR-ATT+RP",
          "comparison_result": "Our S2TD achieves competitive performance, especially in terms of BLEU-1 and CIDEr.",
          "evidence_text": "Performance evaluation compares S2TD with state-of-the-art sequential decoding methods, grouped into hierarchical and non-hierarchical methods."
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "Stanford image paragraph benchmark dataset",
          "dataset_description": "原文无此信息",
          "evidence_text": "Our experiments are conducted on the Stanford image paragraph benchmark dataset."
        }
      ],
      "evaluation_metrics": [
        "BLEU-1",
        "BLEU-2",
        "BLEU-3",
        "BLEU-4",
        "METEOR",
        "CIDEr"
      ],
      "baseline_methods": [
        "DAM-ATT",
        "SCST",
        "SCST+RP",
        "CRL",
        "OR-ATT",
        "OR-ATT+RP"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "BLEU-1",
          "our_result": "44.32",
          "baseline_result": "35.02",
          "dataset": "Stanford image paragraph benchmark dataset",
          "evidence_text": "Our S2TD: 44.32, 25.86, 14.80, 8.33, 16.89, 21.41, [...] compared to DAM-ATT: 35.02, 20.24, 11.68, 6.57, 13.91, 17.32"
        },
        {
          "metric_name": "CIDEr",
          "our_result": "21.41",
          "baseline_result": "17.32",
          "dataset": "Stanford image paragraph benchmark dataset",
          "evidence_text": "Our S2TD: 44.32, 25.86, 14.80, 8.33, 16.89, 21.41, [...] compared to DAM-ATT: 35.02, 20.24, 11.68, 6.57, 13.91, 17.32"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "原文无此信息"
      ],
      "stated_novelty": [
        "We propose Splitting to Tree Decoder (S2TD), a novel tree-structured decoder that models paragraph decoding as top-down binary tree expansion."
      ],
      "stated_advantages": [
        "S2TD includes a split module, a score module, and a word-level RNN. The tree structure loss enables end-to-end learning."
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文无此信息"
      ],
      "failure_cases": [
        "原文无此信息"
      ]
    },
    "extraction_metadata": {
      "file_id": "3469877.3490585",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 13:00:09",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑技术关系method_name: DAM-ATT, SCST, SCST+RP, CRL, OR-ATT, OR-ATT+RP"
        ],
        "suspicious_count": 2,
        "validation_score": 60,
        "quality_level": "medium"
      },
      "text_length": 14823,
      "processed_text_length": 14823
    }
  },
  "3483207.3483233": {
    "document_metadata": {
      "document_type": "原文无此信息",
      "title": "Maintenance Decision Generator for Electrical Equipment Based on Reinforcement Learning",
      "authors_or_creators": [
        "Ruifan Li",
        "Zeyuan Wang",
        "Yifan Du",
        "Zepeng Zhai",
        "Yongping Xiong",
        "Ziqun Liu"
      ]
    },
    "main_content": {
      "stated_purpose": "提出一种基于强化学习的电气设备维护决策模型，并通过电网切割集计算设备权重，增强动态编程决策。",
      "key_concepts": [
        "电气设备维护",
        "强化学习",
        "动态决策制定"
      ],
      "main_methods": [
        "Markov假设",
        "动态编程",
        "多设备维护决策生成"
      ]
    },
    "relationships": {
      "references_to": [
        "[6]",
        "[9]",
        "[1]",
        "[2]",
        "[3]",
        "[4]",
        "[5]",
        "[7]",
        "[8]",
        "[10]",
        "[11]",
        "[12]",
        "[13]",
        "[14]",
        "[15]",
        "[16]",
        "[17]",
        "[18]"
      ],
      "builds_on": [
        "Multi-Agent Reinforcement Learning",
        "Electric Equipment Maintenance"
      ]
    },
    "note": "文档类型不明确，仅提取最基础信息",
    "extraction_metadata": {
      "file_id": "3483207.3483233",
      "detected_doc_type": "unknown",
      "extraction_time": "2025-08-02 13:00:26",
      "validation": {
        "warnings": [],
        "suspicious_count": 0,
        "validation_score": 100,
        "quality_level": "high"
      },
      "text_length": 14263,
      "processed_text_length": 14263
    }
  },
  "3548636.3548646": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "EP-BERTGCN: A Simple but Effective Power Equipment Fault Recognition Method",
      "authors": [
        "Mingcong Lu",
        "Yusong Zhang",
        "Qu-An Zheng",
        "Zhenyuan Ma",
        "Liqing Liu",
        "Yongping Xiong",
        "Ruifan Li"
      ],
      "institutions": [
        "原文未明确提到"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "C-TextGCN",
          "relationship_type": "改进",
          "evidence_text": "Our work spans text-based power equipment fault recognition, graph neural networks, pre-trained BERT, and cross-domain BERT adaption. Previous research in power equipment fault recognition has largely focused on image modalities, with recent inroads into text-based models for TPFR. Graph Convolutional Neural Network (GCN) has been applied to text classification by constructing graphs based on textual relationships. This approach is extended with the integration of pre-trained BERT and domain-specific adaption in our EP-BERTGCN method."
        },
        {
          "method_name": "BERT",
          "relationship_type": "结合",
          "evidence_text": "EP-BERTGCN constructs a graph among documents and words using pre-trained BERT and combines softmax outputs from BERT and GCNs for classification."
        }
      ],
      "compared_methods": [
        {
          "method_name": "TextRNN",
          "comparison_result": "EP-BERTGCN性能更优",
          "evidence_text": "Table 1 presents Experimental Results on the CPTF Dataset: ... EP-BERTGCN 0.6700 ... compared to TextRNN 0.4950"
        },
        {
          "method_name": "TextRNN_Att",
          "comparison_result": "EP-BERTGCN性能更优",
          "evidence_text": "Table 1 presents Experimental Results on the CPTF Dataset: ... EP-BERTGCN 0.6700 ... compared to TextRNN_Att 0.5538"
        },
        {
          "method_name": "TextRCNN",
          "comparison_result": "EP-BERTGCN性能更优",
          "evidence_text": "Table 1 presents Experimental Results on the CPTF Dataset: ... EP-BERTGCN 0.6700 ... compared to TextRCNN 0.6126"
        },
        {
          "method_name": "DPCNN",
          "comparison_result": "EP-BERTGCN性能更优",
          "evidence_text": "Table 1 presents Experimental Results on the CPTF Dataset: ... EP-BERTGCN 0.6700 ... compared to DPCNN 0.5753"
        },
        {
          "method_name": "FastText",
          "comparison_result": "EP-BERTGCN性能更优",
          "evidence_text": "Table 1 presents Experimental Results on the CPTF Dataset: ... EP-BERTGCN 0.6700 ... compared to FastText 0.6098"
        },
        {
          "method_name": "Transformer",
          "comparison_result": "EP-BERTGCN性能更优",
          "evidence_text": "Table 1 presents Experimental Results on the CPTF Dataset: ... EP-BERTGCN 0.6700 ... compared to Transformer 0.4570"
        },
        {
          "method_name": "C-TextGCN",
          "comparison_result": "EP-BERTGCN性能更优",
          "evidence_text": "Table 1 presents Experimental Results on the CPTF Dataset: ... EP-BERTGCN 0.6700 ... compared to C-TextGCN 0.6607"
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "CPTF dataset",
          "dataset_description": "包含1484个实例，跨越12个类别",
          "evidence_text": "The CPTF dataset, after oversampling to address category imbalance, contains 1484 instances across 12 categories."
        }
      ],
      "evaluation_metrics": [
        "Acc",
        "Macro-F1",
        "Weighted Macro-F1"
      ],
      "baseline_methods": [
        "TextRNN",
        "TextRNN_Att",
        "TextRCNN",
        "DPCNN",
        "FastText",
        "Transformer",
        "C-TextGCN"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "Acc",
          "our_result": "0.6700",
          "baseline_result": "0.4950 (TextRNN)",
          "dataset": "CPTF dataset",
          "evidence_text": "Table 1 presents Experimental Results on the CPTF Dataset: ... EP-BERTGCN 0.6700 ... compared to TextRNN 0.4950"
        },
        {
          "metric_name": "Macro-F1",
          "our_result": "0.6645",
          "baseline_result": "0.4217 (TextRNN)",
          "dataset": "CPTF dataset",
          "evidence_text": "Table 1 presents Experimental Results on the CPTF Dataset: ... EP-BERTGCN 0.6645 ... compared to TextRNN 0.4217"
        },
        {
          "metric_name": "Weighted Macro-F1",
          "our_result": "0.6633",
          "baseline_result": "0.4642 (TextRNN)",
          "dataset": "CPTF dataset",
          "evidence_text": "Table 1 presents Experimental Results on the CPTF Dataset: ... EP-BERTGCN 0.6633 ... compared to TextRNN 0.4642"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "引入BERT模块到C-TextGCN中形成EP-BERTGCN",
        "在收集的数据集上进行比较实验，展示方法性能的优越性",
        "对BERT模型进行领域适应，进一步增强了我们方法的能力"
      ],
      "stated_novelty": [
        "原文未明确声明"
      ],
      "stated_advantages": [
        "结合了预训练语言模型的先验知识和GCN在电力领域的鲁棒性"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文未明确承认"
      ],
      "failure_cases": [
        "原文未提到"
      ]
    },
    "extraction_metadata": {
      "file_id": "3548636.3548646",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 13:03:11",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑institutions: ['原文未明确提到']",
          "可疑技术关系comparison_result: EP-BERTGCN性能更优",
          "可疑技术关系evidence_text: Table 1 presents Experimental Results on the CPTF Dataset: ... EP-BERTGCN 0.6700 ... compared to TextRNN 0.4950",
          "可疑技术关系comparison_result: EP-BERTGCN性能更优",
          "可疑技术关系evidence_text: Table 1 presents Experimental Results on the CPTF Dataset: ... EP-BERTGCN 0.6700 ... compared to TextRNN_Att 0.5538",
          "可疑技术关系comparison_result: EP-BERTGCN性能更优",
          "可疑技术关系evidence_text: Table 1 presents Experimental Results on the CPTF Dataset: ... EP-BERTGCN 0.6700 ... compared to TextRCNN 0.6126",
          "可疑技术关系comparison_result: EP-BERTGCN性能更优",
          "可疑技术关系evidence_text: Table 1 presents Experimental Results on the CPTF Dataset: ... EP-BERTGCN 0.6700 ... compared to DPCNN 0.5753",
          "可疑技术关系comparison_result: EP-BERTGCN性能更优",
          "可疑技术关系evidence_text: Table 1 presents Experimental Results on the CPTF Dataset: ... EP-BERTGCN 0.6700 ... compared to FastText 0.6098",
          "可疑技术关系comparison_result: EP-BERTGCN性能更优",
          "可疑技术关系evidence_text: Table 1 presents Experimental Results on the CPTF Dataset: ... EP-BERTGCN 0.6700 ... compared to Transformer 0.4570",
          "可疑技术关系comparison_result: EP-BERTGCN性能更优",
          "可疑技术关系evidence_text: Table 1 presents Experimental Results on the CPTF Dataset: ... EP-BERTGCN 0.6700 ... compared to C-TextGCN 0.6607",
          "可疑性能数据baseline_result: 0.4950 (TextRNN)",
          "可疑性能数据baseline_result: 0.4217 (TextRNN)",
          "可疑性能数据baseline_result: 0.4642 (TextRNN)"
        ],
        "suspicious_count": 19,
        "validation_score": 0,
        "quality_level": "low"
      },
      "text_length": 11873,
      "processed_text_length": 11873
    }
  },
  "3664647.3680897": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "Triple Alignment Strategies for Zero-shot Phrase Grounding under Weak Supervision",
      "authors": [
        "Pengyue Lin",
        "Ruifan Li",
        "Yuzhe Ji",
        "Zhihan Yu",
        "Fangxiang Feng",
        "Zhanyu Ma",
        "Xiaojie Wang"
      ],
      "institutions": [
        "Beijing Natural Science Foundation",
        "National Nature Science Foundation of China",
        "BUPT",
        "High Performance Computing Platform of BUPT"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "CLIP",
          "relationship_type": "基于",
          "evidence_text": "Our framework uses a novel PG framework using triple alignment strategies under weak supervision: 1) Region-Text Alignment (RTA) to associate region-level attributes based on Contrastive Language-Image Pre-Training (CLIP)."
        }
      ],
      "compared_methods": [
        {
          "method_name": "WWbl",
          "comparison_result": "Our CLIP-based heatmap surpasses the pseudo label used by WWbl, explaining a 9% increase in bbox accuracy.",
          "evidence_text": "Our CLIP-based heatmap surpasses the pseudo label used by WWbl, explaining a 9% increase in bbox accuracy."
        },
        {
          "method_name": "ZSGNet",
          "comparison_result": "Our approach exceeds previous methods and will explore interpretable solutions for grounding-related tasks.",
          "evidence_text": "Our approach exceeds previous methods and will explore interpretable solutions for grounding-related tasks."
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "Flickr-Split-S0",
          "dataset_description": "零样本PG设置下的数据集",
          "evidence_text": "我们在零样本PG设置下使用Flickr-Split-S0、Flickr-Split-S1、VG-Split-S2和VG-Split-S3评估我们的框架。"
        },
        {
          "dataset_name": "Flickr-Split-S1",
          "dataset_description": "零样本PG设置下的数据集",
          "evidence_text": "我们在零样本PG设置下使用Flickr-Split-S0、Flickr-Split-S1、VG-Split-S2和VG-Split-S3评估我们的框架。"
        },
        {
          "dataset_name": "VG-Split-S2",
          "dataset_description": "零样本PG设置下的数据集",
          "evidence_text": "我们在零样本PG设置下使用Flickr-Split-S0、Flickr-Split-S1、VG-Split-S2和VG-Split-S3评估我们的框架。"
        },
        {
          "dataset_name": "VG-Split-S3",
          "dataset_description": "零样本PG设置下的数据集",
          "evidence_text": "我们在零样本PG设置下使用Flickr-Split-S0、Flickr-Split-S1、VG-Split-S2和VG-Split-S3评估我们的框架。"
        }
      ],
      "evaluation_metrics": [
        "IoU",
        "bbox accuracy",
        "recognition accuracy"
      ],
      "baseline_methods": [
        "ZSGNet",
        "WWbl",
        "SMST",
        "BBR",
        "VPT"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "IoU",
          "our_result": "0.5",
          "baseline_result": "原文无此信息",
          "dataset": "Flickr-Split-0&1",
          "evidence_text": "For Flickr-Split-0&1, IoU threshold of 0.5 is used."
        },
        {
          "metric_name": "bbox accuracy",
          "our_result": "52.25",
          "baseline_result": "34.26",
          "dataset": "Flickr30K",
          "evidence_text": "MaskCLIP 34.26 37.46 40.93 52.25 48.40 25.87"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "提出了一种零样本短语接地的新框架",
        "引入了三重对齐策略"
      ],
      "stated_novelty": [
        "在弱监督下实现零样本接地",
        "使用CLIP进行区域文本对齐"
      ],
      "stated_advantages": [
        "优于其他弱监督和零样本接地方法",
        "能够泛化到未见过的短语类别"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "对于密集物体的接地判断不够精确",
        "没有考虑上下文短语"
      ],
      "failure_cases": [
        "相似密集物体的接地",
        "上下文相关实体的接地"
      ]
    },
    "extraction_metadata": {
      "file_id": "3664647.3680897",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 13:03:55",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑技术关系evidence_text: Our framework uses a novel PG framework using triple alignment strategies under weak supervision: 1) Region-Text Alignment (RTA) to associate region-level attributes based on Contrastive Language-Image Pre-Training (CLIP)."
        ],
        "suspicious_count": 2,
        "validation_score": 60,
        "quality_level": "medium"
      },
      "text_length": 22082,
      "processed_text_length": 14881
    }
  },
  "3664647.3681466": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "DiffHarmony++: Enhancing Image Harmonization with Harmony-VAE and Inverse Harmonization Model",
      "authors": [
        "Pengfei Zhou",
        "Fangxiang Feng",
        "Guang Liu",
        "Ruifan Li",
        "Xiaojie Wang"
      ],
      "institutions": [
        "Beijing University of Posts & Telecommunications",
        "Beijing Academy of Artificial Intelligence"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "Stable Diffusion",
          "relationship_type": "基于",
          "evidence_text": "Harmony-VAE is a method designed for image harmonization, which includes repairing the content of the foreground area while maintaining a harmonized appearance. It builds upon the Stable Diffusion model"
        }
      ],
      "compared_methods": [
        {
          "method_name": "HDNet",
          "comparison_result": "DiffHarmony++ shows superior performance as the foreground proportion increases.",
          "evidence_text": "On the iHarmony4 dataset, HDNet512 outperforms DiffHarmony++ in samples with small foreground proportions (0% ∼5%), but DiffHarmony++ shows superior performance as the foreground proportion increases."
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "iHarmony4",
          "dataset_description": "Includes HCOCO, HFlickr, HAdobe5K, and Hday2night sub-datasets, consisting of 65,742 training and 7,404 testing pairs of composite and real images.",
          "evidence_text": "In our experiments, we use the iHarmony4 dataset, which includes HCOCO, HFlickr, HAdobe5K, and Hday2night sub-datasets, consisting of 65,742 training and 7,404 testing pairs of composite and real images."
        },
        {
          "dataset_name": "Human Harmony",
          "dataset_description": "Created by filtering the imaterialist-fashion-2020-fgvc7 dataset to exclude images with only products. The cleaned dataset contains 29,106 images.",
          "evidence_text": "We also address the lack of human portrait-specific harmonization datasets by constructing a Human Harmony dataset. Utilizing the imaterialist-fashion-2020-fgvc7 dataset, we pair high-resolution portrait photographs with detailed segmentation maps to create accurate foreground masks."
        }
      ],
      "evaluation_metrics": [
        "PSNR",
        "MSE",
        "fMSE"
      ],
      "baseline_methods": [
        "原文无此信息"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "PSNR",
          "our_result": "原文无此信息",
          "baseline_result": "原文无此信息",
          "dataset": "iHarmony4",
          "evidence_text": "Our approach is compared with various image harmonization methods on the iHarmony4 dataset. Our method significantly outperforms previous SOTA methods on the entire test set, achieving the best results on almost all subsets."
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "Introduction of the Human Harmony dataset",
        "Harmony-VAE's effectiveness in enhancing the VAE component in latent diffusion models for image harmonization"
      ],
      "stated_novelty": [
        "原文无此信息"
      ],
      "stated_advantages": [
        "Reduces training costs",
        "Improves LDM-based harmonization models",
        "Generalizes well to higher-resolution images"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文无此信息"
      ],
      "failure_cases": [
        "原文无此信息"
      ]
    },
    "extraction_metadata": {
      "file_id": "3664647.3681466",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 13:04:29",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 23272,
      "processed_text_length": 14962
    }
  },
  "4930_Article_Text_7995_1_10_20190709": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "Differential Networks for Visual Question Answering",
      "authors": [
        "Chenfei Wu",
        "Jinlai Liu",
        "Xiaojie Wang",
        "Ruifan Li"
      ],
      "institutions": [
        "Center for Intelligence Science and Technology, Beijing University of Posts and Telecommunications"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "Attention-based models",
          "relationship_type": "基于",
          "evidence_text": "We review current VQA models, focusing on attention-based models and fusion strategies."
        }
      ],
      "compared_methods": [
        {
          "method_name": "HighOrderAtt",
          "comparison_result": "原文无此信息",
          "evidence_text": "HighOrderAtt (Schwartz, Schwing, and Hazan 2017) - - - - 69.4"
        },
        {
          "method_name": "MLB(7)",
          "comparison_result": "原文无此信息",
          "evidence_text": "MLB(7) (Kim et al. 2017) 66.77 84.54 39.21 57.81"
        },
        {
          "method_name": "Mutan(5)",
          "comparison_result": "原文无此信息",
          "evidence_text": "Mutan(5) (Ben-younes et al. 2017) 67.42 85.14 39.81 58.52"
        },
        {
          "method_name": "DualMFA",
          "comparison_result": "原文无此信息",
          "evidence_text": "DualMFA (Lu et al. 2018) 66.01 83.59 40.18 56.84 70.04"
        },
        {
          "method_name": "ReasonNet",
          "comparison_result": "原文无此信息",
          "evidence_text": "ReasonNet (Ilievski and Feng 2017) - - - - -"
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "VQA 1.0",
          "dataset_description": "原文无此信息",
          "evidence_text": "VQA 1.0 Test-dev and Test-std"
        },
        {
          "dataset_name": "VQA 2.0",
          "dataset_description": "原文无此信息",
          "evidence_text": "VQA 2.0 Test-dev and Test-std"
        },
        {
          "dataset_name": "COCO-QA",
          "dataset_description": "原文无此信息",
          "evidence_text": "COCO-QA dataset"
        },
        {
          "dataset_name": "TDIUC",
          "dataset_description": "原文无此信息",
          "evidence_text": "TDIUC dataset"
        }
      ],
      "evaluation_metrics": [
        "Acc(ans)"
      ],
      "baseline_methods": [
        "HighOrderAtt",
        "MLB(7)",
        "Mutan(5)",
        "DualMFA",
        "ReasonNet"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "All",
          "our_result": "68.62",
          "baseline_result": "69.4",
          "dataset": "VQA 1.0",
          "evidence_text": "DF (36boxes) (ours) 68.62 86.08 43.52 59.38 73.31"
        },
        {
          "metric_name": "All",
          "our_result": "67.73",
          "baseline_result": "67.50",
          "dataset": "VQA 2.0",
          "evidence_text": "DF (36 boxes) (ours) 67.73 83.91 46.7 58.7"
        },
        {
          "metric_name": "All Obj.",
          "our_result": "69.36",
          "baseline_result": "62.50",
          "dataset": "COCO-QA",
          "evidence_text": "DF (36 boxes) (ours) 69.36 70.53 54.92 73.67 61.22 78.25 92.99"
        },
        {
          "metric_name": "Overall(Arithmetric MPT)",
          "our_result": "72.97",
          "baseline_result": "67.90",
          "dataset": "TDIUC",
          "evidence_text": "DF (36 boxes) 67.90 67.81 69.11 72.97"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "We propose a general DN module and a new DF model for the VQA task."
      ],
      "stated_novelty": [
        "原文明确声明的新颖性，如无则为空数组"
      ],
      "stated_advantages": [
        "By reducing input feature noise and mapping both the image and question to the same differential space, DF effectively improves attention accuracy and confidence."
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文明确承认的局限性，如无则为空数组"
      ],
      "failure_cases": [
        "In Example 4, all four models answered incorrectly, showing that counting remains a challenge for attention-based models."
      ]
    },
    "extraction_metadata": {
      "file_id": "4930_Article_Text_7995_1_10_20190709",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 13:05:16",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 8155,
      "processed_text_length": 8155
    }
  },
  "978_3_642_23223_7_60": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "原文无此信息",
      "authors": [
        "He Chuan",
        "Li Ruifan",
        "Zhong Yixin"
      ],
      "institutions": [
        "School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "KNN",
          "relationship_type": "结合",
          "evidence_text": "K-Nearest Neighbors (KNN) and Singular Value Decomposition (SVD) are two classification methods used in collaborative filtering."
        },
        {
          "method_name": "SVD",
          "relationship_type": "结合",
          "evidence_text": "K-Nearest Neighbors (KNN) and Singular Value Decomposition (SVD) are two classification methods used in collaborative filtering."
        },
        {
          "method_name": "logistic regression",
          "relationship_type": "结合",
          "evidence_text": "We apply logistic regression to build a classifier using the generated feature vectors."
        }
      ],
      "compared_methods": [
        {
          "method_name": "KNN",
          "comparison_result": "原文无此信息",
          "evidence_text": "原文无此信息"
        },
        {
          "method_name": "SVD",
          "comparison_result": "原文无此信息",
          "evidence_text": "原文无此信息"
        },
        {
          "method_name": "logistic regression",
          "comparison_result": "原文无此信息",
          "evidence_text": "原文无此信息"
        },
        {
          "method_name": "combination",
          "comparison_result": "The combined classifier outperforms single classifiers.",
          "evidence_text": "The combined classifier outperforms single classifiers."
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "algebra 2008-2009",
          "dataset_description": "student interaction logs from intelligent tutoring systems",
          "evidence_text": "Participants were provided with student interaction logs from intelligent tutoring systems, with two datasets: algebra 2008-2009 and bridge to algebra 2008-2009."
        },
        {
          "dataset_name": "bridge to algebra 2008-2009",
          "dataset_description": "student interaction logs from intelligent tutoring systems",
          "evidence_text": "Participants were provided with student interaction logs from intelligent tutoring systems, with two datasets: algebra 2008-2009 and bridge to algebra 2008-2009."
        }
      ],
      "evaluation_metrics": [
        "root mean squared error (RMSE)"
      ],
      "baseline_methods": [
        "原文无此信息"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "RMSE",
          "our_result": "0.2820",
          "baseline_result": "原文无此信息",
          "dataset": "Algebra 2008-2009",
          "evidence_text": "The combined classifier achieves an RMSE of 0.2820 on Algebra 2008-2009 dataset."
        },
        {
          "metric_name": "RMSE",
          "our_result": "0.3257",
          "baseline_result": "原文无此信息",
          "dataset": "Algebra 2008-2009",
          "evidence_text": "KNN: RMSE values on Algebra 2008-2009 dataset is 0.3257 with meta-parameters K β."
        },
        {
          "metric_name": "RMSE",
          "our_result": "0.446277",
          "baseline_result": "原文无此信息",
          "dataset": "Algebra 2008-2009",
          "evidence_text": "SVD: RMSE values on Algebra 2008-2009 dataset is 0.446277 with meta-parameters N, η, λ."
        },
        {
          "metric_name": "RMSE",
          "our_result": "0.2895",
          "baseline_result": "原文无此信息",
          "dataset": "Algebra 2008-2009",
          "evidence_text": "Logistic Regression: RMSE on Algebra 2008-2009 dataset is 0.2895."
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "原文无此信息"
      ],
      "stated_novelty": [
        "原文无此信息"
      ],
      "stated_advantages": [
        "The combined classifier outperforms single classifiers.",
        "Logistic regression shows the best performance due to its exploitation of detailed feature vectors."
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文无此信息"
      ],
      "failure_cases": [
        "原文无此信息"
      ]
    },
    "extraction_metadata": {
      "file_id": "978_3_642_23223_7_60",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 13:05:54",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 3728,
      "processed_text_length": 3728
    }
  },
  "A_hybrid_approach_to_identifying_sentiment_polarity_for_new_words": {
    "document_metadata": {
      "document_type": "原文无此信息",
      "title": "原文无此信息",
      "authors_or_creators": []
    },
    "main_content": {
      "stated_purpose": "讨论学术论文的清洗工作，包括去除非学术内容，修正错误，并保留核心学术内容",
      "key_concepts": [
        "学术论文清洗",
        "非学术内容去除",
        "错误修正",
        "核心学术内容保留"
      ],
      "main_methods": [
        "去除页眉、页脚、页码",
        "修正重复信息和乱码",
        "纠正PDF解析错误",
        "去除多余符号和格式问题"
      ]
    },
    "relationships": {
      "references_to": [],
      "builds_on": []
    },
    "note": "文档类型不明确，仅提取最基础信息，且未明确提及作者、标题等元数据信息",
    "extraction_metadata": {
      "file_id": "A_hybrid_approach_to_identifying_sentiment_polarity_for_new_words",
      "detected_doc_type": "unknown",
      "extraction_time": "2025-08-02 13:06:18",
      "validation": {
        "warnings": [],
        "suspicious_count": 0,
        "validation_score": 100,
        "quality_level": "high"
      },
      "text_length": 6591,
      "processed_text_length": 6591
    }
  },
  "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "A Noisy Context Optimization Approach for Chinese Spelling Correction",
      "authors": [
        "Guangwei Zhang",
        "Yongping Xiong",
        "Ruifan Li"
      ],
      "institutions": [
        "School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China",
        "School of Artificial Intelligence, Beijing University of Posts and Telecommunications, Beijing, China"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "BERT",
          "relationship_type": "基于",
          "evidence_text": "BERT-based models, dominant in CSC research, face performance challenges with noisy contexts."
        }
      ],
      "compared_methods": [
        {
          "method_name": "BERT",
          "comparison_result": "NCO-Spell outperforms compared baseline models.",
          "evidence_text": "Main results in Table IV show improvements, especially for PLOME(cfs) and NCO-Spell."
        },
        {
          "method_name": "PLOME",
          "comparison_result": "NCO-Spell shows better performance in certain conditions.",
          "evidence_text": "Main results in Table IV show improvements, especially for PLOME(cfs) and NCO-Spell."
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "ChineseNlpCorpus12",
          "dataset_description": "used as the pre-training data",
          "evidence_text": "Dataset: We utilize ChineseNlpCorpus12 as the pre-training data"
        },
        {
          "dataset_name": "SIGHAN",
          "dataset_description": "training data includes 10K manually annotated samples",
          "evidence_text": "Training data includes 10K manually annotated samples from SIGHAN"
        }
      ],
      "evaluation_metrics": [
        "Precision",
        "recall",
        "F1 scores"
      ],
      "baseline_methods": [
        "BERT",
        "PLOME",
        "REALISE",
        "ECOPO",
        "LEAD",
        "CoSPA",
        "PGBERT",
        "PLOME(cfs)",
        "PLOME(iter)",
        "NCO-Spell(iter)"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "Character-level(%)",
          "our_result": "98.3",
          "baseline_result": "97.1",
          "dataset": "Multi-typo Set",
          "evidence_text": "NCO-Spell 98.3 77.5 86.6 96.9 75.1 84.6"
        },
        {
          "metric_name": "Sentence-level(%)",
          "our_result": "86.8",
          "baseline_result": "85.7",
          "dataset": "Multi-typo Set",
          "evidence_text": "NCO-Spell(iter) 97.5 81.0 88.5 97.1 78.6 86.8"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "propose NCO-Spell for CSC task in noisy contexts"
      ],
      "stated_novelty": [
        "multi-character masking strategy",
        "dynamic confusion sets",
        "iterative inference method"
      ],
      "stated_advantages": [
        "NCO-Spell outperforms compared baseline models"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文无此信息"
      ],
      "failure_cases": [
        "In sentences with discontinuous typos, iterative inference shows less improvement compared to the direct inference method."
      ]
    },
    "extraction_metadata": {
      "file_id": "A_Noisy_Context_Optimization_Approach_for_Chinese_Spelling_Correction",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 13:06:52",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑技术关系comparison_result: NCO-Spell shows better performance in certain conditions."
        ],
        "suspicious_count": 2,
        "validation_score": 60,
        "quality_level": "medium"
      },
      "text_length": 8237,
      "processed_text_length": 8237
    }
  },
  "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "A Weighted Cross-entropy Loss for Mitigating LLM Hallucinations in Cross-lingual Continual Pretraining",
      "authors": [
        "Yuantao Fan",
        "Ruifan Li",
        "Guangwei Zhang",
        "Chuan Shi",
        "Xiaojie Wang"
      ],
      "institutions": [
        "Beijing University of Posts and Telecommunications"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "Cross-lingual Continual Pretraining",
          "relationship_type": "扩展",
          "evidence_text": "Cross-Lingual Continual Pretraining addresses the limitation of LLMs [1, 2, 3, 4] in languages other than English."
        }
      ],
      "compared_methods": [
        {
          "method_name": "traditional cross-entropy loss",
          "comparison_result": "InfoLoss aims to avoid learning incorrect language distributions caused by noisy tokens",
          "evidence_text": "We compare our method, InfoLoss, with the traditional cross-entropy loss for training Llama 2."
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "RedPajama-V2 and MAP-CC",
          "dataset_description": "We sample an equal proportion of 15 billion English and Chinese tokens from RedPajama-V2 and MAP-CC, respectively.",
          "evidence_text": "For the pretraining dataset, we perform data mixture and deduplication."
        }
      ],
      "evaluation_metrics": [
        "average accuracy"
      ],
      "baseline_methods": [
        "GPT-4",
        "GPT-3.5-Turbo",
        "ChatGLM",
        "MPT",
        "Falcon",
        "Llama",
        "Llama 2",
        "C-Llama (w/o InfoLoss)"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "accuracy",
          "our_result": "48.1",
          "baseline_result": "45.7",
          "dataset": "multi-task English understanding benchmarks",
          "evidence_text": "C-Llama | 7B | 48.1 | 9.12 | 29.8"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "the proposal of InfoLoss for continually pretraining LLMs",
        "the first attempt to mitigate hallucinations in a cross-lingual transfer setting",
        "extensive experiments on twelve benchmarks"
      ],
      "stated_novelty": [
        "原文未明确提及"
      ],
      "stated_advantages": [
        "enhancing cross-lingual transfer ability",
        "mitigating the impact of noisy tokens",
        "enhancing the truth and credibility of C-Llama"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文明确承认的局限性，如无则为空数组"
      ],
      "failure_cases": [
        "原文提到的失败案例，如无则为空数组"
      ]
    },
    "extraction_metadata": {
      "file_id": "A_Weighted_Cross_entropy_Loss_for_Mitigating_LLM_Hallucinations_in_Cross_lingual_Continual_Pretraining",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 13:07:22",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑技术关系comparison_result: InfoLoss aims to avoid learning incorrect language distributions caused by noisy tokens"
        ],
        "suspicious_count": 2,
        "validation_score": 60,
        "quality_level": "medium"
      },
      "text_length": 12140,
      "processed_text_length": 12140
    }
  },
  "Designing_a_Japanese_idiom_education_support_system_for_overseas_students": {
    "document_metadata": {
      "document_type": "原文无此信息",
      "title": "Designing a Japanese Idiom Education Support System for Overseas’ Students",
      "authors_or_creators": [
        "KONISHI Yusuke",
        "Ruifan LI",
        "Fuji REN"
      ]
    },
    "main_content": {
      "stated_purpose": "Our Japanese Idiom Education Support System aims to foster learner interest and teach proper idiom usage.",
      "key_concepts": [
        "Japanese idioms",
        "education support system",
        "overseas students",
        "idiom retrieval",
        "teaching functions"
      ],
      "main_methods": [
        "system outline",
        "retrieval method",
        "questionnaire results"
      ]
    },
    "relationships": {
      "references_to": [
        "[1] Statistics Bureau and the Director-General for Policy Planning. Statistics of Japan which sees in graph. http://www.stat.go.jp/data/nihon/pdf/ngraph.pdf",
        "[2] Murano Ryoko. Japanese Language Volunteers in Thai Secondary Schools. The Research Center for Japanese Language Education annual bulletin 9, pp. 1-8, 2000.",
        "[3] The Japan Foundation. 2006 Survey of Overseas Organizations Involved in Japanese-Language Education. The Japan Foundation, 2006.",
        "[4] Ishida Koji. Development of the science e-learning teaching materials which utilized student knowledge. Modern educational needs measure support program in the Heisei 16 fiscal year.",
        "[5] SAITO Koichi and TAKAHASHI Satoshi. A Preliminary Study on Modeling for Making Concerning Cause Belonging of 'Losing Interest in Science'- Based on the Consideration Investigation of the High School Student. Journal of Tokyo University of Information Sciences Vol.9 No.1, pp. 1-9, 2005.",
        "[6] Ren Fuji. Super-function based machine translation. Communications of COLIPS, pp. 83-100, 1999.",
        "[7] Matsumoto Yuji. Japanese Morphological Analysis System Chasen. Information Processing Society of Japan.",
        "[8] NHK Science and Technical Research Laboratories. TVML. http://www.nhk.or.jp/strl/tvml/english/player2/index.html",
        "[9] Suzuki Makoto and Nozaki Yukiko. How is elementary science education advanced in many foreign countries? Chemistry & education, Vol.56, No.12, Page. 638-641, 2008.",
        "[10] ANDO Tadashi. The Advantages and Problems in e-Learning -in the Case of Phonetic Education by Using ALC Net Academy-. Nagoya University of Arts and Sciences, the journal of liberal arts, 2005.",
        "[11] An Open-Source Large Vocabulary CSR Engine Julius. http://julius.sourceforge.jp/en_index.php",
        "[12] Fine Speech. http://www.animo.co.jp/index.jsp",
        "[13] Yin Chengjiu, Ogata Hiroaki, and Yano Yoneo. Participatory Simulation for Collaborative Learning Experiences. Information Science Publishing, New York, USA, Oct. 2008."
      ],
      "builds_on": [
        "[6] Ren Fuji. Super-function based machine translation. Communications of COLIPS, pp. 83-100, 1999."
      ]
    },
    "note": "文档类型不明确，仅提取最基础信息",
    "extraction_metadata": {
      "file_id": "Designing_a_Japanese_idiom_education_support_system_for_overseas_students",
      "detected_doc_type": "unknown",
      "extraction_time": "2025-08-02 13:07:52",
      "validation": {
        "warnings": [],
        "suspicious_count": 0,
        "validation_score": 100,
        "quality_level": "high"
      },
      "text_length": 9382,
      "processed_text_length": 9382
    }
  },
  "Dimensionality_reduction_for_text_using_LLE": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "Dimensionality Reduction for Text Using LLE",
      "authors": [
        "Chuan HE",
        "Zhe DONG",
        "Ruifan LI",
        "Yixin ZHONG"
      ],
      "institutions": [
        "School of Information Engineering, Beijing University of Posts and Telecommunications, Beijing, China"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "Locally Linear Embedding (LLE)",
          "relationship_type": "基于",
          "evidence_text": "This paper introduces LLE, analyzes its advantages and limitations, discusses its relationship with latent semantic indexing (LSI) within the graph embedding framework, and presents experimental results using Reuters21578 and TDT2 datasets."
        }
      ],
      "compared_methods": [
        {
          "method_name": "LSI",
          "comparison_result": "LLE significantly outperforms the other methods",
          "evidence_text": "In terms of classification precision, LLE significantly outperforms the other methods."
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "Reuters21578",
          "dataset_description": "document subsets were vectorized after preprocessing, and the number of topics and documents varied",
          "evidence_text": "Experiments on the Reuters21578 and TDT2 datasets compared text representations in the original space, LLE, and LSI algorithms."
        },
        {
          "dataset_name": "TDT2",
          "dataset_description": "contains 9394 documents across 30 topics",
          "evidence_text": "After preprocessing similar to that of Reuters21578, the TDT2 dataset contains 9394 documents across 30 topics."
        }
      ],
      "evaluation_metrics": [
        "precision"
      ],
      "baseline_methods": [
        "Baseline",
        "LSI"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "precision",
          "our_result": "91.28",
          "baseline_result": "86.06",
          "dataset": "Reuters21578 subset 1",
          "evidence_text": "Precisions Dims Precisions Dims Precisions 1 6 539 86.06 529 86.99 19 91.28"
        },
        {
          "metric_name": "precision",
          "our_result": "95.34",
          "baseline_result": "91.43",
          "dataset": "Reuters21578 subset 3",
          "evidence_text": "Precisions Dims Precisions Dims Precisions 3 5 535 91.43 527 91.24 15 95.34"
        },
        {
          "metric_name": "precision",
          "our_result": "98.83",
          "baseline_result": "94.15",
          "dataset": "Reuters21578 subset 17",
          "evidence_text": "Precisions Dims Precisions Dims Precisions 17 3 940 94.15 923 94.26 68 98.83"
        },
        {
          "metric_name": "precision",
          "our_result": "99.85",
          "baseline_result": "96.09",
          "dataset": "Reuters21578 subset 19",
          "evidence_text": "Precisions Dims Precisions Dims Precisions 19 3 665 96.09 653 96.09 5 99.85"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "introduces LLE for text dimensionality reduction",
        "presents experimental results comparing LLE with LSI"
      ],
      "stated_novelty": [
        "原文无此信息"
      ],
      "stated_advantages": [
        "LLE's nonlinearity",
        "fast computation",
        "LLE significantly outperforms other methods in precision"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "sampling assumption",
        "use of the Euclidean metric",
        "constraint of equalizing reconstruction points to the raw point can lead to suboptimal solutions"
      ],
      "failure_cases": [
        "原文无此信息"
      ]
    },
    "extraction_metadata": {
      "file_id": "Dimensionality_reduction_for_text_using_LLE",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 13:08:30",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 10467,
      "processed_text_length": 10467
    }
  },
  "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "DualGCN for Aspect-Based Sentiment Analysis",
      "authors": [
        "M. Zhang",
        "T. Qian"
      ],
      "institutions": [
        "原文无此信息"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "GCN",
          "relationship_type": "改进",
          "evidence_text": "Incorporating syntactic dependency structure with graph convolutional networks (GCNs) has shown advantages, but performance depends on dependency parsers."
        }
      ],
      "compared_methods": [
        {
          "method_name": "ATAE-LSTM",
          "comparison_result": "DualGCN model consistently outperforms attention-based and syntax-based methods",
          "evidence_text": "The DualGCN model consistently outperforms attention-based and syntax-based methods on Restaurant14, Laptop14 and Twitter datasets."
        },
        {
          "method_name": "GCAE",
          "comparison_result": "DualGCN model outperforms",
          "evidence_text": "DualGCN model outperforms SynGCN-head on the Restaurant14 and Laptop14 datasets"
        },
        {
          "method_name": "R-GAT + BERT",
          "comparison_result": "DualGCN + BERT and DualGCN + BERT-PT show improved performance over the basic DualGCN model and other BERT-based methods",
          "evidence_text": "DualGCN + BERT and DualGCN + BERT-PT show improved performance over the basic DualGCN model and other BERT-based methods."
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "Restaurant14",
          "dataset_description": "All datasets have three sentimental polarities: positive, neutral, and negative",
          "evidence_text": "The first group includes Restaurant14, Laptop14, and Twitter."
        },
        {
          "dataset_name": "Laptop14",
          "dataset_description": "All datasets have three sentimental polarities: positive, neutral, and negative",
          "evidence_text": "The first group includes Restaurant14, Laptop14, and Twitter."
        },
        {
          "dataset_name": "Twitter",
          "dataset_description": "All datasets have three sentimental polarities: positive, neutral, and negative",
          "evidence_text": "The first group includes Restaurant14, Laptop14, and Twitter."
        }
      ],
      "evaluation_metrics": [
        "LAL-Parser",
        "Glove vectors",
        "BiLSTM",
        "dropout",
        "Adam optimizer",
        "learning rate",
        "epochs",
        "batch size"
      ],
      "baseline_methods": [
        "attention-based models",
        "CNN-based models",
        "GNN-based models",
        "BERT-based models"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "performance",
          "our_result": "DualGCN model consistently outperforms attention-based and syntax-based methods",
          "baseline_result": "performance of attention-based and syntax-based methods",
          "dataset": "Restaurant14, Laptop14 and Twitter",
          "evidence_text": "The DualGCN model consistently outperforms attention-based and syntax-based methods on Restaurant14, Laptop14 and Twitter datasets."
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "proposal of DualGCN architecture",
        "integration of syntactic knowledge through SynGCN and semantic information through SemGCN",
        "orthogonal and differential regularizers"
      ],
      "stated_novelty": [
        "DualGCN architecture",
        "orthogonal and differential regularizers"
      ],
      "stated_advantages": [
        "captures both syntactic knowledge and semantic information",
        "effective on various review styles",
        "improved performance over state-of-the-art approaches"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "dependency parsing results can be inaccurate",
        "GCNs may not perform well on datasets insensitive to syntactic dependency"
      ],
      "failure_cases": [
        "The SynGCN model fails in capturing the representation of key words in complex sentences"
      ]
    },
    "extraction_metadata": {
      "file_id": "DualGCN_Exploring_Syntactic_and_Semantic_Information_for_Aspect_Based_Sentiment_Analysis",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 13:10:27",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑title: DualGCN for Aspect-Based Sentiment Analysis",
          "可疑技术关系evidence_text: The DualGCN model consistently outperforms attention-based and syntax-based methods on Restaurant14, Laptop14 and Twitter datasets.",
          "可疑技术关系evidence_text: DualGCN model outperforms SynGCN-head on the Restaurant14 and Laptop14 datasets",
          "可疑性能数据baseline_result: performance of attention-based and syntax-based methods"
        ],
        "suspicious_count": 5,
        "validation_score": 0,
        "quality_level": "low"
      },
      "text_length": 30061,
      "processed_text_length": 15708
    }
  },
  "electronics_12_03521_v2": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "Visually Enhanced NeUral Encoder for Synset Induction",
      "authors": [
        "作者列表在原文中未明确列出"
      ],
      "institutions": [
        "原文明确提到的机构在原文中未明确列出"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "word2vec",
          "relationship_type": "基于",
          "evidence_text": "This module captures tag-level embeddings (El) and word-level tag embeddings (Ew) through different training methods. We specifically use word2vec [19] to train word vectors on an external corpus for the word-level embedding."
        },
        {
          "method_name": "CNN",
          "relationship_type": "基于",
          "evidence_text": "We use ResNet101 as the visual backbone in our VENUE model, with the image representation dimensionality DI set to 2048."
        }
      ],
      "compared_methods": [
        {
          "method_name": "word2vec + k-means/HAC",
          "comparison_result": "我们的方法在某些评估指标上表现更好",
          "evidence_text": "Our VENUE model and other baselines show varying degrees of success, with SynsetMine, Infomap, MWSI, and CLIP demonstrating competitive performance."
        },
        {
          "method_name": "CNN + k-means/HAC",
          "comparison_result": "文本信息在语义区分上比视觉信息更有效",
          "evidence_text": "Text-based methods, such as word2vec + k-means/HAC, InfoMap, and SynsetMine, outperformed vision-based methods."
        },
        {
          "method_name": "MWSI",
          "comparison_result": "我们的方法在所有评估指标上超过了MWSI",
          "evidence_text": "VENUE + HAC outperformed MWSI across all evaluated metrics"
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "MMAI-Synset",
          "dataset_description": "包含8509个名词短语及其对应的425,450张图片",
          "evidence_text": "Our MMAI-Synset dataset is collected using the Wikipedia text subset of the synset dataset. It consists of 8509 noun phrases and their corresponding 425,450 images."
        }
      ],
      "evaluation_metrics": [
        "h",
        "c",
        "v",
        "p",
        "r",
        "f",
        "FMI",
        "ARI",
        "NMI"
      ],
      "baseline_methods": [
        "word2vec + k-means/HAC",
        "CNN + k-means/HAC",
        "[word2vec; CNN] + k-means/HAC",
        "SynsetMine",
        "Infomap",
        "MWSI",
        "CLIP"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "v-measure",
          "our_result": "95.08",
          "baseline_result": "93.80 (MWSI)",
          "dataset": "MMAI-Synset",
          "evidence_text": "VENUE + HAC achieved scores of 96.41, 93.79, and 95.08 for homogeneity, completeness, and v-measure"
        },
        {
          "metric_name": "ARI",
          "our_result": "65.15",
          "baseline_result": "58.78 (MWSI)",
          "dataset": "MMAI-Synset",
          "evidence_text": "ARI, FMI, and NMI scores of 65.15, 65.33, and 95.08"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "提出了一种新的多模态表示学习方法VENUE",
        "通过视觉交互模块、文本多粒度嵌入模块、掩码模块和门控模块提高了多模态语义表征的区分性"
      ],
      "stated_novelty": [
        "将视觉信息融入到同义词集诱导任务中",
        "提出了一种新的多模态编码器结构"
      ],
      "stated_advantages": [
        "在多模态数据集上性能优于强基准方法",
        "能够过滤掉语义上弱相关的图像信息"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "在某些情况下，多模态信息的融合可能导致预测混淆"
      ],
      "failure_cases": [
        "原文提到的失败案例在原文中未明确列出"
      ]
    },
    "extraction_metadata": {
      "file_id": "electronics_12_03521_v2",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 13:12:27",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑title: Visually Enhanced NeUral Encoder for Synset Induction",
          "可疑authors: ['作者列表在原文中未明确列出']",
          "可疑institutions: ['原文明确提到的机构在原文中未明确列出']",
          "可疑技术关系comparison_result: 我们的方法在某些评估指标上表现更好",
          "可疑技术关系evidence_text: Our VENUE model and other baselines show varying degrees of success, with SynsetMine, Infomap, MWSI, and CLIP demonstrating competitive performance.",
          "可疑技术关系comparison_result: 文本信息在语义区分上比视觉信息更有效",
          "可疑技术关系comparison_result: 我们的方法在所有评估指标上超过了MWSI",
          "可疑性能数据baseline_result: 93.80 (MWSI)",
          "可疑性能数据baseline_result: 58.78 (MWSI)"
        ],
        "suspicious_count": 10,
        "validation_score": 0,
        "quality_level": "low"
      },
      "text_length": 27553,
      "processed_text_length": 15817
    }
  },
  "Enhanced_Prompt_Learning_for_Few_shot_Text_Classification_Method": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "增强提示学习的少样本文本分类方法",
      "authors": [
        "李睿凡",
        "魏志宇",
        "范元涛",
        "叶书勤",
        "张光卫"
      ],
      "institutions": [
        "北京邮电大学人工智能学院",
        "教育部信息网络工程研究中心",
        "交互技术与体验系统文化和旅游部重点实验室",
        "北京邮电大学计算机学院"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "基于度量学习的方法和基于提示学习的方法",
          "relationship_type": "基于",
          "evidence_text": "本文提出的方法与基于度量学习的方法和基于提示学习的方法密切相关。"
        }
      ],
      "compared_methods": [
        {
          "method_name": "对比基线方法",
          "comparison_result": "EPL4FTC方法的准确度明显优于对比基线方法",
          "evidence_text": "实验评估表明EPL4FTC方法的准确度明显优于对比基线方法。"
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "中文数据集和英文数据集",
          "dataset_description": "原文无此信息",
          "evidence_text": "实验结果显示，该算法在中文和英文数据集上均优于基线方法"
        }
      ],
      "evaluation_metrics": [
        "原文无此信息"
      ],
      "baseline_methods": [
        "对比基线方法"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "准确度",
          "our_result": "59.8（中文数据集平均值）, 54.1（英文数据集平均值）",
          "baseline_result": "原文无此信息",
          "dataset": "中文数据集和英文数据集",
          "evidence_text": "表6 中文数据集和英文数据集上推理词形式性能比较"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "提出基于提示学习和三元组损失优化的少样本文本分类EPL4FTC算法"
      ],
      "stated_novelty": [
        "将文本分类任务转换为基于自然语言推理的提示学习形式，并引入三元组损失"
      ],
      "stated_advantages": [
        "在少样本场景下，准确度明显优于对比基线方法"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文无此信息"
      ],
      "failure_cases": [
        "原文无此信息"
      ]
    },
    "extraction_metadata": {
      "file_id": "Enhanced_Prompt_Learning_for_Few_shot_Text_Classification_Method",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 13:12:50",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑性能数据our_result: 59.8（中文数据集平均值）, 54.1（英文数据集平均值）"
        ],
        "suspicious_count": 2,
        "validation_score": 60,
        "quality_level": "medium"
      },
      "text_length": 1834,
      "processed_text_length": 1834
    }
  },
  "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "Entailment Method Based on Template Selection for Chinese Text Few-shot Learning",
      "authors": [
        "Zeyuan Wang",
        "Zhiyu Wei",
        "Lihui Zhang",
        "Ruifan Li",
        "Zhanyu Ma"
      ],
      "institutions": [
        "School of Artificial Intelligence, Beijing University of Posts and Telecommunications, Beijing, China"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "MacBERT",
          "relationship_type": "基于",
          "evidence_text": "We choose MacBERT, a pre-training model, as our backbone"
        },
        {
          "method_name": "Entailment-based Few-shot Learning (EFL)",
          "relationship_type": "结合",
          "evidence_text": "We address this by introducing a template selection mechanism using a masked language model to assess candidate templates"
        }
      ],
      "compared_methods": [
        {
          "method_name": "PET",
          "comparison_result": "EFL is more effective on sentence-pair tasks, while PET is better for single sentence classification",
          "evidence_text": "The results on the testing datasets of the nine NLP tasks show that the EFL method with automatic template selection outperforms other methods"
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "Few-CLUE",
          "dataset_description": "encompassing sentiment analysis, short text classification, long text classification, natural language inference, sentence similarity, Chinese cloze, and co-reference resolution",
          "evidence_text": "We evaluate nine Chinese few-shot datasets of Few-CLUE"
        },
        {
          "dataset_name": "CMNLI",
          "dataset_description": "used for intermediate training",
          "evidence_text": "Intermediate training with an entailment dataset like CMNLI"
        }
      ],
      "evaluation_metrics": [
        "原文无此信息"
      ],
      "baseline_methods": [
        "MacBERT-large",
        "MacBERT-base",
        "PET"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "原文无此信息",
          "our_result": "原文无此信息",
          "baseline_result": "原文无此信息",
          "dataset": "原文无此信息",
          "evidence_text": "The results on the testing datasets of the nine NLP tasks show that the EFL method with automatic template selection outperforms other methods"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "introducing a template selection mechanism using a masked language model to assess candidate templates"
      ],
      "stated_novelty": [
        "原文无此信息"
      ],
      "stated_advantages": [
        "effective on sentence-pair tasks",
        "automatic template selection"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文无此信息"
      ],
      "failure_cases": [
        "原文无此信息"
      ]
    },
    "extraction_metadata": {
      "file_id": "Entailment_Method_Based_on_Template_Selection_for_Chinese_Text_Few_shot_Learning",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 13:13:17",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 10337,
      "processed_text_length": 10337
    }
  },
  "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "Cross-modal Global and Local Linguistic Representations-based Generative Adversarial Networks for Text-to-image Synthesis",
      "authors": [
        "Y. Guo",
        "J. Johnson",
        "A. Gupta",
        "L. Fei-Fei"
      ],
      "institutions": [
        "California Institute of Technology",
        "State Grid Corporation of China"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "Generative Adversarial Networks (GANs)",
          "relationship_type": "基于",
          "evidence_text": "Most current approaches rely on generative adversarial network (GAN) models and utilize global linguistic representations."
        }
      ],
      "compared_methods": [
        {
          "method_name": "GAN-INT-CLS",
          "comparison_result": "Our CGL-GAN model achieves a performance comparable to baseline models, often surpassing those without an attention mechanism on the CUB dataset and significantly outperforming them on the MS-COCO dataset.",
          "evidence_text": "Our CGL-GAN model achieves higher Inception scores on the CUB dataset compared to state-of-the-art methods not utilizing multiple discriminators."
        },
        {
          "method_name": "GAWWN",
          "comparison_result": "Our CGL-GAN model generates more realistic images than GAN-INT-CLS, GAWWN, StackGAN, and StackGAN-V2.",
          "evidence_text": "Examples illustrate that our CGL-GAN model generates more realistic images than GAN-INT-CLS, GAWWN, StackGAN, and StackGAN-V2."
        },
        {
          "method_name": "StackGAN",
          "comparison_result": "Our model produces images that are closer to real images, capturing fine-grained details that StackGAN fails to represent.",
          "evidence_text": "For the MS-COCO dataset, our model produces images that are closer to real images, capturing fine-grained details that StackGAN fails to represent."
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "CUB",
          "dataset_description": "Caltech-UCSD Birds-200-2011, consists of 11,788 bird images across 200 categories, partitioned into training and testing subsets of 8,855 and 2,933 images, respectively.",
          "evidence_text": "We use the Inception score [40] and Fréchet Inception Distance (FID) [41] for evaluation on the CUB and MS-COCO datasets."
        },
        {
          "dataset_name": "MS-COCO",
          "dataset_description": "Contains images of multiple objects and diverse backgrounds, with 80,000 and 40,000 images in the training and testing sets, respectively, and each image annotated with five descriptive sentences.",
          "evidence_text": "We use the Inception score [40] and Fréchet Inception Distance (FID) [41] for evaluation on the CUB and MS-COCO datasets."
        }
      ],
      "evaluation_metrics": [
        "Inception score",
        "Fréchet Inception Distance (FID)"
      ],
      "baseline_methods": [
        "GAN-INT-CLS",
        "GAWWN",
        "StackGAN",
        "StackGAN-V2",
        "HDGAN",
        "AttnGAN",
        "MirrorGAN",
        "Obj-GAN"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "Inception score",
          "our_result": "3.67",
          "baseline_result": "2.88",
          "dataset": "CUB",
          "evidence_text": "Our proposed model exhibits a 27.43% improvement in Inception score over GAN-INT-CLS, increasing from 2.88 to 3.67."
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "Proposing cross-modal global and local linguistic representations-based generative adversarial networks (CGL-GAN) for text-to-image synthesis.",
        "Incorporating fine-grained local linguistic information and cross-modal correlation significantly enhances text-to-image synthesis performance."
      ],
      "stated_novelty": [
        "The use of both global and local linguistic representations for text-to-image synthesis.",
        "A high-resolution synthesis model that outperforms state-of-the-art methods on the MS-COCO dataset."
      ],
      "stated_advantages": [
        "Comparable performance with fewer trainable parameters than state-of-the-art methods.",
        "Enhanced text-to-image synthesis performance for high-resolution images."
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "Training difficulties due to the sparsity of global representations and lack fine-grained information in the generated images."
      ],
      "failure_cases": [
        "原文未明确提及"
      ]
    },
    "extraction_metadata": {
      "file_id": "Exploring_Global_and_Local_Linguistic_Representations_for_Text_to_Image_Synthesis",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 13:15:24",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑title: Cross-modal Global and Local Linguistic Representations-based Generative Adversarial Networks for Text-to-image Synthesis",
          "可疑技术关系method_name: Generative Adversarial Networks (GANs)",
          "可疑技术关系evidence_text: Our CGL-GAN model achieves higher Inception scores on the CUB dataset compared to state-of-the-art methods not utilizing multiple discriminators.",
          "可疑技术关系comparison_result: Our CGL-GAN model generates more realistic images than GAN-INT-CLS, GAWWN, StackGAN, and StackGAN-V2.",
          "可疑技术关系comparison_result: Our model produces images that are closer to real images, capturing fine-grained details that StackGAN fails to represent."
        ],
        "suspicious_count": 6,
        "validation_score": 0,
        "quality_level": "low"
      },
      "text_length": 19725,
      "processed_text_length": 14998
    }
  },
  "FAIA_372_FAIA230600": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "Enhanced Machine Reading Comprehension Method for Aspect Sentiment Quadruplet Extraction",
      "authors": [
        "Shuqin Ye",
        "Zepeng Zhang",
        "Ruifan Li"
      ],
      "institutions": [
        "School of Artiﬁcial Intelligence, Beijing University of Posts and Telecommunications, China",
        "Engineering Research Center of Information Networks, Ministry of Education, China",
        "Key Laboratory of Interactive Technology and Experience System, Ministry of Culture and Tourism, China"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "Machine Reading Comprehension (MRC)",
          "relationship_type": "基于",
          "evidence_text": "Our EMRC method is based on Machine Reading Comprehension (MRC)."
        }
      ],
      "compared_methods": [
        {
          "method_name": "Double-Propagation",
          "comparison_result": "原文无此信息",
          "evidence_text": "Our EMRC model is compared with state-of-art baselines, including pipeline methods like Double-Propagation"
        },
        {
          "method_name": "Extract-Classify",
          "comparison_result": "原文无此信息",
          "evidence_text": "Our EMRC model is compared with state-of-art baselines, including pipeline methods like Extract-Classify"
        },
        {
          "method_name": "TAS-BERT",
          "comparison_result": "原文无此信息",
          "evidence_text": "Our EMRC model is compared with state-of-art baselines, including end-to-end methods like TAS-BERT"
        },
        {
          "method_name": "JET",
          "comparison_result": "原文无此信息",
          "evidence_text": "Our EMRC model is compared with state-of-art baselines, including end-to-end methods like JET"
        },
        {
          "method_name": "BARTABSA",
          "comparison_result": "原文无此信息",
          "evidence_text": "Generative methods include BARTABSA"
        },
        {
          "method_name": "GAS",
          "comparison_result": "原文无此信息",
          "evidence_text": "GAS approaches ABSA tasks in a unified generative framework"
        },
        {
          "method_name": "Paraphrase detection",
          "comparison_result": "原文无此信息",
          "evidence_text": "Paraphrase detection aims to jointly detect all sentiment elements in quads"
        },
        {
          "method_name": "Opinion tree generation",
          "comparison_result": "原文无此信息",
          "evidence_text": "Opinion tree generation detects all sentiment elements in a tree for a given review sentence"
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "Restaurant-ACOS",
          "dataset_description": "原文无此信息",
          "evidence_text": "We evaluate our method on two benchmark datasets: Restaurant-ACOS and Laptop-ACOS."
        },
        {
          "dataset_name": "Laptop-ACOS",
          "dataset_description": "原文无此信息",
          "evidence_text": "We evaluate our method on two benchmark datasets: Restaurant-ACOS and Laptop-ACOS."
        }
      ],
      "evaluation_metrics": [
        "Precision",
        "Recall",
        "F1-score"
      ],
      "baseline_methods": [
        "Double-Propagation",
        "Extract-Classify",
        "TAS-BERT",
        "JET",
        "BARTABSA",
        "GAS",
        "Paraphrase detection",
        "Opinion tree generation"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "F1-score",
          "our_result": "63.02",
          "baseline_result": "44.61",
          "dataset": "Restaurant-ACOS",
          "evidence_text": "Restaurant-ACOS EMRC 63.02 78.23"
        },
        {
          "metric_name": "F1-score",
          "our_result": "45.92",
          "baseline_result": "35.80",
          "dataset": "Laptop-ACOS",
          "evidence_text": "Laptop-ACOS EMRC 45.92 60.58"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "A novel EMRC model that effectively builds associations among sentimental subtasks.",
        "A hierarchical category classification strategy to improve the context representation's task-awareness.",
        "Extensive experimental results demonstrating EMRC's superiority over existing baselines."
      ],
      "stated_novelty": [
        "原文未明确提及"
      ],
      "stated_advantages": [
        "Our model extracts the opinion term 'nicest' from the given sentence to avoid such generation errors."
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文明确承认的局限性，如无则为空数组"
      ],
      "failure_cases": [
        "In Example 1, the sentence 'Great service with amazon on fulfilling my order.' leads Extract-Classify to incorrectly classify the aspect 'service' into the category LAPTOP#OPERATION_PERFORMANCE",
        "In Example 2, Paraphrase generates the opinion term 'best', which is not present in the original sentence 'The pizza is delicious and the proprietor is one of the nicest in NYC.'"
      ]
    },
    "extraction_metadata": {
      "file_id": "FAIA_372_FAIA230600",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 13:17:46",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑技术关系evidence_text: Our EMRC model is compared with state-of-art baselines, including pipeline methods like Double-Propagation",
          "可疑技术关系evidence_text: Our EMRC model is compared with state-of-art baselines, including pipeline methods like Extract-Classify",
          "可疑技术关系evidence_text: Our EMRC model is compared with state-of-art baselines, including end-to-end methods like TAS-BERT",
          "可疑技术关系evidence_text: Our EMRC model is compared with state-of-art baselines, including end-to-end methods like JET"
        ],
        "suspicious_count": 5,
        "validation_score": 0,
        "quality_level": "low"
      },
      "text_length": 16456,
      "processed_text_length": 14906
    }
  },
  "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "Image Captioning Based on An Improved Transformer with IoU Position Encoding",
      "authors": [
        "Yazhou Li",
        "Yihui Shi",
        "Yun Liu",
        "Ruifan Li",
        "Zhanyu Ma"
      ],
      "institutions": [
        "School of Artiﬁcial Intelligence, Beijing University of Posts and Telecommunications, Beijing, China"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "Neural Image Captioning (NIC)",
          "relationship_type": "基于",
          "evidence_text": "The Neural Image Captioning (NIC) model uses a convolutional neural network to extract image features and an LSTM to translate these into sentences."
        },
        {
          "method_name": "Transformer",
          "relationship_type": "改进",
          "evidence_text": "To address this, we adopt the transformer structure as the decoder."
        }
      ],
      "compared_methods": [
        {
          "method_name": "CoordNorm(hw), Coord(hw), Coord",
          "comparison_result": "IoUc and IoU+ models further enhance performance",
          "evidence_text": "Table III presents the performance of different position encoding methods. The CoordNorm(hw) model shows improvements over the Coord(hw) and Coord models, indicating the effectiveness of normalization."
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "MS-COCO",
          "dataset_description": "包括 82,783 训练、40,504 验证和 40,775 测试图像",
          "evidence_text": "Our TIP model's effectiveness is examined through experiments on the MS-COCO dataset [22], which includes 82,783 training, 40,504 validation, and 40,775 test images."
        }
      ],
      "evaluation_metrics": [
        "CIDEr",
        "BLEU",
        "METEOR",
        "ROUGE",
        "SPICE"
      ],
      "baseline_methods": [
        "原文无此信息"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "CIDEr",
          "our_result": "最高分",
          "baseline_result": "原文无此信息",
          "dataset": "MS-COCO",
          "evidence_text": "Table I shows that TIP achieves the highest scores in CIDEr and METEOR metrics among different models."
        },
        {
          "metric_name": "METEOR",
          "our_result": "最高分",
          "baseline_result": "原文无此信息",
          "dataset": "MS-COCO",
          "evidence_text": "Table I shows that TIP achieves the highest scores in CIDEr and METEOR metrics among different models."
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "提出了一种改进的Transformer模型TIP",
        "引入了IoU空间位置编码方法"
      ],
      "stated_novelty": [
        "TIP模型结合了模态内注意力机制和视觉与空间特征的融合"
      ],
      "stated_advantages": [
        "解决了传统模型在长期记忆和空间信息表示上的局限性"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文无此信息"
      ],
      "failure_cases": [
        "信息丢失，许多更好的句子被遗漏",
        "当束搜索大小增加到2和3时，单词搜索空间增加，达到最高的度量分数和最优性能。当束搜索大小为4时，模型生成较短的句子。生成词之间的高度相似性和缺乏多样性略微降低了指标。束搜索增加导致内存使用增加和句子生成速度减慢。"
      ]
    },
    "extraction_metadata": {
      "file_id": "Image_Captioning_Based_on_An_Improved_Transformer_with_IoU_Position_Encoding",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 13:18:55",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑技术关系method_name: CoordNorm(hw), Coord(hw), Coord"
        ],
        "suspicious_count": 2,
        "validation_score": 60,
        "quality_level": "medium"
      },
      "text_length": 9331,
      "processed_text_length": 9331
    }
  },
  "Improved+Eavesdropping+Detection+Strategy+Based+on+Extended+Three_particle+Greenberger_Horne_Zeilinger+State+in+Two_step+Quantum+Direct+Communication+Protocol": {
    "document_metadata": {
      "document_type": "原文无此信息",
      "title": "Improved Eavesdropping Detection Strategy Based on Extended Three-particle Greenberger-Horne-Zeilinger State in Two-step Quantum Direct Communication Protocol",
      "authors_or_creators": [
        "LI Jian",
        "YE Xinxin",
        "LI Ruifan",
        "ZOU Yongzhong",
        "LU Xiaofeng"
      ]
    },
    "main_content": {
      "stated_purpose": "提出一种基于扩展三粒子GHZ态的改进窃听检测策略，以提高两步量子直接通信协议的效率。",
      "key_concepts": [
        "量子密钥分发 (QKD)",
        "密集编码",
        "扩展三粒子GHZ态",
        "窃听检测",
        "熵"
      ],
      "main_methods": [
        "TSET协议",
        "使用扩展三粒子GHZ态",
        "通过测量进行窃听检测",
        "使用密集编码传输消息"
      ]
    },
    "relationships": {
      "references_to": [
        "Bennett and Brassard's quantum cryptography",
        "quantum teleportation",
        "Li et al.'s improved quantum 'Ping-pong' protocols",
        "cluster states",
        "Bell-basis measurements",
        "Hillery et al.",
        "Singh and Srikanth",
        "Long and Liu",
        "Deng et al.",
        "Ping-pong protocol detection strategies",
        "eavesdropping concerns in protocols"
      ],
      "builds_on": [
        "两步量子直接通信协议",
        "EPR对块原始协议",
        "Ref.[17]中的计算",
        "量子信息安全性方法的发展"
      ]
    },
    "note": "文档类型不明确，仅提取最基础信息",
    "extraction_metadata": {
      "file_id": "Improved+Eavesdropping+Detection+Strategy+Based+on+Extended+Three_particle+Greenberger_Horne_Zeilinger+State+in+Two_step+Quantum+Direct+Communication+Protocol",
      "detected_doc_type": "unknown",
      "extraction_time": "2025-08-02 13:19:12",
      "validation": {
        "warnings": [],
        "suspicious_count": 0,
        "validation_score": 100,
        "quality_level": "high"
      },
      "text_length": 5477,
      "processed_text_length": 5477
    }
  },
  "Improving_Image_Paragraph_Captioning_with_Dual_Relations": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "IMPROVING IMAGE PARAGRAPH CAPTIONING WITH DUAL RELATIONS",
      "authors": [
        "Yun Liu",
        "Yihui Shi",
        "Fangxiang Feng",
        "Ruifan Li",
        "Zhanyu Ma",
        "Xiaojie Wang"
      ],
      "institutions": [
        "School of Artificial Intelligence, Beijing University of Posts and Telecommunications, China",
        "Beijing Academy of Artificial Intelligence, Beijing, China"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "Faster R-CNN",
          "relationship_type": "基于",
          "evidence_text": "We employ Faster R-CNN [19] to detect N objects in an image, represented as C={c1, · · · , cN}."
        }
      ],
      "compared_methods": [
        {
          "method_name": "Regions-Hierarchical",
          "comparison_result": "DualRel outperforms SCST on all metrics (except for a tie in METEOR) and the recent IMAP method.",
          "evidence_text": "Baselines: We compare DualRel with baselines including Regions-Hierarchical [1], RTT-GAN [5], DAM [7], SCST [8], DCPG-VAE [6], TMOS [27], CAE-LSTM [11], DHPV [9], CVAP [10], CRL [12], Dual-CNN [15], VREN [17], IMAP [14], S2TD [16], and OR-ATT [18]."
        },
        {
          "method_name": "SCST",
          "comparison_result": "DualRel outperforms SCST on all metrics (except for a tie in METEOR)",
          "evidence_text": "Results: Table 1 shows our DualRel method achieves the best scores in B@{1-4} and CIDEr. DualRel outperforms SCST on all metrics (except for a tie in METEOR) and the recent IMAP method."
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "Stanford benchmark dataset",
          "dataset_description": "includes 14575/2487/2489 pairs for training/validation/test. The dataset comprises an average of 67.5 words per paragraph and 5.7 sentences.",
          "evidence_text": "Dataset and Metrics: We utilize the Stanford benchmark dataset [1], which includes 14575/2487/2489 pairs for training/validation/test."
        }
      ],
      "evaluation_metrics": [
        "BLEU@{1, 2, 3, 4}",
        "METEOR",
        "CIDEr",
        "BERTScore F metrics"
      ],
      "baseline_methods": [
        "Regions-Hierarchical",
        "RTT-GAN",
        "DAM",
        "SCST",
        "DCPG-VAE",
        "TMOS",
        "CAE-LSTM",
        "DHPV",
        "CVAP",
        "CRL",
        "Dual-CNN",
        "VREN",
        "IMAP",
        "S2TD",
        "OR-ATT"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "BLEU@{1-4}",
          "our_result": "原文中我们方法的数值或'原文无此信息'",
          "baseline_result": "原文中基准的数值或'原文无此信息'",
          "dataset": "Stanford benchmark dataset",
          "evidence_text": "Table 1 shows our DualRel method achieves the best scores in B@{1-4} and CIDEr."
        },
        {
          "metric_name": "CIDEr",
          "our_result": "原文中我们方法的数值或'原文无此信息'",
          "baseline_result": "原文中基准的数值或'原文无此信息'",
          "dataset": "Stanford benchmark dataset",
          "evidence_text": "Table 1 shows our DualRel method achieves the best scores in B@{1-4} and CIDEr."
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "提出DualRel模型，明确捕捉空间和语义关系以改进图像段落字幕生成"
      ],
      "stated_novelty": [
        "DualRel模型考虑了特定的语义和空间关系，并使用关系感知交互"
      ],
      "stated_advantages": [
        "在Stanford基准数据集上优于现有方法"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文无此信息"
      ],
      "failure_cases": [
        "原文无此信息"
      ]
    },
    "extraction_metadata": {
      "file_id": "Improving_Image_Paragraph_Captioning_with_Dual_Relations",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 13:22:51",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑性能数据our_result: 原文中我们方法的数值或'原文无此信息'",
          "可疑性能数据baseline_result: 原文中基准的数值或'原文无此信息'",
          "可疑性能数据our_result: 原文中我们方法的数值或'原文无此信息'",
          "可疑性能数据baseline_result: 原文中基准的数值或'原文无此信息'"
        ],
        "suspicious_count": 5,
        "validation_score": 0,
        "quality_level": "low"
      },
      "text_length": 11060,
      "processed_text_length": 11060
    }
  },
  "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "原文无此信息",
      "authors": [
        "原文无此信息"
      ],
      "institutions": [
        "原文无此信息"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "原文无此信息",
          "relationship_type": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ],
      "compared_methods": [
        {
          "method_name": "原文无此信息",
          "comparison_result": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "原文无此信息",
          "dataset_description": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ],
      "evaluation_metrics": [
        "原文无此信息"
      ],
      "baseline_methods": [
        "原文无此信息"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "原文无此信息",
          "our_result": "原文无此信息",
          "baseline_result": "原文无此信息",
          "dataset": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "原文无此信息"
      ],
      "stated_novelty": [
        "原文无此信息"
      ],
      "stated_advantages": [
        "原文无此信息"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文无此信息"
      ],
      "failure_cases": [
        "原文无此信息"
      ]
    },
    "extraction_metadata": {
      "file_id": "LGR_NET_Language_Guided_Reasoning_Network_for_Referring_Expression_Comprehension",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 13:23:17",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 30309,
      "processed_text_length": 19588
    }
  },
  "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text": {
    "document_metadata": {
      "document_type": "原文无此信息",
      "title": "Line and Ligature Segmentation of Urdu Nastaleeq Text",
      "authors_or_creators": [
        "I. Ahmad",
        "et al."
      ]
    },
    "main_content": {
      "stated_purpose": "提出两种用于乌尔都语Nastaleeq文本行和连字分割的算法，并提高现有方法的准确性",
      "key_concepts": [
        "乌尔都语Nastaleeq文本",
        "行分割",
        "连字分割",
        "Curved Line Split算法"
      ],
      "main_methods": [
        "水平投影",
        "Curved Line Split算法",
        "基于基线的分割决策"
      ]
    },
    "relationships": {
      "references_to": [
        "Husain, S. A., 'A multi-tier holistic approach for Urdu Nastaliq recognition'",
        "Pal, U., and Sarkar, A., 'Recognition of printed Urdu script'",
        "Satti, D., and Saleem, K., 'Complexities and implementation challenges in offline Urdu Nastaliq OCR'",
        "Hussain, S., 'Complexity of Asian writing systems: A case study of Nafees Nasta'leeq for Urdu'",
        "Javed, S. T., and Hussain, S., 'Improving Nastalique specific pre-recognition process for Urdu OCR'",
        "Lehal, G. S., 'Ligature segmentation for Urdu OCR'",
        "Din, I. U., Malik, Z., Siddiqi, I., and Khalid, S., 'Line and ligature segmentation in printed Urdu document images'",
        "Shamsher, I., Ahmad, Z., Orakzai, J. K., and Adnan, A., 'OCR for printed Urdu script using feed forward neural network'",
        "Pathan, I. K., and Ramteke, R., 'Recognition of offline handwritten isolated Urdu character'",
        "Tariq, J., Nauman, U., and Naru, M. U., 'Softconverter: A novel approach to construct OCR for printed Urdu isolated characters'",
        "Akram, Q. U. A., Hussain, S., and Habib, Z., 'Font size independent OCR for Noori Nastaleeq'",
        "Khan, K., Ullah, R., Khan, N. A., and Naveed, K., 'Urdu character recognition using principal component analysis'",
        "Ahmad, Z., Orakzai, J. K., Shamsher, I., and Adnan, A., 'Urdu Nastaleeq optical character recognition'",
        "Nawaz, T., Naqvi, S., ur Rehman, H., and Faiz, A., 'Optical character recognition system for Urdu (naskh font) using pattern matching technique'",
        "Hussain, S., et al., 'Nastalique segmentation-based approach for Urdu OCR'",
        "Sabbour, N., and Shafait, F., 'A segmentation free approach to Arabic and Urdu OCR'",
        "Javed, S. T., and Hussain, S., 'Segmentation free nastalique Urdu OCR'"
      ],
      "builds_on": [
        "水平投影方法",
        "连字分割的垂直直方图、水平投影和边界框方法",
        "区域分割方法"
      ]
    },
    "note": "文档类型不明确，仅提取最基础信息",
    "extraction_metadata": {
      "file_id": "Line_and_Ligature_Segmentation_of_Urdu_Nastaleeq_Text",
      "detected_doc_type": "unknown",
      "extraction_time": "2025-08-02 13:23:49",
      "validation": {
        "warnings": [],
        "suspicious_count": 0,
        "validation_score": 100,
        "quality_level": "high"
      },
      "text_length": 19049,
      "processed_text_length": 14959
    }
  },
  "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "Obtaining Cross Modal Similarity Metric with Deep Neural Architecture",
      "authors": [
        "Ruifan Li",
        "Fangxiang Feng",
        "Xiaojie Wang",
        "Peng Lu",
        "Bohan Li"
      ],
      "institutions": [
        "School of Computers, Beijing University of Posts and Telecommunications, Beijing 100876, China",
        "Engineering Research Center of Information Networks, Ministry of Education, Beijing 100876, China"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "deep neural learning",
          "relationship_type": "基于",
          "evidence_text": "Deep neural learning, inspired by biological propagation phenomena in the human brain, has seen rapid development since 2006 and has achieved success in tasks involving single modal data."
        },
        {
          "method_name": "topic models, joint models, undirected Markov random fields",
          "relationship_type": "扩展",
          "evidence_text": "Various approaches have been proposed for learning from cross-modal data. These include extensions of topic models, joint models, and undirected Markov random fields, but these single-hidden-layer models struggle with the complexity of images and text."
        }
      ],
      "compared_methods": [
        {
          "method_name": "Multilayer Perceptrons (MLP)",
          "comparison_result": "Our deep neural architecture outperforms an MLP-based system",
          "evidence_text": "Our deep neural architecture outperforms an MLP-based system with two hidden layers and a CCA-based system with two RBMs, achieving an accuracy of 88.96%."
        },
        {
          "method_name": "Canonical Correlation Analysis (CCA)",
          "comparison_result": "The CCA-based system performs better than the MLP-based system",
          "evidence_text": "The CCA-based system performs better than the MLP-based system."
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "Small ESP Game dataset",
          "dataset_description": "contains 100,000 labeled images with corresponding tags",
          "evidence_text": "We evaluate our method for image annotation selection against MLP and CCA on two datasets: the Small ESP Game dataset and the MLC-2013 dataset."
        },
        {
          "dataset_name": "MLC-2013 dataset",
          "dataset_description": "has 1,000 manually labeled images with two labels per image",
          "evidence_text": "We evaluate our method for image annotation selection against MLP and CCA on two datasets: the Small ESP Game dataset and the MLC-2013 dataset."
        }
      ],
      "evaluation_metrics": [
        "accuracy",
        "area under the ROC curve"
      ],
      "baseline_methods": [
        "Multilayer Perceptrons (MLP)",
        "Canonical Correlation Analysis (CCA)"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "accuracy",
          "our_result": "88.96%",
          "baseline_result": "原文无此信息",
          "dataset": "MLC-2013 dataset",
          "evidence_text": "Our deep neural architecture outperforms an MLP-based system with two hidden layers and a CCA-based system with two RBMs, achieving an accuracy of 88.96%."
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "proposes the BDA to measure similarity in multimodal systems",
        "combines feature extraction and deep neural networks",
        "demonstrates effectiveness in classifying image tags"
      ],
      "stated_novelty": [
        "the BDA's three components are designed to extract features, stack RBMs, and use a variant autoencoder for discriminative learning"
      ],
      "stated_advantages": [
        "captures information not addressed by CCA",
        "flexible and can be extended to other modalities"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文明确承认的局限性，如无则为空数组"
      ],
      "failure_cases": [
        "原文提到的失败案例，如无则为空数组"
      ]
    },
    "extraction_metadata": {
      "file_id": "Mathematical_Problems_in_Engineering___2015___Li___Obtaining_Cross_Modal_Similarity_Metric_with_Deep_Neural_Architecture",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 13:24:27",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑技术关系method_name: topic models, joint models, undirected Markov random fields"
        ],
        "suspicious_count": 2,
        "validation_score": 60,
        "quality_level": "medium"
      },
      "text_length": 19030,
      "processed_text_length": 14776
    }
  },
  "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "Modality Disentangled Discriminator for Text-to-Image Synthesis",
      "authors": [
        "Fangxiang Feng",
        "Tianrui Niu",
        "Ruifan Li",
        "Member, IEEE",
        "Xiaojie Wang"
      ],
      "institutions": [
        "原文无此信息"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "AttnGAN",
          "relationship_type": "改进",
          "evidence_text": "The modality disentangled discriminator is integrated into two models: AttnGAN and DM-GAN."
        },
        {
          "method_name": "DM-GAN",
          "relationship_type": "改进",
          "evidence_text": "The modality disentangled discriminator is integrated into two models: AttnGAN and DM-GAN."
        }
      ],
      "compared_methods": [
        {
          "method_name": "GAN-INT-CLS",
          "comparison_result": "原文无此信息",
          "evidence_text": "In terms of style manipulation experiments on the CUB dataset, two types of experiments are conducted. The first involves style transfer, where the model uses modality-specific features for direct style transfer, achieving more accurate style reconstruction compared to GAN-INT-CLS."
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "CUB",
          "dataset_description": "原文无此信息",
          "evidence_text": "These models are evaluated on the CUB, Oxford-102, and COCO datasets."
        },
        {
          "dataset_name": "Oxford-102",
          "dataset_description": "原文无此信息",
          "evidence_text": "These models are evaluated on the CUB, Oxford-102, and COCO datasets."
        },
        {
          "dataset_name": "COCO",
          "dataset_description": "原文无此信息",
          "evidence_text": "These models are evaluated on the CUB, Oxford-102, and COCO datasets."
        }
      ],
      "evaluation_metrics": [
        "Inception Score (IS)",
        "Fréchet Inception Distance (FID)",
        "R-Precision"
      ],
      "baseline_methods": [
        "AttnGAN",
        "DM-GAN"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "Inception Score (IS)",
          "our_result": "4.86",
          "baseline_result": "4.75",
          "dataset": "CUB",
          "evidence_text": "On the CUB dataset, it increases the Inception Score (IS) from 4.75 to 4.86"
        },
        {
          "metric_name": "Fréchet Inception Distance (FID)",
          "our_result": "15.76",
          "baseline_result": "16.09",
          "dataset": "CUB",
          "evidence_text": "On the CUB dataset, it decreases the Fréchet Inception Distance (FID) from 16.09 to 15.76"
        },
        {
          "metric_name": "R-Precision",
          "our_result": "69.88%",
          "baseline_result": "67.82%",
          "dataset": "CUB",
          "evidence_text": "AttnGAN-MDD increasing the R-precision rate from 67.82% to 69.88% on CUB"
        },
        {
          "metric_name": "Inception Score (IS)",
          "our_result": "4.23",
          "baseline_result": "4.18",
          "dataset": "Oxford-102",
          "evidence_text": "On the Oxford-102 dataset, the IS is enhanced from 4.18 to 4.23"
        },
        {
          "metric_name": "Fréchet Inception Distance (FID)",
          "our_result": "40.18",
          "baseline_result": "41.35",
          "dataset": "Oxford-102",
          "evidence_text": "On the Oxford-102 dataset, the FID reduced from 41.35 to 40.18"
        },
        {
          "metric_name": "Inception Score (IS)",
          "our_result": "34.46",
          "baseline_result": "30.49",
          "dataset": "COCO",
          "evidence_text": "On the COCO dataset, the IS jumps from 30.49 to 34.46"
        },
        {
          "metric_name": "Fréchet Inception Distance (FID)",
          "our_result": "24.30",
          "baseline_result": "32.64",
          "dataset": "COCO",
          "evidence_text": "On the COCO dataset, the FID plummets from 32.64 to 24.30"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "The proposed discriminator is the first to learn modality disentangled representation for text-to-image synthesis, enhancing discrimination of image-text correlation and facilitating image synthesis manipulation."
      ],
      "stated_novelty": [
        "原文无此信息"
      ],
      "stated_advantages": [
        "our GAN-MDDs offer similar model size and training/testing time but with improved performance and capabilities"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文无此信息"
      ],
      "failure_cases": [
        "For instance, the orientation of the birds in all generated images is reversed."
      ]
    },
    "extraction_metadata": {
      "file_id": "Modality_Disentangled_Discriminator_for_Text_to_Image_Synthesis",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 13:25:16",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 20254,
      "processed_text_length": 14897
    }
  },
  "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "Multi-level fusion with deep neural networks for multimodal sentiment classification",
      "authors": [
        "Zhang Guangwei",
        "Zhao Bing",
        "Li Ruifan"
      ],
      "institutions": [
        "School of Computer Sciences, Beijing University of Posts and Communications, Beijing 100876, China",
        "School of Science, Yanshan University, Qinhuangdao 066004, China",
        "School of Artificial Intelligence, Beijing University of Posts and Communications, Beijing 100876, China"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "CNNs and RNNs",
          "relationship_type": "基于",
          "evidence_text": "Previous work utilized high-level features for fusion, such as the last outputs of different modal model layers."
        }
      ],
      "compared_methods": [
        {
          "method_name": "Image CNN",
          "comparison_result": "R-MFC shows improved performance",
          "evidence_text": "The MFC model shows improved performance over several baseline methods, with the ITIGNN method achieving the best results."
        },
        {
          "method_name": "Text CNN",
          "comparison_result": "R-MFC shows improved performance",
          "evidence_text": "The MFC model shows improved performance over several baseline methods, with the ITIGNN method achieving the best results."
        },
        {
          "method_name": "ITIGNN",
          "comparison_result": "MFC method employing fewer parameters outperforms ITIGNN",
          "evidence_text": "We propose that our MFC method, employing fewer parameters, outperforms ITIGNN, which uses a pretrained CNN combined with graph neural networks for textual and visual analysis."
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "Flickr dataset",
          "dataset_description": "原文无此信息",
          "evidence_text": "Experiments on the Flickr dataset show the MFC method achieves comparable performance with strong baseline methods."
        }
      ],
      "evaluation_metrics": [
        "Accuracy",
        "Recall",
        "F1 score"
      ],
      "baseline_methods": [
        "Image CNN",
        "Text CNN",
        "ITIGNN"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "Accuracy",
          "our_result": "0.884",
          "baseline_result": "0.675",
          "dataset": "Flickr dataset",
          "evidence_text": "R-MFC        | 0.801    | 0.799  | 0.800    |"
        },
        {
          "metric_name": "F1 score",
          "our_result": "0.891",
          "baseline_result": "原文无此信息",
          "dataset": "Flickr dataset",
          "evidence_text": "R-MFC        | ...      | ...    | 0.800    |"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "A multi-level fusion classification (MFC) model that fuses features from different levels by exploiting their dependencies.",
        "An R-MFC module to address sentiment conflicts in social network posts."
      ],
      "stated_novelty": [
        "The architecture uses convolutional neural networks (CNNs) to extract features in image and text modalities and a bi-directional (Bi) recurrent neural network (RNN) to integrate features from different CNN layers."
      ],
      "stated_advantages": [
        "The proposed method effectively integrates different levels of features from multiple branches in image and text CNNs by exploiting their dependencies with the Bi-GRU approach."
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文未明确提及"
      ],
      "failure_cases": [
        "原文未明确提及"
      ]
    },
    "extraction_metadata": {
      "file_id": "Multi_level_fusion_with_deep_neural_networks_for_multimodal_sentiment_classification",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 13:26:56",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑技术关系comparison_result: R-MFC shows improved performance",
          "可疑技术关系comparison_result: R-MFC shows improved performance",
          "可疑技术关系comparison_result: MFC method employing fewer parameters outperforms ITIGNN"
        ],
        "suspicious_count": 4,
        "validation_score": 20,
        "quality_level": "low"
      },
      "text_length": 12333,
      "processed_text_length": 12333
    }
  },
  "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "Clothing Image Recognition with Multiple Features and Extreme Learning Machines",
      "authors": [
        "Author names not explicitly provided in the text"
      ],
      "institutions": [
        "Institutions not explicitly provided in the text"
      ],
      "publication_venue": "Publication venue not explicitly provided in the text"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "Traditional CNNs",
          "relationship_type": "改进",
          "evidence_text": "Traditional convolutional neural networks (CNNs) do not always provide a satisfactory balance between training time and recognition performance."
        },
        {
          "method_name": "ELMs",
          "relationship_type": "结合",
          "evidence_text": "We propose a recognition framework based on multiple features and extreme learning machines (ELMs)."
        }
      ],
      "compared_methods": [
        {
          "method_name": "Hand-crafted features and deep learning methods",
          "comparison_result": "原文无此信息",
          "evidence_text": "Existing approaches can be categorized into hand-crafted features and deep learning methods."
        },
        {
          "method_name": "CNN features with 4,096 hidden neurons",
          "comparison_result": "Best performance with the ELM classifier is achieved using CNN features with 4,096 hidden neurons, reaching an accuracy of 80.6% in the test set.",
          "evidence_text": "The best performance with the ELM classifier is achieved using CNN features with 4,096 hidden neurons, reaching an accuracy of 80.6% in the test set."
        },
        {
          "method_name": "HOG features with 8,192 hidden neurons",
          "comparison_result": "Best performance is with 8,192 hidden neurons, achieving 71.8% accuracy.",
          "evidence_text": "For HOG features, the best performance is with 8,192 hidden neurons, achieving 71.8% accuracy."
        },
        {
          "method_name": "Color histograms with 4,096 hidden neurons",
          "comparison_result": "Best performance is with 4,096 hidden neurons, at an accuracy of 41.6%.",
          "evidence_text": "For color histograms, the best performance is with 4,096 hidden neurons, at an accuracy of 41.6%."
        },
        {
          "method_name": "MLP classifier with CNN features and 2,048 hidden neurons",
          "comparison_result": "Best performance is with CNN features and 2,048 hidden neurons, at an accuracy of 80.8%.",
          "evidence_text": "The best performance with the MLP classifier is with CNN features and 2,048 hidden neurons, at an accuracy of 80.8%."
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "DeepFashion dataset",
          "dataset_description": "Contains 300,000 clothing images across fifty categories.",
          "evidence_text": "For our experiments, we used the DeepFashion dataset, which contains 300,000 clothing images across fifty categories."
        }
      ],
      "evaluation_metrics": [
        "Accuracy"
      ],
      "baseline_methods": [
        "CNN combined with a multi-layer perceptron (MLP)"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "Accuracy",
          "our_result": "82.0%",
          "baseline_result": "80.8%",
          "dataset": "DeepFashion dataset",
          "evidence_text": "Using the AE-ELM based on CNN and HOG features with an MLP classifier and 2,048 hidden neurons achieves the best performance, with an accuracy of 82.0% in the test set."
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "A recognition framework based on multiple features and ELMs",
        "Deep feature-level fusion using Autoencoder-ELMs",
        "Ensemble decision-making with Ada-ELMs"
      ],
      "stated_novelty": [
        "The proposed framework extracts features, fuses them using an Autoencoder variant of ELM, and classifies images with an ensemble strategy called Ada-ELMs"
      ],
      "stated_advantages": [
        "Competitive performance in clothing image recognition",
        "Balanced time and recognition accuracy"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "The fusion of three features does not improve results",
        "Training time consumption of AE-ELM and Ada-ELMs"
      ],
      "failure_cases": [
        "Misclassifications primarily between similar clothing items"
      ]
    },
    "extraction_metadata": {
      "file_id": "Multiple_Features_With_Extreme_Learning_Machines_For_Clothing_Image_Recognition",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 13:29:46",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑title: Clothing Image Recognition with Multiple Features and Extreme Learning Machines",
          "可疑authors: ['Author names not explicitly provided in the text']",
          "可疑institutions: ['Institutions not explicitly provided in the text']",
          "可疑publication_venue: Publication venue not explicitly provided in the text",
          "可疑技术关系method_name: Traditional CNNs",
          "可疑技术关系method_name: Hand-crafted features and deep learning methods",
          "可疑技术关系comparison_result: Best performance with the ELM classifier is achieved using CNN features with 4,096 hidden neurons, reaching an accuracy of 80.6% in the test set.",
          "可疑技术关系evidence_text: The best performance with the ELM classifier is achieved using CNN features with 4,096 hidden neurons, reaching an accuracy of 80.6% in the test set.",
          "可疑技术关系method_name: HOG features with 8,192 hidden neurons",
          "可疑技术关系comparison_result: Best performance is with 8,192 hidden neurons, achieving 71.8% accuracy.",
          "可疑技术关系method_name: Color histograms with 4,096 hidden neurons",
          "可疑技术关系comparison_result: Best performance is with 4,096 hidden neurons, at an accuracy of 41.6%.",
          "可疑技术关系method_name: MLP classifier with CNN features and 2,048 hidden neurons",
          "可疑技术关系comparison_result: Best performance is with CNN features and 2,048 hidden neurons, at an accuracy of 80.8%.",
          "可疑技术关系evidence_text: The best performance with the MLP classifier is with CNN features and 2,048 hidden neurons, at an accuracy of 80.8%."
        ],
        "suspicious_count": 16,
        "validation_score": 0,
        "quality_level": "low"
      },
      "text_length": 21566,
      "processed_text_length": 14976
    }
  },
  "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "Revisiting Counterfactual Problems in Referring Expression Comprehension",
      "authors": [
        "Zhihan Yu",
        "Ruifan Li"
      ],
      "institutions": [
        "School of Artificial Intelligence, Beijing University of Posts and Telecommunications, Beijing, China"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "ReferItGame",
          "relationship_type": "基于",
          "evidence_text": "Our C-REC samples are based on fine-grained attributes from referring expressions, inspired by the ReferItGame [18]."
        },
        {
          "method_name": "BERT",
          "relationship_type": "结合",
          "evidence_text": "Candidate word prediction using BERT"
        }
      ],
      "compared_methods": [
        {
          "method_name": "SCRE",
          "comparison_result": "原文无此信息",
          "evidence_text": "Related work on counterfactual REC includes approaches like SCRE"
        },
        {
          "method_name": "MTG",
          "comparison_result": "原文无此信息",
          "evidence_text": "and MTG, which treat C-REC as a matching task based on logical rules."
        },
        {
          "method_name": "SimREC",
          "comparison_result": "our model outperforms one-stage REC models, particularly SimREC",
          "evidence_text": "Table 4 shows that our model outperforms one-stage REC models, particularly SimREC [29]."
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "RefCOCO",
          "dataset_description": "based on MS-COCO images and differ in the types of descriptions allowed",
          "evidence_text": "We evaluate our C-REC framework on three REC benchmark datasets (RefCOCO, RefCOCO+, RefCOCOg)"
        },
        {
          "dataset_name": "C-RefCOCO",
          "dataset_description": "generated using the CSG method to balance normal and counterfactual samples",
          "evidence_text": "and our constructed C-REC datasets (C-RefCOCO/+/g)."
        }
      ],
      "evaluation_metrics": [
        "Acc-Box (IoU@0.5)",
        "Acc-Cls",
        "Acc-Cf"
      ],
      "baseline_methods": [
        "Random",
        "Conf. score (0.01)",
        "Conf. score (0.1)",
        "Conf. score (0.5)",
        "Binary classifier"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "Acc-Box",
          "our_result": "原文无此信息",
          "baseline_result": "原文无此信息",
          "dataset": "RefCOCO/+/g",
          "evidence_text": "Table 4. Acc-Box (%) comparison of our model with baseline models on RefCOCO/+/g."
        },
        {
          "metric_name": "Acc-Cls",
          "our_result": "原文无此信息",
          "baseline_result": "up to 90%",
          "dataset": "C-RefCOCO/+/g",
          "evidence_text": "Table 5. Acc-Cls (%) on C-RefCOCO/+/g. Our model's performance against random choice, various confidence scores, and a binary classifier."
        },
        {
          "metric_name": "Acc-Cf",
          "our_result": "原文无此信息",
          "baseline_result": "原文无此信息",
          "dataset": "C-RefCOCO/+/g",
          "evidence_text": "Table 6. Acc-Cf (%) of our model on C-RefCOCO/+/g."
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "deep examination of fine-grained attributes in C-REC",
        "effective sample generation method",
        "robust C-REC framework"
      ],
      "stated_novelty": [
        "原文无此信息"
      ],
      "stated_advantages": [
        "incorporates contrastive learning with generated counterfactual samples"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文无此信息"
      ],
      "failure_cases": [
        "Our model successfully identifies most mismatched attributes but encounters difficulties with complex queries, such as less attention to the size attribute 'small' and an overemphasis on absolute location 'center'."
      ]
    },
    "extraction_metadata": {
      "file_id": "Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 13:30:29",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑技术关系evidence_text: Candidate word prediction using BERT"
        ],
        "suspicious_count": 2,
        "validation_score": 60,
        "quality_level": "medium"
      },
      "text_length": 19809,
      "processed_text_length": 14917
    }
  },
  "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "Visual Prompt Tuning for Weakly Supervised Phrase Grounding",
      "authors": [
        "Pengyue Lin",
        "Zhihan Yu",
        "Mingcong Lu",
        "Fangxiang Feng",
        "Ruifan Li",
        "Xiaojie Wang"
      ],
      "institutions": [
        "School of Artificial Intelligence, Beijing University of Posts and Telecommunications"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "Weakly-Supervised Grounding (WSG)",
          "relationship_type": "改进",
          "evidence_text": "We propose a refinement-based approach using a detector-free phrase grounding model fine-tuned with a visual prompt from CLIP text-related representations."
        },
        {
          "method_name": "CLIP",
          "relationship_type": "结合",
          "evidence_text": "Our approach leverages the relationship between CLIP and the grounding model, refining training through visual prompt tuning."
        }
      ],
      "compared_methods": [
        {
          "method_name": "WWbl",
          "comparison_result": "Our method shows improvements in metrics on Flickr30K and ReferIt",
          "evidence_text": "Our method is trained on COCO and VG train splits and evaluated on the test splits of Flickr30K, VG, and ReferIt. The performance of our method is compared with state-of-the-art DF-WSG methods quantitatively and qualitatively."
        },
        {
          "method_name": "Gbs",
          "comparison_result": "Our approach not only surpasses detector-free methods like Gbs and WWbl",
          "evidence_text": "Our approach not only surpasses detector-free methods like Gbs and WWbl but also outperforms detector-based methods on almost all categories for Flickr30K Entities."
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "Flickr30K Entities",
          "dataset_description": "原文无此信息",
          "evidence_text": "Four benchmark datasets are used for experimental evaluation: Flickr30K Entities"
        },
        {
          "dataset_name": "COCO",
          "dataset_description": "原文无此信息",
          "evidence_text": "Four benchmark datasets are used for experimental evaluation: COCO"
        },
        {
          "dataset_name": "Visual Genome",
          "dataset_description": "原文无此信息",
          "evidence_text": "Four benchmark datasets are used for experimental evaluation: Visual Genome"
        },
        {
          "dataset_name": "ReferIt",
          "dataset_description": "原文无此信息",
          "evidence_text": "Four benchmark datasets are used for experimental evaluation: ReferIt"
        }
      ],
      "evaluation_metrics": [
        "pointing game accuracy",
        "bounding box accuracy"
      ],
      "baseline_methods": [
        "WWbl",
        "Gbs"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "pointing game accuracy",
          "our_result": "原文无此信息",
          "baseline_result": "原文无此信息",
          "dataset": "Flickr30K and ReferIt",
          "evidence_text": "Our method shows improvements in metrics on Flickr30K and ReferIt"
        },
        {
          "metric_name": "bounding box accuracy",
          "our_result": "原文无此信息",
          "baseline_result": "原文无此信息",
          "dataset": "Flickr30K Entities",
          "evidence_text": "Our approach not only surpasses detector-free methods like Gbs and WWbl but also outperforms detector-based methods on almost all categories for Flickr30K Entities."
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "use of similarity tokens for spatial information capture",
        "detector-free network fine-tuning"
      ],
      "stated_novelty": [
        "原文无此信息"
      ],
      "stated_advantages": [
        "enhancing weakly-supervised phrase grounding",
        "overcoming performance constraints of the CLIP structure"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文无此信息"
      ],
      "failure_cases": [
        "原文无此信息"
      ]
    },
    "extraction_metadata": {
      "file_id": "Visual_Prompt_Tuning_for_Weakly_Supervised_Phrase_Grounding",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 13:31:41",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑title: Visual Prompt Tuning for Weakly Supervised Phrase Grounding"
        ],
        "suspicious_count": 2,
        "validation_score": 60,
        "quality_level": "medium"
      },
      "text_length": 8347,
      "processed_text_length": 8347
    }
  },
  "全卷积神经结构的段落式图像描述算法": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "原文无此信息",
      "authors": [],
      "institutions": [],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "编码器与解码器组合的端到端结构",
          "relationship_type": "基于",
          "evidence_text": "当前，段落式图像描述算法主要采用编码器与解码器组合的端到端结构。"
        }
      ],
      "compared_methods": [
        {
          "method_name": "基于RNN的传统方法",
          "comparison_result": "该算法相比基于RNN的传统方法，能生成更连贯的段落式文本描述。",
          "evidence_text": "实验结果表明，该算法相比基于RNN的传统方法，能生成更连贯的段落式文本描述。"
        },
        {
          "method_name": "Hierarchical-RNN方法",
          "comparison_result": "在层次性结构有效性方面优于Hierarchical-RNN方法，提升了17.8%的解码性能。",
          "evidence_text": "本文提出的基于全卷积神经网络结构的段落式图像描述生成模型，在层次性结构有效性方面优于Hierarchical-RNN方法，提升了17.8%的解码性能。"
        },
        {
          "method_name": "Sentence-Concat、Image-Flat和Hierarchical-RNN方法",
          "comparison_result": "在CIDEr指标上，相比Sentence-Concat、Image-Flat和Hierarchical-RNN方法，取得了更优的评测结果。",
          "evidence_text": "所提方法在CIDEr指标上，相比Sentence-Concat、Image-Flat和Hierarchical-RNN方法，取得了更优的评测结果，有效提高了生成段落的品质，弥补了传统方法在描述能力上的不足。"
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "斯坦福大学最新建立的图像段落描述公开数据集",
          "dataset_description": "原文无此信息",
          "evidence_text": "实验采用斯坦福大学最新建立的图像段落描述公开数据集进行验证，结果表明所提方法在CIDEr等评价指标上优于基线方法，验证了其有效性。"
        }
      ],
      "evaluation_metrics": [
        "CIDEr"
      ],
      "baseline_methods": [
        "原文无此信息"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "CIDEr",
          "our_result": "原文无此信息",
          "baseline_result": "原文无此信息",
          "dataset": "斯坦福大学最新建立的图像段落描述公开数据集",
          "evidence_text": "实验结果表明所提方法在CIDEr等评价指标上优于基线方法，验证了其有效性。"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "提出了一种全卷积神经结构的段落式图像描述算法"
      ],
      "stated_novelty": [
        "原文未明确提及"
      ],
      "stated_advantages": [
        "生成更具连贯性的段落式图像描述"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "RNN解码器在长时记忆和梯度消失问题上存在局限"
      ],
      "failure_cases": [
        "原文未明确提及"
      ]
    },
    "extraction_metadata": {
      "file_id": "全卷积神经结构的段落式图像描述算法",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 13:32:12",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑技术关系comparison_result: 在CIDEr指标上，相比Sentence-Concat、Image-Flat和Hierarchical-RNN方法，取得了更优的评测结果。"
        ],
        "suspicious_count": 2,
        "validation_score": 60,
        "quality_level": "medium"
      },
      "text_length": 3158,
      "processed_text_length": 3158
    }
  },
  "基于统计和加权的提高击键认证识别方法(英文)": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "Improved Keystroke Authentication Accuracy Based on Statistics and Weight",
      "authors": [
        "Li Jian",
        "Guo Xiaojing",
        "Li Meiyun",
        "Li Ruifan"
      ],
      "institutions": [
        "School of Computer, Beijing University of Posts and Telecommunications, Beijing 100876, P. R. China"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "Keystroke Dynamics-Based Authentication (KDA)",
          "relationship_type": "改进",
          "evidence_text": "This paper aims to enhance login-password recognition accuracy using biometric characteristics, which are unique and cannot be stolen, lost, or forgotten."
        }
      ],
      "compared_methods": [
        {
          "method_name": "TOP10 detector",
          "comparison_result": "TOP10 showing the best improvement with an increase in the number of samples",
          "evidence_text": "The performance of each detector is analyzed, with TOP10 showing the best improvement with an increase in the number of samples."
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "benchmark dataset",
          "dataset_description": "from 51 subjects typing a strong 10-character password",
          "evidence_text": "A benchmark dataset from 51 subjects typing a strong 10-character password is used."
        }
      ],
      "evaluation_metrics": [
        "FRR",
        "FAR",
        "EER",
        "FAR + FRR"
      ],
      "baseline_methods": [
        "原文无此信息"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "EER",
          "our_result": "原文无此信息",
          "baseline_result": "原文无此信息",
          "dataset": "benchmark data set for keystroke dynamics",
          "evidence_text": "The results of the four detectors, including EER (Equal Error Rate), FAR, FRR, and FAR + FRR, are presented in Table I."
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "proposes a methodology for improving the recognition accuracy of keystroke authentication"
      ],
      "stated_novelty": [
        "uses typing biometrics to enhance login-password authentication"
      ],
      "stated_advantages": [
        "efficiency compared to the other three detectors",
        "smallest sum of FAR and FRR"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文无此信息"
      ],
      "failure_cases": [
        "原文无此信息"
      ]
    },
    "extraction_metadata": {
      "file_id": "基于统计和加权的提高击键认证识别方法(英文)",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 13:32:37",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 8405,
      "processed_text_length": 8405
    }
  },
  "一种使用深层结构获取双模态相似性测度的方法_李睿凡": {
    "document_metadata": {
      "document_type": "unknown",
      "title": "原文无此信息",
      "authors_or_creators": []
    },
    "main_content": {
      "stated_purpose": "本发明通过深度学习框架，实现了双模态数据相似性的计算，适用于多模态数据挖掘和检索等领域。",
      "key_concepts": [
        "双模态相似性测度",
        "深层结构",
        "受限波尔兹曼机",
        "自动编码器",
        "L1范数",
        "相似性测度C",
        "损失函数"
      ],
      "main_methods": [
        "使用经典特征提取方法",
        "堆叠的两层受限波尔兹曼机转换",
        "自动编码器编码",
        "相似性测度计算",
        "训练算法优化"
      ]
    },
    "relationships": {
      "references_to": [],
      "builds_on": [
        "BB-RBM神经网络结构"
      ]
    },
    "note": "文档类型不明确，仅提取最基础信息",
    "extraction_metadata": {
      "file_id": "一种使用深层结构获取双模态相似性测度的方法_李睿凡",
      "detected_doc_type": "unknown",
      "extraction_time": "2025-08-02 13:32:49",
      "validation": {
        "warnings": [
          "可疑document_type: unknown"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 1190,
      "processed_text_length": 1190
    }
  },
  "一种图像的文本描述方法及装置_李睿凡": {
    "document_metadata": {
      "document_type": "unknown",
      "title": "原文无此信息",
      "authors_or_creators": []
    },
    "main_content": {
      "stated_purpose": "提供一种图像的文本描述方法及装置，通过分层结构提高生成的文本段落中句子之间的连贯性，并降低训练过程的计算复杂度",
      "key_concepts": [
        "图像的文本描述方法",
        "区域特征",
        "全局特征",
        "句子级子网络",
        "词汇子网络",
        "连贯性",
        "计算复杂度"
      ],
      "main_methods": [
        "获取待描述图像",
        "提取区域特征和全局特征",
        "输入句子级子网络",
        "输入词汇级子网络",
        "训练神经网络"
      ]
    },
    "relationships": {
      "references_to": [],
      "builds_on": [
        "现有的基于循环神经网络的方案"
      ]
    },
    "note": "文档类型不明确，仅提取最基础信息",
    "extraction_metadata": {
      "file_id": "一种图像的文本描述方法及装置_李睿凡",
      "detected_doc_type": "unknown",
      "extraction_time": "2025-08-02 13:33:00",
      "validation": {
        "warnings": [
          "可疑document_type: unknown"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 1104,
      "processed_text_length": 1104
    }
  },
  "一种图像色彩和谐程度的评估方法及装置_鲁鹏": {
    "document_metadata": {
      "document_type": "patent",
      "title": "原文无此信息",
      "inventors": [],
      "applicant": "原文无此信息",
      "patent_number": "原文无此信息"
    },
    "technical_solution": {
      "technical_problem": "现有技术中，技术人员根据经验选取一些与图像美学评价相关的图像特征描述图像美学质量。然而，经验性地提取图像特征来进行美学评估是一项繁琐的工作，严重依赖于技术人员对照相领域知识的理解，技术人员难以选择合适的图像美学评价的图像特征，致使图像美学评估的准确度不高。",
      "solution_overview": "本发明提出了一种基于条件随机场的色彩和谐模型，同时考虑影响局部区域色彩和谐度的两种因素。",
      "key_technical_features": [
        "利用机器方法从大量的图片中学习色彩和谐关系",
        "基于条件随机场的色彩和谐模型",
        "考虑局部区域色彩和谐度与邻域图像块之间的色彩和谐关系"
      ]
    },
    "implementation": {
      "embodiments": [
        "通过预设的无向图建立初始条件随机场",
        "获取多张高质量样本图像和多张低质量样本图像进行神经网络训练",
        "利用训练后的神经网络确定关联势函数及交互势函数",
        "对待评估图像进行色彩和谐评估"
      ],
      "technical_effects": [
        "提高图像美学评估的准确性",
        "实现自动对图像的色彩和谐程度进行评估"
      ]
    },
    "application_scope": {
      "application_fields": [
        "图像色彩和谐程度的评估"
      ],
      "use_scenarios": [
        "可以应用于可以计算处理图像的终端，例如计算机"
      ]
    },
    "claims_info": {
      "main_claims": [
        "一种图像色彩和谐程度的评估方法、装置、电子设备及计算机可读存储介质"
      ]
    },
    "extraction_metadata": {
      "file_id": "一种图像色彩和谐程度的评估方法及装置_鲁鹏",
      "detected_doc_type": "patent",
      "extraction_time": "2025-08-02 13:33:18",
      "validation": {
        "warnings": [
          "可疑document_type: patent"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 3038,
      "processed_text_length": 3038
    }
  },
  "一种基于人工智能挖掘的网络内容风控管理系统_李睿凡": {
    "document_metadata": {
      "document_type": "unknown",
      "title": "原文无此信息",
      "authors_or_creators": []
    },
    "main_content": {
      "stated_purpose": "提供一种基于人工智能挖掘的网络内容风控管理系统",
      "key_concepts": [
        "网络内容风控管理系统",
        "新业务内容检测模块",
        "网站内容监管模块",
        "企业内容安全治理模块",
        "UGC内容审核模块",
        "人工智能挖掘"
      ],
      "main_methods": [
        "自动获取新业务内容",
        "内容审核",
        "分类",
        "网站内容检测",
        "主动拨测",
        "旁路检测",
        "文件共享的内容采集技术",
        "用户原创内容审核",
        "文本识别",
        "图像识别",
        "视频识别",
        "音频识别"
      ]
    },
    "relationships": {
      "references_to": [],
      "builds_on": [
        "硬件",
        "处理器",
        "计算机程序",
        "计算机可读介质",
        "互联网下载"
      ]
    },
    "note": "文档类型不明确，仅提取最基础信息",
    "extraction_metadata": {
      "file_id": "一种基于人工智能挖掘的网络内容风控管理系统_李睿凡",
      "detected_doc_type": "unknown",
      "extraction_time": "2025-08-02 13:33:30",
      "validation": {
        "warnings": [
          "可疑document_type: unknown"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 1065,
      "processed_text_length": 1065
    }
  },
  "一种基于图卷积神经网络的方面级情感分析方法及装置_冯方向": {
    "document_metadata": {
      "document_type": "原文无此信息",
      "title": "原文无此信息",
      "authors_or_creators": []
    },
    "main_content": {
      "stated_purpose": "提供一种基于图卷积神经网络的方面级情感分析方法",
      "key_concepts": [
        "图卷积神经网络",
        "方面级情感分析",
        "双向长短期记忆网络BiLSTM",
        "自注意力机制",
        "句法加权图"
      ],
      "main_methods": [
        "获取句子及其中的方面词",
        "预处理得到输入向量序列和句法加权图",
        "使用双重图卷积神经网络进行情感分析",
        "训练双重图卷积神经网络模型"
      ]
    },
    "relationships": {
      "references_to": [],
      "builds_on": [
        "Glove词嵌入",
        "依存句法分析器",
        "BiLSTM",
        "自注意力机制"
      ]
    },
    "note": "文档类型不明确，仅提取最基础信息",
    "extraction_metadata": {
      "file_id": "一种基于图卷积神经网络的方面级情感分析方法及装置_冯方向",
      "detected_doc_type": "unknown",
      "extraction_time": "2025-08-02 13:33:42",
      "validation": {
        "warnings": [],
        "suspicious_count": 0,
        "validation_score": 100,
        "quality_level": "high"
      },
      "text_length": 2851,
      "processed_text_length": 2851
    }
  },
  "一种基于图卷积神经网络的方面级情感分析方法及装置_李睿凡": {
    "document_metadata": {
      "document_type": "unknown",
      "title": "原文无此信息",
      "authors_or_creators": []
    },
    "main_content": {
      "stated_purpose": "提供一种基于图卷积神经网络的方面级情感分析方法",
      "key_concepts": [
        "图卷积神经网络",
        "方面级情感分析",
        "句法图卷积子神经网络",
        "自注意力机制的语义图卷积子神经网络"
      ],
      "main_methods": [
        "获取待进行方面情感分析的句子及其中的方面词",
        "对句子和方面词进行预处理",
        "输入预先训练的双重图卷积神经网络",
        "计算损失函数并更新模型参数"
      ]
    },
    "relationships": {
      "references_to": [],
      "builds_on": [
        "图卷积神经网络"
      ]
    },
    "note": "文档类型不明确，仅提取最基础信息",
    "extraction_metadata": {
      "file_id": "一种基于图卷积神经网络的方面级情感分析方法及装置_李睿凡",
      "detected_doc_type": "unknown",
      "extraction_time": "2025-08-02 13:33:53",
      "validation": {
        "warnings": [
          "可疑document_type: unknown"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 1779,
      "processed_text_length": 1779
    }
  },
  "一种基于对应的深层信念网络的跨模态检索方法_李睿凡": {
    "document_metadata": {
      "document_type": "unknown",
      "title": "原文无此信息",
      "authors_or_creators": []
    },
    "main_content": {
      "stated_purpose": "提出一种基于对应的深层信念网络的跨模态检索方法",
      "key_concepts": [
        "深层信念网络",
        "跨模态检索",
        "Corr-DBN",
        "双受限波尔兹曼机RBM",
        "Corr-RBM",
        "相关性约束"
      ],
      "main_methods": [
        "特征提取",
        "双RBM模型",
        "Corr-RBM模型",
        "欧氏距离计算"
      ]
    },
    "relationships": {
      "references_to": [],
      "builds_on": [
        "至少一层双受限波尔兹曼机RBM结构",
        "对应的受限波尔兹曼机Corr-RBM结构"
      ]
    },
    "note": "文档类型不明确，仅提取最基础信息",
    "extraction_metadata": {
      "file_id": "一种基于对应的深层信念网络的跨模态检索方法_李睿凡",
      "detected_doc_type": "unknown",
      "extraction_time": "2025-08-02 13:34:05",
      "validation": {
        "warnings": [
          "可疑document_type: unknown"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 1139,
      "processed_text_length": 1139
    }
  },
  "一种基于强化学习的电力设备检修决策生成方法_李睿凡": {
    "document_metadata": {
      "document_type": "原文无此信息",
      "title": "一种基于强化学习的电力设备检修决策生成方法",
      "authors_or_creators": []
    },
    "main_content": {
      "stated_purpose": "最小化电网整体运行损失",
      "key_concepts": [
        "割集",
        "强化学习",
        "马尔可夫决策过程",
        "动态权重",
        "价值矩阵",
        "最优策略"
      ],
      "main_methods": [
        "计算割集",
        "应用强化学习方法",
        "动态规划求解",
        "加权电网运行损失"
      ]
    },
    "relationships": {
      "references_to": [],
      "builds_on": []
    },
    "note": "文档类型不明确，仅提取最基础信息",
    "extraction_metadata": {
      "file_id": "一种基于强化学习的电力设备检修决策生成方法_李睿凡",
      "detected_doc_type": "unknown",
      "extraction_time": "2025-08-02 15:11:44",
      "validation": {
        "warnings": [],
        "suspicious_count": 0,
        "validation_score": 100,
        "quality_level": "high"
      },
      "text_length": 1151,
      "processed_text_length": 1151
    }
  },
  "一种基于深层模型的跨模态检索方法_李睿凡": {
    "document_metadata": {
      "document_type": "unknown",
      "title": "原文无此信息",
      "authors_or_creators": []
    },
    "main_content": {
      "stated_purpose": "提出一种基于深层模型的跨模态检索方法",
      "key_concepts": [
        "跨模态检索",
        "深层模型",
        "受限波尔兹曼机",
        "Corr-RBMs",
        "低级表达向量",
        "高级表达向量",
        "欧氏距离"
      ],
      "main_methods": [
        "利用特征提取方法获得表达向量",
        "使用Corr-RBMs深层模型处理表达向量",
        "计算模态间的欧氏距离",
        "匹配检索对象"
      ]
    },
    "relationships": {
      "references_to": [],
      "builds_on": [
        "受限波尔兹曼机Corr-RBM模型"
      ]
    },
    "note": "文档类型不明确，仅提取最基础信息",
    "extraction_metadata": {
      "file_id": "一种基于深层模型的跨模态检索方法_李睿凡",
      "detected_doc_type": "unknown",
      "extraction_time": "2025-08-02 15:11:55",
      "validation": {
        "warnings": [
          "可疑document_type: unknown"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 1099,
      "processed_text_length": 1099
    }
  },
  "一种基于自动编码器的视频分类方法及装置_李睿凡": {
    "document_metadata": {
      "document_type": "patent",
      "title": "原文无此信息",
      "inventors": [],
      "applicant": "原文无此信息",
      "patent_number": "原文无此信息"
    },
    "technical_solution": {
      "technical_problem": "原文无此信息",
      "solution_overview": "本发明提供了一种基于自动编码器的视频分类方法及装置，通过获取目标视频的图像、音频和文本三种模态数据的低级表示内容，并利用堆叠自动编码器组、双模态融合器和三模态融合器进行高级表示内容的处理，最后使用有监督分类模型进行视频分类。",
      "key_technical_features": [
        "使用自动编码器进行模态数据的高级表示处理",
        "双模态融合器和三模态融合器的应用",
        "有监督分类模型的运用"
      ]
    },
    "implementation": {
      "embodiments": [
        "获取目标视频的图像、音频和文本模态数据的低级表示内容",
        "使用堆叠自动编码器组获得高级表示内容",
        "通过双模态融合器和三模态融合器处理数据",
        "利用有监督学习方式训练分类模型"
      ],
      "technical_effects": [
        "提高了视频分类的准确性"
      ]
    },
    "application_scope": {
      "application_fields": [
        "视频分类"
      ],
      "use_scenarios": [
        "原文未明确提及"
      ]
    },
    "claims_info": {
      "main_claims": [
        "一种基于自动编码器的视频分类方法及装置"
      ]
    },
    "extraction_metadata": {
      "file_id": "一种基于自动编码器的视频分类方法及装置_李睿凡",
      "detected_doc_type": "patent",
      "extraction_time": "2025-08-02 15:12:11",
      "validation": {
        "warnings": [
          "可疑document_type: patent"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 1487,
      "processed_text_length": 1487
    }
  },
  "一种多特征多通道图卷积网络模型训练方法及属性情感三元组抽取方法_李睿凡": {
    "document_metadata": {
      "document_type": "原文无此信息",
      "title": "一种多特征多通道图卷积网络模型训练方法及属性情感三元组抽取方法",
      "authors_or_creators": [
        "原文无此信息"
      ]
    },
    "main_content": {
      "stated_purpose": "提供一种多特征多通道图卷积网络模型训练方法，以提高属性情感三元组抽取的准确度",
      "key_concepts": [
        "多特征多通道图卷积网络",
        "属性情感三元组抽取",
        "邻接张量",
        "图卷积",
        "注意力机制"
      ],
      "main_methods": [
        "利用双仿射注意力机制生成邻接张量",
        "根据词性、句法依存类型、词对距离生成邻接张量",
        "进行图卷积和平均池化",
        "生成联合特征序列和联合张量",
        "基于分类函数得到概率分布张量",
        "计算总损失函数进行模型训练"
      ]
    },
    "relationships": {
      "references_to": [
        "原文无此信息"
      ],
      "builds_on": [
        "卷积神经网络(CNN)",
        "图卷积神经网络(GCN)"
      ]
    },
    "note": "文档类型不明确，仅提取最基础信息",
    "extraction_metadata": {
      "file_id": "一种多特征多通道图卷积网络模型训练方法及属性情感三元组抽取方法_李睿凡",
      "detected_doc_type": "unknown",
      "extraction_time": "2025-08-02 15:12:25",
      "validation": {
        "warnings": [],
        "suspicious_count": 0,
        "validation_score": 100,
        "quality_level": "high"
      },
      "text_length": 3232,
      "processed_text_length": 3232
    }
  },
  "一种方面级情感分析方法、装置、电子设备及存储介质_李睿凡": {
    "document_metadata": {
      "document_type": "原文无此信息",
      "title": "一种方面级情感分析方法、装置、电子设备及存储介质",
      "authors_or_creators": [
        "原文无此信息"
      ]
    },
    "main_content": {
      "stated_purpose": "提高情感分析结果的准确性",
      "key_concepts": [
        "方面级情感分析",
        "聚合特征处理",
        "双向长短期记忆网络",
        "图卷积神经网络"
      ],
      "main_methods": [
        "聚合特征模块",
        "特征转换模块",
        "位置编码模块",
        "情感分析模块"
      ]
    },
    "relationships": {
      "references_to": [
        "原文无此信息"
      ],
      "builds_on": [
        "相似结构数据集",
        "双向长短期记忆网络",
        "图卷积神经网络"
      ]
    },
    "note": "文档类型不明确，仅提取最基础信息",
    "extraction_metadata": {
      "file_id": "一种方面级情感分析方法、装置、电子设备及存储介质_李睿凡",
      "detected_doc_type": "unknown",
      "extraction_time": "2025-08-02 15:12:36",
      "validation": {
        "warnings": [],
        "suspicious_count": 0,
        "validation_score": 100,
        "quality_level": "high"
      },
      "text_length": 1950,
      "processed_text_length": 1950
    }
  },
  "人脸重定向模型、模型训练方法及装置_张航": {
    "document_metadata": {
      "document_type": "原文无此信息",
      "title": "人脸重定向模型、模型训练方法及装置",
      "authors_or_creators": []
    },
    "main_content": {
      "stated_purpose": "提高视线估计算法的样本集质量",
      "key_concepts": [
        "人脸重定向模型",
        "编码器",
        "无关属性分解模块",
        "交叉属性分离模块",
        "解码器",
        "特征提取",
        "角度旋转",
        "特征交换",
        "参数更新"
      ],
      "main_methods": [
        "特征提取",
        "特征交换",
        "图像生成",
        "参数更新",
        "正交损失函数计算"
      ]
    },
    "relationships": {
      "references_to": [],
      "builds_on": []
    },
    "note": "文档类型不明确，仅提取最基础信息",
    "extraction_metadata": {
      "file_id": "人脸重定向模型、模型训练方法及装置_张航",
      "detected_doc_type": "unknown",
      "extraction_time": "2025-08-02 15:12:48",
      "validation": {
        "warnings": [],
        "suspicious_count": 0,
        "validation_score": 100,
        "quality_level": "high"
      },
      "text_length": 1577,
      "processed_text_length": 1577
    }
  },
  "图像生成模型的训练方法和设备以及图像生成方法_杨博": {
    "document_metadata": {
      "document_type": "原文无此信息",
      "title": "原文无此信息",
      "authors_or_creators": []
    },
    "main_content": {
      "stated_purpose": "本发明涉及一种图像生成模型的训练方法和设备以及图像生成方法。",
      "key_concepts": [
        "图像生成模型",
        "样本数据",
        "句子级别编码",
        "词级别编码",
        "对抗子网络",
        "条件对抗损失函数值",
        "图像生成模型参数更新"
      ],
      "main_methods": [
        "获取样本数据",
        "句子级别和词级别编码",
        "无条件对抗子网络生成第一图像",
        "句子级别对抗子网络生成第二图像",
        "词级别对抗子网络生成第三图像",
        "损失函数值更新图像生成模型参数"
      ]
    },
    "relationships": {
      "references_to": [
        "Transformer",
        "已知技术",
        "注意力机制",
        "动态内存方法",
        "交叉熵损失计算方法"
      ],
      "builds_on": []
    },
    "note": "文档类型不明确，仅提取最基础信息",
    "extraction_metadata": {
      "file_id": "图像生成模型的训练方法和设备以及图像生成方法_杨博",
      "detected_doc_type": "unknown",
      "extraction_time": "2025-08-02 15:13:01",
      "validation": {
        "warnings": [],
        "suspicious_count": 0,
        "validation_score": 100,
        "quality_level": "high"
      },
      "text_length": 1290,
      "processed_text_length": 1290
    }
  },
  "基于特征分布迁移的小样本图像特征学习方法及装置_李晓旭": {
    "document_metadata": {
      "document_type": "原文无此信息",
      "title": "一种基于特征分布迁移的小样本图像特征学习方法及装置",
      "authors_or_creators": []
    },
    "main_content": {
      "stated_purpose": "减少小样本图像分类中的原型偏差，提高分类效果",
      "key_concepts": [
        "特征分布迁移",
        "小样本图像特征学习",
        "分布学习模块",
        "嵌入模块"
      ],
      "main_methods": [
        "对数据进行预处理",
        "预训练嵌入模块",
        "优化分布学习模块",
        "计算分布原型",
        "计算预测概率"
      ]
    },
    "relationships": {
      "references_to": [],
      "builds_on": []
    },
    "note": "文档类型不明确，仅提取最基础信息",
    "extraction_metadata": {
      "file_id": "基于特征分布迁移的小样本图像特征学习方法及装置_李晓旭",
      "detected_doc_type": "unknown",
      "extraction_time": "2025-08-02 15:13:12",
      "validation": {
        "warnings": [],
        "suspicious_count": 0,
        "validation_score": 100,
        "quality_level": "high"
      },
      "text_length": 1132,
      "processed_text_length": 1132
    }
  },
  "基于自训练的小样本图像集成分类方法及装置_李晓旭": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "原文无此信息",
      "authors": [],
      "institutions": [],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "Baseline++",
          "relationship_type": "基于",
          "evidence_text": "本研究中，我们采用Baseline++网络结构构建基分类器"
        }
      ],
      "compared_methods": [
        {
          "method_name": "原文无此信息",
          "comparison_result": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "原文无此信息",
          "dataset_description": "原文对数据集的描述为：有标签的支持集、无标签的支持集和查询集",
          "evidence_text": "对于新类数据，我们抽取三个数据集：有标签的支持集、无标签的支持集和查询集"
        }
      ],
      "evaluation_metrics": [],
      "baseline_methods": [
        "Baseline++"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "原文无此信息",
          "our_result": "原文无此信息",
          "baseline_result": "原文无此信息",
          "dataset": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "解决了基分类器多样性问题",
        "提升了图像分类效果"
      ],
      "stated_novelty": [
        "基于查询样本自训练和模型平均的集成分类方法"
      ],
      "stated_advantages": [
        "具有很高的使用价值"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [],
      "failure_cases": []
    },
    "extraction_metadata": {
      "file_id": "基于自训练的小样本图像集成分类方法及装置_李晓旭",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 15:13:30",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 1361,
      "processed_text_length": 1361
    }
  },
  "基于语言引导的指称表达理解推理网络系统及推理方法_李睿凡": {
    "document_metadata": {
      "document_type": "unknown",
      "title": "基于语言引导的指称表达理解推理网络系统及推理方法",
      "authors_or_creators": [
        "原文未明确提及"
      ]
    },
    "main_content": {
      "stated_purpose": "本发明提供一种基于语言引导的指称表达理解推理网络系统及推理方法",
      "key_concepts": [
        "语言引导",
        "指称表达式",
        "推理网络系统",
        "跨模态特征",
        "文本特征提取器",
        "图像特征提取器",
        "文本特征扩展器(TFE)",
        "跨模态对齐模块(TCA)",
        "跨模态融合模块(TCF)",
        "LGR-NET"
      ],
      "main_methods": [
        "多模态特征提取",
        "文本特征扩展",
        "文本引导的跨模态对齐",
        "文本引导的跨模态融合",
        "预测头",
        "损失和训练"
      ]
    },
    "relationships": {
      "references_to": [
        "原文未明确提及"
      ],
      "builds_on": [
        "Swin Transformer",
        "BERT",
        "注意力机制",
        "多头自注意力机制",
        "残差链接",
        "层归一化",
        "FFN",
        "sigmoid激活函数",
        "GIoU损失",
        "L1损失",
        "REC任务"
      ]
    },
    "note": "文档类型不明确，仅提取最基础信息",
    "extraction_metadata": {
      "file_id": "基于语言引导的指称表达理解推理网络系统及推理方法_李睿凡",
      "detected_doc_type": "unknown",
      "extraction_time": "2025-08-02 15:13:45",
      "validation": {
        "warnings": [
          "可疑document_type: unknown"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 2125,
      "processed_text_length": 2125
    }
  },
  "负例训练样本采集方法、装置及模型训练方法、装置_李睿凡": {
    "document_metadata": {
      "document_type": "原文无此信息",
      "title": "一种负例训练样本采集方法及模型训练方法",
      "authors_or_creators": [
        "原文无此信息"
      ]
    },
    "main_content": {
      "stated_purpose": "提供一种负例训练样本采集方法以及基于该方法的模型训练方法，以提高图像检索模型的检索准确率",
      "key_concepts": [
        "负例训练样本采集方法",
        "模型训练方法",
        "图像检索模型",
        "聚类",
        "目标概率",
        "训练样本"
      ],
      "main_methods": [
        "对表示向量进行聚类",
        "确定目标向量的第一聚类和候选聚类的目标概率",
        "执行聚类抽取操作获取负例训练样本",
        "模型训练过程中调整聚类结果"
      ]
    },
    "relationships": {
      "references_to": [
        "原文无此信息"
      ],
      "builds_on": [
        "原文无此信息"
      ]
    },
    "note": "文档类型不明确，仅提取最基础信息",
    "extraction_metadata": {
      "file_id": "负例训练样本采集方法、装置及模型训练方法、装置_李睿凡",
      "detected_doc_type": "unknown",
      "extraction_time": "2025-08-02 15:14:00",
      "validation": {
        "warnings": [
          "可疑title: 一种负例训练样本采集方法及模型训练方法"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 2527,
      "processed_text_length": 2527
    }
  },
  "全卷积神经结构的段落式图像描述算法_李睿凡": {
    "document_metadata": {
      "document_type": "原文无此信息",
      "title": "原文无此信息",
      "authors_or_creators": []
    },
    "main_content": {
      "stated_purpose": "为图像生成描述性的自然语言段落，是跨模态智能的重要研究方向，也是盲人导航和幼儿早期教育等应用的核心技术",
      "key_concepts": [
        "段落式图像描述",
        "编码器与解码器组合",
        "端到端结构",
        "卷积神经网络",
        "循环神经网络",
        "全卷积结构的图像段落描述算法",
        "门控机制",
        "连贯性"
      ],
      "main_methods": [
        "采用基于卷积网络的区域检测器获取图像表示",
        "构建层次性的深度卷积解码器对图像表示解码",
        "将门控机制嵌入卷积解码器网络中以提升模型的记忆能力"
      ]
    },
    "relationships": {
      "references_to": [
        "Vinyals O, Toshev A, Bengio S, et al. Show and tell: a neural image caption generator",
        "Lu Jiasen, Xiong Caiming, Parikh D, et al. Knowing when to look: adaptive attention via a visual sentinel for image captioning",
        "Mao Yuzhao, Zhou Chang, Wang Xiaojie, et al. Show and tell more: topic-oriented multi-sentence image captioning",
        "...以及其他参考文献"
      ],
      "builds_on": [
        "原文未明确提及"
      ]
    },
    "note": "文档类型不明确，仅提取最基础信息",
    "extraction_metadata": {
      "file_id": "全卷积神经结构的段落式图像描述算法_李睿凡",
      "detected_doc_type": "unknown",
      "extraction_time": "2025-08-02 15:14:16",
      "validation": {
        "warnings": [],
        "suspicious_count": 0,
        "validation_score": 100,
        "quality_level": "high"
      },
      "text_length": 1693,
      "processed_text_length": 1693
    }
  },
  "增强提示学习的少样本文本分类方法_李睿凡": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "针对少样本文本分类任务的一种提示学习增强分类算法",
      "authors": [],
      "institutions": [],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "自然语言推理",
          "relationship_type": "基于",
          "evidence_text": "该算法首先将文本分类任务转换为基于自然语言推理的提示学习形式"
        },
        {
          "method_name": "掩码语言模型",
          "relationship_type": "结合",
          "evidence_text": "同时，掩码语言模型任务作为正则项，提升模型泛化能力"
        }
      ],
      "compared_methods": [
        {
          "method_name": "基于微调方法",
          "comparison_result": "在小样本学习场景中模型性能通过表现不佳",
          "evidence_text": "对于基于微调的方法，在小样本学习场景中模型性能通过表现不佳"
        },
        {
          "method_name": "PET",
          "comparison_result": "在小样本学习场景中模型的准确率都有大幅提高",
          "evidence_text": "通过对比EPL4FTC算法与其他基于提示学习的方法(PET、ADAPET、LM-BFF、EFL和P-tuning等)"
        },
        {
          "method_name": "ADAPET",
          "comparison_result": "在利用预训练模型中已经学习到的通用知识基础上，引入下游任务的类别信息实现更好的建模效果",
          "evidence_text": "与转换为完形填空任务形式的PET和ADAPET等方法相比"
        },
        {
          "method_name": "EFL",
          "comparison_result": "在任务的平均准确率上高出4.2%",
          "evidence_text": "与转化为文本蕴含任务的EFL方法相比"
        },
        {
          "method_name": "LM-BFF",
          "comparison_result": "在任务的平均准确率上高出1.6%",
          "evidence_text": "与使用自动构建模板或是非自然语言形式模板的LM-BFF和P-tuning方法相比"
        },
        {
          "method_name": "P-tuningR",
          "comparison_result": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "FewCLUE",
          "dataset_description": "中文数据集使用少样本评测数据集",
          "evidence_text": "中文数据集使用少样本评测数据集FewCLUE中的文本分类任务对应的数据集"
        },
        {
          "dataset_name": "AG News",
          "dataset_description": "学术新闻搜索引擎从多个新闻来源中搜集超过了100万篇新闻文章构成的数据集",
          "evidence_text": "英文数据集采用3个英文文本分类数据集AG News、TREC以及Yelp Review进行评测"
        },
        {
          "dataset_name": "TREC",
          "dataset_description": "包含6个一级标签和47个二级标签",
          "evidence_text": "其中，TREC数据集包含6个一级标签和47个二级标签"
        },
        {
          "dataset_name": "Yelp Review",
          "dataset_description": "来自Yelp的用户评论，标签是用户对商品的星级打分，共分为5级",
          "evidence_text": "Yelp Review数据集来自Yelp的用户评论"
        }
      ],
      "evaluation_metrics": [
        "准确率(Accuracy)"
      ],
      "baseline_methods": [
        "基于微调方法",
        "Zero-shot方法",
        "Zero-shot(GPT)方法",
        "PET方法",
        "ADAPET方法",
        "LM-BFF方法",
        "P-tuningR方法",
        "EFL方法"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "准确率",
          "our_result": "原文无此信息",
          "baseline_result": "原文无此信息",
          "dataset": "EPRSTMT",
          "evidence_text": "在EPRSTMT数据集上取得了优异的成绩"
        },
        {
          "metric_name": "准确率",
          "our_result": "原文无此信息",
          "baseline_result": "原文无此信息",
          "dataset": "CSLDCP",
          "evidence_text": "在CSLDCP数据集上取得了优异的成绩"
        },
        {
          "metric_name": "准确率",
          "our_result": "原文无此信息",
          "baseline_result": "原文无此信息",
          "dataset": "TNEWS",
          "evidence_text": "在TNEWS数据集上取得了优异的成绩"
        },
        {
          "metric_name": "准确率",
          "our_result": "原文无此信息",
          "baseline_result": "原文无此信息",
          "dataset": "IFLYTEK",
          "evidence_text": "在IFLYTEK数据集上也取得了与其他现有方法同等效果的性能"
        },
        {
          "metric_name": "准确率",
          "our_result": "平均准确率性能上取得了最高的成绩",
          "baseline_result": "原文无此信息",
          "dataset": "中文少样本数据集",
          "evidence_text": "EPL4FTC算法在中文文本分类任务的平均准确率性能上取得了最高的成绩"
        },
        {
          "metric_name": "准确率",
          "our_result": "模型准确率远高于其他方法",
          "baseline_result": "原文无此信息",
          "dataset": "英文数据集",
          "evidence_text": "EPL4FTC算法能够有效地对下游任务进行建模，也进一步说明了该算法的有效性"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "提出了一种名为EPL4FTC的提示学习增强分类算法"
      ],
      "stated_novelty": [
        "利用三元组损失增强提示学习，通过句子和句群粒度的三元组损失优化，捕获下游任务的类别信息"
      ],
      "stated_advantages": [
        "在少样本文本分类任务上取得了明显的准确率性能优势"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文无此信息"
      ],
      "failure_cases": [
        "原文无此信息"
      ]
    },
    "extraction_metadata": {
      "file_id": "增强提示学习的少样本文本分类方法_李睿凡",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 15:15:08",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑title: 针对少样本文本分类任务的一种提示学习增强分类算法"
        ],
        "suspicious_count": 2,
        "validation_score": 60,
        "quality_level": "medium"
      },
      "text_length": 4267,
      "processed_text_length": 4267
    }
  },
  "引入深度学习的人工智能类课程": {
    "document_metadata": {
      "document_type": "原文无此信息",
      "title": "引入深度学习的人工智能类课程",
      "authors_or_creators": [
        "李睿凡",
        "王小捷",
        "钟义信"
      ]
    },
    "main_content": {
      "stated_purpose": "提出在人工智能类课程中引入深度学习的初步内容和实施建议，分析其必要性和可行性",
      "key_concepts": [
        "人工智能",
        "深度学习",
        "教学建议"
      ],
      "main_methods": [
        "原文未明确提及"
      ]
    },
    "relationships": {
      "references_to": [
        "[1] 觚l I，Rose D C，Kamowsl【i TP．Deep machine Learning：a new frontier in artificial intelligence research[J]．IEEE Compumfional Intelligence Magazine，2010，5(4)：13-18．",
        "[2] Dalai G E，Sainath T N，Hinton G E．Improving deep neural networks for LVCSR using rectified linear units and dropout[C]／／2013IEEE Imemafional conference on acoustic speech and signal processing，2013．",
        "[3] Krizhevsky A，Sutskever I，Hinton G．ImageNet classification with deep convolutional neural networks[J]．Advances in Neural Information Processing Systems，2012(25)：1 106-1 114．",
        "[4] Hinton G，Salaldautdinov 1L Reducing the dimensionality of data丽tll neural networks[J]．Science，2006，5786(3 13)：504—507．",
        "[5] Haykin．Neural networks and learning machines[M]．New York：Prentice Hall，2008．",
        "[6] 钟义信．高等人工智能：人工智能理论的新阶段[J]．计算机教育，2012(9)：6．11．",
        "[7] Bengio YLearning de印architecturesfor AJ[J]．Foundations and Trendsin Machine Learning，2009，2(1)，1-127．"
      ],
      "builds_on": [
        "原文未明确提及"
      ]
    },
    "note": "文档类型不明确，仅提取最基础信息",
    "extraction_metadata": {
      "file_id": "引入深度学习的人工智能类课程",
      "detected_doc_type": "unknown",
      "extraction_time": "2025-08-02 15:15:30",
      "validation": {
        "warnings": [],
        "suspicious_count": 0,
        "validation_score": 100,
        "quality_level": "high"
      },
      "text_length": 1528,
      "processed_text_length": 1528
    }
  },
  "智能科学与技术专业本科生导师制的研究与实践": {
    "document_metadata": {
      "document_type": "theoretical_paper",
      "title": "文章针对如何实施智能科学与技术专业本科生导师制进行研究",
      "authors": [
        "无明确列出"
      ],
      "institutions": [
        "北京邮电大学智能科学技术中心"
      ]
    },
    "theoretical_contributions": {
      "main_theoretical_results": [
        "无明确的理论结果"
      ],
      "theorems_proposed": [
        "无明确的定理"
      ],
      "mathematical_models": [
        "无明确的数学模型"
      ]
    },
    "technical_relationships": {
      "builds_upon": [
        "导师制教育模式",
        "智能科学与技术专业教育现状"
      ],
      "extends": [
        "无明确扩展的理论"
      ],
      "relates_to": [
        "已有的导师制实施经验",
        "智能科学与技术专业特点"
      ]
    },
    "innovation_analysis": {
      "theoretical_novelty": [
        "探索适合智能科学与技术专业本科2、3年级学生的导师制工作形式和办法"
      ],
      "mathematical_contributions": [
        "无明确的数学贡献"
      ]
    },
    "applications": {
      "potential_applications": [
        "提高智能科学与技术专业教育质量",
        "促进学生个性化发展"
      ],
      "application_domains": [
        "教育领域",
        "智能科学与技术专业"
      ]
    },
    "note": "理论性论文，不包含实验数据",
    "extraction_metadata": {
      "file_id": "智能科学与技术专业本科生导师制的研究与实践",
      "detected_doc_type": "theoretical_paper",
      "extraction_time": "2025-08-02 15:15:50",
      "validation": {
        "warnings": [
          "可疑document_type: theoretical_paper",
          "可疑authors: ['无明确列出']"
        ],
        "suspicious_count": 2,
        "validation_score": 60,
        "quality_level": "medium"
      },
      "text_length": 7246,
      "processed_text_length": 7246
    }
  },
  "深度学习中卷积神经网络的教学探讨": {
    "document_metadata": {
      "document_type": "原文无此信息",
      "title": "深度学习中卷积神经网络的教学探讨",
      "authors_or_creators": [
        "李睿凡",
        "陈佳洁",
        "周延泉",
        "王小捷",
        "钟义信"
      ]
    },
    "main_content": {
      "stated_purpose": "探讨如何开展卷积神经网络的教学工作，包括教学内容的安排和教学内容之外的考虑两个方面，旨在将智能科学与技术的这一最新成果介绍给学生，使他们能较早接触学科前沿，提升学习兴趣，激发创新动力",
      "key_concepts": [
        "智能科学与技术",
        "深度学习",
        "卷积神经网络",
        "教学建议"
      ],
      "main_methods": [
        "原文未明确提及"
      ]
    },
    "relationships": {
      "references_to": [
        "McCulloch和Pitts的人工神经元",
        "Minsky和Papert的感知器分析",
        "John Hopfield的Hopfield神经网络模型",
        "多层前向神经网络的反向传播算法",
        "Geoffrey Hinton与Salakhutdinov的深度学习"
      ],
      "builds_on": [
        "传统的人工智能专业课程",
        "深度神经网络的发展"
      ]
    },
    "note": "文档类型不明确，仅提取最基础信息",
    "extraction_metadata": {
      "file_id": "深度学习中卷积神经网络的教学探讨",
      "detected_doc_type": "unknown",
      "extraction_time": "2025-08-02 15:16:05",
      "validation": {
        "warnings": [],
        "suspicious_count": 0,
        "validation_score": 100,
        "quality_level": "high"
      },
      "text_length": 1097,
      "processed_text_length": 1097
    }
  },
  "计算机游戏中的智能技术": {
    "document_metadata": {
      "document_type": "原文无此信息",
      "title": "智能信息处理 计算机游戏中的智能技术",
      "authors_or_creators": [
        "李睿凡",
        "左申正",
        "李卫"
      ]
    },
    "main_content": {
      "stated_purpose": "原文未明确提及",
      "key_concepts": [
        "人工智能",
        "有限状态机",
        "群体智能",
        "神经网络",
        "遗传算法"
      ],
      "main_methods": [
        "原文无此信息"
      ]
    },
    "relationships": {
      "references_to": [
        "【1】",
        "【2】",
        "【3】",
        "【4】",
        "【5】",
        "【6】",
        "【8】",
        "【9】",
        "【10】",
        "【11】",
        "【12】",
        "【13】",
        "【14】",
        "【15】",
        "【16】",
        "【17】",
        "【18】",
        "【19】",
        "【20】",
        "【21】",
        "【22】",
        "【23】",
        "【24】",
        "【25】",
        "【26】",
        "【27】",
        "【28】"
      ],
      "builds_on": [
        "原文未明确提及"
      ]
    },
    "note": "文档类型不明确，仅提取最基础信息",
    "extraction_metadata": {
      "file_id": "计算机游戏中的智能技术",
      "detected_doc_type": "unknown",
      "extraction_time": "2025-08-02 15:16:20",
      "validation": {
        "warnings": [],
        "suspicious_count": 0,
        "validation_score": 100,
        "quality_level": "high"
      },
      "text_length": 4775,
      "processed_text_length": 4775
    }
  },
  "面向_智能科学与技术_专业的C语言教学探讨": {
    "document_metadata": {
      "document_type": "原文无此信息",
      "title": "面向‘智能科学与技术’专业的C语言教学探讨",
      "authors_or_creators": [
        "李睿凡",
        "李蕾"
      ]
    },
    "main_content": {
      "stated_purpose": "本文探讨了‘智能科学与技术’专业C语言教学的变革，针对新专业要求和学时压缩的问题，提出教学内容和方法改变的具体措施。引入机器智能前沿问题作为实践项目，旨在培养学生的专业兴趣和能力。",
      "key_concepts": [
        "C语言",
        "智能科学与技术",
        "教学"
      ],
      "main_methods": [
        "教学内容整合设计",
        "前沿研究导向的项目实践",
        "教学相关因素讨论"
      ]
    },
    "relationships": {
      "references_to": [
        "[1]谭浩强．C高级语言程序设计[m]．2版．北京：清华大学出版社，2006．",
        "[2]陈良银，游洪跃，李旭伟．C语言程序设计[m]．C1999．北京：清华大学出版社，2006．",
        "[3]Brian W．K．，Denni S M．R．The C Programming Language．2版．北京：机械工业出版社，2006．",
        "[4]TopCoder．ht tp：／／wwW．topcoder．com／．",
        "[5]蔡自兴，徐光秸．人工智能及其应用(研究生用书)[m]．3版．北京：清华大学出版社，2007．",
        "[6]钟义信．机器知行学原理[m]．北京：科学出版社，2006．",
        "[7]Russell S．，Norvig P．Artificial Intelligence：A Modern Approach．2版．北京：人民邮电出版社，2004．"
      ],
      "builds_on": [
        "原文未明确提及"
      ]
    },
    "note": "文档类型不明确，仅提取最基础信息",
    "extraction_metadata": {
      "file_id": "面向_智能科学与技术_专业的C语言教学探讨",
      "detected_doc_type": "unknown",
      "extraction_time": "2025-08-02 15:16:38",
      "validation": {
        "warnings": [
          "可疑title: 面向‘智能科学与技术’专业的C语言教学探讨"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 1518,
      "processed_text_length": 1518
    }
  },
  "鲁棒局部保持投影的表情识别": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "鲁棒局部保持投影的表情识别",
      "authors": [
        "李睿凡",
        "朱强生",
        "郭燕慧",
        "刘海涛"
      ],
      "institutions": [
        "北京邮电大学信息工程学院",
        "中国民航大学通信工程系"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "局部保持投影",
          "relationship_type": "改进",
          "evidence_text": "针对局部保持投影的流形学习算法对于噪声与异常值的敏感性，提出了一种鲁棒的局部保持投影算法。"
        }
      ],
      "compared_methods": [
        {
          "method_name": "Gabor小波特征提取方法",
          "comparison_result": "原文无此信息",
          "evidence_text": "以Gabor小波为基准测试了算法性能。"
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "JAFFE数据库",
          "dataset_description": "原文无此信息",
          "evidence_text": "JAFFE数据库的表情识别实验中"
        }
      ],
      "evaluation_metrics": [
        "原文无此信息"
      ],
      "baseline_methods": [
        "最近邻分类器"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "原文无此信息",
          "our_result": "原文无此信息",
          "baseline_result": "原文无此信息",
          "dataset": "JAFFE数据库",
          "evidence_text": "实验结果表明鲁棒改进算法的有效性。"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "提出了一种鲁棒的局部保持投影算法"
      ],
      "stated_novelty": [
        "利用了带权值的主成分分析算法",
        "在样本数据集X上执行局部鲁棒主成分分析"
      ],
      "stated_advantages": [
        "有效提高了表情识别的鲁棒性"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文无此信息"
      ],
      "failure_cases": [
        "原文无此信息"
      ]
    },
    "extraction_metadata": {
      "file_id": "鲁棒局部保持投影的表情识别",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 15:16:59",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 2327,
      "processed_text_length": 2327
    }
  },
  "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "低郁密度条件下果园轮式机器人行间运行控制方法研究",
      "authors": [
        "Lu Hao"
      ],
      "institutions": [
        "Beijing Institute of Technology"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "原文无此信息",
          "relationship_type": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ],
      "compared_methods": [
        {
          "method_name": "PID控制方法",
          "comparison_result": "模糊控制方法优于PID控制方法",
          "evidence_text": "模糊控制具有跟踪平滑，超调量小，后期无反复波动等特点。稳态误差问题也可以通过输入及输出端论域划分得到进一步解决。"
        },
        {
          "method_name": "纯追踪方法",
          "comparison_result": "模糊控制方法优于纯追踪方法",
          "evidence_text": "模糊控制具有跟踪平滑，超调量小，后期无反复波动等特点。稳态误差问题也可以通过输入及输出端论域划分得到进一步解决。"
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "原文无此信息",
          "dataset_description": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ],
      "evaluation_metrics": [
        "原文无此信息"
      ],
      "baseline_methods": [
        "PID控制方法",
        "纯追踪方法"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "横向误差",
          "our_result": "0.04米",
          "baseline_result": "原文无此信息",
          "dataset": "原文无此信息",
          "evidence_text": "根据以上仿真结果，模糊控制方法在起伏不平的果园路况下具有较好的适应性。"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "提出了一种基于陀螺仪的四轮垂直载荷实时估计方法",
        "提出了轮式滑动转向机器人坡道稳态转向动力学模型",
        "提出了同侧轮胎最优驱动力估计方法"
      ],
      "stated_novelty": [
        "原文无此信息"
      ],
      "stated_advantages": [
        "提高了滑动轮式机器人在果园环境下运行稳定性"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文无此信息"
      ],
      "failure_cases": [
        "原文无此信息"
      ]
    },
    "extraction_metadata": {
      "file_id": "低郁密度条件下果园轮式机器人行间运行控制方法研究_韩奕非",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 15:19:01",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑技术关系method_name: PID控制方法",
          "可疑技术关系comparison_result: 模糊控制方法优于PID控制方法",
          "可疑技术关系comparison_result: 模糊控制方法优于纯追踪方法"
        ],
        "suspicious_count": 4,
        "validation_score": 20,
        "quality_level": "low"
      },
      "text_length": 34590,
      "processed_text_length": 14788
    }
  },
  "基于优先级的时间敏感网络流量调度算法研究_李红硕": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "基于优先级的时间敏感网络流量调度算法研究",
      "authors": [
        "Yu Q",
        "等人"
      ],
      "institutions": [
        "西安电子科技大学"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "时间敏感网络流量调度",
          "relationship_type": "改进",
          "evidence_text": "本文针对现有调度方法计算复杂度高和路由规划问题，提出以下工作："
        }
      ],
      "compared_methods": [
        {
          "method_name": "基于帧的ILP调度方法",
          "comparison_result": "求解时间减少了36%，但总时延提高了7%",
          "evidence_text": "与基于帧的ILP调度方法相比，求解时间减少了36%，但总时延提高了7%。"
        },
        {
          "method_name": "基于时间窗口的OMT调度方法",
          "comparison_result": "求解时间减少了61%，有效降低了计算复杂度；总时延降低了40%",
          "evidence_text": "与基于时间窗口的OMT调度方法相比，求解时间减少了61%，有效降低了计算复杂度；总时延降低了40%。"
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "原文无此信息",
          "dataset_description": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ],
      "evaluation_metrics": [
        "丢包率",
        "平均端到端时延"
      ],
      "baseline_methods": [
        "SP-TSA算法",
        "CBS-TSA算法",
        "SPF算法"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "求解时间",
          "our_result": "减少了41%",
          "baseline_result": "原文无此信息",
          "dataset": "原文无此信息",
          "evidence_text": "本文提出的基于时间窗口的ILP调度方法求解时间减少了41%。"
        },
        {
          "metric_name": "总时延",
          "our_result": "提高了7%",
          "baseline_result": "原文无此信息",
          "dataset": "原文无此信息",
          "evidence_text": "与基于帧的ILP调度方法相比，总时延提高了7%。"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "提出基于时间窗口的ILP调度方法",
        "引入时间窗口偏移量优化方法"
      ],
      "stated_novelty": [
        "新颖的时间窗口调度对象",
        "结合软件定义网络（SDN）构建系统模型"
      ],
      "stated_advantages": [
        "降低了计算复杂度",
        "改善了时延性能",
        "提高了流量的可调度性"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "在路由规划时主要考虑链路利用率，对于实时调度场景下流量的可调度性问题考虑不足"
      ],
      "failure_cases": [
        "原文未明确提及"
      ]
    },
    "extraction_metadata": {
      "file_id": "基于优先级的时间敏感网络流量调度算法研究_李红硕",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 15:20:32",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑技术关系comparison_result: 求解时间减少了36%，但总时延提高了7%",
          "可疑技术关系evidence_text: 与基于帧的ILP调度方法相比，求解时间减少了36%，但总时延提高了7%。",
          "可疑技术关系comparison_result: 求解时间减少了61%，有效降低了计算复杂度；总时延降低了40%",
          "可疑技术关系evidence_text: 与基于时间窗口的OMT调度方法相比，求解时间减少了61%，有效降低了计算复杂度；总时延降低了40%。"
        ],
        "suspicious_count": 5,
        "validation_score": 0,
        "quality_level": "low"
      },
      "text_length": 12770,
      "processed_text_length": 12770
    }
  },
  "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "原文无此信息",
      "authors": [
        "原文无此信息"
      ],
      "institutions": [
        "原文无此信息"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "原文无此信息",
          "relationship_type": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ],
      "compared_methods": [
        {
          "method_name": "原文无此信息",
          "comparison_result": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "NYT-10",
          "dataset_description": "原文无此信息",
          "evidence_text": "本文使用了NYT-10和GDS两个公开数据集。"
        },
        {
          "dataset_name": "GDS",
          "dataset_description": "原文无此信息",
          "evidence_text": "本文使用了NYT-10和GDS两个公开数据集。"
        }
      ],
      "evaluation_metrics": [
        "PR曲线",
        "AUC值",
        "Precision@N",
        "Hits@K"
      ],
      "baseline_methods": [
        "Mintz",
        "MultiR",
        "MIMLR",
        "PCNN",
        "RESIDE",
        "DCRE",
        "PA-TRP"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "P@MEAN",
          "our_result": "原文无此信息",
          "baseline_result": "原文无此信息",
          "dataset": "原文无此信息",
          "evidence_text": "原文无此信息"
        },
        {
          "metric_name": "AUC",
          "our_result": "原文无此信息",
          "baseline_result": "原文无此信息",
          "dataset": "原文无此信息",
          "evidence_text": "原文无此信息"
        },
        {
          "metric_name": "Hits@K",
          "our_result": "原文无此信息",
          "baseline_result": "原文无此信息",
          "dataset": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "提出了一种从局部到全局进行学习的层级图卷积神经网络框架(L2G-GCN)"
      ],
      "stated_novelty": [
        "原文无此信息"
      ],
      "stated_advantages": [
        "原文无此信息"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文无此信息"
      ],
      "failure_cases": [
        "原文无此信息"
      ]
    },
    "extraction_metadata": {
      "file_id": "基于包结构的图神经网络远程监督关系抽取研究及应用_饶梓钦",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 15:21:14",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 23026,
      "processed_text_length": 14834
    }
  },
  "基于实体的概念指导的少样本关系抽取研究与应用_王泽元": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "原文无此信息",
      "authors": [
        "原文无此信息"
      ],
      "institutions": [
        "原文无此信息"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "embedding models and neural networks",
          "relationship_type": "基于",
          "evidence_text": "embedding models and neural networks have shown promise for improving relation extraction by connecting language and knowledge bases."
        }
      ],
      "compared_methods": [
        {
          "method_name": "Gormley et al. (2015) feature-rich compositional embedding model",
          "comparison_result": "原文无此信息",
          "evidence_text": "Gormley et al. (2015) proposed a model that uses a feature-rich compositional embedding to improve relation extraction."
        },
        {
          "method_name": "Liu et al. (2013) convolutional neural network model",
          "comparison_result": "原文无此信息",
          "evidence_text": "Liu et al. (2013) proposed a convolutional neural network model for relation extraction."
        },
        {
          "method_name": "Zhang et al. (2015) bidirectional long short-term memory network",
          "comparison_result": "原文无此信息",
          "evidence_text": "Zhang et al. (2015) used a bidirectional long short-term memory network for relation classification."
        },
        {
          "method_name": "Devlin et al. (2018) BERT model",
          "comparison_result": "原文无此信息",
          "evidence_text": "Devlin et al. (2018) used BERT for relation extraction by fine-tuning the model on a relation extraction task."
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "原文无此信息",
          "dataset_description": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ],
      "evaluation_metrics": [
        "accuracy of relation extraction"
      ],
      "baseline_methods": [
        "feature-rich compositional embedding model",
        "convolutional neural network model",
        "bidirectional long short-term memory network",
        "BERT model"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "accuracy",
          "our_result": "原文无此信息",
          "baseline_result": "原文无此信息",
          "dataset": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "connecting language and knowledge bases with embedding models for relation extraction"
      ],
      "stated_novelty": [
        "原文无此信息"
      ],
      "stated_advantages": [
        "capture semantic information",
        "improve the accuracy of relation extraction"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文无此信息"
      ],
      "failure_cases": [
        "原文无此信息"
      ]
    },
    "extraction_metadata": {
      "file_id": "基于实体的概念指导的少样本关系抽取研究与应用_王泽元",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 15:27:53",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑技术关系method_name: Gormley et al. (2015) feature-rich compositional embedding model",
          "可疑技术关系method_name: Liu et al. (2013) convolutional neural network model",
          "可疑技术关系method_name: Zhang et al. (2015) bidirectional long short-term memory network",
          "可疑技术关系method_name: Devlin et al. (2018) BERT model"
        ],
        "suspicious_count": 5,
        "validation_score": 0,
        "quality_level": "low"
      },
      "text_length": 9513,
      "processed_text_length": 9513
    }
  },
  "基于常识知识的多选式问答研究_张力翚": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "原文无此信息",
      "authors": [
        "原文无此信息"
      ],
      "institutions": [
        "原文无此信息"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "原文无此信息",
          "relationship_type": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ],
      "compared_methods": [
        {
          "method_name": "原文无此信息",
          "comparison_result": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "CommonsenseQA",
          "dataset_description": "常识问答数据集",
          "evidence_text": "本文在CommonsenseQA、OpenbookQA、SociallQA等三个常识问答数据集上开展了大量的实验。"
        },
        {
          "dataset_name": "OpenbookQA",
          "dataset_description": "常识问答数据集",
          "evidence_text": "本文在CommonsenseQA、OpenbookQA、SociallQA等三个常识问答数据集上开展了大量的实验。"
        },
        {
          "dataset_name": "SociallQA",
          "dataset_description": "常识问答数据集",
          "evidence_text": "本文在CommonsenseQA、OpenbookQA、SociallQA等三个常识问答数据集上开展了大量的实验。"
        }
      ],
      "evaluation_metrics": [
        "答案预测准确率"
      ],
      "baseline_methods": [
        "原文无此信息"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "答案预测准确率",
          "our_result": "原文无此信息",
          "baseline_result": "原文无此信息",
          "dataset": "CommonsenseQA",
          "evidence_text": "在CommonsenseQA数据集上，基于Huggingface框架部署实现了AristoBERTa。"
        },
        {
          "metric_name": "答案预测准确率",
          "our_result": "原文无此信息",
          "baseline_result": "原文无此信息",
          "dataset": "OpenbookQA",
          "evidence_text": "在OpenBookQA数据集上，基于Huggingface框架部署实现了AristoBERTa。"
        },
        {
          "metric_name": "答案预测准确率",
          "our_result": "原文无此信息",
          "baseline_result": "原文无此信息",
          "dataset": "SociallQA",
          "evidence_text": "在SociallQA数据集上，基于Huggingface框架部署实现了AristoBERTa。"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "原文无此信息"
      ],
      "stated_novelty": [
        "原文无此信息"
      ],
      "stated_advantages": [
        "原文无此信息"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文无此信息"
      ],
      "failure_cases": [
        "原文无此信息"
      ]
    },
    "extraction_metadata": {
      "file_id": "基于常识知识的多选式问答研究_张力翚",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 15:28:35",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 27086,
      "processed_text_length": 14999
    }
  },
  "基于提示学习的少样本文本分类研究与应用_魏志宇": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "原文无此信息",
      "authors": [
        "原文无此信息"
      ],
      "institutions": [
        "原文无此信息"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "原文无此信息",
          "relationship_type": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ],
      "compared_methods": [
        {
          "method_name": "原文无此信息",
          "comparison_result": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "原文无此信息",
          "dataset_description": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ],
      "evaluation_metrics": [
        "原文无此信息"
      ],
      "baseline_methods": [
        "原文无此信息"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "原文无此信息",
          "our_result": "原文无此信息",
          "baseline_result": "原文无此信息",
          "dataset": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "原文无此信息"
      ],
      "stated_novelty": [
        "原文无此信息"
      ],
      "stated_advantages": [
        "原文无此信息"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文无此信息"
      ],
      "failure_cases": [
        "原文无此信息"
      ]
    },
    "extraction_metadata": {
      "file_id": "基于提示学习的少样本文本分类研究与应用_魏志宇",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 15:29:10",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 15338,
      "processed_text_length": 14904
    }
  },
  "基于树结构的图像段落描述研究_石祎晖": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "原文无此信息",
      "authors": [
        "原文无此信息"
      ],
      "institutions": [
        "原文无此信息"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "原文无此信息",
          "relationship_type": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ],
      "compared_methods": [
        {
          "method_name": "原文无此信息",
          "comparison_result": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "原文无此信息",
          "dataset_description": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ],
      "evaluation_metrics": [
        "原文无此信息"
      ],
      "baseline_methods": [
        "原文无此信息"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "原文无此信息",
          "our_result": "原文无此信息",
          "baseline_result": "原文无此信息",
          "dataset": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "原文无此信息"
      ],
      "stated_novelty": [
        "原文无此信息"
      ],
      "stated_advantages": [
        "原文无此信息"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文无此信息"
      ],
      "failure_cases": [
        "原文无此信息"
      ]
    },
    "extraction_metadata": {
      "file_id": "基于树结构的图像段落描述研究_石祎晖",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 15:29:43",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 34047,
      "processed_text_length": 14965
    }
  },
  "基于深度卷积结构的图像段落描述研究_梁昊雨": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "基于深度卷积结构的图像段落描述研究",
      "authors": [
        "原文无此信息"
      ],
      "institutions": [
        "原文无此信息"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "Hierarchical-RNN",
          "relationship_type": "改进",
          "evidence_text": "本文针对以上若干问题，开展对段落式图像描述的研宄。"
        }
      ],
      "compared_methods": [
        {
          "method_name": "Hierarchical-RNN",
          "comparison_result": "全卷积解码器生成的段落具有更强的连贯性",
          "evidence_text": "全卷积解码器的平均Anaphora比Hierarchical-RNN高出44.44%，平均Anaphora率高出33.33%，这在一定程度上可说明该解码器生成的段落具有更强的连贯性。"
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "斯坦福图像-段落数据集",
          "dataset_description": "数据集中的每个样本包含一幅图像和一段描述该图像的段落",
          "evidence_text": "Krause等人在2017年公布了斯坦福图像-段落数据集（Stanford-Paragraph Dataset），数据集中的每个样本包含一幅图像和一段描述该图像的段落。"
        }
      ],
      "evaluation_metrics": [
        "CIDEr",
        "BLEU-n",
        "Antecedent",
        "Anaphora",
        "Anaphora率"
      ],
      "baseline_methods": [
        "Hierarchical-RNN"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "CIDEr",
          "our_result": "原文无此信息",
          "baseline_result": "原文无此信息",
          "dataset": "斯坦福图像-段落数据集",
          "evidence_text": "原文无此信息"
        },
        {
          "metric_name": "Antecedent",
          "our_result": "0.92",
          "baseline_result": "0.89",
          "dataset": "斯坦福图像-段落数据集",
          "evidence_text": "全卷积解码器的平均Antecedent比Hierarchical-RNN高出44.44%，平均Anaphora率高出33.33%，这在一定程度上可说明该解码器生成的段落具有更强的连贯性。"
        },
        {
          "metric_name": "Anaphora",
          "our_result": "0.13",
          "baseline_result": "0.09",
          "dataset": "斯坦福图像-段落数据集",
          "evidence_text": "全卷积解码器的平均Anaphora比Hierarchical-RNN高出44.44%，平均Anaphora率高出33.33%，这在一定程度上可说明该解码器生成的段落具有更强的连贯性。"
        },
        {
          "metric_name": "Anaphora率(%)",
          "our_result": "8.4",
          "baseline_result": "6.3",
          "dataset": "斯坦福图像-段落数据集",
          "evidence_text": "全卷积解码器的平均Anaphora比Hierarchical-RNN高出44.44%，平均Anaphora率高出33.33%，这在一定程度上可说明该解码器生成的段落具有更强的连贯性。"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "提出全卷积神经结构的段落解码器",
        "提出衡量段落连贯性的指标",
        "提出融合区域注意力的段落式图像描述模型Dual-CNN",
        "提出衡量段落内句子多样度的指标"
      ],
      "stated_novelty": [
        "原文无此信息"
      ],
      "stated_advantages": [
        "生成的段落具有更好的连贯性",
        "训练时间复杂度更低",
        "生成的段落描述更详细",
        "生成的段落内句子更丰富多样"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文无此信息"
      ],
      "failure_cases": [
        "原文无此信息"
      ]
    },
    "extraction_metadata": {
      "file_id": "基于深度卷积结构的图像段落描述研究_梁昊雨",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 15:31:18",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑技术关系comparison_result: 全卷积解码器生成的段落具有更强的连贯性"
        ],
        "suspicious_count": 2,
        "validation_score": 60,
        "quality_level": "medium"
      },
      "text_length": 15854,
      "processed_text_length": 14839
    }
  },
  "基于知识增强的方面级情感分析研究及应用_杜一帆": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "原文无此信息",
      "authors": [
        "原文无此信息"
      ],
      "institutions": [
        "原文无此信息"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "原文无此信息",
          "relationship_type": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ],
      "compared_methods": [
        {
          "method_name": "原文无此信息",
          "comparison_result": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "原文无此信息",
          "dataset_description": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ],
      "evaluation_metrics": [
        "原文无此信息"
      ],
      "baseline_methods": [
        "原文无此信息"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "原文无此信息",
          "our_result": "原文无此信息",
          "baseline_result": "原文无此信息",
          "dataset": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "原文无此信息"
      ],
      "stated_novelty": [
        "原文无此信息"
      ],
      "stated_advantages": [
        "原文无此信息"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文无此信息"
      ],
      "failure_cases": [
        "原文无此信息"
      ]
    },
    "extraction_metadata": {
      "file_id": "基于知识增强的方面级情感分析研究及应用_杜一帆",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 15:31:46",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 17638,
      "processed_text_length": 14895
    }
  },
  "基于非自回归方法的图像描述研究及应用_张煜松": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "原文无此信息",
      "authors": [
        "原文无此信息"
      ],
      "institutions": [
        "原文无此信息"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "原文无此信息",
          "relationship_type": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ],
      "compared_methods": [
        {
          "method_name": "原文无此信息",
          "comparison_result": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "原文无此信息",
          "dataset_description": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ],
      "evaluation_metrics": [
        "原文无此信息"
      ],
      "baseline_methods": [
        "原文无此信息"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "原文无此信息",
          "our_result": "原文无此信息",
          "baseline_result": "原文无此信息",
          "dataset": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "原文无此信息"
      ],
      "stated_novelty": [
        "原文无此信息"
      ],
      "stated_advantages": [
        "原文无此信息"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文无此信息"
      ],
      "failure_cases": [
        "原文无此信息"
      ]
    },
    "extraction_metadata": {
      "file_id": "基于非自回归方法的图像描述研究及应用_张煜松",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 15:32:16",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 28784,
      "processed_text_length": 14940
    }
  },
  "多元组细粒度情感分析研究及应用_翟泽鹏": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "原文无此信息",
      "authors": [
        "原文无此信息"
      ],
      "institutions": [
        "原文无此信息"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "原文无此信息",
          "relationship_type": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ],
      "compared_methods": [
        {
          "method_name": "原文无此信息",
          "comparison_result": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "原文无此信息",
          "dataset_description": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ],
      "evaluation_metrics": [
        "原文无此信息"
      ],
      "baseline_methods": [
        "原文无此信息"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "原文无此信息",
          "our_result": "原文无此信息",
          "baseline_result": "原文无此信息",
          "dataset": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "原文无此信息"
      ],
      "stated_novelty": [
        "原文无此信息"
      ],
      "stated_advantages": [
        "原文无此信息"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文无此信息"
      ],
      "failure_cases": [
        "原文无此信息"
      ]
    },
    "extraction_metadata": {
      "file_id": "多元组细粒度情感分析研究及应用_翟泽鹏",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 15:32:52",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 28374,
      "processed_text_length": 14964
    }
  },
  "户外种植园行间可通行区域识别与路径生成方法研究_王远航": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "户外种植园行间可通行区域识别与路径生成方法研究",
      "authors": [
        "王远航"
      ],
      "institutions": [
        "原文未明确提及"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "原文无此信息",
          "relationship_type": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ],
      "compared_methods": [
        {
          "method_name": "原文无此信息",
          "comparison_result": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "原文无此信息",
          "dataset_description": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ],
      "evaluation_metrics": [
        "精确率",
        "召回率",
        "F1分数"
      ],
      "baseline_methods": [
        "原文无此信息"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "原文无此信息",
          "our_result": "原文无此信息",
          "baseline_result": "原文无此信息",
          "dataset": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "分析轮式移动机器人不可通行路面的特点及参数",
        "设计适用于户外种植园行间场景的道路通行性评价指标",
        "生成可通行区域约束下的导航路径"
      ],
      "stated_novelty": [
        "原文无此信息"
      ],
      "stated_advantages": [
        "原文无此信息"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文无此信息"
      ],
      "failure_cases": [
        "原文无此信息"
      ]
    },
    "extraction_metadata": {
      "file_id": "户外种植园行间可通行区域识别与路径生成方法研究_王远航",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 15:33:22",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 13031,
      "processed_text_length": 13031
    }
  },
  "果园行间自主导航关键技术研究_王宇豪": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "果园行间自主导航关键技术研究",
      "authors": [
        "毕松",
        "王宇豪"
      ],
      "institutions": [
        "原文未明确提及"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "原文无此信息",
          "relationship_type": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ],
      "compared_methods": [
        {
          "method_name": "统计滤波",
          "comparison_result": "原文无此信息",
          "evidence_text": "本文方法根据点云密度动态调整去噪参数，可有效抑制不同点云密度中的噪声，且目标特征保留相对完整。"
        },
        {
          "method_name": "半径滤波",
          "comparison_result": "原文无此信息",
          "evidence_text": "本文方法根据点云密度动态调整去噪参数，可有效抑制不同点云密度中的噪声，且目标特征保留相对完整。"
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "原文无此信息",
          "dataset_description": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ],
      "evaluation_metrics": [
        "精确率",
        "召回率",
        "F1分数"
      ],
      "baseline_methods": [
        "统计滤波",
        "半径滤波"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "精确率",
          "our_result": "0.858",
          "baseline_result": "0.784",
          "dataset": "白杨树林",
          "evidence_text": "白杨树林的聚类真值分别为20和22，其余部分为点云噪声或信息量很少的点云。"
        },
        {
          "metric_name": "精确率",
          "our_result": "0.858",
          "baseline_result": "0.743",
          "dataset": "旱柳树林",
          "evidence_text": "旱柳树林的聚类真值分别为20和22，其余部分为点云噪声或信息量很少的点云。"
        },
        {
          "metric_name": "召回率",
          "our_result": "0.944",
          "baseline_result": "0.895",
          "dataset": "白杨树林",
          "evidence_text": "白杨树林的聚类真值分别为20和22，其余部分为点云噪声或信息量很少的点云。"
        },
        {
          "metric_name": "召回率",
          "our_result": "0.941",
          "baseline_result": "0.821",
          "dataset": "旱柳树林",
          "evidence_text": "旱柳树林的聚类真值分别为20和22，其余部分为点云噪声或信息量很少的点云。"
        },
        {
          "metric_name": "F1分数",
          "our_result": "0.933",
          "baseline_result": "0.895",
          "dataset": "白杨树林",
          "evidence_text": "白杨树林的聚类真值分别为20和22，其余部分为点云噪声或信息量很少的点云。"
        },
        {
          "metric_name": "F1分数",
          "our_result": "0.923",
          "baseline_result": "0.821",
          "dataset": "旱柳树林",
          "evidence_text": "旱柳树林的聚类真值分别为20和22，其余部分为点云噪声或信息量很少的点云。"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "提出自适应半径滤波方法",
        "提出基于交叉像素的可通行区域提取方法",
        "基于单目相机计算果树相对位置"
      ],
      "stated_novelty": [
        "原文无此信息"
      ],
      "stated_advantages": [
        "原文无此信息"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文无此信息"
      ],
      "failure_cases": [
        "原文无此信息"
      ]
    },
    "extraction_metadata": {
      "file_id": "果园行间自主导航关键技术研究_王宇豪",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 15:34:39",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 19705,
      "processed_text_length": 14996
    }
  },
  "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "Key Technology Research on Strawberry Picking Robot in Greenhouse Environment under Elevated Cultivation",
      "authors": [
        "Wei Pengjun"
      ],
      "institutions": [
        "North China University of Technology"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "卷积神经网络",
          "relationship_type": "结合",
          "evidence_text": "本文采用传统图像方法与卷积神经网络相结合的方式来解决这一问题。"
        },
        {
          "method_name": "全卷积神经网络",
          "relationship_type": "结合",
          "evidence_text": "使用全卷积神经网络更有利于精确的找到自然环境下草莓目标所属的像素。"
        },
        {
          "method_name": "支持向量机",
          "relationship_type": "结合",
          "evidence_text": "本文采用支持向量机(SVM)算法进行可否采摘判别和遮挡类型判别。"
        }
      ],
      "compared_methods": [
        {
          "method_name": "YOLO算法",
          "comparison_result": "YOLO算法对多目标的识别能力有限",
          "evidence_text": "YOLO算法采用回归的方式进行目标检测，每个网格最多只能预测一个物体，这限制了模型对多目标的识别能力。"
        },
        {
          "method_name": "YOLOv2算法",
          "comparison_result": "YOLOv2对长宽比较大的物体的检测存在一定的问题",
          "evidence_text": "YOLOv2网络设计中较大的步幅和较少的卷积层还是限制了其对于小目标的检测性能，采用Anchor Boxes的方式虽然提高了检测的精度，但是对于目标形状比较奇异的情况还是会有误检情况。"
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "草莓数据集",
          "dataset_description": "包含光照亮斑、遮挡阴影、枝条枝叶遮挡等果实像素点",
          "evidence_text": "草莓分割网络数据集来自4.3.1中获得的草莓分割前处理结果图像，为了使数据集具有更好的抗干扰性，标注时包含了光照亮斑、遮挡阴影、枝条枝叶遮挡等果实像素点。"
        }
      ],
      "evaluation_metrics": [
        "原文无此信息"
      ],
      "baseline_methods": [
        "原文无此信息"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "成熟草莓分类准确率",
          "our_result": "96.03%",
          "baseline_result": "原文无此信息",
          "dataset": "草莓数据集",
          "evidence_text": "当把阈值设定在0.2时，对成熟草莓分类的准确率为96.03%，对未成熟草莓分类的准确率为91.67%，满足实际场景的应用需求。"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "提出了一种草莓目标识别和精细分割技术",
        "提出了一种草莓果实的成熟性和遮挡性判断方法",
        "提出了一种草莓果实空间姿态和采摘点估计方法"
      ],
      "stated_novelty": [
        "原文无此信息"
      ],
      "stated_advantages": [
        "原文无此信息"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文无此信息"
      ],
      "failure_cases": [
        "原文无此信息"
      ]
    },
    "extraction_metadata": {
      "file_id": "温室环境下高架栽培草莓采摘机器人的关键技术研究_隗朋峻",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 15:36:53",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑技术关系comparison_result: YOLO算法对多目标的识别能力有限",
          "可疑技术关系evidence_text: YOLO算法采用回归的方式进行目标检测，每个网格最多只能预测一个物体，这限制了模型对多目标的识别能力。",
          "可疑技术关系method_name: YOLOv2算法",
          "可疑技术关系comparison_result: YOLOv2对长宽比较大的物体的检测存在一定的问题"
        ],
        "suspicious_count": 5,
        "validation_score": 0,
        "quality_level": "low"
      },
      "text_length": 25383,
      "processed_text_length": 14999
    }
  },
  "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "原文无此信息",
      "authors": [],
      "institutions": [],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [],
      "compared_methods": []
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "BPT-Power",
          "dataset_description": "原文对数据集的描述或'原文无此信息'",
          "evidence_text": "原文无此信息"
        }
      ],
      "evaluation_metrics": [],
      "baseline_methods": []
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "P",
          "our_result": "92%",
          "baseline_result": "原文中基准的数值或'原文无此信息'",
          "dataset": "BPT-Power",
          "evidence_text": "原文无此信息"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [],
      "stated_novelty": [],
      "stated_advantages": []
    },
    "limitations": {
      "acknowledged_limitations": [],
      "failure_cases": []
    },
    "extraction_metadata": {
      "file_id": "电力运检领域知识图谱的知识抽取算法设计及应用_朱婷婷",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 15:37:14",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑性能数据baseline_result: 原文中基准的数值或'原文无此信息'"
        ],
        "suspicious_count": 2,
        "validation_score": 60,
        "quality_level": "medium"
      },
      "text_length": 21741,
      "processed_text_length": 14959
    }
  },
  "网络化多智能体系统的主动容错预测控制方法研究_王时通": {
    "document_metadata": {
      "document_type": "原文无此信息",
      "title": "网络化多智能体系统的主动容错预测控制方法研究",
      "authors_or_creators": [
        "原文无此信息"
      ]
    },
    "main_content": {
      "stated_purpose": "研究网络化多智能体系统的主动容错预测控制方法，包括对执行器故障和传感器故障的主动补偿，以及在不同通信约束下的系统稳定性分析。",
      "key_concepts": [
        "网络化多智能体系统",
        "主动容错预测控制",
        "执行器故障",
        "传感器故障",
        "随机通信约束"
      ],
      "main_methods": [
        "基于状态观测器的主动容错控制方法",
        "网络化主动容错预测控制方法",
        "云主动容错预测控制方法"
      ]
    },
    "relationships": {
      "references_to": [
        "Pang Z H, Zhao X Y, Sun J, et al. Comparison of three data-driven networked predictive control methods for a class of nonlinear systems. IEEE/CAA Journal of Automatica Sinica, 2022, 9(9): 1714-1716.",
        "Li Y, Hua C, Guan X. Distributed output feedback leader-following control for high-order nonlinear multiagent system using dynamic gain method [J]. IEEE Transactions on Cybernetics, 2020, 50(2): 640-649.",
        "Yu Z, Zhang Y, Jiang B, et al. Decentralized fractional-order backstepping fault-tolerant control of multi-UAVs against actuator faults and wind effects. Aerospace Science and Technology, 2020, 104: 105939.",
        "Yan D H, Zhang W G, Chen H, et al. Robust control strategy for multi-UAVs system using MPC combined with Kalman-consensus filter and disturbance observer. ISA Transactions, 2023, 135: 35-51.",
        "Pang Z H, Xia C G, Sun J, et al. Active fault-tolerant predictive control of networked systems subject to actuator faults and random communication constraints. International Journal of Control, 2022, 95(9): 2357-2363.",
        "Liu G P. Networked predictive control for nonlinear information physical systems with time-varying communication constraints. Control Theory and Application, 2022, 39(1): 145-153.",
        "Pang Z H, Luo W C. Networked multi-agent predictive control based on observers. Control and Decision, 2021, 36(9): 2290-2296.",
        "Yan Y M, Chen Z Y. Cooperative output regulation of linear discrete-time time-delay multi-agent systems by adaptive distributed observers. Neurocomputing, 2019, 331: 33-39.",
        "Wang H, You Y, Li W Q. Distributed output-feedback tracking for stochastic nonlinear multi-agent systems with time-varying delays. IEEE Access, 2022, 10: 69323-69332.",
        "Guo S, You R, Ahn C K. Adaptive consensus for multi-agent systems with switched nonlinear dynamics and switching directed topologies. IEEE Transactions on Control Systems Technology, 2023, 111: 1285-1299."
      ],
      "builds_on": [
        "原文未明确提及"
      ]
    },
    "note": "文档类型不明确，仅提取最基础信息",
    "extraction_metadata": {
      "file_id": "网络化多智能体系统的主动容错预测控制方法研究_王时通",
      "detected_doc_type": "unknown",
      "extraction_time": "2025-08-02 15:37:50",
      "validation": {
        "warnings": [],
        "suspicious_count": 0,
        "validation_score": 100,
        "quality_level": "high"
      },
      "text_length": 19837,
      "processed_text_length": 14947
    }
  },
  "采用音质特征和VLAD编码的新冠肺炎检测算法_张昊然": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "采用音质特征和VLAD编码的新冠肺炎检测算法",
      "authors": [
        "张昊然",
        "韩易辰",
        "谭咏梅",
        "李雅"
      ],
      "institutions": [
        "北京邮电大学",
        "人工智能学院"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "openSMILE、openXBOW、DeepSpectrum、auDeep和End2You",
          "relationship_type": "改进",
          "evidence_text": "Baseline使用openSMILE、openXBOW、DeepSpectrum、auDeep和End2You工具进行特征提取，输入到线性SVM中进行分类。"
        }
      ],
      "compared_methods": [
        {
          "method_name": "AI4COVID-19项目和剑桥COVID-19声音数据库",
          "comparison_result": "原文无此信息",
          "evidence_text": "AI4COVID-19项目通过智能手机应用程序收集咳嗽声进行感染筛查。剑桥COVID-19声音数据库收集朗读语音和咳嗽声。"
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "INTEＲSPEECH 2021 ComParE竞赛数据集",
          "dataset_description": "原文无此信息",
          "evidence_text": "本文提出了基于语音内容分析的新冠肺炎自动识别方法，并在INTEＲSPEECH 2021 ComParE竞赛提供的数据集上进行了验证。"
        }
      ],
      "evaluation_metrics": [
        "原文无此信息"
      ],
      "baseline_methods": [
        "线性SVM分类器"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "UAＲ",
          "our_result": "73.9%和77.2%",
          "baseline_result": "原文无此信息",
          "dataset": "两个子任务验证集",
          "evidence_text": "最后，通过多算法融合进一步提升分类效果，在两个子任务验证集上UAＲ分别达到73.9%和77.2%。"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "使用语音端点检测进行数据增广",
        "引入语音质量特征",
        "对基线提取的低水平特征进行VLAD编码"
      ],
      "stated_novelty": [
        "原文无此信息"
      ],
      "stated_advantages": [
        "在小字典规模下有效提升了系统分类性能",
        "证实了VLAD编码在语音特征编码中的有效性"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "字典的具体大小并不能提前确定，需要进行多次尝试确定最佳取值"
      ],
      "failure_cases": [
        "原文无此信息"
      ]
    },
    "extraction_metadata": {
      "file_id": "采用音质特征和VLAD编码的新冠肺炎检测算法_张昊然",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 15:38:17",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑技术关系method_name: AI4COVID-19项目和剑桥COVID-19声音数据库"
        ],
        "suspicious_count": 2,
        "validation_score": 60,
        "quality_level": "medium"
      },
      "text_length": 5878,
      "processed_text_length": 5878
    }
  },
  "面向复杂文本的指称表达理解研究_陆明聪": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "原文无此信息",
      "authors": [
        "原文无此信息"
      ],
      "institutions": [
        "原文无此信息"
      ],
      "publication_venue": "原文无此信息"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "原文无此信息",
          "relationship_type": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ],
      "compared_methods": [
        {
          "method_name": "原文无此信息",
          "comparison_result": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "原文无此信息",
          "dataset_description": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ],
      "evaluation_metrics": [
        "原文无此信息"
      ],
      "baseline_methods": [
        "原文无此信息"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "原文无此信息",
          "our_result": "原文无此信息",
          "baseline_result": "原文无此信息",
          "dataset": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "原文无此信息"
      ],
      "stated_novelty": [
        "原文无此信息"
      ],
      "stated_advantages": [
        "原文无此信息"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文无此信息"
      ],
      "failure_cases": [
        "原文无此信息"
      ]
    },
    "extraction_metadata": {
      "file_id": "面向复杂文本的指称表达理解研究_陆明聪",
      "detected_doc_type": "experimental_paper",
      "extraction_time": "2025-08-02 15:38:46",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 26858,
      "processed_text_length": 14926
    }
  },
  "Multimodal_Co_Attention_Mechanism_for_One_stage_Visual_Grounding": {
    "document_metadata": {
      "document_type": "unknown",
      "title": "原文无此信息",
      "authors_or_creators": []
    },
    "main_content": {
      "stated_purpose": "原文无此信息",
      "key_concepts": [],
      "main_methods": []
    },
    "relationships": {
      "references_to": [],
      "builds_on": []
    },
    "note": "文档类型不明确，仅提取最基础信息",
    "extraction_metadata": {
      "file_id": "Multimodal_Co_Attention_Mechanism_for_One_stage_Visual_Grounding",
      "detected_doc_type": "unknown",
      "source_type": "cleaned",
      "extraction_time": "2025-08-02 15:44:12",
      "validation": {
        "warnings": [
          "可疑document_type: unknown"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 6327,
      "processed_text_length": 6327
    }
  },
  "多视图有监督的LDA模型": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "多视图有监督的ＬＤＡ模型",
      "authors": [
        "李晓旭",
        "李睿凡",
        "冯方向",
        "曹洁",
        "王小捷"
      ],
      "institutions": [
        "兰州理工大学计算机与通信学院",
        "北京邮电大学计算机学院",
        "教育部信息网络工程研究中心"
      ],
      "publication_venue": "电子学报"
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "LDA",
          "relationship_type": "结合",
          "evidence_text": "本文建立在概率主题模型LDA和集成分类方法Softmax混合模型上"
        }
      ],
      "compared_methods": [
        {
          "method_name": "S-LDA",
          "comparison_result": "原文无此信息",
          "evidence_text": "原文无此信息"
        },
        {
          "method_name": "SBMLR",
          "comparison_result": "原文无此信息",
          "evidence_text": "原文无此信息"
        },
        {
          "method_name": "SVM-POL",
          "comparison_result": "原文无此信息",
          "evidence_text": "原文无此信息"
        },
        {
          "method_name": "SVM-RBF",
          "comparison_result": "原文无此信息",
          "evidence_text": "原文无此信息"
        },
        {
          "method_name": "Fu-L",
          "comparison_result": "原文无此信息",
          "evidence_text": "原文无此信息"
        },
        {
          "method_name": "MCa-sLDA",
          "comparison_result": "原文无此信息",
          "evidence_text": "原文无此信息"
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "LabelMe",
          "dataset_description": "带有标注的场景分类数据集",
          "evidence_text": "一个是带有标注的场景分类数据集：LabelMe的子集"
        },
        {
          "dataset_name": "UIUC-Sport",
          "dataset_description": "带有标注的事件分类数据集",
          "evidence_text": "另一个是带有标注的事件分类数据集：8类的UIUC-Sport数据集"
        }
      ],
      "evaluation_metrics": [
        "原文无此信息"
      ],
      "baseline_methods": [
        "S-LDA",
        "SBMLR",
        "SVM-POL",
        "SVM-RBF",
        "Fu-L",
        "MCa-sLDA"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "准确率",
          "our_result": "92.2%",
          "baseline_result": "81.1%",
          "dataset": "LabelMe",
          "evidence_text": "提出模型在LabelMe数据上可达到92.2%准确率"
        },
        {
          "metric_name": "准确率",
          "our_result": "99.0%",
          "baseline_result": "原文无此信息",
          "dataset": "UIUC-Sport",
          "evidence_text": "提出模型在UIUC-Sport数据上可达到99.0%的准确率"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "提出一个多视图有监督的LDA模型"
      ],
      "stated_novelty": [
        "将集成学习思想引入主题模型中"
      ],
      "stated_advantages": [
        "充分利用多视图信息"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文无此信息"
      ],
      "failure_cases": [
        "原文无此信息"
      ]
    },
    "extraction_metadata": {
      "file_id": "多视图有监督的LDA模型",
      "detected_doc_type": "experimental_paper",
      "source_type": "basic_fallback",
      "extraction_time": "2025-08-02 15:46:11",
      "validation": {
        "warnings": [
          "可疑document_type: experimental_paper",
          "可疑技术关系evidence_text: 本文建立在概率主题模型LDA和集成分类方法Softmax混合模型上",
          "可疑技术关系method_name: S-LDA",
          "可疑技术关系method_name: SBMLR",
          "可疑技术关系method_name: SVM-POL",
          "可疑技术关系method_name: SVM-RBF",
          "可疑技术关系method_name: Fu-L",
          "可疑技术关系method_name: MCa-sLDA",
          "可疑数据集: LabelMe",
          "可疑数据集: UIUC-Sport",
          "可疑性能数据our_result: 92.2%",
          "可疑性能数据baseline_result: 81.1%",
          "可疑性能数据our_result: 99.0%"
        ],
        "suspicious_count": 13,
        "validation_score": 0,
        "quality_level": "low"
      },
      "text_length": 10909,
      "processed_text_length": 10909
    }
  },
  "一种图像检索方法_鲁鹏": {
    "document_metadata": {
      "document_type": "patent",
      "title": "一种图像检索方法",
      "inventors": [
        "鲁鹏",
        "李睿凡",
        "刘咏彬",
        "袁彩霞",
        "王小捷"
      ],
      "applicant": "北京邮电大学",
      "patent_number": "201310722183.3"
    },
    "technical_solution": {
      "technical_problem": "基于相似度的图像检索方法在光照和视角变化下性能下降",
      "solution_overview": "提出一种基于图像相关度的图像检索方法，计算图像数据库中任意两个图像的相关度，构建匹配图，计算查询目标与图像数据库中图像的直接相关度和间接相关度，根据相关度排序得到检索结果",
      "key_technical_features": [
        "计算图像数据库中任意两个图像的相关度",
        "构建匹配图",
        "计算查询目标与图像数据库中图像的直接相关度和间接相关度",
        "根据相关度排序得到检索结果"
      ]
    },
    "implementation": {
      "embodiments": [
        "以不同视点角度下的图像为例说明方法流程",
        "以大规模图像数据库为例说明方法流程"
      ],
      "technical_effects": [
        "有效降低光照和视角变化对图像检索的影响",
        "提高图像检索系统对光照和视角变化的鲁棒性"
      ]
    },
    "application_scope": {
      "application_fields": [
        "图像检索"
      ],
      "use_scenarios": [
        "大规模图像数据库的图像检索"
      ]
    },
    "claims_info": {
      "main_claims": [
        "一种图像检索方法，包括计算图像数据库中任意两个图像的相关度，构建匹配图，计算查询目标与图像数据库中图像的直接相关度和间接相关度，根据相关度排序得到检索结果"
      ]
    },
    "extraction_metadata": {
      "file_id": "一种图像检索方法_鲁鹏",
      "detected_doc_type": "patent",
      "source_type": "basic_fallback",
      "extraction_time": "2025-08-02 15:47:07",
      "validation": {
        "warnings": [
          "可疑document_type: patent"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 14916,
      "processed_text_length": 14916
    }
  },
  "一种基于人工智能的数据安全风险监测追溯系统_黄永军": {
    "document_metadata": {
      "document_type": "patent",
      "title": "一种基于人工智能的数据安全风险监测追溯系统",
      "inventors": [
        "黄永军",
        "李睿凡",
        "周春楠",
        "孙健"
      ],
      "applicant": "北京东方通网信科技有限公司",
      "patent_number": "CN 116821750 A"
    },
    "technical_solution": {
      "technical_problem": "近年全球范围内来针对工业领域的网络攻击事件频发，涉及汽车生产、智能制造、能源电力、烟草等诸多行业，导致工业主机蓝屏、重要文件被加密，更严重的造成了工业企企业停工停产，给企业带来重大损失。这一系列攻击事件表明，工业互联网已成为网络攻击的靶标。",
      "solution_overview": "本申请提供一种基于人工智能的数据安全风险监测追溯系统，包括：数据采集模块，用于实时采集企业侧各类安全事件信息，所述各类安全事件信息包括流量、终端、数据库、应用，建立数据识别特征库；数据流转与分布监测模块，用于解析所述各类安全事件信息的规则与策略，发现数据资产保护对象，生成数据资产清单，并动态监测数据分布与流转，获取数据流转规则和访问控制规则；数据安全事件分析模块，用于智能分析识别数据在采集共享处理过程中的安全风险，对数据分类分级管理，根据数据识别特征库、数据流转规则和访问控制规则建立数据安全风险监测策略库；数据安全事件溯源模块，基于人工智能对实时安全事件进行溯源分析，并将安全事件和溯源信息进行及时上报。",
      "key_technical_features": [
        "数据采集模块",
        "数据流转与分布监测模块",
        "数据安全事件分析模块",
        "数据安全事件溯源模块"
      ]
    },
    "implementation": {
      "embodiments": [
        "数据采集模块的具体实现方法",
        "数据流转与分布监测模块的具体实现方法",
        "数据安全事件分析模块的具体实现方法",
        "数据安全事件溯源模块的具体实现方法"
      ],
      "technical_effects": [
        "实现对数据安全风险的实时监测、主动识别、精准定位、自动溯源",
        "保障工业互联网数据安全",
        "护航工业企业数字化转型"
      ]
    },
    "application_scope": {
      "application_fields": [
        "工业互联网",
        "数据安全"
      ],
      "use_scenarios": [
        "工业企业的数据安全风险监测",
        "工业互联网的数据安全风险监测"
      ]
    },
    "claims_info": {
      "main_claims": [
        "一种基于人工智能的数据安全风险监测追溯系统，其特征在于，包括：数据采集模块，用于实时采集企业侧各类安全事件信息，所述各类安全事件信息包括流量、终端、数据库、应用，建立数据识别特征库；数据流转与分布监测模块，用于解析所述各类安全事件信息的规则与策略，发现数据资产保护对象，生成数据资产清单，并动态监测数据分布与流转，获取数据流转规则和访问控制规则；数据安全事件分析模块，用于智能分析识别数据在采集共享处理过程中的安全风险，对数据分类分级管理，根据数据识别特征库、数据流转规则和访问控制规则建立数据安全风险监测策略库；数据安全事件溯源模块，基于人工智能对实时安全事件进行溯源分析，并将安全事件和溯源信息进行及时上报。"
      ]
    },
    "extraction_metadata": {
      "file_id": "一种基于人工智能的数据安全风险监测追溯系统_黄永军",
      "detected_doc_type": "patent",
      "source_type": "basic_fallback",
      "extraction_time": "2025-08-02 15:47:53",
      "validation": {
        "warnings": [
          "可疑document_type: patent"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 14094,
      "processed_text_length": 14094
    }
  },
  "基于多层神经网络的电力实体识别方法、存储介质和设备_刘子全": {
    "document_metadata": {
      "document_type": "patent",
      "title": "基于多层神经网络的电力实体识别方法、存储介质和设备",
      "inventors": [
        "刘子全",
        "李睿凡",
        "王泽元",
        "胡成博",
        "熊永平",
        "朱雪琼"
      ],
      "applicant": "国网江苏省电力有限公司电力科学研究院 地址211103 江苏省南京市江宁区帕威尔路1号 专利权人国家电网有限公司 北京邮电大学 国网江苏省电力有限公司 江苏省电力试验研究院有限公司",
      "patent_number": "202011337566.5"
    },
    "technical_solution": {
      "technical_problem": "现有技术中电力领域命名实体识别工具存在标注语料不充足的问题，此外命名实体识别任务常出现标签不平衡问题，即不同实体出现的频次差异较大，基于该数据训练的模型会导致模型偏向于预测成频次多的标签，语料不足也加大了不平衡的问题处理难度。而人工标注需要有电力领域专业知识，普通人难以直接准确识别电力领域实体，会造成成本高昂、标注较为缓慢问题。",
      "solution_overview": "本发明提供一种基于多层神经网络的电力实体识别方法、存储介质和设备，解决了电力实体识别标签不平衡、识别不准确、人工标注慢的问题。",
      "key_technical_features": [
        "实体标签的哈夫曼编码",
        "伪标注的数据标注方法",
        "通过BERT预训练模型增强字的语义表示"
      ]
    },
    "implementation": {
      "embodiments": [
        "一种基于多层神经网络的电力实体识别方法，包括以下步骤：提取海量文本语料库，对海量本文语料库进行数据预处理，得到语言模型训练语料；通过语言模型训练语料对BERT语言模型进行预训练；对电力语料数据标注电力实体标签，构建电力实体识别语料；根据电力实体标签在电力实体识别语料中的数量构建电力实体标签的哈夫曼编码；在预训练得到的BERT语言模型后增加分类层构成BERT电力实体识别模型，通过电力实体识别语料对BERT电力实体识别模型进行再次训练，得到训练好的BERT电力实体识别模型；将待识别的电力语料输入至预先构建的BERT电力实体识别模型中，得到电力实体标签的哈夫曼编码，通过哈夫曼编码映射得到实体标签，进而得到识别出的实体。"
      ],
      "technical_effects": [
        "实体标签的哈夫曼编码能通过哈夫曼树结构有效缓解了电力领域实体标签不平衡的问题，提高了电力领域中文命名实体识别的精度；伪标注的数据标注方法，能够有效减少实体识别文本标注的人力成本；通过BERT预训练模型增强字的语义表示，通过微调的方式减少了训练参数，节省了训练时间，数据量较小的情况下模型性能良好。"
      ]
    },
    "application_scope": {
      "application_fields": [
        "电力实体识别"
      ],
      "use_scenarios": [
        "电力领域文本的实体识别"
      ]
    },
    "claims_info": {
      "main_claims": [
        "一种基于多层神经网络的电力实体识别方法，其特征在于：将待识别的电力语料输入至预先构建的BERT电力实体识别模型中，得到电力实体标签的哈夫曼编码，通过哈夫曼编码映射得到实体标签，进而得到识别出的实体；"
      ]
    },
    "extraction_metadata": {
      "file_id": "基于多层神经网络的电力实体识别方法、存储介质和设备_刘子全",
      "detected_doc_type": "patent",
      "source_type": "basic_fallback",
      "extraction_time": "2025-08-02 15:49:15",
      "validation": {
        "warnings": [
          "可疑document_type: patent",
          "可疑applicant: 国网江苏省电力有限公司电力科学研究院 地址211103 江苏省南京市江宁区帕威尔路1号 专利权人国家电网有限公司 北京邮电大学 国网江苏省电力有限公司 江苏省电力试验研究院有限公司"
        ],
        "suspicious_count": 2,
        "validation_score": 60,
        "quality_level": "medium"
      },
      "text_length": 10765,
      "processed_text_length": 10765
    }
  },
  "基于掩码上下文机器阅读理解的方面情感三元组抽取方法_李睿凡": {
    "document_metadata": {
      "document_type": "patent",
      "title": "基于掩码上下文机器阅读理解的方面情感三元组抽取方法",
      "inventors": [
        "李睿凡",
        "翟泽鹏",
        "冯方向",
        "张光卫",
        "王小捷"
      ],
      "applicant": "北京邮电大学",
      "patent_number": "202210599136.3"
    },
    "technical_solution": {
      "technical_problem": "方面级情感分析(ABSA)任务中，当一句话包含多个方面词时，传统基于机器阅读理解(MRC)的方法在方面词附属推理阶段存在其他方面词带来的干扰问题。",
      "solution_overview": "提出一种基于掩码上下文机器阅读理解(COM-MRC)的方面情感三元组抽取方法，在推理时，先推理方面词再掩码无关方面词推理意见词，可以有效减少其他方面词干扰问题；在训练时，应用上下文数据增强，有效地扩充了语料并为推理打下基础；在模型结构方面，设计了四个模块协同工作，这四个模块包括方面词提取模块、意见词提取模块、情感分类模块以及方面词存在探测模块。",
      "key_technical_features": [
        "方面词推理阶段",
        "方面词附属推理阶段",
        "上下文数据增强",
        "四个模块协同工作"
      ]
    },
    "implementation": {
      "embodiments": [
        "使用BERT作为句子的编码器，输入一个固定的查询q和一个原始句子作为上下文，经过模型得到方面词a以及方面词存在标识e，若标识结果为True，则将得到的方面词a加入到方面词集合A中，将上下文把集合A中所有方面词掩码作为掩码上下文，与查询q再次输入至模型中得到方面词a以及方面词存在标识e，重复此流程，直到标识结果为False，得到方面词集合A；探测经过掩码之后的上下文是否仍存在方面词，如果所有的方面词均被掩码，其标识为False，否则为True，得到句子表示、方面词表示和意见词表示，再通过多头注意力神经网络融合句子表示、方面词表示和意见词表示的信息，得到情感s，输出情感s、方面词a、意见词o和方面词存在标识e；",
        "对于方面词集合A中的每个方面词a，在上下文中直接掩码掉除了方面词a以外所有无关的方面词，根据查询q以及掩码所有无关方面词的上下文得到方面词a对应的意见词O集合以及情感s，最后输出句子存在的所有的方面情感三元组(a，o，s)"
      ],
      "technical_effects": [
        "有效解决了以往MRC方法面临的方面词干扰问题"
      ]
    },
    "application_scope": {
      "application_fields": [
        "自然语言处理技术领域"
      ],
      "use_scenarios": [
        "方面级情感分析(ABSA)任务"
      ]
    },
    "claims_info": {
      "main_claims": [
        "一种基于掩码上下文机器阅读理解的方面情感三元组抽取方法，其特征在于：在方面词推理阶段，使用BERT作为句子的编码器，输入一个固定的查询q和一个原始句子作为上下文，经过模型得到方面词a以及方面词存在标识e，若标识结果为True，则将得到的方面词a加入到方面词集合A中，将上下文把集合A中所有方面词掩码作为掩码上下文，与查询q再次输入至模型中得到方面词a以及方面词存在标识e，重复此流程，直到标识结果为False，得到方面词集合A；探测经过掩码之后的上下文是否仍存在方面词，如果所有的方面词均被掩码，其标识为False，否则为True，得到句子表示、方面词表示和意见词表示，再通过多头注意力神经网络融合句子表示、方面词表示和意见词表示的信息，得到情感s，输出情感s、方面词a、意见词o和方面词存在标识e；在方面词附属推理阶段，对于方面词集合A中的每个方面词a，在上下文中直接掩码掉除了方面词a以外所有无关的方面词，根据查询q以及掩码所有无关方面词的上下文得到方面词a对应的意见词O集合以及情感s，最后输出句子存在的所有的方面情感三元组(a，o，s)"
      ]
    },
    "extraction_metadata": {
      "file_id": "基于掩码上下文机器阅读理解的方面情感三元组抽取方法_李睿凡",
      "detected_doc_type": "patent",
      "source_type": "basic_fallback",
      "extraction_time": "2025-08-02 15:55:12",
      "validation": {
        "warnings": [
          "可疑document_type: patent"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 13060,
      "processed_text_length": 13060
    }
  },
  "基于文本生成图像的模型训练方法、设备和图像生成方法_冯方向": {
    "document_metadata": {
      "document_type": "patent",
      "title": "基于文本生成图像的模型训练方法、设备和图像生成方法",
      "inventors": [
        "冯方向",
        "牛天睿",
        "王小捷",
        "李睿凡",
        "袁彩霞"
      ],
      "applicant": "北京邮电大学",
      "patent_number": "202110266563.5"
    },
    "technical_solution": {
      "technical_problem": "现有基于文本生成图像的方案中存在模型学习效率低、效果差等问题。",
      "solution_overview": "本发明提出的技术方案为：一种基于文本生成图像的模型训练方法，包括：对于预设训练样本集合中的每个训练样本，基于该训练样本的文本信息，生成相应的文本嵌入式表示，并将所述文本嵌入式表示输入至图像生成模型，触发所述图像生成模型基于所述文本嵌入式表示，生成所述文本信息对应的人造图像，并采用模态解纠缠方式，提取所述人造图像的真实度参数和相应训练样本的真实图像的真实度参数；基于所述人造图像，确定所述训练样本的正例和负例；利用所述图像生成模型，基于每个所述训练样本的正例、负例和真实图像各自对应的所述真实度参数，计算相应样本的子损失函数，并基于所述子损失函数，计算相应样本的总体损失函数；利用所述总体损失函数，调整所述图像生成模型的参数。",
      "key_technical_features": [
        "基于文本生成图像的模型训练方法",
        "模态解纠缠方式",
        "提取真实度参数",
        "确定正例和负例",
        "计算损失函数",
        "调整模型参数"
      ]
    },
    "implementation": {
      "embodiments": [
        "基于文本生成图像的模型训练方法流程示意图",
        "基于文本生成图像的方法流程示意图"
      ],
      "technical_effects": [
        "提高模型学习效率和图像生成效果"
      ]
    },
    "application_scope": {
      "application_fields": [
        "人工智能技术"
      ],
      "use_scenarios": [
        "基于文本生成图像的技术"
      ]
    },
    "claims_info": {
      "main_claims": [
        "一种基于文本生成图像的模型训练方法，其特征在于，包括：对于预设训练样本集合中的每个训练样本，基于该训练样本的文本信息，生成相应的文本嵌入式表示，并将所述文本嵌入式表示输入至图像生成模型，触发所述图像生成模型基于所述文本嵌入式表示，生成所述文本信息对应的人造图像，并采用模态解纠缠方式，提取所述人造图像的真实度参数和相应训练样本的真实图像的真实度参数；基于所述人造图像，确定所述训练样本的正例和负例；利用所述图像生成模型，基于每个所述训练样本的正例、负例和真实图像各自对应的所述真实度参数，计算相应样本的子损失函数，并基于所述子损失函数，计算相应样本的总体损失函数；利用所述总体损失函数，调整所述图像生成模型的参数。"
      ]
    },
    "extraction_metadata": {
      "file_id": "基于文本生成图像的模型训练方法、设备和图像生成方法_冯方向",
      "detected_doc_type": "patent",
      "source_type": "basic_fallback",
      "extraction_time": "2025-08-02 15:55:50",
      "validation": {
        "warnings": [
          "可疑document_type: patent"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 12306,
      "processed_text_length": 12306
    }
  },
  "基于采集搜索引擎数据的隐私信息评级方法_芦效峰": {
    "document_metadata": {
      "document_type": "patent",
      "title": "基于采集搜索引擎数据的隐私信息评级方法",
      "inventors": [
        "芦效峰",
        "鲁鹏",
        "李睿凡",
        "李蕾",
        "袁彩霞",
        "刘咏彬",
        "曲昭伟",
        "李晖"
      ],
      "applicant": "北京邮电大学",
      "patent_number": "201410441434.5"
    },
    "technical_solution": {
      "technical_problem": "现有隐私保护的不足之一，提供一种进行系统性地隐私信息间比较的方法",
      "solution_overview": "本发明采取如下技术方案：第一步：确定每个隐私信息的普遍性分值。第二步：确定每个隐私信息的敏感性分值。第三步：确定隐私信息的安全等级。",
      "key_technical_features": [
        "使用搜索引擎采集数据确定隐私信息的普遍性分值U",
        "使用搜索引擎采集数据确定隐私信息的敏感性分值S",
        "根据U×S计算结果确定隐私信息的安全等级"
      ]
    },
    "implementation": {
      "embodiments": [
        "确定每个隐私信息的普遍性分值U的具体步骤",
        "确定每个隐私信息的敏感性分值S的具体步骤",
        "确定隐私信息的安全等级的具体步骤"
      ],
      "technical_effects": [
        "提供一种全面地进行隐私信息评级方法",
        "评级方法使用的数据具有公正性",
        "适用于全部隐私信息",
        "具有灵活而广阔的应用领域"
      ]
    },
    "application_scope": {
      "application_fields": [
        "物联网应用",
        "隐私保护技术"
      ],
      "use_scenarios": [
        "评定全体隐私信息",
        "评定应用系统中有限数量的隐私信息"
      ]
    },
    "claims_info": {
      "main_claims": [
        "基于采集搜索引擎数据的隐私信息评级方法，其特征在于，评级包括以下步骤：第一步：从搜索引擎采集数据确定每个隐私信息的普遍性分值U；第二步：从搜索引擎采集数据确定每个隐私信息的敏感性分值S；第三步：根据U×S计算结果确定隐私信息的安全等级"
      ]
    },
    "extraction_metadata": {
      "file_id": "基于采集搜索引擎数据的隐私信息评级方法_芦效峰",
      "detected_doc_type": "patent",
      "source_type": "basic_fallback",
      "extraction_time": "2025-08-02 15:56:24",
      "validation": {
        "warnings": [
          "可疑document_type: patent"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 4494,
      "processed_text_length": 4494
    }
  },
  "融入类别自适应度量学习的小样本图像分类方法及装置_李晓旭": {
    "document_metadata": {
      "document_type": "patent",
      "title": "融入类别自适应度量学习的小样本图像分类方法及装置",
      "inventors": [
        "李晓旭",
        "孙浩",
        "刘俊",
        "武继杰",
        "李真",
        "曾俊瑀",
        "李睿凡",
        "马占宇",
        "陶剑"
      ],
      "applicant": "兰州理工大学",
      "patent_number": "202210480401.6"
    },
    "technical_solution": {
      "technical_problem": "现有小样本图像分类的性能仍不尽人意，很大程度上限制 了小样本图像分类技术的实用化，还面临着以下问题亟待解决：在现有小样本分类方法中，大多假设小样本分类任务使用一个单一的度量方式，例如余弦距离、欧氏距离或一个可学习的度量网络模块。不同的任务包含不同的类别，有些任务适用余弦距离，有些任务适用欧氏距离。因此，如何构建任务自适应的度量也是小样本图像分类值得研究的问题。",
      "solution_overview": "本发明提供了一种融入类别自适应度量学习的小样本图像分类方法及装置，为每个类别构建一个度量模块，通过对类内共性特征的学习，建立基于类内共性特征的度量，利用已经进行过预训练的嵌入模块，输入支持样本得到特征矩阵，并将其输入到类相关自适应度量模块，进行特征拼接并得到关系分数，将相似性最大的类作为预测类别，得到最终预测结果，从而提高小样本图像分类的性能，解决小样本图像分类中基于类内共性特征的度量学习问题。",
      "key_technical_features": [
        "融入类别自适应度量学习的小样本图像分类方法",
        "为每个类别构建一个度量模块",
        "通过对类内共性特征的学习，建立基于类内共性特征的度量",
        "利用已经进行过预训练的嵌入模块",
        "输入支持样本得到特征矩阵",
        "将其输入到类相关自适应度量模块",
        "进行特征拼接并得到关系分数",
        "将相似性最大的类作为预测类别",
        "得到最终预测结果"
      ]
    },
    "implementation": {
      "embodiments": [
        "数据预处理模块：用于对数据进行预处理，将数据划分为训练集和测试集，确定模型的训练方式；网络模型构建模块：用于构建融入类别自适应度量学习的小样本图像分类模型，模型由嵌入模块fθ和类相关自适应度量模块组成；模型参数训练模块：利用基类数据对融入类别自适应度量学习的小样本图像分类模型进行训练，求解模型参数；模型性能测试模块：利用训练后的融入类别自适应度量学习的小样本图像分类模型对新类任务进行预测，测评模型的性能。"
      ],
      "technical_effects": [
        "提高小样本图像分类的性能",
        "解决小样本图像分类中基于类内共性特征的度量学习问题",
        "对于图像的分类效果十分明显",
        "在实践中体现出极大价值"
      ]
    },
    "application_scope": {
      "application_fields": [
        "计算机视觉",
        "人工智能"
      ],
      "use_scenarios": [
        "军事、医疗、工业，天文等领域"
      ]
    },
    "claims_info": {
      "main_claims": [
        "一种融入类别自适应度量学习的小样本图像分类装置，其特征在于，包括以下模块：数据预处理模块：用于对数据进行预处理，将数据划分为训练集和测试集，确定模型的训练方式；网络模型构建模块：用于构建融入类别自适应度量学习的小样本图像分类模型，模型由嵌入模块fθ和类相关自适应度量模块组成；模型参数训练模块：利用基类数据对融入类别自适应度量学习的小样本图像分类模型进行训练，求解模型参数；模型性能测试模块：利用训练后的融入类别自适应度量学习的小样本图像分类模型对新类任务进行预测，测评模型的性能。"
      ]
    },
    "extraction_metadata": {
      "file_id": "融入类别自适应度量学习的小样本图像分类方法及装置_李晓旭",
      "detected_doc_type": "patent",
      "source_type": "basic_fallback",
      "extraction_time": "2025-08-02 15:57:11",
      "validation": {
        "warnings": [
          "可疑document_type: patent"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 11175,
      "processed_text_length": 11175
    }
  },
  "面向小样本图像分类的任务相关度量学习方法及装置_李晓旭": {
    "document_metadata": {
      "document_type": "patent",
      "title": "面向小样本图像分类的任务相关度量学习方法及装置",
      "inventors": [
        "李晓旭",
        "杨世丞",
        "刘俊",
        "燕锦涛",
        "安文娟",
        "张文斌",
        "李睿凡",
        "马占宇",
        "陶剑"
      ],
      "applicant": "兰州理工大学",
      "patent_number": "CN 114943859 B"
    },
    "technical_solution": {
      "technical_problem": "小样本图像分类中存在的自适应度量学习问题",
      "solution_overview": "通过考虑不同任务之间的差异性，引入注意力机制的思想，并学习任务相关的空间映射，利用任务自适应度量学习的方式，解决了小样本图像分类中存在的自适应度量学习问题",
      "key_technical_features": [
        "引入注意力机制",
        "学习任务相关的空间映射",
        "任务自适应度量学习"
      ]
    },
    "implementation": {
      "embodiments": [
        "数据预处理阶段",
        "构建网络模型阶段",
        "训练模型参数阶段",
        "测试模型性能阶段"
      ],
      "technical_effects": [
        "提高小样本条件下目标任务分类的准确性",
        "改善图像的分类效果",
        "具有很高的实用价值"
      ]
    },
    "application_scope": {
      "application_fields": [
        "计算机视觉",
        "图像分类"
      ],
      "use_scenarios": [
        "小样本图像分类任务"
      ]
    },
    "claims_info": {
      "main_claims": [
        "一种面向小样本图像分类的任务相关度量学习方法",
        "一种面向小样本图像分类的任务相关度量学习装置"
      ]
    },
    "extraction_metadata": {
      "file_id": "面向小样本图像分类的任务相关度量学习方法及装置_李晓旭",
      "detected_doc_type": "patent",
      "source_type": "basic_fallback",
      "extraction_time": "2025-08-02 15:57:40",
      "validation": {
        "warnings": [
          "可疑document_type: patent"
        ],
        "suspicious_count": 1,
        "validation_score": 80,
        "quality_level": "high"
      },
      "text_length": 13804,
      "processed_text_length": 13804
    }
  },
  "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "Multi-weight Convolutional Neural Networks for Clothing Image Recognition",
      "authors": [
        "作者列表在原文中未明确列出"
      ],
      "institutions": [
        "原文无此信息"
      ]
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "Convolutional Neural Networks",
          "relationship_type": "改进"
        }
      ],
      "compared_methods": [
        {
          "method_name": "multi-label CNNs",
          "comparison_result": "在七个类别中表现最佳"
        },
        {
          "method_name": "multi-task CNNs",
          "comparison_result": "在五个类别中表现最佳"
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "e-Clothing1.4M",
          "dataset_description": "包含大约一百万张来自四个中国零售商的购物照片的大型数据集"
        }
      ],
      "evaluation_metrics": [
        "mean Average Precision (mAP)"
      ],
      "baseline_methods": [
        "multi-label CNNs",
        "multi-task CNNs"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "mAP",
          "our_result": "55.23%",
          "baseline_result": "52.48% (multi-label), 53.11% (multi-task)"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "提出了一种用于处理噪声和类别不平衡的服装图像的多重量卷积神经网络",
        "在大规模数据集上展示了有效性"
      ],
      "stated_novelty": [
        "引入了多重量学习算法以处理类别不平衡问题"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "原文明确承认的局限性在原文中未列出"
      ]
    },
    "extraction_metadata": {
      "file_id": "Improving_deep_convolutional_neural_networks_for_real_world_clothing_image",
      "detected_doc_type": "experimental_paper",
      "source_type": "cleaned",
      "extraction_time": "2025-08-03 11:52:10",
      "text_length": 14789,
      "processed_text_length": 12000,
      "extraction_method": "supplement_script"
    }
  },
  "考虑合成灰数灰度性质的改进区间灰数预测模型": {
    "document_metadata": {
      "document_type": "theoretical_paper",
      "title": "考虑合成灰数灰度性质的改进区间灰数预测模型",
      "authors": [
        "王大鹏",
        "汪秉文",
        "李睿凡"
      ],
      "institutions": [
        "华中科技大学控制科学与工程系",
        "北京邮电大学计算机学院"
      ]
    },
    "theoretical_contributions": {
      "main_theoretical_results": [
        "提出合成灰数灰度的定义及性质",
        "建立灰度序列的GM(1,1)模型实现灰度预测",
        "改进和完善了原有区间灰数预测模型"
      ],
      "theorems_proposed": [
        "定理1",
        "定理2",
        "定理3",
        "定理4"
      ],
      "mathematical_models": [
        "核序列的GM(1,1)模型",
        "灰度序列的GM(1,1)模型",
        "改进的区间灰数预测模型"
      ]
    },
    "technical_relationships": {
      "builds_upon": [
        "基于核和灰度的区间灰数预测模型"
      ],
      "extends": [
        "扩展了原有模型，支持误差分析和精度检验"
      ],
      "relates_to": [
        "与区间灰数序列预测相关"
      ]
    },
    "innovation_analysis": {
      "theoretical_novelty": [
        "提出了合成灰数灰度的定义及性质"
      ],
      "mathematical_contributions": [
        "建立了灰度序列的GM(1,1)模型"
      ]
    },
    "applications": {
      "potential_applications": [
        "区间灰数序列预测"
      ],
      "application_domains": [
        "系统工程与电子技术"
      ]
    },
    "extraction_metadata": {
      "file_id": "考虑合成灰数灰度性质的改进区间灰数预测模型",
      "detected_doc_type": "theoretical_paper",
      "source_type": "basic",
      "extraction_time": "2025-08-03 11:52:35",
      "text_length": 11546,
      "processed_text_length": 11546,
      "extraction_method": "supplement_script"
    }
  },
  "一种基于关系编码和层次注意力机制的图像段落描述方法_李睿凡": {
    "document_metadata": {
      "document_type": "patent",
      "title": "一种基于关系编码和层次注意力机制的图像段落描述方法",
      "inventors": [
        "李睿凡",
        "刘云",
        "石祎晖",
        "冯方向",
        "马占宇",
        "王小捷"
      ],
      "applicant": "北京邮电大学",
      "patent_number": "CN 114186568 B"
    },
    "technical_solution": {
      "technical_problem": "在以前的图像段落描述方法中，物体之间的关系没有得到充分的利用和编码。",
      "solution_overview": "本发明提供了一种基于关系编码和层次注意力机制的图像段落描述方法，包括关系编码过程和层次注意力解码过程。",
      "key_technical_features": [
        "关系编码模块通过两个编码器捕获空间关系信息和语义关系信息",
        "层次注意力解码模块使用带有关系门和视觉门的层次注意力来动态融合关系信息和物体区域特征"
      ]
    },
    "implementation": {
      "embodiments": [
        "关系编码模块包含空间关系编码器和语义关系编码器",
        "层次注意力解码模块包含区域注意力和关系注意力"
      ],
      "technical_effects": [
        "本发明方法在多个评价指标上显著优于现有方法"
      ]
    },
    "application_scope": {
      "application_fields": [
        "图像处理技术领域"
      ],
      "use_scenarios": [
        "图像/视频检索",
        "幼儿教育",
        "帮助视力受损者理解图像内容"
      ]
    },
    "extraction_metadata": {
      "file_id": "一种基于关系编码和层次注意力机制的图像段落描述方法_李睿凡",
      "detected_doc_type": "patent",
      "source_type": "basic",
      "extraction_time": "2025-08-03 11:52:56",
      "text_length": 15108,
      "processed_text_length": 12000,
      "extraction_method": "supplement_script"
    }
  },
  "基于词性和位置的特征关键词提取方法_芦效峰": {
    "document_metadata": {
      "document_type": "patent",
      "title": "基于词性和位置的特征关键词提取方法",
      "inventors": [
        "芦效峰",
        "王文婷",
        "李睿凡"
      ],
      "applicant": "北京邮电大学, 国网山东省电力公司电力科学研究",
      "patent_number": "202110184849.9"
    },
    "technical_solution": {
      "technical_problem": "在文本挖掘领域，TF-IDF方法经常挑选词频很高但是实际意义却很小的词作为关键词，且不考虑词在文章中的位置和词性对词的特征权重的影响。",
      "solution_overview": "本发明提供了一种基于词性和位置的特征关键词提取方法，包括文本预处理，去除特定词性的候选关键词，计算加权词频，计算增量逆文档频率，计算权重，并按权重排序选择关键词。",
      "key_technical_features": [
        "词性和位置加权",
        "增量逆文档频率",
        "动态数据集适应"
      ]
    },
    "implementation": {
      "embodiments": [
        "对文本进行预处理，包括分词,去除停用词和标点符号；去除特定词性的词；考虑词位置和词性计算加权词频；计算增量逆文档频率；计算权重；排序选择关键词"
      ],
      "technical_effects": [
        "提高了关键词提取的正确率",
        "适用于动态变化的数据集",
        "避免与文本主题无关的词误认为关键词"
      ]
    },
    "application_scope": {
      "application_fields": [
        "文本挖掘",
        "特征提取"
      ],
      "use_scenarios": [
        "实时网络话题检测",
        "动态数据集关键词提取"
      ]
    },
    "extraction_metadata": {
      "file_id": "基于词性和位置的特征关键词提取方法_芦效峰",
      "detected_doc_type": "patent",
      "source_type": "basic",
      "extraction_time": "2025-08-03 11:53:32",
      "text_length": 4098,
      "processed_text_length": 4098,
      "extraction_method": "supplement_script"
    }
  },
  "C语言实例对比_从抽象概念到代码直观": {
    "document_metadata": {
      "document_type": "theoretical_paper",
      "title": "C语言实例对比：从抽象概念到代码直观",
      "authors": [
        "无明确列出"
      ],
      "institutions": [
        "北京邮电大学计算机学院",
        "北京"
      ]
    },
    "theoretical_contributions": {
      "main_theoretical_results": [
        "无明确的理论结果"
      ],
      "theorems_proposed": [
        "无提出的定理"
      ],
      "mathematical_models": [
        "无数学模型"
      ]
    },
    "technical_relationships": {
      "builds_upon": [
        "无明确基于的理论"
      ],
      "extends": [
        "无明确扩展的理论"
      ],
      "relates_to": [
        "谭浩强教授的C高级语言程序设计教材",
        "比较教学法",
        "程序设计语言实用化"
      ]
    },
    "innovation_analysis": {
      "theoretical_novelty": [
        "无明确声明的理论新颖性"
      ],
      "mathematical_contributions": [
        "无明确声明的数学贡献"
      ]
    },
    "applications": {
      "potential_applications": [
        "无明确提到的潜在应用"
      ],
      "application_domains": [
        "C语言教学",
        "程序设计教学",
        "编程习惯培养"
      ]
    },
    "extraction_metadata": {
      "file_id": "C语言实例对比_从抽象概念到代码直观",
      "detected_doc_type": "theoretical_paper",
      "source_type": "basic",
      "extraction_time": "2025-08-03 11:53:45",
      "text_length": 4284,
      "processed_text_length": 4284,
      "extraction_method": "supplement_script"
    }
  },
  "使用LDC码的BI_STCM_ID系统中的星座映射分析": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "使用LDC码的BI—STCM—ID系统中的星座映射分析",
      "authors": [
        "赵传钢",
        "李睿凡"
      ],
      "institutions": [
        "北京林业大学信息学院",
        "北京邮电大学信息工程学院"
      ]
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "BI-STCM—ID系统",
          "relationship_type": "基于"
        }
      ],
      "compared_methods": [
        {
          "method_name": "Alamouti空时编码方案",
          "comparison_result": "原文无此信息"
        },
        {
          "method_name": "OSTBC码的BI—STCM—ID系统",
          "comparison_result": "原文无此信息"
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "原文无此信息",
          "dataset_description": "原文无此信息"
        }
      ],
      "evaluation_metrics": [
        "渐进误比特率性能",
        "分集增益",
        "编码增益"
      ],
      "baseline_methods": [
        "原文无此信息"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "渐进误比特率性能",
          "our_result": "原文无此信息",
          "baseline_result": "原文无此信息"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "提出了一种基于LDC码的BI—STCM—ID系统星座映射分析方法",
        "简化了在使用LDC码的BI—STCM—ID系统中高位星座映射设计问题"
      ],
      "stated_novelty": [
        "原文无此信息"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "对于其他空时编码方案是否适用的问题尚未研究",
        "原文无此信息"
      ]
    },
    "extraction_metadata": {
      "file_id": "使用LDC码的BI_STCM_ID系统中的星座映射分析",
      "detected_doc_type": "experimental_paper",
      "source_type": "basic",
      "extraction_time": "2025-08-03 11:54:23",
      "text_length": 7324,
      "processed_text_length": 7324,
      "extraction_method": "supplement_script"
    }
  },
  "基于KFD_Isomap的人脸识别": {
    "document_metadata": {
      "document_type": "experimental_paper",
      "title": "基于KFD-l somap的人脸识别",
      "authors": [
        "李睿凡",
        "郝红卫",
        "涂序彦",
        "王枞"
      ],
      "institutions": [
        "北京邮电大学信息工程学院",
        "北京科技大学信息工程学院"
      ]
    },
    "technical_relationships": {
      "base_methods": [
        {
          "method_name": "Isomap",
          "relationship_type": "改进"
        }
      ],
      "compared_methods": [
        {
          "method_name": "Isomap",
          "comparison_result": "KFD-Isomap在识别率上有较大提高"
        },
        {
          "method_name": "Ext.Isomap",
          "comparison_result": "KFD-Isomap方法中判别分析的引入使得识别率提高"
        },
        {
          "method_name": "特征脸",
          "comparison_result": "具有类似的识别效果"
        },
        {
          "method_name": "Fisher脸",
          "comparison_result": "KFD-Isomap方法性能优于Fisher脸算法"
        }
      ]
    },
    "experimental_setup": {
      "datasets_used": [
        {
          "dataset_name": "ORL人脸数据库",
          "dataset_description": "包含36位男性和4位女性，每人10幅图像，共400幅面部图像，每幅图像为112×92象素，256灰度级"
        },
        {
          "dataset_name": "Yale人脸数据库",
          "dataset_description": "包括15个人，大小为320×243的165幅人脸图像"
        }
      ],
      "evaluation_metrics": [
        "错误率(％)"
      ],
      "baseline_methods": [
        "Eigenface",
        "Fisherface",
        "Isomap",
        "Ext-Isomap"
      ]
    },
    "performance_results": {
      "quantitative_results": [
        {
          "metric_name": "错误率(％)",
          "our_result": "0.75",
          "baseline_result": "3.50"
        },
        {
          "metric_name": "错误率(％)",
          "our_result": "8.48",
          "baseline_result": "26.06"
        }
      ]
    },
    "innovation_analysis": {
      "stated_contributions": [
        "提出了一种用核Fisher判别函数改进的Isomap算法",
        "该方法用测地线距离矩阵的列向量作为特征，并用核Fisher判别替代多维尺度分析建立最佳投影方向"
      ],
      "stated_novelty": [
        "在人脸识别实验中，该方法性能优于实验采用的其它方法"
      ]
    },
    "limitations": {
      "acknowledged_limitations": [
        "实际模式分类应用中数据的稀疏性",
        "Isomap方法在模式分类问题上的数据处理方式并非最优"
      ]
    },
    "extraction_metadata": {
      "file_id": "基于KFD_Isomap的人脸识别",
      "detected_doc_type": "experimental_paper",
      "source_type": "basic",
      "extraction_time": "2025-08-03 11:55:13",
      "text_length": 6609,
      "processed_text_length": 6609,
      "extraction_method": "supplement_script"
    }
  },
  "一种基于多层次图卷积网络的电力设备故障溯源方法_李睿凡": {
    "document_metadata": {
      "document_type": "unknown",
      "title": "一种基于多层次图卷积网络的电力设备故障溯源方法",
      "authors": [
        "未找到该信息"
      ]
    },
    "main_content": {
      "description": "该文档描述了一种基于多层次图卷积网络的电力设备故障溯源方法，包括从电力工单系统中收集故障文本信息，计算TF-IDF和PMI指标，构建电力工单图和多层次图卷积网络，以及使用该网络识别电力设备故障位置等步骤。",
      "keywords": [
        "电力设备故障溯源",
        "多层次图卷积网络",
        "TF-IDF",
        "PMI",
        "电力工单图"
      ]
    },
    "extraction_metadata": {
      "file_id": "一种基于多层次图卷积网络的电力设备故障溯源方法_李睿凡",
      "source_type": "cleaned",
      "extraction_time": "2025-08-03 12:34:48",
      "extraction_method": "simple_supplement",
      "text_length": 875
    }
  },
  "人工生命中分布智能研究的一种可行方法": {
    "document_metadata": {
      "document_type": "unknown",
      "title": "人工生命中分布智能研究的一种可行方法",
      "authors": [
        "未找到该信息"
      ]
    },
    "main_content": {
      "description": "本文分析了蚁群算法和粒子群算法的提出过程，并基于此提出了一种人工生命分布智能研究的方法。强调了理论生物学对人工生命研究的基础性作用，并探讨了从理论生物学中选择研究方向、利用现有模型的方法，以及分布智能研究的三个过程。",
      "keywords": [
        "人工生命",
        "分布智能",
        "蚁群算法",
        "粒子群优化"
      ]
    },
    "extraction_metadata": {
      "file_id": "人工生命中分布智能研究的一种可行方法",
      "source_type": "cleaned",
      "extraction_time": "2025-08-03 12:34:54",
      "extraction_method": "simple_supplement",
      "text_length": 874
    }
  },
  "使用TAST码的BI_STCM_ID系统中的星座映射分析": {
    "document_metadata": {
      "document_type": "unknown",
      "title": "未找到该信息",
      "authors": [
        "未找到该信息"
      ]
    },
    "main_content": {
      "description": "研究了BI-STCM-ID系统中的星座映射问题。证明了在使用TAST空时编码方案的BI-STCM-ID系统中，基于最大化编码增益的高维星座映射设计优化问题等价于基于最大化欧式距调和均值的一维星座映射设计优化问题。",
      "keywords": [
        "BI-STCM-ID",
        "星座映射",
        "TAST空时编码",
        "编码增益",
        "欧式距",
        "调和均值"
      ]
    },
    "extraction_metadata": {
      "file_id": "使用TAST码的BI_STCM_ID系统中的星座映射分析",
      "source_type": "cleaned",
      "extraction_time": "2025-08-03 12:35:00",
      "extraction_method": "simple_supplement",
      "text_length": 106
    }
  },
  "探索神经网络深度学习的教学": {
    "document_metadata": {
      "document_type": "unknown",
      "title": "探索神经网络深度学习的教学",
      "authors": [
        "未找到该信息"
      ]
    },
    "main_content": {
      "description": "文章讨论如何在本科生与研究生课程中有效开展深度学习教学，介绍智能领域的最新研究成果。内容包括本科生与研究生教学建议，以及深度学习的核心内容与应用。",
      "keywords": [
        "智能科学与技术",
        "深度学习",
        "教学建议"
      ]
    },
    "extraction_metadata": {
      "file_id": "探索神经网络深度学习的教学",
      "source_type": "cleaned",
      "extraction_time": "2025-08-03 12:35:06",
      "extraction_method": "simple_supplement",
      "text_length": 519
    }
  },
  "智能科学技术导论教学目的及策略": {
    "document_metadata": {
      "document_type": "unknown",
      "title": "智能科学技术导论教学目的及策略",
      "authors": [
        "周延泉",
        "李睿凡",
        "焦晨晨"
      ]
    },
    "main_content": {
      "description": "本文针对智能科学技术导论作为智能科学与技术专业基础课的必要性和重要性，分析了教学现状，探讨了课程结构设置及教学策略，包括改革教学模式、采取课堂与实验室结合教学以及建立多样化、全方位的考评模式等。",
      "keywords": [
        "智能科学技术导论",
        "教学目标",
        "教学策略"
      ]
    },
    "extraction_metadata": {
      "file_id": "智能科学技术导论教学目的及策略",
      "source_type": "cleaned",
      "extraction_time": "2025-08-03 12:35:14",
      "extraction_method": "simple_supplement",
      "text_length": 599
    }
  }
}