{
  "innovation_landscape": {
    "top_contribution_types": [
      {
        "contribution": "a novel topic-oriented captioning model",
        "frequency": 1
      },
      {
        "contribution": "the FGU design",
        "frequency": 1
      },
      {
        "contribution": "extensive experimental evaluation",
        "frequency": 1
      },
      {
        "contribution": "Proposing a Multi-scale Two-way Deep Neural Network (MTDNN) for stock trend prediction",
        "frequency": 1
      },
      {
        "contribution": "A novel method combining both MS and TD properties in financial time-series.",
        "frequency": 1
      },
      {
        "contribution": "MS feature extraction with convolutional units without predefined parameters.",
        "frequency": 1
      },
      {
        "contribution": "Fusion of different scale features using a Recurrent Neural Network to capture temporal dependencies.",
        "frequency": 1
      },
      {
        "contribution": "提出了一种结合语法和语义特征的DualGCN模型",
        "frequency": 1
      },
      {
        "contribution": "设计了正交和微分正则化器以增强模型捕获语义关联的能力",
        "frequency": 1
      },
      {
        "contribution": "a novel EMC-GCN model",
        "frequency": 1
      },
      {
        "contribution": "a comprehensive exploitation of linguistic features",
        "frequency": 1
      },
      {
        "contribution": "an effective refining strategy",
        "frequency": 1
      },
      {
        "contribution": "提出了一种结合预训练语言模型和图卷积网络的简单模型",
        "frequency": 1
      },
      {
        "contribution": "propose a COntext-Masked MRC (COM-MRC) framework for ASTE tasks",
        "frequency": 1
      },
      {
        "contribution": "alleviate interference in ASTE tasks",
        "frequency": 1
      },
      {
        "contribution": "the context augmentation strategy effectively expanding the training corpus",
        "frequency": 1
      },
      {
        "contribution": "提出了一种新的KE-GCL模型",
        "frequency": 1
      },
      {
        "contribution": "在CQA任务中整合了GCL",
        "frequency": 1
      },
      {
        "contribution": "通过自适应图增强和选择错误的答案来增强图表示学习",
        "frequency": 1
      },
      {
        "contribution": "a bi-lexical dependency parsing graph converted to a unified 2D table filling scheme (USSA)",
        "frequency": 1
      }
    ],
    "emerging_novelty_areas": [
      {
        "area": "Our TOMS model differs by generating sentences from topics of interest, capturing linguistic distinctions in image descriptions.",
        "frequency": 1
      },
      {
        "area": "The Fusion Gate Unit (FGU) fuses three sources of representations: image, context, and topic.",
        "frequency": 1
      },
      {
        "area": "Using wavelet-based and downsampling-based scale information",
        "frequency": 1
      },
      {
        "area": "Achieving state-of-the-art performance on FI-2010 and CSI-2016 datasets",
        "frequency": 1
      },
      {
        "area": "DualGCN模型的结构设计",
        "frequency": 1
      },
      {
        "area": "正交和微分正则化器的应用",
        "frequency": 1
      },
      {
        "area": "Our proposed framework is EMC-GCN, which addresses Aspect and Opinion Term Co-Extraction (AOTE) and Aspect-Sentiment Pair Extraction (ASPE)",
        "frequency": 1
      },
      {
        "area": "We define ten types of relations between words for the ASTE task",
        "frequency": 1
      },
      {
        "area": "首次使用GCN直接学习实例之间的包表示",
        "frequency": 1
      },
      {
        "area": "COM-MRC’s components work collaboratively",
        "frequency": 1
      },
      {
        "area": "the two-stage inference method reduces interference from other aspects",
        "frequency": 1
      },
      {
        "area": "using product titles to guide the learning of visual features",
        "frequency": 1
      },
      {
        "area": "conversion of product titles into discrete labels for supervised learning",
        "frequency": 1
      },
      {
        "area": "We propose Splitting to Tree Decoder (S2TD), a novel tree-structured decoder that models paragraph decoding as top-down binary tree expansion.",
        "frequency": 1
      },
      {
        "area": "原文未明确声明",
        "frequency": 1
      }
    ]
  },
  "technical_methods": {
    "popular_base_methods": [
      {
        "method": "BERT",
        "usage_count": 3
      },
      {
        "method": "Graph Convolutional Network (GCN)",
        "usage_count": 2
      },
      {
        "method": "Stable Diffusion",
        "usage_count": 2
      },
      {
        "method": "CLIP",
        "usage_count": 2
      },
      {
        "method": "Single-sentence (SS) captioning models",
        "usage_count": 1
      },
      {
        "method": "MS captioning methods",
        "usage_count": 1
      },
      {
        "method": "Support Vector Machine and Neural Networks",
        "usage_count": 1
      },
      {
        "method": "Ensemble-based methods like Random Forest and deep learning models",
        "usage_count": 1
      },
      {
        "method": "sparse filtering",
        "usage_count": 1
      },
      {
        "method": "entropy regularization",
        "usage_count": 1
      },
      {
        "method": "ensemble voting techniques with denoising autoencoders and maxout networks",
        "usage_count": 1
      },
      {
        "method": "Multi-Scale (MS) methods, TD-oriented methods",
        "usage_count": 1
      },
      {
        "method": "Attention-based LSTM",
        "usage_count": 1
      },
      {
        "method": "Recursive neural network by Dong et al. (2014)",
        "usage_count": 1
      },
      {
        "method": "BERT-based Graph Convolutional network Model (BGM)",
        "usage_count": 1
      }
    ]
  },
  "experimental_ecosystem": {
    "popular_datasets": [
      {
        "dataset": "COCO",
        "usage_count": 3
      },
      {
        "dataset": "GDS",
        "usage_count": 2
      },
      {
        "dataset": "CommonsenseQA",
        "usage_count": 2
      },
      {
        "dataset": "OpenbookQA",
        "usage_count": 2
      },
      {
        "dataset": "CustomConcept101",
        "usage_count": 2
      },
      {
        "dataset": "CUB",
        "usage_count": 2
      },
      {
        "dataset": "MS-COCO",
        "usage_count": 2
      },
      {
        "dataset": "Flickr8k",
        "usage_count": 1
      },
      {
        "dataset": "Flickr30k",
        "usage_count": 1
      },
      {
        "dataset": "paragraph dataset from Krause et al. (2017)",
        "usage_count": 1
      },
      {
        "dataset": "FI-2010",
        "usage_count": 1
      },
      {
        "dataset": "CSI-2016",
        "usage_count": 1
      }
    ],
    "common_metrics": [
      {
        "metric": "CIDEr",
        "usage_count": 6
      },
      {
        "metric": "accuracy",
        "usage_count": 5
      },
      {
        "metric": "METEOR",
        "usage_count": 4
      },
      {
        "metric": "F1 score",
        "usage_count": 3
      },
      {
        "metric": "Precision",
        "usage_count": 3
      },
      {
        "metric": "Recall",
        "usage_count": 3
      },
      {
        "metric": "Accuracy",
        "usage_count": 3
      },
      {
        "metric": "F1 scores",
        "usage_count": 2
      },
      {
        "metric": "CLIP",
        "usage_count": 2
      },
      {
        "metric": "mean average precision (mAP)",
        "usage_count": 2
      },
      {
        "metric": "top 20% percentage",
        "usage_count": 2
      },
      {
        "metric": "Fréchet Inception Distance (FID)",
        "usage_count": 2
      }
    ]
  }
}