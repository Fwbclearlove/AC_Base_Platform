Enhanced Machine Reading Comprehension Method for
Aspect Sentiment Quadruplet Extraction
Shuqin Yea, Zepeng Zhaia and Ruifan Lia,b,c;*
aSchool of Artiﬁcial Intelligence, Beijing University of Posts and Telecommunications, China
bEngineering Research Center of Information Networks, Ministry of Education, China
cKey Laboratory of Interactive Technology and Experience System, Ministry of Culture and Tourism, China
{shuqinye, zepeng, rﬂi}@bupt.edu.cn
ORCiD ID: Ruifan Li https://orcid.org/0000-0002-3543-6272
Abstract.
In the NLP domain, Aspect-Based Sentiment Analy-
sis (ABSA) has gained signiﬁcant attention in recent years due to
its ability to perform ﬁne-grained sentiment analysis. A challenging
task in ABSA is Aspect Sentiment Quadruplet Extraction (ASQE),
which involves the extraction of aspect terms and their associated
opinion terms, sentiment polarities, and categories in the form of
quadruplets. However, existing studies have ignored the strong de-
pendence among the multiple subtasks involved in ASQE. In this pa-
per, we propose a novel Enhanced Machine Reading Comprehension
(EMRC) method and formalize ASQE task as a multi-turn MRC task.
Our EMRC effectively learns and utilizes the relationships among
different subtasks by incorporating previously generated query an-
swers into the current queries. We design a hierarchical category clas-
siﬁcation strategy to perform the category prediction in a structured
manner, enabling the model to tackle intricate categories with ease.
Furthermore, we employ the bi-directional attention mechanism, i.e.,
context-to-query and query-to-context attentions, to map the context
into a task-aware representation. We conduct extensive experiments
on two benchmark datasets. The results demonstrate that EMRC out-
performs the state-of-art baselines. The source code is publicly avail-
able at https://github.com/Little-Yeah/EMCR.
1
Introduction
Aspect-Based Sentiment Analysis (ABSA) is a ﬁne-grained senti-
ment analysis task that aims to extract and analyze sentiment infor-
mation of aspect terms within a given text [2, 9, 14, 16, 36, 20, 37].
The main research line of ABSA involves the identiﬁcation of
four aspect-level sentiment elements, namely, aspect terms, opin-
ion terms, aspect categories, and sentiment polarities. Figure 1 gives
an example sentence and illustrates its corresponding sentiment ele-
ments. Correspondingly, ABSA has been decomposed into multiple
subtasks to predict a single element in early studies, including Aspect
Target Extraction (ATE) [18, 24], Opinion Target Extraction (OTE)
[10, 30], Aspect Category Detection (ACD) [13, 41], and Aspect
Sentiment Classiﬁcation (ASC) [19, 26]. Recently, more works have
focused on the extraction of multiple sentiment elements simultane-
ously [6, 22]. For example, Aspect-Opinion Pair Extraction (AOPE)
[6, 40, 29, 8] requires extracting the aspect and its associated opin-
ion term in a compound form. Aspect Sentiment Triplet Extraction
∗Corresponding Author. Email: rﬂi@bupt.edu.cn.
The portions are small but being that the food was so good makes up for that.
negative
positive
FOOD#STYLE_OPTIONS
FOOD#GENERAL
Aspect Terms: {portions, food}
Opinion Terms: {small, good}
Sentiment Polarities: {negative, positive}
Categories: {FOOD#STYLE_OPTIONS, FOOD#GENERAL}
Quadruplets: {(portions, FOOD#STYLE_OPTIONS, small, negative),
(food, FOOD#GENERAL, good, positive)}
A Review Sentence:
Figure 1.
An example of sentimental elements and the output of ASQE
task for a restaurant review sentence. A quadruplet comprises of (aspect,
category, opinion, sentiment). Here, two quadruplets are predicted.
(ASTE) [5, 31, 27, 33] attempts to extract aspect-opinion-sentiment
triplets from the given sentence.
Despite the great progress in ABSA, most existing studies have
only considered the partial extraction of sentiment elements in-
stead of providing a comprehensive aspect-level sentiment struc-
ture. To this end, Aspect Sentiment Quadruplet Extraction (ASQE)
task is proposed, which aims to predict all the four sentiment el-
ements in the quadruplet form. As shown in Figure 1, in the re-
view sentence "The portions are small but being that the food
was so good makes up for that.", two corresponding quadruplets,
(portions, FOOD#STYLE_OPTIONS, small, negative) and (food,
FOOD#GENERAL, good, positive) are produced. As a pioneering
work, Cai et al. [3] studied ASQE task and constructed a series of
pipeline baselines by combining existing models to benchmark the
task. Subsequently, Zhang et al. [38], Bao et al. [1] and Gou et al.
[12]address ASQE task using generative approaches.
However, the existing studies have neglected certain inherent char-
acteristics of ASQE task resulting in unsatisfactory results. To this
end, we consider the following three challenges. First, how can the
rich associations among different subtasks be exploited? ASQE task
can be considered as a composition of two extraction subtasks and
two classiﬁcation subtasks that are interrelated and mutually inﬂuen-
tial. Therefore, learning the dependencies among these subtasks can
lead to a more precise prediction. In particular, the aspect term ex-
traction and opinion term extraction focus on identifying the aspects
ECAI 2023
K. Gal et al. (Eds.)
© 2023 The Authors.
This article is published online with Open Access by IOS Press and distributed under the terms
of the Creative Commons Attribution Non-Commercial License 4.0 (CC BY-NC 4.0).
doi:10.3233/FAIA230600
2874
and opinions that are expressed in a given sentence. The sentiment
polarity classiﬁcation aims to assign positive, negative, or neutral po-
larity to the identiﬁed aspect-opinion pairs. In addition, in the cate-
gory prediction task is to predict the category of each aspect term.
Hence, to achieve a satisfactory performance in ASQE, modeling
the dependencies among these subtasks is crucial. Second, how can
the aspect category classiﬁcation be accurately performed especially
when there exist multiple categories? ASQE task can be regarded as
an extension of the conventional ASTE task which additionally in-
volves the prediction of aspect category. Therefore, the performance
improvement of ASQE is intrinsically related to the accuracy of cat-
egory prediction. Notably, we have observed that the commonly used
dataset Laptop-ACOS [3] contains up to 121 categories, making the
accurate category classiﬁcation erroneous. Therefore, it is of impor-
tance to develop an effective strategy for dealing with multiple cat-
egories. Third, how can the representation of the input sentence be
enhanced to adapt to different subtasks? In ASQE task, predicting
four sentiment elements from a single input sentence requires a ro-
bust sentence representation that can adapt to the different subtasks.
Simply enhancing the sentence representation may not be sufﬁcient,
as it is also crucial to map the sentence into a task-aware represen-
tation that is tailored to speciﬁc subtasks. This task-aware represen-
tation can enable the model to better understand the dependencies
between the sentence and the subtasks, and therefore, improve the
performance of the quadruplet extraction.
In this paper, we propose an Enhanced Machine Reading Com-
prehension (EMRC) method for ASQE to address the aforemen-
tioned challenges. First, the method formulates ASQE as a multi-
turn machine reading comprehension task. Our method introduces
the answers of the previous turns to the current turn as the prior
knowledge. Thus, the relations among different subtasks can be nat-
urally captured. For example, given a review sentence in Figure 1,
after extracting the aspect term portions, we feed it into the next
turn to jointly identify its corresponding opinion term small. This
multi-turn information interaction allows our model to effectively
learn and leverage the associations among different subtasks. Sec-
ond, we have noticed that most aspects can be categorized in two
dimensions, namely general categories and subcategories. For ex-
ample, as shown in Figure 2, the aspect term portions falls in cat-
egory FOOD#STYLE_OPTIONS, whose general category is FOOD
and subcategory is STYLE_OPTIONS. To fully leverage the hierar-
chical structure, we devise a hierarchical category classiﬁcation strat-
egy. Speciﬁcally, we ﬁrst design a general category classiﬁcation
query in a coarse-grained manner. Then, we utilize a subcategory
classiﬁcation query to determine its ﬁne-grained category. This two-
turn hierarchical query strategy allows our model to fast focus on
the most relevant candidate categories. Third, we incorporate the bi-
directional attention mechanism [25] to model the relations between
the queries and the context. We compute the context-to-query atten-
tion and the query-to-context attention, integrating information from
both the queries and context. With the task-aware context represen-
tation, the model can make the ﬁnal prediction under the guidance of
queries.
Our major contributions can be summarized as follows:
• We propose a novel EMRC model by formalizing ASQE task
into a multi-turn machine reading comprehension task. Our multi-
turn mechanism effectively builds the associations among differ-
ent sentimental subtasks. To the best of our knowledge, we are the
ﬁrst to propose the MRC-based method for ASQE.
• We devise a hierarchical category classiﬁcation strategy to ad-
portions
FOOD
STYLE_OPTIONS
QUALITY
SERVICE
General category
Subcategory
Hierarchical Category
...
...
Figure 2.
An hierarchical category classiﬁcation example of the aspect
term portions with its general category FOOD and subcategory
STYLE_OPTIONS. The SERVICE and QUALITY are candidate categories.
dress the complexity of aspect categories. And we introduce the
bi-directional attention mechanism to enhance the strong associ-
ation between the context and queries, making the context repre-
sentation task-aware.
• We conduct extensive experiments on two benchmark datasets.
The experimental results demonstrate that our EMRC substan-
tially outperforms the existing baselines. The source code and data
of our work are released for knowledge sharing.
2
Related work
Aspect-Based Sentiment Analysis (ABSA) has shown signiﬁcant in-
terest in recent years [2, 9, 14, 16, 36],. In the early years, researchers
focused on the independent prediction of a single element. For in-
stance, Aspect Term Extraction (ATE) [18, 24] and Opinion Term
Extraction (OTE) [10, 30] are to extract all mentioned aspect terms
or opinion terms in the given sentence. In contrast, Aspect Category
detection (ACD) [13, 41] aims to identify the discussed aspect cate-
gories. Aspect Sentiment Classiﬁcation (ASC) [19, 26, 4] is to pre-
dict the sentiment polarity for a speciﬁc aspect within a sentence.
More recently, to understand more complete aspect-level opinion and
recognize the correspondence and dependency among different sen-
timent elements, several recent studies have explored the joint detec-
tion of multiple sentiment elements in the pair, triplet, or quadruplet
formats, including Aspect-Opinion Pair Extraction (AOPE) [11, 41],
Aspect Sentiment Triplet Extraction (ASTE) [5, 31, 27], and Aspect
Sentiment Quadruplet Extraction (ASQE) [1, 3, 38].
Compared to the other ABSA tasks, ASQE is considered as the
most comprehensive and challenging task due to its complexity in
capturing and combining more sentiment elements. As a pioneering
work, Cai et al. [3] introduce two new datasets with sentiment quad
annotations and construct a series of pipeline baselines by combin-
ing existing models to benchmark the task. Zhang et al. [38] propose
a Paraphrase modeling strategy to predict the sentiment quads in an
end-to-end manner. Bao et al. [1] introduce a new Opinion Tree Gen-
eration model, which aims to jointly detect all sentiment elements in
a tree. However, these methods ignore the interaction among quadru-
plets, which could result in error propagation.
Recently, Machine Reading Comprehension (MRC) has emerged
as a promising approach in the ﬁeld of ABSA. This mainly dues to
its ability to effectively predict sentiment elements within a uniﬁed
framework. Researchers have proposed several MRC-based models
for various ABSA subtasks. For example, Mao et al. [21] introduced
a dual-MRC framework that jointly trains two MRC models, while
Chen et al. [7] developed a bi-directional MRC structure for identi-
fying sentiment triplets in ASTE. Additionally, Zhai et al. [35] pro-
posed a context-masked MRC approach to enhance contextual in-
S. Ye et al. / Enhanced Machine Reading Comprehension Method for Aspect Sentiment Quadruplet Extraction
2875
What opinions?
Answer: small, good
What aspects?
Answer: portions, food
{(portions, FOOD#STYLE_OPTIONS,small, negative),
{(food,FOOD#GENERAL,good,positive)}
The portions are small but being that the food was so good makes up for that.
What general category does
the aspect portions belong to?
Answer: FOOD
What general category does
the aspect food belong to?
Answer: FOOD
What subcategory of the general
category FOOD does the aspect portions belong to?
Answer: STYLE_OPTIONS
What subcategory of the general
category FOOD does the aspect food belong to?
Answer: GENERAL
What opinions given
the aspect portions?
Answer: small
What opinions given
the aspect food?
Answer: good
What aspects does
the opinion small describe?
Answer: portions
What aspects does
the opinion good describe?
Answer: food
What sentiment given the aspect portions and the opinion small ?
Answer: negative
What sentiment given the aspect food and the opinion good ?
Answer: positive
{(portions, small)
(food,good)}
Bi-directional Extraction Procedure
First Turn
Second Turn
Classification Procedure
Figure 3.
The ﬂow diagram of our Enhanced Machine Reading Comprehension (EMRC) model. Given a review sentence, the bi-directional (forward and
backward) extraction queries determine the aspect-opinion pairs. Subsequently, the determined pairs will be fed into the sentiment and category classiﬁcation
queries to predict the sentiment polarities and aspect categories, respectively. Finally, the four elements will be combined into a sentiment quadruplet.
formation. Inspired by these studies that demonstrated the signiﬁ-
cant potential of MRC in tackling ABSA task, we propose an MRC-
based method for ASQE task, which helps to capture the associations
among multiple subtasks and enhance the relationship between the
context and queries.
3
Methodology
In this section, we describe our proposed EMRC method in details.
The overall framework of our EMRC is depicted in Figure 3.
3.1
Problem Formulation
Given a sentence X
=
{x1, x2, · · · , xn} with n words, the
goal of our model is to produce a set of quadruplets Q
=
{(a, c, o, s)m}|Q|
m=1, where a is the extracted aspect term, c ∈C is
its category which is concatenated by its general category cG ∈CG
and subcategory cS ∈CS, o is the extracted opinion term, and
s ∈{POS, NEU, NEG} is its sentiment polarity. The sentence
X has a total number of |Q| quadruplets.
3.2
Query Construction
3.2.1
Bi-directional Extraction Query
Our bi-directional (forward and backward) extraction procedure
deals with the ATE and OTE subtasks. The details are described one
after another. Firstly, we deﬁne the procedure of forward extraction,
i.e., from an aspect term to an opinion term as A →O. The objec-
tive of the forward extraction is two-fold. The ﬁrst is to identify all
aspect terms present in the given sentence. The second is to recog-
nize the corresponding opinion terms for each identiﬁed aspect term.
To achieve this, queries are constructed in the following manner:
qA
A→O: What aspects?
qO
A→O: What opinions describe the aspect ai?
We employ qA
A→O to obtain all the aspect terms A = {ai}|A|
i=1 in
the given sentence. Then we use qO
A→O to extract the corresponding
opinion terms Oai = {oai,j}|Oai|
j=1
for each aspect term ai.
Secondly, the backward extraction procedure aims to identify all
opinion terms in the given sentence and recognize the corresponding
aspect terms for each identiﬁed opinion term. Similarly, we deﬁne
the procedure of backward extraction, i.e., from an opinion term to
an aspect term as O →A. Compared with the forward extraction,
the backward extraction queries are constructed in a reverse manner
as follows,
qO
O→A: What opinions?
qA
O→A: What aspects does the opinion oi describe?
We employ qO
O→A to obtain all the opinion terms O = {oi}|O|
i=1 in
the given sentence. Then we use qA
O→A to extract the corresponding
aspect terms Aoi = {aoi,j}|Aoi|
j=1
described by each opinion term oi.
Thus, the bi-directional extraction procedure gives a comprehen-
sive understanding of the relations between aspect terms and opinion
terms. Then, based on produced results, the model further predicts
the aspect categories and aspect-opinion pair sentiment polarity.
3.2.2
Hierarchical Category Classiﬁcation Query
Our proposed hierarchical category classiﬁcation strategy offers a
structured approach to categorizing aspects. Our strategy involves
an initial classiﬁcation of aspects into general categories, followed
by a ﬁne-grained subcategory classiﬁcation. For instance, illus-
trated in Figure 2, when classifying the category of the aspect term
portions, the model ﬁrst identiﬁes the general category FOOD to
which portions belongs, and then determine its speciﬁc subcategory
STYLE_OPTIONS by referring to the answer FOOD. By adopting
such a hierarchical strategy, the model is able to progressively de-
termine the category of aspect, leading to more precise classiﬁcation
results. To this end, our queries are constructed as follows:
S. Ye et al. / Enhanced Machine Reading Comprehension Method for Aspect Sentiment Quadruplet Extraction
2876
qG
C: What general category does the aspect ai belong to?
qS
C: What subcategory of the general category cG
ai does the
aspect ai belong to?
Speciﬁcally, we utilize qG
C to retrieve the general category cG
ai of
the aspect ai. Subsequently, we employ qS
C to identify its correspond-
ing ﬁne-grained subcategory cS
ai. By concatenating the two answers,
we obtain the ﬁnal category cai for the aspect ai.
3.2.3
Sentiment Classiﬁcation Query
To determine the sentiment polarity expressed towards the given as-
pect term and its associated opinion terms, we devise the following
sentiment classiﬁcation queries:
qS: What sentiment given the aspect ai and the opinion
oai,j?
Using the above three types of designed queries, we can obtain the
target sentiment quadruplet.
3.3
Bi-directional Attention
To further capture the relationship between the input context and
a given query and merge the information into a task-aware rep-
resentation, we adopt the bi-directional attention mechanism, i.e.,
context-to-query and query-to-context attention. The overall frame-
work is depicted in Figure 4. Firstly, we adopt BERT [15] as the
encoder to obtain the hidden representations. Given a sentence X
with n words and each query Q
=
{q1, q2, . . . , qm} with m
words, the encoding layer outputs the context hidden representa-
tion H = {h1, h2, . . . , hn} and the query hidden representation
U = {u1, u2, . . . , um}.
Next, to create a task-aware representation, we need to combine
the context representation H and query representation U to make
the context more suited to the current sub-task. To this end, the bi-
directional attention mechanism [25], is integrated into the second-
turn bi-directional extraction, i.e., qO
A→O and qA
O→A. As the qO
A→O
and qA
O→A queries contain the aspect and opinion term existed in
the input context, the utilization of the bi-directional attention mech-
anism facilitates the extraction of the contextual and query-related
relations, which can be fused into a task-aware representation. This
enables the model to provide reliable guidance as to where the model
should focus its attention.
This mechanism involves two types of attention: context-to-query
and query-to-context attentions. These attentions are derived from
the similarity matrix S ∈Rn×m computed between the context and
query representations. Speciﬁcally, the similarity score between the
i-th context word and j-th query word is denoted as Sij. The simi-
larity matrix is calculated as follows:
Sij = α (H:i, U:j) ,
(1)
α(h, u) = w⊤
(S)[h; u; h ⊙u],
(2)
where α is a trainable scalar function that measures the similarity
between its two input vectors, H:i represents the i-th column vector
of H. Moreover, w(S) ∈R3d is a trainable weight vector, where d
is the dimension of the word embeddings. The operator ⊙denotes
Context
BERT
Similarity
matrix
...
Query
...
softmax
Task-aware Representation
max
softmax
Q-to-C
Attention
C-to-Q
Attention
Figure 4.
The framework of the bi-directional attention mechanism. The
left is query-to-context attention, and the part is context-to-query attention.
The trainable scalar function α measures the similarity between its two input
vectors. The function β is used to merge the original context representation
and two attention-enhanced representations into a task-aware representation.
element-wise multiplication, while [; ] denotes vector concatenation
across rows. The implicit multiplication is equivalent to matrix mul-
tiplication. Thus, we use the similarity matrix S to obtain the atten-
tions and the attention-enhanced representations in both directions:
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
ai = softmax (Si:) ∈Rm

H:i =

j
aijU:j
b = softmax

max
i
(Sij)
∈Rn
ˆh =

j
bjH:j,
(3)
where ai ∈Rm represents the attention weights on the query words
by i-th context word, to indicate which query words are the most
relevant to each context word, and 
H encapsulates the information
about the relevance of each query word to each context word. Simi-
larly, b represents the attention weights on the context word by j-th
query word, signifying which context words are most relevant to each
query word. To obtain the attention-enhanced presentation ˆH, which
encapsulates the information about the most important words in the
context with respect to the query, we duplicate ˆh for n times and
combine these copies into a d-by-n matrix.
Finally, we merge the original context representation H and the
two attention-enhanced representations 
H and ˆH produced from Eq.
(3) to obtain the task-aware representation ¨H:
¨H:t = β

H:t, ˜H:t, ˆH:t
,
(4)
where β(a, b, c) = [a; b; a ⊙b; a ⊙c].
Through bi-directional attention, we effectively capture and link
information from both the queries and context, enabling a more com-
prehensive understanding of the relations between them.
S. Ye et al. / Enhanced Machine Reading Comprehension Method for Aspect Sentiment Quadruplet Extraction
2877
3.4
Answer Prediction
To produce correct answers, we design multiple classiﬁers. As mul-
tiple queries are involved in our model, to avoid query conﬂicts, each
query employs at least one unique classiﬁer. For extraction queries,
we design two classiﬁers for each query to obtain the answer spans.
Speciﬁcally, eight classiﬁers are employed to predict whether the
word xi is the starting or ending position of the answer of the four
queries, i.e., qA
A→O, qO
A→O, qO
O→A, and qA
O→A as follows,
⎧
⎨
⎩
p
ystart
i
| xi, q
= softmax
h|q|+2+iWs
p

yend
i
| xi, q
= softmax
h|q|+2+iWe
,
(5)
where Ws ∈Rdh×2 and We ∈Rdh×2 are model parameters, dh
denotes the dimension of hidden representations of H, which is H
in the ﬁrst-turn extraction, and ¨H in the second-turn extraction. And
|q| means the length of each query.
For classiﬁcation queries, the answer is predicted based on the rep-
resentation of [CLS]. Hence, the prediction of the three queries, i.e.,
qG
C, qS
C, and qS is as follows :
p (y∗| X, q) = softmax (h1W∗) ,
(6)
where y∗denotes yG
C, yS
C, and yS for different queries. W∗are model
parameters and denotes Wp ∈Rdh×3, Wgc ∈Rdh×ngc and Wsc ∈
Rdh×nsc. In addition, ngc denotes the number of general categories,
and nsc denotes the number of subcategories.
3.5
Training Objective
To jointly learn the subtasks in ASQE and make them mutually ben-
eﬁcial, we design the following loss,
L(θ) = LFT + LST + LS + LGC + LSC.
(7)
For the ﬁrst-turn extraction queries in both two directions, i.e.,
qA
A→O and qO
O→A, we minimize the cross-entropy loss as follows:
LFT = −
|QFT |

i=1
N

j=1
p

ystart
j
| xj, qFT
i
log ˆp

ystart
j
| xj, qFT
i
+p

yend
j
| xj, qFT
i
log ˆp

yend
j
| xj, qFT
i
,
(8)
where p(·) represents the gold distribution, and ˆp(·) denotes the pre-
dicted distribution.
Similarly, the loss of second-turn extraction queries in both two
directions, i.e., qO
A→O and qA
O→A is calculated as follows:
LST = −
|QST |

i=1
N

j=1
p

ystart
j
| xj, qST
i
log ˆp

ystart
j
| xj, qST
i
+p

yend
j
| xj, qST
i
log ˆp

yend
j
| xj, qST
i
.
(9)
For the classiﬁcation queries, including sentiment classiﬁcation,
general category classiﬁcation, and subcategory classiﬁcation, we
minimize the cross-entropy loss functions respectively as follows:
L∗=
−|Q∗|
i=1 p (y∗| X, q∗
i ) log ˆp (y∗| X, q∗
i ) ,
(10)
where ∗represents S, GC and SC, respectively.
Thus, with the total loss L(θ), we can train the EMRC model us-
ing the gradient back-propagation algorithm. Once the training is ﬁn-
ished, we use the model to infer the quadruplets for given sentences.
Table 1.
Statistics on experimental datasets. The preﬁx #- denotes the
number of sentences, quadruplets and categories, respectively.
Statistics
Restaurant-ACOS
Laptop-ACOS
#Sentence
2286
4076
#Quadruplets
3658
5758
General-
6
23
#Category
Sub-
5
9
All
13
121
4
Experiments
4.1
Datasets
Following the previous studies[3, 1], we evaluate our method on two
benchmark datasets: Restaurant-ACOS and Laptop-ACOS [3]. These
two datasets are constructed based on the SemEval 2016 datasets
[23]. Following the experimental setting in Cai et al. [3], we divided
the original datasets into the training, validation, and testing sets.
Statistics of these two ACOS datasets are shown in Table 1.
4.2
Implementation and Evaluation
We use the BERT-base [15] with 12 hidden layers and the hidden size
of 768 as our encoder. AdamW optimizer [17] is used with weight
decay 0.01 and warm-up rate 0.1. The learning rates for training clas-
siﬁers and the ﬁne-tuning rate for BERT are set to 10−3 and 10−5,
respectively. The model is trained in 40 epochs with a batch size of
4 and a dropout rate of 0.1. We use a GeForce RTX 3090 to train
the model for an average of nearly three hours. We save the model
parameters according to the best performance of the model on the
development set. In addition, to ensure the reproducibility, we report
the testing results by averaging over ﬁve runs with different random
seeds. We choose the testing results of the best-performing model on
the development set during each run.
To measure the performance of our model and the baselines, we
adopt Precision, Recall, and F1-score as the evaluation metrics for
the ﬁve tasks, which include ATE, OTE, AOPE, ASC, ACD, and
ASQE. Note that in the evaluation, a quadruplet is considered cor-
rect only if all four elements and their combination match those in
the gold quadruplet [3].
4.3
Baselines
To evaluate our proposed EMRC model, we compare EMRC with
the state-of-the-art baselines. All these models are grouped and
brieﬂy reviewed as follows. 1) Pipeline methods include a) Double-
Propagation (DP) [24] is a rule-based method for aspect-opinion-
sentiment (AOS) triple extraction. For ASQE task, it ﬁrst extracts all
AOS triples by utilizing syntactic relations between aspects and opin-
ions and assigning sentiments using a sentiment lexicon, followed
by assigning aspect categories to each extracted triple. b) Extract-
Classify [3] performs aspect-opinion co-extraction ﬁrst and then pre-
dicts category-sentiment given the extracted aspect-opinion pairs. c)
JET [32] is a leading method for aspect-opinion-sentiment triple ex-
traction, which employs an end-to-end framework incorporating a
position-aware tagging scheme to identify aspects, opinions, and sen-
timent polarities. To tackle ASQE task, JET is modiﬁed to extract
triples ﬁrst, and then predict the corresponding aspect categories for
each triple. 2) End-to-end methods include TAS-BERT [28] adopts
the input transformation strategy to perform category-sentiment con-
ditional aspect-opinion co-extraction and then ﬁlter out the invalid
S. Ye et al. / Enhanced Machine Reading Comprehension Method for Aspect Sentiment Quadruplet Extraction
2878
aspect-opinion pairs to form the ﬁnal ACOS quadruplets. 3) Gen-
erative methods include a) BARTABSA [34] uniﬁes all subtasks
of Aspect-Based Sentiment Analysis (ABSA) by transforming them
into a single generative formulation, where the target of the gener-
ation model is the class index. b) GAS [39] tackles all ABSA tasks
in a uniﬁed generative framework, and formulates ABSA task as a
sentiment element sequence generation problem. c) Paraphrase [38]
aims to jointly detect all sentiment elements in quads. They propose a
paraphrase modeling paradigm to cast the ABSA task to a paraphrase
generation process, and joint extract all the sentiment elements. d)
Opinion tree generation [1] aims to jointly detect all sentiment ele-
ments in a tree for a given review sentence. The opinion tree can be
considered a semantic representation to better represent the structure
of sentiment elements. Compared with these baselines, our EMRC
method is an MRC-based approach.
4.4
Main Results
The experimental results are presented in Table 2. Our results sug-
gest that the uniﬁed and generative approaches are more effective
than the pipeline methods. The former two approaches can better
capture the dependence between subtasks and alleviate the problem
of error propagation by jointly training multiple subtasks. In contrast,
the pipeline methods may lead to severe error accumulation.
Furthermore, the results demonstrate that the proposed model
EMRC outperforms all baselines for overall performance. In par-
ticular, our model achieves the highest precision scores on the two
datasets, with 64.9% on Restaurant-ACOS and 47.27% on Laptop-
ACOS. In terms of recall, our model achieves the second highest
scores with 61.18% on Restaurant-ACOS and 44.66% on Laptop-
ACOS. These are still a signiﬁcant improvement compared to the
other baselines. As for the F1 score, the most critical metric for
evaluating the overall performance, the results show that our model
outperforms all pipeline, uniﬁed, and generative methods, achieving
an average improvement of 16.07% F1 score on Restaurant-ACOS
dataset and 12.56% F1 score on Laptop-ACOS dataset. This indi-
cates that our model is highly effective in extracting quadruplets from
the reviews and outperforms the existing state-of-the-art methods in
ASQE. In addition, we observe that the precision score contributes
most to the boost of F1 score. This indicates that our model features
higher reliability than other baselines. This is a crucial advantage in
real-world applications, as incorrect predictions could have a neg-
ative impact on decision-making processes based on the extracted
information.
Table 2.
Experimental results (%) on two benchmark datasets. All baseline
results are retrieved from the original papers. The ﬁrst four baselines are
pipeline methods. The ﬁfth baseline is an end-to-end method. The last four
methods are generative methods. Our method is an MRC-based approach.
The results in bold indicate the best performance and the underlined results
indicate the second-best performance.
METHOD
Restaurant-ACOS
Laptop-ACOS
P
R
F1
P
R
F1
DP [24]
34.67
15.08
21.04
13.04
0.57
8.00
Extract-Classify [3]
38.54
52.96
44.61
45.56
29.48
35.80
JET [32]
59.81
28.94
39.01
44.52
16.25
23.81
TAS-BERT [28]
26.29
46.29
33.53
47.15
19.22
27.31
BARTABSA [34]
56.62
55.35
55.98
41.65
40.46
41.05
GAS [39]
60.69
58.52
59.59
41.60
42.75
42.17
Paraphrase [38]
58.98
59.11
59.04
41.77
45.04
43.34
Opinion tree generation [1]
63.96
61.74
62.83
46.11
44.79
45.44
Our EMRC
64.97
61.18
63.02
47.27
44.66
45.92
Table 3.
F1 score (%) of ablation studies on hierarchical category
classiﬁcation strategy. ‘QUADRUPLET’ denotes quadruplet extraction.
‘CATEGORY’ denotes category classiﬁcation.
DATASET
METHOD
QUADRUPLET
CATEGORY
Restaurant-ACOS
EMRC
63.02
78.23
w/o hierarchical
61.85 (1.17↓)
70.91 (7.32↓)
Laptop-ACOS
EMRC
45.92
60.58
w/o hierarchical
43.52 (2.40↓)
47.79 (12.79↓)
4.5
Ablation Study
To thoroughly investigate the effectiveness of different modules in
EMRC, we conducted an ablation study on both the Restaurant-
ACOS and Laptop-ACOS datasets. The F1 scores are reported.
Firstly, Table 3 shows the effectiveness of the hierarchical cate-
gory classiﬁcation strategy. It is worth stating that ‘w/o hierarchical’
means the model will predict the category in a single step using the
query ‘What category does the aspect ai belong to?’. Removing the
hierarchical category classiﬁcation query leads to a signiﬁcant de-
cline in performance, especially in the aspect category classiﬁcation.
The performance drop is more apparent in the Laptop-ACOS dataset.
In fact, the Laptop-ACOS dataset features a more complex and di-
verse set of aspect categories compared to the Restaurant-ACOS
dataset. This highlights the crucial role played by the hierarchical
category classiﬁcation strategy in handling the complexity of aspect
categories. Secondly, as shown in Table 4, when we remove the bi-
directional attention mechanism, F1 scores decrease across all sub-
tasks. This suggests that the bi-directional attention mechanism is
vital in capturing the dependencies between the context and queries,
and enhancing the context representation with task-aware informa-
tion. Thus, our model achieves an improvement in the overall per-
formance. Additionally, as shown in Figure 5, the bi-directional ex-
traction queries contribute to the aspect and opinion term extraction,
leading to the better performance of aspect-opinion pair extraction.
The reason is that the two types of queries can fuse information from
both directions, enabling the model to capture the associations be-
tween aspects and opinions.
Figure 5.
Ablation experimental results on the forward and backward
extraction queries with three subtasks, aspect term extraction, opinion term
extraction, and aspect-opinion pair extraction on two datasets.
S. Ye et al. / Enhanced Machine Reading Comprehension Method for Aspect Sentiment Quadruplet Extraction
2879
Table 4.
F1 score (%) of ablation studies on bi-directional attention mechanism. ‘ASQE’, ‘ATE’, ‘OTE’, ‘AOPE’, and ‘ACD’ denote quadruplet extraction,
aspect term extraction, opinion term extraction, aspect-opinion pair extraction, and aspect category detection, respectively.
DATASET
METHOD
SUBTASK
ASQE
ATE
OTE
AOPE
ACD
Restaurant-ACOS
EMRC
63.02
83.98
85.74
75.61
78.23
w/o bi-directional attention
60.92 (2.1↓)
82.13 (1.85↓)
84.01 (1.73↓)
72.59 (3.02↓)
76.68 (1.55↓)
Laptop-ACOS
EMRC
45.92
82.95
89.79
77.32
60.58
w/o bi-directional attention
42.46 (3.46↓)
81.52 (1.43↓)
88.31 (1.48↓)
75.26 (2.06↓)
59.84 (0.94↓)
Query
Sentence
what
opinion
given
the
aspect
service
?
[SEP]
service
the
was
the
food
excellent
and
was
delicious
[SEP]
Query
Sentence
what
opinion
given
the
aspect
service
?
[SEP]
service
the
was
the
food
excellent
and
was
delicious
[SEP]
(a) Task-aware
(b) Original
Figure 6.
The visualization of attention distribution between the query and
the different context representations.
4.6
Attention Visualization
To show the effectiveness of the bi-directional attention mechanism
in EMRC, we visualize the attention distribution between the query
and context. The visualization results are shown in Figure 6. Given
the query ‘what opinion given the aspect service?’, and a review
sentence ‘the service was excellent and the food was delicious’ as
the context, Figure 6 (a) denotes the attention distribution under the
task-aware context representation. And Figure 6 (b) is the visualiza-
tion of attention distribution under the original context representation
for the identical query. We observe that the task-aware context rep-
resentation can gain more attention on the query-related word, i.e.,
‘excellent’. In addition, the attention seems to be scattered under the
original representation. The two words ‘excellent’ and ‘delicious’ are
almost attended. Therefore, with the task-aware representation, our
model will perform a more reliable prediction.
4.7
Case Study
A case study is given in Figure 7, where we select two error cases to
better understand in which cases the methods would fail. In Example
1, given the review sentence ‘Great service with amazon on fulﬁlling
my order.’, Extract-Classify classiﬁes the aspect service into a wrong
category LAPTOP#OPERATION_PERFORMANCE, whereas SUP-
PORT#GENERAL is correctly predicted by our EMRC model. As
we have previously discussed in the ablation study, the hierarchical
category classiﬁcation strategy plays an essential role in handling the
complexity of aspect categories. However, both two models fail to
extract the aspect term service with amazon. It seems that the model
often struggles to detect the exact text spans as the ground truths. In
Example 2, Paraphrase predicts the opinion term best which is not
present in the original sentence ‘The pizza is delicious and the pro-
prietor is one of the nicest in NYC.’, because the generative model
tends to perform ‘generation’ instead of ‘extraction’. In contrast, our
Great service with amazon on fulfilling my order.
Great service with amazon on fulfilling my order.
+
SUPPORT#GENERAL
+
LAPTOP#OPERATION_PERFORMANCE
Great service with amazon on fulfilling my order.
+
SUPPORT#GENERAL
Gold
Extract-
Classify
EMRC
Example 1:
Example 2:
Gold
Paraphrase
EMRC
The pizza is delicious and the proprietor is one of the nicest in NYC .
+
+
FOOD#QUALTY
SERVICE#GENERAL
The pizza is delicious and the proprietor is one of the best in NYC .
+
+
FOOD#QUALTY
SERVICE#GENERAL
The pizza is delicious and the proprietor is one of the nicest in NYC .
+
+
FOOD#QUALTY
SERVICE#GENERAL
Figure 7.
Case study on results produced by different models, including
Extract-Classify (a pipeline method), Paraphrase (a generative method), and
our EMRC. The aspect and opinion terms are highlighted in yellow and
green, respectively. Each bracket indicates the pairing relation between
aspect and opinion terms. Above the bracket, the sentiment polarity (‘+’ or
‘-’) of each pair is shown. The sentiment category of each aspect is given
under the aspect term. Words framed in red indicate wrong predictions.
model extracts the opinion term nicest from the given sentence to
avoid such generation errors.
5
Conclusion and Future Work
In this paper, we propose a novel EMRC model that formulates
ASQE task as a multi-turn machine reading comprehension task.
Speciﬁcally, the model enhances the dependencies and facilitates the
ﬂow of information among multiple subtasks by introducing the an-
swer of the last turn into the current query. Moreover, a hierarchical
category classiﬁcation strategy is used to handle complex category
situations. The model also utilizes the bi-directional attention mech-
anism, with both context-to-query and query-to-context attentions,
to enhance the context representation by fusing the information from
the context and queries. Extensive experimental results demonstrate
that EMRC outperforms state-of-the-art baselines.
In the future, we will consider enhancing our EMRC method with
span-level representation to improve the performance of predicting
aspect or opinion terms that contain multiple words. In addition, we
will explore a novel query-construction strategy that automatically
constructs queries based on the given context.
S. Ye et al. / Enhanced Machine Reading Comprehension Method for Aspect Sentiment Quadruplet Extraction
2880
Acknowledgements
This work was supported in part by the National Natural Science
Foundation of China under Grant 62076032 and in part by the 111
Project under Grant B08004. We appreciate constructive feedback
from the anonymous reviewers for improving the ﬁnal version of this
conference paper.
References
[1]
Xiaoyi Bao, Zhongqing Wang, Xiaotong Jiang, Rong Xiao, and
Shoushan Li, ‘Aspect-based sentiment analysis with opinion tree gen-
eration’, in Proc. IJCAI, pp. 4044–4050, (2022).
[2]
Jeremy Barnes, Robin Kurtz, Stephan Oepen, Lilja Øvrelid, and Erik
Velldal, ‘Structured sentiment analysis as dependency graph parsing’,
in Proc. ACL-IJCNLP, pp. 3387–3402, (2021).
[3]
Hongjie Cai, Rui Xia, and Jianfei Yu, ‘Aspect-category-opinion-
sentiment quadruple extraction with implicit aspects and opinions’, in
Proc. ACL-IJCNLP, pp. 340–350, (2021).
[4]
Chenhua Chen, Zhiyang Teng, Zhongqing Wang, and Yue Zhang, ‘Dis-
crete opinion tree induction for aspect-based sentiment analysis’, in
Proc. ACL, pp. 2051–2064, (2022).
[5]
Hao Chen, Zepeng Zhai, Fangxiang Feng, Ruifan Li, and Xiaojie Wang,
‘Enhanced multi-channel graph convolutional network for aspect senti-
ment triplet extraction’, in Proc. ACL, pp. 2974–2985, (2022).
[6]
Shaowei Chen, Jie Liu, Yu Wang, Wenzheng Zhang, and Ziming Chi,
‘Synchronous double-channel recurrent network for aspect-opinion
pair extraction’, in Proc. ACL, pp. 6515–6524, (2020).
[7]
Shaowei Chen, Yu Wang, Jie Liu, and Yuelin Wang, ‘Bidirectional ma-
chine reading comprehension for aspect sentiment triplet extraction’, in
Proc. AAAI, volume 35, pp. 12666–12674, (2021).
[8]
Hongliang Dai and Yangqiu Song, ‘Neural aspect and opinion term ex-
traction with mined rules as weak supervision’, in Proc. ACL, pp. 5268–
5277, (2019).
[9]
Junqi Dai, Hang Yan, Tianxiang Sun, Pengfei Liu, and Xipeng Qiu,
‘Does syntax matter? a strong baseline for aspect-based sentiment anal-
ysis with RoBERTa’, in Proc. NAACL, pp. 1816–1829, (2021).
[10]
Zhifang Fan, Zhen Wu, Xinyu Dai, Shujian Huang, and Jiajun Chen,
‘Target-oriented opinion words extraction with target-fused neural se-
quence labeling’, in Proc. NAACL, pp. 2509–2518, (2019).
[11]
Lei Gao, Yulong Wang, honglei Liu, Jingyu Wang, Lei Zhang, and
Jianxin Liao, ‘Question-driven span labeling model for aspect–opinion
pair extraction’, in Proc. AAAI, volume 35, pp. 12875–12883, (2021).
[12]
Zhibin Gou, Qingyan Guo, and Yujiu Yang, ‘MvP: Multi-view prompt-
ing improves aspect sentiment tuple prediction’, in Proc. ACL, pp.
4380–4397, (2023).
[13]
Mengting Hu, Shiwan Zhao, Honglei Guo, Chao Xue, Hang Gao, Tie-
gang Gao, Renhong Cheng, and Zhong Su, ‘Multi-label few-shot learn-
ing for aspect category detection’, in Proc. ACL-IJCNLP, pp. 6330–
6340, (2021).
[14]
Lianzhe Huang, Xin Sun, Sujian Li, Linhao Zhang, and Houfeng Wang,
‘Syntax-aware graph attention network for aspect-level sentiment clas-
siﬁcation’, in Proc. COLING, pp. 799–810, (2020).
[15]
Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova,
‘Bert: Pre-training of deep bidirectional transformers for language un-
derstanding’, in Proc. NAACL-HLT, pp. 4171–4186, (2019).
[16]
Ruifan Li, Hao Chen, Fangxiang Feng, Zhanyu Ma, Xiaojie Wang,
and Eduard Hovy, ‘Dual graph convolutional networks for aspect-based
sentiment analysis’, in Proc. ACL-IJCNLP, pp. 6319–6329, (2021).
[17]
Ilya Loshchilov and Frank Hutter, ‘Fixing weight decay regularization
in adam’, in Proc. ICLR, (2018).
[18]
Dehong Ma, Sujian Li, Fangzhao Wu, Xing Xie, and Houfeng Wang,
‘Exploring sequence-to-sequence learning in aspect term extraction’, in
Proc. ACL, pp. 3538–3547, (2019).
[19]
Dehong Ma, Sujian Li, Xiaodong Zhang, and Houfeng Wang, ‘Inter-
active attention networks for aspect-level sentiment classiﬁcation’, in
Proc. IJCAI, pp. 4068–4074, (2017).
[20]
Fukun Ma, Xuming Hu, Aiwei Liu, Yawen Yang, Shuang Li, Philip S.
Yu, and Lijie Wen, ‘AMR-based network for aspect-based sentiment
analysis’, in Proc. ACL, pp. 322–337, (2023).
[21]
Yue Mao, Yi Shen, Chao Yu, and Longjun Cai, ‘A joint training dual-
mrc framework for aspect based sentiment analysis’, in Proc. AAAI,
volume 35, pp. 13543–13551, (2021).
[22]
Haiyun Peng, Lu Xu, Lidong Bing, Fei Huang, Wei Lu, and Luo Si,
‘Knowing what, how and why: A near complete solution for aspect-
based sentiment analysis’, in Proc. AAAI, volume 34, pp. 8600–8607,
(2020).
[23]
Maria Pontiki, Dimitris Galanis, Haris Papageorgiou, Ion Androut-
sopoulos, Suresh Manandhar, Mohammad AL-Smadi, Mahmoud Al-
Ayyoub, Yanyan Zhao, Bing Qin, Orphée De Clercq, Véronique Hoste,
Marianna Apidianaki, Xavier Tannier, Natalia Loukachevitch, Evgeniy
Kotelnikov, Nuria Bel, Salud María Jiménez-Zafra, and Gül¸sen Ery-
i˘git, ‘SemEval-2016 task 5: Aspect based sentiment analysis’, in Pro-
ceedings of the 10th International Workshop on Semantic Evaluation
(SemEval-2016), pp. 19–30, (2016).
[24]
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen, ‘Opinion word expan-
sion and target extraction through double propagation’, Computational
linguistics, 37(1), 9–27, (2011).
[25]
Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Ha-
jishirzi, ‘Bidirectional attention ﬂow for machine comprehension’, in
Proc. ICLR, (2019).
[26]
Kai Sun, Richong Zhang, Samuel Mensah, Yongyi Mao, and Xudong
Liu, ‘Aspect-level sentiment analysis via convolution over dependency
tree’, in Proc. EMNLP-IJCNLP, pp. 5679–5688, (2019).
[27]
Chang-Yu Tai, Ming-Yao Li, and Lun-Wei Ku, ‘Hyperbolic disentan-
gled representation for ﬁne-grained aspect extraction’, in Proc. AAAI,
volume 36, pp. 11358–11366, (2022).
[28]
Hai Wan, Yufei Yang, Jianfeng Du, Yanan Liu, Kunxun Qi, and Jeff Z
Pan, ‘Target-aspect-sentiment joint detection for aspect-based senti-
ment analysis’, in Proc. AAAI, volume 34, pp. 9122–9129, (2020).
[29]
Wenya Wang, Sinno Jialin Pan, Daniel Dahlmeier, and Xiaokui Xiao,
‘Recursive neural conditional random ﬁelds for aspect-based sentiment
analysis’, in Proc. EMNLP, pp. 616–626, (2016).
[30]
Meixi Wu, Wenya Wang, and Sinno Jialin Pan, ‘Deep weighted maxsat
for aspect-based opinion extraction’, in Proc. EMNLP, pp. 5618–5628,
(2020).
[31]
Lu Xu, Yew Ken Chia, and Lidong Bing, ‘Learning span-level interac-
tions for aspect sentiment triplet extraction’, in Proc. ACL-IJCNLP, pp.
4755–4766, (2021).
[32]
Lu Xu, Hao Li, Wei Lu, and Lidong Bing, ‘Position-aware tagging for
aspect sentiment triplet extraction’, in Proc. EMNLP, pp. 2339–2349,
(2020).
[33]
Ting Xu, Huiyun Yang, Zhen Wu, Jiaze Chen, Fei Zhao, and Xinyu
Dai, ‘Measuring your ASTE models in the wild: A diversiﬁed multi-
domain dataset for aspect sentiment triplet extraction’, in Findings of
ACL: ACL, pp. 2837–2853, (2023).
[34]
Hang Yan, Junqi Dai, Tuo Ji, Xipeng Qiu, and Zheng Zhang, ‘A uniﬁed
generative framework for aspect-based sentiment analysis’, in Proc.
ACL-IJCNLP, pp. 2416–2429, (2021).
[35]
Zepeng Zhai, Hao Chen, Fangxiang Feng, Ruifan Li, and Xiao-
jie Wang, ‘COM-MRC: A context-masked machine reading compre-
hension framework for aspect sentiment triplet extraction’, in Proc.
EMNLP, pp. 3230–3241, (2022).
[36]
Zepeng Zhai, Hao Chen, Ruifan Li, and Xiaojie Wang, ‘USSA: A uni-
ﬁed table ﬁlling scheme for structured sentiment analysis’, in Proc.
ACL, pp. 14340–14353, (2023).
[37]
Mao Zhang, Yongxin Zhu, Zhen Liu, Zhimin Bao, Yunfei Wu, Xing
Sun, and Linli Xu, ‘Span-level aspect-based sentiment analysis via table
ﬁlling’, in Proc. ACL, pp. 9273–9284, (2023).
[38]
Wenxuan Zhang, Yang Deng, Xin Li, Yifei Yuan, Lidong Bing, and Wai
Lam, ‘Aspect sentiment quad prediction as paraphrase generation’, in
Proc. EMNLP, pp. 9209–9219, (2021).
[39]
Wenxuan Zhang, Xin Li, Yang Deng, Lidong Bing, and Wai Lam,
‘Towards generative aspect-based sentiment analysis’, in Proc. ACL-
IJCNLP, pp. 504–510, (2021).
[40]
He Zhao, Longtao Huang, Rong Zhang, Quan Lu, and Hui Xue, ‘Span-
mlt: A span-based multi-task learning framework for pair-wise aspect
and opinion terms extraction’, in Proc. ACL, pp. 3239–3248, (2020).
[41]
Xinjie Zhou, Xiaojun Wan, and Jianguo Xiao, ‘Representation learning
for aspect category detection in online reviews’, in Proc. AAAI, vol-
ume 29, (2015).
S. Ye et al. / Enhanced Machine Reading Comprehension Method for Aspect Sentiment Quadruplet Extraction
2881
