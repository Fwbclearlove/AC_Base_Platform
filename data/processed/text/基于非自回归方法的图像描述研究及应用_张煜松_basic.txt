密级 ： 公开  

避耆部ｆＡｆ  

硕士学位论文   纖  

题目 ： 基于非自回归方法的图像描述研究及应用  

学号 ：２０２１１４１０８４  

姓名 ：张煜松  

学科专业 ：计算机技术  

培养方式 ：全日制  

导师 ：李睿凡  

学院 ：计算机学院  

２０２４年０５月 ２８日  

■北京  

密级 ： 公开  

分名却索大聲  

硕士学位论文 （专业学位）   ？  

题目 ： 基于非自回归方法的图像描述研究及应用  

学 号 ：２０２１１４１０８４  

姓 名 ：张煜松  

学科专业 ：计算机技术  

培养方式 ：全日制  

导 师 ：李睿凡  

学 院 ：人工智能学院  

２０２４年 ５月 ２８日  

ＢＥＩＪＩＮＧＵＮＩＶＥＲＳＩＴＹＯＦ  

ＰＯＳＴＳＡＮＤ  

ＴＥＬＥＣＯＭＭＵＮＩＣＡＴＩＯＮＳ  

ＭａｓｔｅｒＴｈｅｓｉｓ  

ＲｅｓｅａｒｃｈａｎｄＡｐｐｌｉｃａｔｉｏｎｏｆＮｏｎ －Ａｕｔｏ ｒｅｇｒｅｓｓｉｖｅ  

ＩｍａｇｅＣａｐｔｉｏｎｉｎｇＭｅｔｈｏｄｓ  

ＳｔｕｄｅｎｔＩＤ ：２０２１１４１０８４  

Ａｕｔｈｏｒ ：ＹｕｓｏｎｇＺｈａｎｇ  

Ｓｕｂ ｊｅｃｔ ：ＣｏｍｐｕｔｅｒＴｅｃｈｎｏｌｏｇｙ  

Ｓｕｐｅｒｖｉｓｏｒ ：ＲｕｉｆａｎＬｉ  

Ｉｎｓｔｉｔｕｔｅ ：ＳｃｈｏｏｌｏｆＡｒｔｉｆ ｉｃｉａｌＩｎｔｅｌｌｉｇｅｎｃｅ  

Ｄａｔｅ２８Ｍｏｎｔｈ５Ｙｅａｒ２０２４  

答辩委员会名单   

职务姓名职称工 作 单 位  

主席徐其志教授北京理工大学  

委员陈孝良研宄员北京声智科技有限公司  

委员阎岩研宄员航天科技集团创新研宄院  

委员赵鑫教授北京科技大学  

委员王云龙副教授中国科学院自动化研宄所  

秘书刘咏彬讲师北京邮电大学  

答辩日期２０２４年 ５月 ２８日  

ｍｍ  

摘要  

图像描述 （ＩｍａｇｅＣａｐｔ ｉｏｎｉｎｇ ） 旨在为给定的图片生成人类可理  

解的描述性文本 ， 具有丰富的应用价值 。 目前最先进的图像描述模  

型大多基于自回归的文本生成方法 ， 每次推理基于先前生成的单词  

一个单词 ， 在效率上存在不足 。 近年来

一些非自回归图像描  

述工作试图改善图像描述的效率 ， 但是在效果上相比现有的自回归  

基线仍存在差距 。 本文聚焦于非自回归的图像描述以提升图像描述  

的效率和效果 ， 并在此基础上进

一步研究了具有现实意义的可控图  

像描述任务 。  

本文的主要研宄工作如下 ：  

１ ）为了提升非自回归图像描述方法的效率和效果 ， 本文提出了  

一种结合人类编辑操作的非自回归图像描述方法 。 包括

一编  

辑器和适用于编辑操作的两阶段图像描述训练策略 。 统

一编辑器通  

一种创新的位置预测操作同时执行多种编辑任务 ， 提高了基于编  

辑操作生成图像描述的效率 。 两阶段训练策略优化了图像描述方法  

中传统的交叉熵和强化学习训练阶段以更好地适应编辑操作 。 实验  

结果表明本文提出的方法取得了先进的速度

－性能平衡 ， 验证了方法  

的有效性 。  

２）在可控图像描述任务中 ， 现有的大部分方法都局限于初步地  

引入控制信号 。 对可控图像描述的推理速度和数据集的关注相对较  

少 。 首先 ， 本文基于非自回归图像描述方法引入控制信号 ， 提出了  

一种高速的可控非自回归图像描述方法 。 其次 ， 本文提出了

一种基  

于大语言模型的风格化数据生成方法 ， 可以缓解目前工作中存在的  

数据集限制 。 实验结果表明本文提出的可控非自回归图像描述方法  

可以高效地完成可控图像描述任务 ， 验证了方法的有效性 。  

３ ）基于对非自回归图像描述的研宄 ， 本文设计并实现了

一个非  

自回归图像描述系统 ， 该系统支持用户的注册和登录以及图像描述  

的在线生成 ， 有助于图像描述的社区传播 。  

关键词 ： 深度学习 ； 图像描述 ； 非自回归方法 ； 注意力机制 ；  

可控生成  

北京邮电大学硕士学位论文  

ＡＢＳＴＲＡＣＴ  

Ｔｈｅｍａｉｎｔａｓｋｏｆｉｍａｇｅｃａｐｔｉｏｎｉｎｇ ｉｓｔｏｇｅｎｅｒａｔｅｄｅｓｃｒ ｉｐ ｔｉｖｅｔｅｘｔｆｏｒ  

ｔｈｅｇ ｉｖｅｎｉｍａｇｅｓ ，ｗｈｉｃｈｈａｓｒｉｃｈ ｐｒａｃｔｉｃａｌｖａｌｕｅ ．Ｃｕｎ

＊ ｅｎｔｌｙ，ｍｏｓｔｓｔａｔｅ

－ａｒｔｉｍａｇｅｃａｐ ｔｉｏｎｉｎｇｍｏｄｅｌｓａｒｅｂａｓｅｄｏｎａｕｔｏｒｅｇｒｅｓｓｉｖｅｔｅｘｔ  

ｇｅｎｅｒａｔｉｏｎｍｅｔｈｏｄｓ ，ｗｈｅｒｅｅａｃｈｓｔｅｐｇｅｎｅｒａｔｅｓｔｈｅｎｅｘｔｗｏｒｄｂａｓｅｄｏｎ  

ｐｒｅｖｉｏｕｓｌｙｇｅｎｅｒａｔｅｄｗｏｒｄｓ ．Ｓｕｃｈｇｅｎｅｒａｔｉｏｎｍｅｔｈｏｄｓｒｅｓｕｌｔｉｎｓｏｍｅ  

ｅｆｆｉｃｉｅｎｃｙｓｈｏｒｔｃｏｍｉｎｇｓ ．Ｉｎｒｅｓｐｏｎｓｅｔｏｔｈｅｌａｔｅｎｃｙｒｅｑｕｉｒｅｍｅｎｔｓｏｆｒｅａｌ

ｗｏｒｌｄａｐｐ ｌｉｃａｔｉｏｎｓ ，ｓｏｍｅｎｏｎ

－ａｕｔｏｒｅｇｒｅｓｓｉｖｅｉｍａｇｅｃａｐ ｔｉｏｎｉｎｇｍｅｔｈｏｄｓ  

ａｒｅｐｒｏｐｏｓｅｄｉｎｒｅｃｅｎｔｙｅａｒｓ ．Ｈｏｗｅｖｅｒ ，ｔｈｅｓｅｍｅｔｈｏｄｓｓｔｉｌｌｌａｇｂｅｈｉｎｄ  

ｅｘｉｓｔｉｎｇａｕｔｏｒｅｇｒｅｓｓｉｖｅｂａｓｅｌｉｎｅｓｉｎｔｅｒｍｓｏｆｅｆｆｉｃｉｅｎｃｙａｎｄｅｆｆｅｃｔｉｖｅｎｅｓｓ ．  

Ｔｈｉｓｔｈｅｓｉｓｆｏｃｕｓｅｓｏｎｒｅｓｅａｒｃｈｉｎｇｎｏｎ

－ａｕｔｏｒｅｇｒｅｓｓｉｖｅｉｍａｇｅｃａｐ ｔｉｏｎｉｎｇ  

ｍｅｔｈｏｄｓｔｏｉｍｐｒｏｖｅｔｈｅｏｖｅｒａｌｌｅｆｆｉｃｉｅｎｃｙ

．Ｔｈｅｍａｉｎｃｏｎｔｒｉｂｕｔｉｏｎｓｏｆｔｈｉｓ  

ｔｈｅｓｉｓａｒｅａｓｆｏｌｌｏｗｓ ：  

１ ） Ｉｎｏｒｄｅｒｔｏｆｕｒｔｈｅｒｉｍｐｒｏｖｅｔｈｅｅｆｆｉｃｉｅｎｃｙａｎｄｅｆｆｅｃｔｉｖｅｎｅｓｓｏｆ  

－ａｕｔｏｒｅｇｒｅｓｓｉｖｅｉｍａｇｅｃａｐ ｔｉｏｎｉｎｇｍｅｔｈｏｄｓ

，ｔｈｉｓｔｈｅｓｉｓｐｒｏｐｏｓｅｓａｎｏｎ

ａｕｔｏｒｅｇｒｅｓｓｉｖｅｉｍａｇｅｃａｐ ｔｉｏｎｉｎｇｍｅｔｈｏｄ ，ｗｈｉｃｈｉｎｔｅｇｒａｔｅｓｈｕｍａｎｅｄｉｔｉｎｇ  

ｏｐｅｒａｔｉｏｎｓ ．Ｔｈｅｍａｉｎｉｎｎｏｖａｔｉｏｎｏｆｏｕｒｍｅｔｈｏｄｌｉｅｓｉｎａｕｎｉｆｉｅｄｅｄｉｔｏｒａｎｄ  

－ｓｔａｇｅｔｒａｉｎｉｎｇｓｔｒａｔｅｇｙ

．Ｔｈｅｕｎｉｆｉｅｄｅｄｉｔｏｒ ，ｔｈｒｏｕｇｈａｎｉｎｎｏｖａｔｉｖｅ  

ｐｏｓｉｔｉｏｎ ｐｒｅｄｉｃｔｉｏｎｏｐｅｒａｔｉｏｎ ，ｃａｎｓｉｍｕｌｔａｎｅｏｕｓｌｙｐｅｒｆｏｒｍｖａｒｉｏｕｓｅｄｉｔｉｎｇ  

ｔａｓｋｓ ，ｔｈｕｓｉｍｐｒｏｖｉｎｇｇｅｎｅｒａｔｉｏｎｓｐｅｅｄ ．Ｔｈｅｔｗｏ

－ｓｔａｇｅｔｒａｉｎｉｎｇｓｔｒａｔｅｇｙ  

ｏｐ ｔｉｍｉｚｅｓｔｒａｄｉｔｉｏｎａｌｃｒｏｓｓ

－ｅｎｔｒｏｐｙａｎｄｒｅｉｎｆｏｒｃｅｍｅｎｔｌｅａｒｎｉｎｇｓｔａｇｅｓｉｎ  

ｉｍａｇｅｃａｐ ｔｉｏｎｉｎｇｍｅｔｈｏｄｓｔｏｂｅｔｔｅｒａｄａｐ ｔｔｏｅｄｉｔｉｎｇｏｐｅｒａｔｉｏｎｓａｎｄ  

ｉｍｐｒｏｖｅｔｒａｉｎｉｎｇｅｆｆｅｃｔｉｖｅｎｅｓｓ ．Ｅｘｐｅｒｉｍｅｎｔａｌｒｅｓｕｌｔｓｓｈｏｗｔｈａｔｏｕｒ  

ｍｅｔｈｏｄａｃｈｉｅｖｅｓａｎａｄｖａｎｃｅｄｔｒａｄｅ

－ｏｆｆｂｅｔｗｅｅｎｅｆｆｉｃｉｅｎｃｙａｎｄ  

ｅｆｆｅｃｔｉｖｅｎｅｓｓ ，ｖａｌｉｄａｔｉｎｇ ｔｈｅｍｅｔｈｏｄ

＇ ｓｅｆｆｅｃｔｉｖｅｎｅｓｓ ．  

２） Ｉｎｔｈｅｒｅｓｅａｒｃｈｏｎｉｎｔｒｏｄｕｃｉｎｇｃｏｎｔｒｏｌｌａｂｉｌｉｔｙ ｉｎｔｏｉｍａｇｅ  

ｃａｐ ｔｉｏｎｉｎｇｍｏｄｅｌｓ ，ｍｏｓｔｅｘｉｓｔｉｎｇｍｅｔｈｏｄｓｆｏｃｕｓｏｎｉｎｉｔｉａｌｌｙ ｉｎｔｒｏｄｕｃｉｎｇ  

ｃｏｎｔｒｏｌｓｉｇｎａｌｓｔｈｒｏｕｇｈｖａｒｉｏｕｓｍｅｃｈａｎｉｓｍｓ ，ｗｉｔｈｒｅｌａｔｉｖｅｌｙ ｌｅｓｓａｔｔｅｎｔｉｏｎ  

ｔｏｔｈｅｉｎｆｅｒｅｎｃｅｓｐｅｅｄｏｆｃｏｎｔｒｏｌｌａｂｌｅｉｍａｇｅｃａｐ ｔｉｏｎｉｎｇａｎｄｔｒａｉｎｉｎｇｄａｔａ ．  

Ｆｉｒｓｔｌｙ，ｂａｓｅｄｏｎｔｈｅｐｒｏｐｏｓｅｄｎｏｎ

－ａｕｔｏｒｅｇｒｅｓｓｉｖｅｍｅｔｈｏｄ ，ｔｈｉｓｔｈｅｓｉｓ  

ｉｎｔｒｏｄｕｃｅｓａｃｏｎｔｒｏｌｓｉｇｎａｌｍｅｃｈａｎｉｓｍａｎｄｐｒｏｐｏｓｅｓａｃｏｎｔｒｏｌｌａｂｌｅｎｏｎ

ａｕｔｏｒｅｇｒｅｓｓｉｖｅｉｍａｇｅｃａｐ ｔｉｏｎｉｎｇｍｅｔｈｏｄ ．Ｓｅｃｏｎｄｌｙ，ｔｈｉｓｔｈｅｓｉｓｐｒｏｐｏｓｅｓａ  

Ｉ Ｉ  

ｍｍ  

ｓｔｙ ｌｅ －ｂａｓｅｄｄａｔａｇｅｎｅｒａｔｉｏｎｍｅｔｈｏｄｂａｓｅｄｏｎｌａｒｇｅｌａｎｇｕａｇｅｍｏｄｅｌｓ ，  

ｗｈｉｃｈｃａｎａｌｌｅｖｉａｔｅｔｈｅｌｉｍｉｔａｔｉｏｎｓｏｆｃｕｒｒｅｎｔｃｏｎｔｒｏｌｌａｂｌｅｉｍａｇｅ  

ｃａｐｔｉｏｎｉｎｇｗｏｒｋｒｅｌｙ ｉｎｇｏｎｄｉｓｃｒｉｍｉｎａｔｉｖｅｌｙｏｂｔａｉｎｉｎｇｓｔｙ ｌｉｚｅｄｄａｔａｆ ｒｏｍ  

ｅｘｉｓｔｉｎｇｄａｔａｓｅｔｓ ．Ｅｘｐｅｒｉｍｅｎｔａｌｒｅｓｕｌｔｓｓｈｏｗｔｈａｔｏｕｒｍｅｔｈｏｄｃａｎ  

ｅｆｆｉｃｉｅｎｔｌｙａｃｃｏｍｐ ｌｉｓｈｃｏｎｔｒｏｌｌａｂｌｅｉｍａｇｅｃａｐｔｉｏｎｉｎｇ ｔａｓｋｓ ，ｖａｌｉｄａｔｉｎｇｔｈｅ  

ｍｅｔｈｏｄ

＇ ｓｅｆｆｅｃｔｉｖｅｎｅｓｓ ．  

３ ）Ｂａｓｅｄｏｎｔｈｅｒｅｓｅａｒｃｈｏｎｔｈｅｔｗｏｍｅｔｈｏｄｓ ，ｔｈｉｓｔｈｅｓｉｓｄｅｓｉｇｎｓａｎｄ  

ｉｍｐ ｌｅｍｅｎｔｓａｎｏｎ －ａｕｔｏｒｅｇｒｅｓｓｉｖｅｉｍａｇｅｃａｐｔｉｏｎｉｎｇｓｙｓｔｅｍｔｈａｔｓｕｐｐｏｒｔｓ  

ｕｓｅｒｒｅｇ ｉｓｔｒａｔｉｏｎａｎｄｌｏｇ ｉｎａｓｗｅｌｌａｓｏｎｌｉｎｅｇｅｎｅｒａｔｉｏｎｏｆｃａｐｔｉｏｎｓ ，  

ｃｏｎｔｒｉｂｕｔｉｎｇ ｔｏｔｈｅｃｏｍｍｕｎｉｔｙｄｉｓｓｅｍｉｎａｔｉｏｎｏｆｉｍａｇｅｃａｐ ｔｉｏｎｉｎｇ

ＫＥＹＷＯＲＤＳ ：ｄｅｅｐ ｌｅａｒｎｉｎｇ；ｉｍａｇｅｃａｐｔｉｏｎｉｎｇ；ｎｏｎ －  

ａｕｔｏｒｅｇｒｅｓｓｉｖｅｍｅｔｈｏｄｓ ；ａｔｔｅｎｔｉｏｎｍｅｃｈａｎｉｓｍ ；ｃｏｎｔｒｏｌｌａｂｌｅ ｇｅｎｅｒａｔｉｏｎ  

ｈｉ  

目 录  

一章绪论 １  

１ ．１研宄背景及其意义 １  

１ ．２ 国内外研宄现状 ３  

１ ．２ ．１ 自回归图像描述 ３  

１ ．２ ．２非自回归图像描述 ６  

１ ．２ ．３可控图像描述 ８  

１ ．３本文研宄内容与主要工作 ９  

１ ．３ ．１基于编辑的非自回归图像描述方法 ９  

１ ．３ ．２可控非自回归图像描述 １２  

１ ．４本文的组织结构 １４  

１ ，５本章小结 １５  

第二章相关技术 １７  

２ ． １深度学习技术 １７  

２ ． １ ．１Ｔｒａｎｓｆｏｒｍｅｒ技术 １７  

２ ． １ ．２编解码器结构     １８  

２ ．２视觉特征编码 １９  

２ ．２ ．１全局视觉特征 １９  

２ ．２ ．２ 目标检测视觉特征 ２１  

２ ．３ 图像描述评估技术 ２３  

２ ．４本章小结 ２４  

第三章基于编辑的非自回归图像描述方法 ２５  

３ ． １鍵 ２５  

３ ．２ＵｎｉＣａｐ模型框架 ２５  

３ ．２ ．１框架总览 ２５  

３ ．２ ．２跨模态特征抽取器 ２６  

３ ．２ ．３统

一编辑模块 ２７  

３ ．２ ．４两阶段训练方法 ３２  

３ ．３实验 ３７  

３ ．３ ．１ 数据集介绍 ３７  

３ ．３ ．２实验配置 ３８  

３ ．３ ．３基线方法 ３８  

３ ．３ ．４实验结果 ４１  

３ ．３ ．５消融分析 ４３  

３ ．３ ．６案例分析… ４４  

３ ．４本章小结  ４５  

第四章基于视觉提示词的可控非自回归图像描述４７  

４ ． １擁 ４７  

４ ．２模型框架 ４８  

４ ．２ ．１ 图像编码层 ４８  

４ ．２ ．２提示词机制 ４８  

４ ．２ ．３数据风格判别器 ４９  

４ ．２ ．４ｎｊ控图像描述数据生成策略 ５１  

４ ，３实验对比和分析 ５５  

４ ．３ ．１ 实验配置 ５５  

４ ．３ ．２基线方法 ５６  

４ ．３ ．３实验结果 ５６  

４ ．３ ．４实验分析 ５７  

４ ．３ ．５ 案例分析 ５８  

４ ．４本章小结 ５９  

第五章 非自回归图像描述系统 ６１  

５ ． １ 系统需求分析 ６１  

５ ，２系统整体设计 ６１  

５ ．３前端设计 ６２  

５ ．４后端设计 ６４  

５ ．５系统工作流｜呈 ６４  

第六章 总结与展望 ６７  

６ ． １ 工作总结 ６７  

６ ．２未来工作展望 ６８    ６９  

一章绪论  

第 一章绪论  

１ ． １研究背景及其意义  

随着人工智能的飞速发展 ， 多模态理解与生成己经在多个领域落地并取得  

了巨大的成功 。 图像描述 （ ＩｍａｇｅＣａｐ ｔｉｏｎｉｎｇ ）是多模态生成重要的子任务之

其主要任务是基于情景图片生成人类可理解的描述性文本 。 图像描述要求算法  

对图片具有从概括到细节的多层级理解 ， 同时又要求良好的文本生成能力 ， 是  

多模态生成模型的基本任务 。 图像描述在现实中受到了广泛的研宄与应用 。 可  

以赋予人工智能的图文理解能力以支持多种多样的任务

［ １＜ 。 例如 ， 在搜索技术  

中可以使用图像描述技术为图像生成描述性语句 ， 用户能通过这些语句检索到  

相应的图片 ， 从而实现图文互搜的功能 ； 在无障碍领域 ， 图像描述技术可以帮  

助视力障碍人士通过生成的文本感知图片 。  

：｜ 编码器 —＼ 解码器   Ｅｎｃｏｄｅｒ ＾Ｄｅｃｏｄｅｒ  

语义表示  

ａｗｏｍａｎｉｎｔｈｅ ｙｅ ｌ ｌｏｗｔｒｕｃｋ  

一个女人在黄色卡车里）  

－ ｉ主流基于编解码器架构的图像描述算法示例  

如图 ｉ －

ｉ 所示 ， 现代基于深度学习的图像描述算法将图像描述任务视为

个序列到序列 （Ｓｅｑ２Ｓｅｑ ） 任务 ， 目前流行的图像描述算法均基于编解码器  

（Ｅｎｃｏｄｅｒ

－Ｄｅｃｏｄｅｒ ） 架构 ， 即首先通过

一个图像编码器提取图像特征 ， 获取图  

像特征的语义表示 ， 然后将语义特征输入解码器 ， 通过解码器生成图像的描述 。  

近年来 ， Ｔｒａｎｓｆｏｒｍｅｒ架构在机器翻译任务中被首先提出 ， 随后因为其强大的建  

模能力逐渐成为序列到序列类任务中的主流架构 。  

自从图像描述任务被提出以来 ， 已经在视觉编码 、 文本解码 、 训练策略等  

一定的进展 ， 但是图像描述任务仍面临几个关键问题 ， 包括本文主  

要关注的推理速度和生成可控性问题 。  

图像描述的推理速度问题主要来自 目前主流的自回归 （Ａｕｔｏｒｅｇｒｅｓｓｉｖｅ ） 文  

本生成范式 ， 自回归生成范式起源于先前文本生成中流行的循环祌经网络  

北京邮电大学硕士学位论文  

（ＲＮＮ ） ， 循环神经网络每

一步根据先前已经生成的句子推理下

一个词 ， 逐步  

生成整个句子的过程被称为自回归生成 。 自回归生成的主要缺点在于生成的时  

间复杂度 ， 如果要生成长度为ｎ的句子 ， 那么模型将需要进行ｎ次前向推理 ， 因  

而导致了〇（ｎ）的时间复杂度 ， 限制了模型推理的速度 。  

近年来 ， Ｔｒａｎｓｆｏｒｍｅｒ架构逐渐成为人工智能领域的主流架构 ， 与专注于串  

行生成的循环神经网络不同 ， 这种架构具有强大的并行能力 ， 因此 ， 文本生成  

的并行化在这个时代成为了最值得探讨的问题之

， 也被称为非自回归 （Ｎｏｎ

Ａｕｔｏｒｅｇｒｅｓｓｉｖｅ ）生成 。 在文本生成领域 ， 如机器翻译 、 人机对话 、 图像描述等 ，  

基于Ｔｒａｎｓｆｏｒｍｅｒ的非自回归生成方法近来受到了广泛的关注 。  

自回归方法〇 （ｎ ）  

Ｗ〇ｍａ ，１￣＇？

－—＞ ｔｈｅ

ｙｅｌ ｌｏｗ   ｍｌＡ  

＋步数与句子长度正相关  

Ｅｎｃｏｄｅｒ  

｜菲自回归方法０ （１ ）  

１ＮＡ１Ｃ ＾＾置＞ Ｗｏｍａｎｉｎｔｈｅ  

＾Ｄｅｃｏｄｅｒｔｈｅｐｅｃｏｄｃｒｙｄ ｌｏｗｔｒｕｃｋ  

Ｉ ！  

＇— ＝＝＝＝＝  ＇  

步数固定 ， 与句子长度无关  

－２ 自回归解码器和非 自回归解码器生成文本的对比

， 自回归方法生成五个单词的句子  

共需要六步 （包括终止符 ） ， 而非 自回归方法生成的步数与句子的长度无关  

非自回归方法与自回归方法的主要区别如图 １ －２所示 ， 自回归方法每次根  

据先前己经生成的句子生成

一个单词 ， 而非自回归方法并行地处理整个句子 ，  

与句子的长度无关 ， 因此可以达到０（１）的时间复杂度 。 因此 ， 非自回归的生成  

方法相比自回归生成在速度方面有着巨大的优势 。 考虑到图像描述的各种现实  

应用都对图像描述算法的速度提出了要求 ， 因此对非自回归方法的研究无疑是  

具有现实意义的 。  

Ｔｒａｎｓｆｏｒｍｅｒ ：  

ｔｈｅｒｅａｒｅｔｗｏｓｉｎｋｓｉｎｔｈｅｂａｔｈｒｏｏｍ  

ｔｈｅｒｅａｒｅｔｗｏｓｉｎｋｓｉｎｔｈｅｃｌｅａｎｂａｔｈｒｏｏｍ  

ｔｈｅｒｅａｒｅｔｗｏｓｉｎｋｓｗｉｔｈｍｉｒｒｏｒｓｉｎｔｈｅｒｏｏｍ  

ｔｈｅｒｅａｒｅｔｗｏｓｉｎｋｓｗｉｔｈｍｉｒｒｏｒｓｉｎｔｈｅｂａｔｈｒｏｏｍ  

、 ｔｗｏｓｉｎｋｓａｎｄｔｗｏｍｉｒｒｏｒｓｉｎｔｈｅｂａｔｈｒｏｏｍ  

图 １ －３ 图像描述中存在的模式崩溃 ， 风格单 一的问题的例子  

生成的可控性问题则来自生成式任务中广泛存在的

“ 模式崩溃

” 问题 ， 作  

为生成式模型 ， 目前的最先进图像描述模型普遍面临不同程度的模式崩溃问题 ，  

一章绪论  

即在很大的模式空间中 ， 模型收敛到了某几个固定的简单模式 ， 在图像描述中 ，  

这个问题体现为生成收敛在某几个固定的句式 ， 如图 １

－３所示 ， 生成的结果明  

显地局限在了几个简单的 ｔｈｅｒｅａｒｅ＿ ｉｎ＿句式中 ， 现有的效果评价指标 ， 如  

ＣＩＤＥｒ、 ＢＬＥＵ等 ， 都没有对这种情况进行限制 。 这导致最先进 （ＳＯＴＡ） 模型  

生成的图像描述尽管在现有的评价指标上超越了人类 ， 但是实际上与人类的描  

述能力仍具有相当的距离 。 因此 ， 通过各种方式提升生成的描述与图片相关性  

是 一个关键的科学问题 ， 衍生出了包括多样图像描述 （ＤｉｖｅｒｓｅＩｍａｇｅ  

Ｃａｐ ｔｉｏｎｉｎｇ ） 、 可控图像描述 （ＣｏｎｔｒｏｌｌａｂｌｅＩｍａｇｅＣａｐｔｉｏｎｉｎｇ ） 、 唯

一图像描述  

（Ｄｉｓｔｉｎｃｔ ｉｖｅＩｍａｇｅＣａｐ ｔｉｏｎｉｎｇ）等多种研宄方向 。 其中 ， 可控图像描述在图像  

描述的训练和推理过程中加入控制信号以赋予图像描述可控性 ， 是目前图像描  

述领域最关注的方向之

１ ．２国内外研究现状  

本节将针对基于图像描述任务中的相关研宄方向 ， 简述对应的相关工作及  

研宄现状 。  

１ ．２ ． １ 自回归图像描述  

图像描述的研究由来已久 ， 早期的图像描述方法主要基于模板或检索生成  

， 这类方法通过手工定义

一系列与图像对应的模板 ， 然后通过传  

统机器学习算法对图像进行特征提取 ， 最后通过特征匹配的方式把单词填入到  

模板 ， 生成图像的描述 。 而最早的基于神经网络的图像描述模型 ＮｅｕｒａｌＩｍａｇｅ  

Ｃａｐ ｔｉｏｎｇｅｎｅｒａｔｏｒ（ＮＩＣ）由Ｖｉｎｙａｌｓ等人

［６］于 ２０１４年提出 ， 采用了编解码器架构 ，  

以卷积神经网络 （ＣＮＮ ）作为图像编码器 ， 长短期记忆网络 （ＬＳＴＭ ）作为文  

本解码器 ， 取得了当时最好的表现 。 随后 ， 随着对模型各方面能力的不断探索 ，  

更多的基于神经网络的图像描述模型被提出 。 以编解码器模型的角度来看图像  

描述模型 ， 可以将其拆解为三个部分 ： 图像编码器、 文本解码器 （本节中将专  

注于自回归的文本解码器 ）和整体训练策略 。 本小节中 ， 本文将从这三个部分  

出发 ， 简述近年来自回归图像描述的相关工作 。  

图像编码器的主要目标是赋予图像描述模型抽取图像特征的能力 ， 在最初  

的 ＮＩＣ模型中 ， 通过 ＧｏｏｇＬｅＮｅｔ ［７］来实现图像特征的提取 ， 而显然地 ， 使用更  

强的视觉网络可以提升图像描述的性能 ， 因此在基于神经网络的图像描述发展  

早期 ， 各种不同的视觉网络被用于图像编码 ， 包括 ＡｌｅｘＮｅｔ ［８ ］ 、 ＶＧＧ

［９ ］和  

尺￡办过 ［ １ （）］等 。 直接升级使用更好的视觉网络无疑是最简单的提升性能的路径 ，  

但是早期的图像描述方法将从视觉编码器中提取到的全局特征图直接送入解码  

器中以生成文本 ， 而图像中是存在区域之间的重点和非重点关系的 ， 不作区分  

北京邮电大学硕士学位论文  

地输入导致模型只能看到全局信息 ， 而很难捕捉到图片中的局部信息 ， 从而很  

难识别到图片中的小物体 。 因此 ，

一些工作致力于在视觉编码器中引入注意力  

机制 。 如 Ｘｕ等人 率先提出了基于注意力机制的视觉编码器 ， 通过文本解码  

器当前步的隐藏层特征生成与图像特征图同样大小的注意力图 ， 并基于注意力  

图对图像特征进行加权得到最终的图像特征 ， 基于注意力的视觉编码器使得图  

像描述模型可以在解码时关注到图像中更重要的部分 。 Ｓｕｇａｉｉｏ等人

［ １２ ］则进

一步  

一种基于人类凝视信息对齐人类注意力的方法 ， 进

一步地提高了注意力  

生成的质量 。  

： ： ．１１ — ：  

．—  －

ｖ ｆ  

—— — ？  

一  ＇ｕ

＞４两种图像特征的直观理解 ， 左图为全局特征 ， 右图为区域特征  

以上的特征抽取方法都是基于卷积神经网络提取的全局图像特征 ， 但是随  

着目标检测的蓬勃发展 ， 基于目标检测＋注意力机制的视觉特征也逐渐被引入到  

图像描述领域 。 基于目标检测的特征将图片视为

一系列区域 （Ｒｅｇ ｉｏｎ ） 的集合 ，  

其中最突出的工作是Ａｎｄｅｒｓｏｎ等人

［ １３］提出的 ＢＵＴＤ（Ｂｏｔｏｍ

－ｕｐＡｎｄＴｏｐ

－ｄｏｗｎ  

Ａｔｔｅｎｔｉｏｎ ）方法 ， 其中自底向上 （Ｂｏｔｏｍ

－ｕｐ ） 的过程指通过目标检测器提取图  

片中的目标检测级 （Ｒｅｇ ｉｏｎ

－ ｌｅｖｅｌ ）特征 ， 自顶向下 （Ｔｏｐ

－ｄｏｗｎ ） 的过程指为通  

过神经网络学习的方式在区域间实现注意力机制 ， 从而使模型学习到关注到图  

片中某些区域 。 同时 ， 在训练中 ＢＵＴＤ方法还引入了区域的属性信息预测任务  

以提高模型的特征提取能力 。 最终极大地提高了图像描述特征提取的精细程度 。  

Ａｎｄｅｒｓｏｎ等人还开源了基于 Ｆａｓｔｅｒ

－ＲＣＮＮ的 ＢＵＴＤ特征 ， 被广泛地应用在图像  

描述工作中 。 在对区域特征的研究中还出现了基于图神经网络对区域间的关系  

进行建模 ， 从而更好地描述区域间的关系的工作 ， 如Ｙａｎｇ等人 提出了利用图  

一章绪论  

结构表示图像的场景图 （ＳｃｅｎｅＧｒａｐｈ ） ， 用图建模图像中物体间的关系 。 Ｙａｏ等  

１５ ］则提出了利用树创建

一种层次化的图来建模图像的区域特征 。  

综上所述 ， 图像描述任务中

一般采用以上全局和区域两种视觉特征 ， 两种  

特征的直观理解如图 １

－４所示 。  

在文本解码器方面 ， 最初的 ＮＩＣ模型使用了当时较为先进的长短期记忆网  

络作为解码器 ， 在后续的工作中 ， 不断有新的模型被用于文本解码 。 ＮＩＣ模型  

采用的是原始的单层长短期记忆网络 ， 随着相关研究的进展 ， 研究者们首先在  

长短期记忆网络上取得了

一些进展 ， Ｘｕ等人

［ １ １ ］提出的基于注意力的图像描述模  

型应用了基于注意力的长短期记忆网络 ， 在输入的图像特征Ｘ上基于隐藏层特  

征计算注意力 。 Ｌｕ等人

［ １ ６ ］在注意力机制的基础上提出用

一个专门单独训练的  

“ 视觉哨兵

” 参与注意力图的计算 ， 进

一步地提升了文本解码器的性能 。 更进  

一步地 ， Ａｎｄｅｒｓｏｎ等人

［ １３ ］则率先在图像描述中应用了

一个双层的长短期记忆网  

络 ， 将文本和图像的特征进行多层的计算 ， 提升了网络的建模能力 。 上述几个  

基于长短期记忆网络的文本解码器如图 １ －５所示 。  

单层长短期记忆网结单 结视觉哨兵长短期记忆网结双届长短期记忆网络  

ｒ ＞（ｙｔ（ｙ＊（ｙｉ

ｌ   Ｌ＾ ｝ ＬＳＴＭ ｔ ｜

？ｔ  

图＊舰 ＩＸ

￣ ＭＬＰｌｆＭ ｌ？ｌ ｌ￥ＴＭ ；＾

－ －＞  

ａｔｅｎ ｈ ＾－＊

｛ ａｔｔｅｎ  ｜＾ ｆ ａｔｔｅｎ  ［  

－ｒ＊ － ｒ＊ － ４＊ ？ ？ ？ ｅ｜ ■ ｆ   Ｋ

：ＬＪ 丨ＬＪ

：ＬＪ４

ＬＳＴＭ］

－ － － －＊

－ － － －４ＬＳＴＭ

ｉ ； 丨

１Ｌｉｘｐ

？ ￣ｒ１ ＂ ｔ Ｊ  

－ ？？／ｉ － －— —Ｊ Ｖｔ

ｉＭＪ ｈｊ ＩＨ ｉ ＼Ｊ  

－５基于长短期记忆网络的文本解码器  

除了长短期记忆网络 ， 学术界还提出了基于卷积神经网络的文本解码器 ，  

Ａｎｅｙａ等人 提出了

一个基于卷积神经网络解码文本的图像描述模型 ， 将图文  

特征拼接后送入卷积神经网络 ， 通过卷积结果预测输出的文本 。  

Ｔｒａｎｓｆｏｒｍｅｒ

［ １８］近年来成为人工智能领域的基本结构之

， 以其可并行 、 效  

果好 、 结构简单的优势被引入到于各种各样的深度学习任务中 。 首批被提出的  

基于 Ｔｒａｎｓｆｏｒｍｅｒ的图像描述方法

１９％］直接在图像描述中采用了其架构 ， 但是对  

架构本身并没有太多的修改 。 随着研宄的进

一步深入 ，

一些工作对 Ｔｒａｎｓｆｏｒｍｅｒ  

架构进行了修改 ， 使其更贴近图像描述任务的要求 ， 如 Ｃｏｍｉａ等人

［２ １ ］提出的  

Ｍ ２Ｔｒａｎｓｆｏｒｍｅｒ修改了原本的架构 ， 从原来的仅使用编码器最后

一层的特征修  

改为使用全部编码层的特征 ， 提出了

一种网状的 Ｔｒａｎｓｆｏｒｍｅｒ结构 ， 这种改进赋  

予了模型更好地利用图像特征的能力 ， 提升了图像描述的性能 。 同时 ，  

Ｔｒａｎｓｆ ｏｒｍｅｒ强大的并行能力使得构建巨大参数量的预训练模型成为可能 ，

一些  

在大规模语料库上训练 ， 支持多任务的预训练模型被提出 ， Ｗａｎｇ等人

［２２ ］率先  

提出了基于图文预训练的预训练模型 ， 支持图像描述任务 。 此后 ， ＧＰＴ的流行  

北京邮电大学硕士学位论文  

使得预训练成为近年来最受关注的研究方向 ， 大量的多模态预训练模型

－２５ ］被  

用于图像描述领域 ， 极大地促进了相关研究和应用的发展 。  

图像描述的训练策略则主要集中在对图像描述任务中评估指标不可微分的  

问题的研宄 ， 评估指标不可微分是指在自然语言处理领域 ， 常用的评估指标如  

ＢＬＥＵ和 ＣＩＤＥｒ等 ， 均依赖于对模型产出的样本进行采样 ， 然后将生成的样本  

结果与真实数据进行比较以得出评估分数 ， 这个过程通常是无法微分 ， 无法计  

算梯度的 ， 因此无法直接通过评估结果反向传播 ， 因此使得不可能用评估分数  

直接用作模型损失 。 为了绕过这个问题 ， 传统的深度学习图像描述方法使用交  

一个代理指标 ， 将图像描述看作

一个分类任务 ， 对每个位置都计算模  

型输出概率分布与真实数据之间的交叉熵作为损失函数 。 然而 ， 交叉熵损失作  

一种代理指标 ， 与 ＣＩＤＥｒ等评估指标不同 ， 是词级别而不是句子级别的 ， 并  

不能从全局的角度优化模型以达到最佳性能 ， 限制了模型的整体表现 。 因此 ，  

如何引入句子级别的优化成为

一直以来图像描述与其它文本生成领域关心的问  

。 基于强化学习中的策略梯度算法估算梯度是图像描述领域常用的方法 ，  

最早由 Ｒａｎｚａｔｏ等人

［２６ ］提出 ， 目前最常使用的图像描述强化学习方法则由  

Ｒｅｎｎｉｅ等人

［２７ ］提出 ， Ｒｅｎｎｉｅ等人提出的方法采用了模型输出的贪婪采样结果与  

波束搜索结果作为奖励和基线 ， 提升了策略梯度算法的表现 （关于奖励 ， 基线  

等强化学习策略梯度概念的描述将在第二章相关技术中详细讨论 ） 。 在后续的研  

究中 ， Ｃｏｍｉａ

［２ｎ等则从实践中发现可以直接将波束搜索的 ｎ个波束的平均值直  

接作为基线 ， 这样做不仅节省了训练的时间 ， 效果也有所提升 。 Ｈｏｎｄａ等人

［２８ ］  

在对强化学习的研宄中发现强化学习在提升性能的同时也引入了

一些局限性 ，  

体现在生成的句子比较重复 ， 生成某些简单单词的概率很高 ， 因此提出了

一种  

强化学习后再微调模型的训练方式 ， 改善了此问题 。  

１ ．２ ．２非自回归图像描述  

近年来 ， 图像描述工作开始转向纯 Ｔｒａｎｓｆｏｒｍｅｒ结构 ， 其强大的学习能力大  

大地提高了图像描述模型的表现 。 这些工作在训练时通过上三角掩码实现了并  

行的语言模型训练 ， 而解码时 ， 这些模型仍然使用了串行自回归的解码方式 ，  

即每个时间步只预测

一个词 ， 虽然这样做得到了很好的效果 ， 但无疑是对并行  

一种浪费 ， 在效率上有很大的优化空间 ， 因此非自回归的 Ｔｒａｎｓｆ ｏｒｍｅｒ解  

码器被提出用于并行解码 。 非自回归是与自回归相对的概念 ， 表示生成文本方  

式的改变 ， 自回归生成在每

一个时间步都只能利用前ｔ

－ １步生成的文本生成第ｔ  

个单词 ， 即串行生成 ， 非自回归生成方法致力于通过不同方式向生成中引入了  

一定程度的并行性 ， 在图像描述中 ， 目前被提出的非自回归模型可以被分为三  

类 ： 单步非自回归图像描述 、 迭代细化 （多步 ）非自回归图像描述 、 半自回归  

一章绪论  

图像描述 。 以单步非自回归举例 ， 自回归生成和非自回归生成的区别如图 １

－６  

所示 。 本节将对相关的方法进行简要的讨论 。  

ＡＢＣＤ  

个＼个＼ 个＼ 个  

—？ 自回归生成  

－ ７ ： ： ：  

—＞軸非自回归贼  

［ＢＯＳ ］  

－６上部是 一个自回归生成方法的示意 ， 下部是 一个单步非 自回归生成的示意 ， 单步非  

自回归生成方法直接根据图 片 一步生成所有的单词  

非自回归生成方法最早起源于机器翻译领域 ， 随着 Ｔｒａｎｓｆ ｏｒｍｅｒ的流行 ， 利  

用其并行性创建新的生成范式最早在 Ｇｕ等人

［２９ ］的工作中提出 ， 定义了最早的  

非自回归方法 。 随后 ， 受到机器翻译领域的相关进展的启发 ， 大量的非自回归  

图像描述工作被提出 。 早期的非自回归图像描述工作集中在单步的非自回归生  

成 ， 即将整个解码过程压缩到

一次推理输出所有的单词 。 但是单步非自  

回归存在显著的缺点 ： 相比自回归中充足的文字信息 （在第ｔ步可以参考前ｔ 一１  

步生成的单词信息 ） ， 单步非自回归方法没有任何显式的语义信息可供参考 。 在  

图像描述领域 ， Ｆｅｉ等人

［％］最早提出了

一个单步非自回归图像描述方法 ， 使用了  

位置对齐机制来弥补单步图像描述中语言信息缺失的缺陷 ， 位置对齐机制是指  

一个很小的自回归模型 ， 通过这个自回归模型先生成

一些单词 ， 然后在这  

些单词的引导下生成最终的描述 ， 因此提升了非自回归图像描述的性能 。 而  

［３ １ ］则提出用强化学习的理论来解释单步非自回归图像描述 ， 即将单步  

非自回归的过程看作

一个多智能体强化学习过程 ， 用强化学习理论进行序列级  

的优化 ， 并引入了反事实训练提升训练的效果 ， 因此提出了效果和速度均达到  

先进水平的单步非自回归图像描述模型 。  

在单步图像描述之外 ， 还有研究提出了多步的非自回归图像描述 ， 也叫做  

迭代细化的非自回归图像描述 。 即在并行处理所有位置的前提下 ， 增加

一些步  

北京邮电大学硕士学位论文  

骤 ， 以从先前的步骤中得到语言信息 ， 提升生成文本的质量。 Ｇａｏ等人

［３２］提出  

一个迭代细化的图像描述方法 ， 他们提出的迭代细化图像描述方法的流程如  

下 ： 在第

一个完整的句子 （和单步非自回归相同 ） ， 在之后的步骤中每  

次 ： １ ）将上

一步中生成的句子中置信度较低的单词置为掩码标记［ＭＡＳＫ ］ 。 ２ ）  

通过解码器将［ＭＡＳＫ ］重新预测为单词 。 由此 ， 迭代细化的非自回归图像描述模  

型可以逐步优化早期步骤中生成的质量低的单词 ， 从而提升总体的非自回归生  

成质量 。 然而 ， Ｇａｏ等人的工作中 ， 存在

一个问题 ： 多步生成的过程中 ， 生成  

的句子无法动态地变化 。 Ｆｅｉ等人

［３３］则更进

一步 ， 在 Ｇａｏ等人的工作基础上加  

入了长度预测信息 ， 允许在Ｇａｏ等人提出的掩码

－重新预测流程中动态地更改句  

子的长度 ， 实现了更为动态的迭代细化非自回归图像描述 。掩码

－重新预测的过  

程其实可以看做

一种生成顺序的隐式控制 ， 因此 ， Ｆｅｉ等人

一步提出在非自  

回归方法中根据不确定性 （Ｕｎｃｅｒ ｔａｉｎｔｙ）来显式地控制单词生成的顺序 ， 通过  

设计网络预测和学习词的不确定性 ， 在生成时优先生成语义比较复杂 ， 对生成  

质量影响大的单词 ， 通过预训练双向语言模型 ＢＥＲＴ实现了高效且快速的图像  

描述 。  

值得注意的是 ， 近年来 ， 扩散方法 （Ｄｉｆ ｉｘｓｉｏｎ）

［３５］在图像生成领域取得了  

极好的效果 ， 因此在各种生成领域广受关注

［３６１ 在非自回归图像描述中也出现  

一些基于扩散方法的工作 。 扩散方法是基于加噪去噪的 ， 在图像生成领域 ，  

扩散方法的原理是在图像上加入连续的高斯噪声 ， 但是连续高斯噪声并不适用  

于离散的文本生成 。 目前 ， 主流的扩散图像描述方法由采用了

一种折中的方法  

以利用连续扩散方法 ， 即先将文本映射到连续空间 ， 最后再在连续空间应用扩  

散方法 ， 最后再从连续空间映射回文本 。 Ｃｈｅｎ等人 最早在图像描述中提出了  

上述的连续扩散方法 ， Ｌｕｏ等人

［３８］在此基础上优化了网络的架构 ， 引入了序列  

级优化和知识蒸馏 ， 构建了强大的扩散图像描述模型 。 上述工作在对文本进行  

操作时都遵循了多步 、 并行的思路 ， 可以视作迭代细化非自回归模型的

一种变  

种 。  

１ ．２ ．３可控图像描述  

和大部分生成工作类似 ， 图像描述领域中大部分努力集中在无条件的图像  

描述生成上 ， 在图像描述中引入控制信号的工作相对较少 ， 但是仍有

一部分学  

者在这个有意义的方向上保持探索 ， 本节将对这些在可控图像描述领域的探索  

进行讨论 。  

相对早期的可控图像描述方法在 ２０１６年由 Ｍａｔｈｅｗｓ等人

［３９ ］提出 ， 他们提  

出的 Ｓｅｎｔ ｉＣａｐ方法可以用情感描述图像 ， 原理在于应用了两个独立的循环神经  

一个生成正常的图像描述 ， 另

一个生成情感化的图像描述 ， 并在每

一步  

一章绪论  

的生成中动态地判断使用哪

一个网络 。 Ｃｏｒ ａｉ＃ Ｑ］等人在 ２０１９年提出了  

ＳＨＯＷＣＯＮＴＥＬＬ模型 ， 为了解决图像描述模型中描述内容难以控制的问题 ，  

ＳＨＯＷＣＯＮＴＥＬＬ应用了

一种基于图像区域特征控制的模型 。 该模型通过调节  

输入的区域特征 ， 能够显式地指导模型关注并描述图像中特定的物体 。 Ｄｅｎｇ等  

人Ｍ则提出了

一个通过计算文本详略嵌入的方式控制生成文本的详细与否的方  

法 ， 并提出了

一种基于双向语言模型的迭代细化非自回归图像描述方法 ， 支持  

对详略程度的控制 ， 称为ＬａＢＥＲＴ。 Ｗａｎｇ等人Ｐ侧提出了

一种基于提示词的可  

控图像描述方法ＣｏｎＣａｐ

， 利用提示词对不同风格的图像进行训练 ， 最终得到可  

以控制图像描述风格的提示词 。 ＣｏｎＣａｐ和ＬａＢＥＲＴ的共同点在于都通过某种方  

式对图像描述数据集进行了划分 ， 并用不同的嵌入或者提示词进行端到端的训  

练 ， 从而让嵌入学习到相应的风格化知识 ， 完成风格化的图像描述 。  

此外 ， 可控性还可以体现在很多其它需要控制信号的场景方面 ： 某些图片  

一些文本 （如路牌 ， 招牌等 ） ， Ｚｈａｎｇ等人

［４３］提出了

一种纯文本 、 无需图  

－文本对的训练方法以训练模型捕捉更好地捕捉文本信息 。 在现实中 ， 由于数  

据集的限制 ， 性别或者种族偏见也是可控图像描述涉及的

一个重要问题 ， 针对  

性别偏见 ， Ｈｉｒｏｔａ等人

［４４］提出将其分为两种 ： 词语的偏见和上下文的偏见 ， 并  

利用合成的偏见样本以减少这两种性别偏见 。 Ｑ ｉｕ等人

［４５ ］则提出近年来基于模  

型的评估指标并没有对偏见保持足够的关注 ， 可能导致预训练模型无意间编码  

社会偏见 ， 因此 Ｑ ｉｕ等人对图像描述中的评估指标进行了系统的研宄 ， 提出了  

一种基于强化学习的方法对消除模型的偏见 。针对种族偏见 ， 目前检验和评估  

性的工作居多 ， Ｚｈａｏ等人

［４６ ］对ＭＳＣＯＣＯ中的种族偏见问题基于肤色标注进行  

了研究 ， 发现现代的图像描述系统中存在相比经典方法更严重的种族偏见问题  

需要解决 。 Ｂａｋｒ等人 则提出了

一种结合图像的方式评估描述中存在的偏见 。  

综上所述 ， 关于可控图像描述的研宄目前在图像描述领域仍然较为欠缺 ，  

集中在提示词等机制在图像描述中的初步应用和图像描述的社会影响方面 ， 并  

且现有的方法大部分受限于现有的相对客观的通用图像描述数据集 ， 限制了可  

控图像描述的发展 。  

１ ．３本文研究内容与主要工作  

本节将针对图像描述任务 ， 与当前存在的问题结合 ， 阐述本文的主要研究  

内容与工作贡献 。  

１ ．３ ． １基于编辑的非自回归图像描述方法  

目前 ， 在图像描述领域中 ， 主流的工作都使用自回归的语言模型作为解码  

器 ， 自回归的语言模型在推理时从左到右逐步生成文本 ， 这种生成方式是串行  

北京邮电大学硕士学位论文  

的 ， 效率比较低下 ， 而图像描述作为

一个现实应用广泛的任务 ， 对推理速度和  

性能都有着要求 ， 自回归的图像描述模型由于其低效率 ， 在推理速度上有着

定的缺陷 。而现有的非自回归图像描述算法虽然具有很高的效率 ， 但是在性能  

一定的不足 ， 因此 ， 目前现有的图像描述工作都没有达到

一个比较  

－性能平衡 。  

近年来 ， 非自回归文本生成领域出现了

一些效仿人类编辑操作生成文本的  

工作 ， Ｇｕ等人

［４８ ］最早提出了ＬｅｖｅｎｓｈｔｅｉｎＴｒａｎｓｆｏｒｍｅｒ ， 其中ｌｅｖｅｎｓｈｔｅｉｎ是

一种  

编辑距离 ， 表示模型是基于编辑操作的 。在后续的工作中 ， Ｘｕ等人

［４９］又在研宄  

中基于 ＬｅｖｅｎｓｈｔｅｉｎＴｒａｎｓｆｏｒｍｅｒ提出了可以包含编辑任务的重定位 （Ｒｅｐｏｓｉｔ ｉｏｎ）  

操作 ， 定义为 ： “ 为每个位置预测最匹配的当前句子中的单词 ， 并删除预测到特  

定值的位置 。 ” 重定位在兼容删除任务的基础上增加了人类修改句子时会用到的  

换位操作 ， 使得编辑的灵活度进

一步提高 。 虽然这些工作提出了丰富的编辑操  

作 ， 但是编辑操作整体的效率仍然首先 ， 需要至少两个解码器才能实现全部的  

编辑操作 （传统编辑操作的低效性将在第三章进行相信的讨论） 。 因此如何实现  

更先进的编辑操作成为在非自回归图像描述中应用编辑操作的挑战之

此外 ， 本文的目标是通过实现基于编辑的非自回归图像描述模型 ， 实现

－性能平衡中表现良好的图像描述模型 。为了达到这个目标 ， 如何与图  

像描述领域的现存训练方法结合成为了

一个主要的问题 ， 近年来的图像描述模  

一般通过两阶段训练 ： 交叉熵训练和强化学习训练 ， 在交叉熵训练阶段 ， 需  

要解决的问题在于目前在图像描述的数据集中只有图片和对应的描述 ， 没有编  

辑操作相关的标注 ， 因此在交叉熵训练阶段如何基于现有的数据生成合适的编  

辑训练数据是

一个问题 。其次 ， 在强化学习阶段本文面临

一个非自回归模型中  

普遍存在的问题 ： 强化学习是与解码过程高度耦合的 ， 在目前主流的自回归图  

像描述工作中 ， 解码过程相对统

， 因此可以使用统

一的强化学习流程 ， 但是  

在非自回归图像描述工作中 ， 文本解码方式是主要的研究内容 ， 现存的工作在  

解码方式上多种多样 ， 因此不存在

一个公认的强化学习方法 ，

一些工作也仅在  

交叉熵阶段进行了训练 。 因此 ， 如何根据编辑操作的过程设计合适的强化学习  

一个值得研宄的问题 。  

通过上述的分析 ， 本文总结了在图像描述中应用编辑操作的两大挑战 ：  

（ １ ）现存的编辑解码器的速度优势相比其它非自回归解码器并不明显 ， 如  

何实现更高效的编辑操作以提升算法的速度 ， 使其达到更好的综合  

表现 。  

（２）基于编辑的非自回归图像描述模型与其它的图像描述方法差异较大 ，  

如何构建符合编辑操作需求的训练策略 。  

１０  

一章绪论  

为了克服推理速度上的挑战 ， 本文提出了

一编辑非自回归图像描述  

方法 （Ｕｎｉｆ ｉｅｄＥｄｉｔ

－ｂａｓｅｄＮｏｎ

－ａｕｔｏｒｅｇｒｅｓｓｉｖｅＩｍａｇｅＣａｐｔｉｏｎｉｎｇＭｅｔｈｏｄ ， 简称  

ＵｎｉＣａｐ） 。 ＵｎｉＣａｐ提出了

一编辑任务 ， 统

一了上述的两个编辑操作 ，  

简单来说 ， 新的统

一编辑任务定义为

“ 为每个单词预测

一个新的位置。 ” 值得注  

意的是 ， 这个任务的定义与上文中提到的 Ｘｕ等人 提出的重定位任务恰恰相  

反 ， 这样的改动使得这个任务在重定位任务兼容删除任务的基础上 ， 能够进

步地兼容位置插入任务 ， 至此实现两个任务的统

一步地 ， 相比于重定位  

任务 ， 统

一编辑任务并不是在先前工作上的简单修改 ， 任务定义的修改使得统  

一编辑任务虽然可以表示更多的操作 ， 但是使得这个任务的输出存在不合法的  

情况 ， 这与重定位任务是不同的 ， 重定位任务无法包括位置插入操作 ， 但是相  

对地不存在非法输出 。 为了解决任务定义中存在的缺陷 ， 本文又进

一步提出了  

一个冲突消解算法 ， 关于冲突和具体的技术细节将在第三章进行讨论 。  

为了克服训练策略的挑战 ， 本文在图像描述的两阶段训练中分别作出了改  

进以适应编辑操作 ， 在交叉熵训练阶段 ， 本文提出了

一种新的生成编辑数据的  

方法 ， 通过向真实数据加入噪声生成修改后的句子 ， 然后让模型学习还原这个  

句子 ， 从而让模型学习到编辑能力 。 为了使得生成的数据多样性更强 ， 本文创  

新性地提出基于噪声通过三种路径生成数据进行训练 ， 包括 ：  

（ １ ） 路径

一通过直接加入各种随机噪声生成基础的训练数据 。  

（２ ） 路径二通过词预测任务为统

一编辑任务生成更高难度的训练数据 。  

（３ ） 路径三对输入的句子不做任何的修改 ， 直接输入到统

一编辑模型中 。  

一编辑模型修改所有的输入 ， 提升模型的效率 。  

通过三个路径的结合 ， 模型可以更好地学习到编辑能力 ， 提升模型的效率  

和效果 。  

而在强化学习阶段 ， 为了在本文提出的解码策略中训练出效果更好的图像  

描述模型 ， 本文吸收自回归强化学习的思想 ， 设计了在编辑操作设置下适用的  

强化学习算法 ， 此算法吸收了非自回归并行性的优点 ， 在采样中可以生成更多  

样的路径 ， 降低了强化学习的方差 ， 因此提升了总体的效果表现 。  

综上 ， 本文在基于编辑的非自回归图像描述方面的研宄内容总结如下 ：  

（ １ ） 本文提出了

一种新颖的非自回归图像描述方法 ， 命名为 ＵｎｉＣａｐ 。  

ＵｎｉＣａｐ利用本文设计的统

一编辑任务实现了编辑操作的简化 。此外 ，  

本文提出了冲突消解算法来解决统

一编辑任务中的无效输出 。  

（２） 为了解决训练中编辑数据缺失和传统强化学习算法不匹配的问题 ，  

本文结合图像描述任务的特点为ＵｎｉＣａｐ提出了有效的训练策略 。在  

交叉熵训练阶段 ， 本文提出了

一种多路径训练方法 ， 用于从真实数  

１ １  

北京邮电大学硕士学位论文  

据生成编辑训练数据 。 在强化学习阶段 ， 本文提出了

一种阈值釆样

方法 ， 以更好地适应基于编辑的非自回归生成 。  

（３ ） 本文在 ＭＳＣＯＣＯ基准数据集上进行了广泛的实验 。 实验结果证明  

了提出的 ＵｎｉＣａｐ方法的有效性 。 与自回归图像描述基线相比 ，  

ＵｎｉＣａｐ实现了８倍的速度提升 ， 同时在非自回归图像描述生成方法  

中达到了最先进的效果 。  

１ ．３ ．２可控非白回归图像描述  

主流的图像描述工作大部分聚焦于无条件的自回归图像描述生成 ， 对于生  

成图像的多样性或者可控性的关注则不足 ， 目前最先进的图像描述模型普遍面  

临不同程度的模式崩溃问题 ， 即在很大的模式空间中 ， 模型收敛到了某几个固  

定的简单模式 ， 这样的模型不符合实际运用中的复杂需求 。 因此 ， 研宄在图像  

描述中引入控制信号的方式来实现高可用性的图像描述生成是有意义的 。  

经过本文的分析 ， 目前的可控图像描述中存在两个主要的问题 ， 首先 ， 目  

前的可控图像描述模型的研究主要集中在自回归领域 ， 并且并未对可控图像描  

述的推理速度做出优化及改进 ， 作为

一个符合现实使用需求的模型 ， 通过非自  

回归方法对其速度进行优化是有极大现实意义的 。其次 ， 目前的可控图像描述  

往往受到数据集的限制 ， 如近年Ｗａｎｇ等人 的工作中 ， 提供风格化数据的方  

法是从数据集中通过基于情感词或者长度匹配的方式筛选相应的风格数据 ， 这  

样的数据生成方法导致风格受限于图像描述的数据集 ， 而且主流的图像描述数  

据集 ， 如ＭＳＣＯＣＯ等 ， 往往标注风格相对客观 ， 从而进

一步限制了生成相对  

主观的图像描述的能力 ， 因此 ， 如何获取可控图像描述的数据是

一个挑战 。  

为了克服可控图像描述推理速度的挑战 ， 本文在本文的另

一个研究成果  

ＵｎｉＣａｐ 的基础上实现了

一编辑的可控非自回归图像描述方法  

（ＣｏｎｔｒｏｌｌａｂｌｅＵｎｉｆｉｅｄＥｄｉｔ －ｂａｓｅｄＮｏｎ

－ａｕｔｏｒｅｇｒｅｓｓｉｖｅＩｍａｇｅＣａｐ ｔｉｏｎｉｎｇＭｅｔｈｏｄ ，  

简称 ＣｏｎＵｎｉＣａｐ） 。为了基于ＵｎｉＣａｐ实现可控图像描述 ， 本文采取了先前工作  

中出现过的提示词方法 ， 用

一个可学习的张量提示词作为控制信号 ， 并在训练  

时让提示词与目标的风格对应 。 从而实现兼顾可控性和推理速度的可控非自回  

归图像描述方法 。  

为了克服风格化数据缺失的挑战 ， 本文提出利用近年来流行的大语言模型  

强大的零样本生成和上下文学习能力 ， 实现生成式的风格化数据 ， 通过大语言  

模型生成的数据不受图像描述数据集的限制 ， 可以更好地让模型学习风格化的  

生成 。 另

一方面 ， 在使用大语言模型生成数据时 ， 由于大语言模型并不能完全  

精确地理解提示词 ， 且存在幻觉问题 ， 为了更好地判断大语言模型生成的文本  

１２  

一章绪论  

是否满足要求 ， 本文提出了大语言模型数据校验机制 ， 结合风格判别和多步的  

幻觉判别保证生成的风格化数据的高质量 。  

综上所述 ， 在可控图像描述中 ， 本文的研宄内容总结如下 ：  

（ １ ） 提出了

一种基于编辑的非自回归可控图像描述模型 ＣｏｎＵｎｉＣａｐ ， 通  

过在可控图像描述中引入快速的非自回归解码提升推理的速度 ， 使  

其更符合现实世界应用的需求 。  

（２ ） 针对可控图像描述受数据集限制的问题 ， 提出了

一种基于大语言模  

型的可控图像描述数据生成方法及相应的数据校验方法 。  

（３ ） 在 ＭＳＣＯＣＯ基准数据集上进行了广泛的实验 。 证明了提出的模型  

的有效性并在本文提出的五种风格上验证了控制图像描述风格的能  

力 ， 实验结果表明通过生成式地创造数据 ， 提出的方法可以简单地  

拓展到其它风格上 。  

１３  

北京邮电大学硕士学位论文  

１ ．４本文的组织结构  

一章縱  

ｆ  

第：３相難术  

＂  

：   ｉ

： ｆ織图 可）

：跳 引入轩麵 雛賊 ，神仔实 ：  

；獅自回＿去 际朗 ：  

＇ＶＪ ＶＪ Ｉ  

； ｉ ＾ＸＸ

ｉｆ＿ ：（ｍｍ ：１（ｎｍ ：ｍｎｍ

ｉ 步提升基于＾＾５训琢 以适应编 可控图像１＾的￥可控图雛述面瞄 ：  

： 方＾ｓ率 辑搡作 用性限制 Ｊ  

ＩＶＪＶＪＶＪＶ ．ＪＪ  

；ｉｉ

；笛＝音甚千编嫿的兆第四章基于视 ＼  

：ｌｉｉｓｓ词的可ｇ｜ 图图 ；  

ｉ  

第五章非自回归图像   统  

ｉ  

第鴻雖与展望  

图 １ ＿７冬文的组织结构示意图  

本文共包含六个章节 ， 章节结构如图 １ －７所示 。每

一章内容介绍如下 ：  

一章 ， 绪论 。 首先介绍了图像描述任务的研宄背景以及相关工作 ， 其次  

介绍了本文将要研究的两部分内容 ， 包括基于编辑的非自回归图像描述和可控  

图像描述 ， 以及其研究现状 。 最后阐述了本文的具体研究内容 。  

第二章 ， 相关技术 。 介绍了本文在后续的讨论中将会涉及的

一些技术 ， 包  

括深度学习技术 ， 视觉特征相关技术和图像描述的评估的相关技术 。  

第三章 ， 基于编辑的非自回归图像描述方法 。 介绍了本文提出的方法之

一编辑图像描述模型 ＵｎｉＣａｐ ， 包括模型每个模块的设计细节 ， 以及为  

１４  

一章绪论  

ＵｎｉＣａｐ适应图像描述任务改进的训练策略 。 最后在ＭＳＣＯＣＯ数据集上进行了  

实验并对结果进行了分析 。  

第四章 ， 基于视觉提示词的可控非自回归图像描述 。 介绍了本文提出的基  

于ＵｎｉＣａｐ的可控图像描述模型 ： ＣｏｎＵｎｉＣａｐ 。 介绍了模型使用到的组件和其原  

理 ， 包括模型使用的提示词机制以及提出的数据风格判别器 。 同时也介绍了基  

于大语言模型生成与校验可控图像描述数据的方法 ， 最后同样在ＭＳＣＯＣＯ数  

据集上进行了实验并对结果进行了分析 。  

第五章 ， 非自回归图像描述系统 。 在本文提出的两个算法模型的基础上 ，  

一个图像描述系统 ， 本系统包括通常的基于编辑的非自回归图像描述功  

能和可控非自回归图像描述功能 。 通过本文创建的系统 ， 可以让本文研究的模  

型得到更好的应用 。  

第六章 ， 总结与展望 。 对本文提及的研究内容做了简要的总结 ， 并思考了  

研宄的不足与对未来的展望 。  

１ ．５本章小结  

本章首先对图像描述任务的定义 、 研宄背景 、 研宄现状作了详细的描述 ，  

然后针对图像描述任务中的

一些不足进行了进

一步的论述 ， 包括图像描述的推  

理速度和图像描述的可控性 ， 基于这些问题本文提出了本文的研究内容 ， 最后 ，  

本文简要介绍了本文的文章组织结构 。  

１５  

第二章相关技术  

第二章相关技术  

２ ． １深度学习技术  

２ ． １ ． １Ｔｒａｎｓｆｏｒｍｅｒ技术  

Ｔｒａｎｓｆｏｒｍｅｒ由Ｖａｓｗａｎｉ等

［ １８］于 ２０１７年提出 ， 深刻地改变了深度学习领域的  

方方面面 。 先前的循环神经网络在计算过程中需依赖上

一时刻的输出 ， 无法实  

现计算的并行处理 ， 导致训练效率低下 。如 ＬＳＴＭ等循环神经网络架构仍面临  

梯度消失或爆炸的问题 。 针对这些问题 ， Ｔｒａｎｓｆｏｍｉｅｉ

？通过多头注意力机制来捕  

获序列之间的上下文依赖 ， 有效降低了训练的复杂度 ， 实现了训练过程的并行  

化 ， 近年来逐步成为了深度学习领域的基础模型 。  

２ ．１ ．１ ．１ 自注意力机制  

Ｔｒａｎｓｆ ｏｒｍｅｒ的主要特点在于提出了多头注意力机制 ， 首先 ， 定义注意力机  

制如下 ：  

ｆＴ ＼  

Ａｔｔｅｎｔｉｏｎ ｛Ｑ，Ｋ ，Ｖ ） ＝ ｓｏｆｔｍａｘＶ（２ ． １）  

其中 ， （？ ｅ ｉ？ ｉｘｄｆ ｃ

， 分别代表查询向量 （Ｑｕｅｒｙ） 、  

键值向量 （Ｋｅｙ） 以及值向量 （Ｖａｌｕｅ ） 。 Ｌ为序列长度 ， 知和 分别为查询向量  

和键值向量的维度 。将原始的 ＜２ ， 欠， Ｖ先投影到维度ｄｍ ， 分别进行ｈ个独立注  

意力操作 ， 将最终得到的结果拼接 ， 可得多头注意力机制如下 ：  

ＭｕｌｉｔｉＨｅａｄ（Ｑ，Ｋ ，Ｖ ） ＝ Ｃｏｎｃａｔ＾ｈｅａｄ＾

－ ．＾ｈｅａｄ＾Ｗ

ｗｈｅｒｅｈｅａｄ

ｔ ＝Ａｔｔｅｎｔｉｏｎ（ＱＷＰ ，ＫＷ

其中 ， Ｍ／产ｅｅ只 ｄｍｘｄｐ以及ｅ尺

／ｕ＾ｘｄｍ 。  

Ｃｏｎｃａｔ用于表示多个头向量的合并操作 ， 而ｎ表示划分的特征子空间数量 。 因  

此 ， 在多头自注意力机制中 ， 各位置的向量用于计算其他位置的注意力权重 ，  

且在不提高时间复杂度的前提下 ， 每个头部能够学习到不同的注意力权重 ， 进  

而增强了模型的表示能力 。  

２ ．１ ． １ ．２位置编码  

由于 Ｔｒａｎｓｆｏｒｍｅｒ的并行性 ， 各个位置之间是平等的 ， 但是在大部分任务中 ，  

输入的序列都有先后或者其它位置关系 。 为了捕捉到这种位置关系 ，  

Ｔｒａｎｓｆｏｒｍｅｒ在所有的位置上加入

一个位置相关的编码 ， 这个编码可以理解为位  

１７  

北京邮电大学硕士学位论文  

一个函数／（ｐｏｓ） ， 在原始的 Ｔｒａｎｓｆｏｒｍｅｒ中 ， 这个函数被定义为

一个基  

于三角函数的函数 ：  

｜ ｆ（ｐ〇ｓ ，２ｉ ） ＝ ｓｉｎ（ｐｏｓ／ １００００

（２ ， ／＜／ｍ ＊

Ｉ ｆ（Ｐ〇ｓ ＾２／＋ １ ） ＝ ｃｏｓ（ｐｏｓ／ １００００

（ １ ， ｌｄ

其中ｐｏｓ表示位置 ， Ｚ表示这个向量的第 ｉ维 ， ＾＾＾表示这个向量的维度 。  

２ ． １ ． １ ．３网络结构  

Ｏｕｔｐｕｔ  

Ｐｒｏｂａｂｉ ｌ ｉ ｌ ｉｅｓ  

「 Ｓｏｆｔｍａｘ］  

［Ｌ ｉｎｅａｒ］  

ｚ— ｉ — － ｎ  

Ａｄｄ＆ＮｏｒｍＨ

－ｎ  

Ｆｅｅｄ  

Ｆｏｒｗａｒｄ  

（ —ｎ＿ｉ＝ｒ＾   ＜ Ｉ飞Ａｄｄ＆Ｎｏｒｍ  

气呻而Ｍｄｔ ． Ｈｅａｄ

 ＦｅｅｄＡｔｔｅｎｔ ｉｏｎ  

Ｆｏｒｗａｒｄ 

３Ｎｘ  

１＿  

Ａ雜 Ｍ＾ｋＳｄ

Ｍｕ ｆｔｉ

－ＨｅａｄＭｕ ｌｔ ｉ

－Ｈｅａｄ  

Ａｔｔｅｎｔ ｉｏｎＡｔｔｅｎｔ ｉｏｎ  

Ｖ／Ｖ

ＩＪ  

Ｐｏｓ ｉｔ ｉｏｎａ ｌ ｆＴ＼１ｌ ＾ＴＸＰｏｓｉｔ ｉｏｎａ ｌ  

Ｅｎｃｏｄｉｎｇ ｔ ｙＶＥｎｃｏｄ ｉｎｇ  

ｉｎｐｕｔＯｕｔｐｕｔ  

Ｅｍｂｅｄｄ ｉｎｇＥｍｂｅｄｄ ｉｎｑ  

ＩｎｐｕｔｓＯｕｔｐｕｔｓ  

（ｓｈ ｉｆｔｅｄｒ ｉｇｈｔ ）  

图 ２ －１原版Ｔｒａｎｓｆｏｒｍｅｒ网络结构示意图

１ ８ ］  

Ｔｒａｎｓｆ ｏｒｍｅｒ结构如图 ２ － １所示 ，

一个标准的编码器块由多头注意力 ， 残差  

及规范化层 ， 前馈层组成 。 残差及范化层通过对特征向量的每个位置执行标准  

化处理来避免数据的偏差 ， 并通过残差连接减轻了训练过程中梯度消失的问题 。  

而全连接的前馈网络包含了两个线性变换和

一个激活函数 ， 初次线性变换用于  

将特征向量投影到较高维度空间 ， 随后的线性变换则是将其重新映射到原始维  

度 ， 并使用ＲｅＬＵ激活函数

［５（） ］以增加模型的非线性 。  

２ ． １ ．２编解码器结构  

－解码器架构是

一种通用的神经网络架构 ， 由编码器与解码器两大部  

分组成 。 编码器主要负责把输入数据转换成

一种中间表示形式 ， 而解码器则将  

此中间表示解码 ， 以产生最终的输出结果 。  

１８  

第二章相关技术  

－解码器架构通过更改输入和输出的定义可以拓展出多种多样的功能 ，  

例如在自然语言处理领域 ， 编码器

－解码器架构可以用于机器翻译 ， 其中编码器  

一种语言的文本输入转换为中间表示 ， 而解码器则将该表示转换为另

一种语  

言的文本输出 。 这种架构也被广泛应用于语音识别和文本生成等任务中 ， 如生  

成式对话系统和文本摘要生成 。 在图像处理领域 ， 编码器

－解码器架构可以用于  

图像分割任务中 ， 编码器负责提取图像的特征信息 ， 而解码器则基于这些特征  

生成图像的每个像素所属的类别 。这种方法不仅适用于静态图像 ， 也可以扩展  

到视频内容的处理 ， 如视频分割和动作识别 。 此外 ， 广泛应用的 Ｔｒａｎｓｆｏｒｍｅｒ架  

构和在变分自编码器 （ＶＡＥ）也遵循编码器

－解码器架构 。  

对于本文着眼的非自回归图像描述任务 ， 如果给定图像Ｊ 。 图像描述可以理  

解为首先从／中抽取图像特征 ， 然后根据图像特征生成 （解码）文本 ， 这个过程  

同样可以抽象为

－解码器 ：  

Ｖ＝Ｅｎｃｏｄｅｒ（Ｉ）  

ｔ ＝ Ｄｅｃｏｄｅｒ（ｓ

＇ ，Ｖ）  

２ ．２视觉特征编码  

视觉特征编码器的目标是提取图像中的特征到

一系列向量中 ， 根据提取的  

特征的种类不同 ， 可以分为初步的全局视觉特征和更细粒度的目标检测视觉特  

征 。根据网络架构不同 ， 近年来主要使用卷积神经网络或者 Ｔｒａｎｓｆ ｏｒｍｅｒ网络提  

取图像特征 。 本节将从这两个角度分类对视觉特征的编码进行讨论 。  

２ ．２ ． １全局视觉特征  

２ ．２ ． １ ．１基于卷积神经网络的视觉特征编码  

卷积神经网络是最具代表性的神经网络模型 ， 主要的代表工作包括  

ＲｅｓＮｅｔ＾

， ＧｏｏｇＬｅＮｅｆ等 ， 在提取图像特征方面有极大的优势 。

一个卷积神经  

网络通常由卷积层 、 池化层 、 归

一化层和其它部件叠加组成 。其中主要发挥特  

征提取作用的是卷积层和池化层 。 卷积层 通过滑动具有预定步长的卷积核模  

板对特征图进行分析 ， 以提取新的特征图 。 这种操作处理的特征图是三维张量 ，  

最初来源于经过预处理的图像像素 ， 其中三维分别代表图像的长度 、 宽度及  

ＲＧＢ颜色通道 。卷积核的参数共享机制相较于全连接层的矩阵运算 ， 既降低了  

计算量 ， 使得对更高分辨率图像的处理成为可能 ， 又增强了模型对图像平移不  

变性特征的捕捉能力 ， 从而提升了模型的泛化性 。 以二维图像／为例 ， 当应用二  

？时 ， 卷积计算可以形式化为 ：  

Ｓ （ ｉ ，ｊ） ＝

（ｌ ＊Ｋ

）（ ｉ ，ｊ） ＝Ｙ，ｎｌ （ ｉ＋ｍＪ＋ｎ ）Ｋ （ｍ ，ｎ ）（２ ．５）  

１９  

北京邮电大学硕士学位论文  

池化层也可以称为下采样操作 ， 通过采集临近特征图区域的统计数据作为  

样本 ， 旨在从经卷积处理的特征图中筛选信息并进行过滤 。 此过程的

一个显著  

属性是导致特征图维度 ， 即长度和宽度的减少 。类似于卷积过程 ， 池化要求预  

设其作用域及步进长度 。 主要的池化技术包括最大池化和平均池化 。 最大池化  

一个邻近区域 （如 ２Ｘ２） 内的最大值来更新特征图的相应位置 ， 而平  

均池化通过计算邻近区域内的平均值来进行更新 。  

一化由 Ｓｅｒｇｅｙ等人

［５２ ］提出 ， 广泛应用于卷积神经网络中 ， 批归

一化的  

原理是将卷积神经网络中每

一层输出的方差和均值归

一化 。批归

一化的目的是  

减少训练过程中网络层输入分布的变化 。 通过对每

一批数据进行归

一化处理 ，  

可以使网络更加稳定 ， 加快训练速度 ， 并有助于减轻过拟合 。 具体来说 ， 批归  

一化在每个训练批次中 ， 对于给定的特征 ， 在批次的维度上计算均值和方差 ，  

然后使用这些统计量来归

一化该批次的数据 ， 使其均值接近 ０且方差接近 １ 。  

此外 ， 批归

一化还引入了两个可训练参数 ， 以恢复在某些情况下可能需要的网  

络表达能力 。  

一化层通常置于卷积层 （或全连接层） 的输出和激活函数的输入之间 。  

这种做法可以有效减少训练过程中梯度消失或梯度爆炸的问题 ， 从而允许使用  

更高的学习率 ， 加速网络的收敛 。  

２ ．２ ． １ ．２基于Ｔｒａｎｓｆｏｒｍｅｒ的视觉特征编码  

Ｔｒａｎｓｆｏｒｍｅｒ架构首先在纯文本领域被提出 ， 由于其强大的建模能力迅速被  

拓展到其它场景 ， 比如图片的特征抽取 ， 早期被提出的纯Ｔｒａｎｓｆｏｒｍｅｒ图像特征  

编码器是 Ｄｏｓｏｖｉｔｓｋｉｙ等人％提出的 ＶｉＴ模型 ， ＶｉＴ 虽然不是第

一篇将  

Ｔｒ ａｎｓｆｏｒｍｅｒ应用在视觉任务的论文 ， 但是因为其模型简单且效果好 ， 可扩展性  

强 ， 被广泛应用于各种视觉任务中 。  

Ｖ ｉＴ将输入图片分为多个块 （ １６ｘ１６） ， 再将每个块投影为固定长度的向量  

送入模型 ， 后续编码器的操作和原始 Ｔｒａｎｓｆｏｒｍｅｒ中完全相同 。在对这些块的处  

理中 ， 采用了与处理文本类似的方法 ， 这

一点也体现了Ｔｒａｎｓｆｏｒｍｅｒ架构的通用  

性和灵活性 。  

在将图像分割成块之后 ， ＶｉＴ会将这些图像块展平 （即将二维像素矩阵转  

一维向量） ， 并通过

一个线性层或简单的全连接层将其映射成

一个固定长度  

的向量 。 这

一过程可以看作是对每个图像块进行特征提取 。 然后 ， 为了使模型  

能够理解这些图像块之间的相对位置 ， ＶｉＴ还会向每个块的特征向量中添加位  

置编码 。 这与 Ｔｒａｎｓｆｏｒｍｅｒ在处理文本数据时添加位置编码的做法是

一致的 ， 目  

的是提供序列中不同元素的位置信息 。  

２０  

第二章相关技术  

ＶｉＴ是 Ｔｒａｎｓｆｏｒｍｅｒ思想在视觉领域的直接应用 ， 直接对全部的块应用注意  

力 。 然而 ， 图像天然具有局部性 ， 卷积神经网络中的卷积操作也天然蕴含了局  

部性质 。 因此 ， 兼容局部特征处理的视觉Ｔｒａｎｓｆｏｒｍｅｒ在研宄中被提出 ， 目前最  

流行的是Ｌｉｕ等人

［５４ ］提出的 ＳｗｉｎＴｒａｎｓｆｏｒｍｅｒ 。 ＳｗｉｎＴｒａｎｓｆｏｒｍｅｒ使用了类似卷  

积神经网络中的多尺度特征 ， 即特征图包括对图像下采样 ４倍 、 ８倍及 １６倍的  

特征 。 而且 ＳｗｉｎＴｒａｎｓｆｏｒｍｅｒ首次提出了窗口多头自注意力 （ＷｉｎｄｏｗｓＭｕｌｔｉ －  

ＨｅａｄＳｅｌｆ

－Ａｔｅｎｔｉｏｎ）机制 ， 使得自注意力的范围局限在窗口中 ， 减少了计算量 ，  

额外地 ， 为了保证全局的特征传递 ， 作者还提出了移位窗口自注意力 （Ｓｈｉｆ ｔｅｄ  

ＷｉｎｄｏｗｓＭｕｌｔｉ －ＨｅａｄＳｅｌｆ －Ａｔｅｎｔｉｏｎ ） 的概念 ， 通过应用移位窗口自注意力 ， 可  

以在得到窗口局部化的优势的同时保留全局特征流动的能力 。  

２ ．２ ．２目标检测视觉特征  

全局的图像特征是粗粒度的 ， 然而真实图像中往往存在复杂 、 多层次多尺  

度的细粒度信息 ， 目标检测任务就是对这种信息的

一种捕捉过程 ， 为了在图像  

描述和其它跨模态任务中应用细粒度的信息 ， 目标检测模型经常被用于各种交  

叉领域任务中以提取细粒度的区域特征 。 与全局特征同样 ， 目标检测视觉特征  

也可以通过卷积神经网络或者Ｔｒａｎｓｆｏｒｍｅｒ网络实现 。  

２ ．２ ．２ ． １基于卷积神经网络的目标检测  

Ｇｉｒｓｈｉｃｋ等人

［５５ ］提出的 ＲＣＮＮ是最早引入卷积神经网络的目标检测算法 ，  

整体的网络结构如图 ２ －２所示 ， ＲＣＮＮ 由三个模块组成 ， 包括候选框生成 ， 特  

征提取和分类模块 ， 候选框生成模块首先在图片中找到很多的候选框  

（Ｐｒｏｐｏｓａｌ ） ， 然后特征提取模块基于ＣＮＮ提取每个候选框的图像特征 ， 分类模  

块则将每个框的图像特征通过全连接层后执行分类任务 ， 判断是否是待检测的  

目标 ， 后续的大部分基于卷积神经网络的目标检测模型都沿用了ＲＣＮＮ的架构 。  

＿顧 ＿麵   

输入提出候选区域提取特征分类  

图 ２ －２ＲＣＮＮ示意图  

ＲＣＮＮ 中存在很多的不足 ， 其中仍然存在很多传统的机器学习算法 ， 如通  

过选择性搜索 （Ｓｅ ｌｅｃｔｉｖｅＳｅａｒｃｈ ） 获得候选区域 ， 选择性搜索是

一种在传统图  

像处理领域中采用的图像层次分割技术 ， 先在图像中创建初始区域 ， 然后基于  

颜色 、 纹理等特征将这些区域合并 ， 从而实现图像区域的最终分割 。 分类任务  

２ １  

北京邮电大学硕士学位论文  

也是通过传统的支持向量机进行 。 其次 ， 通过选择性搜索获得的候选区域数量  

级往往很大 ， 带来了巨大的成本问题 。  

为了解决ＲＣＮＮ的不足 ， Ｇ ｉｒｓｈｉｃｋ等人

［５６ ］提出了ＦａｓｔＲＣＮＮ ，

—种端到端  

的目标检测方法 ， 相比 ＲＣＮＮ ，ＦａｓｔＲＣＮＮ首先计算

一次全局的卷积特征 ， 然  

后直接映射到选择性搜索的候选框中 ， 避免重复计算 ， 同时在候选框集合上应  

用了非极大值抑制 （Ｎｏｎ

－ｍａｘｉｍｕｍＳｕｐｐｒｅｓｓｉｏｎ ）算法以压缩候选框的数量 ， 提  

升了ＲＣＮＮ推理的速度 。 最后 ， Ｆａｓｔ ＲＣＮＮ还将ＲＣＮＮ中应用的支持向量机替  

换为基于神经网络的分类器 。 实现了端到端的训练 。  

Ｆａｓｔ ＲＣＮＮ实现了端到端的训练和推理 ， 但候选框的提取仍然采用的是选  

择性搜索 ， 非常耗时 ， 为了支持目标检测的实时应用 ， 替换选择性搜索是必须  

的 ， 因此 Ｒｅｎ等人％提出了ＦａｓｔｅｒＲＣＮＮ ， 在其中首次应用了区域提案网络  

（Ｒｅｇ ｉｏｎＰｒ ｏｐｏｓａｌＮｅｔｗｏｒｋ） 以基于卷积神经网络生成候选框 ， 为了整体结构的  

轻量化 ， 区域提案网络的权重是与特征提取使用的卷积神经网络共享的 。  

目前图像描述领域最常用的 目标检测特征均基于 ＦａｓｔｅｒＲＣＮＮ ， 如  

Ａｎｄｅｒｓｏｎ等人

［ １３］提出了

一个用于图像描述的版本 ， 使用在 ＩｍａｇｅＮｅｔ上分类预训  

练的 ＲｅｓＮｅｔ

－ １０ １作为初始化的权重 ， 接着在ＶｉｓｕａｌＧｅｎｏｍｅ数据集

［５８ ］上进行训  

练 ， 为了帮助目标检测模型学习更好的特征表达 ， 加入了额外预测属性类别的  

训练输出 ， 使得模型可以学习到更好的目标检测特征表示 。  

２ ．２ ．２ ．２基于Ｔｒａｎｓｆｏｒｍｅｒ的目标检测  

近年来 ， 目标检测领域的巨大进展之

一在于 （：以加等人岡提出的  

Ｔｒａｎｓｆｏｒｍｅｒ 目标检测器 ＤＥＴＲ 。 相比传统的基于卷积神经网络的目标检测 ，  

ＤＥＴＲ使用了完全不同的编码器

－解码器架构来实现目标检测任务 ， 因此完全不  

需要传统的候选框机制 。  

ｎｏ ｏｂ ｊｅｃｔ（ｏ ）？＃ｎｏｏｂ ｊｅｃｔ（〇 ）  

． ． ．咖￣ ■／／ ／／  

ｔｒａｎｓｆｏｒｍｅｒ霤／／

丨 丨 丨＾

丨 丨—  

－３ＤＥＴＲ工作流程示意图＿  

ＤＥＴＲ的基本流程如图 ２ －３所示 ， 假设图像输入为 ， 首先 ， ／被  

输入卷积神经网络获取下采样后的特征图 ， 如 随后 ， ｆ经过

维卷积降低通道数到ｄ 。 向特征图加入

一个二维的位置编码后将二维的特征图  

展平 ， 得到

一维特征图／

。 随后的过程与正常的 Ｔｒａｎｓｆ ｏｒｍｅｒ编码器

－解码器架构  

基本相同 。 不同之处在于 ， 解码器的输入是

一系列可学习的向量 ， 称作查询 ，  

２２  

第二章相关技术  

一种可学习的候选框 。 ＤＥＴＲ将解码器每个位置的输出都通过

一个  

预测头翻译为框和对象类别 ， 从而完成目标检测的任务 。  

由于查询的数量往往超过图中对象的数量 ， 为了保证在训练端到端的  

ＤＥＴＲ时每个目标匹配到

一个预测结果 ， 使用二部图匹配算法计算损失 ：  

首先用ｙ表示对象的真实集合 ， 用５

＞＝ ｛５＾｝ｆ＝１表示Ｗ个预测的集合 。 假设Ｗ  

大于图像中对象的数量 ， 也将ｙ视为大小为Ｗ的集合 ， 用 表示无对象）填充 。  

为了在这两个集合之间找到

一个二部图匹配 ， 可以寻找具有最低成本的ＪＶ个元  

素的排列 ：  

■ ＝ ａｒｇｍｉｎ

＾（ｙ， ，ｙａ（ ｉ ）） （２ ．６）  

其中 是真实？＾和秦引为吨）的预测之间的成对匹配成本 。  

在 ＤＥＴＲ中 ， 由于存在框的回归和类别的预测 ， 匹配成本考虑了类别预测和预  

测框与真实框的相似性 。 包括预测类别的成本

－ 和预测框的成本  

在给定匹配成本计算方法的前

１ 提下 ， 最优分配通过匈牙利  

算＾进行高效计算 。  

ＤＥＴＲ的训练通过基于匈牙利算法的匹配和传统目标检测的框回归损失 、  

目标分类损失 ， 从而保证了无需候选框的高效推理 ， 在推理速度上远远高于传  

统的Ｆａｓｔ ｅｒＲＣＮＮ 。  

２ ．３图像描述评估技术  

在自然语言处理领域 ， 评价两个句子相似度的常见指标包括ＢＬＥＵ 、 ＣＩＤＥｒ、  

ＲＯＵＧＥ、 ＭＥＴＥＯＲ等 。 这些评价方法主要通过ｎ元语法 （ｎ － ｇｒａｍ） 的视角来分  

析文本 ， 它依据ｎ的大小 ， 把连续的ｒｉ个单词视作

一个单位 ， 进而把文本理解为  

由这些ｒｉ元词组单元构成的序列 。  

１ ）ＢＬＥＵ  

ＢＬＥＵ在 ２００２年由 Ｐａｐ ｉｎｅｎｉ ［６Ｑ辱提出 ， 首次被应用于评估机器翻译模型的  

性能 。 该指标因与人类评价之间的高度相关性 、 低计算成本以及易用性而广泛  

用于评价各种文本生成任务 。 ＢＬＥＵ对传统的文本匹配精确度进行了优化 ， 通  

过不同的ｎ元语法提高评估的参考意义 。 同时 ， 为了解决文本过短带来的评价偏  

差 ， ＢＬＥＵ引入了简短度惩罚机制 （ＢｒｅｖｉｔｙＰｅｎａｌｔ ｙ，ＢＰ） 。  

２）ＲＯＵＧＥ  

ＢＬＥＵ算法主要评估生成文本中的词汇是否出现在参考文本中 ， 忽略参考  

文本中词汇在生成文本中的出现情况 ， 可能影响对生成文本质量的准确评估 。  

为了改善这个问题 ， Ｌｉｎ等人

［６ １ ］提出了ＲＯＵＧＥ算法 ， 通过比较机器生成的摘要  

一组人工编写的参考摘要 ， 并计算两者之间重叠的基本单元 （ｎ元语法 、  

２３  

北京邮电大学硕士学位论文  

词序列 、 词对 ） 的数量给出得分 。 ＲＯＵＧＥ实际上测量的是生成文本的召回率 ，  

更全面地评价了文本的质量。  

３ ）ＭＥＴＥＯＲ  

ＭＥＴＥＯＲ由Ｂａｎｅｉｊｅｅ等

［６２ ］提出 ， 旨在对ＢＬＥＵ指标进行改良 。 与ＢＬＥＵ不  

同 ， 后者只基于准确度评估并要求严格的 ｎ － ｇｒａｍ匹配 ， 未涉及语义相近词汇的  

匹配。 ＭＥＴＥＯＲ指标在评估过程中同时兼顾了精确率和召回率 ， 并对召回率赋  

予较高的权重 。 此外 ， ＭＥＴＥＯＲ通过考虑词干匹配和同义词匹配 ， 增强了其与  

一致性 。  

４）ＣＩＤＥｒ  

ＣＩＤＥｒ指标由Ｖｅｄａｎｔａｍ等人

［６３］提出 ， 专为图像描述领域的自动评估而设计 。  

ＣＩＤＥｒ通过应用 ＴＦ －ＩＤＦ（词频

－逆文档频率）加权机制 ， 对词汇进行差异化权重  

分配 。 这意味着 ， 在图像描述中频繁出现的词 ， 由于其较低的视觉信息贡献 ，  

会被赋予较低的权重 。 因此 ， ＣＩＤＥｉ

？在评价图像描述文本时 ， 能更加贴近人类  

的评价标准 ， 相较于ＢＬＥＵ和ＭＥＴＥＯＲ指标 ， 实现了更高的

一致性。  

２ ．４本章小结  

本章介绍了图像描述任务中涉及到的

一些相关技术 ， 首先介绍了目前图像  

描述中流行的 Ｔｒａｎｓｆ ｏｒｍｅｒ及相关的编解码器技术 ， 包括多头自注意力机制和位  

置编码机制 ， 接着介绍了图像描述中图像编码部分的技术背景 ， 包括基于卷积  

神经网络的和基于 Ｔｒａｎｓｆ ｏｒｍｅｒ的全局特征以及区域特征 。最后介绍了图像描述  

模型的评估方法。 为本文进

一步对图像描述任务展开研宄奠定了基础 。  

２４  

第三章基于编辑的非自回归图像描述方法  

第三章基于编辑的非自回归图像描述方法  

３ ．１概述  

图像描述任务致力于为输入的图像输出相应的描述性文本 ， 在现实世界中  

有着广泛的应用 ， 如无障碍 、 图文检索等 。

一些实时性的现实应用对图像描述  

的效果和效率都有很高的要求 。近年来流行的基于 Ｔｒ ａｎｓｆｏｒｍｅｉ

？的图像描述模型  

普遍应用自回归生成方法 ， 即每

一步都基于前ｎ － １步生成的单词生成第ｎ个单  

词 ， 导致了线性的时间复杂度 。 因此 ， 近年来出现了充分利用 Ｔｒａｎｓｆｏｒｍｅｒ的并  

行性实现并行解码的非自回归方法 ， 通过并行地生成整个句子 ， 可以达到更为  

优秀的时间复杂度 。 但是相比自回归的图像描述 ， 在并行生成时 ， 各个单词间  

没有依赖 ， 容易生成

一些重复或者错误的语法结构 ， 导致了生成质量的降低 。  

一些多步非自回归方案致力于通过隐式的掩码

－重预测结构 ， 多步地修改  

优化已经生成的句子 ， 但是现有的修改方法在灵活性上存在

一些限制 ， 导致修  

正错误又需要更多的步骤 ， 最终的推理速度提升有限 。 因此 ， 本文效仿人类高  

效的写作步骤 ， 提出了

一种基于编辑的非自回归图像描述模型 ， 本文提出的模  

型在支持了插入 ， 删除 ， 换位等编辑操作的基础上 ， 还创新性地将所有的序列  

一个解码器中 ， 称为统

一编辑非自回归图像描述模型 （Ｕｎｉｆ ｉｅｄ  

Ｅｄｉｔ －ｂａｓｅｄＮｏｎ －ａｕｔｏｒｅｇｒｅｓｓｉｖｅＩｍａｇｅＣａｐ ｔｉｏｎｉｎｇＭｅｔｈｏｄ ，ＵｎｉＣａｐ） 〇ＵｎｉＣａｐ  

可以更高效 ， 直接地在迭代修改的过程中完成图像描述的生成 ， 达到更好的效  

率和效果 。  

３ ．２ＵｎｉＣａｐ模型框架  

３ ．２ ． １框架总览  

本文提出的 ＵｎｉＣａｐ的整体结构如图 ３

－１所示 。 ＵｎｉＣａｐ遵循编解码器架构 ，  

在图像编码器部分 ， ＵｎｉＣａｐ采用了融合区域特征和全局特征的双路图像特征 。  

在文本解码器部分 ， ＵｎｉＣａｐ使用两个独立的解码器执行编辑操作 ： 统

一编辑器  

（Ｕｎｉｆ ｉｅｄＥｄｉｔ ｏｒ ， 简称ＵＥ）负责所有的序列编辑操作 （包括位置的插入 ， 删除 ，  

交换） ， 掩码预测器 （ＭａｓｋＰｒｅｄｉｃｔｏｒ ， 简称ＭＰ）则负责单词的生成 。 此外 ， 为  

了在图像描述任务上训练基于编辑的 ＵｎｉＣａｐ ， 本文还改进了自回归图像描述的  

两阶段训练策略。  

２５  

北京邮电大学硕士学位论文  

ｌｈｅ   臀 ａ 、ｗ？ｍａｎｍａ ｗｏｍａｊ ｉ ｉｎ ■

ｔｒｕｃｋ ＾

ｌｒｕｃｋ ；

．命 仰；

＂ ＜Ｍ＞＜Ｍ＞ｔｒｕｃｋ

＾Ｅ＞Ｊａ  

— １＾－４ － ０ Ｉ

ｃｆ ａＡ＾Ｔ第：步  

－ －ＯＴ７

，Ｚ－＾＜Ｖ＞Ｍ ｃ ｉｍａｎ ｉ？ｔｒａｃｋｒｒｄ＜Ｅ＞ ） ３

ｒＭ ｊ ｉＭＨＭ ｊ ｉＭ

：Ｉ ＿ｖ＾ Ｌ－ ｌ＾」 ＜ＭＰ

ｉ ＾＾ —  ■ 一  

－ａａａａｍ ｔｒａｉＶ ｆｅｅ ｆＳｇ

＾￣ ＜Ｍ＞＜Ｖ｜＞＜Ｍ＞＜＾１  

ｉ ： ｜｝ ： ｗｘ？ｎａＲ ｉｓｗｄ  ｔｒｊｄ ｔ ＝Ｅ 口Ｂ ， — 二＾二二 笛 １步  

丨一° 閑一丄统 一编辑器 （ｌｉＥ ＞ ；

：＿＾ｒ＾ｈ［ＴＩ 

， 二 … … ． ． — … ； ■ ■ ： ： ― … －  

Ｔ ＬＪ Ｉ

０Ｉ２３

？５０７８９ＵＭ ｌ统 一编辑器   飞 ｊＶ

＜Ｂ＞ ［＿）ｎＨ 丨

１ ［ ＡＢ＾ ＜Ｍ＞＾１  

ｊ ＩＡ

￣ ￣  卜

■？團々」办

ｌ＜Ｍ＞ Ｃ

－ ．Ｍ＞  

ｌ１ Ｇｒ ｉｄＩ 到 ２ ，禍 １ Ａ    Ｇ Ｉ Ｌ－ ％

￣ １Ｃ Ｊ ／ｙ／Ｚｍ  ｜

－？■重定位Ｃ到￣通入＾个

Ｍ到 ［３ ．４ ．５

ｊ翮插入  

ｒ －Ｈｇ头注意力

’  ｜ 下ｄ圆＾：  

－ ． ．—ｔ 」 位酿入 啦

１ １ １ １ Ｉ＼ｋ明繊  

ｆ ｃＬ？＾一        ＾ｈｇｓ

－ １本文提出的ＵｎｉＣａｐ模型的整体框架图  

３ ．２ ．２跨模态特征抽取器  

为了获取图像的特征表示 ， 本文采用了双路的跨模态特征抽取器 ， 同时提  

取区域和全局特征 。 首先 ， 本文采用了Ｌｉｕ等人

［５４ ］提出的 ＳｗｉｎＴｒａｎｓｆｏｒｍｅｒ作  

为骨干网络 ， 用于提取高质量的图像特征 ， ＳｗｉｎＴｒａｎｓｆｏｒｍｅｒ的具体技术细节在  

第二章中已经讨论 。  

一个大小为／ＧＲ ３ｘＨｘＭ／的图像输入／ ， ＳｗｉｎＴｒａｎｓｆｏｒｍｅｒ提取到的多尺  

度图像特征图可以表示为 ：  

ｆＣＪＬＪＬＣＪＬ＾ＣｘＭｃｘ＾ ｌ   Ｆ－

ｊ Ｆ ０ｅＲ ８ １６ １６

，Ｆ ２ ｅＲ ３２３２

，ｅＲ ６４６４ｌ（３ ． １ ）  

其中 ， Ｃ表示通道数 ， ／／表示图像的高度 ， Ｍ／表示图像的宽度 。 随后 ， 图像  

特征Ｆ被用于进

一步提取全局和区域特征 。  

在图像描述中 ， 全局特征可以通过对Ｆ进行更进

一步的特征计算获取 ， 本  

文通过在特征图的最后

一层特征巧上应用自注意力获取全局特征 ， 计算过程可  

以表不为 ：  

Ｇ＝ＦＦＮ

（ＭＨＡ （Ｆ ３ ，Ｆ ３ ，Ｆ ３ ）） （３ ．２ ）  

其中ＭＨＡ表ｔｋ多头注意力操作 ， ＦＦＮ表ｔｋＴｒａｎｓｆｏｒｍｅｒ中的前馈层 。  

对于目标检测级别的区域特征 ， 为了获取目标检测级别的特征 ， 本文采用  

了基于 Ｔｒａｎｓｆｏｒｍｅｒ的目标检测器ＤＥＴＲ ， 关于ＤＥＴＲ训练的技术细节已经在第  

二章中讨论 。 这里 ， 目标检测特征 的计算可以表示为 ：  

ｉ？＝ＦＦＮ

（ＭＨＡ （ａ？＾

． ，＾ ）） （３ ．３ ）  

其中 ， 〇

■表示 ＤＥＴＲ中的查询 ， 尽表示堆叠特征图Ｆ得到的多尺度特征图 。  

２６  

第三章基于编辑的非自回归图像描述方法  

得到两个图像特征ｇ和ｉ？后 ， 再结合输入句子的嵌入ｒ进行多头注意力计算 ，  

计算最终的跨模态表示＆的过程可以形式化为 ：  

＝ＭＥＡ（Ｔ ，Ｔ ，Ｔ）  

Ｃ ｒ ＝ＭＨＡ （Ｔ

，Ｒ ，Ｒ ）  

＝ＭＨＡ（Ｔ

，Ｇ ，Ｇ）  

Ｃ， ＝Ｃ ｒ？Ｓｉｇｍｏｉｄ

（［Ｃ ｒ ；Ｔ ］Ｗ Ｔｒ＋ｂＴｒ）

（３ ．４ ）  

ｌ＝ Ｃ ｇ？Ｓｉｇｍｏｉｄ（［ｑ ；ｒ

］＾＋ｂ Ｔｇ）  

Ｆ ｃ ＝ＬａｙｅｒＮｏｒｍ （Ｃ ｒ

－＋Ｃ ｇ

３ ．２ ．３统 一编辑模块  

在图像描述领域实现兼顾效率和效果的基于编辑的非自回归图像描述模型  

过程中 ， 本文面临的

一个主要挑战是 ： 直接

一一实现编辑操作的速度优势相比  

其它非自回归解码器并不明显 ， 因此需要实现更高效的编辑操作以提升算法的  

速度 ， 使其达到更好的综合表现 。 本文提出了基于统

一编辑操作的方式实现文  

本的生成 ， 包括

一个掩码预测器 （ＭａｓｋＰｒｅｄｉｃｔｏｒ ， 简称 ＭＰ）和 一个统

一编辑  

器 （Ｕｎｉｆ ｉｅｄＥｄｉｔｏｒ ， 简称ＵＥ） 。  

掩码预测器负责执行词预测任务 ， 可以被理解为

一个执行填空任务的双向  

语言模型 ， 在词表的预测空间内执行词预测任务 ， 在可以被形式化为 ：  

ｆＦ ｍ ＾Ｆ ｃＷ ｍ＋Ｂ ｍｎ＾  

［Ｐ ｕ ＝ Ｓｏｆｔｍａｘ （Ｆ ｍ ）

为了更详细地阐述本文提出的统

一编辑器 ， 本文将从人类编辑文字的过程  

和先前的相关工作出发 ， 讨论统

一编辑器的核心思想和面临的挑战 ： 首先 ， 通  

过对人类编辑文字的过程进行分析 ， 编辑过程可以被理解为由两个操作直接构  

成 ： 位置的插入和删除 。 Ｇｕ等人

［４８］最早提出了ＬｅｖｅｎｓｈｔｅｉｎＴｒａｎｓｆｏｒｍｅｒ， 其中  

ｌｅｖｅｎｓｈｔｅｉｎ表示

一种编辑距离 ， 说明模型是基于编辑操作的 。 如图 ３ －２所示 ，  

ＬｅｖｅｎｓｈｔｅｉｎＴｒ ａｎｓｆｏｒｍｅｒ由三个基于序列标注的解码器生成文本 ， 其中占位符插  

入器通过令模型预测在每个位置后边插入位置的数量来实现位置的插入 。 删除  

分类器通过令模型预测是否删除位置来实现位置的删除 。 最终 ， Ｌｅｖｅｎｓｈｔｅｉｎ  

Ｔｒａｎｓｆｏｒｍｅｒ 由负责序列编辑的两个分类器和

一个负责词预测的掩码预测器组成 ，  

通过以上三个部件的交替运行生成文本 。  

２７  

北京邮电大学硕士学位论文  

 ａｃａｔｓｅｔｏｎｔｈｅｍａｔ   — ：＾？？ ｜  

＇ ＾ｔ

１，６ＦｉＨ

－ｍＴｏｋ，為德： ， 、

＿＿＿  ．ｒ： ！参 ｉ －ｔａ

 ［ｒＡ １ Ｋ

：  ； ＾ ／

：  ｖｉ － ｉ 

－ｒｎａ： ？ 插入  

￣ ￣ ￣ Ｚ

￣ ￣ Ｚ＿  ＿ＺＺ—Ｈ

ｔ＜ｓ＞ ［ＰｌＫｃａ ｔ ｆＰＬＨ

ｊｐＬＨ ］ｐ －Ｈ ｊｍａｔ＜Ｊｓ＞ ｆ  

｜Ｔｒａｎｓｔｏｒｍｅｆ ８ｔ ｏｏＫ Ｌ Ｓ

＇ ／

？ｍ ｉ３］ｐｊ ｉ  

Ｉ Ｉｎｓｅｆ： ｉｉｉ

ｉＰ ｌａｃｅｈｏｄｓｎ：ｆＸ＿＿＿Ｊ 、  

｜  Ｌｅｖｅｎｓｆ ｔｔａｍＴｒａｉｓｆｃｎｎｅｆ ；

！Ｃ３ ｔｆＨ３ｔ  

ＩＴｒ ａｉｓｆｏｒｍｏｒ

Ｂｔ ｏｃｋ ２ ｝

ｉ 、￣？一 

Ａｉ — —＾  

？ ▲  

［［ Ｔ＾ｏｍ＾ｒＢＵｘ＊ １ｊ ；

＾＾＾＾  

  － ￣

＂  ＊ １ －   Ｄｅ ｌｅｔｅ Ｔｏｋｅｎｓ／ｘ＾  

加一 ［ｊ］由Ｌ ｊＪＬｉＪＥＢ ＜Ｌ１ ｍａ ．Ｘ ｃａｔｓ ｉｔｍａｔ  

－２ＬｅｖｅｎｓｈｔｅｉｎＴｒａｎｓｆｏｒｍｅｒ

］的模型与推理过程 ， 展示了基本的基于编辑的文本生成模  

型的工作原理  

在后续的工作中 ， Ｘｕ等人

［４９ ］又基于 ＬｅｖｅｎｓｈｔｅｉｎＴｒａｎｓｆｏｒｍｅｒ提出了可以包  

含编辑任务的重定位 （Ｒｅｐｏｓｉｔｉｏｎ ） 任务 ， 定义为 ：

“ 为每个位置预测最匹配的  

当前句子中的单词 ， 并删除预测到特定值的位置 。

” 重定位任务在兼容删除任务  

的基础上增加了人类修改句子时会用到的换位操作 ， 使得编辑的灵活度进

一步  

提高 。 但是 ， 不管是Ｘｕ等人的还是Ｇｕ等人的工作 ， 都存在着复杂度过高的问  

题 ， 分析如下 ： 在Ｇｕ等人提出的 ＬｅｖｅｎｓｈｔｅｉｎＴｒａｎｓｆｏｒｍｅｒ中 ， 三个序列标注任  

务里位置插入任务的预测空间约为 １０左右 （

一个位置后插入位置的数量是有上  

限的 ） 。 删除任务作为二分类的任务 ， 预测空间为 ２ 。 而单词预测任务则需要在  

词表上预测 ， 预测空间根据词表有所不同 ， 根据经验来说词表大小约为 １万

－５  

万 。 从这个对比可以看出 ， 如果将预测空间作为任务执行的难度来看的话 ， 删  

除任务 、 位置插入任务是远远比单词预测任务简单的 ， 但是在模型中 ， 这三个  

任务需要大小相同的模型执行三次 ， 因此存在效率上的

一些浪费 ， 可以说从效  

率角度来看是冗余的 ， 导致了基于编辑的文本生成虽然在生成质量上有

一些提  

升 ， 但是生成速度不太理想 。 Ｘｕ等人提出的重定位操作从预测空间上看也是

个简单的任务 ， 还是没有改变有两个任务的预测空间很小的问题 。 更进

一步地 ，  

这两个任务代表的都是对位置的编辑操作 ， 作为对比 ， 词预测任务执行的仅为  

掩码预测 。 通过某种方式统

一上述的两个编辑任务 ， 提出

一的编辑模型  

是改善上述问题的

一条可行路径 ， 可以提高本文研究的基于编辑的非自回归图  

像描述模型的效率 。 基于这种思想 ， 本文提出了

一种将所有编辑操作统

个解码器中的统

一编辑器 。  

一编辑器执行的编辑操作可以定义为 ：

“ 为每个单词预测

一个新的位置 。

值得注意的是 ， 本文定义的统

一编辑操作看似与 Ｘｕ等人提出的重定位操作  

“ 为每个位置预测

一个新的单词 。

” 比较相似 ， 但是在本质上是不相同的 ， 最大  

２８  

第三章基于编辑的非自回归图像描述方法  

的区别在于 Ｘｕ等人提出的重定位操作无法实现句子的变长操作 。 因此 ， 统

编辑器的输出可以通过如下的逻辑理解为位置插入 ， 删除和换位三种操作 ：  

（ １ ） 插入操作 ： 对于将每个单词放置到对应位置后留下的空位 ， 用掩码  

［Ｍａｓｋ ］填充 。  

（２ ） 删除操作 ： 额外定义如果单词预测到的对应位置为 ０ ， 则视为删除 。  

（３ ） 换位操作 ： 对于所有单词 ， 将它们放置到统

一编辑器预测的位置上 。  

－３展现了本文提出的统

一编辑操作相比先前的编辑操作的不同 。  

产０１ ２３ ４ ５ ６７８９１０Ｉ Ｉ   奴严

 ｜Ｍ Ｉ ｜

Ｉ ｜ ＪＭ Ｉ Ｉ

、Ｂａｖ不作操作 ｜  

丄ａｍ ＼ ＼ ＼

麵立置 ｉ ］  

本文提出的统

一编辑器 ＪｌＭａＬｕ ！ ｌｃ＾Ｂｒ■ Ｉｎ 幽獅獅 丨  

－」ｃＷｍ／ｍ

￣？Ｈ把ｃ獅ｊｇ， （縣于在 难入１１圍１  

ｊ ｜齲除Ｄ

位盪特径 ＜ＥＯＳ＞

１ １ １ｎｗ放到 （等价于在７ 插入位藍 ） Ｉ  

ｒ＾ＡＢＣＤ＾   奴

丨 ［Ｊ 丨

１１无操作 丨  

１＿卜画茌髓

Ｘｕ等人提出的换位操作 — ｉＭａｌｉｕ ｌ Ｉｃｒ＞ ２團

￣ 一 在赚放适Ａ

３ＩＢ —■汪

４ｎｗ翮在腿４触ｒ＞ １   ―姐 ５

丨 丨 １ １ｎ

ｉ Ｈ１无揆ｑ  

－３本文提出的统 一编辑器与现存的换位操作的直观比较  

－３所示 ， 统

一编辑器的优势在于在计算将单词放置在哪个位置的时  

候 ， 可以假设有比当前文本长度更多的位置 ， 并通过计算与这些位置的相似度  

来实现将单词放到更后边的位置 ， 从而使得统

一编辑器支持了文本的变长 ， 从  

一了原有的位置插入操作 。  

０１ ２３４５６０１２３４５６０１ ２３４５６   ■ 叫Ｕ Ｉ

Ｉ Ｉ ＩＵ

１ １ １Ｈ

！ＵＭＭ Ａ■ 

Ｉ■ Ｉ Ｉ■ ＿ ］

１＾  

＜ｅ〇ｓ＞ ！ ■

－ｍ １Ｍｌ   ｘＶ  

－４冲突问题及其解决  

然而 ， 为了实现统

一编辑器 ， 需要克服统

一编辑器定义中存在的预测冲突  

问题 ， 即在统

一编辑器中 ， 根据定义 ， 不同单词的位置预测不能相同 。 但是由  

一编辑器执行的是并行的分类任务 ， 位置与位置之间的预测结果相对独立 ，  

导致会出现不同单词被预测到相同位置的不合法情况 。 因此在遇到冲突的时候 ，  

２９  

北京邮电大学硕士学位论文  

需要将预测位置重叠的单词通过后处理错开 ， 使其变得合法 。冲突问题及其修  

正思路如图 ３ －４所示 。  

实现如图所示解决方案的

一个简单方法是直接遍历每

一个样本 ， 每当检测  

到冲突 ， 就保留冲突的第

一个位置 ， 并将所有预测位置在这个位置之后的单词  

一格 ， 遍历

一遍样本后 ， 可以解决全部的冲突问题 。 但是以上的解决方法  

考虑的是单个样本的情况 ， 在实际的应用中 ， 上述的冲突消解算法的实现存在  

两个会影响效率的问题 ： （ １ ）通过遍历的方式处理单个样本复杂度高 。 （２ ）深  

度学习的训练和推理往往采用批处理的方式 ， 即同时处理多个样本 ， 简单遍历  

在多个样本之间无法并行 。  

为了改进冲突消解的效率以保证统

一编辑器的速度 ， 本文提出了

一种矩阵  

版本的冲突消解算法 ， 矩阵版本的冲突消解算法遵循如下步骤 ：  

（ １ ） 对预测结果Ｍ排序 ， 并保留原顺序的下标 。  

（２ ） 对排序后的预测结果应用差分得到ＭｄＷ

（３ ） 构建

一个新的矩阵Ｍ

， 规则为 ： 将峋 ￡／／中等于 ０的值设为 １ ， 其他  

值设为０。  

（４） 求Ｍ

＇ 的累积总和值矩阵  

（５ ） 即为消解冲突所需的偏移量 ， 将 累加到对应位置的  

预测值即可完成冲突的消解 。  

算法 ３ － １冲突消解算法  

输入 ： 非法预测结果Ｍ  

输出 ： 合法预测结果  

１ 排序结果Ｍｓａｒ

＞ ｔｅｄ ， 排序下标ｉｄｘ ＜ － ｓｏｒｔ（Ｍ）  

２Ｍｄｉｆｆ差分（Ｍｓｏｒｔｅｄ）  

— 设置 值为 １ ， 非０值为０  

４Ｍｃｕｍｓｕｍ— 累积总和（Ｍ）  

５Ｍｐａｔｆｄ ＜ －根据ｉｄｘ将Ｍｅｕｍｓｕｊｎ累加回Ｍ  

６返回  

本文提出的冲突消解算法最终的形式化表达如算法 ３ －１所示 ， 为了方便理  

解 ， 给定算法 ３ － １ ， 图 ３

－４中所示冲突的计算过程如图 ３ －５所示 ： 计算得到排序  

矩阵的差分财＾＾后 ， 可以发现 ， Ｍｄ〇７中为 ０的部分代表排序后相邻的值相同 ，  

即存在冲突的位置 ， 随后 ， 为了避免使用遍历的方式实现右移操作 ， 采用矩阵  

累计总和操作 （Ｃｕｍｓｕｍ）直接计算到每个位置对应的偏移量 ， 从而直接与原  

矩阵Ｍ相加得到移动后的合法输出Ｍｖａ？ｄ 。 以上的冲突消解算法保证了统

一编辑  

器高效的运行效率 。  

３０  

第三章基于编辑的非自回归图像描述方法  

０Ｊ ２３４５ 

６０１２３４５６０１２３４５６  

ｂｉｙｍ Ｌｉ

ｉ ｎｉｎｉｒｚｉｘ］  

Ｉ Ｍｌ  

—  ｊｇ ？ ＾

〇ｚｚｊｔｚｚ二ｎ ｎ＊  

＜￡〇ｓ＞ ｉ ｌｌ Ｂ

Ｍｓｏｒｔｃｄ：［〇

， ｉ ，３

： １排序结果Ｍｓｏｒｔｅｄ ， 排序下标 ｉｄｘ— ｓｏｒｔ（Ｍ）  

Ｍ对厂 ［ １ ，０

２Ｍ时广差分（Ｍｓ＿ｄ）  

：［０ ， １ ，０ ， １

，〇 ］ ：３设置 中０值为ｉ ， 非０值为０  

Ｍｃｕｎ＾ｕｍ

，２ ，２ ］ ；４Ｍ ｃｕｍｓｕｍ卜累积总和（Ｍ ）  

：［０ ， １ ，２

，６ ］ ｉ ５Ｍ— ｄ卜根据 ｉｄ尤将Ｍｃｕｍｓｕｍ累加回 ｉＷ  

图 ３ －５本文提出冲突消解算法的运行过程  

算法 ３ －２基于编辑的图像描述推理  

输入 ： 跨模态特征表示巧  

输出 ： 图像描述Ｃ  

１ 初始化图像描述Ｃ＝ ［＜ＳＯＳ＞ ，＜ＥＯＳ＞ ］  

２ｆｏｒｉｉｎｍａｘ

＿ｓｔｅｐ  

３ ｜Ｃ＝ＵＥ （Ｃ）  

４ ｜ ｉｆ

［ｍａｓｋ ］

ｉｎＣ  

５ ｜ ｜Ｃ＝ＭＰ（Ｃ）  

６ ｜ｅｎｄ  

ｉｆＣ无变化  

８ ｜ 丨提前结束循环  

９ ｜ｅｎｄ  

１０ｅｎｄ  

１ １ｒｅｔｕｒｎＣ  

最终 ， 统

一编辑器ＵＥ可以被形式化为 ：   飞 ＝

［£ 难＋足  

Ｐ ｕ ＝Ｓｏｆ ｔｍａｘ

（Ｆ ｗＦ Ｔ

（３ ．６）  

其中％和礼表示线性层的参数 ， 尽表示由 ３ ． １ 中讨论的跨模态特征抽取器  

所提取的特征 ， ｒ表示词嵌入 。 ［Ｆｅ ；ｎ表示连接操作 ， 即将向量巧和ｒ在最后

维拼接 。  

３ １  

北京邮电大学硕士学位论文  

最后 ， 如图 ３

－１右上角所示 ， 算法 ３

－２介绍了如何联合使用统

一编辑器和掩  

码预测器执行从零开始的图像描述生成过程 ， 可以实现动态结束编辑过程 ， 并  

且在不需要的时候不进行掩码预测操作 ， 支持了高效的图像描述生成 。  

３ ．２ ．４两阶段训练方法  

ＵｎｉＣａｐ作为基于编辑的非自回归图像描述模型 ， 传统的图像描述训练交叉  

－强化学习两阶段训练方法并不能直接应用 ， 总的来说 ， ＵｎｉＣａｐ在训练中面  

临两个主要的问题 ：  

（ １ ） 交叉熵训练阶段

－编辑操作标注问题 ： 当前流行的图像描述数据集主要包  

含图片及其对应的图像描述 ， 但不包含编辑操作的相关标注 。 而交叉熵  

训练依赖编辑操作的数据以训练统

一编辑器和掩码预测器 。 因此需要设  

计方法从现有数据中生成合适的编辑训练数据 。 以便在不改变原始数据  

集结构的前提下 ， 合理生成编辑操作的训练样本 。  

（２） 强化学习阶段

－强化学习方法的适用问题 ： 在非自回归模型的图像描述任  

务中 ， 由于文本解码方式的多样性 ， 强化学习的应用变得复杂 。相比之  

下 ， 自回归图像描述模型由于其解码过程的

一致性 ， 可以采用统

一的强  

化学习流程 。但在ＵｎｉＣａｐ等基于编辑的模型中 ， 由于解码方式的主要研  

宄内容是多样化的 ， 导致缺乏

一个通用的强化学习方法 。 因此需要为  

ＵｎｉＣａｐ设计

一个适应编辑操作过程又能符合强化学习理念的算法。  

本文在交叉熵阶段和强化学习阶段均提出了有效的改进方法 ， 分别是多路  

径交叉熵训练方法和阈值采样的强化学习训练方法。  

３ ．２ ．４ ．１多路径交叉熵训练方法  

在交叉熵阶段 ， 为了解决编辑训练数据的问题 ， 考虑到编辑本质上就是

种对于文本的修改 ， 因此可以通过加噪的方式模拟这种修改 。 本文提出了

一种  

多路径交叉熵训练方法 ， 通过向数据集中的图像描述加噪的方式生成训练数据 。  

本文提出的多路径交叉熵训练方法创建了

一个噪声生成器 对输入的文  

本进行加噪 ， 加噪的方式包括文本重排序 ， 随机删除 ， 随机插入等 。 在每

一步 ，  

给定真值文本 本文设计三种路径来利用 和（Ｘ生成训练时的输入数据 ：  

在路径 １ 中 ， 将带有噪声的文本（Ｔ＆ｔ）直接输入到统

一编辑器ＵＥ或掩码  

预测器ＭＰ中 ， 而真值 作为交叉熵优化的目标 。  

在路径 ２中 ， 将带有的带噪声文本＜７（讲）输入到ＭＰ中 ， 得到ＭＰ的输出结  

果ＭＰＯ＆ｔ）） 。 然后 ， 将 用作ＵＥ的输入 。  

在路径 ３中 ， 直接将真值用作ＵＥ的输入和目标 ， 期望在这种情况下不进  

行任何修改 。  

３２  

第三章基于编辑的非自回归图像描述方法  

＃１Ｐａｔｈ＃２Ｐａｔｈ＃３Ｐａｔｈ  

？ｃｔｇｔａ ［ｇｔ）七 （７ｇｔｇｔ  

ｉｉ１  

ｆｕＥ］ Ｉｍｐ］ｆｕＥｌＦｕｅ

！   ＴＴＴＴ  

ｇｔｇｔｇｔｇｔ  

｜ 噪音生成器 ｜

－？ 训练输入

丨＾＞ 训练目标  

－６多路径交叉熵训练中三个路径的训练流程  

三个路径的可视化如图 ３

－６所示 ， 其中 ， 路径 １ 的作用是独立训练 ＵＥ和  

ＭＰ ， 诉加噪后直接作为ＵＥ和ＭＰ的输入 ； 路径 ２的作用是借助ＭＰ对抗训练  

ＵＥ ， 诉随机加入掩码噪声后 ， 首先作为掩码预测器ＭＰ的输入 ， 得到基于模型  

预测的噪声ＭＰ〇（５ｔ））

， 相比随机插入单词 ， 考虑模型生成单词作为噪声 ， 难  

度更大 ， 可以更好地训练 ＵＥ修改句子的能力 。 路径 ３的作用则是作为

一种正  

则化项 ： 在路径 ３ 中 ， 输入和输出的句子是

一致的 ， 这实际上是在要求 ＵＥ对  

输入的句子不做修改 ， 这与本文在 ＵＥ 中采用的动态退出机制是相关的 ， 可以  

使ＵＥ对于质量高的句子停止修改 ， 从而进

一步地提升推理的速度 。  

在训练时 ， 三个路径的选择遵循多项式分布 ， 因此 ， 多路径交叉熵训练可  

以形式化为 ：  

Ｉｎ证 ＝ｃｒ （ｇ ｔ）ｌ（ｊ ＝１）＋ＭＰ （ｃｒ （ｇ ｒ ）） ｌ （ｊ ＝２ ）＋

ｇ ／Ｉ （７ ＝３ ）  

其中Ｉ表示指示器函数 ， 对于Ｈ（／ ＝ｎ）

， 当且仅当 ｉ ＝＝

＾／时 ， ｎ（／ ＝ｎ）＝ｌ ，  

否则为 ０ 。 在本文提出的多路径交叉熵训练方法中 ， ｊ取 ［ １ ，２

，３ ］ ， 取每个值的概  

率服从多项式分布 ， 多项式分布的参数是超参数 。  

最终 ， 交叉熵阶段的训练损失可以表示为 ：  

ｒ－ｒ＋ｒ   ＸＥＵＥＭＰ  

＾ ｘｊｅ

ｌｏｓ （Ｆ （Ｐｏｓ

ｆ ： Ｉ５ ） ） （３ ．８）  

ｋ＝ｌ   Ｈ

１〇ｇ（ｐＫ

丨 ｓ ））  

Ｊｔ＝ｉ  

３３  

北京邮电大学硕士学位论文  

其中 ， 表示统

一编辑器中的目标位置 ， 表示掩码预测器中的目标单  

词 ， ５＝ ｛＞（） ， １４／１， ． ． ． ， １＾｝表示输入的序列 ， １

（ （ ：是指示器函数 ， 在 １＾为 １＞仏８１（＿ ］时  

取 １ ， 用于限制只在有［ＭＡＳＫ ］的位置上计算损失 。  

额外地 ， 给定了训练的输入句子和目标句子后 ， 需要根据输入句子和输出  

句子生成对应的统

一编辑标签 ， 即在输入句子上采用什么样的统

一编辑操作可  

以得到目标句子 ， 出于统

一编辑操作的定义 ， 这个问题可以抽象为 ： 为所有输  

入句子中的单词找到

一个目标句子中的位置 。本文提出

一个基于队列的统

一编  

辑标签生成算法 。 如算法 ３ －３所示 ， 通过队列可以实现从输入句子到输出句子  

的位置对齐。  

算法 ３ －３统 一编辑标签生成算法  

输入 ： 输入图像描述Ｃ ｉＴｌ ， 目标图像描述  

输出 ： 编辑标签Ｅ  

１ 为每个扒中的单词建立队列 ， 得＜３卜 ［

，…  

２ｆｏｒｗｏｒｄｉｎ ｇｔ  

丨 入队当前位置  

４ｅｎｄ  

５ 初始化空列表Ｆ  

６ｆｏｒｗｏｒｄｉｎＣ ｉｎ  

７ ｜ ｉｆ ｗｏｒｄｉｎ  

８ ｜ ＼ｐｏｓ＜ｒ －ｌＨＰＡｉＱｗ〇ｒｄ）  

丨 ｜将ｐｏｓ插入到￡  

１０ ｜ｅｎｄ  

１ １ｅｎｄ  

１２ｒｅｔｕｒｎＦ  

３ ．２ ．４ ．２阈值釆样的强化学习训练方法  

在图像描述训练中 ， 强化学习方法用于直接对不可微分的评估指标进行优  

化以解决评估指标不可微分的问题 ， 基于策略梯度算法 ， 根据评估指标建立对  

梯度的估计以优化模型 。本节中 ， 本文将首先引入图像描述中用到的策略梯度  

算法。  

策略梯度算法

［６４ ］最早提出于强化学习领域 ， 当时强化学习遇到的问题是基  

于值 （Ｖａｌｕｅ －ｂａｓｅｄ） 的强化学习算法 （以 Ｑ学习为代表 ）遇到了

一系列瓶颈 ，  

包括连续动作空间和随机策略等 。 因此人们设计了基于策略 （Ｐｏｌｉｃｙ

－ｂａｓｅｄ） 的  

策略梯度算法 ， 用于解决动作空间过大的问题中基于值的强化学习算法难以建  

模的问题 。策略梯度算法的思想及其推导如下 ：  

３４  

第三章基于编辑的非自回归图像描述方法  

设策略为 ７Ｔ０

一组由策略 ７Ｔ０生成的状态 Ｓ －行为 ￡１序列  

（Ｓｏ ．ａｏ＾！ ，％力 ，屮） ， 对于ｔ ， 可以计算它的奖励／？〇〇 ， 则策略的价值可以  

表示为 ：  

Ｊ｛７ｔ ｅ） ＝Ｅ ［ｉ？（ｒ）］ （３ ．９）  

ｒ －７ｔＱ  

使用梯度下降的方式优化７Ｔ０ ， 则可推导梯度的计算公式如下 ：  

Ｖ ｅＪ｛７ｔ ｅ） ＝Ｖ ，Ｅ ［ｉ？（ｒ）］   卜艽Ｑ  

＾Ｖ ｄＰ（Ｔ

＼９）Ｒ（ｒ） （３ １〇）  

Ｙ ，Ｐ（ｒ

ｌ〇）ＶＪ〇ｇＰ（Ｔ

＼０）Ｒ（ｒ）  

Ｔ ？芘ｅ  

＝ Ｅ ［Ｖ ｅ ｌｏｇＰ（Ｔ

＼０）Ｒ（Ｔ）］  

其中利用到了变换Ｖｊ（ｒ

丨沒） ＝

／ ？（ ：

丨沒） ， 对  

Ｐｉｊ Ｉ  

）ｇＰ〇 ｒ

｜的 ， 又有 ：  

ＶＪｏｇＰ（ｒ  ｜＆） ＝ Ｖ＾ｏｇ［Ｐ（５ ０）ｆｊＰ （５

ｉ＋！ ｜ａ＾ｓ＾ ｉａ

， ｜ｓ ；）］  

１＝０  

＝ Ｖ ｆ ｌ ／ｏｇＰ（５ ０）＋

＾ ［Ｖ ｅ ／ｏｇＰ（

．＾ ．Ｏ＋Ｖ＾ｏｇ＾Ｃａ

． ）］ （３ ． １ １）  

ｉ＝０  

＝ｊｙｊ〇ｇ％（ａ

ｉｈ）  

／＝０  

代入式（３ ． １ １）到式（３ ． １０） ， 可得 ：  

Ｖ ，Ｊ（＾） ＝Ｅ［Ｘｖｊｏｇ＾ｒＭ ｜Ｓ

ｉ）Ｒ（ｒ）］（３ ． １２）  

强化学习的目标是使Ｊ（＆）最大化 ， 因此使用 ？／（＆）的梯度进行梯度上升 ，  

也就是说 ， 损失函数是

－／（％） ， 即 ：  

— １  

Ｌ｛ｎ０） ＝－Ｅｌｏｇｎ ６（ａ

ｔ ＼ｓ

，）Ｒ（ｔ）］（３ ．１３）  

其中 ， 巧 就是每

一步通过网络计算出的概率 ， 而表示奖励越大 ， 梯  

度越大 ， 直觉上可以将奖励看作对于梯度的

通过策略梯度算法 ， 可以根据评估指标得到

一个对梯度的估计 ， 从而

一定  

程度上解决了不能通过评估指标得到梯度的问题 。 在实际的图像描述应用中 ，  

３５  

北京邮电大学硕士学位论文  

由于生成空间很大 ， 根据策略梯度算法计算全部的Ｔ是不可能的 ， 因此结合了蒙  

特卡洛采样的 ＲＥＩＮＦＯＲＣＥ算法

［６５］被提出并用于解决策略梯度的实际计算问题 。  

ＲＥＩＮＦＯＲＣＥ算法是策略梯度算法的

一种变形 ， 将公式中的期望改写成了采样  

均值形式 ， 如下式所示 ：  

＝ － ７７Ｘ＆ ｌｏＳ＾Ｍ

Ｉｓ＾Ｒｉｒ）］（３ ． １４）  

ＲＥＩＮＦＯＲＣＥ算法利用了蒙特卡洛采样的思路 。 根据蒙特卡洛采样的理论 ，  

可以证明 ， 在采样次数足够大时 ， ＲＥＩＮＦＯＲＣＥ算法和策略梯度算法拥有同样  

的期望 。 ＲＥＩＮＦＯＲＣＥ算法还额外地引入了

一个基线６以降低强化学习训练中由  

于采样存在的高方差问题 ：  

Ａ？） ＝ －去Ｚ的⑷？）（聊）

－＊））（３ ． １５）  

Ｔ ￣７Ｚｄ ／＝０  

其中 ， ６在当前的图像描述实践

一般取当前批次的评估指标的平均值 ，

一般  

认为 ， 引入基线可以使得梯度的方差更小 ， 模型的训练效果更好 。 先前的工作  

证明 ， 引入的基线Ｚ）越接近真实奖励 ， 方差就越小 。 同时 ， 为了保持结果的无偏  

性 ， ｂ需要与模型的参数无关 ， 即计算６的时候不能涉及模型参数 。  

对以上策略梯度公式的直观理解是 ： 将不可微的奖励值直接乘以输出中每  

个单词的生成概率作为损失 。 然而在实践中 ， 本文发现很多单词实际上是以很  

高的概率被选择的 ， 针对这种情况 ， 本文对单词的采样概率进行了分析 ， 使用  

模型对训练集中所有的图像进行

一次推理 ， 并统计每个单词被选择的概率 ， 通  

一个置信度得分 设Ｐ为单词被预测的概率 ， Ｓ是待计算的得分 ， 定  

义得分Ｓ为 ：  

ｉｆｐ＞０ ．７  

ｓ＝＜ｌｏｇ（ ｌ

－ ＾） （３ ． １６）  

－ ｐ ｉｆ； ？＜０ ．７  

其中 ， 当ｐ＞０ ．７时 ， 使用倒数和对数的意义是建模当概率接近 １时的指数关系 。  

最终的统计结果如图 ３

－７所示 。  

— １４ ． １％ １置信单词８５ ．９％  

非置信单词ｏｆ ｜ａ Ｉｔｈｅ Ｉａｎｄ Ｉｏｎ Ｉｔｏ Ｉｉｎ ｜ｔｅｎｎｉｓ Ｉｗｈ ｉｔｅ ｜ｐｅｏｐ

ｌｅ    ．   —  

ｌｏｖｅｓ  ｜ Ｗｅｎｄｅｒ

 ｜ ｔｉｒｅ ｜

ｔｖ ｜ｐａ ｉｒｓ ｜ ｃｕｔｔｅｒ ｜ｐｏ ｌ ｉｃｅ  ｜ ｖａｒ ｉｅｔｙ ｜

ｔｏｙｓ ｜ｆｅｎｃｅ  

－７关于单词概率的统计结果图  

３６  

第三章基于编辑的非自回归图像描述方法  

通过统计可以发现 ， 模型以非常高的概率生成 ａ ，ｔｈｅ ，ａｎｄ等单词 ， 但是在  

面对 ｇ ｌｏｖｅｓ ，ｂｌｅｎｄｅｒ等名词时往往会以较低的概率选择 ， 即模型在生成这些词  

” 的 。 从生成单词的分布看 ， 其中 ８５ ．９％的单词都是置信单词 。  

根据统计数据可以得出结论 ： 训练时大部分时间模型都以在以较高的概率生成  

与图像关系较小的单词 ， 由于这部分单词的占比很大 ， 而且生成这些置信单词  

的 ｌｏｇ概率较小 ， 因此在训练时对考虑这些单词是低效的 。为了提升训练的效  

率 ， 本文提出了阈值采样强化学习策略 ， 即在训练时过滤掉置信单词 ， 只对非  

置信单词对应位置的结果应用策略梯度算法 ， 计算梯度 。 提升训练的效率 。 同  

时 ， 对于自回归方法来说 ， 计算完整句子的奖励是

一个耗时的多步决策过程 。  

因此 ， 考虑到非自回归ＵｎｉＣａｐ中推理是并行的 ， 可以以非常低的成本生成大量  

样本并同时反向传播 ， 降低ＲＥＩＮＦＯＲＣＥ算法的方差 。  

阈值采样强化学习策略的梯度计算可以表示为 ：   ＾ｍｎ）

￣ｂ）＾ｅ ｌ〇ｇ＾ ［ ｕ 〇 ｎ

Ｉ＼）ｙｎ ｅ｛ｕ ａ ｎ ）＜ａ（３ ．１７）  

其中ｎ代表同时采样的数目 ， Ｖ７ｒ０（ｕａ？）＜ａ表示计算梯度时只考虑概率小于  

ａ的位置 ， 在本文的实验中 ， ｎ取 ５ ， ａ取 ０ ．７ 。  

３ ．３实验  

３ ．３ ． １数据集介绍  

本文使用 ＭＳＣＯＣＯ数据集

［６６ ］作为主要的实验和验证数据集 。 ＭＳＣＯＣＯ  

一个大型的图像数据集 ， 由微软的研宄团队提出 ， 其中包含了超过数  

十万张各种分类的图片并附带了各种任务所需的标注信息 ， ＣＯＣＯ数据集自提  

出以来就因其大规模且完善的各类标注成为图像识别领域最受关注的数据集 ，  

广泛用于图片分割 ， 目标识别 ， 图像描述等各种领域 。 数据集网站为  

ｈｔｔｐｓ ：／／ｃｏｃｏｄａｔａｓｅｔ ．ｏｒｇ ／。共包含 １６ ．４万张图片 ， 其中 ８ ．３万张作为训练集 ， 分别  

有 ４ ． １万张图片作为验证集和测试集 ， 对于图像描述任务 ， 每张图片有 ５句人  

工标注的英语描述。  

需要说明的是 ， 在图像描述领域 ， 为了防止对训练集的划分影响训练的结  

果 ， 比如在早期有人因为随机划分了训练集而不能复现出应有的结果 ， 所以相  

一部分的成果都遵循了Ｋａｒｐａｔｈｙ等人Ｗ基于 ＣＯＣＯ数据集划分的训练

验证集以保证成果的可复现性 。 本文将遵循图像描述领域的通常做法 ， 采用  

Ｋａｒｐａｔｈｙ分割 。  

３７  

北京邮电大学硕士学位论文  

３ ．３ ．２实验配置  

本次实验的硬件环境为 Ｕｂｕｎｔｕ２２ ．０４操作系统 、 ＮＡＶＩＤＩＡＡ６０００显卡 、  

Ｐｙｔｏｒｃｈ深度学习框架 、 Ｐｙｔｈｏｎ３ ．８版本 。  

实验中采用的 Ｔｒａｎｓｆ ｏｍｉｅｒ架构遵循原始方法的模型超参数设置 。 多模态特  

征抽取器中的ＤＥＴＲ网络和 Ｓｗｉｎ骨干网络的权重由Ｎｇｕｙｅｎ等人

［６８］提供 。 在对  

骨干网络进行消融时 ， 采用 Ａｎｄｅｒｓｏｎ等人

［ １３］提出的自底向上目标检测特征和  

Ｚｈａｎｇ等人 提出的图像全局特征 。 对于词嵌入和模型中间维度的设置 ， 词嵌  

入的维度设置为 ５１２维 ， 中间层的向量维度设置为 ５１２维 。  

本文采取 Ｄｒｏｐｏｕｔ机制以缓解模型的过拟合问题 ， 词嵌入层的丢弃率设置  

为 ０ ．９ ，Ｔｒａｎｓｆｏｒｍｅｒ块的丢弃率设置为 ０ ．７ 。 所有的模型权重在初始化时遵循均  

匀分布 。  

本文采用 ＡｄａｍＷ作为模型的优化器 ， 对于两阶段训练 ， 在交叉熵优化阶  

段训练 ３０个 ｅｐｏｃｈ ， 学习率设置为 ｌｅ

－５ ， 并采用了ｗａｒｍｕｐ机制以保证训练的  

稳定性 。 在强化学习阶段训练 １０个 ｅｐｏｃｈ ， 学习率设置为 ｌｅ

－６。对于本文提出  

的多路径交叉熵训练方法 ， 三条路径的概率设置为［０ ．５ ，０ ．４ ，０． １ ］ 。 对于本文提出  

的阈值采样强化学习技术 ， 阈值设置为０ ．７。  

生成质量评估方面 ， 对于生成质量 ， 采用学术界常用的图像描述评估指标  

进行评估 ， 包括标准的评估指标ＣＩＤＥｒ ， ＭＥＴＥＯＲ ， ＢＬＥＵ ， ＲＯＵＧＥ等 。  

对于推理速度 ， 采用加速率作为指标进行评估 ， 加速率定义为相比于没有  

任何优化的Ｔｒａｎｓｆｏｒｍｅｒｄｅｃｏｄｅｒ的推理速度倍率 。  

由于非自回归图像描述的很多历史工作未开源 ， 且它们的实验设置都是  

１０８０ｔ ｉ ， 为了进行公平比较加速率 ， 本实验在配备

一个 １０８０ｔ ｉＧＰＵ的ＧＰＵ服务  

器上评估ＵｎｉＣａｐ的加速率 。报告的加速率指标是五次运行的平均值 。  

３ ．３ ．３基线方法  

本实验选取了２２个代表性的图像描述模型作为基线与ＵｎｉＣａｐ进行比较 ，  

根据文本生成方法区分 ， 可以分为自回归方法和非自回归方法 ， 简介如下 ：  

３ ．３ ．３ ． １ 自回归方法  

（ １ ）Ｍ２ －Ｔｒａｎｓｆｏｒｍｅｒ ［２１ ］提出了网状 Ｔｒａｎｓｆｏｒｍｅｉ

， 提升图像和文本特征的  

交互 ， 使得提取特征的表现更好。  

（２）Ｘ －Ｔｒａｎｓｆｏｒｍｅｒ ［７（）］提出

一种基于双线性映射的Ｔｒａｎｓｆｏｒｍｅｒ结构 ， 利用  

了空间和通道双线性注意力来建模图像文本的多模态交互 。提高多  

模态特征抽取的效果 。  

（３ ）ＤＩＣＴ ［７ １］提出同时使用全局特征和目标检测特征 ， 并提出

一种局部约  

束的交叉注意力以解决直接融合两种特征导致的问题 。  

３８  

第三章基于编辑的非自回归图像描述方法  

（４）ＤＲＴ

［７２］提出了

一种新颖的方向关系 Ｔｒａｎｓｆｏｒｍｅｒ， 以提高图像字幕中  

视觉对象之间方向感知能力 ， 并且设计了高级相对方向类别 ， 其由  

预定义的单位方向向量与从区域对中心坐标计算得到的标准化向量  

之间的余弦相似性确定 。  

（５）ＤｅｅＣａｐ

［７３］提出了用于高效图像描述的自回归 ＤｅｅＣａｐ框架 ， 该框架  

从全局视角动态选择合适大小的解码层以早期退出 。 从而在牺牲少  

量准确性的情况下显著减少计算成本 。  

（６）ＰＴＳＮ ［７４］尝试对概念的层次语义信息进行建模 ， 将其称为树结构原型  

（ＴＳＰ）。提出了

一个渐进聚合 （ＰＡ）模块 ， 以粗糙到精细的方式 ，  

使视觉网格特征从树结构原型中捕捉到语义信息 。  

（７）ＬＳＴＮｅｔ ［７５聰出

一种新颖的局部敏感 Ｔｒａｎｓｆｏｒｍｅｒ网络 （ＬＳＴＮｅｔ） ， 分  

别从层内交互和层间融合两个方面加强了局部建模 ， 以感知对象级  

信息 。  

（８ ）ＳＣＰ －ＷＧＣＮ

［７６ ］提出了

一种结构化概念预测器 （ＳＣＰ） ， 将概念预测集  

成到端到端的图像字幕生成中 ， 而且根据单词之间的依赖关系预测  

获概念结构 。  

（９）ＶｉｎＶＬ ［７７ｌ是早期的图像描述预训练模型 ， 进行了

一项全面的经验研  

宄 ， 以证明在视觉语言模型中视觉特征的重要性 ， 并提出了

一种新  

的目标检测模型 ， 提供更好的视觉特征 。  

（ １０ ）ＳｉｍＶＬ ［２２ ］通过利用大规模弱监督来降低训练复杂性 ， 并使用单

一的  

前缀语言建模目标进行端到端训练 。 在不使用额外数据或特别任务  

的情况下 ， 所得到的模型表现显著优于先前的预训练方法 。  

（ １ １ ）ＯＦＡＰ５＾出了

一的多模态预训练范式 ， 以打破复杂任务順态  

特定定制的框架 。 ＯＦＡ支持任务全面性的任务无关和模态无关的框  

一了各种跨模态和单模态任务 ， 包括图像生成 、 视觉定位 、  

图像字幕 、 图像分类 、 语言建模等 。 并在这些任务中取得了最先进  

的结果 。  

（ １２ ）ＧＩＴ ［２３３也提出利用生成式的方法统

一各种多模态任务 ， 并将模型简  

一个图像编码器和

一个文本解码器 ， 并扩大了预训练数据和模  

型规模 ， 以提升模型性能 。  

（ １３ ）ＢＬＩＰ －２ ［２４］是

一种通用且高效的预训练策略 ， 它从现成的冻结的预训  

练图像编码器和冻结的大型语言模型中开始训练

一个小型中间网络 ，  

以连接视觉和语言 ， 以解决大规模模型的端到端训练 ， 视觉与语言  

预训练的成本变得日益高昂的问题 。  

３９  

北京邮电大学硕士学位论文  

３ ．３ ．３ ．２非自回归方法  

（ １ ）ＦＭＣ

［３＜） ］为了解决非自回归解码的质量问题 ， 提出了

一种位置对齐方  

法 ， 以对图像中检测到的内容进行排序 ， 使得非自回归并行生成质  

量更好 。  

（２）ＭＮＩＣ ［３２］应用了掩码

－重预测的范式 ， 多步地重新生成

一些己经生成  

的位置的单词 ，

一定程度上提升了非自回归图像描述模型的质量。  

（３ ）ＣＭＡＬ ［３ １ ］首次提出以纯强化学习多智能体决策的角度看待非自回归  

图像描述 ， 并提出了

一种基于反事实的图像描述强化学习算法 。  

（４）提出使用低维连续潜变量来捕获从提取的图像特征到句子解  

码之前的语义信息和单词依赖关系 ， 从而实现了

一种高生成质量的  

迭代细化非自回归图像描述模型 。  

（５ ）８“ ＜： ［７８］提出了

一种半自回归的图像描述方法 ， 即保持从左到右的生  

成顺序 ， 但是

一步生成多个单词 ， 因此在提升图像描述速度的同时  

保持了自回归生成的质量 。  

（６）ＵＡＩＣ ［３４］提出迭代细化的非自回归图像描述的短板之

一在于生成顺序  

的控制 ， 因此提出了基于单词的不确定度来衡量生成的难度 ， 并根  

据生成的难度来控制多步生成的顺序 。  

（７）ＢｉｔＤｉｆ ｉｉＳｉ〇ｎ ［３７］首先将连续扩散模型引入到图像描述中 ， 采用先将单  

词映射到连续空间 ， 再将结果从连续空间翻译到离散的词空间的方  

法 ， 实现了利用连续扩散模型的方法完成离散的图像描述任务 。  

（８ ）ＳＣＤ －Ｎｅｔ ［３８％ＢｉｔＤｉｆ ｉｉｓｉｏｎ基础上提出了结合知识蒸谐的强化学习方  

法 ， 提升了基于扩散模型的非自回归图像描述的效果 。  

（９）Ｂ〇ＦｉＣａｐ

［７９］提出了

一种更精细控制半自回归图像描述的方式 ， 通过  

一步边界的方式 ， 取代了先前固定步长的方法 ， 提升了  

半自回归图像描述的效率和效果 。  

４０  

第三章基于编辑的非自回归图像描述方法  

３ ．３ ．４实验结果  

３ ．３ ．４ ． １离线结果  

表 ３ － １ 离线对比表格 ， ｆ符号表示该模型是预训练的 ， 表示该指标在工作中并未被报告  

Ｍｏｄｅｌ Ｂ －ｌＢ －４ＭＥＴＥＯＲＲＯＵＧＥＣＩＤＥｒＳＰＩＣＥＳｐｅｅｄＵｐ   ￣￣

自回归模型  

－Ｔ８０ ．８３９ ．１２９ ．２５８ ．６１３１ ．２２２ ．６ －  

Ｘ －Ｔｒａｎｓｆｏｎｎｅｒ８０ ．８３９ ． １２９ ．２５８ ．６１３ １ ．２２２ ．６ －  

ＤＩＣＴ８１ ．４３９ ．８２９ ．４５９ ．１１３３ ．８２３ ．０ －  

ＧＥＴ８１ ．５３９ ．５２９ ．３５８ ．９１３１ ．６２２ ．８ －  

ＤＲＴ８１ ．７４０ ．４２９ ．５５９ ．３１３３ ．２２３ ．３ －  

ＰｕｒｅＴ８２ ． １４０．９３０ ．２６０ ． １１３８ ．２２４．２ －  

ＤｅｅＣａｐ８０ ． １３８ ．７２９ ． １５８ ． １１２９ ．０２２ ．５４ ．３５ｘ  

ＰＴＳＮ８３ ．６４１ ．７３０ ．４６０ ．２１４４．２２３ ．７ －  

ＬＳＴＮｅｔ８ １ ．５４０ ．３２９ ．６５９ ．４１３４ ．８２３ ． １ －  

ＳＣＰ －ＷＧＣＮ８２ ．６４１ ．５３０ ．２６０ ．２１３９ ．０２４ ．２ －  

ｆＶｉｎＶＬ

－４１ ．０３ １ ． １ －１４０ ．９２５ ．２ －  

ｆＳｉｍＶＬ －４０ ．６３３ ．７

－１４３ ．３２５ ．４ －  

－４４ ．９３２ ．５ －１５４ ．９２６ ．６

－４４ ． １３２．２ －１５ １ ．１２６ ．３ －  

卞ＢＬＩＰ２

－  ４２Ａ 

１４４ ．５ 

非自回归模型  

－３２ ．５２７ ．２５５ ．４１０９ ．５２０ ．６１ ．５６ｘ  

ＭＮＩＣ７５ ．４３０ ．９２７ ．５５５ ．６１０８ ． １２１ ．０２ ．８０ｘ  

－３６ ．２２７ ． １５５ ．３１ １５ ．７２０ ．２８ ． １５ｘ  

ＣＭＡＬ８０ ．３３７ ．３２８ ． １５８ ．０１２４ ．０２１ ．８１３ ．９０ｘ  

ＩＢＭ７７ ．２３６ ．６２７ ．８５６ ．２１ １３ ．２２０ ．９３ ．０６ｘ  

ＳＡＩＣ８０ ．３３８ ．４２９ ．０５８ ． １１２７ ．１２１ ．９３ ．４２ｘ  

ＵＡＩＣ８０ ．９３８ ．８２９ ．２５８ ．７１３１ ．７２２ ．８３ ． １８ｘ  

ＳＣＤ －Ｎｅｔ８１ ．３３９ ．４２９ ．２５９ ． １１３ １ ．６２３ ．０ －  

ＢｉｔＤｉｆ ｉａｉｓｉｏｎ８１ ．３３４ ．７２９ ．２５８ ．０１ １５ ．０ － －  

ＢｏＦｉＣａｐ８０ ．５３８ ．９２８ ．８５８ ．８１２８ ．４２２ ．７３ ．６９ｘ  

ＵｎｉＣａｐ ８４ ． １４０ ．７２９ ．６６０ ．０１３６ ．５２３ ．０８ ．０５ｘ  

表 ３ －１报告了在Ｋａｒｐａｔｈｙ测试集上的离线评估结果 ， 从表中可以看到 ， 与  

自回归方法相比 ， 本文提出的ＵｎｉＣａｐ在保持可比较的性能的同时 ， 实现了高达  

８倍的显著加速 。 ＵｎｉＣａｐ的质量表现甚至好于

一些近年来的自回归图像描述方  

法 ， 如ＬＳＴＮｅｔ。  

与非自回归方法相比 ， ＵｎｉＣａｐ也实现了良好的推理速度 ， 并在大多数指标  

上优于所有的非自回归基线 。 为了更直观地展示 ＵｎｉＣａｐ在速度

－性能平衡上的  

４１  

北京邮电大学硕士学位论文  

优势 ， 本文绘制了ＵｎｉＣａｐ的 ＣＩＤＥｉ

＊分数和加速率 ＳｐｅｅｄＵｐ的二维可视化图以  

直观地展示整体性能 。 可以从中看出 ， ＵｎｉＣａｐ在质量和速度之间实现了良好的  

平衡 。  

１５０   

１４０ＭＣ  

Ｕｎ ｉＣａｐ  

１３０ＵＡＩＣ？  

ｑＤｅｅＣａｐ？   Ｑ

丄 ２〇 ＣＭＡＬ  

，ＦＮ ＩＣ  

１ １０？ ＩＢＭ  

ＭＩＲＭＮＩＣ  

１００  

Ｊ２４ １２  

Ｓｐｅｅｄｕｐ  

－８ＵｎｉＣａｐ的ＣＩＤＥｒ分数和加速率 ＳｐｅｅｄＵｐ的二維可视化图 （靠近右上角的模型更好 ）  

３ ．３Ａ２在线结果  

－２在线测试结果 ， 表示在ＭＳＣＯＣＯ在线测试集上的性能比较 。 所有结果均在强化学习  

阶段后报告 。 符号ｔ表示预训练方法 。 此外 ， ｃ５和 ｃ４０分别表示使用 ５个和 ４０个参考字幕 。  

非 自回归图像描述方法用灰色背景表示  

ＭｏｄｅｌＢ

－３Ｂ ４ＭＥＴＥＯＲＲＯＵＧＥＣＩＤＥｒ  

ｃ５ｃ４０ｃ５ｃ４０ｃ５ｃ４０ｃ５ｃ４０ｃ５ｃ４０ｃ５ｃ４０ｃ５ｃ４０  

Ｔｒａｎｓｆｏｒｍｅｒ８ １ ．６９６ ．０６６ ．４９０ ．８５ １ ．８８２ ．７３９ ．７７２ ．８２９ ．４３９ ．０５９ ．２７４ ．８ １２９ ．３１３２ ． １  

ＣＭＡＬ７９ ．Ｓ９４ ．３６３ ．８８７ ．２４８ ．８７７ ．２３６ ．８６６

． １２７ ．Ｑ３６ ．４５７ ．６７２ ．０ １ １９ ．３１２ １ ．２  

Ｘ Ｔｒａｎｓｆｏｒｍｅｒ８ １ ．９９５ ．７６６ ．９９０ ．５５２ ．４８２ ．５４０ ．３７２ ．４２９ ．６３９ ．２５９ ．６７５ ．０ １３ １ ． １ １３３ ．５  

ｔＶｉｎＶＬ８ １ ．９９６ ．９６６ ．９９２ ．４５２ ．６８４ ．７４０ ．４７４ ．９３０ ．６４０ ．８６０ ．４７６ ．８ １３４ ．７ １３８ ．７  

Ｄ ＩＣＴ８２ ．４９６ ．６６７ ．４９ １ ．７５２ ．８８３ ．８４０ ．６７４ ．０２９ ．８３９ ．６５９ ．８７５ ．３ １３３ ．４１３５ ．４  

ＰｕｒｅＴ８２ ．８９６ ．５６８ ． １９ １ ．８５３ ．６８３ ．９４ １ ．４７４ ． １３０ ． １３９ ．９６０ ．４７５ ．９ １ ３６ ．０１３８ ．３  

ＰＴＳＮ８４ ．０９７ ．５６９ ．２９３ ．２６４ ．５８５ ．７４２ ． １７６ ． １３０ ．５４０ ．２６０ ．４７５ ．６ １４ １ ．４ １４３ ．９  

ＤｃｃＣａｐ８０ ．５９５ ． １６５ ．２８９ ． １５０ ．３８０ ．０３８

． １６９ ．５２８ ．０３７ ．０５８ ．４７３ ．５ １２ １ ．４１２４ ．４  

ｔＧ ＩＴ８４ ．３９８ ． １７０ ．０９４ ．４５５ ．７８７ ．６４３ ．２７８ ．３３ １ ．９４２ ． １６２ ．０７８ ．４ １４６ ．４ １４９ ．８  

ｔＯＦＡ８４ ．５９８ ． １７０ ． １９４ ．４５５ ．９８７ ．８４３ ．６７８ ．７３２ ． １４２ ．７６２ ．５７９ ．０ １４７ ．２ １４９ ．６  

ＵＡ ＪＣ８ １ ．９９６ ．３６６ ．５９ １ ． １５ １ ．８８３ ．０３９ ．６７２ ．９２９ ．２３８ ．９５９ ．２７４ ．７ １２９ ．０１ ３２

．８  

ＬＳＴＮｅｔ８２ ．６９６ ．７６７ ．８９２ ．０５３ ．３８４ ．３４ １ ． １７４ ．７２９ ．９３９ ．６６０ ．０７５

．４ １ ３４ ．０ １ ３６ ．３  

－Ｎｅｉ８０ ．２９５ ． １６４ ．９８９ ．３５０ ． １８０ ． １３８ ． １６９ ．４２９ ０３８ ．２５８ ．５７３ ．５ １２６ ．２１２９

．０  

Ｕｎ ｉＣａｐ８４ ．０９７ ．２６７ ．８９ １ ．５５２ ．６８２ ．７４０ ． １７２ ．２２９ ．４３８ ．６５９ ．６７４ ．４ １ ３ １ ．９ １ ３４ ． １  

为了更公平地对比 ， 本文也在 ＭＳＣＯＣＯ的测试服务器上提交了在线测试  

结果 ， 在线测试是在 ＣＯＣＯ未开放答案的数据集上运行的 ， 可以更直观地反映  

出模型的性能 。 在线测试结果如表 ３

－２所示 ， 可以从中看出 ， ＵｎｉＣａｐ在在线测  

试上的性能也优于大部分的自回归图像描述模型和全部的非自回归图像描述模  

型 。 由于在线服务器测评 ， 无法衡量推理速度 ， 因此推理速度并未在在线测试  

中报告 。  

４２  

第三章基于编辑的非自回归图像描述方法  

３ ．３ ．５消融分析  

一步地检验提出的各种组件的有效性 ， 本节将基于 ＭＳＣＯＣＯ数据  

集对ＵｎｉＣａｐ进行进

一步的消融实验 。  

３ ．３ ．５ ． １统

一编辑操作  

一编辑操作的有效性 ， 本文实现了先前的朴素编辑操作 ， 包括  

位置的插入 、 删除以及词的预测三个解码器 。 并遵循同样设置进行了训练 ， 结  

－９所示 ， 根据分析 ， 本文提出的ＵｎｉＣａｐ相比朴素方法 ， 使用两个解码  

器完成图像描述任务 ， 速度上的收益在理论上应该为朴素方法的 ３３％ 。 而根据  

实验结果 ， ＵｎｉＣａｐ的加速率为 ８ ．０２ｘ ， 朴素方法的加速率为 ５ ．６２ｘ ， 则速度的收  

益约为 ２９ ．８４％ ， 考虑到 ＵｎｉＣａｐ需要做

一些冲突消解的操作 ， 这个速度收益符  

合本文的预期 。  

Ｂ － Ｉ  

＾ ８４２  

Ｓｐｅｅｄｌ

： ｐ  

、 ８ ０５  

＇ 珍 ，Ｚ＇ 、 ＇

４０ ６５ ．６２  

２９ ．６

１ ３６ ５  

ＭＥＴＥＯＲ

２９Ｋ ＣＩＤＥｒ  

６０－  

Ｕｎ ｉＣａｐ  

朴素方法  

ＲＯ ｌＧＥ  

－９朴素方法和ＵｎｉＣａｐ的性能对比雷达图  

３丄５ ．２训练策略  

在本节 ， 本文对提出的两阶段训练策略的改进分别地和统

一地进行了实验 。  

－３交叉熵阶段多路径组合实验结果  

－４ＭＥＴＥＯＲＲＯＵＧＥＣＩＤＥｒＳｐｅｅｄｕｐ  

＃ １６６ ．７２６ ．９２８ ．７５４ ．３９５ ．２４ ．３９ｘ  

＃ １＆＃２８３ ．９４０ ．６２９ ．６５９ ．９ １３６ ．７７ ．８５ｘ  

＃ １ ，＃２＆＃３８４ ．２４０ ．７２９ ．６６０１３６ ．５８ ．０５ｘ  

４３  

北京邮电大学硕士学位论文  

在交叉熵阶段 ， 用不同的路径组合训练模型 ， 结果如表 ３

－３所示 ， 可以观  

察到的结论包括 ：  

（ １ ） 路径 ２显著地提升了模型的表现 ， 这表明通过掩码预测器ＭＰ为模  

型生成更难的样本有助于提升模型的生成性能 。  

（２） 路径 ３提升了模型的加速率 ， 这表明通过加入

一个专门的路径让模  

型学习减少不必要的操作是有效的 。  

在强化学习阶段 ， 本文进行了各种设置的实验 ， 包括无强化学习 （ｗ／〇ＲＬ ）、  

强化学习 （ｗ／ ｃｏｍｍｏｎＲＬ）和带有阈值采样的ＲＬ（ｗ／ＴｈｒｅｓｈｏｌｄＲＬ） 。 结果如  

表 ３ －４所示 。 从设置 ４、 ５和 ６中 ， 可以观察到阈值采样方法与多路径交叉熵训  

练方法结合在

一起取得了最佳性能 （相比无强化学习 ＣＩＤＥｒ＋２ ．２ ， 相比强化学  

习 ＣＩＤＥｒ＋１ ．４） 。 从设置 １ 、 ２和 ３中 ， 可以观察到即使没有多路径交叉熵训练  

方法 ， 本文提出的阈值采样方法在三个 ＲＬ设置中仍然表现更好 （相比无强化  

学习ＣＩＤＥｒ＋１ ．９ ， 相比强化学习ＣＩＤＥｒ＋ １ ．７） 。  

表 ３ ￣４训练策略综合实验结果  

编号阶段设置Ｂ

－４ＭＥＴＥＯＲ＾ ＯＵＧＣＩＤＥｒ Ｓｐｅｅｄ  

ｔ，ｕｐ  

＃１ ｗ／° Ｔｎｐ ｌｅ

ｐａｔｈｇ２ ．８３９ ．５２９ ．５５９ ．６１３４ ．８６ ．８３ｘ   强化学习ｗ／ｏＲＬ  

ｗ／ｏＴｒ ｉｐ ｌｅ

－ ｐａｔｈ

￣￣＾￣￣￣￣￣￣￣

１３５ ．０６ ．９７ｘ   强化学习ｗ／ＣｏｍｍｏｎＲＬ  

ｕｎｗ／ｏＴｒｉｐ ｌｅ

－ ｐａｔｈ   ＃３强化学习ｗ／ＴｈｒｅｓｈｏｌｄＲＬ

８３ ￣９４ａ６２９ ￣６５９ ＿９１３６＇７７ ＿８５ｘ  

Ｔ＂Ｐ ｌｅ － Ｐａｔｈ８２ ．５３９ ．３２９ ．５５９ ．５１３４ ．３７ ．２９ｘ   强化学习ｗ／ｏＲＬ  

＃５ ｗ ＾

－ ｐａｔｂ８３ １３９ ．７２９ ．４６０ ．０１３５ ． １７ ＿８８ｘ   强化学习ｗ／ＣｏｍｍｏｎＲＬ   ￣

： Ｗ／？Ｐ ｌ

１３６ ．５８．〇５ｘ   强化学习ｗ／Ｔｈｒｅｓｈｏ丨ｄＲＬ ｜  

同时 ， 表 ３４中展示的两阶段训练所有训练方法组合的结果表明 ， 本文提  

出的两种方法的组合取得了最佳的表现 。  

３ ．３ ．６案例分析  

－ １０所示 ， 本文从ＭＳＣＯＣＯ数据集抽样了

一些图片并用ＵｎｉＣａｐ推  

理 ， 将推理过程 、 目标图像描述和自回归模型给出的结果展示 。 可以观察到 ，  

与自回归方法相比 ， 本文提出的ＵｎｉＣａｐ生成的图像描述质量更好 ， 并且可以动  

态地修改己经输出的错误单词 。 例如 ， 在案例 １ 中 ， “ ｄｏｇ

”一词是不正确的 。  

在案例 ２中 ， 两个

ｓｔｒｅｅｔ ” 单词重复 。 在案例 ３ 中 ， “ ｂｌｕｅｔｒａｉｎ ” 和

“ ｗｈｉｔｅ  

” 重复 ， 且

“ ｗｈｉｔｅｔｒａｉｎ ” 是不正确的 。此外 ， 在案例 ４中 ， “

ｔｈｒｅｅｇｒｏｕｐ

一词与给定的图片不匹配 。  

４４  

第三章基于编辑的非自回归图像描述方法  

１Ａ ＩＣ ：ａｄｏｙｓｎ ｉｆ ｌｍ

．ｕ ａｄｏｙｂｅｈ ｉｎｄａｆｅｎｃｅ寶 ＡＫ ：ａｂｕｓｄｒ ｉ ｖ  ｉｎｕ ｄｏｗｎａｓｔｒｅｅ ｔｎｅｘｔ ｔｏａ ｃａｒ   ＾

ｌ ｎ ｉＣａＰ

Ｊ ＇ Ｕｎ ｉＣａｐ   Ｈｗ

 ＾９０＾＾

Ｉｎｐｕｔ ： Ｉｎｐｕ ｔ ： ＇ Ｂ＞  

ｉ ｔｅｒ１ ：ａ ｄｏｇ ａｎｄａ ｄｏ ＞

： ｌｏｏｋ ｉｎｇ

ｔｈｒｏｕｇｈ ａｃａｇｅ Ｉ ｔｅｒＩ ：ａ ｂｕ＞ ｄｎ＼

 ｉｎｙ ｄｏｗｎ ａ ｃｉ ｔｙ ｓｔ；ｃｔｉｓｉｒｅ ，  ｉｃａｎ

＞ ｔｆｒ２ ：ａｄｏｇ ａｎｄａｓ ｌ ｉｃｃｐＵｘ ｉｋ ｉｎｇ ｔｈ ｒｏｕｇｈａ ｃａｕｅ ｈｅｒ２ ：ａｂｕｓｄｒ ｉｖ ｉｎｇｄｏｗｎ ａ ｃ ｉ ｔｙｓｔｒｅｅ ｔ ｗ ｉ ｔｈｃａｒｓ  

ＧＴ ：ａ ｄｏｇ ａｎｄ ａ

ｇｏａ ｌ ｗ ｉ ｔｈ ｔｈｅ ｉｒ ｎｏｓｅｓ  ｔｏｕｃｈ ｉｎｇ ａ ｔｆｅｎｃｅＧＴ： ａ ｌａｒｇｅｂｕｓ ａｎｄ ｃａｒ ｏｎ ａｓ ｔｒｅｅ ｔ  

０ ） ⑵  

：ａｂ ｌｕｅｔｒ ａ ｉｎｃａｒｓ ｉ ｔ ｔ ｉｎｇｏｎａ ｔ ｒａ ｉｎｕｕｃｋ Ａ ＩＣ

： ｉｈ ｉ ｔｘ ｃｈｅｆ ＾

 ｉｎ ａ ｋｎｃｈｃｉ ｉ ｐ

ｉｃ ｉｘｍｍ：  ｔｏｏｄ  ｉｒ． ａ ｋ ｉｔｃｈｅｎ  

ｌｎ ｉ （ ２，０ Ｉｎ ｉＣ

ａｐ  

ＩｎＰｕｔ ：＜Ｂ＞Ａ

，ｎｐｕｔ ：

＇ Ｂｖ  

Ｉ ｔｅｒ１ ：ａ ｂ ｌｕｅ ｔｎ ｉ ｉｎｗｈ ｉｔｅ  ｛ｒａ ｉｎ ｓ ｉｔｉｎｇ ｏｎ  ｔｈｅ ｔｒａｃｋｓ Ｉｔｅｒ１  ： ｔｈｒｅｏ

ｇ ｒｏｕｐ ｏｆｃｈｅｆｓ  ｉｎ ａ ｋ ｉ ｔｃｈｅｎ

ｐ ｒｅｐａｒ ｉｎｇ ｆｏｏｄ  

ＩＩｔｅｒ ２ ：ａｂ ｌｕｅａｎｄｗｈ ｉ ｔｅ ｔｒａ ｉｎｓ ｉ ｔｔ ｉｎｇ ｏｎ ｔｈｅ ｔｒ ａｃｋｓ Ｉ ｔｅｒ ２ ：ａｔ ｒ ｒｏｕｐ ｏｆ ｃｈｅｆｓ ｉｎｕ ｋ ｉ ｔｃｈｅｎ ｐｒｅｐａｎｎｇｆｏｏｄ  

（＞Ｔ：ａ ｂ ｌｕｅｔｒａｉｎ

ｇｏｉｎｇ ｄｏｗｎｔｈｅ ｔｒａ ｉｎｔｒａｃｋｓ （ｉＴ：ｃｈｃ ｌｓ ａ ｉ ． ｒｅｓ ｔａｎｒｄｎ ｉ ｃｗｋａ ｉ ｔ： ａｉｕｌｔ ｉｐ

ｉｔ ｆ ｄ ｉｉｈｅｓｏｎ  ＜５ ｓ ｔｏｖｅ  

（３） （４ ）  

Ａ ｌＣ ：ｕ ｔｒａｍｓ ｔａ ｔ ｉｏｎｗ ｉ ｔｈａ ｉｒｊ ｉｎｏｎ ｔｈｅｔｒ ａｃｋ ｓ Ａ ＩＣ ：ａ ｌａｒｕｃ ｊｅ ｔ ｌ ｉｎｅｒ ｆｌｙ ｉｎｇ ｔｈｒｏｕｅｈａｃ ｌｏｕｄｙｓｋ＞  

， ＾ｌｎ ｉＣａｐ

１／ｎ ｉＣａｐ  

Ｉｎｐｕ ｔ ：Ｂ＞ｌ２—  ＜Ｂ ：＞  

？ ？？ ？

ｌ  ：ａｂ ｌａｃｋ ． ｎ ． ｉａｔｒａ ｉｎｓｔａ ｔ ｉｏｎｕ  ｉ ｔｈ ａ＾Ｉ ｔｅｒ１ ： ａａ ｉｒｐ

ｌａｎｅ ｉｓ ｆｌｙ

ｉｎ ｉｈｃｓｋｙ

Ｉ ｔｅｒ２ ：３ｂｂｅｋ

ｐｈｏ ｉｏ ｏｆａ ｔｒａ ｉｎ ｓ ｔａｔ ｉｏｎ ｗ

 ｉ ｔｈ ａｓ ｉｇｎ

Ｉ ｔｅｒ ２ ：ａａ ｉｒｐ

ｌ ．ｍｅ ｉｓｔｌｙ ｉｎｇ

ｉｎｔｈｅｓｋｙ   為Ｍ ＧＴ ：ａ ｔｒａ ｉｎｓ ｔａ ｔｉｏｎｗ  ｉｌｌ ｉａｎａｗｎ ｉｎｇ

ｉｓ ｄｅｐ

ｉｃ ｔｅｄ

：ＧＴ ：ａ ｌａｒｇｅ ａ ｉ ｒｐ ｌａｎｅｎ ｉｏｓｔｈｒｏｕｇｈ ｔｈｅ ｃ ｌｏｕｄｙ ｓｋ ＞  

（５ ）ｗｉ ｔｉｊ ａ ｔｒａ ｉｎｏｎｄｉｅ ｒｉｇｈｔ

ｌａｔｆｏｒｍ ． （６ ）  

ＡＩＣ ：ａｒｅｄｂｕｓ ｉｓ ｄｒ ｉｖ ｉｎｇ ｄｏｗｎｔｈｅｓｔｒｅｅｔＡ１Ｃ ：ａｃｒｏｗｄ〇 ｉ ｐｅｏｐ

ｌｅｓ ｉａ ｉｕ ｌｍｇ ｏｎ  ； ｉ ｓ ｔｒｅｅ ｔ ｃｏｍｅｒ  

ＪＴｍＴｌ＾

Ｉ？ （ａ ｐ  

ｉｎｐｕ ｔ ：＜Ｂ＞ ＾

Ｉｎｐｕ ｔ ：＜Ｉｉ＞  

Ｉｔｅｒ１ ：ａｒｅｄｂｕｓｄｒ ｉ ｖ  ｉｎｇ ｄｏｗｎｔｈｅＪ

． ｎ ：ａ Ｉ ｔｅｒ１ ： ａ ｒｎ＞？ ．＜ 〇ｆｐ ｅｏｐｋ ｕａ ；ｂｎ ＞ ： ＞ 喊咖  

，＿Ｊ Ｉｔｅｒ ２ ：ａｒｅｄｂｕｓ ｄｒｉｖ ｉｎｇ ｄｏｗｎ ｔｈｅｓ ｔｒｅｅｔ ａｈｏｕｓｅ Ｉ ｔｅｒ ２ ：ａ ｃｒ ｏｗｄ 〇ｆｐｅｏｐ

ｌｅ ＇ｖａ ｉｂｎ＾ ｄｏｗｒ； ａ ｉｕｃｃｔ ｗ ｉｔｈ    ． Ｉ ｔｅｒ ３ ：ｎｒｅｄｂｕ＞ ｕｎ ｖ  ｒ． ｔ ： ｃ ｉｏｕ ｎ ＵＫ

ｓ ｔｒｅｅ ｔａ ｈｏｕｓｅ Ｉ ｔｅｒ ３ ： ＞ ＞ｆｐ＊ｃ  ｔ

？ ｆｐｏｐｋ

 ．ｔ ｊＵｄｎｇ

： ｉ ｓ ｉｒｃｃ ｗ ： ｉ： ａ  ； ［ ＞ ： １ ：   聽麵 ＧＴ ：ａ ｔｒａｍ ｌ ｉｋｅｂｕｓ ｉｎａ

ｐａｒｋ ｉｎｇ

ｌｏ ｔ＾ｉｒｎＷｆＫ＾ｓｍ（ＪＴ： Ａ ｃｒｏｗｄｏｆｐｅｏｐ ｌｅ ｊｎａｒｃｈ ｉｎｇ ｄｏｕａ ｉａｓ ｉｒｅｄ  

） （８ ） ｉｎｆｒｏｎｔ ｏｆ ａｒｅｄ ｓｔｏｐ ｓ ｉｇｎ  

－ 】０ＵｎｉＣａｐ在 一些案例上的生成过程  

３ ．４本章小结  

本章在图像描述方向开展了基于编辑的非自回归图像描述模型的研究 ， 提  

一编辑非自回归图像描述模型 ＵｎｉＣａｐ

， 可以更高效 ， 直接地在迭代  

修改的过程中完成图像描述的生成 ， 达到更好的效率和效果 。 本章详细介绍了  

ＵｎｉＣａｐ的各个组件 ， ＵｎｉＣａｐ 由

一个跨模态特征抽取器和两个文本解码器组成 ，  

两个文本解码器分别负责所有的编辑操作和词的预测工作 。 随后 ， 本文介绍了  

为了在图像描述中实现基于编辑的训练 ， 在训练策略上做出的优化 ， 包括交叉  

熵阶段和强化学习阶段 。 最后 ， 本章在 ＭＳＣＯＣＯ 图像描述数据集上进行了大  

量的实验 ， 从模型的性能 、 消融实验 、 案例分析等方面充分验证了ＵｎｉＣａｐ方法  

的可行性和有效性 。  

４５  

第四章基于视觉提示词的可控非自回归图像描述  

第四章基于视觉提示词的可控非自回归图像描述  

４ ． １概述  

作为生成式模型 ， 目前的先进图像描述模型普遍面临不同程度的模式崩溃  

问题 ， 即在很大的模式空间中 ， 模型收敛到了某几个固定的简单模式 ， 在图像  

描述中 ， 这个问题体现为生成收敛在某几个固定的句式 ， 这导致 ＳＯＴＡ模型生  

成的图像描述尽管在现有的评价指标上超越了人类 ， 但是实际上与人类的描述  

能力仍具有相当的距离 。 目前已经有

一些工作致力于赋予图像描述可控性 ， 但  

是研究整体处于早期阶段 ， 并且相关工作还并未着眼于与非自回归生成方式相  

结合 ， 因此本文基于第三章介绍的 ＵｎｉＣａｐ ， 提出了基于统

一编辑的可控非自回  

归图像描述方法 （ＣｏｎｔｒｏｌｌａｂｌｅＵｎｉｆ ｉｅｄＥｄｉｔ

－ｂａｓｅｄＮｏｎ

－ａｕｔｏｒｅｇｒｅｓｓｉｖｅＩｍａｇｅ  

Ｃａｐ ｔｉｏｎｉｎｇＭｅｔｈｏｄ ， 简称 ＣｏｎＵｎｉＣａｐ ） ， 基于视觉提不词控制图像描述的生成过  

程 。 同时 ， 现有的追求可控性的图像描述模型在训练可控性时 ， 普遍采用了自  

动化标注现有数据集 ， 然后训练模型的方法 ， 这种方法的缺点在于受数据集的  

限制 ， 无法拓展到所有可能的风格上 ， 为了解决这个问题 ， 本文提出引入近年  

来流行的大语言模型 ， 借助其强大的零样本生成能力 ， 动态地根据所需的风格  

生成训练所需的图像描述 。 模型架构如图 ４

－ １所示 。  

： 描入ｃａｐｔ ｉｏｎ

Ｐｒ° ｍＰ纖

？ｈａｐｐｙ国＾  

：： 风格判别＼ ；

ｓａｄ■？  

； ＼ １：＿圍   

‘  ？ 厂 Ｐｒｏｍｐ傾典丄、丄

． Ｊ   雖集 ＾ 图像描述 ； ｈａｐｐｙ

１ １ － ＼  

自回归图像描 模里 ： ｆ非自回归图像描述模型

＂＂＂  

Ｅｄ ｉ ｔ  ｔｎｅ ｓｅｎ ｌｅｒｃｅ  ｔｏ ｍａｋｅ  ｉ＼ ５ 爹 ：

ｗ ｉｔｈｏｕｔ ａｄｄ ｉｎｇ ａｎｙ ？ｃｒ ｌ 、

ａｄｄ ｉ ｔ ｉｏｎａ ｌ ｏｂ

ｊｅｃｔｓ ｏ，

ｄｅｔａ ｌｓ ｎｏｔ ？ ？

ｐｒｅｓｅｎ ｔ  ｉｎ  ｔｈｅ ｏｒ ｉｇ

ｉｎａ ｓｅｎ ｔｅｎｃｅ＾ｆ  

？ ＜Ｅ像癌述 ｎｏｔｅ  ｔｈａｔ

ｙｏｕ ｒ ▼ ；

ｋ ＪＳ — 风格化图像描述 ： ：  

： Ｖ．Ｊ：ＶＪ  

数据处理 １训练阶段 ｜推理阶段 ｜  

－ １ＣｏｎＵｎｉＣａｐ模型总体架构  

４７  

北京邮电大学硕士学位论文  

４ ．２模型框架  

４ ．２ ． １图像编码层  

在 ＣｏｎＵｎｉＣａｐ中 ， 为了获取图像的特征表示 ， 本文采用了与 ＵｎｉＣａｐ同样  

的跨模态特征抽取器 ， 并同时提取图像的区域和全局特征 。 为了更进

一步地轻  

量化网络 ， 本文采用了基于卷积神经网络的特征 。  

对于区域特征 ， 本文采用了图像描述领域先前工作中常用的 ＢＵＴＤ特征 ，  

ＢＵＴＤ特征由Ａｎｄｅｒｓｏｎ等人提出 ， 包括

一个在 ＩｍａｇｅＮｅｔ和ＶｉｓｕａｌＧｅｎｏｍｅ数据  

集上预训练的ＦａｓｔｅｒＲＣＮＮ 目标检测器 ， 通过ＦａｓｔｅｒＲＣＮＮ提取图像区域特征／？  

的过程如下 ：  

给定输入图像／ ， 首先通过 ＦａｓｔｅｒＲＣＮＮ中的卷积神经网络提取特征图Ｆ ，  

随后将Ｆ输入区域提议网络 ＲＰＮ ， ＲＰＮ在Ｆ上提出

一系列候选区域 。 这个过程  

可以形式化为 ：  

＝ＲＰＮ（Ｆ ） （４ ．１）  

其中 ， 代表由 ＲＰＮ生成的Ｎ个提议区域集合 ， Ｐ ￡ ，是第ｉ个提议区域的  

对象得分 ， 尽是对应的边界框 。 通过边界框和 ＲＯＩＰｏｏｌｉｎｇ操作可以从特征图Ｆ  

中提取到区域对应的特征向量圮 。 最终 ， 可以得到

一系列区域特征  

｛＆，ｉ？２ ， … ， ｉ？ｗ｝ ， 其中每个＆对应

一个巧 。  

对于全局特征 ， 本文采用了ＲｅｓＮｅｔ作为全局特征提取器 ， ＲｅｓＮｅｔ是

一种基  

于卷积神经网络的特征提取器 。 卷积神经网络提取到的二维图像特征被展平为  

一维图像特征Ｇ 。  

得到两个图像特征Ｇ和２？后 ， 再根据输入句子的嵌入Ｔ进行多头注意力计算 ，  

与ＵｎｉＣａｐ中的跨模态特征抽取器相同 ， 计算最终的跨模态表示 的过程可以形  

式化为 ：  

ｒ＝ＭＨＡ（ｒ ，ｒ ，ｒ）  

Ｃ ｒ ＝ＭＥＡ（ｒ ，Ｒ ，Ｒ）  

Ｃ ｇ ＝ＭＨＡ（Ｔ

，Ｇ ，Ｇ）  

Ｃ， ＝ Ｃ ｒ？Ｓｉｇｍｏｉｄ （［Ｃ ｒ ；ｒ ］

＞Ｆ ｒｒ＋ｂＴｒ）

（４ ．２）  

，＝Ｃ ｇ？Ｓｉｇｍｏｉｄ（［Ｃ ｇ

；ｒ ］＾＋ｂＴｇ）  

Ｆ ｃ ＝ＬａｙｅｒＮｏｎｎ

，＋Ｃ ｇ

４ ．２．２提示词机制  

ＣｏｎＵｎｉＣａｐ使用连续空间的可学习提示词以控制图像的风格 。提示词机制  

在非自回归图像描述中的引入如下 ：  

４８  

第四章基于视觉提示词的可控非自回归图像描述  

首先定义提示词词典户＝ ［仏／ ＞

２ ， ． ． ．＆］ ， 其中 ， 每个风格均对应为

一个嵌入  

向量 ， 具有同样的维度 。在将控制信息输入模型时 ， 风格被初始化为对应的嵌  

入向量并拼接在句子开始符＜ＢＯＳ＞之前 。 如图 ４ － １所示 ， 在训练时 ， 输入的图  

像描述经过风格判别器后 ， 风格判别器给定

一个风格 ， 然后拼接好的输入被输  

入到非自回归模型中开始训练 。 风格判别器可以理解为对数据集的

一种分割 ，  

一个提示词在训练时都只能看到对应风格的图像描述 ， 等价于将数据集  

分割为数个集合／） ＝ ｛１）１ ，２）２ ， ． ． ．仏１｝ ， 交由不同的提示词训练 ， 因此 ， 最终  

ＣｏｎＵｎｉＣａｐ的训练目标可以形式化为 ：  

（ｖ ，啤Ｚ喊⑷ｖ ，Ｐ

； ，Ｖｉ） （４ ．３）  

■ ＬＩ＞Ｊ －  

其中 ， ｒ表示图像特征 ， Ｓｔｌ表示非自回归图像描述中输入的句子 ， ｓｔ表示目标句  

子 ， Ａ表示提示词 ， 即 ＣｏｎＵｎｉＣａｐ的训练目标是给定输入句子、 图像特征和风  

格提示词先验 ， 对所有的数据集的子集Ａ最大化生成对应风格的句子的期望 。  

４ ．２ ．３数据风格判别器  

为了实现提示词机制 ， 需要准确地分割子集Ａ ， 数据风格判别器是至关重  

要的 ， 同时 ， 数据风格判别器也应用于后续的生成可控图像描述数据的校验 ，  

对于不同的分割需求 ， 本文提出了基于不同原理的数据风格判别器 。  

４ ．２ ．３ ． １描述详细度判别器  

对于描述的详细度 ， 本文直接通过图像描述的长度来判别 ， 本文启发式地  

定义三种详细度类型 ： 简短 ， 中等 ， 详细 。 并映射到图像描述的长度上 ， 其中  

简短对应长度 ［０ ，１０） ， 中等对应长度 ［１０ ，１６） ， 详细对应长度 ［１６ ，＋〇〇 ） 。 根据详  

略度可以将数据集分割为 ３个子集 ， 每个子集对应不同的提示词 。  

４ ．２Ｊ ．２基于情感词典的正负情感判别器  

对于正负情感的判别 ， 本文采用了基于情感词典的方法 。 情感词典是

一种  

自然语言处理中的工具 ， 标注了每个单词的情感极性 （正面或负面 ） ， 这些标注  

根据每个词在语料库中的使用情况进行分配 。情感极性值范围从

－１到 １ ， 表示  

词汇的情感倾向 。  

一个句子５＝ ｛＞

？／ １， １＾ ，＾／ ３

． ． ． １＾｝ ， 通过情感词典 ， 可以得到对应的情感  

极性 ． ． ．ｐｎ｝ ， 随后 ， 对句子中的上下文进行处理 ， 包括否  

定处理和程度副词调整 ， 否定处理代表识别否定词 （如

＂ ｎｏｔ ＂

＇ ｔ ＂等 ） ，  

当否定词出现在

一个句子中时 ， 反转后边的词的情感极性 。程度副词调整代表  

对于对于程度副词 （如

＂ ｖｅｒｙ

＂ ｑｕｉｔｅ

＂ ｅｘｔｒｅｍｅｌｙ

＂等 ） ， 根据程度副词改变后

个词的情感极性 。  

一个句子的情感极性被定义为 ：  

４９  

北京邮电大学硕士学位论文  

Ｐｓ （４ ．４ ）  

ｎｉ  

在本文的设置中 ， 极性大于 ０的认为是正面情感 ， 小于 ０的认为是负面情  

感 。  

４ ．２ ．３ ．３基于预训练双向语言模型的正负情感判另

Ｉ Ｊ器  

为了弥补传统的方法对情感判断的不足 ， 本文还提出利用预训练的双向语  

言模型ｂｅｒｔ＾Ｂ虽大的零样本建模能力来判断数据的情感极性 。  

／ ＾ＮＳＰＭａｓｋ ｔＭＭａｓｋ ＬＭ

，／ＵＨＵ Ｓｔａｒｔ／ＥｎｄＳｐａｎ＼  

￣￣金＾—

！广厂＾＾：ｉ 會 舍

丨   ＣＤＱＤｒｖｉｒｗｆ＾ｒ ｉ

．ＱＱＣｌＤＧＤＧＳＺＯＤ

… Ｓｊ     

； ； ； ； ； ； ；ＢＥＲＴ  

ｒ＾ｊｒ＾ｎｒｔｉ ｉ

… ｒｈｓｉｓｄ ＳＪＧ３Ｅ］ Ｏ  

．Ｕ ＂— ＊ ｉ＿ｒＵＬ ｒ ＾  

ｆＣＬＳ）  ｜ ［ Ｔｏ ｔ ． －

！ ＜＊ ？＜

５Ｓ£ Ｐ ５

ｊ ［ ｍａ ：

ｖＨ ｆ］ｆ

１ ］！］［

）１ ｜

． ．ｆ碰

ＭａｓｋｅｄＳｅｎｔｅｎｃｅ ＡＭａｓｋｅｄＳｅｎ ｔｅｎｃｅＢＱｕｅｓｔ ｉｏｎ▲Ｐａｒａｇ ｒａｐｈ   含 ７＼＼＼會７  

Ｘ ＼ Ｕｎ ｌａｂｅ ｉｅｄＳｅｎ ｔｅｎｃｅ Ａ ａｎｄＢＰａ ｉｒｙ

Ｑｕｅｓｔ ！〇ｎＡｎｓｗｅｒＰａ ｉｒ ＾／  

－ ｔｒａ ｉｎ ｉｎｇ Ｆ ｉｎｅ

－Ｔｕｎ ｉｎｇ  

－２ＢＥＲＴ模型网的结构与训练思想  

ＢＥＲＴ 的模型结构和训练过程如图 ４

－２所示 ， ＢＥＲＴ模型的结构基于  

Ｔｒａｎｓｆｏｒｍｅｒ架构 ， 特别强调在预训练阶段利用双向上下文来理解语言 。 ＢＥＲＴ  

的核心是 Ｔｒａｎｓｆ ｏｒｍｅｉ

？的编码器部分 ， 该编码器基于自注意力机制 ， 允许模型在  

处理每个词元时考虑到整个文本序列的上下文信息 。 相比之前的模型 ， 如单向  

的语言模型或基于窗口的方法 ， ＢＥＲＴ能够捕捉到更加丰富的语言特征 ， 从而  

在各种自然语言处理任务上实现突破性的性能提升 。  

ＢＥＲＴ 的预训练包含两个主要任务 ： 掩码语言模型 （ＭａｓｋｅｄＬａｎｇｕａｇｅ  

Ｍｏｄｅｌ ，ＭＬＭ ） 和下

一句预测 （ＮｅｘｔＳｅｎｔｅｎｃｅＰｒｅｄｉｃｔｉｏｎ ，ＮＳＰ ） 。 在ＭＬＭ任务中 ，  

输入文本序列中的部分词会被随机替换为

一个特殊的掩码标记 ， ＢＥＲＴ的目标  

是预测这些词元原本是什么 。 这种预训练方式使得模型必须基于整个输入序列  

的上下文来理解和预测每个词元 。 在 ＮＳＰ任务中 ， 模型被训练来预测给定两个  

句子Ａ和 Ｂ ，Ｂ是否是Ａ的下

一句 。 ＮＳＰ任务要求模型理解句子之间的关系 ，  

一步增强模型对于长篇文本结构的理解能力 。  

形式化地 ， 给定

一个词元序列Ｘ＝ＢＥＲＴ首先通过

一个嵌  

入层将每个词元转换为对应的向量表示 。 然后 ， 这些向量经过Ｌ层 Ｔｒａｎｓｆｏｒｍｅｒ  

编码器 ， 每

一层都包含多头自注意力机制和前馈神经网络 。 对于每

一层的输出 ，  

都会加上残差连接和层归

一化操作 ， 以促进深层网络的训练 。 通过这种方式 ，  

５０  

第四章基于视觉提示词的可控非自回归图像描述  

ＢＥＲＴ能够生成每个词元的深层双向表示 。 在预训练结束后 ， ＢＥＲＴ模型可以通  

过微调的方式适应于各种下游任务 ， 如情感分析 、 问题回答、 命名实体识别等 ，  

仅需在 ＢＥＲＴ的基础上添加少量的任务特定层即可完成任务的学习 ， 本文之所  

以能借助 ＢＥＲＴ低成本地实现数据风格判别器就借助了ＢＥＲＴ微调方便 ， 支持  

任务多的特点 。 本文利用ＢＥＲＴ执行数据风格判别的形式化描述如下 ：  

首先 ， 定义情感极性判别问题为

一个分类任务 ， 其中输入是 一个文本序列  

ｓ＝ ｛％

． ． ．ｗｎ） ， 目标是预测文本的情感极性ｙ ， 其中ｙｅ｛０ ，１｝表示负面情  

感和正面情感 ， 对于给定的文本输入ｓ ， 利用 ＢＥＲＴ的分词器 （ｔｏｋｅｎｉｚｅｒ）将ｓ  

转换为令牌 （ｔｏｋｅｎ）序列７ ， 即

！Ｔ＝ｔｏｆ ｃｅｎｉｚｅｒ（ｓ） 。  

随后 ， 在序列的开始和结束位置添加特殊令牌 ＣＬＳ和 ＳＥＰ ， 以适应 ＢＥＲＴ  

的输入 ， 即７

＝ ［〔２乳 ７

， ［兑？ ］ 。 同时 ， 在批处理时 ， 由于会将小于最大长度的  

令牌序列填充到最大长度 ， 因此在令牌序列中存在填充 （Ｐａｄｄｉｎｇ ） ， 为了保证  

模型可以区分真实的令牌和填充的令牌 ， 生成注意力掩码矩阵４ 。  

最后 ， ＢＥＲＴ接受令牌化的输入ｒ

＇ 和注意力掩码矩阵儿 此处省略对 ＢＥＲＴ  

内部多头 Ｔｒａｎｓｆｏｒｍｅｒ块的形式化表述 ， 将 ＢＥＲＴ模型执行的运算记为／

， 则计  

算输出ｈ的过程如下式所示 ：  

，Ａ ） （４ ．５）  

其中 ， ｈ是模型最后

一层的输出 ， 包含了序列每个令牌的隐藏表示 。 可以从 ／ｉ中  

提取 ［ＣＬＳ ］令牌的表示 作为整个序列的表示 。 随后 ， 在［ＣＬＳ］令牌的  

隐藏表示后添加全连接层和 ｓｏｆ ｔｍａｘ层作为分类头 ， 用于情感极性的分类 ：  

＝ ｓｏｆ ｔｍａｘＣＨ＾ ｃＬｓ＋ｂ） （４ ．６）  

其中 ， ，是模型对情感极性的预测分布 ， Ｗ和６是全连接层的权重和偏置 。  

为了更好地计算情感极性 ， 本文将预测得到的ｙ

＇ 映射到 ［

－ｕ］ ， 与基于情  

感词典计算的极性加权得到最终的情感极性 。  

４ ．２ ．４可控图像描述数据生成策略  

基于提示词的可控图像描述模型具有

一定的局限性 ， 即 ， 所有的数据都是  

从现有的数据集中通过各种方式分类得到 ， 这限制了可控图像描述模型的上限 ，  

比如 ， 如果给定

一个风格 ， 不能保证这个风格对应的文本在数据集中

一定存在 ，  

其次 ， 各种风格对应的数据数量也存在不平衡 。 同时 ， 随着生成式大语言模型  

（ＬＬＭＳ） 的发展 ， 通过提示词 ， 使用大模型生成全新的可控图像描述数据成  

为了可能 。  

提示词 （Ｐｒｏｍｐｔ）是

一种文本输入 ， 用于指导人工智能模型如何回应或生  

成内容 。在与大型语言模型 （如 ＧＰＴ系列 ）或图像生成模型 （如 ＤＡＬＬ ？Ｅ）  

的交互中 ， 提示词起到了至关重要的作用 。  

５ １  

北京邮电大学硕士学位论文  

本文采用大语言模型＋文本提示词的方式生成可控图像图像描述的训练数据 ，  

一个给定的风格描述词 ， 以如下的模板构建提示词并利用大语言模型返回  

相应的风格化图像描述 ：  

“ Ｅｄｉｔｔ ｉａｅｓｅｎｔｅｎｃｅｔｏｍａｋｅｉｔ＜风格〉ｗｉｔｈｏｕｔａｄｄｉｎｇａｎｙａｄｄｉｔ ｉｏｎａｌｏｂｊｅｃｔｓｏｒ  

ｄｅｔａｉｌｓｎｏｔ ｐｒｅｓｅｎｔｉｎｔｈｅｏｒ ｉｇ ｉｎａｌｓｅｎｔｅｎｃｅ ：

’ 〈图像描述＞

＇ ｎｏｔｅｔｈａｔ ｙｏｕｒｏｕｔｐｕｔｓｈｏｕｌｄ  

ｋｅｅｐ ｔｈｅｍｅａｎｉｎｇｏｆ

＇＜图像描述  

其中 ， ＜风格＞和＜图像描述＞表示模板中的变量 ， 在将提示词提交给大语言  

模型时被填入模板内组成最终的提示词 。对于图像描述数据集 ， 每

一个图片都  

有五个对应的描述 ， 在每次填入模板时对五个描述进行抽样 ， 随机选择

一个。  

４ ．２ ．４ ． １数据标注样例  

为了直观理解本文基于大语言模型提示词的数据标注策略 ， 提供数个标注  

样例如表４ －１和表４

－２所示 ：  

表４ － １ 大语言模型自动数据标注样例

－正面  

｜大语言模型交互过程  

Ｑ ：Ｅｄｉｔｔｈｅｓｅｎｔｅｎｃｅｔｏｍａｋｅｉｔｐｏｓｉｔｉｖｅｗｉｔｈｏｕｔａｄｄｉｎｇａｎｙａｄｄｉｔｉｏｎａｌ  

ｏｂｊｅｃｔｓｏｒｄｅｔａｉｌｓｎｏｔｐｒｅｓｅｎｔｉｎｔｈｅｏｒ ｉｇ ｉｎａｌｓｅｎｔｅｎｃｅ ：

＇Ｔｗｏｋｉｄｓｗｉｔｈ  

ｈｅｌｍｅｔｓｏｎｓｉｔｔｉｎｇ ｉｎｔｈｅｓｎｏｗ．

＇ｎｏｔｅｔｈａｔｙｏｕｒｏｕｔｐｕｔｓｈｏｕｌｄｋｅｅｐｔｈｅ  

ｍｅａｎｉｎｇｏｆ

＇Ｔｗｏｋｉｄｓｗｉｔｈｈｅｌｍｅｔｓｏｎｓｉｔｔｉｎｇ ｉｎｔｈｅｓｎｏｗ．

Ａ ：Ｔｗｏｋｉｄｓ

ｊｏｙｆｉｉｌｌｙｓｉｔｔｉｎｇｏｎｔｈｅｓｎｏｗｗｉｔｈｓｋｉｓｎｅｘｔｔ ｏｔｈｅｍ ．  

正面Ｑ ：Ｅｄｉｔｔｈｅｓｅｎｔｅｎｃｅｔｏｍａｋｅｉｔｐｏｓｉｔ ｉｖｅｗｉｔｈｏｕｔａｄｄｉｎｇａｎｙａｄｄｉｔ ｉｏｎａｌ  

ｏｂｊｅｃｔｓｏｒｄｅｔａｉｌｓｎｏｔｐｒｅｓｅｎｔｉｎｔｈｅｏｒ ｉｇ ｉｎａｌｓｅｎｔｅｎｃｅ ：

＇Ｔｗｏｔｅｄｄｙｂｅａｒｓ  

ｐｏｓｉｔｉｏｎｔｏｌｏｏｋｌｉｋｅｔｈｅｙａｒｅｐ ｌａｙ ｉｎｇ ｉｎｓｔｒｕｍｅｎｔｓ ．

＇ ｎｏｔｅｔｈａｔｙｏｕｒｏｕｔｐｕｔ  

ｓｈｏｕｌｄｋｅｅｐ ｔｈｅｍｅａｎｉｎｇｏｆ

＇Ｔｗｏｔｅｄｄｙｂｅａｒｓ ｐｏｓｉｔｉｏｎｔｏｌｏｏｋｌｉｋｅｔｈｅｙａｒｅ  

ｐ ｌａｙ ｉｎｇ ｉｎｓｔｒｕｍｅｎｔｓ ．

Ａ ：Ｔｅｄｄｙｂｅａｒｓｗｉｔｈａ ｇｕｉｔａｒａｒｅｈａｖｉｎｇａｃｏｚｙｃｏｎｃｅｒｔ．  

５２  

第四章基于视觉提示词的可控非自回归图像描述  

表４ －２大语言模型自动数据标注样例 ？负面  

风格 大语言模型交互过程  

Ｑ ：Ｅｄｉｔｔｈｅｓｅｎｔｅｎｃｅｔｏｍａｋｅｉｔｓａｄｗｉｔｈｏｕｔａｄｄｉｎｇａｎｙａｄｄｉｔｉｏｎａｌｏｂｊｅｃｔｓ  

ｏｒｄｅｔａｉｌｓｎｏｔ ｐｒｅｓｅｎｔｉｎｔｈｅｏｒ ｉｇ ｉｎａｌｓｅｎｔｅｎｃｅ ：

＇Ａｍａｎｉｎａｂａｓｅｂａｌｌｕｎｉｆｏｒｍ  

ｔｈｒｏｗｉｎｇａｂａｌｌ ．

’ ｎｏｔｅｔｈａｔ ｙｏｕｒｏｕｔｐｕｔｓｈｏｕｌｄｋｅｅｐ ｔｈｅｍｅａｎｉｎｇｏｆ

＇Ａｍａｎ  

ｉｎａｂａｓｅｂａｌｌｕｎｉｆｏｒｍｔｈｒｏｗｉｎｇａｂａｌｌ ．

Ａ ：Ａｌｏｎｅｌｙｍａｎｉｎａｔａｔｔｅｒｅｄｂａｓｅｂａｌｌｕｎｉｆｏｒｍ ，ｄｅｓｐｅｒａｔｅｌｙｔｈｒｏｗｉｎｇａ  

负面ｂａｌｌ ．  

Ｑ ：Ｅｄｉｔｔｈｅｓｅｎｔｅｎｃｅｔｏｍａｋｅｉｔｓａｄｗｉｔｈｏｕｔａｄｄｉｎｇａｎｙａｄｄｉｔｉｏｎａｌｏｂｊｅｃｔｓ  

ｏｒｄｅｔａｉｌｓｎｏｔ

ｐｒｅｓｅｎｔｉｎｔｈｅｏｒ ｉｇ ｉｎａｌｓｅｎｔｅｎｃｅ ：

ｊｅｔｆ ｌｙ ｉｎｇ ｔｈｒｕｔｈｅａｉｒｗｉｔｈａ  

ｓｋｙｂａｃｋｇｒｏｕｎｄ

１ ｎｏｔｅｔｈａｔｙｏｕｒｏｕｔｐｕｔｓｈｏｕｌｄｋｅｅｐｔｈｅｍｅａｎｉｎｇｏｆ

＇ ａ ｊｅｔ  

ｆｌｙ ｉｎｇ ｔｈｒｕｔｈｅａｉｒｗｉｔｈａｓｋｙｂａｃｋｇｒｏｕｎｄ

Ａ ：Ａｌｏｎｅｌｙｊｅｔｆｌｙ ｉｎｇｔｈｒｏｕｇｈｔｈｅｖａｓｔｅｍｐｔｉｎｅｓｓｏｆｔｈｅｓｋｙ

４ ．２ ．４ ．２数据验证策略  

在利用大语言模型生成图像描述后 ， 由于大语言模型存在指令遵循失败或  

者幻觉等现象 ， 因此适当的验证是必须的 。 本文通过多种方式对大语言模型的  

输出进行了验证 。验证主要包含幻觉验证和风格验证 ， 整体的验证流程如图 ４ －３  

所示 。  

幻觉指大语言模型输出的与事实不符的输出 ， 在本文的设置中 ， 大语言模  

型的幻觉体现在大语言模型在修改句子的风格时 ， 可能会出现与图像不符 ， 没  

有在图像中出现过的事物 。 由于通过图文匹配分辨这点较为困难 ， 因此本文提  

出利用大语言模型对生成的内容进行

一定的校验 。考虑到本文主要涉及的 ＭＳ  

ＣＯＣＯ数据集中 ， 对应

一张图片有五句描述 ， 从文本层面的不同角度展示了图  

片的信息 ， 因此本文利用数据集中同图片的其它描述提供的信息对大语言模型  

输出的描述进行校验 。 为了避免在校验时仍出现幻觉的问题 ， 本文采取选择题  

评分制的形式 ， 可以避免在评分时大语言模型输出无关的结论 。  

基于选择题评分的提示词的思路是将上

一步生成的风格化描述和图片的标  

注与评分标准构建为选择题 。 最终的提示词为 ：  

５３  

北京邮电大学硕士学位论文  

Ｇ ｉｖｅｎｆ ｉｖｅｃａｐ ｔｉｏｎｓｏｆａｉｍａｇｅａｎｄａｇｅｎｅｒａｔｅｄｃａｐ ｔｉｏｎ ，ｊｕｄｇｅｉｆｔｈｅｇｅｎｅｒａｔｅｄ  

ｃａｐ ｔｉｏｎｄｅｓｃｒｉｂｅｓａｎｙｏｂ ｊｅｃｔｓｏｒａｃｔｉｏｎｓｔｈａｔｎｏｔｉｎｔｈｅｏｒｉｇ ｉｎｆｉｖｅｃａｐ ｔｉｏｎｓ ．  

Ｆｉｖｅｃａｐ ｔｉｏｎｓ ：  

１ ．＜图像描述 １＞  

２ ．＜图像描述２＞  

３ ．＜图像描述 ３＞  

４ ．＜图像描述４＞  

５ ．＜图像描述 ５＞  

Ｇｅｎｅｒａｔｅｄｃａｐ ｔｉｏｎ ：＜生成的图像描述＞  

Ａ ．Ｔｈｅｇｅｎｅｒａｔｅｄｃａｐ ｔｉｏｎｄｏｎ＾ｄｅｓｃｒ ｉｂｅａｎｙｏｔｈｅｒｏｂｊｅｃｔｓｏｒａｃｔｉｏｎｓｔｈａｔｎｏｔａｐｐｅａｒ  

ｉｎｔｈｅｆｉｖｅｃａｐ ｔｉｏｎｓ ．  

Ｂ ．Ｔｈｅｇｅｎｅｒａｔｅｄｃａｐ ｔｉｏｎｄｅｓｃｒｉｂｅｓｏｍｅｏｂ ｊｅｃｔｓｏｒａｃｔｉｏｎｓｔｈａｔａｐｐｅａｒｉｎｔｈｅｆ ｉｖｅ  

ｃａｐ ｔｉｏｎｓ ．  

Ｔｈｅｒｅｓｕｌｔｉｓ ：  

，一— ￣ …  

一＾ 图像描述：：  

Ｙ  

文本Ｐｒｏｍｐ ｔ ＞大语言模型  

Ｅｄ ｉｔ ｔｈｅｓｅｎ ｔｅｎｃｅ ｔｏｍａｋｅ ｉｔ  

格＞ ｗ ｉ ｔｈｏｕｔ ａｄｄ ｉｎｇ ａｎｙ  

ａｄｄ ｉｔ ｉｏｎａ ｌｏｂ ｊｅｃｔｓ ｏｒ ｄｅｔａ ｉ ｌｓｎｏｔ  

ｐｒｅｓｅｎｔ ｉｎ ｔｈｅｏｒ ｉｇ

ｉｎａ ｌｓｅｎｔｅｎｃｅ ：＾ ｆ  

乂图像摘述 ｎｏｔｅ ｔｈａｔ ｙｏｕｒ／ 

＇  

ｏｕｔｐｕｔ ｓｈｏｕ ｌｄｋｅｅｐ ｔｈｅｍｅａｎ ｉｎｇｄ 士 间／＆相：

＇＋  

’ ＜图像插述〉

，风格化匿ｊ塚描还  

ｒ  ｙ 

ｊ ｜？ 幻觉判别 ｉ  

！Ｙ ；  

ｉ数据风格判别

：▼Ｙ▼  

ｊ潍不合格 ｉ＂  

－３使用大模型生成图像描述时的校验过程  

在使用上述提示词对生成的风格化描述评分时 ， 本文还采用了上下文学习  

的思想 ， 即在真正的问题之前拼接数个问题的样例 ， 以令大语言模型学习到问  

５４  

第四章基于视觉提示词的可控非自回归图像描述  

题的回答方法 ， 更好地发挥大语言模型的能力 。 在本文的设置中使用了５

－ ｓｈｏｔ  

机制 ， 即在问题前拼接 ５个带答案的问题

－答案对 。  

综上 ， 基于上下文机制和提示词对生成数据进行校验的过程如图 ４ －４所示 。  

； 原始５条 ］

－ｓｈｏｔ｝Ｍｕｉｉｋ ；  

｜＾—

＝＾ ［

＾ 卜问题＋答案；  ：  

ｉ賊ｃａｐ ｔ ｉｏｎ Ｉ   ＾￣￣￣￣＾： Ｉ  

｜ （Ｂ＾） ｜

Ｉ 上 卜又

１ ｜  

＾ 丨 上下文

；ｃａｐ ｔｉｏｎ ＼

＾ ；  

； 问题 ：

丨上下文 ：  

； ＾ｌ＆ｃａｐ ｔｉｏｎ ：

ｉ（未校验） ：问题“ ：  

Ｉ ；  ｖ ｊ  

＂ ＂ ＂ ＂

Ｉ １ 

—＾馨大语言模型 —？ 校雖果  

图 ４ －４基于上下文机制和提示词对生成数据进行校验的过程  

对风格的验证则依托每种风格己经实现的风格判别器实现 ， 对于大语言模  

型输出的图像描述 ， 通过输入风格对应的判别器判断是否属于该风格 ， 如果属  

于 ， 则通过 ， 否则不通过 。  

４ ．３实验对比和分析  

４ ．３ ． １实验配置  

对于 ＣｏｎＵｎｉＣａｐ

， 本文采用与第三章相同的 ＭＳＣＯＣＯ数据集作为评估数  

据集 ， 同样遵循了Ｋａｒｐａｔｈｙ等人

［６７堪于 ＣＯＣＯ数据集划分的训练

－验证集  

以保证成果的可复现性 。  

硬件环境为Ｕｂｕｎｔｕ２２ ．０４操作系统 、 ＮＡＶＩＤＩＡＡ６０００显卡 、 Ｐｙ ｔｏｒｃｈ深度学  

习框架 、 Ｐｙｔｈｏｎ３ ．８版本 。  

本文采取 Ｄｒｏｐｏｕｔ机制以缓解模型的过拟合问题 ， 词嵌入层的丢弃率设置  

为 ０ ．９ ， Ｔｒａｎｓｆｏｒｍｅｒ块的丢弃率设置为 ０ ．７ 。 所有的模型权重在初始化时遵循均  

匀分布 ， 词嵌入的维度设置为 ５ １２维 ， 中间层的向量维度设置为 ５ １２维 。采用  

ＡｄａｍＷ作为模型的优化器 。 学习率设置均与第三章中讨论的ＵｎｉＣａｐ

—致 。  

对于可控图像描述相关的配置 ， 本文

一共设置了五种风格 ： 关于详略程度  

的风格包括简短 、 中等 、 详细 ， 关于情感极性的风格包括正面和负面 。 相应的  

情感提示词的维度设置为 ５ １２维 。  

５５  

北京邮电大学硕士学位论文  

生成质量评估方面 ， 对于生成质量 ， 采用学术界常用的图像描述评估指标  

进行评估 ， 包括标准的评估指标 ＣＩＤＥｒ， ＭＥＴＥＯＲ ，ＢＬＥＵ ， ＲＯＵＧＥ等 Ｐ 对于  

可控性的评估 ， 本文额外地在模型生成的数据上 ， 利用数据风格判别器判断生  

成的文本是否属于相应风格 ， 计算模型的控制成功率作为评估可控性的指标 。  

在基于大语言模型的可控图像描述数据生成中 ， 本文在正面和负面两个情  

感风格上应用 ＣｈａｔＧＰＴ３ ．５生成数据以进行实验 ， 对每个情感生成 ２５０００条图  

像描述 ， 经过校验后 ， 正面情感描述剩余 １６８６８条 ， 可用率 ６７ ．４７％ ， 负面情感  

描述剩余２００４５条 ， 可用率 ８０．１８％。  

４．３ ．２基线方法  

为了对比生成的质量 ， 本实验选取了近年的可控图像描述方法作为基线 ，  

这些方法简介如下 ：  

（ １ ）ＬａＢＥＲＴ ［４１］提出通过计算文本详略嵌入的方式控制生成文本的详细  

与否 ， 并提出了

一种基于双向语言模型的迭代细化非自回归图像描  

述方法 ， 支持对详略程度的控制 。  

（２）ＣｏｎＣａｐ

［４２ ］在自回归方法中首先提出了基于提示词的可控图像描述方  

法 ， 利用提示词对不同风格的图像进行训练 ， 最终得到可以控制图  

像描述风格的提示词 。  

４ ．３ ．３实验结果  

如表 ４ －３所示 ， 本实验在Ｋａｒｐａｔｈｙ划分的测试集上评估ＣｏｎＵｎｉＣａｐ的性能 ，  

对比了任务相似的ＬａＢＥＲＴ和ＣｏｎＣａｐ两个可控图像描述模型 ， 其中 ， ＬａＢＥＲＴ  

是基于嵌入的 ， ＬａＢＥＲＴ的详细等级 １４表示从简略到详细的风格表示 ，  

ＣｏｎＣａｐ和 ＣｏｎＵｎｉＣａｐ是基于提示词的 。 需要注意的是 ， ＣｏｎＣａｐ是基于大规模  

多模态预训练模型 ＢＬＩＰ２的 ， 因此指标相对较高 。 通过性能的对比 ， 可以得出  

ＣｏｎＵｎｉＣａｐ在生成各种风格的描述时的表现部分好于先前的最先进可控模型  

ＬａＢＥＲＴ ， 与基于大规模预训练模型的 ＣｏｎＣａｐ相比达到了可比较的表现 。 同时 ，  

在速度的比较上 ， 由于 ＬａＢＥＲＴ模型是非自回归的 ， 但是论文中没有给出具体  

的推理速度 ， 为了大致地比较推理速度 ， 本文从迭代步数估算了ＣｏｎＵｎｉＣａｐ相  

比ＬａＢＥＲＴ的速度优势 ： 由于ＣｏｎＵｎｉＣａｐ和ＬａＢＥＲＴ使用了相似的Ｔｒａｎｓｆｏｒｍｅｒ  

结构 ， 这里首先假设执行

一次前向的时间是相同的 ， 设为ｔ 。 ＬａＢＥＲＴ每步只执  

一次前向传播 ， ＣｏｎＵｎｉＣａｐ则每步执行两次 （ＵＥ和ＭＰ分别需要

一次前向传  

播 ， 详见第三章对ＵｎｉＣａｐ的生成策略的描述 ） 。 ＬａＢＥＲＴ设置了固定 ２５步的迭  

代步数 ， 因此共耗时２５ｔ ， ＣｏｎＵｎｉＣａｐ实现了生成时的动态退出机制 ， 并设置了  

４步的步数上限 ， 通过实验统计 ， 平均步数约为 ２ ．５ ， 因此ＣｏｎＵｎｉＣａｐ的平均耗  

时约为５ｔ ， 相比ＬａＢＥＲＴ可以达到五倍的加速 。 同时 ， 由于在ＵｎｉＣａｐ基础上实  

５６  

第四章基于视觉提示词的可控非自回归图像描述  

现的提示词机制并未对模型的结构进行改变 ， 因此在整体的推理速度上 ，  

ＣｏｎＵｎｉＣａｐ保持了ＵｎｉＣａｐ的高效性 。  

－３ＣｏｎＵｎ ｉＣａｐ与不同可控模型的对比  

模型风格Ｂ

－４ＭＥＴＥＯＲＲＯＵＧＥＣＩＤＥｒＳＰＩＣＥ  

无７７ ＿４０３５ ．００２７ ．９０５７ ．００１ １６ ．８０２ １ ．７０  

详细等级１７２ ．５０３０ ．００２５ ．４０５４ ．７０１０ １ ．６０１９ ．５０  

ＬａＢＥＲ丁详细等级２７７ ．６０３５ ．３０２８ ．４０５７ ．４０１ １８ ．２０２ １ ．８０  

详细等级３６６ ．８０２６ ．８０２８ ．６０５３ ． １０９０ ．５０２２ ．３０  

详细等级４５６ ． １０１９ ．９０２７ ．７０４６ ．９０３９ ．９０２２ ．２０  

－４０ ．５０３０ ．９０

－１３３ ．７０２３ ．８０  

－３９ ．９０３０ ．２０

－１３２ ．３０２３ ．００  

－３５ ． １０３０ ．９０

－１２２ ．９０２３ ．９０  

〇ｎａｐ高长度

－２６ ．９０３０ ．７０

－７ １ ．６０２５ ．００  

－２７ ．００２５ ．８０

－９７ ．６０２０ ．７０  

－３７ ．００２９ ．３０

－１２ １ ．５０２２ ．９０  

无８０ ．９９３６ ．３ １２８ ．９４５７．６０１２３ ．０６２ １ ．３０  

短长度７９ ．９８３４ ．６９２６ ．８０５５ ．９６１ １６ ．０ １２０ ．７０  

．中等长度７７ ．４２３２ ．７６２７ ．８８５５ ．９９１ １５ ．４７２ １ ．４８   Ｃ〇ｎｎ ＇Ｃａｐ高长度６２ ． １２１９ ． １２２４ ．８８４６ ．６２６６ ．８０１９ ． １９  

正面情感７４ ．９０２８ ．３２２５ ．５６５２ ．９４１０３ ．５３２０ ． １ １  

负面情感７５ ．５５３０ ．０７２６ ．３７５４ ． １０１０６ ．９３２０ ．３３  

４ ．３ ．４实验分析  

４ ．３ ．４ ． １控制能力分析  

为了研究ＣｏｎＵｎｉＣａｐ的控制能力 ， 本文对ＣｏｎＵｎｉＣａｐ进行风格控制的成功  

率进行了分析 。 ＣｏｎＵｎｉＣａｐ

— 共实现了５种风格 ， 对于所有的测试集图片 ， 利  

用 ＣｏｎＵｎｉＣａｐ的五种不同风格分别生成描述 ， 并通过数据风格判别器判断输出  

描述的风格是否与输入的风格

一致性通过精度衡量 ， 即

一致数量＋总数 。  

各个风格对应的精度如表４

－４所示 。  

－４ＣｏｎＵｎｉＣａｐ控制能力分析表  

风格精度   ￣ 短长度９９ ．６６  

中等长度９５ ． １６  

高长度３９ ． １０  

正面情感７０ ．３０  

负面情感６８ ．４６  

４ ．３Ａ２生成式风格化数据消融分析  

为了证明生成的风格化数据的有效性 ， 本文执行了生成式风格数据的消融  

实验 。 训练了

一个没有加入额外风格化数据的 ＣｏｎＵｎｉＣａｐ

， 并与原始模型进行  

５７  

北京邮电大学硕士学位论文  

比较。结果如表 ４ －５所示 。 实验结果表示额外的风格化数据提升了模型输出情  

感图像描述时的生成质量 。 由于成本问题 ， 本文标注的数据仅为小规模的情感  

风格化数据 ， 因此提升幅度有限 ， 但是本文所提出的生成数据方法完全可以拓  

展到更大的规模和更多的风格上 ， 以达到更好的效果 。  

表４ －５生成式数据消融分析  

设置风格Ｂ －ｌＢ －４ＭＥＴＥＯＲＲＯＵＧＥＣＩＤＥｒＳＰＩＣＥ  

正面情感７３ ．２０２８ ． １０２５ ．４１５２ ．５８１０３ ．４０２０ ．０１  

负面情感乃刀２９ ．３４２５ ．９７５３ ．２６１０５ ．４８２０ ．３０  

， 正面情感７４ ．９０２８ ．３２２５ ．５６５２ ．９４１０３ ．５３２０ ．１ １   ｗ／ｊｆ ｅｒ   负面情感７５ ．５５３０ ．０７２６ ．３７５４ ． １０１０６ ．９３２０ ．３３  

４ ．３ ．５案例分析  

本节提供了ＣｏｎＵｎｉＣａｐ的

一些生成案例 ， 以建立对ＣｏｎＵｎｉＣａｐ生成的优势  

和不足的直观认识 。 如图 ４

－５所示 。 首先从详细度进行分析 ， 在第

一张图片中 ，  

三个不同详细度的描述以不同的详细度描述了图片 ， 短长度的描述仅包括女人  

坐在卫生间 ； 中等长度的描述则包括了女人具体坐的位置 ： 地板 ； 高长度的描  

一步地注意到女人的坐姿信息 ， 并给出了最详细的描述。 在第二张图片  

中同样可以注意到类似的效果 ， 中等长度的描述额外注意到了叉子 ， 高长度的  

描述则注意到了更细节的信息 ， 包括蛋糕的种类和奶油 。在第三张图片中 ， 中  

等长度的描述注意到了办公桌 ， 而高长度的描述则注意到了办公桌上的书籍 。  

从情感控制的角度分析 ， 样例展示了ＣｏｎｌＭＣａｐ具有的控制描述情感能力 ，  

ＣｏｎＵｎｉＣａｐ可以通过改变用词以保持句子的语义的同时改变句子的风格 ， 如使  

用ｓａｄ ，ｌｏｎｅｌｙ ， ｐｏｏｒ等使句子变得偏向消极 ， 使用ｈａｐｐ ｌｉｙ ， ｄｅｌｉｃｉｏｕｓ ， ｎｅａｔｌｙ  

等词使句子变得偏向积极 。  

５８  

第四章基于视觉提示词的可控非自回归图像描述  

？短长度 ： ａｗｏｍａｎ ｉｓｓ ｉ ｔｔｉｎｇｏｎｔｈｅｔｏ ｉ ｌｅｔ  

‘麵中等长度

＿ ａｗｏｍａｎ ｉｓｓｉｔｔｉｎｇｏｎｔｈｅｆｌｏｏｒｉｎｔｈｅｂａ ｔｈｒｏｏｍ  

： ａｗｏｍａｎｉｓｓ ｉ ｔｔ ｉｎｇｏｎａｂａｔｈｔｕｂ ｉｎｔｈｅｂａ ｔｈｒｏｏｍｗ ｉｔｈｈｅｒｆｅｅｔｕｐ ｉｎｈｅｒｈｅａｄ  

正面情感 ：ａｗｏｍａｎｈａｐｐ

ｉ ｌｙ ｓ ｉｔｔ ｉｎｇｏｎａｔｏｗｅ ｌｕｐ

ｉｎｔｈｅｂａｔｈｒｏｏｍ  

负面情感 ： ａｗｏｍａｎ ｉｓｓ ｉ ｔｔｉｎｇｏｎａｂａｔｈｔｕｂｓ ｉ ｔｔ ｉｎｇ

ｉｎｔｈｅｅｍｐ ｔｙｂａｔｈｒｏｏｍ  

： ａ ｐ

ｉｅｃｅｏｆ ｃａｋｅｓ ｉｔｓｏｎａ ｐ

ｌａｔｅ  

ｗ中等长度 ： ａ ｐ

ｉｅｃｅｏｆ ｃａｋｅａｎｄａｆｏｒｋｏｎｔｈｅ ｐ

ｌａｔｅ  

、 】 高长度 ： ａ ｐ

ｌａｔｅｏｆ ｃｈｏｃｏｌａｔｅｃａｋｅｗ ｉｔｈａｎｃｒｅａｍａｎｄｆｏｒｋｏｎｔｈｅｉｔ  

■譯隊乙／正面情感 ： ａｄｅ ｌ ｉｃ ｉｏｕｓ ｐ

ｉｅｃｅｏｆ ｃａｋｅｗ ｉ ｔｈｗｈ ｉｐｐｅｄｓ ｉｔｓｏ丨ｉａ ｐ

ｌａｔｅ  

负面倩感 ： ａｌｏｎｅ ｌｙｐ

ｉｅｃｅｏｆｃａｋｅａｎｄａｎｅｍｐ ｔｙｓ ｉｔｓｏｎａ ｐ ｌａｔｅ  

短长度 ： ａ ｌａｐ ｔｏｐ

ｌａｃｅｄｏｎｔｈｅｄｅｓｋ  

： ａｌａｐ ｔｏｐｃｏｍｐｕｔｅｒｓ ｉ ｔｓｏｎａｏｆｆｉｃｅｄｅｓｋ  

二 ， 高长度 ： ａｌａｐ ｔｏｐｃｏｍｐｕｔｅｒｓ ｉｔｓｏｎｔｈｅｄｅｓｋｗ ｉｔｈｖａｒｉｏｕｓｂｏｏｋｓｏｎｉｔ  

正面情感 ： ａｌａｐ ｔｏｐｃｏｍｐｕ ｔｅｒｉｓｎｅａｔ ｌｙｐ

ｌａｃｅｄｏｎａｄｅｓｋ  

负面情感 ： ａｎａｄｏｒｎｅｄｌａｐ

ｔｏｐｃｏｍｐｕｔｅｒｓｉｔｔ ｉｎｇｏｎａｎｅｍｐ ｔｙｗｏｏｄｅｎｄｅｓｋ  

－５ＣｏｎＵｎｉＣａｐ的生成案例展示  

４ ．４本章小结  

本章在图像描述方向开展了可控非自回归图像描述模型的研究 ， 在第三章  

提出的 Ｕｎ ｉＣａｐ的基础上提出了ＣｏｎＵｎ ｉＣａｐ 。 为了实现可控的图像描述模型 ，  

ＣｏｎＵｎｉＣａｐ中引入了基于提示词的风格控制机制 ， 由于有着ＵｎｉＣａｐ强大的推理  

速度作为基础 ， ＣｏｎＵｎｉＣａｐ可以满足现实可控图像描述应用中对于推理速度的  

要求 。 随后 ， 本章介绍了

一种基于大语言模型生成式地训练可控图像描述的方  

法 ， 并提出了

一种基于双向预训练语言模型的数据风格判别方法 ， 加强了对大  

语言模型生成数据的校验 。 最后 ， 本章在 ＭＳＣＯＣＯ 图像描述数据集上进行了  

大量的实验和消融实验 ， 从模型的性能 、 消融实验 、 案例分析等方面充分验证  

了ＣｏｎＵｎ ｉＣａｐ方法的可行性和有效性 。  

５９  

第五章非自回归图像描述系统  

第五章非自回归图像描述系统  

基于本文对非自回归图像描述的研究 ， 本章设计了

一个非自回归图像描述  

系统 ， 旨在可视化应用本文的研究成功 ， 让更多的人体验到多模态广阔的应用  

场景 。 本系统采用前后端分离架构 ， 前端采用ＶＵＥ３设计 ， 后端采用 ｐｙｔｈｏｎ的  

ｆ ｌａｓｋ库实现 。  

５ ． １系统需求分析  

经过对提出算法的分析 ， 非自回归图像描述系统主要包含以下需求 ：  

（ １ ） 用户管理功能 ： 对于

一个系统而言 ， 用户管理功能至关重要 。 对普  

通用户而言 ， 系统支持注册和登录操作 ； 而对于管理员 ， 则提供了  

展示用户信息列表的功能 。 此外 ， 为了优化使用体验 ， 系统还设计  

了游客模式 ， 允许用户在不登录的情况下直接访问 。  

（２ ） 图像描述功能 ： 该功能包含无条件的基于编辑的非自回归图像描述  

和可控非自回归图像描述 ， 并可以通过用户的选择切换功能 。 包括  

切换可控非自回归图像描述中图像描述的风格 。  

（３ ） 数据管理 ： 在本系统中 ， 会产生各种各样的用户数据 ， 如用户上传  

的图片和用户的账号密码 ， 因此需要将各种各样的数据存到数据库  

中 ， 方便查看 。  

（４） 方便部署 ： 对于深度学习系统而言 ， 部署需要环境配置 ， 因此整个  

系统需要以镜像的方式提供 。  

５ ．２系统整体设计  

基于系统的需求分析 ， 本系统的整体架构如图 ５ －１所示 ， 后端包括模型层 ，  

框架层 ， 部署层三层封装 。 模型层主要负责封装基础的模型前向推理过程 ， 供  

后续流程调用 。框架层则负责调用 ｆ ｌａｓｋ和 ｓｑ ｌｉｔｅ等库编写业务逻辑代码 ， 实现  

ＡＰＩ 。 框架层和模型层实现了整个后端的功能 。 部署层中 ， 首先使用 ｇｕｎｉｃｏｍ  

包装ｆ ｌａｓｋ应用 ， ｇｕｎｉｃｏｍ是

一个实现了ＷＳＧＩ协议的服务 ， 可以提供多进程支  

持 ， 提升多核服务器的处理性能 。 然后将整个 ｇｕｎｉｃｏｍ部署的后端程序打包为  

ｄｏｃｋｅｒ ， 支持多端的分发 ， 使得整个系统具有可拓展性和可移植性 。  

前端则使用Ｖｕｅ． ｊｓ框架实现 ， Ｖｕｅ框架是

一种流行的 ＪａｖａＳｃｒ ｉｐｔ框架 ， 用于  

构建用户界面和单页应用程序 。 它的设计目标是通过尽可能简单的 ＡＰＩ实现响  

６ １  

北京邮电大学硕士学位论文  

应的数据绑定和组合的视图组件 。 Ｖｕｅ的核心库只关注视图层 ， 易于学习且集  

成 。  

本系统的前后端采用分离的设计 ， 后端经过包装 ， 提供调用方便的 ＡＰＩ ，  

然后前后端系统遵循预先设计的通信方式完成用户的请求 。  

ｍｍ  

ｖ ｙ   八   调用ＡＰ Ｉ提供ＡＰ Ｉ  

ｉ  

部署层冰ｇｕｎ

丨ｃｏｒｎ  

ｄｏｃｋｅｒ＾  

【   、   后端 １ 框架层隱ＱＬｉｔｅ   ／   八  

＊ －— — ｚｍｚｚｚｉｚｉｚｚｉｚｚｉｉｚ  、  

模型层Ｕｎ ｉＣａｐ

ＩＣｏｎＵｎ ｊＣａｐ  

－１ 系统整体架构设计  

５ ．３前端设计  

本系统的注册登录界面如图 ５

－２所示 ， 当用户首次进入系统时 ， 可以选择  

登录或者注册 。 登录选项涉及到对数据库的查询 ， 以确认用户是否已注册 ： 如  

果用户已注册且密码匹配 ， 则允许登录 ； 如果用户存在但密码不匹配 ， 则登录  

尝试将被拒绝 ； 若未找到用户信息 ， 则系统将自动进行用户注册 。 注册过程同  

样需要验证数据库中用户的存在性 ， 仅当用户未被记录时 ， 注册才能完成 。  

一开始见到的界面如图 ５

－３所示 ， 左侧是图片预览区 ， 右侧是  

操作区 ， 下边是结果展示区 。 用户将会被要求点击选择文件按钮 ， 选择

一张本  

地的图片上传并预览 ， 预览无误后 ， 用户需要点击描述风格选单 ， 选择所需要  

的风格 ， 额外地 ， 描述风格中有

” 选项 ， 对应无条件的图像描述生成  

（ＵｎｉＣａｐ ） 。 最后 ， 点击

“ 请求图像描述

” 按钮 ， 上传图像到服务端 ， 服务端  

将处理用户的请求 。  

６２  

第五章非自回归图像描述系统  

欢迎登录  

租户８  

密骑  

ＥＳＩＳ＾Ｓ  

图 ５ －２前端注册登录界面  

图像描述项目  

ｆ ｉＳＳｐ  １州？以 ： （符  

： ： ＾  

－３前端服务页面

－输入前  

图像描述项目  

ＡＢＴ＾ＲＳＪ ＩＵＷｉ－ ｆ ｃ３？ ．ＵｓＭ ？ ｉ  （Ｍ３   ＾  － ｆ ｆ   ｒ  

， ：厲 １  

：喻邏！：：  

＜ＰＭ ｌ ： ｉ

－ｗｈ ｉ ｔＰａｎｄｓ ｉ ｔｔ ｉｎｇ  

ｉＶ＾２ ：ｕｗ ｉ ｉ ｉ ｔ？ｐ ｌａｎ ｔ

＊ｓ ｉ ｔ ｔ ｉｎｇｏｎｈｔａ ｒｍａｃ  

．＾３ ：ａｗｉｌｌｉ ｔ？ａｎｄｒｅ ｅ ｌｐｌａｎｅ＆ ｉ ｔ ｔ ｉｎ？ｏｒ；ａｔａｒｍａ ｃ

－４前端结果展示  

６３  

北京邮电大学硕士学位论文  

用户点击请求图像描述成功后的页面如图 ５

－４所示 ， 首先 ， 图片将被预览  

在左侧的窗口里 ， 其次 ， 由于本文提出的两个模型都是基于编辑操作的 ， 因此  

后端将返回编辑操作的过程并由前端展示在页面的下方 。  

５ ．４后端设计  

后端系统主要实现的是数据的处理 ， 模型前向过程的封装 ， 以及数据的返  

回 。  

数据处理方面 ， 后端接收到前端发送的 ＰＯＳＴ请求后 ， 会将其中的请求字  

段取出 ， 拿到用户提交的图片 ， 转换为模型能够识别的形式 ， 用于后续调用模  

型 。  

模型前向过程的封装主要体现在对模型的输入输出的封装 ， 首先 ， 本文实  

现了两个模型 ： ＵｎｉＣａｐ和 ＣｏｎＵｎｉＣａｐ ， 因此需要根据用户的选择将调用不同的  

模型 。其次 ， 由于前端支持可视化非自回归模型迭代的每

一步 ， 因此需要对每  

一步的中间结果进行保存 ， 体现在最后的输出结果里 ， 在前端展示每

一步的编  

辑过程。  

数据返回的过程主要体现在需要将模型的输出转换为对 ＡＰＩ的响应 ， 具体  

来说 ， 前端页面期望的响应是 ｊｓｏｎ格式的数据 ， 因此 ， 需要将模型的输出首先  

通过分词器转换为文本形式 ， 再将保留的中间步骤信息堆叠成为列表 ， 最后将  

这些数据都转化为 ｊｓｏｎ格式 ， 返回给前端页面渲染 。  

５ ．５系统工作流程  

本系统的工作流程可以总结为图 ５ －５ 。 首先 ， 用户通过注册和登录操作进入  

系统 ， 在注册阶段 ， 用户的数据会被存入数据库中 。在登录阶段 ， 用户的数据  

从数据库中被取出进行校验 。  

用户登录进入系统后 ， 可以在前端与后端进行交互 ， 当用户确定需求 ， 发  

一个图像描述请求后 ， 后端将会根据前端所提供的请求参数确定用户的请  

求类型 ， 并将用户的请求分配给两个不同的模型 ， 在模型底层 ， 需要对用户输  

入的实际数据进行处理 ， 使其变成模型可以前向推理的格式 ， 随后模型进行前  

向推理 ， 推理的结果经过封装返回上级框架并返回给前端并展示 。 完成

一次图  

像描述过程 。  

６４  

第五章非自回归图像描述系统  

！ｆ ｛

） ｉ ；  

； 用户注册 — 用户登录 ￣？ 用户交互 ：  

＇ 、— ｔ

■  前纖辑 ｉ  

Ｉ ｙ ｒｖ

；  ／ 

＾ ！  

；登莉郷接口图像描趨口

丨 ｉ  

！ Ｉ／

！ｉ ；  

．［根据参数选择 ｜  

；ｉ ；  

Ｉ择ｔ   

；  ？  

！＾ ｊ  

ｉＣｏｎＵｎ  丨Ｃａｐ＋■

； ＾果 ｉ  

Ｏｎ＾ａ ｆＴ

－ ｉＵｊ ＇０３３＾

－Ｃ＝ Ｉ  

－５ 系统整体工作流程  

６５  

第六章总结与展望  

第六章总结与展望  

６ ． １工作总结  

本文开展了基于非自回归方法的图像描述研宂 ， 目前 ， 主流研究中存在两  

个问题亟待解决 ： １） 自回归图像描述方法效率低 ； ２）模型缺乏可控性 。 为此 ，  

本文开展了基于编辑的非自回归图像描述和基于视觉提示词的可控非自回归图  

像描述的研宄 。 分别聚焦于图像描述任务的效率和可控性这两个对其实际应用  

有意义的研究方向 。  

针对问题 １ ， 本文提出了

一编辑的非自回归图像描述方法 ， 分  

一种可以用更少推理次数完成图像描述编辑操作的统

一编辑器和适应  

编辑操作的两阶段非自回归图像描述训练策略 。 统

一编辑器实现了

一种新颖的  

位置预测操作 ， 这种操作可以同时涵盖多个传统的编辑操作 ， 因此可以用

一个  

解码器完成以往多个解码器完成的工作 ， 提升生成速度 。 两阶段训练策略在图  

像描述的两个传统训练阶段 ： 交叉熵和强化学习阶段都做出了优化 ： 在交叉熵  

阶段 ， 为了解决编辑操作数据在训练数据集中缺失的问题 ， 本文提出了多路径  

交叉熵训练方法 ， 通过多种基于加噪的方式生成多样且高质量的编辑数据训练  

数据 。 在强化学习阶段 ， 本文提出了

一种阈值采样的强化学习算法 ， 借助非自  

回归方法的并行优势进

一步地提升了强化学习训练的效果 。  

针对问题 ２ ， 本文提出了

一种基于视觉提示词的可控非自回归图像描述方  

法 ， 分别设计了

一种基于视觉提示词的控制信号机制和

一种基于大语言模型的  

风格化数据生成方法 。基于视觉提示词的控制信号机制通过将数据集划分为不  

同的风格子集 ， 并初始化对应的视觉提示词向量 ， 在训练和推理的时候将视觉  

提示词拼接在输入特征前 ， 从而引入控制信号 。 基于大语言模型的风格化数据  

生成方法借助大语言模型强大的零样本生成与上下文学习能力 ， 可以动态地根  

据风格生成风格化的图像描述训练数据 ， 缓解了传统方法面临的风格化数据不  

足的问题 ， 同时 ， 针对大语言模型生成数据的不稳定性 ， 本文提出了多步的数  

据验证机制以保证数据质量 。  

本文提出的两个创新点均在 ＭＳＣＯＣＯ数据集上进行了实验 ， 证明了提出  

的方法的有效性 。  

６７  

北京邮电大学硕士学位论文  

６ ．２未来工作展望  

尽管本文在基于编辑的非自回归图像描述方法和基于视觉提示词的可控非  

自回归图像描述方法都取得了

一定的进展 ， 但是本文的成果依然存在不足之处 ，  

还有以下的方向可以继续研究 ：  

１ ） 在 ＵｎｉＣａｐ 中 ， 本文提出了基于加噪的方式创建编辑训练数据的多路径  

交叉熵训练方法 。 这种方法的原理在于对于句子 Ａ ， 首先加噪得到噪音句子 Ｂ ，  

然后通过算法生成加噪后的文本Ｂ到加噪前的文本Ａ的编辑过程 。 这样的学习  

一个显著的缺点在于比较

。 这是因为实际上的需要是模型把 Ｂ  

一个更能描述输入图像的文本 ， 而不是把 Ｂ编辑成 Ａ 。 这样的不匹配会  

一定程度上学习到的不是最优解 。 关于这

一点的直观理解见图 ６

－ １ ， 但  

是如何生成最优解 ， 也就是正确描述这个图像且和加噪后文本编辑距离最小的  

一个难题 ， 这

一问题随着图文多模态大语言模型的研究持续深入有望  

得到解决 。  

实际学到的   

目前的 丨ａｂｅ 丨生成？〇   ＠參⑩  

＾ｎｏｉｓｅｄ０／（％Ｌ．）￡＾２  

、 ／  

ｌａｂｅ ｌ ｉｎｐｕｔ／Ｊ  

Ｏ ９ ＾ －ｎｏｉｓｅｄＯｆ＾ｎＭｓｅｄ  

最终模型学到的是我们希琴   

腿我们胃望鍵学到的是 （身

］   “一介于一个句子 ：、  

＝介于五个ｇｔＳ ：间的句子且考虑和／  

ｎｏ ｉｓｅｄ的缤辑距离尽置小 ＩＶ  

Ｓ ＾ｎｏｉｓｅｄ  

－ １训练时存在的不匹配问题的直观理解  

２ ）在非自回归的可控图像描述中 ， 目前的实现方法是在

一次推理中 ， 不同  

的步之间保持

一个风格提示词 ， 因此也只支持同时接收

一个风格信号 。 然而 ，  

一个多步之间接受不同风格信号的可控非自回归图像描述方法在理论上是  

可行的 ， 但是需要在数据的生成和处理上做进

一步的努力 。  

６８  

参考文献  

参考文献  

［ １ ］沈佳敏，鲍秉坤 ．基于深度学习的广告布局图片美学属性评价［Ｊ ］

．计算机技术与发展 ，  

２０２１ ， ３ １ （３ ） ：３９ －４４  

［２ ］徐守坤 ，倪楚涵，吉晨晨，等 ．基于ＹＯＬＯＶ３的施工场景安全帽佩戴的图像描述［Ｊ ］

．计算机  

科学，２０２０ ，４７ （８）

：２３３ －２４０  

［３ ］杨润霞，邵洁，罗岩 ，等 ．基于编解码器的电力施工场景可控图像字幕生成［Ｊ］

？ 电网技术 ，  

２０２２ ，４６ （７） ：２５７２ －２５８ １  

［４ ］陈悦，郭宇，谢圆琰，等 ．基于图像描述算法的离线盲人视觉辅助系统［Ｊ］

．电信科学， ２０２２ ，  

３８（１ ）

：６１ －７２  

［５ ］ＢｅｍａｒｄｉＲ ，＾ａｋｉｃｉＲ ，ＥｌｌｉｏｔｔＤ ，ｅｔａｌ ．Ａｕｔｏｍａｔｉｃｄｅｓｃｒ ｉｐ ｔｉｏｎ ｇｅｎｅｒａｔｉｏｎｆ ｒｏｍｉｍａｇｅｓ ：ａｓｕｒｖｅｙ  

ｏｆ ｍｏｄｅｌｓ ，ｄａｔａｓｅｔｓ ，ａｎｄｅｖａｌｕａｔｉｏｎｍｅａｓｕｒｅｓ［Ｊ］

．Ｊ ．Ａｒｔｉｆ．Ｉｎｔｅｌｌ．Ｒｅｓ ． ，２０１６ ，５５ ：４０９

＊４４２  

［６ ］Ｘ＾ｎｙａｌｓＯ ，ＴｏｓｈｅｖＡ ，Ｂｅｎｇ ｉｏＳ ，ｅｔａｌ ．ＳｈｏｗａｎｄＴｅｌｌ ：Ａｎｅｕｒａｌｉｍａｇｅｃａｐｔｉｏｎｇｅｎｅｒａｔｏｒ ［Ｃ］／／  

ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄＰａｔｔｅｒｎＲｅｃｏｇｎｉｔ ｉｏｎ．２０１５ ：  

３ １５６ －３ １６４  

［７ ］ＳｚｅｇｅｄｙＣ ，ＷｅｉＬｉｕ ，Ｙａｎｇｑ ｉｎｇＪｉａ ，ｅｔａｌ ．Ｇｏｉｎｇｄｅｅｐｅｒｗｉｔｈｃｏｎｖｏｌｕｔｉｏｎｓ［Ｃ ］ ／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆ  

ｔｈｅＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄＰａｔｅｒｎＲｅｃｏｇｎｉｔｉｏｎ．２０１５ ：１

－９  

［８］ＫｒｉｚｈｅｖｓｋｙＡ ，ＳｕｔｓｋｅｖｅｒＩ ，ＨｉｎｔｏｎＧＥ ．ＩｍａｇｅＮｅｔｃｌａｓｓｉｆ ｉｃａｔｉｏｎｗｉｔｈｄｅｅｐｃｏｎｖｏｌｕｔｉｏｎａｌ  

ｎｅｕｒａｌｎｅｔｗｏｒｋｓ

．ＡｄｖａｎｃｅｓｉｎＮｅｕｒａｌＩｎｆｏｒｍａｔｉｏｎＰｒｏｃｅｓｓｉｎｇＳｙｓｔｅｍｓ ，２０１２ ，２５  

［９］ＳｉｍｏｎｙａｎＫ ， ＺｉｓｓｅｒｍａｎＡ ．Ｖｅｒｙｄｅｅｐｃｏｎｖｏｌｕｔｉｏｎａｌｎｅｔｗｏｒｋｓｆｏｒｌａｒｇｅ

－ｓｃａｌｅｉｍａｇｅ  

ｒｅｃｏｇｎｉｔ ｉｏｎ ［Ｊ ］

．ａｒＸｉｖ ｐｒｅｐｒ ｉｎｔａｒＸｉｖ： １４０９ ． １５５６ ，２０１４ ．  

［ １０ ］ＨｅＫ ｓＺｈａｎｇＸ ，ＲｅｎＳ ，ｅｔａｌ ．Ｄｅｅｐｒｅｓｉｄｕａｌｌｅａｒｎｉｎｇｆｏｒｉｍａｇｅｒｅｃｏｇｎｉｔｉｏｎ ［Ｃ ］／／Ｐｒｏｃｅｅｄｉｎｇｓ  

ｏｆｔｈｅＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄＰａｔｔｅｒｎＲｅｃｏｇｎｉｔｉｏｎ．２０１６ ：７７０ －７７８  

［１ １ ］ＸｕＫ ，ＢａＪＬ ，ＫｉｒｏｓＲ ，ｅｔａｌ ．Ｓｈｏｗ，ａｔｔｅｎｄａｎｄｔｅｌｌ ：ｎｅｕｒ ａｌｉｍａｇｅｃａｐｔｉｏｎｇｅｎｅｒａｔｉｏｎｗｉｔｈ  

ｖｉｓｕａｌａｔｅｎｔｉｏｎ ［Ｃ ］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ３２ｎｄＩｎｔｅｒ ｎａｔ ｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＩｎｔｅｒｎａｔｉｏｎａｌ  

ＣｏｎｆｅｒｅｎｃｅｏｎＭａｃｈｉｎｅＬｅａｒ ｎｉｎｇ

－ Ｖｏｌｕｍｅ３７ ．２０１５ ：２０４８ －２０５７  

［ １２ ］ＳｕｇａｎｏＹ ，ＢｕｌｌｉｎｇＡ ．Ｓｅｅｉｎｇｗｉｔｈｈｕｍａｎｓ ：Ｇａｚｅ －ａｓｓｉｓｔｅｄｎｅｕｒａｌｉｍａｇｅｃａｐｔｉｏｎｉｎｇ［Ｊ］

．ａｒＸｉｖ  

ｐｒｅｐｒｉｎｔａｒＸｉｖ：１６０８ ．０５２０３ ，２０１６ ．  

［ １３ ］ＡｎｄｅｒｓｏｎＰ ，ＨｅＸ ５ＢｕｅｈｌｅｒＣ ５ｅｔａＬＢｏｔｔｏｍ

－ｕｐａｎｄｔｏｐ

－ｄｏｗｎａｔｔｅｎｔｉｏｎｆｏｒｉｍａｇｅｃａｐｔ ｉｏｎｉｎｇ  

ａｎｄｖｉｓｕａｌ ｑｕｅｓｔｉｏｎｍｉｓｗｅｒ ｉｎｇ［Ｃ ］ ／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎ  

ａｎｄＰａｔｅｒｎＲｅｃｏｇｎｉｔｉｏｎ．２０ １８ ：６０７７ －６０８６  

［ １４ ］ＹａｎｇＸ ，ＴａｎｇＫ ，ＺｈａｎｇＨ ，ｅｔａｌ ．Ａｕｔｏ

－ｅｎｃｏｄｉｎｇｓｃｅｎｅｇｒａｐｈｓｆｏｒｉｍａｇｅ  

ｃａｐｔｉｏｎｉｎｇ［Ｃ ］ ／／ＰｒｏｃｅｅｄｍｇｓｏｆｔｈｅＩＥＥＥ／ＣＶＦＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄＰａｔｅｒｎ  

６９  

北京邮电大学硕士学位论文  

Ｒｅｃｏｇｎｉｔｉｏｎ．２０１９ ：１０６８５ － １０６９４  

［１５ ］ＹａｏＴ ，ＰａｎＹ ，ＬｉＹ ，ｅｔａｌ ．ＨｉｅｒａｒｃｈｙＰａｒｓｉｎｇｆｏｒｉｍａｇｅｃａｐｔｉｏｎｉｎｇ［Ｃ ］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ  

ＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄＰａｔｔｅｒｎＲｅｃｏｇｎｉｔｉｏｎ ．２０１９ ：２６２ １ －２６２９  

［ １６ ］ＬｕＪ ，ＸｉｏｎｇＣ ｓＰａｒｉｋｈＤ ，ｅｔａｌ ．Ｋｎｏｗｉｎｇｗｈｅｎｔｏｌｏｏｋ：ａｄａｐｔｉｖｅａｔｔｅｎｔｉｏｎｖｉａａｖｉｓｕａｌ  

ｓｅｎｔｉｎｅｌｆｏｒｉｍａｇｅｃａｐｔｉｏｎｉｎｇ［Ｃ ］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒ＼ｌｓｉｏｎ  

ａｎｄＰａｔｔｅｒｎＲｅｃｏｇｎｉｔｉｏｎ．２０ １７ ：３７５ －３８３  

［ １７ ］ＡｎｅｊａＪ ，ＤｅｓｈｐａｎｄｅＡ ，ＳｃｈｗｉｎｇＡＧ ．Ｃｏｎｖｏｌｕｔ ｉｏｎａｌｉｍａｇｅｃａｐｔｉｏｎｉｎｇ［Ｃ ］ ／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆ ｔｈｅ  

ＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄＰａｔｅｒｎＲｅｃｏｇｎｉｔｉｏｎ ．２０１８ ：５５６１ －５５７０  

［ １８ ］ＶａｓｗａｎｉＡ ， ＳｈａｚｅｅｒＮ ， ＰａｒｍａｒＮ ， ｅｔａｌ ．Ａｔｅｎｔ ｉｏｎｉｓａｌｌｙｏｕｎｅｅｄ ［Ｊ］

．ＡｄｖａｎｃｅｓｉｎＮｅｕｒａｌ  

ＩｎｆｏｒｍａｔｉｏｎＰｒｏｃｅｓｓｉｎｇＳｙｓｔｅｍｓ ，２０１７ ，３０ ．  

［ １９ ］ＨｅｒｄａｄｅＳ ，ＫａｐｐｅｌｅｒＡ ，ＢｏａｋｙｅＫ ，ｅｔａｌ ．Ｉｍａｇｅｃａｐ ｔｉｏｎｉｎｇ

：ｔｒａｎｓｆｏｒｍｉｎｇｏｂ ｊｅｃｔｓｉｎｔｏ  

ｗｏｒｄｓ ［Ｊ］

．ＡｄｖａｎｃｅｓｉｎＮｅｕｒａｌＩｎｆｏｒｍａｔｉｏｎＰｒｏｃｅｓｓｉｎｇＳｙｓｔｅｍｓ ，２０１９ ， ３２  

［２０ ］ＧｕｏＬ ，ＬｉｕＪ ５ＺｈｕＸ ，ｅｔａｌ ．Ｎｏｒｍａｌｉｚｅｄａｎｄ ｇｅｏｍｅｔｒｙ

－ａｗａｒｅｓｅｌｆ －ａｔｅｎｔ ｉｏｎｎｅｔｗｏｒｋｆｏｒｉｍａｇｅ  

ｃａｐｔｉｏｎｉｎｇ［Ｃ ］ ／／ＰｒｏｃｅｅｄｍｇｓｏｆｔｈｅＩＥＥＥ／ＣＶＦＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄＰａｔｅｒｎ  

Ｒｅｃｏｇｎｉｔｉｏｎ．２０２０ ：１０３２７ －１０３３６  

［２１ ］ＣｏｍｉａＭ ， ＳｔｅｆａｎｉｎｉＭ ，ＢａｒａｌｄｉＬ ，ｅｔａｌ ．Ｍｅｓｈｅｄ －Ｍｅｍｏｒｙｔｒａｎｓｆｏｒｍｅｒｆｏｒｉｍａｇｅ  

ｃａｐ ｔｉｏｎｉｎｇ［Ｃ ］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＩＥＥＥ／ＣＶＦＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄＰａｔｔｅｒｎ  

Ｒｅｃｏｇｎｉｔｉｏｎ，２０２０ ：１０５７８ －１０５８７  

［２２ ］ＷａｎｇＺ ，ＹｕＪ ，ＹｕＡＷ ５ｅｔａｌ ．Ｓｉｍｖｌｍ ：Ｓｉｍｐ ｌｅｖｉｓｕａｌｌａｎｇｕａｇｅｍｏｄｅｌ ｐｒｅｔｒａｉｎｉｎｇｗｉｔｈｗｅａｋ  

ｓｕｐｅｒｖｉｓｉｏｎ［Ｊ ］

．ａｒＸｉｖ ｐｒｅｐｒｉｎｔａｒＸｉｖ ：２１０８ ．１０９０４ ，２０２１ ．  

［２３ ］ＷａｎｇＪ ，ＹａｎｇＺ ？ＨｕＸ ，ｅｔａｌ ．Ｇｉｔ ：Ａｇｅｎｅｒａｔｉｖｅｉｍａｇｅ －ｔｏ － ｔｅｘｔｔｒａｎｓｆｏｒｍｅｒｆｏｒｖｉｓｉｏｎａｎｄ  

ｌａｎｇｕａｇｅ［Ｊ］

．ａｒＸｉｖ ｐｒｅｐｒｉｎｔａｒＸｉｖ：２２０５ ． １４１００ ，２０２２ ．  

［２４ ］ＬｉＪ，ＬｉＤ ５ＳａｖａｒｅｓｅＳ ５ｅｔａｌ ．Ｂｌｉｐ

－２ ：Ｂｏｏｔｓｔｒａｐｐ ｉｎｇ ｌａｎｇｕａｇｅ

－ ｉｍａｇｅｐｒｅ

－ｔｒａｉｎｉｎｇｗｉｔｈｆ ｒｏｚｅｎ  

ｉｍａｇｅｅｎｃｏｄｅｒｓａｎｄｌａｒｇｅｌａｎｇｕａｇｅｍｏｄｅｌｓ［Ｃ ］／／ＩｎｔｅｍａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＭａｃｈｉｎｅ  

Ｌｅａｒ ｎｉｎｇ

．２０２３ ：１９７３０

－１９７４２ ．  

［２５ ］ＷａｎｇＰ ，ＹａｎｇＡ ，ＭｅｎＲ ，ｅｔａｌ ．Ｏｆａ ：Ｕｎｉｆ ｙ ｉｎｇａｒｃｈｉｔｅｃｔｕｒｅｓ ，ｔａｓｋｓ ，ａｎｄｍｏｄａｌｉｔｉｅｓｔｈｒｏｕｇｈａ  

ｓｉｍｐ ｌｅｓｅｑｕｅｎｃｅ －ｔｏ

－ｓｅｑｕｅｎｃｅｌｅａｒｎｉｎｇｆ ｒａｍｅｗｏｒｋ ［Ｃ］／／ＩｎｔｅｍａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＭａｃｈｉｎｅ  

Ｌｅａｒｎｉｎｇ

．２０２２ ：２３３ １８ －２３３４０ ．  

［２６ ］ＲａｎｚａｔｏＭ ，ＣｈｏｐｒａＳ ，ＡｕｌｉＭ ，ｅｔａｌ ．Ｓｅｑｕｅｎｃｅｌｅｖｅｌｔｒａｉｎｉｎｇｗｉｔｈｒｅｃｕｒｒｅｎｔｎｅｕｒａｌ  

ｎｅｔｗｏｒｋｓ［Ｃ ］／／４ｔｈＩｎｔｅｒ ｎａｔ ｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＬｅａｒｎｉｎｇＲｑ ｊｒｅｓｅｎｔａｔｉｏｎｓ ．２０１６  

［２７ ］ＲｅｎｎｉｅＳＪ ，ＭａｒｃｈｅｒｅｔＥ ，ＭｒｏｕｅｈＹ ，ｅｔａｌ ，Ｓｅｌｆ －Ｃｒ ｉｔｉｃａｌｓｅｑｕｅｎｃｅｔｒａｉｎｉｎｇｆｏｒｉｍａｇｅ  

ｃａｐｔｉｏｎｉｎｇ［Ｃ］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄＰａｔｔｅｒｎ  

Ｒｅｃｏｇｎｉｔｉｏｎ ．２０ １７ ：７００８ －７０２４  

［２８ ］ＨｏｎｄａＵ ，Ｗａｔａｎａｂｅ％ＭａｔｓｕｍｏｔｏＹ．Ｓｗｉｔｃｈｉｎｇｔｏｄｉｓｃｒｉｍｉｎａｔｉｖｅｉｍａｇｅｃａｐｔｉｏｎｉｎｇｂｙ  

７０  

参考文献  

ｒｅｌｉｅｖｉｎｇａｂｏｔｌｅｎｅｃｋｏｆｒｅｉｎｆｏｒｃｅｍｅｎｔｌｅａｍｉｎｇ［Ｃ ］ ／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＩＥＥＥ／ＣＶＦＷｉｎｔｅｒ  

ＣｏｎｆｅｒｅｎｃｅｏｎＡｐｐ ｌｉｃａｔｉｏｎｓｏｆ ＣｏｍｐｕｔｅｒＶｉｓｉｏｎ ．２０２３ ：１ １２４ － １ １３４  

［２９］ＧｕＪ ，ＢｒａｄｂｕｒｙＪ ５ＸｉｏｎｇＣ ，ｅｔａｌ ．Ｎｏｎ －ａｕｔｏｒｅｇｒｅｓｓｉｖｅｎｅｕｒａｌｍａｃｈｉｎｅｔｒａｎｓｌａｔｉｏｎ ［Ｃ ］／／６ｔｈ  

Ｉｎｔｅｒ ｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＬｅａｒｎｉｎｇＲｅｐｒｅｓｅｎｔａｔｉｏｎｓ ．２０１８  

［３０ ］ＦｅｉＺ ，ＦａｓｔｉｍａｇｅｃａｐｔｉｏｎｇｅｎｅｒａｔｉｏｎｗｉｔｈｐｏｓｉｔｉｏｎａｌｉｇｎｍｅｎｔＪ］

．ａｒＸｉｖｐｒｅｐｒ ｉｎｔ  

ａｒＸｉｖ： １９ １２ ．０６３６５ ，２０１９ ．  

［３ １ ］ＧｕｏＬ ５ＬｉｕＪ ，ＺｈｕＸ ，ｃｔａｌ．Ｎｏｎ －Ａｕｔｏｒｅｇｒｅｓｓｉｖｅｉｍａｇｅｃａｐｔｉｏｎｉｎｇｗｔｈｃｏｕｎｔｅｒｆａｃｔｕａｌｓ －  

ｃｒｉｔｉｃａｌｍｕｌｔｉ －ａｇｅｎｔｌｅａｍｉｎｇ［Ｃ ］ ／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＴｗｅｎｔｙ

－ＮｉｎｔｈＩｎｔｅｒ ｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅ  

ｏｎＩｎｔｅｒｎａｔｉｏｎａｌＪｏｉｎｔＣｏｎｆｅｒｅｎｃｅｓｏｎ ＡｒｔｉｆｉｃｉａｌＩｎｔｅｌｌｉｇｅｎｃｅ ．２０２１ ：７６７ －７７３  

［３２ ］ＧａｏＪ ，ＭｅｎｇＸ ，ＷａｎｇＳ ｓｅｔａｌ ．Ｍａｓｋｅｄｎｏｎ －ａｕｔｏｒｅｇｒｅｓｓｉｖｅｉｍａｇｅｃａｐｔ ｉｏｎｉｎｇ［Ｊ］

．ａｒＸｉｖ  

ｐｒｅｐｒｉｎｔａｒＸｉｖ ：１９０６ ．００７１７ ，２０１９ ．  

［３３ ］ＦｅｉＺ ．Ｉｔｅｒａｔｉｖｅｂａｃｋｍｏｄｉｆ ｉｃａｔ ｉｏｎｆｏｒｆａｓｔｅｒｉｍａｇｅｃａｐｔｉｏｎｉｎｇ［Ｃ ］ ／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２８ｔｈ  

ＡＣＭＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＭｕｌｔｉｍｅｄｉａ ．２０２０：３ １８２ －３ １９０  

［３４ ］ＦｅｉＺ ，ＦａｎＭ ，ＺｈｕＬ ，ｅｔａｌ ．Ｕｎｃｅｒｔａｉｎｔｙ

－ａｗａｒｅｉｍａｇｅｃａｐｔｉｏｎｉｎｇ［Ｃ］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ  

ＡＡＡＩＣｏｎｆｅｒｅｎｃｅｏｎＡｒｔｉｆ ｉｃｉａｌＩｎｔｅｌｌｉｇｅｎｃｅ ．２０２３ ，３７（１ ）

：６１４ －６２２ ．  

［３５ ］ＨｏＪ ？ＪａｉｎＡ ，ＡｂｂｅｅｌＰ．Ｄｅｎｏｉｓｉｎｇｄｉｆｆｕｓｉｏｎｐｒｏｂａｂｉｌｉｓｔｉｃｍｏｄｅｌｓ ［Ｊ］

．ＡｄｖａｎｃｅｓｉｎＮｅｕｒａｌ  

ＩｎｆｏｒｍａｔｉｏｎＰｒｏｃｅｓｓｉｎｇＳｙｓｔｅｍｓ ， ２０２０ ， ３３ ：６８４０ －６８５１  

［３６ ］ＬｉＹ ，ＺｈｏｕＫ ，ＺｈａｏＷＸ ，ｅｔａｌ ．Ｄｉｆｕｓｉｏｎｍｏｄｅｌｓｆｏｒｎｏｎ

－ａｕｔｏｒｅｇｒｅｓｓｉｖｅｔｅｘｔｇｅｎｅｒａｔｉｏｎ：Ａ  

ｓｕｒｖｅｙ［Ｊ ］

．ａｒＸｉｖ ｐｒｅｐｒｉｎｔａｒＸｉｖ ：２３０３ ．０６５７４ ？２０２３ ．  

［３７ ］ＣｈｅｎＴ ，ＺｈａｎｇＲ ，ＨｉｎｔｏｎＧ ．ＡｎａｌｏｇＢｉｔｓ ：Ｇｅｎｅｒａｔ ｉｎｇｄｉｓｃｒｅｔｅｄａｔａｕｓｉｎｇｄｉｆｆ ｉｉｓｉｏｎｍｏｄｅｌｓ  

ｗｉｔｈｓｅｌｆ －ｃｏｎｄｉｔｉｏｎｉｎｇ［Ｃ］ ／／ＴｈｅＥｌｅｖｅｎｔｈＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＬｅａｒｎｉｎｇ  

Ｒｅｐｒｅｓｅｎｔａｔｉｏｎｓ ．２０２２  

［３８ ］ＬｕｏＪ ，ＬｉＹ ， ＰａｎＹ ， ｅｔａＬＳｅｍａｎｔｉｃ －Ｃｏｎｄｉｔｉｏｎａｌｄｉｆｕｓｉｏｎｎｅｔｗｏｒｋｓｆｏｒｉｍａｇｅ  

ｃａｐｔｉｏｎｉｎｇ［Ｃ］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒ＼＾ｓｉｏｎａｎｄＰａｔｔｅｒｎ  

Ｒｅｃｏｇｎｉｔｉｏｎ．２０２３ ：２３３５９ －２３３６８  

［３９ ］ＭａｔｈｅｗｓＡ ，ＸｉｅＬ ，ＨｅＸ ．Ｓｅｎｔｉｃａｐ

：Ｇｅｎｅｒａｔｉｎｇ ｉｍａｇｅｄｅｓｃｒｉｐｔｉｏｎｓｗｉｔｈ  

ｓｅｎｔｉｍｅｎｔｓ ［Ｃ］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆ ｔｈｅＡＡＡＩｃｏｎｆｅｒｅｎｃｅｏｎａｒｔｉｆ ｉｃｉａｌｉｎｔｅｌｌｉｇｅｎｃｅ ．２０１６ ，３０（１）

［４０ ］ＣｏｍｉａＭ ， ＢａｒａｌｄｉＬ ， Ｃｕｃｃｈｉａｒａ化Ｓｈｏｗ ， ＣｏｎｔｒｏｌａｎｄＴｅｌｌ ：Ａｆ ｒａｍｅｗｏｒｋｆｏｒｇｅｎｅｒａｔ ｉｎｇ  

ｃｏｎｔｒｏｌｌａｂｌｅａｎｄｇｒｏｕｎｄｅｄｃａｐｔ ｉｏｎｓ ［Ｃ ］ ／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒ  

ＶｉｓｉｏｎａｎｄＰａｔｅｒｎＲｅｃｏｇｎｉｔｉｏｎ ．２０１９ ：８３０７ －８３ １６  

［４ １ ］ＤｅｎｇＣ ， ＤｉｎｇＮ ，ＴａｎＭ ，ｅｔａｌ ．Ｌｅｎｇｔｈ －Ｃｏｎｔｒｏｌｌａｂｌｅｉｍａｇｅｃａｐｔｉｏｎｉｎｇ［Ｃ ］／／Ｅｕｒｏｐｅａｎ  

ＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎ．２０２０ ：７１２ －７２９ ．  

［４２ ］ＷａｎｇＮ ，ＸｉｅＪ ，ＷｕＪ ，ｅｔａｌ ．Ｃｏｎｔｒｏｌｌａｂｌｅｉｍａｇｅｃａｐｔｉｏｎｉｎｇｖｉａ

ｐｒｏｍｐｔｉｎｇ［Ｃ ］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆ  

ｔｈｅＡＡＡＩＣｏｎｆｅｒｅｎｃｅｏｎＡｒｔｉｆｉｃｉａｌＩｎｔｅｌｌｉｇｅｎｃｅ ．２０２３ ：２６１７ －２６２５  

７１  

北京邮电大学硕士学位论文  

［４３ ］ＺｈａｎｇＷ ，ＳｈｉＨ ，ＧｕｏＪ ，ｅｔａｌ．Ｍａｇ ｉｃ ：Ｍｕｌｔｉｍｏｄａｌｒｅｌａｔｉｏｎａｌｇｒａｐｈａｄｖｅｒｓａｒ ｉａｌｉｎｆｅｒｅｎｃｅｆｏｒ  

ｄｉｖｅｒｓｅａｎｄｕｎｐａｉｒｅｄｔｅｘｔ －ｂａｓｅｄｉｍａｇｅｃａｐｔｉｏｎｉｎｇ［Ｃ］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＡＡＡＩＣｏｎｆｅｒｅｎｃｅ  

ｏｎＡｒｔｉｆｉｃｉａｌＩｎｔｅｌｌｉｇｅｎｃｅ．２０２２ ：３３３５ －３３４３  

［４４ ］ＨｉｒｏｔａＹ ｓＮａｋａｓｈｉｍａＹ ，ＧａｒｃｉａＮ ．Ｍｏｄｅｌ －ａｇｎｏｓｔｉｃｇｅｎｄｅｒｄｅｂｉａｓｅｄｉｍａｇｅ  

ｃａｐｔｉｏｎｉｎｇ［Ｃ ］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄＰａｔｔｅｒｎ  

Ｒｅｃｏｇｎｉｔｉｏｎ．２０２３ ：１５ １９ １ － １５２００  

［４５ ］Ｑ ｉｕＨ ，ＤｏｕＺＹ ，ＷａｎｇＴ ，ｅｔａｌ ．Ｇｅｎｄｅｒｂｉａｓｅｓｉｎａｕｔｏｍａｔｉｃｅｖａｌｕａｔｉｏｎｍｅｔｒ ｉｃｓｆｏｒｉｍａｇｅ  

ｃａｐｔｉｏｎｉｎｇ［Ｃ ］ ／／Ｔｈｅ２０２３ＣｏｎｆｅｒｅｎｃｅｏｎＥｍｐ ｉｒｉｃａｌＭｅｔｈｏｄｓｉｎＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ

２０２３  

［４６ ］ＺｈａｏＤ ，ＷａｎｇＡ ，ＲｕｓｓａｋｏｖｓｋｙＯ ．Ｕｎｄｅｒｓｔａｎｄｉｎｇａｎｄｅｖａｌｕａｔｉｎｇｒａｃｉａｌｂｉａｓｅｓｉｎｉｍａｇｅ  

ｃａｐｔｉｏｎｉｎｇ［Ｃ ］ ／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆ ｔｈｅＩＥＥＥＩｎｔｅｒ ｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎ ．２０２１ ：  

１４８３０ －１４８４０  

［４７ ］ＡｂｄｅｌｒａｈｍａｎＥ ，ＳｕｎＰ ，ＬｉＬＥ ，ｅｔａｌ ．ＩｍａｇｅＣａｐ ｔｉｏｎｅｒ２ ：Ｉｍａｇｅｃａｐ ｔｉｏｎｅｒｆｏｒｉｍａｇｅｃａｐｔｉｏｎｉｎｇ  

ｂｉａｓａｍｐ ｌｉｆ ｉｃａｔｉｏｎａｓｓｅｓｓｍｅｎｔ ［Ｃ］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＡＡＡＩＣｏｎｆｅｒｅｎｃｅｏｎＡｒｔ ｉｆｉｃｉａｌ  

Ｉｎｔｅｌｌｉｇｅｎｃｅ ．２０２４ ，３８（１９）

：２０９０２

－２０９１ １ ．  

［４８ ］ＧｕＪ ５ＷａｎｇＣ ，ＺｈａｏＪ．Ｌｅｖｅｎｓｈｔｅｉｎｔｒａｎｓｆｏｒｍｅｒ ［Ｊ ］

．ＡｄｖａｎｃｅｓｉｎＮｅｕｒａｌＩｎｆｏｒｍａｔ ｉｏｎ  

ＰｒｏｃｅｓｓｉｎｇＳｙｓｔｅｍｓ ，２０１９ ，３２  

［４９ ］ＸｕＷ ，ＣａｒｐｕａｔＭ ．Ｅｄｉｔｏｒ：Ａｎｅｄｉｔ －ｂａｓｅｄｔｒａｎｓｆｏｒｍｅｒｗｉｔｈｒｅｐｏｓｉｔｉｏｎｉｎｇｆｏｒｎｅｉｘｒａｌｍａｃｈｉｎｅ  

ｔｒａｎｓｌａｔｉｏｎｗｉｔｈｓｏｆ ｔｌｅｘｉｃａｌｃｏｎｓｔｒａｉｎｔｓ ［Ｊ］

．Ｔｒａｎｓａｃｔ ｉｏｎｓｏｆ ｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌ  

Ｌｉｎｇｕｉｓｔｉｃｓ ，２０２１ ，９ ：３１１ －３２８  

［５０ ］ＧｌｏｒｏｔＸ ５ＢｏｒｄｅｓＡ ，Ｂｅｎｇ ｉｏＹ．Ｄｅｅｐｓｐａｒｓｅｒｅｃｔｉｆｉｅｒｎｅｕｒａｌｎｅｔｗｏｒｋｓ ［Ｃ ］ ／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ  

ＦｏｕｒｔｅｅｎｔｈＩｎｔｅｒ ｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＡｒｔｉｆｉｃｉａｌＩｎｔｅｌｌｉｇｅｎｃｅａｎｄＳｔａｔｉｓｔｉｃｓ ．２０１ １ ：３ １５ －３２３  

［５ １ ］ＬｅＣｕｎＹ ，ＢｏｔｏｕＬ ， Ｂｅｎｇ ｉｏＹ ， ｅｔａｌ ．Ｇｒａｄｉｅｎｔ －Ｂａｓｅｄｌｅａｒ ｎｉｎｇａｐｐ ｌｉｅｄｔｏｄｏｃｕｍｅｎｔ  

ｒｅｃｏｇｎｉｔｉｏｎ［Ｊ ］

．Ｐｒｏｃｅｅｄｉｎｇｓｏｆ ｔｈｅＩＥＥＥ ，１９９８ ，８６ （ １ １ ）

：２２７８ －２３２４  

［５２ ］ ＩｏｆｆｅＳ ，ＳｚｅｇｅｄｙＣ ．ＢａｔｃｈＮｏｒｍａｌｉｚａｔ ｉｏｎ：Ａｃｃｅｌｅｒａｔｉｎｇｄｅｅｐｎｅｔｗｏｒｋｔｒａｉｎｉｎｇｂｙｒｅｄｕｃｉｎｇ  

ｉｎｔｅｒ ａａｌｃｏｖａｒｉａｔｅｓｈｉｆ ｔ ［Ｃ ］ ／／Ｉｎｔｅｒ ａａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＭａｃｈｉｎｅＬｅａｒｎｉｎｇ

．２０１５ ：４４８ －４５６  

［５３ ］ＤｏｓｏｖｉｔｓｋｉｙＡ ，ＢｅｙｅｒＬ ，ＫｏｌｅｓｎｉｋｏｖＡ ，ｅｔａＬＡｎｉｍａｇｅｉｓｗｏｒｔｈ１６ｘ１６ｗｏｒｄｓ ：Ｔｒａｎｓｆｏｒｍｅｒｓ  

ｆｏｒｉｍａｇｅｒｅｃｏｇｎｉｔｉｏｎａｔｓｃａｌｅ ［Ｊ ］

．ａｒＸｉｖ ｐｒｅｐｒｉｎｔａｒＸｉｖ：２０１０ ． １ １９２９ ，２０２０ ．  

［５４ ］ＬｉｕＺ ，ＬｉｎＹ ，ＣａｏＹ ，ｅｔａｌ ．Ｓｗｉｎｔｒａｎｓｆｏｒｍｅｒ ：ｈｉｅｒａｒｃｈｉｃａｌｖｉｓｉｏｎｔｒａｎｓｆｏｎｎｅｒｕｓｉｎｇｓｈｉｆ ｔｅｄ  

ｗｉｎｄｏｗｓ［Ｃ ］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＩＥＥＥＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎ．２０２１ ：  

１００ １２ －１００２２  

［５５ ］ＧｉｒｓｈｉｃｋＲ ，ＤｏｎａｈｕｅＪ ，ＤａｒｒｅｌｌＴ ，ｅｔａｌ．Ｒｉｃｈｆｅａｔｕｒｅｈｉｅｒａｒｃｈｉｅｓｆｏｒａｃｃｕｒａｔｅｏｂｊｅｃｔｄｅｔｅｃｔｉｏｎ  

ａｎｄｓｅｍａｎｔｉｃｓｅｇｍｅｎｔａｔｉｏｎ ［Ｃ ］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎ  

ａｎｄＰａｔｅｒ ｎＲｅｃｏｇｎｉｔｉｏｎ．２０１４ ：５８０

－５８７  

７２  

参考文献  

［５６ ］ＧｉｒｓｈｉｃｋＲ．ＦａｓｔＲ －ＣＮＮ ［Ｃ ］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆ ｔｈｅＩＥＥＥＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒ  

Ｖｉｓｉｏｎ．２０１５ ：１４４０ －１４４８  

［５７ ］ＲｅｎＳ ｓＨｅＫ ，ＧｉｒｓｈｉｃｋＲ ，ｅｔａｌ ．ＦａｓｔｅｒＲ －ＣＮＮ ：Ｔｏｗａｒｄｓｒｅａｌ －ｔｉｍｅｏｂｊｅｃｔｄｅｔｅｃｔｉｏｎｗｉｔｈ  

ｒｅｇ ｉｏｎ ｐｒｏｐｏｓａｌｎｅｔｗｏｒｋｓ ［Ｊ ］

．ＡｄｖａｎｃｅｓｉｎＮｅｕｒａｌＩｎｆｏｒｍａｔｉｏｎＰｒｏｃｅｓｓｉｎｇＳｙｓｔｅｍｓ ，２０１５ ，２８  

［５８ ］ＫｒｉｓｈｎａＲ ，ＺｈｕＹ ，ＧｒｏｔｈＯ ，ｅｔａｌ ．Ｖｉｓｕａｌｇｅｎｏｍｅ ：Ｃｏｎｎｅｃｔｉｎｇ ｌａｎｇｕａｇｅａｎｄｖｉｓｉｏｎｕｓｉｎｇ  

ｃｒｏｗｄｓｏｕｒｃｅｄｄｅｎｓｅｉｍａｇｅａｎｎｏｔａｔｉｏｎｓ［Ｊ ］

．ＩｎｔｅｒｎａｔｉｏｎａｌＪｏｕｒｎａｌｏｆＣｏｍｐｕｔｅｒＶｉｓｉｏｎ ，２０１７ ？  

１２３ ：３２ －７３ ．  

［５９ ］Ｃａｒ ｉｏｎＮ ，ＭａｓｓａＦ ，ＳｙｎｎａｅｖｅＧ ，ｅｔａｌ ．Ｅｎｄ －ｔｏ －Ｅｎｄｏｂｊｅｃｔｄｅｔｅｃｔｉｏｎｗｉｔｈ  

ｔｒａｎｓｆｏｒｍｅｒｓ［Ｃ］／／ＥｕｒｏｐｅａｎＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎ．２０２０ ：２ １３

－２２９ ．  

［６０ ］Ｐａｐ ｉｎｅｎｉＫ ，ＲｏｎｋｏｓＳ ，ＷａｒｄＴ ，ｅｔａｌ ．Ｂｌｅｕ：ａｍｅｔｈｏｄｆｏｒａｕｔｏｍａｔｉｃｅｖａｌｕａｔｉｏｎｏｆｍａｃｈｉｎｅ  

ｔｒａｎｓｌａｔｉｏｎ ［Ｃ ］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ４０ｔｈａｎｎｕａｌｍｅｅｔｉｎｇｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌ  

Ｌｉｎｇｕｉｓｔｉｃｓ ．２００２ ：３ １ １ －３ １８ ．  

［６１ ］ＬｉｎＣＹ．Ｒｏｕｇｅ：Ａ ｐａｃｋａｇｅｆｏｒａｕｔｏｍａｔｉｃｅｖａｌｕａｔｉｏｎｏｆｓｕｍｍａｒ ｉｅｓ［Ｃ］／／ＴｅｘｔＳｕｍｍａｒ ｉｚａｔ ｉｏｎ  

ＢｒａｎｃｈｅｓＯｕｔ ．２００４ ：７４ －８１  

［６２ ］ＢａｎｅｉｊｅｅＳ ，ＬａｖｉｅＡ ．Ｍｅｔｅｏｒ ：Ａｎａｕｔｏｍａｔｉｃｍｅｔｒｉｃｆｏｒｍｔｅｖａｌｕａｔｉｏｎｗｉｔｈｉｍｐｒｏｖｅｄ  

ｃｏｒｒｅｌａｔ ｉｏｎｗｉｔｈｈｕｍａｎ ｊｕｄｇｍｅｎｔｓ ［Ｃ］ ／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＡＣＬＷｏｒｋｓｈｏｐｏｎＩｎｔｒ ｉｎｓｉｃａｎｄ  

ＥｘｔｒｉｎｓｉｃＥｖａｌｕａｔｉｏｎＭｅａｓｕｒｅｓｆｏｒＭａｃｈｉｎｅＴｒａｎｓｌａｔ ｉｏｎＡｎｄ／ＯｒＳｕｍｍａｒｉｚａｔ ｉｏｎ．２００５ ：６５ －７２  

［６３ ］ＶｅｄａｎｔａｍＲ ，ＺｉｔｎｉｃｋＣＬ ， ＰａｒｉｋｈＤ ．ＣＩＤＥｒ ：Ｃｏｎｓｅｎｓｕｓ

－ｂａｓｅｄｉｍａｇｅｄｅｓｃｒ ｉｐ ｔｉｏｎ  

ｅｖａｌｕａｔｉｏｎ ［Ｃ］ ／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄＰａｔｔｅｒｎ  

Ｒｅｃｏｇｎｉｔｉｏｎ ．２０ １５ ：４５６６ －４５７５  

［６４ ］ＳｕｔｔｏｎＲＳ ，ＭｃＡｌｌｅｓｔｅｒＤ ， ＳｉｎｇｈＳ ， ｅｔａｌ ．Ｐｏｌｉｃｙｇｒａｄｉｅｎｔｍｅｔｈｏｄｓｆｏｒｒｅｍｆ ｏｒｃｅｍｅｎｔｌｅａｒｎｉｎｇ  

ｗｉｔｈｆ ｉｍｃｔｉｏｎａｐｐｒｏｘｉｍａｔ ｉｏｎ ［Ｊ］

．ＡｄｖａｎｃｅｓｉｎＮｅｕｒａｌＩｎｆｏｒｍａｔ ｉｏｎＰｒｏｃｅｓｓｉｎｇＳｙｓｔｅｍｓ，１９９９ ，  

１２  

［６５ ］ＷｉｌｌｉａｍｓＲＪ ．Ｓｉｍｐ ｌｅｓｔａｔ ｉｓｔｉｃａｌｇｒａｄｉｅｎｔ －ｆｏｌｌｏｗｉｎｇａｌｇｏｒ ｉｔｈｍｓｆｏｒｃｏｎｎｅｃｔｉｏｎｉｓｔ  

ｒｅｉｎｆｏｒｃｅｍｅｎｔｌｅａｍｉｎｇ［Ｊ］

．ＭａｃｈｉｎｅＬｅａｒｎｉｎｇ，１９９２ ，８（３

：２２９ －２５６  

［６６ ］ＬｉｎＴＹ ｓＭａｉｒｅＭ ，Ｂｅｌｏｎｇ ｉｅＳ ，ｅｔａｌ ．Ｍｉｃｒｏｓｏｆ ｔＣＯＣＯ ：Ｃｏｍｍｏｎｏｂｊｅｃｔｓｉｎ  

ｃｏｎｔｅｘｔ ［Ｃ ］／／ＥｕｒｏｐｅａｎＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎ．２０１４ ：７４０ －７５５ ．  

［６７ ］ＫａｒｐａｔｈｙＡ ，Ｆｅｉ －ＦｅｉＬ．Ｄｅｅｐｖｉｓｕａｌ －ｓｅｍａｎｔｉｃａｌｉｇｎｍｅｎｔｓｆｏｒｇｅｎｅｒａｔｉｎｇ ｉｍａｇｅ  

ｄｅｓｃｒ ｉｐｔｉｏｎｓ［Ｃ］／／ＰｒｏｃｅｅｄｍｇｓｏｆｔｈｅＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄＰａｔｅｒｎ  

Ｒｅｃｏｇｎｉｔｉｏｎ．２０１５ ：３ １２８

－３ １３７  

［６８ ］ＳｕｇａｎｕｍａＭ ，ＯｋａｔａｎｉＴ．ＧＲＩＴ：Ｆａｓｔｅｒａｎｄｂｅｔｔｅｒｉｍａｇｅｃａｐｔｉｏｎｉｎｇ ｔｒａｎｓｆｏｒｍｅｒｕｓｉｎｇｄｕａｌ  

ｖｉｓｕａｌｆｅａｔｕｒｅｓ ［Ｃ ］／／１７ｔｈＥｕｒｏｐｅａｎＣｏｎｆ ｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎ ．２０２２ ：１６７ －１８４  

［６９ ］ＺＨＡＮＧＸ ，ＳｕｎＸ ，ＬｕｏＹ ，ｅｔａｌ ．ＲＳＴＮｅｔ ：Ｃａｐｔｉｏｎｉｎｇｗｉｔｈａｄａｐ ｔｉｖｅａｔｔｅｎｔｉｏｎｏｎｖｉｓｕａｌａｎｄ  

ｎｏｎ －ｖｉｓｕａｌｗｏｒｄｓ ［Ｃ ］ ／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄＰａｔｅｒｎ  

７３  

北京邮电大学硕士学位论文  

Ｒｅｃｏｇｎｉｔｉｏｎ．２０２ １ ：１５４６５ － １５４７４  

［７０ ］ＰａｎＹ ，ＹａｏＴ ，ＬｉＹ ，ｅｔａｌ ．Ｘ －ｌｉｎｅａｒａｔｅｎｔｉｏｎｎｅｔｗｏｒｋｓｆｏｒｉｍａｇｅｃａｐｔｉｏｎｉｎｇ［Ｃ ］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆ  

ｔｈｅＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄＰａｔｔｅｒｎＲｅｃｏｇｎｉｔｉｏｎ．２０２０ ：１０９７ １ －１０９８０  

［７１ ］ＬｕｏＹ ，ＪｉＪ ，ＳｕｎＸ ，ｅｔａｌ ．Ｄｕａｌ － ｌｅｖｅｌｃｏｌｌａｂｏｒａｔｉｖｅｔｒａｎｓｆｏｒｍｅｒｆｏｒｉｍａｇｅ  

ｃａｐｔｉｏｎｉｎｇ［Ｃ］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔ ｉｉｅＡＡＡＩＣｏｎｆｅｒｅｎｃｅｏｎＡｒｔ ｉｆｉｃｉａｌＩｎｔｅｌｌｉｇｅｎｃｅ．２０２１ ：２２８６ －  

２２９３  

［７２ ］ＳｏｎｇＺ ，ＺｈｏｕＸ ， ＤｏｎｇＬ ，ｅｔａｌ ．Ｄｉｒｅｃｔ ｉｏｎｒｅｌａｔｉｏｎｔｒａｎｓｆｏｒｍｅｒｆｏｒｉｍａｇｅ  

ｃａｐｔｉｏｎｉｎｇ［Ｃ］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆ ｔｈｅ２９ｔｈＡＣＭＩｎｔｅｒ ｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＭｕｌｔｉｍｅｄｉａ ．２０２１ ：  

５０５６ －５０６４  

［７３ ］ＦｅｉＺ ， ＹａｎＸ ， ＷａｎｇＳ ， ｅｔａｌ．ＤｅｅＣａｐ

：Ｄｙｎａｍｉｃｅａｒｌｙｅｘｉｔｉｎｇｆｏｒｅｆｆｉｃｉｅｎｔｉｍａｇｅ  

ｃａｐ ｔｉｏｎｉｎｇ［Ｃ］／／２０２２ＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄＰａｔｅｒｎＲｅｃｏｇｎｉｔｉｏｎ ．２０２２ ：  

－１２２１６  

［７４ ］ＺｅｎｇＰ ， ＺｈｕＪ ， ＳｏｎｇＪ ， ｅｔａｌ ．Ｐｒｏｇｒｅｓｓｉｖｅｔｒｅｅ －ｓｔｒｕｃｔｕｒｅｄｐｒｏｔｏｔｙｐｅｎｅｔｗｏｒｋｆｏｒｅｎｄ －ｔｏ －ｅｎｄ  

ｉｍａｇｅｃａｐ ｔｉｏｎｉｎｇ［Ｃ］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆ ｔｈｅ３０ｔｈＡＣＭＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＭｕｌｔｉｍｅｄｉａ ．  

２０２２ ：５２１０ －５２１８  

１７５ ］ＭａＹ ， ＪｉＪ ， ＳｕｎＸ ， ｅｔａｌ ＿Ｔｏｗａｒｄｓｌｏｃａｌｖｉｓｕａｌｍｏｄｅｌｉｎｇｆｏｒｉｍａｇｅｃａｐｔ ｉｏｎｉｎｇ［Ｊ ］

．Ｐａｔｅｒ ｎ  

Ｒｅｃｏｇｎｉｔｉｏｎ ，２０２３ ，１３８ ：１０９４２０  

［７６ ］ＷａｎｇＴ ，ＣｈｅｎＷ ，ＴｉａｎＹ

，ｅｔａｌ ．Ｉｍｐｒｏｖｉｎｇ ｉｍａｇｅｃａｐ ｔｉｏｎｉｎｇｖｉａｐｒｅｄｉｃｔｉｎｇｓｔｒｕｃｔｕｒｅｄ  

ｃｏｎｃｅｐｔｓ［Ｃ ］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２０２３ＣｏｎｆｅｒｅｎｃｅｏｎＥｍｐ ｉｒｉｃａｌＭｅｔｈｏｄｓｉｎＮａｔｕｒａｌ  

ＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ

．２０２３ ：３６０ －３７０  

［７７ ］ＺｈａｎｇＰ ，ＬｉＸ ，ＨｕＸ ，ｅｔａｌ ．＼＾ｎｖｌ ：Ｒｅｖｉｓｉｔｉｎｇｖｉｓｕａｌｒｅｐｒｅｓｅｎｔａｔｉｏｎｓｉｎｖｉｓｉｏｎ － ｌａｎｇｕａｇｅ  

ｍｏｄｅｌｓ ［Ｃ］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄＰａｔｅｒｎ  

Ｒｅｃｏｇｎｉｔｉｏｎ．２０２１ ：５５７９ －５５８８  

［７８ ］ＹａｎＸ ，ＦｅｉＺ ，ＬｉＺ ，ｅｔａｌ．Ｓｅｍｉ －ａｕｔｏｒｅｇｒｅｓｓｉｖｅｉｍａｇｅｃａｐｔｉｏｎｍｇ［Ｃ］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２９ｔｈ  

ＡＣＭＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＭｕｌｔｉｍｅｄｉａ．２０２１ ：２７０８ －２７１６  

［７９］ＭａＺ ，ＷａｎｇＣ，ＨｕａｎｇＢ ，ｅｔａｌ ．Ｂｏｕｎｄｉｎｇａｎｄｆｉｌｌｉｎｇ

：Ａｆａｓｔａｎｄｆ ｌｅｘｉｂｌｅｆ ｒａｍｅｗｏｒｋｆｏｒ  

ｉｍａｇｅｃａｐｔｉｏｎｉｎｇ［Ｃ ］ ／／ＣＣＦＩｎｔｅｒ ｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇａｎｄ  

ＣｈｉｎｅｓｅＣｏｍｐｕｔｉｎｇ

．２０２３ ：４６９

－４８ １ ．  

［８０ ］ＤｅｖｌｉｎＪ ，ＣｈａｎｇＭＷ ，ＬｅｅＫ ，ｅｔａｌ．ＢＥＲＴ：Ｐｒｅ

－ ｔｒａｉｎｉｎｇｏｆｄｅｅｐｂｉｄｉｒｅｃｔ ｉｏｎａｌｔｒａｎｓｆｏｒｍｅｒｓｆｏｒ  

ｌａｎｇｕａｇｅｕｎｄｅｒｓｔａｎｄｉｎｇ［Ｃ］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２０１９ＣｏｎｆｅｒｅｎｃｅｏｆｔｈｅＮｏｒｔｈＡｍｅｒｉｃａｎ  

Ｃｈａｐ ｔｅｒｏｆ ｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ．２０１９ ：４ １７ １ －４１８６ ．  

７４  