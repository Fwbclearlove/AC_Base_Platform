Multi-scale Two-way Deep Neural Network for Stock Trend Prediction

Guang Liu1,2,† , Yuzhao Mao1,2,† , Qi Sun1 , Hailong Huang1 , Weiguo Gao1,∗, Xuan Li1 , JianPing Shen1 , Ruifan Li2 and Xiaojie Wang2

1PingAn Life Insurance Company of China, Ltd. 2School of Computer Science, Beijing University of Posts and Telecommunications {liuguang230, maoyuzhao258,sunqi149}@pingan.com.cn {huanghailong590,gaoweiguo801,lixuan208,shenjianping324}@pingan.com.cn {rﬂi,xjwang}@bupt.edu.cn

Abstract

Stock Trend Prediction(STP) has drawn wide at- tention from various ﬁelds, especially Artiﬁcial Intelligence. Most previous studies are single- scale oriented which results in information loss from a multi-scale perspective. In fact, multi- scale behavior is vital for making intelligent in- vestment decisions. A mature investor will thor- oughly investigate the state of a stock market at various time scales. To automatically learn the multi-scale information in stock data, we pro- pose a Multi-scale Two-way Deep Neural Net- work(MTDNN). It learns multi-scale patterns from two types of scale information, wavelet-based and downsampling-based, by eXtreme Gradient Boost- ing and Recurrent Convolutional Neural Network, respectively. After combining the learned patterns from the two-way, our model achieves state-of-the- art performance on FI-2010 and CSI-20161, where the latter is our published long-range stock dataset to help future studies for STP task. Extensive ex- perimental results on the two datasets indicate that multi-scale information can signiﬁcantly improve the STP performance and our model is superior in capturing such information.

1 Introduction

Stock Trend Prediction (STP), which automatically predicts future direction of the stock price movement, is of great im- portance for investors. It is challenging because stock data is non-stationary time series dominated by chaotic. It has at- tracted many researchers to explore such stochastic data [Tsai and Hsiao, 2010; Kara et al., 2011; Li et al., 2016]. To reduce the chaos in stock data, previous studies smooth the data with a single speciﬁc time scale to analyze the behav- ior of stock price movement (e.g. 5-minute moving average).

1https://github.com/marscrazy/MTDNN 2†:equal comtribution 3∗:corresponding author

However, the single-scale analysis ignores the multi-scale be- havior within stock data. As depicted in Figure 1, stock trend moves toward different direction with multiple time scales, where s1,s2,s3 indicate the wrong direction and s4 conveys information towards the correct direction. Single-scale is in- sufﬁcient to predict the moving trend.

Raw stock data

t+w Multi-scale s t

Figure 1: Intuitionistic view of multi-scale patterns within a stock data.

Notice that time scale is just one type of scale-information. [Hu and Qi, 2017] proposed a state-frequency memory net- work that uses Fourier transform to decompose memory state into multi-frequency components. [Lahmiri, 2014] used Dis- crete Wavelet Transform (DWT) to decompose a stock time series into multi-scale components of different resolutions. [Cui et al., 2016] obtained multi-scale patterns directly by downsampling with different time scales. It is worth men- tioning that all the above methods are not for STP task. In this paper, we insist that multi-scale information refers to stock price behavior not only at multiple scales but also in multiple types of scale-information. To explore the multi- scale patterns from two types of scale-information for the STP task, we propose a novel Multi-scale Two-way Deep Neural Network (MTDNN). One way is DWT-based. It uses eXtreme Gradient Boosting (XGBoost) to automatically en- semble the DWT-based multi-scale patterns. The other is downsampling-based. It uses Recurrent Convolutional Neu- ral Network (RCNN) structure with a key operation to tempo-

Proceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20) Special Track on AI in FinTech

rally cascade the downsampling-based multi-scale patterns. Finally, we fuse the two types of multi-scale patterns by a fully connected layer to make predictions. We evaluate our model on a benchmark dataset FI-2010. However, FI-2010 only has 10-day stock events which easily result in over-ﬁtting. To address the above concerns, we col- lect and build a one-year range of one-minute stock dataset, China Stock Index 2016 (CSI-2016). Our model achieves state-of-the-art performance on both datasets. The major contributions of this paper are summarized as follows: (1) We propose a model that achieves state-of-the-art STP performance on a benchmark dataset and a long-range dataset, which strongly demonstrates the superiority of our model. (2) We conduct a series of experiments to 1) compare dif- ferent approaches of using multi-scale patterns and 2) analyze the scale characteristics of different types. (3) We publish a new minute-level stock index dataset to help future studies on the task of STP. The rest of this paper is organized as follows. Section 2 dis- cusses the related works. Section 3 generally formalizes the STP task. Section 4 describes the architecture of MTDNN. Section 5 ﬁrst introduces the dataset and experimental set- tings, then analyze the results. Section 6 is the conclusion and future works.

2 Related Works 2.1 Multi-scale for Time Series Many studies concentrate on extracting the multi-scale pat- tern from time-series, to describe time-series more precisely. The multi-scale information of ﬁnancial time-series has been extensively investigated [Dacorogna et al., 1996]. By the similarity measured on multiple scales, the future price of given security can be estimated by ﬁnding a similar history price sequence across different ﬁnancial markets [Papadim- itriou and Yu, 2006]. In AI, some studies have explored the multi-scale information of time-series. The most prior work, ScaleNet [Geva, 1998], decomposes the time-series into dif- ferent scales by Wavelet transform and extracts features from each scale by different Neural networks to obtain a prediction. More recently, Cui et al. [Cui et al., 2016] use Convolutional Neural Network (CNN) to enhance the feature extraction abil- ity, [Fern´andez et al., 2019] apply extreme learning machine (ELM) and a Discrete Wavelet Transform (DWT) to capture the scaling-properties. The above methods with multi-scale information achieved remarkable improvement compared to the single-scale methods.

2.2 Stock Trend Prediction Stock Trend Prediction (STP) is a typical classiﬁcation task. Traditionally, Support Vector Machine (SVM) and Neural Network (NN) are thought to be very effective for STP [Kara et al., 2011]. Due to the excessive parameter size, models are easily over-ﬁtting to the training set. Ensemble-based methods, such as Random Forest (RF) [Patel et al., 2015] which ensembles multiple trees to achieve better prediction

and generalization performance, are introduced in STP. Re- cently, some pioneer researches have explored the effective- ness of deep learning models in STP [Deng et al., 2017; Lin et al., 2017]. The above researches indicate that STP task is lacking all kinds of publicly available benchmark dataset and only focusing on single-scale models.

3 Task Formulation STP takes stock data as input to predict its moving trend. A stock data is a stock events time-series of T length, which we denote as x = {xt}T where xt ∈Rd is one stock event at t-th timestep with d dimensions (e.g. prices, volumes). The stock dataset is a collection of paired data D = {(xn, yn)}N where N is the number of samples in the dataset. yn is the category given the n-th stock data xn, where

(−1 ∆pT ≤−α 0 −α < ∆pT < α 1 ∆pT ≥α (1)

represent the downward, stationary and upward stock price moving trend, respectively. The α is a threshold for trend direction judgement. ∆pT is the percentage change of the future mid-price compared with the current price, which is calculated as follows,

∆pT = mT (k) −pT

pT , (2)

where mT (k) = 1

k Pk i=1 pT +i, k is the prediction horizon. STP is to construct a nonlinear function that can map an input stock data xn to a category yn as follows:

ˆyn = f(xn; θ), (3)

where f(·) is the nonlinear mapping function, θ is the param- eters and ˆyn is the predicted category. The objective is to learn a set of parameters θ that best ﬁt f(·) to map an input xn to the correct category yn.

4 Model 4.1 Overview The architecture of MTDNN is depicted in Figure 2. Our MTDNN model is a two-way end-to-end model. It comprises one wavelet-based way and one downsampling-based way. The two ways convey discriminative information, where the multi-scale information is the dominant force to help enhance the prediction of the stock trend. In the following of this sec- tion, we ﬁrst deﬁne the wavelet-based way, then describe the downsampling-based way, at last, explain output and objec- tive of the MTDNN.

4.2 Wavelet-based Way In this way, we explore the multi-scale behavior of the stock data from a signal processing perspective. A set of stock data is regarded as a non-stationary and discrete signal. After a recursive decomposition of the signal by DWT, we can ob- tain a series of transformed multi-scale components. We ﬁrst concatenate those components, then feed them to an XGBoost model to automatically ensemble the multi-scale information, ﬁnally output the category scores.

Proceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20) Special Track on AI in FinTech

1 scale

1×d @16 3 Max Pooling

2×1 Conv

3×1 @32

Max Pooling

3×1 @32

Downsampling-based way

Wavelet-based way

d dimentions

2 scale

s scale

32×s Paddings

Paddings CNN2

Key operation

XGBoost

wavelet

wavelet

am DWT

m scales

Flattened

d×m dimentions

GRU GRU

s scale Downsampling

Figure 2: The architecture of MTDNN.

Discrete Wavelet Transform DWT is the discrete version of the wavelet transform. It trans- fers the decomposition of a discrete signal into multi-scale components. Top-left of Figure 2 depicts the decomposi- tion process of DWT. At the ﬁrst level, given original signal x(i) = {xt,i}t∈[1,T ] on the i-th dimensional, it is decom- posed into approximation components a1(i) = {a1 n(i)} T

2 and detail components d1(i) = {d1 n(i)} T

2 , by passing the signal through a Low-Pass Filter (LPF) and a High-Pass Fil- ter (HPF), respectively. In this way, signals are downsampled by 2 so that frequency resolution is increased. For simplic- ity, we drop the index i whenever it is unambiguous from the context. The decomposition of the original signal can be for- mulated as, a1 n = X

t h[2n −t]xt , (4)

d1 n = X

t g[2n −t]xt , (5)

where the superscript of a and d indicate the level of DWT. h and g are the LPF and HPF, respectively. n and t are the index of the corresponding components. The second level of DWT decompose the ﬁrst level output a1 into a2 and d2, then the third level till a speciﬁed level has been reached. The re- cursive iteration of wavelet decomposition can be illustrated as, am n = X

t h[2n −t]am−1 t , (6)

dm n = X

t g[2n −t]am−1 t , (7)

where m is the level index. am−1 = {am−1 t }t∈[1, T 2m−1 ] is ap- proximation components obtained from previous level. For a

set of stock data, the approximation components am (low- frequency) maintain the information of the long-term moving trend within the historical data, and the detail component dm (high-frequency) maintains its short-term moving trend infor- mation. Levels of DWT represent different resolutions of the original signal, which capture information about long-short term moving trends of different scales. We concatenate those components into a single vector. Thus given x, the output is,

vi = [dm(i), dm−1(i), ..., d1(i), a1(i)] , (8)

where vi is the multi-scale feature vector for i-th feature di- mension. We use V to represent output [v0, v1, ..., vd] for simplify.

XGBoost XGBoost is a scalable machine learning system for tree boosting [Chen and Guestrin, 2016]. Based on the gradient enhancement decision tree, it produces a prediction model with an ensemble of weak tree-based prediction models. It builds the model in a stage-wise fashion as other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function. The tree-based na- ture is suitable for extracting features from mixed multi-scale information.

ˆswavelet = fxgb(v) (9)

where ˆswavelet ∈ R3 denotes the category score from wavelet-based way, fxgb represents the XGBoost model.

4.3 Downsampling-based Way In this way, we propose a novel strategy to temporally cascade a sequence of increasing multi-scale information by a RCNN structure. Firstly, we use a simple downsampling technique to transform stock data into multi-scale formations, then they

Proceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20) Special Track on AI in FinTech

are fed to CNNs obtaining multi-scale spatial features. After a key operation, we obtain a sequence of increasing multi- scale information. Finally, we use GRU to temporally cas- cade those information and output categories.

Downsampling Downsampling technique is a straightforward way to trans- form the original stock data x into multi-scale formations. Let s be the scale for downsampling. Then every s-th data point in x is kept to construct the new data xs = {x1+ls}l∈[0,Ls], where Ls = ⌊T/s −1⌋is the length of xs. By setting dif- ferent scales, we obtain a collection of down sampled multi- scale stock data X = {xs}S.

RCNN Structure To extract multi-scale features from X, we propose a RCNN structure to cascade outputs from a series of CNNs by RNN, where each independent CNN captures the spatial informa- tion from one scale formation of stock data and RNN tempo- rally cascade such multi-scale spatial information. The key operation is how to construct and cascade the information. We follow the structure of CNNpred [Ehsan and Saman, 2019] as our CNN method. CNNpred is a stock data-oriented CNN whose structure outperforms the other CNNs[Gunduz et al., 2017; Di Persio and Honchar, 2016] for STP task. The conﬁguration is depicted in the bottom of Figure 2. It is a 5-layer CNN. Given an input of stock data xs ∈RL×d, the ﬁrst layer is a 1D convolution over features with 16 ﬁlters of 1 × d, after which is stacked with two convolutional layers with 32 ﬁlters of 1×3, each followed by a 2×1 max-pooling layer. The calculation of CNN can be simply represented as, us = f s cnn(xs) (10)

where us = {us i}i∈[1, ˙Ls]. Here, us i ∈R32 is the i-th of all ˙Ls

spatial feature vector obtained by CNN f s cnn(·) which is for stock data in scale s. The key operation is to concatenate these multi-scale spa- tial features in such a way,

      

     

[u1 i ,0, ..., 0], i ∈[0, ˙L1 −˙L2]

[u1 i ,u2 i−˙L1+ ˙L2, ..., 0], i ∈( ˙L1 + ˙L2, ˙L1 + ˙L3] ...

[u1 i ,u2 i−˙L1+ ˙L2, ..., uS i−˙L1+ ˙LS], i ∈( ˙L1 −˙LS, ˙L1] (11) where [, ..., ] concatenates multiple vectors into single vector vi. 0 is zero padding ensuring the same dimension as {vi} ˙L1. Such operation can 1) make {vi} contains multi-scale infor- mation at each time-step; 2) let the multi-scale information increases over time. We use Gated Recurrent Unit (GRU) to temporally cascade {vi}, which can be represented as hi = fgru(vi, hi−1) . (12) where hi is the hidden state at the i-th time-step and fgru is the GRU cell. The last hidden state h ˙L1 is passed to a fully connected neural network to make prediction, ˆssample = fnn(h ˙L1) . (13) where ˆssample ∈ R3 is the category score from downsampling-based way, fnn denotes the fully connected neural network.

4.4 Output and Objective We use a network with two fully connected layers to fuse category scores from both ways, and to output the category prediction results.

ˆy = flogit(ˆssample, ˆswavelet) , (14)

where ˆy is the output score of our model, flogit(·) denotes the output layer. We use cross-entropy as our loss function to measure the difference between our predicted classiﬁcation distribution ˆyn and real distribution yn:

J = −1

n=1 ynlog(ˆyn), (15)

where θ represents all the parameter of the model, N is the total number of samples.

5 Experiments 5.1 Datasets and Settings We test our model on two datasets: FI-2010 [Ntakaris et al., 2018] and CSI-2016. The statistics of the two datasets are presented in Table 1.

Dataset Train Test

(%) samples (%) samples

FI-2010 ↓ 32.03 352,300

31.18 31,837 − 36.91 40.66 ↑ 31.06 28.16

CSI-2016 ↓ 38.34 143,262

25.99 30,000 − 25.21 48.99 ↑ 36.45 25.02

Table 1: Dataset statistics.

FI-2010 is the ﬁrst publicly available benchmark dataset of high-frequency Limit Order Book (LOB)1 data. It comprises approximately 4.5 million events of 5 stocks from 10 con- secutive days. Every 10 non-overlapping events are ofﬁcially represented as a 144-D feature vector. Experimental settings on this dataset are as follows. Set- ting the label threshold α = 0.002, prediction horizon k = 50 and the input window size T = 100. The dataset provides 3 off-the-shelf normalised data: z-score, min-max and dec- imal precision normalisation. We follow the most previous work [Zhang et al., 2019; Tran et al., 2018] that use the ﬁrst 40 z-score normalized dimensions as the feature vector xt = [pi a(t), vi a(t), pi b(t), vi b(t)]i∈[1,10] which represent the top 10 prices and volumes of both ask and bid orders. CSI-2016 is our collected dataset from three one-minute stock index data, including the Shanghai Stock Exchange

1A limit order book is a record of unexecuted limit orders main- tained by the security specialist who works at the exchange. A limit order is a type of order to purchase or sell a security at a speciﬁed price or better, which is opposed to orders that match immediately.

Proceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20) Special Track on AI in FinTech

(SSE) Composite Index SH000001, Shenzhen Stock Ex- change Small & Medium Enterprises (SME Boards) Price Index SZ399005 and ChiNext Price Index SZ399006. It has over 170, 000 samples spanning a year from January 1st, 2016, to December 30th, 2016. Each sample xt = [ph(t), pl(t), po(t), pc(t), v(t), a(t)] is a one minute data of 6 dimensions which are high, low, open, close, volume and amount, respectively. Experimental settings on this dataset are as follows. The datasets are splited in strictly temporal order. Setting the la- bel threshold α = 0.01, prediction horizon k = 5, the input window size T = 100 and the feature dimension d = 6. All features are normalized by z-score. We ﬁrstly train the wavelet-based way and freeze its parameters, then train the rest of the model using the SGD algorithm with a learning rate of 0.0001 and weight decay 0.9.

5.2 Results and Analysis

We conduct a series of experiments to evaluate the perfor- mance of our model. We choose not only classical methods but also recently proposed an advanced model for compari- son. In this section, we ﬁrst analyze benchmark performance on FI-2010, then, analyze the results on our CSI-2016, ﬁnally give an ablation study to help understand the modules in our MTDNN. [Ntakaris et al., 2018] suggests to use F1 as the major metrics, while we also present ACC performance for reference.

Model ACC % F1 %

SVM [Tsantekidis et al., 2017b] - 49.42 MLP [Tsantekidis et al., 2017b] - 55.95 CNN-I [Tsantekidis et al., 2017a] - 59.44 LSTM [Tsantekidis et al., 2017b] - 61.43 CNN-II [Tsantekidis et al., 2018] - 47.00 B (TABL) [Tran et al., 2018] 75.58 73.64 C (TABL) [Tran et al., 2018] 79.87 78.44 DeepLOB [Zhang et al., 2019] 80.51 80.35 BL-GAM-RHN-7 [Luo and Yu, 2019] 82.00 80.88

Downsampling RCNN 80.79 80.72 DWT XGBoost 80.81 80.74 MTDNN 81.12 81.05

Table 2: Results of predicting the mid-price movements in the next 50 events on FI-2010 dataset.

Results on FI-2010 The model performance on FI-2010 is presented in Table 2, in which all the results are quoted from the original paper. All of the listed models for comparison are single-scale oriented methods. Our two-way model achieved SOTA performance with 81.05% F1 score and 81.12% accuracy. In STP, a tiny im- provement in classiﬁcation would lead to a dramatic rise in proﬁts. MTDNN achieves a higher F1 score than the previ- ous SOTA model BL-GAM-RHN-7. We analyze the result from three aspects, 1) The two-way structure of MTDNN is

more effective in extract multi-scale patterns than the one- way models. The one-way model can promote trend pre- diction performance to the same level (80+%). By combin- ing the output score of two single-way models, our model achieves higher performance. 2) As we can see, DWT, Neu- ral Tensor Network [Luo and Yu, 2019] and CNN are use- ful feature extractors in 80%-club models. Besides, most of the 80%-club models have the RNN structure, except DWT XGBoost. 3) Our key operation can effectively utilize the multi-scale patterns for STP. The Downsampling RCNN and DeepLOB has a similar structure, the major difference is the multi-scale transform and key-operation, which help Down- sampling RCNN outperform the strong baseline DeepLOB.

Model ACC % F1 %

SVM [Kim, 2003] 51.50 51.81 RF [Kara et al., 2011] 52.30 51.96 TreNet [Lin et al., 2017] 52.38 52.50 FDNN [Deng et al., 2017] 52.32 52.45 CNNPred [Ehsan and Saman, 2019] 56.63 52.93

SFM [Hu and Qi, 2017] 52.96 52.97 DWT MLP [Lahmiri, 2014] 57.29 54.19

Downsampling RCNN 62.74 61.35 DWT XGBoost 62.19 60.74 MTDNN 63.07 61.65

Table 3: Results on CSI-2016.

Results on CSI-2016 We choose seven models for comparison, where SFM is of- ﬁcially implemented and the others are implemented by our- selves. The middle two models are original multi-scale mod- els for regression of stock prices, we modify them into STP models. The ﬁrst ﬁve models are single-scale models origi- nally for STP. We present both ACC and F1 results in the table 3. Our MTDNN achieves the highest accuracy 63.07% and F1 score 61.65%. Compared with single-scale models. CNN was the strongest model for STP, however, it falls behind a simple MLP with just DWT multi-scale features. Both our single- way and two-way models outperform the other multi-scale models. It reveals that our models are superior existing multi-scale models in extracting and utilizing multi-scale fea- tures. Compared with the two adapted multi-scale models, our model obtains higher scores. It’s noticeable that the DWT MLP uses the same input as our DWT XGBoost. The only difference between the aforementioned methods is the model used for extracting multi-scale features. The results suggest that the XGBoost is more effective than MLP in extracting multi-scale features from data after DWT.

Ablation Study To further understand the multi-scale behavior in stock data, we make several variations of our model. The variations are tested under single- and multi-scale environment. The results are presented in Table 4. In single-scale environment, vari- ations are fed with only raw data. The results are listed in

Proceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20) Special Track on AI in FinTech

Model CSI-2016 FI-2010

ACC F1 ACC F1

Single-scale

XGBoost 54.16 53.52 45.56 41.85 CNN 56.63 52.93 56.73 58.22 RNN 57.56 51.21 57.29 56.65 RCNN 57.11 54.12 65.16 64.72

Multi-scale

DWT XGBoost 62.19 60.74 80.81 80.74 DWT CNN 57.90 54.96 57.56 59.24 DWT RNN 57.14 54.17 44.02 40.35 DWT RCNN 57.58 50.21 44.58 37.71 Downsampling XGBoost 54.18 53.51 45.60 41.82 Downsampling RCNN* 57.21 51.95 59.45 58.03 Downsampling RCNN 62.74 61.35 80.79 80.72 Downsampling RCNN XGBoost 62.42 60.73 80.75 80.69 Multi-kernel RCNN 58.14 54.40 67.16 66.85

Table 4: Ablation study

single-scale rows, which reveal the capability of each single model in STP task. In multi-scale environment, variations are fed with three types of scale-information. The results are listed in multi-scale rows, which demonstrate that models are sensitive to the type of scale-information. In this section, we give our analysis from two aspects as follows. The single-scale rows presents the performance of four variations, where XGBoost directly regards the raw data as features to make predictions, CNN has a convolutional fea- ture extraction before making predictions, RNN makes pre- dictions by considering the temporal information over time, RCNN captures the temporal information from a series of CNN receptive ﬁelds (local spacial information). From the results, RCNN achieves the best performance on most of the indices, and XGBoost gets weak performance. It means 1) CNN features can signiﬁcantly imporve the prediction of RNN, and raw data is hard to predict the moving trend even by a strong classiﬁer (XGBoost achieves many SOTA perfor- mance in Kaggle tasks). Note that the structure of RCNN is very similar to [Zhang et al., 2019], however we cannot reproduce their results in FI-2010. In multi-scale rows, the ﬁrst four rows are aforementioned variations fed with DWT-based multi-scale features. The following four variations are fed with downsampling-based multi-scale features. The last variation is fed with a new type of scale-information obtained by CNN with multiple kernel size. Comparing the ﬁrst four variations with their counterpart, XGBoost obtains signiﬁcant increase, from the worst to the best, about 8 points in CSI-2016 and over 30 points in FI- 2010. While the other variations get negative or slightly pos- itive results. We analyze that DWT is a recursive convolution over low-frequency components with speciﬁed ﬁlters, which is similar to the operation in CNN. Thus CNN is hard to ex- tract more information from DWT convolved features. The bad performance of RNN is due to the broken temporal struc- ture in DWT features. Comparing downsampling RCNN* and downsampling

RCNN, where RCNN* is an alternative way of using output features from different CNNs and RCNN use our proposed key operation. In RCNN*, each CNN is followed with an in- dependent RNN to temporally fuse the features of one scale. The output of each RNN is concatenated to make predictions. The results shows the superior of our key operation in using multi-scale information. Comparing XGBoost, DWT XGBoost, downsampling XGBoost, downsampling RCNN and downsampling RCNN XGBoost, we can understand that 1) downsampling features valid on RCNN is useless on XGBoost, and 2) downsampling do not provide any new features which result in an equiva- lent performance between XGBoost and downsampling XG- Boost, and 3) features from dowsampling RCNN cannot fur- ther boost the performance XGBoost. Last, but not the least, Comparing DWT RCNN, downsam- pling RCNN and multi-kernel RCNN, we can understand that RCNN is better at capturing downsampling-based features. We think that downsampling can weaken the nonstationary in the raw data by explicitly droping out points in the raw data, and the denoised raw data can help CNN to extract more use- ful features.

6 Conclusion and Future Works

This paper proposes a multi-scale oriented model, MTDNN, for STP. Our motivation is to fully explore the potential of multi-scale information within the stock data. We explore two types of multi-scale information extracted from two mod- ules, downsampling with RCNN and DWT with XGBoost, as the two way of our MTDNN. By combining the two mod- ules, MTDNN achieves state-of-the-art performance on the benchmark FI-2010 dataset. We publish a one-minute dataset CSI-2016 and present the result for further study on the STP task. The results on both datasets reveal the effectiveness of multi-scale information and the superior of our model in us- ing such information. In the future, we prepare to introduce an attention mechanism to dynamically choose the most rele- vant scale of information.

Proceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20) Special Track on AI in FinTech

References [Chen and Guestrin, 2016] Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In SIGKDD 2016, pages 785–794. ACM, 2016. [Cui et al., 2016] Zhicheng Cui, Wenlin Chen, and Yixin Chen. Multi-scale convolutional neural networks for time series classiﬁcation. arXiv preprint arXiv:1603.06995, 2016. [Dacorogna et al., 1996] Michel M Dacorogna, Cindy L Gauvreau, Ulrich A M¨uller, Richard B Olsen, and Olivier V Pictet. Changing time scale for short-term forecasting in ﬁnancial markets. Journal of Forecasting, 15(3):203–227, 1996. [Deng et al., 2017] Yue Deng, Zhiquan Ren, Youyong Kong, Feng Bao, and Qionghai Dai. A hierarchical fused fuzzy deep neural network for data classiﬁcation. IEEE Trans- actions on Fuzzy Systems, 25(4):1006–1012, 2017. [Di Persio and Honchar, 2016] Luca Di Persio and Olek- sandr Honchar. Artiﬁcial neural networks architectures for stock price prediction: Comparisons and applications. In- ternational journal of circuits, systems and signal process- ing, 10:403–413, 2016. [Ehsan and Saman, 2019] Ehsan and Saman. Cnnpred: Cnn- based stock market prediction using a diverse set of vari- ables. Expert Systems with Applications, 129:273–285, 2019. [Fern´andez et al., 2019] C´esar Fern´andez, Luis Salinas, and Claudio E Torres. A meta extreme learning machine method for forecasting ﬁnancial time series. Applied In- telligence, 49(2):532–554, 2019. [Geva, 1998] Amir B Geva. Scalenet-multiscale neural- network architecture for time series prediction. IEEE Transactions on neural networks, 9(6):1471–1482, 1998. [Gunduz et al., 2017] Hakan Gunduz, Yusuf Yaslan, and Zehra Cataltepe. Intraday prediction of borsa istanbul us- ing convolutional neural networks and feature correlations. Knowledge-Based Systems, 137:138–148, 2017. [Hu and Qi, 2017] Hao Hu and Guo-Jun Qi. State-frequency memory recurrent neural networks. In ICML, pages 1568– 1577, 2017. [Kara et al., 2011] Yakup Kara, Melek Acar Boyacioglu, and ¨Omer Kaan Baykan. Predicting direction of stock price index movement using artiﬁcial neural networks and sup- port vector machines: The sample of the istanbul stock exchange. Expert systems with Applications, 38(5):5311– 5319, 2011. [Kim, 2003] Kyoung-jae Kim. Financial time series fore- casting using support vector machines. Neurocomputing, 55(1-2):307–319, 2003. [Lahmiri, 2014] Salim Lahmiri. Wavelet low-and high- frequency components as features for predicting stock prices with backpropagation neural networks. Journal of King Saud University-Computer and Information Sci- ences, 26(2):218–227, 2014.

[Li et al., 2016] Xiaodong Li, Haoran Xie, Ran Wang, Yi Cai, Jingjing Cao, Feng Wang, Huaqing Min, and Xi- aotie Deng. Empirical analysis: stock market prediction via extreme learning machine. Neural Computing and Ap- plications, 27(1):67–78, 2016. [Lin et al., 2017] Tao Lin, Tian Guo, and Karl Aberer. Hy- brid neural networks for learning the trend in time series. In IJCAI-17, pages 2273–2279, 2017. [Luo and Yu, 2019] Wei Luo and Feng Yu. Recurrent high- way networks with grouped auxiliary memory. IEEE Ac- cess, 7:182037–182049, 2019. [Ntakaris et al., 2018] Adamantios Ntakaris, Martin Magris, Juho Kanniainen, Moncef Gabbouj, and Alexandros Iosi- ﬁdis. Benchmark dataset for mid-price forecasting of limit order book data with machine learning methods. Journal of Forecasting, 37(8):852–866, 2018. [Papadimitriou and Yu, 2006] Spiros Papadimitriou and Philip Yu. Optimal multi-scale patterns in time series streams. In SIGMOD 2006, pages 647–658. ACM, 2006. [Patel et al., 2015] Jigar Patel, Sahil Shah, Priyank Thakkar, and K Kotecha. Predicting stock and stock price index movement using trend deterministic data preparation and machine learning techniques. Expert Systems with Appli- cations, 42(1):259–268, 2015. [Tran et al., 2018] Dat Thanh Tran, Alexandros Iosiﬁdis, Juho Kanniainen, and Moncef Gabbouj. Temporal attention-augmented bilinear network for ﬁnancial time- series data analysis. IEEE transactions on neural networks and learning systems, 30(5):1407–1418, 2018. [Tsai and Hsiao, 2010] Chih-Fong Tsai and Yu-Chieh Hsiao. Combining multiple feature selection methods for stock prediction: Union, intersection, and multi-intersection approaches. Decision Support Systems, 50(1):258–269, 2010. [Tsantekidis et al., 2017a] Avraam Tsantekidis, Nikolaos Passalis, Anastasios Tefas, Juho Kanniainen, Moncef Gab- bouj, and Alexandros Iosiﬁdis. Forecasting stock prices from the limit order book using convolutional neural net- works. In CBI 2017, volume 1, pages 7–12. IEEE, 2017. [Tsantekidis et al., 2017b] Avraam Tsantekidis, Nikolaos Passalis, Anastasios Tefas, Juho Kanniainen, Moncef Gab- bouj, and Alexandros Iosiﬁdis. Using deep learning to de- tect price change indications in ﬁnancial markets. In EU- SIPCO 2017, pages 2511–2515. IEEE, 2017. [Tsantekidis et al., 2018] Avraam Tsantekidis, Nikolaos Passalis, Anastasios Tefas, Juho Kanniainen, Moncef Gabbouj, and Alexandros Iosiﬁdis. Using deep learning for price prediction by exploiting stationary limit order book features. arXiv preprint arXiv:1810.09965, 2018. [Zhang et al., 2019] Zihao Zhang, Stefan Zohren, and Stephen Roberts. Deeplob: Deep convolutional neural net- works for limit order books. IEEE Transactions on Signal Processing, 67(11):3001–3012, 2019.

Proceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20) Special Track on AI in FinTech