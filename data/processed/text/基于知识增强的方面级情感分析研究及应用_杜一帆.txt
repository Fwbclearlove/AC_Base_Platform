密级
：
公开


ｂ


硕士学位论文


＿


题目
：
基于知识增强的方面级情感分析研究及应用


学号
：２０１９１８０１１
１


姓名
：杜
一帆


学科专业
：计算机技术


培养方式
：非全曰制


导师
：李睿凡


学院
：人工智能学院


２０２３年
６月
１
曰


中国
＿北京


密级
：
公开


分立却耄大綮


硕士学位论文
（专业学位）


题目
：基于知识増强的方面级


情感分析研究及应用


学号
：２０１９１８０１１１


姓
名
：
杜
一帆


学科专业
：计算机技术


培养方式
：非全日制


导师
：李睿凡


学院
：人工智能学院


２０２３年
６
月
１
日


？
ＢＥＩＪＩＮＧＵＮＩＶＥＲＳＩＴＹＯＦ


ＰＯＳＴＳＡＮＤ


ＴＥＬＥＣＯＭＭＵＮＩＣＡＴＩＯＮＳ


ＭａｓｔｅｒＤｉｓｓｅｒｔａｔｉｏｎ


ＲＥＳＥＡＲＣＨＡＮＤＡＰＰＬＩＣＡＴＩＯＮ


ＯＦＫＮＯＷＬＥＤＧＥＥＮＨＡＮＣＥＤ


ＡＳＰＥＣＴ
－ＢＡＳＥＤＳＥＮＴＩＭＥＮＴＡＬＡＮＹＳＩＳ


ＳｔｕｄｅｎｔＩＤ
：２０１９１８０１１１


Ａｕｔｈｏｒ：ＤｕＹｉｆａｎ


Ｓｕｂ
ｊｅｃｔ
：ＣｏｍｐｕｔｅｒＴｅｃｈｎｏｌｏｇｙ


Ｓｕｐｅｒｖｉｓｏｒ：ＬｉＲｕｉｆａｎ


Ｉｎｓｔｉｔｕｔｅ
：ＳｃｈｏｏｌｏｆＡｒｔｉｆ
ｉｃｉａｌＩｎｔｅｌｌｉｇｅｎｃｅ


Ｊｕｎｅ１
，２０２３


答辩委员会名单


职务姓
名职
称工
作
单
位


主席刘军副教授北京邮电大学


委员祝闯副教授北京邮电大学


委员刘芳副教授北京邮电大学


委员何刚副研究员北京邮电大学


委员张乃柏教授中国电科网络通信研究院
（５４所
）


秘书陈科良讲师北京邮电大学


答辩日期２０２３年
５月
２６
日


基于知识增强的方面级情感分析研究及应用


摘
要


方面级情感分析
，
旨在以
“ 方面
” 为分析粒度
，
预测出文本中每


个方面的情感极性
，
是自然语言处理领域中的前沿研究之
一
。
其中
，


方面词情感分析和方面类别情感分析是最重要的两个子任务
，两者的


主要区别在于预测对象是否显式地存在于句子之中
。近年来图神经网


络在方面级情感分析领域取得了较优效果
。然而
，大多数方法的性能


提升有限
，主要原因在于对外部知识利用不充分
，对句中概念词与方


面类别关系构造不合理
，以及对句法结构和语义关系互补性的建模缺


失
。


针对以上问题
，
本文开展了如下工作
：


１
．针对方面词情感分析任务
，提出
一种情感感知的双通道图卷积


神经网络模型
，其中双通道分别为语法通道和语义通道
。在语法通道


中
，本文首先利用依存句法分析方法捕获句子的语法结构
，得到原始


的句法依存树矩阵
，
然后利用
ＳｅｎｔｉｃＮｅｔ中词的情感数值构建句子的


情感值矩阵
，
以对原始矩阵增强情感知识
，
另外使用词性知识构建词


性知识矩阵
，对句法依存树矩阵进
一步增强
；
在语义通道中
，
本文将


基于Ｃｏｎｃｅｐ
ｔＮｅｔ知识库训练的词向量与经过ＢＵＬＳＴＭ编码的隐向量


融合
，对词语进行知识语义增强
。最终解决了语法
、语义通道对外部


知识利用不足的问题
。


２
．针对方面类别情感分析任务
，
提出
一种知识增强的多通道图卷


积神经网络模型
，其中多通道分别为知识通道
、语法通道和语义通道
。


在知识通道中
，
本文利用基于ＷｏｒｄＮｅｔ
的相似度函数计算方面类别


与句子上下文之间的语义相似度
，进而得到与方面类别相关的相似度


矩阵
，解决了方面类别与句中相关概念词关系的捕获和利用不合理的


问题
；在语法和语义通道中
，
本文分别使用句法依存分析和自注意力


机制构建对应的邻接矩阵
，设计基于注意力的特征融合模块
，
融合语


法和语义通道的特征
，解决了模型对句法结构和语义关系互补性的建


模缺失问题
。


３
．设计并实现了面向评论的方面级情感分析系统
，
该系统由用户


信息管理模块
、模型管理模块和情感分析与可视化模块组成
。该系统


Ｉ


平台无关
、用户友好
，
允许用户在进行可视化参数配置、模型训练以


及对文本进行方面级情感分析
。


关键词
：
方面级情感分析方面词情感分析方面类别情感分析
图卷


积神经网络知识增强


ＩＩ


ＲＥＳＥＡＲＣＨＡＮＤＡＰＰＬＩＣＡＴＩＯＮＯＦ


ＫＮＯＷＬＥＤＧＥＥＨＡＮＣＥＤＡＳＰＥＣＴ
－


ＢＡＳＥＤＳＥＮＴＩＭＥＮＴＡＮＡＬＹＳＩＳ


ＡＢＳＴＲＡＣＴ


Ａｓｐｅｃｔ
－ｂａｓｅｄｓｅｎｔｉｍｅｎｔａｎａｌｙｓｉｓ
，ｗｈｉｃｈａｉｍｓｔｏｐｒｅｄｉｃｔｔｈｅｓｅｎｔｉｍｅｎｔ


ｐｏｌａｒｉｔｙｆｏｒ
ｇ
ｉｖｅｎａｓｐｅｃｔｓｏｆａｓｅｎｔｅｎｃｅ
，
ｉｓｏｎｅｏｆｔｈｅｆｒｏｎｔｉｅｒｓｏｆｒｅｓｅａｒｃｈ


ｉｎｎａｔｕｒａｌｌａｎｇｕａｇｅｐｒｏｃｅｓｓｉｎｇ
．Ｔｈｅｔｗｏｍｏｓｔｉｍｐｏｒ
ｔａｎｔｓｕｂ
－ｔａｓｋｓａｒｅ


ａｓｐｅｃｔｔｅｒｍｓｅｎｔｉｍｅｎｔａｎａｌｙｓｉｓａｎｄａｓｐｅｃｔｃａｔｅｇｏｒｙｓｅｎｔｉｍｅｎｔａｎａｌｙｓｉｓ
，ｔｈｅ


ｍａｉｎｄｉｆｆｅｒｅｎｃｅｂｅｉｎｇｗｈｅｔｈｅｒｔｈｅ
ｐｒｅｄｉｃｔｅｄｏｂ
ｊｅｃｔｉｓｅｘｐ
ｌｉｃｉｔｌｙｐｒｅｓｅｎｔｉｎ


ｔｈｅｓｅｎｔｅｎｃｅｏｒｎｏｔ
．Ｉｎｒｅｃｅｎｔｙｅａｒｓｇｒａｐｈｎｅｕｒａｌｎｅｔｗｏｒｋｓｈａｖｅａｃｈｉｅｖｅｄ


ｓｕｐｅｒｉｏｒｒｅｓｕｌｔｓｉｎｔｈｉｓａｒｅａ．Ｈｏｗｅｖｅｒ
，ｍｏｓｔｏｆｔｈｅｍｅｔｈｏｄｓｈａｖｅｓｈｏｗｎ


ｌｉｍｉｔｅｄ
ｐｅｒｆｏｒｍａｎｃｅｉｍｐｒｏｖｅｍｅｎｔ
，ｍａｉｎｌｙｄｕｅｔｏｉｎｓｕｆｆｉｃｉｅｎｔｅｘｐ
ｌｏｉｔａｔｉｏｎ


ｏｆｅｘｔｅｒ
ｎａｌｋｎｏｗｌｅｄｇｅ
，ｐｏｏｒｃｏｎｓｔｒｕｃｔｉｏｎｏｆｔｈｅｒｅｌａｔｉｏｎｓｈｉｐｂｅｔｗｅｅｎ


ｃｏｎｔｅｘｔｓａｎｄａｓｐｅｃｔｃａｔｅｇｏｒｉｅｓｉｎｓｅｎｔｅｎｃｅｓ
，ａｎｄｌａｃｋｏｆｍｏｄｅｌｌｉｎｇｏｆｔｈｅ


ｃｏｍｐ
ｌｅｍｅｎｔａｒｉｔ
ｙｏｆｓｙｎｔａｃｔｉｃｓｔｒｕｃｔｕｒｅａｎｄｓｅｍａｎｔｉｃｒｅｌａｔｉｏｎｓ
．


Ｔｏａｄｄｒｅｓｓｔｈｅａｂｏｖｅ
－ｍｅｎｔｉｏｎｅｄ
ｐ
ｒｏｂｌｅｍｓ
，ｏｕｒｗｏｒｋｓａｒｅａｓｆｏｌｌｏｗｓ
，


１
．Ｆｏｒｔｈｅａｓｐｅｃｔｔｅｉｉｎｓｅｎｔｉｍｅｎｔａｎａｌｙ
ｓｉｓｔａｓｋ
，ｗｅ
ｐ
ｒｏｐｏｓｅａｓｅｎｔｉｍｅｎｔ
－


ａｗａｒｅｄｕａｌ
－ｃｈａｎｎｅｌ
ｇｒａｐｈｃｏｎｖｏｌｕｔｉｏｎａｌｎｅｕｒａｌｎｅｔｗｏｒｋｓｍｏｄｅｌ
，ｗｈｅｒｅｔｈｅ


ｔｗｏｃｈａｎｎｅｌｓａｒｅｋｎｏｗｌｅｄｇｅｅｎｈａｎｃｅｄｓｙｎｔａｃｔｉｃｃｈａｎｎｅｌａｎｄｋｎｏｗｌｅｄｇｅ


ｅｎｈａｎｃｅｄｓｅｍａｎｔｉｃｃｈａｎｎｅｌ
．Ｉｎｔｈｅｆｉｒｓｔｃｈａｎｎｅｌ
，ｍａｔｒｉｘｃｏｎｓｔｒｕｃｔｅｄｆｒｏｍ


ｄｅｐｅｎｄｅｎｃｙ
ｔｒｅｅｓａｒｅｆｉｒｓｔｕｓｅｄｔｏｃａｐｔｕｒｅｔｈｅｓｙｎｔａｃｔｉｃｓｔｒｕｃｔｕｒｅｏｆｔｈｅ


ｓｅｎｔｅｎｃｅ
．Ｔｈｅｎ
，ｔｈｅｎａｉｖｅｍａｔｒｉｘｉｓｅｎｈａｎｃｅｄｂｙｂｏｔｈｔｈｅｓｅｎｔｉｍｅｎｔｓｃｏｒｅ


ｆｒｏｍｗｏｒｄｓｉｎＳｅｎｔｉｃＮｅｔａｎｄｔｈｅｌｅｘｉｃａｌｋｎｏｗｌｅｄｇｅｏｆ
ｐａｒｔ
－ｏｆ
－ｓｐｅｅｃｈ
；Ｉｎ


ｔｈｅｓｅｃｏｎｄｃｈａｎｎｅｌ
，ｔｈｅｗｏｒｄｖｅｃｔｏｒｓｔｒａｉｎｅｄｂａｓｅｄｏｎｔｈｅＣｏｎｃｅｐ
ｔＮｅｔａｒｅ


ｆｕｓｅｄｗｉｔｈｔｈｅｈｉｄｄｅｎｒｅｐ
ｒｅｓｅｎｔａｔｉｏｎｓｅｎｃｏｄｅｄｂｙ
ｔｈｅＢｉ
－ＬＳＴＭｔｏｂｅｔｅｒ


ｕｔｉｌｉｚｅｔｈｅｋｎｏｗｌｅｄｇｅｓｅｍａｎｔｉｃｓｏｆｗｏｒｄｓ
．Ｆｉｎａｌｌｙ，ｔｈｅｐ
ｒｏｂｌｅｍｏｆ


ｉｎｓｕｆｆｉｃｉｅｎｔｕｓｅｏｆｅｘｔｅｒ
ｎａｌｋｎｏｗｌｅｄｇｅｉｎｂｏｔｈｓｙｎｔａｃｔｉｃａｎｄｓｅｍａｎｔｉｃ


ｃｈａｎｎｅｌｓｉｓｓｏｌｖｅｄ
，


ｉｎ


２
．Ｆｏｒｔｈｅａｓｐｅｃｔｃａｔｅｇｏｒｙｓｅｎｔｉｍｅｎｔａｎａｌｙｓｉｓｔａｓｋ
，ｗｅｐｒｏｐｏｓｅａ


ｋｎｏｗｌｅｄｇｅ
－ｅｎｈａｎｃｅｄｍｕｌｔｉ
－ｃｈａｎｎｅｌｇｒａｐｈｃｏｎｖｏｌｕｔｉｏｎａｌｎｅｕｒａｌｎｅｔｗｏｒｋｓ


ｍｏｄｅｌ
，
ｉｎｗｈｉｃｈｔｈｅｍｕｌｔｉ
－ｃｈａｎｎｅｌｓａｒｅｋｎｏｗｌｅｄｇｅｃｈａｎｎｅｌ
，ｓｙｎｔａｃｔｉｃ


ｃｈａｎｎｅｌａｎｄｓｅｍａｎｔｉｃｃｈａｎｎｅｌ
，ｒｅｓｐｅｃｔｉｖｅｌｙ
．Ｉｎｔｈｅｋｎｏｗｌｅｄｇｅｃｈａｎｎｅｌ
，ｗｅ


ｕｓｅｔｈｅＷｏｒｄＮｅｔ
－ｂａｓｅｄｓｉｍｉｌａｒｉｔｙｆｕｎｃｔｉｏｎｔｏｃａｌｃｕｌａｔｅｔｈｅｃｏｎｃｅｐ
ｔ
－ｂａｓｅｄ


ｓｉｍｉｌａｒｉｔｙｂｅｔｗｅｅｎｔｈｅａｓｐｅｃｔｃａｔｅｇｏｒｙａｎｄｔｈｅｃｏｎｔｅｘｔ
，ｂａｓｅｄｏｎｗｈｉｃｈ
，ｗｅ


ｃｏｎｓｔｒｕｃｔｔｈｅｓｉｍｉｌａｒｉｔｙｍａｔｒｉｘｃｏｎｃｅｒｎｉｎｇ
ｔｈｅａｓｐｅｃｔｃａｔｅｇｏｒｙ
．Ｔｈｉｓ


ａｄｄｒｅｓｓｅｓｔｈｅｉｓｓｕｅｏｆｕｎｒｅａｓｏｎａｂｌｅｃａｐ
ｔｕｒｅａｎｄｅｘｐ
ｌｏｉｔａｔｉｏｎｏｆｔｈｅ


ｒｅｌａｔｉｏｎｓｈｉｐｂｅｔｗｅｅｎｔｈｅａｓｐｅｃｔｃａｔｅｇｏｒｙａｎｄｔｈｅｒｅｌｅｖａｎｔｗｏｒｄｓｉｎｔｈｅ


ｓｅｎｔｅｎｃｅ
；Ｉｎｔｈｅｓｙｎｔａｃｔｉｃａｎｄｓｅｍａｎｔｉｃｃｈａｎｎｅｌｓ
，ｄｅｐｅｎｄｅｎｃｙａｎａｌｙｓｉｓａｎｄ


ｔｈｅｓｅｌｆ
－ａｔｔｅｎｔｉｏｎｍｅｃｈａｎｉｓｍａｒｅｕｓｅｄｔｏｃｏｎｓｔｒｕｃｔｔｈｅｃｏｒｒｅｓｐｏｎｄｉｎｇ


ａｄ
ｊａｃｅｎｃｙｍａｔｒｉｃｅｓｆｏｒＧＣＮｔｏｇｅｎｅｒａｔｅｓｙｎｔａｃｔｉｃａｎｄｓｅｍａｎｔｉｃｆｅａｔｕｒｅｓ
，


ｗｈｉｃｈａｒｅｔｈｅｎｆｕｓｅｄｂｙａｄｅｓｉｇｎｅｄａｔｅｎｔｉｏｎ
－ｂａｓｅｄｆｕｓｉｏｎｍｏｄｕｌｅ
．Ｔｈｉｓ


ｓｏｌｖｅｓｔｈｅｌａｃｋｏｆｌｅｖｅｒａｇ
ｉｎｇｔｈｅｃｏｍｐ
ｌｅｍｅｎｔａｒｉｔｙｏｆｓｙｎｔａｃｔｉｃｓｔｒｕｃｔｕｒｅａｎｄ


ｓｅｍａｎｔｉｃｉｎｆｏｒｍａｔｉｏｎ
．


３
．Ａｒｅｖｉｅｗ
－ｏｒｉｅｎｔｅｄａｓｐｅｃｔ
－ｂａｓｅｄｓｅｎｔｉｍｅｎｔａｎａｌｙｓｉｓｓｙｓｔｅｍｉｓ


ｄｅｓｉｇｎｅｄａｎｄｉｍｐ
ｌｅｍｅｎｔｅｄ
，ｗｈｉｃｈｃｏｎｓｉｓｔｓｏｆａｕｓｅｒｉｎｆｏｒｍａｔｉｏｎ


ｍａｎａｇｅｍｅｎｔｍｏｄｕｌｅ
，ａｍｏｄｅｌｍａｎａｇｅｍｅｎｔｍｏｄｕｌｅａｎｄａｓｅｎｔｉｍｅｎｔ


ａｎａｌｙｓｉｓａｎｄｖｉｓｕａｌｉｚａｔｉｏｎｍｏｄｕｌｅ
．Ｔｈｅｓｙｓｔｅｍｉｓ
ｐ
ｌａｔｆｏｒｍ
－
ｉｎｄｅｐｅｎｄｅｎｔａｎｄ


ｕｓｅｒ
－ｆｒｉｅｎｄｌｙ，ａｌｌｏｗｉｎｇｕｓｅｒｓｔｏｖｉｓｕａｌ
ｌｙｃｏｎｆｉｇｕｒｅ
ｐａｒａｍｅｔｅｒｓ
，ｔｒａｉｎｍｏｄｅｌｓ


ａｎｄ
ｐｅｒｆｏｒｍａｓｐｅｃｔ
－ｂａｓｅｄｓｅｎｔｉｍｅｎｔａｎａｌｙｓ
ｉｓｏｎｔｅｘｔｓ
．


ＫＥＹＷＯＲＤＳ
：ａｓｐｅｃｔ
－ｂａｓｅｄｓｅｎｔｉｍｅｎｔａｎａｌｙｓｉｓ
，ａｓｐｅｃｔｔｅｒｍｓｅｎｔｉｍｅｎｔ


ａｎａｌｙｓｉｓ
，ａｓｐｅｃｔｃａｔｅｇｏｒｙｓｅｎｔｉｍｅｎｔａｎａｌｙｓ
ｉｓ
，ｇｒａｐｈｃｏｎｖｏ
ｌｕｔｉｏｎａｌｎｅｕｒａｌ


ｎｅｔｗｏｒｋ
，ｋｎｏｗｌｅｄｇｅｅｎｈａｎｃｅｍｅｎｔ


ＩＶ


目
录


第
一章绪论


１


１
．
１研究背景及其意义
１


１
．２
国内外研宄现状
２


１
．２
．１
文本情感分析
２


１
．２
．２文木方面级情感分析
３


１
．３主要研究内容
６


１
．３
．
１
现存问题
６


１
．３
．２研宄内容
７


１
．４本文结构安排
８


第二章背景知识及相关技术
１１


２
．
１
词向量表示技术
１１


２
．
１
．
１Ｗｏｒｄ２Ｖｅｃ
１１


２
．
１
．２ＧｌｏＶｅ
１２


２
．
１
．３ＥＬＭｏ
１２


２
．２循环神经网络
１３


２
．２
．
１循环神经网络
１３


２
．２
．２长短期记忆网络
１４


２
．３
图神经网络
１５


２
．４注意力机制
１６


２
．４
．
１
软注意力
１６


２
．４
．２硬注意力
１７


２
．４
．３
自注意力
１７


２
．５预训练语言模型
１８


２
．５
．
１Ｔｒａｎｓｆｏｒｍｅｒ




１８


２
．５
．２ＢＥＲＴ
２
１


２
．５
．３ＧＰＴ
２２


２
．６知识表
：本技术
２３


２
．７本章小节
２３


第三章情感感知的双通道图卷积神经网络的方面词情感分析研宄
２５


３
．
１情感感知的双通道图卷积神经网络算法
２５


３
．２
矢口识库与知识表不
２６


３
．２
．
１Ｃｏｎｃｅｐ
ｔＮｅｔ
２６


３
．２
．２ＳｅｎｔｉｃＮｅｔ
２６


３
．２
．３知识表示
２６


３
．３编码器

２７


３
．３
．
１
输入层
２７


３
．３
．２基于知识增强的语法感知通道
２７


３
．３
．３基于知识增强的语义感知通道
３
１


３
．３
．４非方面词掩盖层
３３


３
．４情感分类器
３３


３
．５损失函数
３４


３
．６实验设计与分析
３４


３
．６
．
１实验数据
３４


３
．６
．２评价指标介绍
３４


３
．６
．３实验参数及设置
３５


３
．６
．４对比模型
３６


３
．６
．５
结果分析
３７


３
．６
．６消融实验与分析
３８


３
．６
．７
图卷积层数影响
３９


３
．６
．８案例分析

４０


３
．７本章小结
、，
４１


第四章基于知识增强的多通道图卷积神经网络的方面类别情感分析


研究

４３


４
．
１基于知识增强的多通道图卷积神经网络算法４３


４
．２基于ＷｏｒｄＮｅｔ的语义相似度
４４


４
．３编码器
４５


４
．３
．
１
输入层
４５


４
．３
．２基于依存句法分析的语法图通道
４５


４
．３
．３基于自注意力机制的语义图通道
４６


４
．３
．４基于知识增强的知识图通道
４７


４
．４多通道融合模块
４９


４
．５分类与损失函数
５０


４
．６实验设计与分析
５１


４
．６
．
１
实验数据
５１


４
．６
．２评价指标介绍
５
１


４
．６
．３实验参数及设置
５１


４
．６
．４对比模型
５
１


４
．６
．５
结果分析
５２


４
．６
．６消融实验与分析
５４


４
．６
．７
图卷积层数影响
５５


４
．６
．８案例分析

５６


４
．７本章小结
５７


第五章面向用户评论的方面级情感分析系统设计与实现５９


５
．
１
系统需求分析
５９


５
．
１
．
１
功能性需求
５９


５
．
１
．２非功能性需求
６１


５
．２系统概要设计
６１


５
．２
．
１
总体功能设计
６２


５
．２
．２
系统架构设计
６２


５
．３系统详细设计
６３


５
．３
．
１
用户管理模块
６３


５
．３
．２模型训练模块设计
６４


５
．３
．３情感分析与可视化模块设计
６４


５
．３
．４数据库设计
６５


５
．４系统实现
６６


５
．４
．
１
系统实现环境
６６


５
．４
．２
用户管理模块实现
６７


５
．４
．３模型训练模块实现
６７


５
．４
．４情感分析与可视化模块实现
６８


５
．５
系统测试
７０


５
．５
．
１
功能件测试
７０


５
．５
．２非功能性测试
７０


５
．６本章小结
，
７
１


第六章总结与展望
７３


６
．
１Ｘ作总结
＂
．
．
．
７３


６
．２工作展望
，
７４


参考文献
７５


第
一章
绪论


第
一章绪论


１
．
丨
研究背景及其意义


互联网及移动终端设备的普及为广大的互联网用户提供了信息交流的硬件


基础
。
自
２００４年以来
，Ｗｅｂ２
．０革命深刻地改变了传统Ｗｅｂ
ｌ
．Ｏ的信息传播方式
，


信息门户网站从信息分发的
“ 只读
” 模式慢慢发展延伸出新的商业模式
。
各种社


交媒体
、在线社区
、
电子商务
、
本地生活和视频直播等应用开始更加注重用户使


用时的交互体验
，
增强了互联网用户使用互联网应用时的
“ 参与感
”
。
互联网用


户既是互联网内容的消费者
，又是互联网内容的生产者
。以ＵＧＣＯＪｓｅｒＧｅｎｅｒａｔｅｄ


Ｃｏｎｔｅｎｔ
）为代表的信息内容在互联网上的爆发式增长加速推动了大数据
、数据挖


掘和人工智能等理论和技术的发展
，
让我们深刻地认识到数据的价值和作用
。


，
、
、


／
＼


，体评分
＞□味賑务
注伦比
坏渙— ＿

？
逆
ｆ＼


＇★食★★喻５
．０５０４９５
０
？


食＊會會舍ｒ
＇



Ｉ


味谨赞
３６１缀努好
１７０往份比萬
８６
二
：？约
漂亮
．不悚逞苹粜的设计
，換起来也很有质
｜


？感
：


１装修ｍ葉
７７翁材新＿５３分１足
４７
：
：飞塔免
很薄
很轻巧
ｆ


Ｉ
Ｅ


＇
运行诱ｔ
基越苡
我是办公
够用了费是很快的了
ｉ


？
一

＾
？
－歡热擦二
不怎么热
．没珣筠磨
．没有嫌爸
！


粱
屏驀非常漭羝
对餱譌很友好
｜


来
．踞努非常贴心
．为等待的客人准
■＿

＊


备了好呓的小点心
．其次
．脹务人员非常热情关注客
－ｍｍｍｍｍｍｒ
ｎ


人需求
．最后吃了之后味道非常不锗琨
，菜品多样
．ＢＵＳＨＩ
！


ｒ＾ＢＢ
１８＾
８
— ，
，
扩
，


Ｈｉ


、己消费
．
等闲识得东圾面双人餐
￥
１９８
〉
，Ｑ５￡）
１


＼
／


、
／


、


？
一
一＿
＿
＿一＿
＿＿
＿＿
＿
＿
＿
＿
＿
＿
＿
＿＿一
＿＿
＿
嫌
＿＿
＿
＿＿
＿＿
＿一
一
一＿＿
＿
＿＿
＿
＿＿
＿＿
＿＿一
一＿＿
辟


图
卜
１
本地生活应用中关于餐饮店铺的评论及电商应用中关于电脑的评论


以
ＵＧＣ为代表的文本数据
，
是人们在互联网深度
“冲浪
” 时表达情感或观点


的重要载体之
一
。
从社会治理的角度来看
，
“ 微博热榜
”
和
“ 抖音热榜
” 常常代


表着社会层面的舆论走向
，挖掘文本中的情感信息有利于相关部门及时掌控舆情
，


尽早处理问题
，快速回应社会关切
。
从金融投资的角度来看
，
社会层面的舆情深


刻地影响着投资者对市场未来的预期
，
从而影响着他们的投资行为
，进
一步决定


了股票的涨跌
。从商业竞争角度来看
，无论是电商平台里的商品评论
，抑或是本


地生活
、视频直播里的评论数据
，
都蕴含着丰富的情感价值
。
图
１
－
１展示了本地


Ｉ


北京邮电大学工程硕士学位论文


生活应用中关于餐饮店铺的评论以及电商应用中关于电脑的评论
，这些评论的情


感极性深刻影响着用户的购物选择和消费体验
，利用好这些评论数据
，商家可以


做好精细化运营从而提高销售额
；平台则可以利用这些数据构建用户画像
，做出


精准推荐
，
促成交易
，
提高平台利润
。
除此之外
，情感分析还在生活中的方方面


面发挥着重要作用
。


情感分析
［
１
）的研究目标是根据
一段自然语言文本分析出其中的评价对象
、
观


点表达以及情感极性等等
。
情感极性可以分为三类
：
积极的
、
消极的和中立的
。


按照情感极性评价对象的粒度粗细
，
文本情感分析任务可以分为文档级别ｍ的情


感分析
、
句子级别
［３
］的情感分析和方面级别
［４
］情感分析
。
文档级别
［５
］的情感分析


研宄对象粒度最大
，研宄内容是对
一篇文章或文档的整体表达的情感分析进行判


断
。句子级别
１６
１情感分析研宄目标粒度小于文档级别
，
它将
一个句子作为研宄对


象
，
来预测句子整体所表达的情感极性
。


方面级别情感分析
，
又被称为细粒度情感分析
，
分析的对象粒度比句子级别


情感分析更小
。该任务的目标是对评论文本中不同的评价对象或评价对象的属性


进行情感极性的判断和预测
。文本中的评价对象或评价对象的属性即为方面级情


感分析任务中的
“ 方面
”
。真实语境场景下
，
一句话中可能出现多个
“ 方面
”
，
而


且用户对不同的
“ 方面
” 有着不同情感倾向
。
例如
，
评论数据
“这家店的牛肉面


很好吃
，
但服务不太好
。
”
中
，
用户对于
“ 牛肉面
”这个方面
，
表达的情感是正向


的
。而用户对于
“服务
” 这个方面表达的情感却是负向的
。
由于用户对两个评价对


象的情感态度截然相反
，因此仅仅使用
一个粗粒度的情感极性来概括整个句子的


情感极性是不合理和不准确的
。


面向评论的方面级评价体系为用户提供了更多元的评价维度
，让用户能从多


角度多方面对评价对象整体进行全方位多维度的评价
。相应地
，方面级情感分析


能够为消费者提供每个评价维度的情感极性和统计指标
，帮助消费者更加细致和


理性地进行消费决策
。
同时
，方面级情感分析能够帮助商家从多方面对自身进行


评估
，
并据此进行更合理的精细化运营
，
提高用户黏性
。
总而言之
，
方面级情感


分析具有十分重要的学术意义和广泛的应用价值
。


１
．２
国内外研究现状


１
．２
．
１
文本情感分析


传统的文本情感分析方法包括使用情感词典
［７
］的方法和使用机器学习
［８
］的方


法
。


基于情感词典的方法首先需要构造情感词典
，这是后续开展情感分析的关键
。


１


第
一章
绪论


然后
，利用情感词典获取文本中词语的情感分值
。接着
，
利用句子中每个词语的


情感分值采用
一定的算法计算得到整个句子的情感分值和情感极性
。然而
，这种


方法需要先构造出高质量的情感词典
，词典质量的好坏决定了情感分析的准确性
。


但是
，
语言发展迅速
，
新词层出不穷
，
而构造好的词典更新频率相对较低
，
因而


难以处理新词对分类结果的影响
。


基于机器学习的方法
，
一般需要经过数据预处理
、特征提取
、特征选择
、
分


类器学习和预测情感极性几大步骤
，通过对大量数据进行训练得到分类模型用于


预测文本数据的情感极性
。其中
，
从文本中提取构造出具有情感语义的特征是问


题的关键和核心
。


随着深度学习ｍ的蓬勃发展
，
神经网络开始在各领域中得到广泛应用
。
相比


于传统的机器学习方法
，深度学习将人工从特征工程中解放出来
。
同时
，深度学


习表现出了更好的适用性
，
在各个领域中的表现超过了传统的机器学习方法
。


在基于深度学习方法的自然语言处理任务中
，把词语表示为
一个低维稠密向


量是面临的第
一个挑战
。
词向量技术
［
１Ｇ］的发展推动了深度学习在自然语言处理


领域中应用
，它将文本中以字符形式存在的词表示为
一个低维的稠密向量
，这解


决了传统文本表示模型中向量维度高且稀疏
、
词与词之间关系无法度量的问题
。


针对特定任务
，
采用合适的神经网络结构提取特征是深度学习的另
一个挑战
。


Ｋ
ｉｍ使用卷积神经网络
（Ｃｏｎｖｏ
ｌｕｔｉｏｎａｌＮｅｕｒａｌＮｅｔｗｏｒｋ
，
ＣＮＮ
）对文本进行分


类
，开创了ＣＮＮ在自然语言处理任务应用的先河
。接着
，ＸｕｅＭ等人将基于ＣＮＮ


的模型用于细粒度情感分析
，
取得让人满意的结果
。
ＣＮＮ可以利用
一维卷积核


在
一维的序列数据上捕获序列的局部窗口ｎ
－
ｇｒａｍ特征
＜＝然而
，
ＣＮＮ的结构决定


了其无法处理变长文本
，无法感知词的相对位置关系
，难以建模文本的长期的语


义关系
。
因此
，
它不是天然地适合处理文本情感分析任务
＝然而
，
循环神经网络


（ＲｅｃｕｒｒｅｎｔＮｅｕｒａｌＮｅｔｗｏｒｋ
，ＲＮＮ
）
的模型结构使得其天然地适合编码文本这类


序列数据
，但ＲＮＮ在训练时依然存在梯度爆炸和梯度消失等问题
。
ＲＮＮ模型是


一个变体
一长短期记忆网络
（ＬｏｎｇＳｈｏｒｔＴｅｒｍＭｅｍｏｒｙＮｅｔｗｏｒｋ
，ＬＳＴＭ
）解决了


上述问题
，
被广泛应用于情感分析任务之中
。


１
．２
．２文本方面级情感分析


随着深度学习在自然语言处理上的广泛应用
，越来越多的深度学习模型开始


应用于情感分析任务
。方面级情感分析作为情感分析领域中的
一个子领域
，
也受


到了越来越多的关注
。


对于方面级情感分析
［
１３
］（ＡｓｐｅｃｔＢａｓｅｄＳｅｎｔｉｍｅｎｔＡｎａｌｙｓｉｓ
，ＡＢＳＡ
）来说
，
方


面类别
（ａｓｐｅｃｔｃａｔｅｇｏｒｙ
）
、
方面词
（ａｓｐｅｃｔ
ｔｅｒｍ
）
、
观点词
（ｏｐ
ｉｎｉｏｎ
ｔｅｒｍ
）
和情感


极性
（
ｓｅｎｔｉｍｅｎｔｐｏ
ｌａｒｉｔｙ
）这四个元素的组合构成了方面级情感分析的各种任务
，


北京邮电大学工程硕士学位论文


是方面级情感分析的研究关键
。本文主要围绕ＡＢＳＡ中的方面词情感分析（ＡＴＳＡ
，


ＡｓｐｅｃｔＴｅｒｍＳｅｎｔ
ｉｍｅｎｔＡｎａｌｙｓ
ｉｓ）
任务和方面类别情感分析
（ＡＣＳＡ
，
Ａｓｐｅｃｔ


ＣａｔｅｇｏｒｙＳｅｎｔｉｍｅｎｔＡｎａ
ｌｙｓ
ｉｓ
）任务展开研宄


方面词情感分析
，
旨在预测句子中显式出现的
“ 方面词
” 的情感极性
。给定


句子ｓ＝和方面词ａ＝
｛＾
，七
，
．
．
．
，
＾＾｝
．其中方面词ａ是句子ｓ的子


句
，
模型需要预测方面词
ａ的情感极性
。
其中
，
情感极性
ｐｏＺａｒｉｔｙ
ｅ


｛ｎｅｇａｔｉｖｅ
，ｎｅｗｔｍＺ
．ｐｏｓｉｄｙｅ
｝
。
如图１
－２所示
，
在例句
“
Ｔｈｅｐ
丨ｚｚａ
ｉｓｇｏｏｄ
．ｂｕｔ
ｔｈｅ


ｗａ
ｉｔｅｒ
ｉｓ
ｒｕｄｅ
．
”
中
，
ＡＴＳＡ任务的目标是预测出方面词
“
ｐ
ｉｚｚａ
” 和
“
ｗａ
ｉｔｅｒ
” 的情


感极性
。
模型需要捕捉到句子中
“
ｐ
ｉｚｚａ
” 对应的观点表达词
“
ｇｏｏｄ
”
，
从而判断


出
“
ｐ
ｉｚｚａ
” 的情感极性为正向极性
。
同理
，
方面词
“
ｗａｉｔｅｒ
”
的情感极性为负向


极性
。


Ｔｈｅｐ
ｉｚｚａ
ｉｓｇｏｏｄ
，ｂｕｔｔｈｅｗａ
ｉｔｅｒ
ｉｓｒｕｄｅ
．


Ａｓｐｅｃｔｔｅｒｍ
：ｐ
ｉｚｚａｗａ
ｉｔｅｒ


，

＇ＪＬｒ
Ｔ


／
？

Ａｓｐｅｃｔｃａｔｅｇｏｒｙ
：ｆ
ｏｏｄｓｅｒｖ
ｉｃｅ


＿
＿
＿— 
；ＡＴＳＡ
：｛ｐ
ｉｚｚａ
：ｐｏｓ
ｉｔ
ｉｖｅ
｝
，｛ｗａ
ｉｔｅｒ
：ｎｅｇａｔ
ｉｖｅ｝
；


＼















？
？


；ＡＣＳＡ
：｛ｆｏｏｄ
：ｐｏｓ
ｉｔ
ｉｖｅ｝
，｛ｓｅｒｖ
ｉｃｅ
：ｎｅｇａｔ
ｉｖｅ
｝
；


＼
＿＿＿
＿
＿
＿
＿
＿
＿


图
１
－２ＡＴＳＡ任务和ＡＣＳＡ任务对比


方面类别情感分析
，
旨在预测数据集中预先定义好的方面类别的情感极性
，


方面类别并不
一定显示地出现在句子中
。
给定句子Ｓ＝及数据


集中预先定义好的ｍ个方面类别Ｃ＝
｛Ｃ１
＞Ｃ２
Ｃｍ
；｝
，
模型需要预测出每个方面类


别
￡＾
？的情感极性
。
其中
，
情感极性ｐｏ〖ａｒｉｔｙ
Ｇ
｛ｎｅｇａｔｉｙｅ
，ｎｅｕｔｒａ／
，ｐｏｓｉｔＺｉ；ｅ
｝
。
如


图１
－２所不
，
在例句
“
Ｔｈｅ
ｐ
ｉｚｚａｉｓｇｏｏｄ
，ｂｕｔｔｈｅｗａ
ｉｔｅｒｉｓｒｕｄｅ
．
”
中
，
ＡＣＳＡ任务的


目标是预测方面类别
“ ｆ
ｏｏｄ
” 和
“
ｓｅｒｖ
ｉｃｅ
” 的情感极性
，
而这两个方面类别并没


有显式地出现在句子之中
。
然而
，
方面类别
“
ｆｏｏｄ
” 与句子中出现的
“
ｐ
ｉｚｚａ
” 却


有着上下位概念上的语义关系
，
同理方面类别
“
ｓｅｒｖ
ｉｃｅ
” 与句子中的
“
ｗａ
ｉｔｅｒ
” 也


存在类似关系
。
因此
，
想要正确预测出方面类别
“
ｆｏｏｄ
” 和
“
ｓｅｒｖ
ｉｃｅ
”
的情感极


性
，必须捕获到该方面类别与句子中出现的具体的下位概念之间的语义关系
，
从


而由具体的下位词的情感极性推理出方面类别的情感极性
。总而言之
，无论句子


中显式或隐式地提及某个预定义好的方面类别
，模型都需要对方面类别进行情感


４


第
一章
绪论


极性预测
。
因此
，
ＡＣＳＡ任务比ＡＴＳＡ任务在工业界有着更广泛的应用
。


对于ＡＴＳＡ任务而言
，
建模方面词与上下文之间的语义联系
，
捕获对应的观


点表达部分是问题的核心
。近年来
，越来越多的模型使用注意力机制来建模词与


词之间的语义关系
，尤其是通过注意力机制来捕获句子中方面词与对应观点表达


部分的语义关系
。
Ｗａｎｇ
［Ｒ
｜等人提出了ＡＴＡＥ
－ＬＳＴＭ模型
，
最早将注意力机制引


入细粒度情感分析任务
，该模型将方面词或方面类别的表示和经过编码的词向量


表示进行拼接
，通过注意力机制加权聚合得到带有情感的方面表示
。为了更好地


生成方面词
，
Ｍａ
［
１Ｋ等人提出了ＩｎｔｅｒａｃｔｉｖｅＡｔｅｎｔ
ｉｏｎＮｅｔｗｏｒｋ
，
分别对句子和方面


词进行编码
，
然后利用注意力机制对句子和方面词进行交互
。
由于单
一的


Ａｔｅｎｔｉｏｎ机制不能较好的捕获远距离的信息特征
，
Ｃｈｅｎ
［
１６
］等人提出了ＲＡＭ模


型
，通过引入多级别注意力来计算长距离的特征信息
。
由于方面词通常由多个词


语组成
，
在粗粒度注意力机制的基础上
，
Ｆａｎ
ｔｎ
］等人提出了ＭＧＡＮ
，
引入了多粒


度注意力机制
，
更好地建模每个词语与上下文间的关系
。


为了缓解序列学习模型中长距离依赖问题
，
同时减缓注意力机制引入的噪声
，


一些方法将基于依存句法分析的句法依存树引入到模型中
，通过句法解析结果来


缩短句子中词与词之间的距离同时避免噪声
。这些模型将依存句法解析的结果转


化为图结构
，通过图卷积神经网络
［
１８１（ＧｒａｐｈＣｏｎｖｏ
ｌｕｔｉｏｎａｌＮｅｕｒａｌＮｅｔｗｏｒｋ
）的传


播聚合
，
来更新方面词的表示
＝
比如
，
Ｚｈａｎｇ
［
１９＾人提出ＡＳＧＣＮ模型
，
将句法


依存树引入细粒度情感分析任务中来
，显式地利用依存句法分析结果中词与词的


连接关系
，建立了方面词与情感词直接的联系
。
５１！！１
［２
（）
］等人提出
ＣＤＴ模型
，
同样


利用句法依存树来学习句子的句法结构信息
。
ＨｕａｎｇＰ
ｌ等人提出使用图注意力网


络结合句法依存树对句子进行建模
。随后
，ＷａｎｇＰ
ｌ等人提出模型ＲＧＡＴ
，对句法


依存树进行改造
，构建出以方面词作为根节点的树结构
，结合图注意力网络对句


子进行上下文建模
＝考虑到句法结构和语义相关性之间的互补性
，
ＩＪＭ等人提出


Ｄｕａｌ
－ＧＣＮ模型
，用两个图卷积网络分别来处理句法信息和语义信息
。
１＾＾＾
［２４
］等


人提出
Ｂ
ｉＳｙｎ
－ＧＡＴ＋模型
，
该模型不再使用句法依存树学习句子的语法信息
，
而


是通过短语结构树学习与每个方面词相关的上下文语义信息
。
同时
，构建
一套规


则来学习多个方面词之间的关系
。


对于ＡＣＳＡ任务而言
，方面类别可能不会显式地出现在句子之中
，
该任务预


定义了多个不同的方面类别
＝对于
一个给定的句子而言
，不同的方面类别可能有


着相反的情感极性
。建模方面类别与句子中相关概念词的语义关系
，通过句中下


位概念词对应的观点表达部分
，推理出方面类别的情感极性是解决该问题的核心
。


Ｗａｎｇ
［
１４
］等人提出了ＡＴＡＥ
－ＬＳＴＭ模型
，
最早将注意力机制引入方面类别情感分


析任务
，通过词的不同的注意力权重
，编码与方面类别相关的句子表示
。
为了学


５


北京邮电大学工程硕士学位论文


习方面类别与上下文之间的语义关系
，
Ｈｕ
［２５
］等人通过引入辅助任务方面类别检


测
（ＡｓｐｅｃｔＣａｔｅｇｏｒｙＤｅｔｅｃｔｉｏｎ
，ＡＣＤ）
，
利用与ＡＣＳＡ任务的相关性以及正则化损


失
，
对两个任务同时进行学习
，
使得每个方面类别关注句子中不同的单词
。
Ｌｉ
［２６
Ｉ


等人提出了ＡＣ
－ＭＩＭＬＬＮ模型
，
利用多实例多标记学习网络框架
，
该模型将句


子作为包
，单词作为包中实例
，将句子中表示方面类别的单词作为指示方面类别


的关键实例
。该模型首先找到文本里指示方面类别的词
，然后对指示词进行情感


分类
，
最终聚合指示词的情感得到方面类别的情感
。


越来越多的研究发现
，
对于很多自然语言处理任务而言
，
仅仅依靠句子内部


信息编码得到的特征无法解决特定的问题
。于是
，
一些研究开始利用外部知识库


对特定任务
，
甚至预训练语言模型进行不同方式的知识增强
。
Ｚｈｏｕ
［２７
】等人将


ＳｅｎｔｉｃＮｅｔ作为常识知识库引入到方面词情感分析任务中
，
通过图卷积神经网络


同时融合语法与知识信息
，
对句法依存矩阵进行知识增强
。
Ｚｈａｎｇ
ｔ％等人将知识


图谱的结构化信息引入预训练语言模型Ｂｅｒｔ
，对模型进行知识实体层面的掩盖处


理
，
使模型学到更多的语义知识
。


对于ＡＴＳＡ任务而言
，
Ｌ
ｉａｎｇ
ｔ２９
］等人提出
Ｓｅｎｔｉｃ
－ＧＣＮ模型
，
在利用句法依存


树的基础上引入了ＳｅｎｔｉｃＮｅｔ
，
利用
ＳｅｎｔｉｃＮｅｔ中的情感信息对句法依存树做情感


语义的增强
。对于ＡＣＳＡ任务中方面类别不显式出现在句子中的情况而言
，通过


句子内部的结构和语义挖掘方面类别词和句中相关词之间的语义关系十分困难
。


Ｌｉａｎｇ＾等人提出ＡＡＧＣＮ模型
，利用外部知识库中先验Ｂｅｔａ分布的统计特征来


计算其句子中词语对方面类别的重要性
，
从而达到知识增强的目的
。


综上所述
，
对于方面词情感分析和方面类别情感分析两个任务而言
，
如何同


时利用好语法和语义信息
，更好地利用外部知识对语法和语义信息进行知识增强


仍然面临挑战
。


１
．３
主要研究内容


１
．３
．
１
现存问题


针对方面词情感分析任务而言
，
目前存在以下问题
：


１
）语法通道对外部知识利用不足


对于长文本评论数据而言
，通过句法依存树捕获句子的语法结构信息十分重


要
。然而
，
依存解析结果的误差常常会导致在信息聚合过程引入噪声
。
同时
，
单


一的句法依存树也无法捕获句子中带有
“ 情感
” 极性的词语和含有
“ 否定
” 语义


的词语
，而这些词语往往决定了评价对象的情感极性
＝如何利用外部常识知识对


语法结构信息进行知识增强和综合利用
，
是当前语法通道面临的问题
。


６


第
一章
绪论


２
）语义通道对外部知识利用不足


口语化的短文本评论数据而言
，
这些短文本对语法结构不敏感
，
依存句法解


析不能很好地捕捉句子句法结构
，从而影响模型获取语义特征
。
因此
，
一些模型


常常利用自注意力机制来建模词语之间的语义关系
。然而
，
自注意力机制的核心


是计算词语之间的语义相似度
，当前模型忽略了外部知识对语义相似信息的增强


作用
。在外部知识库中
，
语义相似的词语往往在知识空间中距离更近
。如何利用


外部知识库的先验相似度知识
，对文本进行结构化的语义增强
，是当前语义通道


面临的问题
。


针对方面类别情感分析任务而言
，
目前存在以下问题
：


１
）对方面类别与句中相关概念词语义关系的捕获和利用不足


方面类别往往不显式地出现在句子之中
，是
一个预先定义好的概念
。建模方


面类别与句子中下位词之间的关系才能进
一步地做出正确的情感极性预测
。然而
，


现有的研宄利用句子内部的结构信息建模上述关系
，这种方法复杂且困难
。如何


利用外部知识增强和扩展方面类别的表示
，从而更好地建模方面类别与句子中下


位词之间的关系
，
是当前面临的问题
。


２
）对语义和语法信息互补性的建模和利用不足


对于长文本评论数据而言
，
需要句法依存树来捕获句子的语法信息
。对于短


文本评论而言
，需要自注意力机制来捕获句子的语义信息
。单独利用句法依存树


或者自注意力机制不能捕获到语法和语义的综合特征
。如何综合建模和利用语义


信息和语法信息的互补性
，
是当前面临的问题
。


１
．３
．２研究内容


本文针对方面词情感分析和方面类别情感分析两个任务进行研宂
。具体研究


内容如下
：


１
）针对方面词情感分析任务
，
提出
一种情感感知的双通道图卷积神经网络


模型
，
其中双通道分别为语法通道和语义通道
。在语法通道中
，
本文首先利用依


存句法分析方法捕获句子的语法结构
，
得到原始的句法依存树矩阵
，
然后利用


ＳｅｎｔｉｃＮｅｔ中词的情感数值构建句子的情感值矩阵
，以对原始矩阵增强情感知识
，


另外使用词性知识构建词性知识矩阵
，对句法依存树矩阵进
一步增强
；在语义通


道中
，
本文将基于
Ｃｏｎｃｅｐ
ｔＮｅｔ知识库训练的词向量与经过模型编码的隐向量融


合
，
对词语进行知识语义增强
。最终解决了语法
、语义通道对外部知识利用不足


的问题
。


２
）针对方面类别情感分析任务
，提出
一种知识增强的多通道图卷积神经网络


模型
，
其中多通道分别为知识通道
、语法通道和语义通道
。在知识通道中
，
本文


利用基于ＷｏｒｄＮｅｔ的相似度函数计算方面类别与句子上下文之间的语义相似度
，


７


北京邮电大学工程硕士学位论文


进而得到与方面类别相关的相似度矩阵
，解决了方面类别与句中相关概念词关系


的捕获和利用不合理的问题
；在语法和语义通道中
，本文分别使用句法依存分析


和自注意力机制构建对应的邻接矩阵
，设计基于注意力的特征融合模块
，融合语


法和语义通道的特征
，解决了模型对句法结构和语义关系互补性的建模缺失问题
。


３
）设计并实现了
一套面向用户评论的方面级情感分析系统
。该系统的搭建过


程包括需求分析
、功能设计
、功能实现等阶段
。主要功能包括用户管理模块
、模


型管理模块
、情感分析及可视化模块
。用户可以在系统中进行模型训练
，
同时也


可以使用训练好的模型进行方面级情感分析
，
并对结果进行可视化展示
。


１
．４本文结构安排


如本文章节组织结构如图
１
－３所示
，
本文总计包括六个章节
，
每章核心内容


如下
：


第
一章
：
绪论
。
首先介绍了方面级情感分析任务的研宄背景以及该任务的应


用价值与意义
，并对方面级情感分析的国内外研究的现状进行梳理
，分析了当前


模型遇到的问题与不足
，
最后对本文主要的研究内容进行了简要总结
。


第二章
：
背景知识及相关技术
。
简要介绍了本文涉及的基础理论知识和相关


技术
。
首先介绍了词向量技术
，接着介绍了循环神经网络
、
图卷积神经网络
、注


意力机制
。
最后
，
梳理了预训练语言模型和知识表示方法
。


第三章
：
情感感知的双通道图卷积神经网络的方面词情感分析研究
。
首先
，


介绍了本文设计的
一种增强句法依存树的方法
，并介绍了基于图卷积神经网络建


模语法信息的细节
。接着
，
介绍了本文采用的
一种利用知识库增强句子词向量的


方法
，
并介绍了基于图卷积神经网络建模语义信息的细节
。
最终通过实验分析
，


验证了所提出的框架的有效性
。


第四章
：
基于知识增强的多通道图卷积神经网络的方面类别情感分析研宄
。


首先
，介绍了本文基于外部知识库相似度函数设计的
一种用于构建方面类别与句


中词相关性的
一种方法
，
并介绍了基于图卷积神经网络建模知识信息的细节
。接


着
，
介绍了语义通道模块
、语法通道模块以及通道信息融合的细节
。最终通过实


验分析
，
验证了所提出的框架的有效性
。


第五章
：
面向评论的方面级情感分析系统的实现与应用
。
主要介绍了该系统


的需求分析
、功能模块设计
、架构设计及系统实现的整个软件生命周期的具体工


作
。


第六章
：
总结与展望
＝
对本文所做的工作进行了总结
，
展望了今后的发展趋


势
，
提出了下
一步可能的研宄方向
＝


８


第
一章
绪论


第
一童绪论


第二章背景知ｉ只及相关技术


Ｉ＋
￣￣
．
ｊＩ


：第三章情感感知的双通通图卷积第四章基于知识增强的多通道图卷积
：


：坤经网络的方面词情感分析砑究神经网络的方靣类别情感分拆研究
ｉ


ｉＶ
￣Ｖ
－
＼


＼
１
Ｉ
１１
．
；


：
Ｉ
－ｎ
、Ｅ
＇土
、，
｜ｎＷ
、ｓ
｜
问题１
：
对方面
｜
问题２
：
对语义
：


：
ｉＤ
＾｜｜
ｆｓｓｎｍｕ
；


：
｜糊相
｜糊相
丨
利ｉ永足
Ｉ利４不足
：


Ｉ


第五章面向评论的方面级情感分


拆系统设计与实现


— １—


第六童总结与展望


图
１
－３
本论文组织结构


９


北京邮电大学工程硕士学位论文


１０


第二章
背景知识及相关技术


第二章背景知识及相关技术


本文围绕方面级情感分析中方面词情感分析和方面类别情感分析两个任务


的模型研宄与改进
￡基于此
，本章节将介绍本文后续工作将用到的背景知识及相


关技术
，详细介绍了词向量表示技术
、循环神经网络
、图神经网络
、注意力机制
、


预训练语言模型和知识表示技术在内的基础理论和关键技术
。


２
．
１词向量表示技术


２
．
１
，
１Ｗｏｒｄ２Ｖｅｃ


Ｍ
ｉｋｏｌｏｖ等人在
２０
１３年提出了Ｗｏｒｄ２Ｖｅｃ
［３
１＾向量表示技术
。该项技术通过


无监督学习任务
，
从大量文本语料中学习词的连续向量化表示
。该技术的核心思


想是建模目标词与上下文词间的关系
，并提供了两种不同的训练模型
，ＣＢＯＷ和


Ｓｋｉｐ
－Ｇｍｍ
。
如图
２
－
１
所示
，
这两种模型具有相似的网络结构
，
都采用
“ 训练窗


口
” 来确定当前词的上下文语境词
。


ＩＮＰＵＴＰＲＯＪＥＣＴ
ＩＯＮＯＵＴＰＵＴ
ＩＮＰＵＴＰＲＯＪＥＣＴ
ＩＯＮＯＵＴＰＵＴ


ｗ“
－２
）＼
＿
ｗ
（
ｔ
－２
）


＼＼
／ｙ
ｗ
（ｔ
－
ｉ
）


Ｘｖ
Ｊ／
—


＾
？ｗｆｔ
）ｗ
（
ｔ
）
？、


Ａ


难
１
）’／
＾〇
＊
！
）


：／＼
ｗ
（
ｔ＋２
）
’
＼ｗ
（
ｔ＋２
）


图２
－
１ＣＢＯＷ与Ｓｋｉｐ
－Ｇｒａｍ模型
［３
１
］


１
）ＣＢＯＷ（Ｃｏｎｔ
ｉｎｕｏｕｓＢａｇ
ｏｆＷｏｒｄｓ）模型
，
即连续词袋模型
，
旨在通过局部


窗口里的上下文语境词预测出中心词的概率
。如图
２
－
１所示
，输入层为中心词周


围滑动窗口大小范围内的语境词的独热编码
，通过投影层运算及求和运算
，获得


隐层的向量表示
＝
然后经过输出层和
ｓｏｆｔｍａｘ函数得到词袋中每个词的概率
，
其


中概率最大的索引即为预测的中心词
。


在ＣＢＯＷ模型中
，
每个词都会成为待预测的中心词
，
以及上下文语境词
。


所以每个词都会表示成两个ｄ维向量参与运算
。
则已知上下文语境词
，
生成中心


词的条件概率为
：


１
１


北京邮电大学工程硕士学位论文


ｅｘＰ
（去
Ｈ
ｕｏ２ｍ）
ｊ


Ｐ（ｗｃ
｜Ｗ０ｌ
，
．
．
．
，ｗ０２ｍ
）
＝

ｒ（２
－１）


Ｅ
ｉｗｅｘｐ
（況
ｖ＂ｕ０１＋
… ＋ｕ０２ｍ）
）


其中
，
设滑动窗口大小
ｍ
，
ｃ表示中心词的索引
，
＆则表示索引为ｃ的中心词的向


量
ｃ背景词的索引
，
分别表示索引为ＯｉｄＯｚｍ的上下文


语境词向量
。


２
）Ｓｋｉｐ
－Ｇｒ
ａｍ模型的思想是利用给定的中心词去预测它的语境词
，模型如图


所示
。
在中心词确定的情况下
，
生成背景词的条件概率可以表示为
：


，、
ｅｘｐ（ｕ＞ｃ）… 、


Ｐ（ｗ０
｜ｗｃ
）
＝
（２
－２）


ｌ
ｉｅｖｅｘｐ（Ｕ
ｊ
＇
ｌ７
ｃ）


其中
，
Ｃ表示中心词的索引
，
１７
ｅ表示索引为Ｃ的中心词的向量
。
０为背景词的索引
，


Ｕ。表示索引为０的中心词向量
。


２
．
１
．２ＧｌｏＶｅ


Ｗ〇ｒｄ２ＶｅＣ
只考虑了
一定窗口长度的语境信息
，
即只能利用语料库中的部分


信息
。为了获取整个语料库的全局信息
，
Ｐｅｎｎｉｎｇ
ｔｏｎ等人提出了ＧｌｏＶｅ
［３２
］
（Ｇｌｏｂａｌ


ＶｅｃｔｏｒｓｆｏｒＷｏｒｄＲｅｐ
ｒｅｓｅｎｔａｔｉｏｎ
）模型
，它是
一种结合语料库全局信息和局部的上


下文特征的词向量算法
。


Ｇ
ｌｏＶｅ模型通过引入全局共现概率矩阵来融入全局先验统计信息
，
Ｇ
ｌｏＶｅ的


代价函数为
：


；
＝ＶＷｘ（ｖｆ
Ｖ
ｊ＋ｂ
ｔ＋ｂ
ｊ
－
ｌｏｇＣＸ
ｉｊ））
２（２
－３）


其中
，
％
、
４分别表示词语／
、
Ｊ／的词向量
，
表示权重矩阵
。


２
．
１
．３ＥＬＭｏ


在语言学和自然语言处理领域中
，
一词多义是
一个常见的现象
，常常表现为


一个词的语义由其所在的上下文语境决定
。同
一个词处于不同语境下往往有着不


同的语义
。
对于Ｗｏｒｄ２Ｖｅｃ和ＧｌｏＶｅ而言
，
它们都是静态词向量
，
即在不同语境


下具有统
一的语义
。而且这些词向量都是由
一层神经网路训练得到
，抽象程度和


表达能力并不高
。
因此
，
Ｐｅｔｅｒｓ等人在
２０
１８年提出了与上下文相关的词向量技


术ＥＬＭｏ
［３３
］（ＥｍｂｅｄｄｉｎｇｓｆｒｏｍＬａｎｇｕａｇｅＭｏｄｅｌｓ
）
。
如图２
－２所不
，
它是
一个基于


长短期记忆网络
（ＬＳＴＭ
）
的两层模型结构
，
并且每
一层包含
一个基于ＬＳＴＭ的


前向语言模型和
一个基于ＬＳＴＭ的后向语言模型
。


１２


第二章
背景知识及相关技术


ｈ｝（
Ｔ２）
…
（
Ｔｋ）


１
—＾
Ｌｓｌｒ
ｏ
＾
）

ｊ
１
｜


ｉ
ｉ
ｉ
ｉ


｜（＾
ｓｌｍｙ
－＞
（＾
Ｌｓｉｍ＼＾
—？ｗ－ｓｔｎｙ
｜
｜
？？
－
＾
ＬｓｉｎＴ
）＼


Ｅ
！Ｈ２…Ｅ３


图
２
－２ＥＬＭｏ模型结构


模型的优化目标是前向模型与后向模型的联合最大似然
：


Ｎ


＾ＬＭ
＝
〉
ｌ〇ｇｐ（
ｔｋ｜
ｔ
ｌ
ｆｔ２
，
．
．
．
，ｔ＾ｉ
，６Ｘ
，〇ｉＳＴＭ
？）＋ｒｎ


ｉ＝ｌ
（
）


ｌｏｇｐ（
ｔ
ｉ
｜
ｔ
ｊ
＋
１
，ｔ
ｉ＋２
＞
？
■
？／＾Ｎ
＞＾Ｘ
＞＾ＬＳＴＭ
＞Ｓｓ）


其中
，
Ａ表示后向
ＬＳＴＭ
与前向
ＬＳＴＭ结合后的隐层参数
；
＆ＳＴＭ表示前向


ＬＳＴＭ隐层参数
；
＆ｓ
：ｒＭ表示后向
ＬＳＴＭ
层参数
；
０Ｓ表示
ｓｏｆｔｍａｘ层参数
；


对于长度为Ａ／的输入序列（ｂｈ
，
．
．
．
，？）
，根据前ｋ
－１个时刻的文本序列
，预测


第ｆ
ｃ个时刻“ 的出现概率
。
前向语言模型的概率如公式
２
＿５所示
：


Ｎ


ｐ（ｔ
ｌ
ｆ
ｔ２
ｆ
＾
ｔ
ｔＮ）＝
￣
［
ｐ（
ｔｆ
ｃＵｉ
，
ｔ２
．
－＾ｆ
ｅ
－ｉ
）（２
－５）


ｋ＝ｌ


同理
，
后向语言模型的概率如公式
２
－６所示
：


Ｎ


Ｐ（亡１
＇
亡２
，
…
，￣）
＝
￣
Ｊ
Ｐ（Ｇ
丨Ｇ＋１
山＋２
，
…
？
）（２
－６）


ｋ＝ｌ


２
．２循环神经网络


序列数据在自然语言处理任务重随处可见
，
如双语翻译
、文字序列转语音序


列等
。
序列数据通常是连续的且长度不固定的
，
通常符合时间顺序或逻辑顺序
，


在时间或逻辑上具有明显的先后关系
。序列数据中蕴藏着十分重要的上下文关系
，


换句话说
，
长距离样本之间具有关联关系
。


２
．２
．
１
循环神经网络


循环神经网络（ＲｅｃｕｒｒｅｎｔＮｅｕｒａ
ｌＮｅｔｗｏｒｋ
，ＲＮＮ
）
ｂ４
］是
一
■种用于处理各种序列


数据神经网络
，
在自然语言处理任务中
，
通常用来对文本进行建模
：循环神经网


１３


北京邮电大学工程硕士学位论文


络使用了循环重复的网络结构
，可以看作
一个时间链
。时间链上的节点存在依赖


关系
。
当前时刻的计算需要当前时刻的输入和上
一时刻节点的输出
。
因此
，
循环


神经网络不仅能够建模长距离节点之间的依赖关系
，还能够保存序列中压缩后的


前文信息
。
ＲＮＮ的结构如图
２
－３所示
。


ｈｔ


管
ｙ
｛ｈ〇
１ｈｉｆｔ】ｈ２


— ＂
＾
＂ｔ丫
、
ｒｒ


－？Ａ
—＝Ａ
—？Ａ
— ？Ａ？Ａ


ｔ１ｔｉ
．１


４）＾
－改


图
２
－３ＲＮＮ模型结构


ＲＮＮ将输入序列编码成
一个固定维度的隐层状态／ｌ
。
ｔ时刻编码后的状态／ｉ
ｔ


由前
一时刻编码后的状态／ｌ
ｔｄ和当前时刻的输入
共同经过计算得到
。


假设Ｘ＝＆）是输入序列
，
则与其对应时刻ｔ的编码后状态计算公


式如下
：


ｈ
ｔ
＝
ｆ（Ｕｘ
ｔ＋Ｗｈ
ｔ＾＋ｂ）（２
－７）


其中
，
ｔ代表时刻
，
［／
、
Ｗ代表权重矩阵
，
６代表偏置项
。


由于状态之间具有依赖关系
，
因此ＲＮＮ
在处理序列数据时
，
不能够进行并


行计算
，
因此速度比较慢
。此外
，
随着序列长度增加
，
前文信息会有
一定程度的


损失
，
当前时刻很难获得长距离之前的信息
，
长距离的序列信息捕捉能力有限
。


在训练时
，
ＲＮＮ可能会出现梯度爆炸抑或梯度消失的问题
。


２
．２
．２长短期记忆网络


为了解决梯度爆炸或梯度消失的问题
，门控机制被提出并用于改进循环神经


网络的节点的内部结构
。如图
２
－４所示
，
ＬＳＴＭ
［３５
］就是其中最著名的
一种带有门


控机制的循环神经网络
。
ＬＳＴＭ利用三个不同的门控机制
，
可以选择输入
，
存储


以及输出的信息
。
ＬＳＴＭ的结构如图
２
－４所示
。


ｘＶ


一一
等＾
＂


ＡＪｒ
－Ａ


＜７ｔａｒ＊ｔｒ


ｒ
￣ｒ
￣
ｒ
￣
ｒ
￣￣
ｔ
￣
ｌ＿ｊ＿．


ｔ
ｆ
１


无々Ｘ＊
ＵＰ
＊
．
？


图
２
－４ＬＳＴＭ模型结构


设
ｔ时刻输入为
隐状态表示为
／ｉ
ｔｙ
，
额外引入的细胞状态
（Ｃｅ
ｌ
ｌＳｔａｔｅ
）


为
Ｃ
ｔ＾
，
ＬＳＴＭ的前向过程如下所示
：


１４


第二章
背景知识及相关技术


ｒ
ｆｔ
＝＋Ｕ
ｆｈ＾ｉ＋ｂ
ｆ）


ｉ
ｔ
＝ａ（Ｗ
ｔＸ
ｔ＋＋ｂ
ｉ）


，
〇
ｔ
＝＾（Ｗｏ＾
ｔ＋Ｕ０ｈ
ｔ
－
－
ｉ＋ｂ０）（２
－８）


ｃｔ
＝ｔａｎｈ（Ｍ／
ｃｘ
ｔ＋＋ｂ〇）


ｃ
ｔ
＝
ｆｔ〇ｃ
ｔ
－
ｉ＋ｉ
ｔＱ
ｃ
ｔ


＜ｈ
ｔ
＝ｏ
ｔＱｔａｎｈ（ｃ
ｔ）


其中
，
队和队均为可学习的参数矩阵
，
汉均为可学习的偏置向量
，
〇代表逐元


素乘积
。
尺为遗忘门
，
为输入门
，
〇
￡为输出门
。遗忘门为
一个在
０到
１之间的


数值
，用于选择性遗忘细胞状态中保留的信息
，决定保留或忘细胞状态
中的信


息
。输入门为
一个在
０到
丨之间的数值
，
用于将新的信息选择性的记录到细胞状


态中
，
决定更新什么数值
。输出门为
一个在
０到
１之间的数值
，
用于确定细胞状


态的哪个部分将输出
。


２
．３
图神经网络


卷积神经网络在网格型数据上表现优异
，解决了现实生活中图像视频领域的


一系列问题
。循环神经网络则在拟合序列数据上更胜
一筹
，解决了学术界和工业


界诸多的文本
，
语音
，
行为序列等领域问题
。与这些规则的数据结构相比
，
图数


据是非欧式空间数据
，数据结构的定义和描述更接近现实生活中广泛存在的数据


组织形式
，
如社交网络
，
分子结构等
。


图是
一种由顶点和边两部分构成的数据结构
，
可以表示为Ｇ＝Ｅｈ
其中


Ｋ表示图中节点的集合
，
￡
■表示图中边的集合
，
ｅ
ｉ；
？表示了图中从节点
ｉ到节点ｙ的边
。


图中节点的连接关系可以通过矩阵进行表示
，
称为邻接矩阵九
可以按照如下公


式表不
：


＿（
ｌ
，
如果Ｖ
ｉ
，巧有边连接


〖
，广ｋ
其他
（°


对于图类型数据结构而言
，可以直接使用图神经网络（ＧｒａｐｈＮｅｕｒａｌＮｅｔｗｏｒｋ
，


ＧＮＮ
）对其进行建模
，利用图神经网络进行图表示学习
，不仅可以学习到图中的


节点特征
，还能进
一步学习到图中的空间结构信息
。基于图神经网络的图表示学


习旨在提取节点
、
边以及子图的特征
，
为它们生成低维向量
＝


图神经网络有很多类型
，其中
一种是图卷积神经网络
［
１
８
］
（ＧｒａｐｈＣｏｎｖｏ
ｌｕｔｉｏｎａｌ


Ｎｅｔｗｏｒｋ
，ＧＣＮ
）
。
图卷积是将卷积操作迁移使用在图结构的数据上
，通过卷积的


方式获取每个节点的邻居节点的信息
，
即每个节点的空间特征
，最终来更新图中


每个节点或图的向量表示
。


１
５


北京邮电大学工程硕士学位论文


ｃ（
ｐ
＇
Ｆ
，
〒
，二？


－＾
■２
■
—
－
—

＾２


＾ｒ＼
ｒｗｓ
ｎ（＾＼


＠
一匆
？卜⑨


ｓＷＩ


ｉｎｐｕｔｌａｙｅｒｏｕｔｐｕｔｌａｙｅｒ


图
２
－５
图卷枳神经网络
［
１８
］


在ＧＣＮ学习过程中
，
不仅要为模型输入每个节点的特征向量表示
，
还要输


入节点之间的连接关系
，
即邻接矩阵
。
如图
２
－５所示
，
对于堆叠多层的ＧＣＮ模


型
，
每层的映射关系表示如下
：


Ｈ
ｌ＋１＝ｆ（Ｈ
ｌ
，Ａ）
（２
－１０）


ＧＣＮ在学习过程中
，
可以简单地通过将邻接矩阵和节点特征相乘
，
来获取


并聚合节点的邻接节点信息
，
如公式所示
：


＝ａ（ＡＨ
ｌＷ
ｌ
）
（２
－１１）


其中
，
是第
／层神经网络的权重矩阵
，
ａ是非线性的激活函数
＝


进
一步地
，
通过加入节点度的对角矩阵
，
对上述模型中邻接矩阵Ａ进行归
一


化操作
，
如下所示
：


＝ｏ
（ｄ
￣
２ＡＤ
￣
Ｈ
ｌＷ
１
＾
（２
－１２）


其中
，
又称为图的拉普拉斯矩阵
。


２
．４注意力机制


Ｂａｈｄａｎａｕ
１
３６
１等人在机器翻译任务上使用了注意力
（Ａｔｅｎｔ
ｉｏｎ
）机制
，
第
一个


将该机制应用到
ＮＬＰ领域中来
。本节简要介绍软注意力
（Ｓｏｆ
ｔＡｔｅｎｔｉｏｎ
）
、硬注


意力
（ＨａｒｄＡｔｅｎｔｉｏｎ
）和自注意力
（Ｓｅｌｆ
－Ａｔｅｎｔｉｏｎ）
［３７
］
。


２
．４
．
１
软注意力


软注意力机制
，指对模型的各个输入项都有关注
，
即对各个输入项的权重设


定在
０
￣
１
之间
。换句话说
，该机制关注输入的全局信息
，对某些部分关注的多
，


对某些部分关注的少
。软注意力机制通过注意力分布来加权求和融合各个输入向


量
，
计算公式如下
：


１６


第二章
背景知识及相关技术


ａ
ｉ
＝
ｓｏｆ
ｔｍａｘ
（５ｃｏｒｅ（＾，））
＝
ｅｘｐ（了
（２
．１３）


Ｅ
；＝１ｅｘｐ
＾
ｓｃｏｒｅ（ｘ
ｊ
，ｑ）ｊ


软注意力机制的核心是各种打分函数
，打分函数计算方式主要包括加性算法
、


点积算法
、
缩放点积算和双线性算法
。


加性算法计算公式如下
：


ｓｃｏｒｅ（
＿ｘ
，ｑ）
＝Ｖ
Ｔ
ｔａｎｈ（Ｗｘ＋Ｕｑ）（２
－１４）


点积算法计算公式如下
：


ｓｃｏｒｅ（ｘ
，ｑ）
＝ｘ
Ｔ
ｑ
（２
－１５）


缩放点积算法对注意力分布进行平滑处理
，
计算公式如下
：


ｘ
Ｔ
ｑ


ｓｃｏｒｅ｛ｘ
，ｑ）＝—＝（２
－１６）


ｖｄ


双线性算法是
一种泛化点积模型
，在计算时引入了非对称性
，计算公式如下
：


ｓｃｏｒｅ｛ｘ
，ｑ）
＝
（Ｕｘ）
Ｔ
（Ｖｑ）（２
－１７）


在上述公式中
，
ｄ是输入向量的维度
，
Ｖ
、
Ｕ和Ｋ是可学习的参数
。


２
．４
．２硬注意力


硬注意力机制
，
又称为局部注意力
（ＬｏｃａｌＡｔｅｎｔｉｏｎ
）
，
该机制根据注意力分


布选择输入向量中的某个特定输入项作为输出
。
因此
，给模型的各个输入项分配


的注意力权重中只有
一项权重为
］
，
其余项权重都是
０
。
通常
，
硬注意力的输出


可以采用注意力得分最高项对应的向量
，也可以随机采样注意力分布的其中
一项


对应的向量
。硬性注意力的优点是可以降低
一定的时间开销和计算成本
，但缺点


就是可能会忽略掉某些本应被重视的信息
。


２Ａ３
自注意力


自注意力机制＾通过计算输入项内部的相关性
，经过输入项内部
“ 投票
” 得


到不同部分的不同重要程度
，
从而对模型的各个输入项赋予不同的注意力权重
。


在自注意力机制中
，查询项可以由输入信息本身经过计算后得到
。模型读到输入


信息后
，
根据输入信息本身决定当前最重要的信息
。


如图
２
－６所示
，自注意力机制往往采用查询项
－键项
－值项（Ｑｕｅｉｙ
－Ｋｅｙ
－Ｖａｌｕｅ
）


的模式
，
且
（？
，
１／为相同的输入特征
，
计算公式如下
：


Ａｔｔｅｎｔｉｏｎ＾Ｑ
，Ｋ
，Ｖ）＝Ｓｏｆｔｍａｘ＾
（２
－１８）


１７


北京邮电大学工程硕士学位论文


ｔ


ＭａｔＭｕ
ｌ


ｆｔ


ＳｏｆｔＭａｘ｜


寺


Ｍａｓｋ
（ｏｐｔ
．）


？


Ｓｃａ
ｌｅ


Ｉ


ＭａｔＭｕ
ｌ


ｉ
￣
ｒ


〇ＫＶ


图
２
－６
注意力点积机制
【３７
］


其中
，
Ｋ
ＧＭ
ｉｘ办和
］／
￡妒
＞＜４分别代表查询项
、
项以及值项
。
［为


序列长度
，
七和七分别为查询向量和键值向量的维度
。
自注意力机制在处理长


序列输入时
，
具备并行计算的能力
。


２
．５预训练语言模型


预训练语言模型
（Ｐｒｅｔｒａ
ｉｎｅｄＬａｎｇｕａｇｅＭｏｄｅ
ｌｓ
，ＰＴＭｓ
）
成为近年来人工智能


以及自然语言处理领域技术发展史上的
一个突破
。训练语言模型的过程无需人工


标注好的数据
，
是
一个无监督的学习过程
。


预训练语言模型通过特定的语言任务对语言模型进行训练
，可以有效地从大


量的数据中获取知识
，并将学习到的知识存储到大量的参数中
。这些参数可以作


为下游任务模型的初始化参数
，然后在预训练好的大模型之上针对特定下游任务


对初始化参数进行微调
－
这直接催生出
“ 预训练
－微调
” 范式
。


ＧＰＴ＾就是典型的自回归语言模型
。通常用于自然语言生成任务
，
例如
：
机


器翻译
、基于生成式的文本摘要等
。
自编码语言模型则是利用上下文语境信息来


预测中心词出现的概率
。
ＢＥＲＴＰＩ就是经典的自编码语言模型
，
对输入中的部分


词进行随机替换
，
然后根据上下文信息预测
［ＭＡＳＫ
］位置处的词
。
由于自编码语


言模型可以同时利用上下文信息
，
通过在自然语言理解任务表现优异
。


２
．５
．
１Ｔｒａｎｓｆｏｒｍｅｒ


Ｖａｓｗａｎ
ｉＭ等人于
２０
１
７年提出了新
一代神经网络模型
Ｔｒａｎｓｆｏｒｍｅｒ
。
该模型


包含
一套完整的编码器和解码器
。
为解决循环神经网络ＲＮＮ因在计算时依赖前


一时刻的输出而无法进行并行化计算
，
以及梯度消失和爆炸等问题
，
Ｔｒａｎｓｆｏｒｍｅｒ


１８


第二章
背景知识及相关技术


使用多头注意力机制来建模和编码序列中词与词之间的语义关系
。这种结构使得


模型在训练阶段能够进行并行计算
，
极大的降低了训练复杂度和时间
。
因而
，
建


模文本等序列化数据越来越多地使用到Ｔｒａｎｓｆｏｒｍｅｒ
。


Ｏｕｔｐｕｔ


Ｐｒｏｂａｂ
ｉ
ｌ
ｉｔ
ｉｅｓ


ｔ


１Ｓｏｆｔｍａｘ１


ｔ


ＩＵｎｅａｒ
１


Ａｄｄ＆Ｎｏｒｍ


Ｆｅｅｄ


Ｆｏｒｗａｒｄ


（
—
＿１＝＾


／
■
Ｉ飞Ａｄｄ＆Ｎｏｒｍ


ｐ｜
Ａｄｄ
＆
．
Ｎ〇
ｒｍ
；



ＦｅｅｄＡｔｔｅｎｔ
ｉｏｎ


Ｆｏｒｗａｒｄ＾
＂
Ｊ

Ｎｘ


ｒ—


ＭＡｄｄ＆Ｎｏｒｍ


ｐ
（Ａｄｄ＆Ｎｏｒｍ


Ｍｕ
ｌｔ卜ＨｅａｄＭｕ
ｌｔ
ｉ
－Ｈｅａｄ


Ａｔｔｅｎｔ
ｉｏｎＡｔｔｅｎｔ
ｉｏｎ


，ＬＪ＝ｐ


Ｐｏｓ
ｉｔ
ｉｏｎａ
ｌ１ｌｙＴＸＰｏｓ
ｉｔ
ｉｏｎａｌ


Ｅｎｃｏｄ
ｉｎｇ
Ｅｎｃｏｄ
ｉｎｇ


ＩｎｐｕｔＯｕｔｐｕｔ


Ｅｍｂｅｄｄ
ｉｎｇＥｍｂｅｄｄ
ｉｎｇ


Ｉ１


ＩｎｐｕｔｓＯｕｔｐｕｔｓ


（ｓｈ
ｉｆｔｅｄｒ
ｉｇｈｔ）


图２
－７Ｔｒａｎｓｆｏｒｍｅｒ模型架构
［３７
］


如图
２
－７所示
，
Ｔｒａｎｓｆ
ｏｒｍｅｒ由编码器和解码器两个部分组成
。其中
，编码器


Ｅｎｃｏｄｅｒ由
６个相同的层构成
，
每层通过多头自注意力机制模块和全连接前向网


络模块进行实现
＝
此外
，
残差网络
（Ｒｅｓ
ｉｄｕａｌＣｏｎｎｅｃｔ
ｉｏｎ
）
和层次归
一化
（Ｌａｙｅｒ


Ｎｏｒｍａｌ
ｉｚａｔｉｏｎ
）
用于每
一层
。
Ｔｒａｎｓｆｏｒｍｅｒ的编码器重要组件如下
。


１
）
多头注意力机制
（Ｍｕ
ｌｔ
ｉ
－ＨｅａｄＡｔｅｎｔ
ｉｏｎ
）


注意力机制
，
可以让模型并行地计算句子中词与词之间的语义相似度
，
对序


列数据建模
注意力机制Ａｔｔｅｎｔｉｏｎ的定义如下
：


Ａｔｔｅｎｔｉｏｎ（Ｑ
，ｍ
＝Ｓｏｆｔｍａｘ
（＾
＿
）
Ｖ（２
－
１９）


其中
，
尺
￡Ｒ
ｉｘ办和ｙ
ｅＭ
Ｌｘｄ？分别代表查询项
、
项以及值项
。
ｌ为


序列长度
，
ｄｆ
ｃ和内分别为查询向量和键值向量的维度
。


１９


北京邮电大学工程硕士学位论文


ｔ


Ｌｉｎｅａｒ


Ｃｏｎｃａｔ


一妒
＾


Ｓｃａ
ｌｅｄＤｏｔ
－Ｐｒｏｄｕｃｔ＾


Ａｔｔｅｎｔ
ｉｏｎ＾ｕ


Ｌｉｎｅａｒ
ｊＪＬ
ｉｎｅａｒ
ｊＪＬ
ｉｎｅａｒ
ｊｊ


ＴＴＴ


ＶＫ〇


图
２
－８
多头注意力
［３７１


如图
２
－８所示
，
将原始的Ｑ
，１Ｋ先投影到维度ｄｍ
，
分别进行
／ｉ个独立注


意力操作
，
将多个头的结果进行拼接得到了最终输出
，
多头注意力机制如下
：


ＭｕｌｔｉＨｅａｄ（Ｑ
，
／＾
，Ｋ）
＝Ｃｏｎｃａｔｉｈｅａｄ
ｘ
，
．
．
．
，ｈｅａｄｈ）Ｗ
°（２
－２０）


ｗｈｅｒｅｈｅａｄ
ｊ
＝Ａｔｔｅｎｔｉｏｎ
＾ＱＷ＾
，ＫＷ＾
．ＶＷ＾）


其中
，
ｅＥ
ｄｍＸｄｆ
ｃ
，叫
尺
ｅＥ
ｄＷｄｆ
ｃ
，ｅ股‘ 吨以及＾
０
ｅＲ
／ｉｄ？ｘｄｍ
ｃ此外
，


如果
＜２
，Ｘ
，
Ｆ为相同的输入特征则将该注意力称为多头自注意力机制
。


２
）位置编码
（Ｐｏｓｉｔ
ｉｏｎＥｎｃｏｄ
ｉｎｇ
）


变压器网络通过在词向量中引入额外的位置编码
，解决由于多头注意力无法


建模序列数据的顺序信息的缺陷
。
位置编码定义如下
：


（
ＰＥ
（ｐ〇ｓ
，２〇
＝Ｓ
ｉｎ（ｐｏｓ／１００００
２
ｌ／ｄ＾
）


ｌＰＥ
（ｐ〇ｓ
，２
ｉ＋１）
＝ｃｏｓ（ｐｏｓ／１００００
２
ｉ／ｄ－
）


其中
，
ｐｏｓ为词向量在序列中的下标值
，
ｉ为词向量对应维度的下标值
＝


３
）
前向神经网络


前向神经网络定义如下
：


ＦＦＮ（ｘ）＝ＲｅＬｕＣｘ＾＋ｂ１）Ｗ２＋ｂ２（２
－２２）


其中
，
叭和％为线性变换的权重
，
＆和６２为偏置项
，
ＲｅＬｕ（〇为激活函数
。


解码器和编码器的结构类似
，但在文本解码器中
，
多头注意力需要引入
一个


掩码矩阵（Ｍａｓｋ
）
，引入掩码的多头注意力机制记为ＭａｓｋｅｄＭｕ
ｌｔｉＨｅａｄ（Ｑ
，欠
，
１〇
。


２０


第二章
背景知识及相关技术


２
．５
．２ＢＥＲＴ


Ｄｅｖ
ｌ
ｉｎ等人在
２０
１
８年提出了预训练语言模型
ＢＥＲＴ％
。
该模型的基本结构


如图
２
－９所示
，
由多层双向
Ｔｒａｎｓｆｏｒｍｅｒ编码器堆叠而成
。
其中
，
自注意力机制


（Ｓｅ
ｌｆ
－Ａｔｅｎｔｉｏｎ
）
是整个编码器的核心部分
，
通过自注意力机制模型不仅可以捕


获自身的信息
，
也可以关注并捕获到文本中其他词的信息
。


（
’
Ｔ
！
＇
：（
＇
Ｔ
２
）
…
丨


｜
Ｔ
＾
ＹＹ


ＴｒａｎｓｆｏｒｍｅｒＴｒａｎｓｆｏｒｍｅｒＴｒａｎｓｆｏｒｍｅｒ


ＴｒａｎｓｆｏｒｍｅｒＴｒａｎｓｆｏｒｍｅｒＴｒａｎｓｆｏｒｍｅｒ


Ｅ
，Ｅ
２Ｅｎ


图
２
－９Ｂｅｒｔ模型架构


ＢＥＲＴ在大量无标注文本上通过两个无监督语言任务完成对模型预训练
。


其中之
一的任务掩码语言模型
（ＭａｓｋｅｄＬａｎｇｕａｇｅＭｏｄｅ
ｌ
）
，采用了
一种类似


完形填空的双向语言建模思想
，
即通过
［ＭＡ
ＳＫ
］周围的上下文语境词来预测


［ＭＡＳＫ
］位置的单词
。掩码语言模型任务核心思想是通过Ｂｅｒ
ｔ模型利用上下文信


息
，将输入文本中的被掩盖处理的部分单词还原为原单词
。具体做法为将输入序


列中
１５％的单词进行掩盖处理
。
模型使用
［ＭＡＳＫ
］标记替换原单词以表示该位置


己被掩盖处理
。
然而
，
［ＭＡＳＫ
］标记并不会出现在在实际的下游任务中
。
为了缓


解这
一问题
，
当对输入序列中的词进行掩盖处理时
，被掩盖的词中
８０％被替换为


［ＭＡＳＫ
］标记
；
１０％的词被替换为词表中的任意
一个随机词
；
另外
１０
°／。的词保持


原词不变
。


为了显式地学习两段输入文本之间的关系
，
ＢＥＲＴ
引入了下
一个句子预测


（ＮＳＰ
）任务
。
训练样本中有
５０％的正样本
，
它们来自自然文本中相邻的两个句


子
，
构成
“ 下
一个句子
” 关系
；
另外
５０％的数据为负样本
，
即将两个句子中的后
一


句替换为语料库中任意
一个其他句子
，
构成
“ 非下
一个句子
”关系
。


ＢＥＲＴ同样采用了预训练
－微调的训练范式
。首先
，在大量的无标注文本语料


上进行预训练
。接着
，
在具体的下游任务对预训练模型进行微调
。


２
１


北京邮电大学工程硕士学位论文


基于掩码形式的预训练语言模型
，通过对上下文信息的双向编码
，更适合处


理自然语言理解类问题
。在ＢＥＲＴ
的基础上
，
先后出现了Ｒｏｂｅｒｔａ
ｌ４Ｑ
］
、
￡〇１４
１４
｜
】等


不同的掩码形式的预训练语言模型
，
提升预训练模型性能
。


２
．５
．３ＧＰＴ


ＧＰＴ（Ｇｅｎｅｒａｔ
ｉｖｅＰｒｅ
－Ｔｒａ
ｉｎ
ｉｎｇ
，
ＧＰＴ
）Ｍ是ＯｐｅｎＡＩ在２０
１
８年提出了
一种生


成式预训练模型
。
如图
２
－
］０所示
，
ＧＰＴ模型由
１２个
Ｔｒａｎｓｆｏｒｍｅｒ中的解码器模


块经修改后组成
，
代替了传统的
ＬＳＴＭ
网络
。
ＧＰＴ在解码器端采用了Ｍａｓｋｅｄ


Ｍｕ
ｌｔｉ
－ＨｅａｄＡｔｅｎｔｉｏｎ
的方式来避免预测当前词时会看见后面的词
，
是
一个单向


语言模型
。
ＧＰＴ的整体结构如图所示
：


…ｆ＾
、
）


，
ＴＴ＾
，
Ｉ


Ｔｒａｎｓｆｏｒｍｅ
ｒＴｒａｎｓｆ
ｏｒｍｅｒＴｒａｎｓｆｏｒｍｅｒ


ｉＴｒａｎｓｆｏｒｍｅｒＴｒａｒｔｓｆｏｒｍｅｒ
．
．Ｔｒａｎｓｆｏｒｍｅｒ


一


Ｅ
ｌＥ２Ｅｎ


图
２
－
１０ＧＰＴ模型架构


如图
２
－
１
１所示
，对于不同类型的下游任务
，对输入给模型的数据进行适当的


改造
，就可以利用预训练阶段产出的初始化好的模型参数
，实现快速的在下游任


务中微调
，
大大提高了下游任务的训练效率
。


Ｃ
ｔ＾Ｂｅｒ
＊Ｃ
ｌａｓｓｉｆ
ｉｃａｔ
ｉｏｎＳｔａｒｔ
｜
ＴｅｘｔＥｘ
ｔｒａｃ
ｔ
＞－
｛
Ｔｆａｎｓｔｏｒｍｅｆ
－Ｌ
ｉｎｅａ
ｒ


ｒ
＂



Ｅｎｔａ
ｉ
ｌｍｅｎ
ｔＳｔ？？
｜ＰｒｅｍｉｓｅＤｅｆ
ｃｍＨｙｐｏｔｔ
ｉｅｇ
ｔｓ

Ｅｘｔｒａｃｔ，
Ｔｆａｎｓｔｏｒｍｅｒ— 
ｌ

ｉｎｅａ
ｒ


Ｉｒｅｅｄ
ｒｅｗａｒｄ＾
ＩＴｅｘｔ１Ｄｅｆ
ａｆｎＴｅｘｉ
２
｜Ｃ
？ａｒａａ－Ｔｒａｎｓｌｏｒｍｅｒ
￣
？ａ
１Ｓ
ｉｍ
ｉ
ｌａｒ
ｉ
ｔｙ
＿Ｕｎｅａｆ


１２＊
ｊ
Ｓｔａｎ｜Ｔｅｘｔ
２ＤｅｉａｒＴｅｗ１｜
Ｅｘｉｒａｃ— Ｔｒａｎｓｆｏｒｍｅｒ
—


ｉ



Ｖ
＂
— —Ｓｔａｎ
＼ＣｏｎｒｅｘｉＤｒ
ｔｍＡｎｓｗｅｒ１ＩＥｘ？ｂｃ５ＴｒａｎｓｆｏｒｍｅｒＬｉｎｅａｒ
＾
―


Ｉ
ｔ
■
ａ




ｊ
？

＞
－

．
．
— 
■

‘ 

■
＿


■


１Ｓｔ＾Ａａｅｎｆ
ａｏ
ｊ



卞

Ｍｕ
ｌ
ｔ
ｉｐ
ｌｅ
Ｃｈｏ
ｉｃｅ

Ｓａｎ［ＣｏｎｔｅｘｔＤｅｉｍ
ｊＡｎｓｗｅｒ
２
｜
ＥｘｔｒａｃｔＴｒａｎｓｆｏｒｍｅｒＬ
ｉｎｅａｒ
—


ｌｏａ
‘
Ｐ〇ｓ？ｏｒ
Ｅｎｅｗｏ

｜
ＳｔａｎＣｏｎｔｅｘｔ
＿
＿

Ａｎｓｗｅｒ
Ｎ
ｊ
Ｅｘｔｒａｓ？
｜ＴｒａｎｓＪｏｒｍｅｒ？Ｌｉｎｅａｒ


图
２
－
１
１
下游任务数据改造
［３８
］


ＧＰＴ在很适合完成生成式任务
＝


２２


第二章
背景知识及相关技术


２
．６知识表示技术


目前
，
知识以
三元组的形式存储在在知识库中
，其中
／Ｉ表示头实体
，
ｒ


表示联系
，
ｔ表示尾实体
。知识表示学习
就是通过基于深度学习的表示学习方


法
，将知识图谱中的三元组知识
编码为低维稠密的分布式表示
。通过知识


表示学习
，
知识以低维向量的形式分布在语义向量空间中
，方便对知识进行度量


和语义相似度计算
。两个向量在空间中的距离越近
，
则这两个知识在语义上越相


似
。


衡量
一个知识三元组
的可信度有两类方法
：
平移距离和语义匹配
＜：基


于此
，衍生出基于平移距离和语义匹配两种打分函数
。基于这两种打分函数
，
知


识表示学习方法包括了基于平移的方法和基于语义的方法
。
常用模型包括


ＴｒａｎＳＥ
［４３
］
、
ＴｒａｎＳＨ
［４４
ｌ等等都输基于平移的方法
。这种方法从三元组的结构出发学


习知识图谱中实体和联系
＝而
ＲＥＳＣＡＬ
［４５
］
、ＤｉｓｔＭｕ
ｌｔ
［４６
］等方法是基于语义的方法
，


该方法利用文本语义来学习知识图谱中的实体和联系
。


２
．７本章小节


本章节介绍了方面词情感分析任务和方面类别情感分析任务涉及到的背景


知识及相关技术
。
详细介绍了词向量表示技术
、循环神经网络
、
图神经网络
、注


意力机制
、
预训练语言模型和知识表示技术在内的基础理论和关键技术
。


２３


北京邮电大学工程硕士学位论文


２４


第三章
情感感知的双通道图卷积神经网络的方面词情感分析研究


第三章情感感知的双通道图卷积神经网络的方面词情感分


析研究


当前主流的方面词情感分析模型忽略了结合句法信息捕获句子中带有
“ 情感
”


极性的词语
，
以及句子含有
“ 否定
” 语义的词语
。
而这些词语往往决定了评价对


象的情感极性
。这些模型简单地利用句法依存树和图卷积神经网络对句子的语法


信息进行建模
。此外
，这些模型也忽略了利用外部知识库中丰富的结构化语义来


增强词语的语义表示
，简单利用自注意力机制对句子的语义信息进行建模
。因此
，


本章
，
引入外部情感知识和句子的词性知识对原始的句法依存树矩阵进行增强
，


显式地对句子中带有
“ 情感
” 及含有
“ 否定
” 语义的词语进行建模
。
同时
，
本章


采用了
一种基于词向量融合的知识增强方式
，对句子中的词语向量进行词向量层


次的知识增强
实验结果表明
，通过对语义和语法信息进行显式地知识增强
，模


型能够更好地捕捉到句子的语义和语法信息
，
情感极性分类效果更好
。


３
．
１情感感知的双通道图卷积神经网络算法


本章提出
一种情感感知的双通道图卷积神经网络模型框架
，
即
Ｓｅｎｔｉｍｅｎｔ


ＡｗａｒｅＤｕａｌＣｈａｎｎｅｌＧｒａｐｈＣｏｎｖｏ
ｌｕｔｉｏｎａｌＮｅｔｗｏｒｋ（ＳＡＤＣ
－ＧＣＮ
）
。
该框架主要包


含两个通道
，
即基于知识增强的语法感知通道和基于知识增强的语义感知通道
。


基于知识增强的语法感知通道
，
利用外部情感知识库
ＳｅｎｔｉｃＮｅｔ来增强句法依存


树的结构信息
，
以增强句子中情感表达部分的权重
。此外
，
利用词性知识来进
一


步增强模型对蕴含
“ 否定
” 语义词的关注
，
以解决
“ 否定
” 语义影响的问题
。


同时
，
基于知识増强的语义感知通道
，
利用外部知识库
Ｃｏｎｃｅｐ
ｔＮｅｔ对句子


中的词语进行词向量层次的知识增强
。接着
，利用自注意力机制对经过知识增强


的词语的语义关系进行捕获
，以缓解单纯的注意力机制引入噪声而导致的分类错


误的问题
。总体而言
，
本文对基于依存分析树构建的矩阵进行情感知识和词性知


识的增强
，
并通过
ＫＳｙｎ
－ＧＣＮ模块进行对节点特征进行更新
，
从而得到感知句


子的句法信息的表示
。
同时
，本文将句子中的经过知识增强的词作为节点构建
一


个基于自注意力机制的完全连通的有权图
，
通过ＫＳｅｍ
－ＧＣＮ［对节点特征进行更


新
，
得到感知句子的语义信息的表示
。最终
，将两个通道的向量表示拼接起来进


行情感分析
。


ＳＡＤＣ
－ＧＣＮ模型整体框架如图
３
－
１所示
，
该模型主要包括输入编码层
、
基


于知识增强的语法通道模块
、基于知识增强的语义通道模块和情感分类器等模块
。


２５


北京邮电大学工程硕士学位论文


ＫＳｙｎ
－ＧＣＮ


ｓｅｎｔ
ｉｍｅｎ
ｔｓｃｏｒｅＰａｒｔ
－ｏｆ
－Ｓｐｅｅｃｈ
ｗ
ｎ
■
丁Ｔ１
｜
ｒ
Ｉ
Ｉ
ｒ．
｜ＩＩＩ
ｉ
丨Ｃ


！


－Ｊ－
—
．
．９
！＾


｜
Ｄｅｐｅｎｄｅｎｃｙ
Ｊ＿＾
＂
Ｋ＾１


ｌ
Ｐａｒｓｅｒ
Ｐ
一
一
—一〇
－
－
＞
ＩＷ
ｉ
ｆ


｜
．
：
＿Ｌｉ± ＩＪ二±±± ＝
￣￣￣
ｒ
°
ＩＡ


ｉ“
ｊ
－
？


ＢｒｎｆｉＥｌ？


５
：ｐ
：
丨〇
？
Ｖ
１１
Ｉ圍
ｌ


＇
＇—
ｒ
— ｙ＾
ｒ
— ■
ａｔｅｎｔｏｎｓｃｏｒｅ斗


／广

ｃｏ■
鼸
ＫＳｅｍ
－ＧＣＮ


Ｓ１
、／
ｒ— —
Ｉｒ＾？


３０ｃ
．Ｉｆ
＞０丄


Ｉ＾ｒ
ｎ
ｍ
ｍ样、


〇Ｍｏｄｕ
ｌｅＱ：＾
．
．ｇ
．
．
．
．


｜
ｖ
．ｉ：ｏｃ
ｉＳＯＬＺ：
？
… 二：
？


！〇〇
＂ＶＳ５Ｔ
＂
＇
：
；
；
… … …
＝
Ｌ＾


ｖ
￣ｘ
ｙ


图
３
－
１ＳＡＤＣ
－ＧＣＮ模型架构


３
．２
知识库与知识表示


３
．２
．
１Ｃｏｎｃｅｐ
ｔＮｅｔ


为了更好地利用外部知识库中的结构化语义知识信息
，本文引入了外部常识


知识库Ｃｏｎｃｅｐ
ｔＮｅｔ
［４７
］
。
ＣｏｎｃｅｐｔＮｅｔ包含
２
１００多万条边和
８００多万个节点
，
是
一


个大型免费的知识图谱
。
Ｃｏｎｃｅｐ
ｔＮｅｔ重点关注自然语言中的词语
，而非命名实体


的语义
，因此提供了基于知识图谱的结构化词向量表示Ｃｏｎｃｅｐ
ｔＮｅｔＮｕｍｂｅｒｂａｔｃｈ
。


３
．２
．２Ｓｅｎｔ
ｉｃＮｅｔ


Ｓｅｎｔ
ｉｃＮｅｔ
ｌ４８
，４％是
一个概念层面的知识库
，
常常用于概念层面的情感分析
。
该


知识库为自然语言概念定义了语义
、情感和极性的概念
。其中
，语义定义了自然


语言概念的外延信息
，
即语义相关概念
。
情感定义了自然语言概念的内涵信息
，


即通过四个情感维度表达的情感分类值
。而极性定义了自然语言概念的情感数值
，


情感数值在
－
１到＋
１之间
，其中
－
１表示极端的负面情感
，＋
１表示极端的正面情感
。


３
．２
．３
知识表示


通常在知识库中使用三元组（ｈｅａｄ
，ｒｅ
ｌａｔｉｏｎ
，
ｔａｉｌ
）来表示知识
。为了能将知识表


示成向量
，
一些方法用独热向量来表示知识
，这种表示方法受限于实体和关系太


多
，
因而导致向量维度太大
。
同时
，
独热向量表示法也无法捕捉到不同知识之间


的相似度
。
受
＼Ｖｏｒｄ２Ｖｅｃ词向量模型的启发
，
一些方法开始把知识图谱中的三元


２６


第三章
情感感知的双通道图卷积神经网络的方面词情感分析研究


组编码成低维度的分布式向量表示
。这些方法称为知识表示模型
。知识表示模型


主要包括以
ＴｒａｎｓＥ
、
ＴｒａｎｓＨ等为代表的基于平移的模型
，
以及以
Ｄ
ｉｓＭｕ
ｌｔ为代


表的基于语义的模型
。


除了使用上述模型显式地对三元组知识进行编码
，部分知识库还自建有词向


量形式的知识表示
，直接利用这种知识表示
，可以在不使用知识表示模型的情况


下
，
更高效地获取和利用外部知识库中的知识
。
Ｃｏｎｃｅｐ
ｔＮｅｔＮｕｍｂｅｒｂａｔｃｈ
｜４７
］是由


ＣｏｎｃｅｐｔＮｅｔ提供的
一组词向量工具
，
它同时利用了文本和
Ｃｏｎｃｅｐ
ｔＮｅｔ中的半结


构化信息进行学习
，
可以直接以
３００维的向量形式表达词的语义
。


３
．３编码器


３
．３
．
１
输入层


在输入层通过词向量模块
，
将文本中每个单词映射为
一个
维度的分布式


词向量Ｘ
；
。


ｈ２ｈｎ
—
ｊｈｎ


．ＪＪｙＪ？


？＜Ｌｓｔｍ４
—
ＬｓｔｍＸ
－
…
－
Ｌｓｔｍ＜
Ｌｓｔｍ
＊４
｛？
，


ｉ
ｊ
ＴＡ
￣￣￣
Ｉ
＂
ａ
不
１


＾
？
〇
：？Ｌｓｔｍ
—
Ｌｓｔｍ
ｒ

？Ｌｓｔｍ
— ？Ｌｓｔｍ


Ａ个个


＾Ｌｉ
，


Ｘ
ｉＸ２
＊＾ｎ
—
１


图
３
－２
双向
ＬＳＴＭ结构


然后
，
如图
３
－２所示
，
采用双向长短期记忆神经网络＿
（Ｂ
ｉ
－ｄ
ｉｒｅｃｔ
ｉｏｎａ
ｌＬＳＴＭ
．


Ｂ
ｉ
－ＬＳＴＭ
）对每个词向量＆进行编码
，得到每个词的隐状态表示
。具体而言
，输入


单词４通过前向网络计算得到前向向量／ｉｆ
，
通过后向网络计算得到后向向量＃
。


将两者进行拼接
，得到
［
／ｉｆ
；ｈｆ］
。拼接后的向量代表相应时刻的隐层状态
，可表示


为
：


ｈ
ｔ
＝Ｂ
＼
－
ＬＳＴＭ（Ｘ
ｉ），ｈ
ｉＧＲ
２ｘｄ＾（３
－１）


３
．３
．２基于知识增强的语法感知通道


１
）
依存句法分析


２７


北京邮电大学工程硕士学位论文


依存句法分析
［５
１
］主要通过分析句子中词语与词语之间的依存关系来确定句


子的句法结构
＝依存句法分析可以解析出句子中词与词的直接依存关系
，这种依


存关系组成
一个依存对
。
一个依存对中包含
一个支配词和
一个从属词
。动词是
一


个句子的核心
，在依存树中以根节点的形式存在
。句子中的其他词
，
通过直接或


间接的边与动词形成关联
。
句法依存结果示例如图
３
－３所示
：


（

—
ｐ
＾
＂
￣
？
＼


广
ｓｕｂ
ｊ、／／＾
－．ｎｓｕｂ
ｊ
－＾＼


［５ｇＴＴ
：：ｇ
：＞
ｉｉ
：
：ｉ＾ｙ
ｉ；ｒｆＵｕｘ
ｆ
＊１
：ｏｐ＾Ｘ５Ｊ＾
￣￣
ｐｕｎＣｔ＞￣＊
ＴＰＵＮＣｎ
ＩＣＣＯＮＪｒ
［Ｄ＾
ＳｅＩ
￣
ＹＮＯＵＮ
Ｉ
｜ＡＵＸ
；
＂ｃ０ｐ
ｌＡＰｒ
ＩＰＵＮＣＴ
Ｉ


Ｔｈｅｐ
ｉｚｚａｉｓｇｏｏｄ
，ｂｕｔｔｈｅｗａ
ｉｔｅｒ
ｉｓｒｕｄｅ．


图
３
－３解析工具生成的句法依存树示例


此前的大部分工作都是基于句法依存树
，
对句子进行依存分析后
，
得到句子


中词与词的依存关系
，从而根据算法构建出基于句法依存树的邻接矩阵
。本文延


续这
一思路
，
将
Ｓｐａｃｙ作为依存句法分析工具
，
对给定的
一个包含ｎ个词的句子


进行解析
，
并利用解析结果构建出邻接矩阵
，
如下所示
：


｜
１
，
如果叫，有边连接


ｗ１〇
，
其他


其中
，
Ｄ
ＧＭ
ｎｘｎ表示该句子的图结构
，
？
表示节点对
＜Ｗ
ｉ
，ｗ
；
＿
＞的边
。
具体


地
，
如果第
ｉ
个节点与第
Ｊ／个节点相连
，
则
Ｄ
［７
＝ｌ
，
否则
Ｄ
ｉ；
＝０
。为了突


出方面词在句子中的重要性
，更好地让模型感知到句子中的方面词
，本文同样设


置方面词标识权重
：


＿
（
１
，
如果ｗ
，
？或
ｕ／
；
ｅ方面词


ｃ
ｉ
，ｊ
＝
１＾
（３
－３）


（０
，
其他


其中
，
ＣｅＭ
ｎｘｎ表示该句子的图结构
，
Ｃｙ
表示节点对
＜＞的边
。具体地
，


如果第
丨个节点或第
／个节点属于方面词或方面词的
一部分
，
则
＝ｌ
，
否


则Ｃ＂
＝０
。


２
）情感增强的句法依存树


在ＡＴＳＡ任务中
，
最重要的两个元素是方面词和相关的观点词
，
与方面词对


应的观点词的情感极性在很大程度上决定的方面词的情感极性
。为了更好地让模


型捕捉到句中的情感词或观点词
，本文参考Ｌ
ｉａｎｇＰｌ等的研宄
，将
Ｓｅｍ
ｉｃＮｅｔ作为


外部情感知识库来补充更多的情感知识
。
本文同样认为词语％在
Ｓｅｎｔ
ｉｃＮｅｔ中的


情感数值绝对值越高
，
这个词越有可能是观点词或观点相关词
。


对于句子中任意两个词语ｕ／
，
、ｗ
；组成的单词对＜Ｗｐｗ
；
＞
，其情感权重定义为
：


Ｓ
ｔｊ
＝ＳｅｎｔｉｃＮｅｔ（Ｗ
ｉ）＋ＳｅｎｔｉｃＮｅｔ（Ｗ
ｊ）（３
－４）


２８


第三章
情感感知的双通道图卷积神经网络的方面词情感分析研究


其中
，
ＳｅｎｔｉｃＮｅｔ＾ｗＪ
ｅ
ｌ
— ｌ
，
：！
］表示词Ｗ
ｉ在
ＳｅｎｔｉｃＮｅｔ中的情感数值
。
当Ｗ
ｉ为中性


词或者不在
ＳｅｎｔｉｃＮｅｔ情感词汇表中时
，
ＳｅｎｔｉｃＮｅｌ＾Ｗ
ｉ）＝０
。
基于情感知识增强


的依存句法关系邻接矩阵通过算法
１进行具体的构建
。


算法
１
：
基于情感知识增强的依存句法关系邻接矩阵的构建



输入
：
句子
ｓ单词序列
句子的依存句法树
ｄｅｐｅｎｄｅｎｃｙ（ｓ
）


输出
：
基于情感知识增强的依存句法关系邻接矩阵次


１ｆｏｒｉ＝１＾ｎｄｏ


２ｆｏｒ７
＝１ｎｄｏ


３ｉｆｄｅｐｅｎｄｅｎｃｙ（ｗ
ｉ
ｔｗ
ｊ）Ｅｄｅｐｅｎｄｅｎｃｙ（ｓ）ｏｒｉ＝
ｊｔｈｅｎ


４Ｄ
ｉｊ
１


５Ｓ
（ｊ
＜
－
ＳｅｎｔｉｃＮｅｔ（Ｗ
ｉ）＋ＳｅｎｔｉｃＮｅｔ（ｗ
ｊ）


６
ｉｆｏｒ％Ｅ方面词


５Ｃｕ
＝１


６ｅｌｓｅ


７Ｑ
＝０


８ｅｎｄｉｆ


９Ａ
ｔｊ
＜
－＊
（Ｓ
ｉｊ＋Ｃ
ｔｊ＋１）


１０ｅｌｓｅ


１
１Ａ
（ｊ
＜
—
０


１２ｅｎｄｉｆ


１３ｅｎｄｆｏｒ


１４ｅｎｄｆｏｒ


３）
词性增强的句法依存树


句子中含有
“ 否定
” 语义的词会严重地影响到细粒度情感分析的结果
，
例如


“
Ｔｈｅｆｏｏｄｉｓｎｏｔｓｏ
ｇｏｏｄ
．
”
，
“ Ｔｈｅｓｅｒｖ
ｉｃｅｓｈｏｕｌｄｂｅｂｅｔｅｒ．
” 等
，
虽然句子中出现了


正向情感极性的形容词
“
ｇｏｏｄ
” 和
“
ｂｅｔｔｅｒ
”
，
但这些词的周围都有表示
“ 否定
”


含义的词出现
，
例如
“
ｎｏｔ
” 和
“
ｓｈｏｕ
ｌｄ
”
。


为了将这些词纳入考虑
，
本文对语料库进行了统计
，
发现含有
“ 否定
” 语义


的词大部分词的词性是形容词
、副词和动词
＝
因此
，本文决定将词性这
一重要特


征也考虑进来
，提出了
一种基于词性的矩阵增强方法
，对带有情感语义信息的句


法依存树进行语义增强
。


２９


北京邮电大学工程硕士学位论文


：
ＴＴｉｅｐ
ｉｚｚａ
ｉｓｇ
ｒｅａｔｂｕｔｅｘｐｅｎｓ
ｉｖｅ
Ｉ


／、


ｗ１Ｊｆ
－
奢
。￣￣￣

｜”
ｊ
…
丨
＇

１
１


！
－ＪＦｆ
ｌ三Ｆｆ
ｌ
－
Ｈｊｔｊ
．
，
．
．
ｆ


ｗ４—
—
—ＸＴ
—
— ＩＣｔ：＿
＾。


ｗｓｉｍｎ
Ｊ
ＩＭｉｉ
＇
＇


ｗ６
？



ｄｅｐｅｎｄｅｎｃｙｍａｔｒ
ｉｘｓｅｎｔ
ｉｍｅｎｔｅｎｈａｎｃｅｄＰＯＳｅｎｈａｎｃｅｄＫＳｙｎ
－ＧＣＮ


图
３
－４
基于知识增强的句法依存树矩阵的构建


首先
，
根据上文所述
，
模型需要关注到可能含有
“ 否定
” 含义的词
，
即形容


词
、
副词和动词
。
本文预先设置好
一个需要关注的词性表Ｎ＝
［形容词
、
副词
、动


词］
。
对于任意词对＜
Ｗ
ｉ
，
ＶＶ
ｙ
＞
，
如果Ｗ
ｔ
？和Ｗ
ｙ的词性在词性表中
，


Ｓ（
ｉｖ
；）
｜
；
否则；＼；
＝０
。


＿０５（Ｗ
ｊ）
－
Ｓ
（ｗ
；）
｜，
如果Ｗ
ｉ或Ｗ
；
．的词性
ｅ词表Ｎ


ｉＪ
￣
（〇
，其他
＇


基于情感知识和词性知识增强后的邻接矩阵如图
３４所示
，
具体按照算法
２


进行构建
，
计算表达式如下
：


＝＊
（？＋＋Ｐｙ＋１
）（３
－６）


算法
２
：
基于词性知识增强的依存句法关系邻接矩阵的构建


输入
：
句子
ｓ单词序列｛ａ
，
句子的依存句法树ｄｅｐｅｎｄｅｎｃｙ（ｓ）


输出
：
基于词性知识增强的依存句法关系邻接矩阵次
＞；


１ｆｏｒｉ＝１＾ｎｄｏ


２ｆｏｒ７
＝１
－
＞ｎｄｏ


３
ｉｆｄｅｐｅｎｄｅｎｃｙ（ｗ
ｉ
ｔＷ
ｊ）Ｅｄｅｐｅｎｄｅｎｃｙ（ｓ）ｏｒｉ
＝
ｊ
ｔｈｅｎ


４Ｄ
ｉ
ｔｊ
＾１


５Ｓ
ｉｊ
＜
－
ＳｅｎｔｉｃＮｅｔ｛ｗ｛）＋ＳｅｎｔｉｃＮｅｔ（ｗ
ｊ）


６
ｉｆｏｒＥ方面词


７Ｑｊ
— １


８ｅｌｓｅ


９Ｑ
，广
０


１０ｅｎｄｉｆ


１
１
ｉｆｕ／
〖
ｏｒｕ／
ｙｅ词表Ｎ


１２ｐ
ｉ
ｊＨ
ｓ（Ｗ
ｉ）
－ＳＣｗ
ｊ）
ｌ


１３ｅｌｓｅ


３０


第三章
情感感知的双通道图卷积神经网络的方面词情感分析研究


续上表


１４Ｐ
ｉｊ
＜
－０


１５ｅｎｄｉｆ


１６Ａ
ｉｊ
＜
－
Ｄ
ｉｊ
＊
（Ｓ
ｔｊ＋Ｃ
ｉｊ＋Ｐ
ｔｊ＋１）


１７ｅｌｓｅ


１８Ａ
ｉｊ
０


１９ｅｎｄｉｆ


２０ｅｎｄｆｏｒ


２
１ｅｎｄｆｏｒ


４
）基于知识增强的图卷积神经网络


图卷积神经网络
（ＧＣＮ
）能更好地利用图结构对邻居节点特征进行聚合来更


新当前节点表示
。为了更好地捕捉方面词节点与情感表达节点及
“ 否定
” 含义节


点之间的关系
，从而得到更准确的方面词节点特征表示
，
本文采用图卷积神经网


络作为编码器
，将基于情感信息和词性信息的依存句法分析矩阵作为图结构
，对


图中的单词节点进行特征聚合与更新
，得到包含增强的句法信息的特征向量
。对


于ｎ个节点ｉ层的ＧＣＮ模型
，
Ｚ层的第
ｉ个节点可由如下公式更新
：


ｈ
ｉ
＝
ａ（＾
Ａ
ｉｊＷ
１
＾
＇１
＋ｂ＾
ｊ
（３
－７）


其中
，
／ｉ
丨为节点
ｉ在第
Ｚ层的输出表示
。
？为邻接矩阵
，
Ｖ为可学习的参数
。


＜
？
）表示非线性激活函数
，
比如ＲｅＬＵ函数
。


本文通过算法
２将经过知识增强的矩阵
，
转化为图结构
，
表示为邻接矩阵


。
知识增强的语法图卷积神经网络将基于邻接矩阵Ｚ
ｆｆ
７１作为图结构
，
将经


过
Ｂｉ
－ＬＳＴＭ编码过的隐藏状态Ｗ＝
｛＆
，＆
作为图中节点的初始化表示
，通


过
ＫＳｙｎ
－ＧＣＮ对节点表示进行更新
，
基于知识增强的语法感知通道的最终表示


如下
：


ｈｆ
ｓｙｎ
Ｌ
＝＋ｂ＾
ｊ
（３
－８）


Ｈ＾ｙｎ
＝
｛ｈ＾
ｙｎ
ｉｈ
ｋＳｙｎｉｈ
ｋｓｙｎ
］（３
－９）


３
．３
．３基于知识增强的语义感知通道


１
）
知识表示增强


为了更好地理解长难句
，捕捉到句子中词与词之间的进
一步的语义关系
，本


文引入外部知识库对单词向量进行词向量表示层面的增强
＝知识库中的概念经过


３
１


北京邮电大学工程硕士学位论文


知识表示模型
，
得到低维稠密的向量
，
在向量空间中局具有可度量计算的关系
。


如果两个概念语义上存在相似性
，
则这两个概念在向量空间里相互靠近
。


本文直接利用
Ｃｏｎｃｅｐ
ｔＮｅｔ
中的训练好的词向量
Ｃｏｎｃｅｐ
ｔＮｅｔＮｕｍｂｅｒｂａｔｃｈ
［４７
］


工具获取词＆的知识向量的表示
，
公式如下
：


ｈ
ｉ
〇ｎ＝ＮｕｍｂｅｒＢａｔｃｈ（Ｘ
ｉ）
（３
－１０）


其中
，
ｆ
ｃ
ｉ为词々在Ｃｏｎｃｅｐ
ｔＮｅｔＮｕｍｂｅｒｂａｔｃｈ中的词向量表不
。


ｗｉ
［Ｐｎ
ｉ—
，


ｗ２
．
？…
ｗ３


Ｗ４：：：ｊ
｜

Ｈ
｜
；
＾
：
＝


ｗ５［］
ｊＣＺＩＺＺ


！
ｉ
丨
；

？＇


ｗ６
；＾
？
？
？
’
，
）
（５３ｆ〇ｗｉ
、


Ｉ
Ｉ


ＩＩ（ｑ〇
ｌ
ｉ〇ＱＣ
＂
：：
：：２


§．
ａ
ｉ
＇
ｏｏｌ
ｌｏｃｉ
＇
＇
：
＂
－
…
…
…
：


ｐａ
；
＂
－
－
—


ｋｎｏｗ
ｌｅｄｇｅｋｎｏｗ
ｌｅｄｇｅｅｎｈａｎｃｅｄ


ｅｍｂｅｄｄ
ｉｎｇｓｒｅｐ
ｒｅｓｅｎｔａ
ｔ
ｉｏｎｓ


图
３
－５
基于分布式词向量形式的知识表示增强


如图
３
－５所示
，
对经过
Ｂ
ｉ
－ＬＳＴＭ的单词向量进行增强
，
将知识向量与
Ｂ
ｉ
－


ＬＳＴＭ输出的隐层表示进行拼接
，
得到经过知识增强的向量表示
，
公式如下
：


／ｉｆ
ｎｏ＝ｈｉ
〇ｎ
］
（３
－１１）


其中
，
＆表示经过知识增强的对应于词＆的向量表示
。


２
）基于自注意力机制的图卷积神经网络


大部分的评论短句语法结构混乱或者不按照语法结构组织句子
，
使用句法结


构分析可能会引发错误
。因此
，本节使用自注意力机制来构建句子中词与词的关


系
。经过知识增强的向量表示引入了知识向量在外部知识语义空间里的先验相似


性特征
。本节尝试结合图卷积神经网络相关结构和自注意力矩阵
，将经过知识表


示增强后的表示中的语义相关信息融入到句子的编码信息中
，生成包含知识增强


的语义信息的特征向量
。


注意力得分矩阵＃
可以通过自注意力机制计算得到
，
用公式表述


为
：


（ＱＷ＾ｘ｛ＫＷ
ｋＹ＼


＝
Ｓ〇ｆ
ｔｍａＸ
（
—
７Ｔ
ｋ
＾
Ｊ（３
－１２）


３２


第三章
情感感知的双通道图卷积神经网络的方面词情感分析研宄


ｄｋ
＝
＼
（３
－１３）


其中
和
是输入的隐层向量表示
，
和
都是可学习的参数权重矩阵
，


ｄ
是输入节点特征的维度
，
ｈ是注意力头的数量
。


根据上节中图卷积公式
（３
－７）
，将基于注意力得分矩阵Ｚ
ｆｆ
１作为图结构
，
将


经过
Ｂｉ
－ＬＳＴＭ编码过的隐藏状态Ｈ
ｆ
ｃｎ°作为图中节点的初


始化表示
，通过ＫＳｅｍ
－ＧＣＮ对节点表示进行更新
，注意力得分矩阵表示为


基于知识增强的语义通道的最终表示
如下
：


Ｈ
ｋｓｅｍ＝
｛ｈ￥
ｓｅｍ
，ｆ＾
ｓｅｍ
，
…
，咕
ｓｅｍ
）（３
－１４）


３
．３
．４非方面词掩盖层


为了对方面词进行情感分析
，
本文采用掩盖层对句子中的非方面词部分进行


掩盖处理
，
公式如下
：


，
，０
，ｌ
＜
ｔ＜
Ｔ＋
ｌ
ｏｒｒ＋ｍ＜
ｔ＜ｎ，〇
ｉ
ｒ、


ｍａｓｋ＝
＾
１
，Ｔ＋ｌ
＜
ｔ
＜
ｒ＋ｍ＿


其中
，
Ｔ＋ｌ
Ｓ
ｔＳ
ｒ＋ｍ是句子中方面词的下标索引
。


３
．４情感分类器


在得到基于知识增强的语法特征和基于知识增强的语义特征之后
，
本文直接


将两个特征进行拼接操作
，
然后进行情感分析
，
公式表示如下
：


ｈ
ｊｃｓｅｍ
＝
ｆｉｊ＾ａｓｋＱＨｋｓｅｍｙ）
（３
－１６）


ｈｋｓｙｎ
＝
ｆ（ｊ
＾ａｓｋ
ＱＨｋｓｙｎ））
（３
－１７）


九
＝
［打ｆ
ｃｓｅｍ
；打ｆ
ｃｓｙｎ］
（３
－１８）


ｙ
— ｓｏｆｔｍａｘ
（Ｗ
ｐ
／ｉ＋ｂ
ｐ）（３
－１９）


其中
，
／ｉ
ｓｅ为语义表不向量
，
／ｉｓｙ为语法表不向量
，
／ｉ为拼接后的向量
，
／（
？
）是平


均池化函数
，
Ｗ
ｐ与ｂ
ｐ为可学习的参数
。


３３


北京邮电大学工程硕士学位论文


３
．５损失函数


本任务的目标是通过最小化预测值和真实值之间的交叉熵损失来训练情感


分类器
。其中
，
Ｓ是训练集的大小
，
Ｃ是类别的数量
，
＃是真实分布
，
２是正则项


的权重
，
０表示所有可训练的参数
。


ｓｃ


ｙ／
？
ｌｏｇ（ｙ／）＋Ａ
｜
｜０
｜
｜
２（３
－２０）


ｉ＝ｌｊ
＝ｌ


３
．６实验设计与分析


３
．６
．
１
实验数据


实验数据集主要由三个基准数据集组成
，分别是Ｒｅｓｔａｕｒａｎｔ
－
１４
、
Ｌａｐ
ｔｏｐ
－
１４和


ＭＡＭＳ
。其中Ｒｅｓｔａｕｒａｎｔ
－
１４、
Ｌａｐ
ｔｏｐ
－
１４来自ＳｅｍＥｖａｌ２０
１４语义评测
ｃＭＡＭＳ
［５２
］


数据集则来自
２０
１９年ＥＭＮＬＰ
－
ＩＪＣＮＬＰ的论文
。
ＭＡＭＳ数据集中的句子中含有


多个不同情感极性的方面词或方面类别
。


所有的数据集包含三种情感极性
：
正向
、
中性和负向
。数据集中的每条句子


都被标注出了属性词和相应的情感极性
。
三个数据集的统计如表
３
－
１所示
：


表
３
－
１ＡＣＳＡ任务数据集统计



Ｄａｔａｓｅｔ
Ｄ
ｉｖ
ｉｓｉｏｎ＃Ｐｏｓｉｔｉｖｅ＃Ｎｅｇａｔｉｖｅ＃Ｎｅｕｔｒａｌ


￣Ｔｒａｉｎ２
１６４８０７６３７
￣


Ｒｅｓｔａｕｒａｎｔ
－
１４


Ｔｅｓｔ

７２７

１９６

１９６


Ｔｒａｉｎ９７６８５
１４５５
￣


Ｌａｐｔｏｐ
－
１４



Ｔｅｓｔ

３３７

１２８

１６７


Ｔｒａｉｎ３３８０２７６４５０４２
￣


ＭＡＭＳ




｜Ｔｅｓｔ４００３２９６０７


３
．６
．２评价指标介绍


本文选用准确率
（Ａｃｃｕｒａｃｙ
）
和
Ｆ
］
值作为本模型性能的评价指标
。


１
）准确率


准确率是指分类正确的样本数量在总样本数中所占的百分比
。
准确率是


分类问题中最简单也是最直观的评价指标
，
定义如下
：


ＴＰ
ｔ＋ＴＮ
ｔ
ｆ、


ＡｃｃｕｒａｃＶｉ＝— — — — —（３
－２１）


力ＴＰ
ｔ＋ＴＮ
ｔ＋ＦＮ
ｔ＋ＦＰ
（
ｖＪ


对于三分类问题而言
，
平均准确率计算如下
：


３４


第三章
情感感知的双通道图卷积神经网络的方面词情感分析研究


！Ａｃｃｕｒａｃｙ
ｉ
，、


Ａｃｃｕｒａｃｙ
＝（３
－２２）


２
）精确率


精确率
Ｐｒｅｃｉｓｉｏｎ是指分类正确的正样本个数在分类器判定为正样本的样


本个数中的百分比
，
它是相对于预测结果而言的
，
定义如下
：


ＴＰ
，


Ｐｒｅｃｉｓｉｏｒｉ
ｉ
＝
Ｔｐ＋
ｌ
ｐｐ
（３
－２３）


对于三分类问题而言
，
平均精确率计
（
算如


ｎ
．
．Ｉｆ＝ｉｐｒｅｃｉｓｉｏｒｉ
ｉ…


Ｐｒｅｃｉｓｉｏｎ＝（３
－２４）


３
）
召回率


召回率
Ｒｅｃａｌ
ｌ是指分类正确的正样本个数在真正的正样本个数中的百分


比
，
它是相对于原来的样本而言的
，
定义如下
：


ＴＰ
．


Ｒｅｃａｌｌ＝— ￣￣
（３
－２５）


１ＴＰ
ｔ＋ＦＮ
ｔ


对于三分类问题而言
，
平均召回计算如下
：


Ｒｅｃａｌｌ＝＾＝ｌＲ
＾
ＣａＵ
ｉ
（３
－２６）


４
）Ｆ
１值


精确率和召回率值是既矛盾又统
一的两个指标
，
为了同时考虑精确度和召回


率的影响
，
Ｆ
１值通过精确率和召回率的调和平均值计算得出
，对应的公式如下
：


２ＰｒｅｃｉｓｉｏｎｘＲｅｃａｌ
ｌ
＿、


ＦＩ（３
－２７）


Ｐｒｅｃｉｓｉｏｎ＋Ｒｅｃａｌｌ


其中
，
ＴＰ（ＴｒｕｅＰｏｓｉｔｉｖｅ
）表示样本数据实际为正例
，预测结果也为正例的数值
＝


ＴＮ（ＴｒｕｅＮｅｇａｔｉｖｅ
）表示样本数据实际为负例
，
预测结果也为负例的数值
。
ＦＰ


（ＦａｌｓｅＰｏｓｉｔｉｖｅ
）表示样本数据实际为负例
，
预测结果为正例的数值
。
ＦＮ（Ｆａｌｓｅ


Ｎｅｇａｔ
ｉｖｅ
）表示样本数据实际为正例
，
预测结果为负例的数值
。


３
．６
．３实验参数及设置


针对本文提出的
ＳＡＤＣ
－ＧＣＮ模型
，
为了验证其有效性
，
本文采用深度学习


框架Ｐｙｔｏｒ
ｃｈ开发和训练模型
，并在基准数据集上对模型进行了定性和定量实验
。


实验环境为Ｕｂｕｎｔｕ１６
．０４服务器
，
ＧＰＵ为ＮＶＩＤＩＡＧｅＦｏｒｃｅＲＴＸ２０８０Ｔｉ显卡
。


文本输入阶段
，
采用
Ｇｌｏｖｅ词向量来初始化句子中的单词
，
维度为
３００维
。


紧接着
，
使用
Ｂ
ｉ
－ＬＳＴＭ作为句子特征编码器
，
隐状态表示维数为
３００维
＝
各个


通道的ＧＣＮ层数为
２
，
隐状态表示维数为
１００维
＝
Ｃｏｎｃｅｐ
ｔＮｅｔＮｕｍｂｅｒｂａｔｃｈ向量


维数为
３００维
＝


为了防止过拟合
，本文把ｄｒｏｐｏｕｔ设置为
０
．４
。模型所有的权重初始化服从均


匀分布
。
在监督训练阶段
，
采用Ａｄａｍ优化器
，
学习率为０
．００１
，
数据批量大小


３５


北京邮电大学工程硕士学位论文


为
１６
。
对于基于Ｂｅｒｔ的模型而言
，
本文使用
ｂｅｒｔ
－ｂａｓｅ
－ｕｎｃａｓｅｄ
，
维数为
７６８维
，


学习率为０
．０００２
。


３
．６
．４对比模型


本节选取近年的针对ＡＴＳＡ任务的情感分析模型进行定量的性能对比
，包括


基于注意力机制的模型
，
如ＡＴＡＥ
－ＬＳＴＭ
、
ＩＡＮ
；
同时
，
也包括
一些基于句法依


存树和图神经网络的模型
，
如ＡＳＧＣＮ
、
Ｒ
－ＧＡＴ、
Ｄｕａｌ
－ＧＣＮ和Ｂ
ｉｓｙｎ
－ＧＡＴ等
。


以下简要说明对比模型
：


１
）ＡＴＡＥ
－ＬＳＴＭ
ｌ
１４
］模型
：
该模型最早将注意力机制引入到细粒度情感分析模


型
，
将
Ａｓｐｅｃｔ的表示和隐状态表示拼接作为输入
，
利用注意力机制对句子中不


同重要程度的单词给予不同的权重
，
最后将
ＬＳＴＭ
的输出与注意力计算得到
一


个与给定方面相关的句子加权表示
。


２
）ＩＡＮ模型
该模型分别对方面词和句子进行编码
，
同时通过交互式注


意力结构交互地学习句子的上下文和方面词之间的关系
。


３
）ＣＤＴ模型
该模型通过图卷积神经网络
（ＧＣＮ
）和句法依存树对句子


结构进行建模
，
学习句子的句法特征
。


４）ＡＳＧＣＮ模型
［
１９］
：
该模型将ＧＣＮ引入ＡＴＳＡ任务
，
首次使用ＧＣＮ学习


与方面词相关的表示
。具体来说
，模型先使用
ＬＳＴＭ层对句子进行编码
，接着在


ＬＳＴＭ输出之上使用多层图卷积结构学习与方面词相关的特征
。


５
）Ｒ
－ＧＡＴ模型
［２２
］
：
该模型对原始的依存分析树进行重建和修剪
，
从而获得


以目标方面为根的面向方面的新的树结构
。然后
，通过关系图注意力网络（ＧＡＴ
）


来对新的树结构进行编码来学习句子的表示
。


６
）Ｓｅｎｔｉｃ
－ＧＣＮ模型
［２９
１
：该模型在利用句法依存树的基础上引入
ＳｅｎｔｉｃＮｅｔ
，


利用
ＳｅｍｉｃＮｅｔ中的词语情感信息对句法依存树做情感语义的增强
。


７
）Ｄｕａｌ
－ＧＣＮ模型
ｔＭ
：
该模型考虑句法结构和语义相关性之间的互补性
，
提


出了
一种双图卷积网络来同时处理句法信息和语义信息
。具体而言
，该模型设计


了
一个基于句法的图卷积网络模块和
一个基于语义的图卷积网络模块
。
同时
，该


模型提出了正交正则和差异正则
，通过约束语义图中的注意力得分矩阵来更加精


确地捕获词之间的语义联系
。


８
）Ｂ
ｉｓｙｎ
－ＧＡＴ模型
［２４
］
：该模型不再使用句法依存树来学习句子的语法信息
，


而是通过短语结构树来学习与每个方面词相关的上下文语义信息
。
同时
，构建
一


套规则来学习多个方面词之间的关系
。


３６


第三章
情感感知的双通道图卷积神经网络的方面词情感分析研宄


３
．６
．５结果分析


表
３
－２展示了相关模型方法在不同数据集的性能结果
。需要说明的是
，
由于


大部分研宄工作模型设计复杂且缺少有效的开源代码
，
难以在与
ＳＡＤＣ
－ＧＣＮ相


似的实验环境下重复相关实验并评测
，故表中的性能结果均沿用相关研究论文中


的指标进行展示
＝


实验主要结果如表
３
－２所示
，
除了使用
Ｂｅｒ
ｔ进行编码的模型
Ｂ
ｉｓｙｎ
－ＧＡＴ
，


ＳＡＤＣ
－ＧＣＮ在三个数据集上准确率和
Ｆ
１值指标超过了其他非
Ｂｅｒ
ｔ的方法
，
取


得了较好的效果
。
基于
Ｂｅｒｔ编码的
ＳＡＤＣ
－ＧＣＮ＋Ｂｅｒｔ在三个数据集上取得了与


现有最新方法可比较的性能
。


表
３
－２ＡＴＳＡ任务实验主要结果对比


ＭｏｄｅｌｓＲｅｓｔａｕｒａｎｔ
－
１４
Ｌａｐ
ｔｏｐ
－
１４
ＭＡＭＳ



Ａｃｃ
．（％
）Ｆ
ｌ
（
°／〇
）Ａｃｃ
．（％
）Ｆ
ｌ
（
°／〇）Ａｃｃ
．
（％）Ｆ
ｌ
（
°／〇）


ＡＴＡＥ
－ＬＳＴＭ７７
．２０
－６８
．７０


ＩＡＮ７８
．６０
－７２
．
１０６２
．８７
－－


ＣＤＴ８２
．３０７４
．０２７７
．
１９７２
．９９
－
－


ＡＳＧＣＮ８０
．７７７２
．０２７５
．５５７
１
．０５
－
－


Ｒ
－ＧＡＴ８３
．３０７６
．０８７５
．９３６３
．
１８
－
－


Ｓｅｎｔｉｃ
－ＧＣＮ８４
．０３７５
．３８７７
．９０７４
．７
１
－
－


Ｄｕａｌ
－ＧＣＮ８４
．２７７８
．０８７８
．４８７４
．７４
－
－


Ｂ
ｉｓｙｎ
－ＧＡＴ８７
．４９８
１
．６３８２
．４４７９
．
１５８４
．９０８４
．４３


ＯｕｒＳＡＤＣ
－ＧＣＮ８４
．３５７９
．７２７９
．４７７５
．２９７９
．９４７６
．
１６


ＳＡＤＣ
－ＧＣＮ＋


８７
．５５８２
．４２８２
．８４７９
．９８８４
．４８８２
．７
１


Ｂｅｒｔ


具体分析来看
，
１
）与单纯基于注意力的方法相比
，例如ＡＴＡＥ
－ＬＳＴＭ
，
ＩＡＮ
，


ＳＡＤＣ
－ＧＣＮ模型利用了句法结构建立了词之间的依赖
，
能够对句子进行语法信


息方面的建模
＝
２
）
与单纯基于句法依存树的模型相比
，
例如ＡＳＧＣＮ
、
Ｒ
－ＧＡＴ
，


ＳＡＤＣ
－ＧＣＮ取得了更好的结果
，
这说明了基于自注意力机制的语义通道模型有


效地融合了语义信息
，
能够很好地处理缺失语法结构的短文本
＝
３
）
与基于句法


依存树的语法通道和基于自注意力机制语义信息的双通道模型相比
，
例如Ｄｕａｌ
－


ＧＣＮ
，ＳＡＤＣ
－ＧＣＮ也取得了更好的效果
，
说明了引入外部知识分别对两个通道


信息进行增强可以更好的利用语法信息和语义信息
。４
）与基于知识增强的
Ｓｅｎｔｉｃ
－


ＧＣＮ模型相比
，ＳＡＤＣ
－ＧＣＮ同样取得了更好的结果
，
说明了双通道特征之间的


互补性
。


３７


北京邮电大学工程硕士学位论文


总的来说
，
通过定量实验
，
本文所提出的
ＳＡＤＣ
－ＧＣＮ模型取得了与现有最


新方法可比较的性能
，
具有
一定的优势
。


３
．６
．６消融实验与分析


ＳＡＤＣ
－ＧＣＮ模型主要包括语义通道
，语法通道
。为了进
一步分析
ＳＡＤＣ
－ＧＣＮ


模型的各个组件的有效性
，
本文进行了以下消融实验分析
。


消融实验结果如表
３
－３所示
，
对比方法如下
：


１
）ＳＡＤＣ
－ＧＣＮｗ／ｏＫＳｙｎ
－ＧＣＮ表示完整模型中不含
Ｓｙｎ
－ＧＣＮ模块
。


２
）ＳＡＤＣ
－ＧＣＮｗ／ｏＫＳｅｍ
－ＧＣＮ表示完整模型中不含
Ｓｅｍ
－ＧＣＮ模块
。


３
）ＳＡＤＣ
－ＧＣＮｗ／ｏｓｅｎｔｉｃ＆ｐｓ表示完整模型中不含情感增强方法和词性增


强方法
。


４
）ＳＡＤＣ
－ＧＣＮｗ／ｏｓｅｎｔｉｃ表示完整模型中不含情感增强方法
。


５
）ＳＡＤＣ
－ＧＣＮｗ／ｏ
ｐｓ表示完整模型中不含词性增强方法
。


６
）ＳＡＤＣ
－ＧＣＮｗ／ｏｗｅ表示完整模型中不含词向量增强方法
。


表
３
－３
消融实验主要结果


ＭｏｄｅｌｓＲｅｓｔａｕｒａｎｔ
－
１４Ｌａｐｔｏｐ
－
１４ＭＡＭＳ


Ａｃｃ
．
（％）Ｆ
ｌ
（％
）Ａｃｃ
．（％）Ｆ
ｌ
（
°／〇）Ａｃｃ
．
（％）Ｆ
ｌ
（％）


ＳＡＤＣ
－ＧＣＮｗ／ｏ


８
１
．９８７６
．８７７６
．３２７２
．０２７６
．４２７３
．０３


ＫＳｙｎ
－ＧＣＮ


ＳＡＤＣ
－ＧＣＮｗ／ｏ


８２
．４３７７
．４
１７７
．３０７３
．８７７７
．２９７４
．
１６


ＫＳｅｍ
－ＧＣＮ


ＳＡＤＣ
－ＧＣＮｗ／ｏ


．
？８２
．９０７６
．８９７７
．
１３７３
．２２７７
．９３７４
．９
１


ｓｅｎｔｉｃ＆
ｐｓ


ＳＡＤＣ
－ＧＣＮｗ／ｏ


．８３
．５
１７８
．２０７７
．７９７３
．９６７８
．４０７５
．２３


ｓｅｎｔｉｃ


ＳＡＤＣ
－ＧＣＮｗ／ｏ


８３
．８２７８
．３８７８
．９５７４
．
１８７８
．８７７５
．４７


ｐｓ


ＳＡＤＣ
－ＧＣＮｗ／ｏ


８２
．９６７８
．０３７７
．６８７４
．０
１７７
．６８７４
．５３


ｗｅ


ＳＡＤＣ
－ＧＣＮ８４
．３５７９
．７２７９
．４７７５
．２９７９
．９４７６
．
１６


如表３
－３所示
，
ＳＡＤＣ
－ＧＣＮｗ／ｏＫＳｙｎ
－ＧＣ模型在数据集Ｒｅｓｔａｕｒａｎｔ
－
１４上Ａｃｃ
．


和
Ｆ
丨性能分别下降了２
．３７％和
２
．８５％
，
在其他数据集上也有所下降
，
这表明缺


乏语法信息模块
，模型不能很好地捕获句子的句法结构
，模型损失了
一定的句法


结构信息
。
ＳＡＤＣ
－ＧＣＮｗ／ｏＫＳｅｍ
－ＧＣＮ模型在数据集Ｒｅｓｔａｕｒａｎｔ
－
１４上Ａｃｃ
．和Ｆ
１


分别下降了１
．９２％和
２
．３
１％
，
在其他数据集有着不同程度地下降
。
这表明缺乏语


义信息模块
，
模型不能很好地捕获句子的语义结构
。
ＳＡＤＣ
－ＧＣＮｗ／ｏｓｅｎｔｉｃ模型


３８


第三章
情感感知的双通道图卷积神经网络的方面词情感分析研究


在数据集Ｒｅｓｔａｕｒａｎｔ
－
１４上Ａｃｃ
．和
Ｆ
１指标分别下降了０
．８４
°／。和
１
．５２％
，
在其他几


个数据集上Ａｃｃ
．和
Ｆ
１
．都有所下降
。这验证了情感増强方法的有效性
，
说明了使


用外部情感知识库增强依存树矩阵可以更好地帮助模型捕获到句子中的情感表


达部分
＜：
ＳＡＤＣ
－ＧＣＮｗ／ｏｐｓ模型在数据集Ｒｅｓｔａｕｒａｎｔ
－
１４上
Ａｃｃ
，和
Ｆ
１指标分别


下降了０
．５３％和
１
．３４％
，
在其他数据集上指标同样都有所下降
。
这验证了词性增


强方法的有效性
，也说明了使用外部词性知识来增强依存树矩阵可以更好地帮助


模型捕获到句子中含有
“ 否定
” 语义的情感表达
。
ＳＡＤＣ
－ＧＣＮｗ／ｏ
ｓｅｎｔ
ｉｃ＆ｐｓ模


型在三个数据集上Ａｃｃ
．和
Ｆ
１性能分别下降较多
。
这说明了两种知识增强方法的


有效性
。
ＳＡＤＣ
－ＧＣＮｗ／ｏｗｅ性能下降也进
一步说明了词向量层面的知识增强的


有效性
。


３
，６
，７
图卷积层数影响


为了进
一步研宄
ＳＡＤＣ
－ＧＣＮ模型中
ＧＣＮ层数的影响
，
本文在
Ｒｅｓｔａｕｒａｎｔ
－


１４数据集上评估了本文提出的
ＳＡＤＣ
－ＧＣＮ
模型
，
并且将
ＧＣＮ
层数设置在


｛
］
，２
，３
．４
．５
，６
，７
｝层的范围内
。为了研究不同模块对实验结果的影响
，
本文分别评估


了ＫＳｅｍ
－ＧＣＮ模块和ＫＳｙｎ
－ＧＣＮ模块
。
实验结果如图
３
－６
，
图
３
－７
所示
。


根据实验结果可知
，
２层的
ＫＳｙｎ
－ＧＣＮ和
２层的ＫＳｅｍ
－ＧＣＮ是最佳的参数


组合
。从实验结果可以观察出
，
当图神经网络层数太少时
，
信息无法通过图结构


聚合远距离节点的信息
，
当前节点只能捕获周围邻居节点的信息
，
图中节点和结


构的信息捕获太少
＝
此外
，
当ＧＣＮ层数层数太多
，
结构过深时
，
图中的每个节


点都将聚合图中大多数节点的信息
，这会导致大部分节点聚合了过多的噪声信息
，


表示变得趋于相似
。


？ＫＳｅｍ＾ＣＮ


｛
Ｉ


芑
７０
－
＼


＜６５
－


＼


６０
－
＼


５５
、
？


５０
－


１２３４５６７


ＮｕｍｂｅｒｏｆＫＳｅｍ
－ＧＣＮ
ｌａｙｅｒｓ


图
３
－６ＫＳｅｍ
－ＧＣＮ层数对模型
Ａｃｃ
．的影响


３９


北京邮电大学工程硕士学位论文


８５＋ＫＳｙｎ
￣ＧＣＮ


ｒ＼


６０
－
＼


５５
－
＼


５０
？


１２３４５６７


ＮｕｍｂｅｒｏｆＫＳｙｎ
－ＧＣＮ
ｌａｙｅｒｓ


图
３
－７ＫＳｙｎ
－ＧＣＮ层数对模型Ａｃｃ
．的影响


３
．６
．８案例分析


表
３
－４展示了３个从测试集中挑选的例子
，ｐｏｓ
ｉｔ
ｉｖｅ
、ｎｅｇａｔｉｖｅ分别表示正向
、


负向情感极性
。表中用
（预测值
，标签值）来对模型预测结果和真实结果进行对比
。


从表中可以看出
，
本文提出的模型
ＳＡＤＣ
－ＧＣＮ能很好地对句子中方面词的情感


极性做出准确的预测
。


表
３
－４
案例分析


案例句子方面词ＳＡＤＣ
－ＧＣＮ


ｃ」（ｐｏｓ
ｉｔ
ｉｖｅ
，


ｆｏｏｄ
．
．


Ｉ
？Ｇｒｅａｔｆｏｏｄｂｕｔｔｈｅｓｅｒｖ
ｉｃｅｗａｓｄｒｅａｄｆｕ
ｌ
．

：



．（ｎｅｇａｔｉｖｅ
，


ｓｅｒｖ
ｉｃｅ
．


ｎｅｇａｔ
ｉｖｅ）


＇ｎ〇（ｎｅｇａｔ
ｉｖｅ
，


．ｉ１Ｔ
ｒ
．，
，Ｗ
ｉｎｄｏｗｓ８
．
、


２
．Ｄ
ｉｄｎｏｔｅｎ
ｊｏｙｔｈｅｎｅｗＷ
ｉｎｄｏｗｓ８ａｎｄ

ｎｅｇａｔｉｖｅ
）


ｔｏｕｃｈｓｃｒｅｅｎｆｕｎｃｔｉｏｎｓ
．
，（ｎｅｇａｔ
ｉｖｅ
，


ｔｏｕｃｈｓｃｒｅｅｎ
＂
．
、



ｎｅｇａｔ
ｉｖｅ
）


ｆｏｏｄ
（ｐｏｓ
ｉｔ
ｉｖｅ
，


３
．Ｔｈｅｆｏｏｄｉｓｓｕｒｐｒｉｓ
ｉｎｇ
ｌｙｇｏｏｄ，ａｎｄｔｈｅ
ｐｏｓ
ｉｔ
ｉｖｅ）


ｄｅｃｏｒｉｓｎ
ｉｃｅ
．
，（ｐｏｓ
ｉｔｉｖｅ
，


ｄｅｃｏｒ
？？
、




ｐｏｓ
ｉｔｉｖｅ）
４０


第三章情感感知的双通道图卷积神经网络的方面词情感分析研究


３
．７本章小结


本章对ＡＴＳＡ任务及当前模型存在的问题进行了研宄
，提出
一种情感感知的


双通道图卷积神经网络模型框架
ＳＡＤＣ
－ＧＣＮ
。
随后详细介绍了该模型的核心模


块
：
基于知识增强的语法通道模块
、
基于知识增强的语义通道模块
。


基于知识增强的语法通道模块为了更好地捕捉
“ 方面词
”
、
“ 情感词
” 节点及


“ 否定
” 含义词节点之间的关系
，采用图卷积神经网络作为编码器
，将基于情感


信息和词性信息的依存句法分析矩阵作为图结构
，对图中的单词节点进行特征聚


合与更新
，得到包含增强的句法信息的特征向量
。为了更好地捕捉句子中词与词


之间的语义关系
，基于知识增强的语义通道模块利用外部知识库对单词向量进行


词向量表示层面的增强
，并通过图卷积神经网络抽取出包含知识语义的方面词特


征向量
。


紧接着的实验部分介绍了ＡＴＳＡ任务实验的主要环境
、实验所使用的数据


集
、评估指标和基线模型方法
。然后
，通过定量实验
、
消融对比实验和案例分析


展示等
，验证了本文提出的模型
ＳＡＤＣ
－ＧＣＮ在方面词情感分析任务上的有效性
。


４
１


北京邮电大学工程硕士学位论文


４２


第四章
基于知识增强的多通道图卷积神经网络的方面类别情感分析研究


第四章基于知识增强的多通道图卷积神经网络的方面类别


情感分析研究


目前大部分方面类别情感分析模型忽略了对方面类别与句中相关概念词关


系的建模和利用
。方面类别往往不会显式地出现在句子之中
，是
一个在数据集中


预先定义好的抽象上位概念
。建模方面类别与句中相关下位词之间的语义关系才


能进
一步地对方面类别做出正确的情感极性预测
。然而
，现有的研究工作利用句


子内部的结构来对上述关系建模
，
忽略了利用外部知识来解决这
一问题
。
此外
，


这些模型也忽略了对语义信息和语法信息等的综合建模和利用
，简单利用单
一通


道信息
，
不能同时捕获长文本和短文本的语法和语义特征
。
因此
，
本章对基于外


部知识库的相似度函数进行研宄
，
并通过相似度函数构建方面类别相似度矩阵
。


同时
，
本章探索了
一种多通道特征抽取和基于注意力机制的特征融合方法
，对句


子的语义信息和语法信息进行综合利用进行
。实验结果表明
，模型能够更好地捕


捉到句子的语义和语法信息
，
以及精准地建模方面类别词与句中词的关系
。


４
．
１
基于知识增强的多通道图卷积神经网络算法


？
．
．
．６
．
．
．
一■
：
１


■？〇

Ｉ１


：ＣＯ
— Ｃ▼
一ｒＳ
＇
１



ａｔｔｅｎｔ
ｉｏｎＳｅｍａｎｔ
ｉｃ
－ＧＣＮＭｏｄｕ
ｌｅ
？
二
—
」


ｗ１ｆＳ厂「
一二二
：，
—
＾
一
＊２ｒｍ巡
：立


二Ｅ：—
．
＂
．ｉ
．
．
．
－ｖ
：麵
：


；；
［
＝
＾：
；
：￡；；
；
；
；ＭＭ
：
：〇



■
Ｉ
｜
｜
］
Ｌ
｜


ｗ６
１
１
１
１
１

、
…

■
：

ｄｅｐｅｎｄｅｎｃｙＳｙｎ
ｔａｃｔ
ｉｃ
－ＧＣＮ
，
、
：


Ｉ
Ｉ
？

－
＞
、
．
＊ｖ

＊


聲ｆ
＇
［〇〇３
ｊ
＇


Ｚ— ＊—
■
｜？
’
Ｘ
．
—？藍；：石
：



Ａｓｏｅｃｔ、
二
＇。＊咖
．
°
＂
Ｑ
－？ＯＣ
：Ｖｃ


＾
．
ｒ？Ｅｈａｎｃｅｍｅｎｔ？
？
二二＾：：二二


Ｃａｌｅ９〇ｒｙＭｏｄｕ
ｌｅ
卜


Ｋｎｏｗ
ｌｅｄｇｅ
－ＧＣＮ


ｓ
ｉｍ
ｉ
ｌａｒ
ｉ
ｔｙ


图
４
－
１ＫＥＭＣ
－ＧＣＮ馍型框


本文提出
一种基于知识增强的多通道图卷积神经网络模型框架
，
即


４３


北京邮电大学工程硕士学位论文


ＫｎｏｗｌｅｄｇｅＥｎｈａｎｃｅｄＭｕｌｔｉ
－Ｃｈａｎｎｅ
ｌＧｒａｐｈＣｏｎｖｏｌｕｔｉｏｎａｌＮｅｔｗｏｒｋ（ＫＥＭＣ
－ＧＣＮ）〇


该框架在外部知识通道利用外部知识库来延伸扩展方面类别的语义
，通过语义相


似度函数计算方面类别与句子中相关词语的相似性
，以建立方面类别与句子中相


关词语的关系
，
并通过Ｋｎｏｗｌｅｄｇｅ
－ＧＣＮ进行更新节点表示
，
从而解决方面类别


词不显式出现在句子中的问题
。
同时
，该框架同时利用语义和语法两个通道学习


不同的特征
，以缓解长句中噪声问题和短句中句子缺乏句法结构问题
＜＝总体而言
，


本文对输入文本进行依存句法分析
，根据依存分析结果构建语法信息通道图
，并


通过
Ｓｙｎｔａｃｔｉｃ
－ＧＣＮ进行更新
，
从而得到句子的句法信息的表示
。
同时
，
将词作


为节点构建
一个完全连通的加权图
，并利用注意力机制得到注意力分数矩阵
，作


为语义信息通道图
，通过
Ｓｅｍａｎｔｉｃ
－ＧＣＮ进行更新
，得到句子的语义信息的表示
。


ＫＥＭＣ
－ＧＣＮ模型整体框架如图
４
－
１所示
，
该模型主要包括输入编码层
、
语


法感知通道模块
、语义感知通道模块
、方面类别知识增强模块
、多通道特征融合


模块以及情感分类器
。


４
．２基于ＷｏｒｄＮｅｔ的语义相似度


ＷｏｒｄＮｅｔ
［５３
］是
一种基于本体的大规模英语词汇数据库
。
ＷｏｒｄＮｅｔ
［５４
］中的基本


单元是
“ 概念
”
，
概念都由对应的同义词集合进行表达
。
概念与概念之间存在人


工定义好的语义关系
，
上下位关系最为常见
。
ＷｏｒｄＮｅｔ中只有名词
、动词
、形容


词和副词四种词性
，
四个语义网相互之间独立
，
不存在边进行连接
。
“
ｉｓ
－ａ
” 关系


是名词语义网中最多的上下位关系
，
包含丰富的语义关系
。


基于本体的语义相似度计算方法
［５５
］主有以下几种方法
，基于距离的计算方法
、


基于信息量
ＩＣ（
ＩｎｆｏｒｍａｔｉｏｎＣｏｎｔｅｎｔ）
的计算方法和基于概念特征的计算方法
。


基于距离的计算方法通过计算概念节点之间的最短路径距离来计算相似度
。两个


概念节点在本体中的距离越近
，
它们之间的相似度越高
。
基于
１Ｃ的计算方法通


过概念节点自身涵盖的语义信息来计算相似度
。
１Ｃ的大小代表了信息量的多少
，


每个单词的信息量通过语料库计算得到
，单词在语料库中出现的频率越低
，
则信


息量越大
。基于特征的方法通过概念之间共享信息
，
比如两个概念在ＷｏｒｄＮｅｔ中


释义的覆盖率
，
来计算语义相似度
。


４４


第四章
基于知识增强的多通道图卷积神经网络的方面类别情感分析研究


４
．３编码器


４
．３
．
１
输入层


在输入层通过词向量模块
，
将文本中每个单词映射为
一个ｄｗ维度的分布式


词向量４
。
然后
，
使用双向长短期记忆神经网络（Ｂ
ｉ
－ｄ
ｉｒｅｃｔｉｏｎａｌＬＳＴＭ
，
Ｂ
ｉ
－ＬＳＴＭ）


对每个词向量ｘ
，
？进行编码
，
得到隐状态表示
。
具体而言
，
输入单词４通过前向网


络计算得到前向向量ｈｆ
，通过后向网络计算得到后向向量
／ｉｆ
。将两者进行拼接
，


得到
［＜
；吋］
。
拼接后的向量代表相应时刻的隐层状态
，
可表示为
：


＾
＝Ｂ
ｉ
－
ＬＳＴＭ
（＾），ｅＲ
２ｘｄ＾（４
－１）


４
．３
．２基于依存句法分析的语法图通道


针对长文本而言
，软注意力机制可以高效地捕捉到词与词之间的关系
，并对


这种关系进行建模
。然而单纯的软注意力机制往往会关注到句子中的每个词
，这


不可避免地会引入噪声
，
即将与当前词不太相关的单词特征聚合到当前词表示
。


特别地
，对于方面级情感分析任务而言
，会把不相关的情感表达词聚合到方面词
，


从而造成错误的情感分析
。当句子中的方面类别的下位词与情感表达词在句子中


相距较远时
，本文通过句法依存树分析得到的句子结构帮助建立起方面部分与情


感部的直接联系
，
从而缩短它们之间的距离
，
降低方面类别相关词与情感词之间


冗余长文本对情感极性分类的影响
。


Ｗ１ｒ
￣
ａ
￣
■
■］
丨


ｗ４
：＜
ＩＴｈｅｐ
ｉｚｚａ
ｉｓｇｒｅａｔｂｕｔｅｘｐｅｎｓ
ｉｖｅ
；
〇
．Ｍ


ｗ５
Ｉｉ
— 
．
．
．
．
．
．—
ｍ
￣
ｎ
￣Ｈ


ｗ６
；
；

ｄｅｐｅｎｃｅｎｃｙ
ｔｒｅｅｄｅｐｅｎｃｅｎｃｙＳｙｎｔａｃｔ
ｉｃ
－ＧＣＮ


图
４
－２
基于句法依存树的语法通道


本文设计的基于依存句法分析的语法图神经网络模块
，
如图
４
－２所示
。具体


地
，
首先使用
Ｓｐａｃｙ对文本进行分析
，
然后根据算法
３将分析结果句法依存树转


化为图结构
，
并表示为如下邻接矩阵
：


Ｓｙ（
ｌ
，
如果
Ｕ＾Ｗ
／有边连接


＝
＼
（４
－２）


１］１〇
，
其他


为了得到含有语法信息的方面词节点特征表示
，
本文采用图卷积神经网络作


为编码器
，
将基于依存句法分析矩阵Ｄ
ｆ／作为图结构
，
将经过
Ｂ
ｉ
－ＬＳＴＭ编码过的


隐藏状态／／＝作为图中节点的初始化表示
，
通过
Ｓｙｎｔａｃｔ
ｉｃ
－ＧＣＮ对


４５


北京邮电大学工程硕士学位论文


图中的单词节点进行特征聚合与更新
，将句法结构信息融入到节点的特征中
。根


据图卷积计算式
（３
－７
）
，
语法通道模块的最终表示
如下
：


Ｈ
ｓｙｎ＝
［ｈ
ｓｙｎ
ｆｈ
ｓｙｎ
（４
－３）


算法
３
：
基于依存句法关系的邻接矩阵构建


输入
：
句子
ｓ单词序列｛
：＾心
，
…
，ｘ
ｔ｝
，
句子的依存句法树
ｄｅｐｅｎｄｅｎｃｙ（ｓ）


输出
．
？
基于句法依存树的邻接矩阵


１ｆｏｒ￡＝１ｎｄｏ


２ｆｏｒ；
＝１ｎｄｏ


３
ｉｆｄｅｐｅｎｄｅｎｃｙ［ｗ
ｉ
，Ｗ
ｊ）Ｅｄｅｐｅｎｄｅｎｃｙ＇ｓ）ｏｒｉ＝
ｊｔｈｅｎ


４Ｄ
ｔｊ
＾１


５ｅｌｓｅ


６Ｄ
ｌＪ
＜ｒ
－０


７ｅｎｄｉｆ


８ｅｎｄｆｏｒ


９ｅｎｄｆｏｒ


４
．３
．３基于自注意力机制的语义图通道


依存句法解析特别适合符合句法结构的文本
，通过依存句法解析工具可以获


得较为准确的解析结果
。然而
，部分网络评论数据是没有严谨语法结构的短文本
，


使用依存句法分析反而可能会产生错误的解析结果
，从而影响后续情感分析结果
。


ｗｉ｜〇
￣
〇ｊ
「
丄


ｗ２
丨
…」Ｊ（ＭＨ
．ｑ


ｗ３
＿＾Ｓｅ
ｌｆＡｔｅｎｔ
ｉｏｎ＾＾
＾


ｗ４
…
…
Ｍｏｄｕ
ｌｅ〇
？
？
？
？
？
？
？
？
？
：０


ｗｓ
：
—
ｃｊ＾
ｙ■ｍ


ｗ６ｆＣ［〇
＇＾


ａｔｔｅｎｔ
ｉｏｎｓｃｏｒｅＳｅｍａｎｔ
ｉｃ
－ＧＣＮ


图
４
－３
基于
自注意力机制的语义通道


为解决上述问题
，本文使用通过自注意力机制捕捉不含语法结构的短句中词


与词之间的语义关系
，如果两个单词表达的语义相关
，
则这两个单词语义相似度


更高
，
通过自注意力机制得到的分值更高
。
相反
，
弱相关的词汇之间注意力分值


低
。
注意力分值在
０到
Ｉ之间
，
由模型在训练中学习得到
。


４６


第四章
基于知识增强的多通道图卷积神经网络的方面类别情感分析研宄


如图
４
－３所示
，
基于自注意力机制构建以词为节点的无向全连通有权图
，
并


在训练中得到相应的注意力得分矩阵Ｚ
ｆｆ
１
。
矩阵
中的元素数值代表相应单


词之间的相关性得分
，
可以用公式表述为
：


＾（Ｑ＾Ｑｘ｛ＫｗＫｙ＼


Ａ
ｉｊ
＝
Ｓ〇ｆ
ｔｍａＸ
（
—
７＾
—
Ｊ（４
－４）


ｄｆ
ｃ
＝
ｉ
（４
－５）


其中
，
（３
和
Ｋ是输入
，
和
州
〃都是可学习的参数权重矩阵
，
ｄ是输入节点


特征的维度
，
／Ｉ是注意力头的数量
。


根据式
（３
－７
）
，
将经过
Ｂｉ
－ＬＳＴＭ编码过的隐藏状态
心｝作为图


中节点的初始化表示
，注意力得分矩阵
作为图结构
，通过
Ｓｅｍａｎｔｉｃ
－ＧＣＮ对


节点特征进行更新
，
语义通道最终表示
如下
：


Ｈ
ｓｅｍ＝
＾
ｓｅｍ
）ｈ
ｓｅｍ
，
，咕
ｍ
）（４
－６）


４
．３
．４基于知识增强的知识图通道


在ＡＣＳＡ任务中
，
预先定义好的方面类别可能不会显式地出现在句子之中
，


例如
：
“
Ｔｈｅｂｅｅｆａｎｄｎｏｏｄ
ｌｅｔａｓｔｅｇｏｏｄ
，ｂｕｔｔｈｅｓｅｒｖ
ｉｃｅｉｓ
ｐｏｏｒ．
”
该任务需要预测方


面类别
“ ｆ
ｏｏｄ
” 的情感极性
，
而
“ ｆ
ｏｏｄ
” 这
一方面类别并没有显式地出现在例句


之中
。
“
ｆｏｏｄ
”这
一方面类别是
一个上位概念
，是食物的总称
，而
“
ｂｅｅｆ
” 和
“
ｎｏｏｄｌｅ
”


是
“ ｆ
ｏｏｄ
” 这
一方面类别的下位概念
，是食物的具体形式和种类
。在整个句子中
，


只有
“
ｎｏｏｄｌｅ
” 和
“
ｂｅｅｆ
” 是食物相关概念
，其他词与这
一概念无关
。
因而
，
当预


测
“ ｆ
ｏｏｄ
” 这
一方面类别的情感极性时
，实际上是需要预测出
“
ｂｅｅｆ
” 和
“
ｎｏｏｄ
ｌｅ
”


的整体情感极性
＝
如何建立方面类别
“
ｆｏｏｄ
” 与句子中
“
ｂｅｅｆ
” 和
“
ｎｏｏｄｌｅ
”
的


语义关系是解决问题的核心
＝


针对这
一问题
，
本文引入了Ｌ
ｉｎ相似度
ｔ５６
］来计算与度量方面类别与句中相关


词在概念层面上的内在相似性与语义相似度
。
Ｌｉｎ相似度认为
，
每个概念既有共


性信息
，也有自己单独的信息容量
，可以通过概念之间的信息共性与信息总量的


比值来计算两个概念的相似性
，
计算方法如下
：


？，、
２ｘｌｇｐ（
／ｓｏ（ｃ１
（ｃ２））２ｘ
／Ｃ
（
Ｚｓｏ（ｃ１
，ｃ２））


ｓｉｍＬ（Ｃｌ
，ｃ２）
＝
ｉｇｐ（Ｃｌ）＋ｉｇｐ（Ｃ２）
＝
／Ｃ（Ｃｌ）＋ｌＣ（ｃ２）
（３


其中
，
＾
（＾而）表示
处于ＷｏｒｄＮｅｔ
“
ｉｓ
＿ａ
”树中最深层的公共父节点
。


对于句子中的单词％
，通过
Ｌ
ｉｎ相似度可以计算得到其与方面类别在概念层


４７


北京邮电大学工程硕士学位论文


面的语义相似
，
Ｌ
ｉｎ相似度的结果在
０到
］之间
。
具体计算方法如下
：


Ｌ（ｗ
ｉ）
＝ｓｉｍＬ（Ｗ
ｉ
，ａｓｐｅｃｔ）
（４
－８）


其中
，
当
与ａｓｐｅｃｔ的词性不同时
，
Ｌ（ｗＤ
＝０
。


为了更好地利用ＷｏｒｄＮｅｔ建模方面类别与句中词的关系
，本文通过
Ｌ
ｉｎ相似


度根据算法４构建出方面类别相似度矩阵
，
如下所示
：


ｋｊ
＝＋Ｌ（ｗ
ｊ）
（４
－９）


其中
，
Ｌ〇ｖｄ表示方面类别与词叫在概念层面的相似度
，
Ｌ〇；）
表示方面类别与


词％在概念层面的相似度
。


ｗ１
丨
｜
上一
，

…〒


ｗ２
１Ｚ３Ｔ—厂
门
．
．
．Ｑ


ｗ３ｉ
＂Ｋｎｏｗｌｅｄｇｅ
；


ｗ４
；＝厂Ｍｏｄｕ
ｌｅ＾
ｍ°
＂


ｗ５
ｉ
＇
一
—


ｗ６



ｓ
ｉｍ
ｉ
ｌａｒ
ｉｔｙｓｃｏｒｅＫｎｏｗ
ｌｅｄｇｅ
－ＧＣＮ


图
４
－４
基于知识增强模块的语义相似度通道


如图
４
－４所示
，
为了得到包含外部知识信息的特征向量
，
本文将图卷积神经


网络作为编码器
，
将方面类别相似度矩阵＆；
．作为输入的图结构
，
经过
Ｂ
ｉ
－ＬＳＴＭ


编码过的隐藏状态＂
＝
｛
／１
１
，
／１
２
，
．
．
．
，
／１
７１丨作为图中节点的初始化表示
，
通过


Ｋｎｏｗ
ｌｅｄｇｅ
－ＧＣＮ对节点进行更新
，进
一步将外部知识融入到语句的编码信息中
。


根据式
（３
－７
）
，
基于知识增强的知识通道的最终输出如下
：


Ｈ
ｋｎｏ＝
ｊｉ
（４
－１０）


算法
４
：
基于Ｌｉｎ相似度的邻接矩阵构建


输入
：
句子
ｓ单词序列
…
，ｘ
ｔ｝
，
Ｌ
ｉｎ相似度Ｌ
（Ｗ
ｉ）


输出
：
基于Ｌ
ｉｎ相似度的邻接矩阵


１ｆｏｒｉ＝１
－
＞ｎｄｏ


２ｆｏｒ＿／＝１— ｎｄｏ


３ＥｎｈａｎｃｅｄｂｙＬ
ｉｎＳ
ｉｍ
ｉ
ｌａｒ
ｉｔｙＭｅａｓｕｒｅ


４Ｌ
ｉｊ
＜
－
Ｌ（＼Ｖ
ｉ）＋Ｌ（ｗ
ｊ）


５ｅｎｄｆｏｒ


６ｅｎｄｆｏｒ


４８


第四章
基于知识増强的多通道图卷积神经网络的方面类别情感分析研宄


４
．４
多通道融合模块


语法特征通道将句法结构信息融入到语句的编码信息中
，
生成了包含句法信


息的特征向量
。语义特征通道将句法结构信息融入到语句的编码信息中
，生成了


包含语义信息的特征向量
。多通道特征融合模块需要将两种来自不同通道的句子


表示进行融合
，
得到最终的多个通道特征的句子表示
。
受Ｗａｎｇ等人
的启发
，


如图
４
－５所示
，
本文利用采用注意力机制来设计多通道特征融合模块
，
来学习每


个通道的权重
，
对多通道的特征进行融合
：


（
＾
ｃｃ
Ｓｙｎ
，ｏ＾
ｓｅｒｎ）
＿Ａｔｔｅｎｔｉｏｎ
Ｗ
ｓｅ７ｎ）（４
－１１）


ｈ


个


，


０＜Ｓ
＞＜
、


ＡＡ


＾ｓｙｎ＾
－ｓｅｍ


ｔＪ一


ｓｏｆｔｍａｘ


Ａ


广
￣


ｓｃｏｒｅｓｃｏｒｅ


［
＇
Ｊ＾Ｊ


１ｋＡＡ


ｈＳｙｎｈｓｅｍ


图
４
－５
基于注意力机制的多通道融合模块


具体地
，
步骤如下
：


１
）
随机初始化
一个可学习的
ｑｕｅｒｙ向量
，
以节点
ｉ为例
，
得到第
ｉ个节点的句


法通道权重
：


Ｐ
ｌ
ｓｙｎ
＝
ｑ
Ｔ＇
ｔａｎｈ（ｗ
？
（ｈｉｙｎ）
Ｔ
＋（４
－１２）


４９


北京邮电大学工程硕士学位论文


ａ
ｌ
ｓｙｎ＝ｓｏｆｔｍａｘ
（／？ｉｙｎ）
＝—
７Ｒ
ｉ
、
（４
－１３）


ｅｘｐ＾ｉｙｎ）＋ｅｘｐ（＾ｅｍＪ


２
）
同理
，
得到另外
一个通道的关于第
ｉ个节点的权重
：


ａＬｍ
＝ｓｏｆｔｍａｘ（／？ｉｅｍ）
（４
－１４）


３
）将所有节点的权重组合得到权重矩阵
，得到最终的多通道融合向量表示
：


ａｓｙｎ
＝
［〇４ｙｎ］
，则
％ｙ
７ｔ
＝ｄｉａｇ（ａｓｙｎ）（４
－１５）


同理可得〇：５６７７１
，
有


＝＾Ｓｙｎ
＇
＾ｓｙｎ＾Ｓｅｍ
＇
＾ｓｅｍ（４
－１６）


４
．５分类与损失函数


基于外部知识的Ｋｎｏｗｌｅｄｇｅ
－ＧＣＮ将外部常识库中的知识信息融入到语句的


编码信息中
，生成了包含知识语义信息的特征向量
。该特征蕴含着了方面类别的


信息
，
具有外部先验知识信息
。
本文使用注意力机制
，
根据式
（４
－
１０
）将该通道


的特征向量表示为
并作为
ｑｕｅｒｙ
，
与另外两个通道的融合后的特征向量进


行计算
，
得到能够感知方面类别情感特征的最终表示
：


Ｐｔ
＝
Ｔ
ｎ
ｈ
ｔｈｆ
ｎｏ
（４
－１７）


＾— ＜
￡
＝
１


ｅｘｐ（）５
ｔ）
，… 、



Ｔ＾Ｔ
（４
－１８）


ｅｘｐ（＾（）


＝
Ｔａｔｈ
ｔ
（４
－１９）


ｙ
＝ｓｏｆｔｍａｘ
（Ｗ
ｐｈｃ＋ｂ
ｐ）
（４
－２０）


其中
，
Ｗ
ｐ与ｂ
ｐ为可学习的参数
，
ｔ是时间步和下标
。


情感分析任务的目标是通过最小化预测值和真实值之间的交叉熵损失来训


练情感分类器
。
其中
，
Ｓ是训练集的大小
，
Ｃ是类别的数量
，
夕／是真实分布
，
又是


正则项的权重
，
０表示所有可训练的参数
。


ｓｃ


￡＝
Ｈ
夕／
？
ｌ〇ｇ（ｙ／）＋Ａ
丨
丨０Ｉ
Ｉ
２（４
－２１）


ｉ＝ｌｊ
＝ｌ


５０


第四章
基于知识增强的多通道图卷积神经网络的方面类别情感分析研究


４
．６实验设计与分析


４
．６
．
１
实验数据


实验数据集主要由三个基准数据集组成
，分别是Ｒｅｓｔａｕｒａｎｔ
－
１５
、
Ｌａｐｔｏｐ
－
１５和


ＭＡＭＳ
＝
其中Ｒｅｓｔａｕｒａｎｔ
－
１５
、
Ｌａｐ
ｔｏｐ
－
１５来自ＳｅｍＥｖａｌ２０
１５语义评测
。
ＭＡＭＳ数


据集中的句子中含有多个不同情感极性的方面词或方面类别
。


所有的数据集包含三种情感极性
：
正向
、
中性和负向
。数据集中的每条句子


都被标注出了属性词和相应的情感极性
。
三个数据集的统计如表
４
－
１所示
：


表
４
－
丨ＡＣＳＡ任务数据集统计



Ｄａｔａｓｅｔ
Ｄ
ｉｖ
ｉｓｉｏｎ＃Ｐｏｓｉｔｉｖｅ＃Ｎｅｇａｔｉｖｅ＃Ｎｅｕｔｒａｌ


Ｔｒａｉｎ１０５８３４４４９


Ｒｅｓｔａｕｒａｎｔ
－
１５


Ｔｅｓｔ
４００

３
１９

４２


Ｔｒａｉｎ
￣
ｌＴ〇ｉ７６３１０６


Ｌａｐｔｏｐ
－
１５


Ｔｅｓｔ

５４０

３２８

７９


Ｔｒａｉｎ１９２９２０８４３０７７
￣


ＭＡＭＳ



Ｔｅｓｔ２４５２６３３９３


４
．６
．２评价指标介绍


本文选用准确率Ａｃｃｕｒａｃｙ和
Ｆ
Ｉ
值作为本模型性能的评价指标
。


４
．６
．３
实验参数及设置


为了验证
ＫＥＭＣ
－ＧＣＮ模型的有效性
，
本文采用深度学习框架
Ｐｙｔｏｒｃｈ开发


和训练模型并在基准数据集上进行了定性和定量实验
。
实验环境为Ｕｂｕｍｕ１６
．０４


服务器
，
ＧＰＵ为ＮＶＩＤＩＡＧｅＦｏｒｃｅＲＴＸ２０８０Ｔｉ显卡
。


文本输入阶段
，
釆用
Ｇ
ｌｏｖｅ词向量来初始化句子中的单词
，
维度为
３００维
。


紧接着
，使用Ｂ
ｉＬＳＴＭ作为句子特征编码器
，
隐状态表示维数为
３００维
。各个通


道的ＧＣＮ层数为
２
，
隐状态表示维数为
３００维
。


为了防止过拟合
，
本文把ｄｒｏｐｏｕｔ设置为
０
．３
。模型所有的权重初始化服从均


匀分布
。
在监督训练阶段
，
采用
Ａｄａｍ
ｔＭ优化器
，
学习率为０
．００１
，
数据批量大


小为
１６
。对于基于Ｂｅｒｔ的模型而言
，本文使用
ｂｅｒｔ
－ｂａｓｅ
－ｕｎｃａｓｅｄ
，维数为
７６８维
，


学习率为０
．０００２
。


４
．６
．４对比模型


本节选取了近年的针对
ＡＣＳＡ
任务的细粒度情感分析模型进行定量的性能


对比
，
包括
ＴＣ
－ＬＳＴＭ模型
、
ＡＴＡＥ
－ＬＳＴＭ模型
、
ＧＣＡＥ模型
、
ＣａｐｓＮｅｔ模型
、


５
１


北京邮电大学工程硕士学位论文


ＡＳ
－Ｃａｐｓｕ
ｌｅｓ模型
、
ＧＩＮ模型
、
ＭＩＭＬＬＮ模型
、
ＡＡＧＣＮ模型
。


以下简要说明对比模型
：


１
）ＴＣ
－ＬＳＴＭ
ｔ％模型
：该模型首次提出目标对象对句子情感分类结果的影响
，


提出将目标对象的表示和句子上下文表示进行拼接
，从而将目标对象的信息融入


到分类模型中
。


２
）
ＡＴＡＥ
－ＬＳＴＭＷ模型
：
该模型是对ＴＣ
－ＬＳＴＭ模型的改进
，将Ａｓｐｅｃｔ的表


示和隐状态表示拼接作为输入
。
同时
，该模型最早将注意力机制引入方面级情感


分析模型
，使用注意力机制对句中不同重要程度的单词给予不同的权重
，最后将


ＬＳＴＭ的输出与注意力计算得到与给定方面相关的句子加权表示
。


３
）ＧＣＡＥ
［
１２
］模型
：
该模型基于
ＣＮＮ
，
并行处理速度较快
，
通过卷积层不同


大小的卷积核捕捉句子的
ｎ
－
ｇｒａｍ特征
。
模型上层接了
一个门控机制用于保留与


方面类别有关的信息
。


４
）ＣａｐｓＮｅｔ
［５２
］模型
：
该模型提出了ＭＡＭＳ（Ｍｕ
ｌｔｉ
－ＡｓｐｅｃｔＭｕ
ｌｔｉ
－Ｓｅｎｔｉｍｅｎｔ
）数


据集
，
以及
一种基于胶囊网络的模型来学习方面类别和上下文之间的复杂关系
。


该模型使用
一个情感类别先验知识矩阵指导路由权重的调整
。


５
）ＡＳ
－Ｃａｐｓｕｌｅｓ＾］模型
：
该模型使用胶囊网络同时进行方面识别和情感分类


两个任务
，
不同的胶囊对应不同的方面类别
。
该模型在低层采用共享的＿对


两个任务进行联合学习
。


６
）ＭＩＭＬＬＮ
［２６坤莫型
：
该模型提出了
一种多实例多标记学习的网络框架
，
将


句子作为包
，
单词作为包中实例
。具体地
，将句子中表示方面类别的单词作为指


不方面类别的关键实例
，
并利用方面类别检测
（ＡｓｐｅｃｔＣａｔｅｇｏｒｙＤｅｔｅｃｔｉｏｎ
）
任务


辅助训练
。该模型首先找到文本里指示方面类别的词
，然后对指示词进行情感分


类
，
最终聚合指示词的情感得到方面类别的情感
。


７
）ＡＡＧＣＮ
ｔＷ模型
：
该模型利用外部知识库检索获取与方面类别高度相关的


扩展词
，
并利用统计得到的
Ｂｅｔａ分布概率来计算对方面类别的重要性
，
从而构


建出句子中词对的邻接矩阵
，
结合图卷积神经网络来聚合更新节点的特征
。


８
）ＳＲＧＮ
［６
１
］模型
：
该模型利用基于本体相似度的方法ＮＰＭＩ计算方面类别与


句子中词语的语义相似度
，构建出相似度单词和方面类别的异质图邻接矩阵
，结


合边控图卷积神经网络来聚合更新节点的特征
。
同时
，
该模型利用ＡＣＤ任务辅


助训练
。


４
．６
．５结果分析


表
４
－２展示了相关模型方法在不同数据集的性能结果
。
需要说明的是
，
由于


大部分研宄工作模型设计复杂且缺少有效的开源代码
，
因此
，
难以在与
ＫＥＭＣ
－


５２


第四章
基于知识増强的多通道图卷积神经网络的方面类别情感分析研究


ＧＣＮ
相似的实验环境下重复相关实验并评测
，
故表中的性能结果均沿用相关研


究论文中的指标或其他论文中复现的指标进行汇报展示
。


为了评估本文提的ＫＥＭＣ
－ＧＣＮ模型
，本文使用Ａｃｃｕｒａｃｙ
（Ａｃｃ
．
）和
Ｆ
ｌ
－ｓｃｏｒｅ


（Ｆ
１
）
作为主要评估指标
。
实验主要结果如表
４
－２所示
，
ＫＥＭＣ
－ＧＣＮ在三个数


据集上准确率和
Ｆ
１值指标超过了其他的方法
，
取得了较好的效果
＝


具体分析来看
，
１
）
与基于句法依存树和图卷积神经网络的模型相比
，
如


ＡＡＧＣＮ
，
ＫＥＭＣ
－ＧＣＮ取得了更好的结果
，这说明了基于自注意力机制的语义通


道模型有效地融合了语义信息
，能够很好地捕获到不符合语法结构短文本的语义


ｆｇ息
。


表
４
－２
实验主要结果对比


Ｍｏｄｅ
ｌｓＲｅｓｔａｕｒａｎｔ
－
１５Ｌａｐ
ｔｏｐ
－
１５ＭＡＭＳ


Ａｃｃ
．（％）Ｆ
ｌ
（
°／〇）Ａｃｃ
．（％
）Ｆ
ｌ
（％
）Ａｃｃ
．
（％）Ｆ
ｌ
（％
）


ＴＣ
－ＬＳＴＭ７６
．３９５８
．７０７４
．
１３６０
．０８
－
－


ＡＴＡＥ
－ＬＳＴＭ７８
．４８５９
．７７７５
．３２６３
．０２７０
．６３


ＧＣＡＥ７７
．５５５７
．４３７５
．３０６２
．８７７２
．
１０
－


ＣａｐｓＮｅｔ７８
．
１４６
１
．５７７４
．７
１６
１
．７５７３
．９９
－


ＡＳ
－Ｃａｐｓｕ
ｌｅｓ
－
－
－
－７５
．
１２
－


ＧＩＮ８
１
，
１７６２
．３８７５
．９３６３
．
１８
－
－


ＭＩＭＬＬＮ７８
．２７６０
．５９７５
．３０６
１
．３９７６
．４３
－


ＡＡＧＣＮ８２
．７９６７
．４３８０
．０２６５
．８７７７
．５２７６
．８９


ＳＲＧＮ７４
．６５４５
．６５８２
．
１２３９
．３
１７４
．０３５
１
．
１４


ＫＥＭＣ
－ＧＣＮ８３
．４６６８
．０４８２
．３３６６．７９７８
．２１７７．９０


２
）与基于注意力的方法相比
，
如ＴＣ
－ＬＳＴＭ
、
ＡＴＡＥ
－ＬＳＴＭ
，ＫＥＭＣ
－ＧＣＮ模


型利用了句法结构建立了词与词之间的直接关系
，可以缩短方面类别相关词与情


感表达词的距离
，避免其他词带来的噪声
。这进
一步说明
，基于句法依存树的句


法通道模型有效地融合了句法结构信息
。从整体来看
，基于多通道特征的ＫＥＭＣ
－


ＧＣＮ模型可以学习到语义通道和语法通道的互补信息
，
取得更好的结果
。
３
）与


基于外部知识的模型相比
，
比如ＡＡＧＣＮ和
ＳＲＧＮ
，ＫＥＭＣ
－ＧＣＮ模型通过外部


知识库
，
引入基于概念的相似度函数
，来更高效地建立方面类别与句子中相关词


的关系
，
取得了更好的效果
。
４
）
与多实例框架和胶囊网络模型相比
，
本模型的


结果同样更具有竞争力
。


总的来说
，
通过引入语法通道特征
、语义通道特征和知识通道特征
，
本文所


提出的模型与方法取得了更优的指标性能
，
具有
一定的优势和竞争力
＝


５３


北京邮电大学工程硕士学位论文


４
．６
．６消融实验与分析


ＫＥＭＣ
－ＧＣＮ模型主要包括语义特征通道
，语法特征通道以及知识增强通道
。


为了进
一步分析本文提出的ＫＥＭＣ
－ＧＣＮ模型的各个组件的有效性
，本文进行了


消融实验并进行了分析
。表４
－３展示了消融实验主要的结果
。如下对比方法如下
：


１）ＫＥＭＣ
－ＧＣＮｗ／ｏＳｙｎｔａｃｔｉｃ
－ＧＣＮ表示完整模型中不含Ｓｙｎｔａｃｔｉｃ
－ＧＣＮ模块
。


２
）ＫＥＭＣ
－ＧＣＮｗ／ｏＳｅｍａｎｔｉｃ
－ＧＣＮ表示完整模型中不含Ｓｅｍａｎｔｉｃ
－ＧＣＮ模块
。


３
）ＫＥＭＣ
－ＧＣＮｗ／ｏＫｎｏｗ
ｌｅｄｇｅ
－ＧＣＮ表不完整模型中不含Ｋｎｏｗ
ｌｅｄｇｅ
－ＧＣＮ


模块
。


４
）ＫＥＭＣ
－ＧＣＮｗ／Ｂｅｒ
ｔ模型表示使用预训练模型Ｂｅｒｔ对文本进行编码
。



４
－３
消融实验主要结果


ＭｏｄｅｌｓＲｅｓｔａｕｒａｎｔ
－
１５Ｌａｐ
ｔｏｐ
－
１５ＭＡＭＳ


Ａｃｃ
．
（％
）Ｆ
ｌ
（％
）Ａｃｃ
．（％
）Ｆ
ｌ
（％
）Ａｃｃ
．（％
）Ｆ
ｌ
（
°／〇
）


ＫＥＭＣ
－ＧＣＮｗ／ｏ


？８０
．４８６５
．７７８０
．４８６４
．０２７４
．６３７４
．０
１


Ｓｙｎｔａｃｔｉｃ
－ＧＣＮ


ＫＥＭＣ
－ＧＣＮｗ／ｏ


。
．
一
，８
１
．５５６６
．４３８
１
．６５６３
．８７７５
．
１０７４
．３８


Ｓｅｍａｎｔｉｃ
－ＧＣＮ


ＫＥＭＣ
－ＧＣＮｗ／ｏ


，
，
，７９
．８９６４
．５７７８
．７
１６２
．７５７３
．９９７３
．２０


Ｋｎｏｗｌｅｄｇｅ
－ＧＣＮ


ＫＥＭＣ
－ＧＣＮ８３
．４６６８
．０４８２
．３３６６
．７９７８
．２
１７７
．９０


ＫＥＭＣ
－ＧＣＮｗ／


８７
．２３７２
．６
１８６
．８０７
１
．
１０８２
．２０８０
．０２


Ｂｅｒｔ


本文以实验Ｒｅｓｔａｕｒａｎｔ
－
１５数据集为例
，对模型各个组件的有效性进行分析
。


ＫＥＭＣ
－ＧＣＮｗ／ｏＳｙｎｔａｃｔｉｃ
－ＧＣＮ的Ａｃｃ
？和Ｆ
１性能指标分别下降２
．９８％和２
．２７％
。


这表明缺乏语法信息模块
，使得模型不能很好地捕获句子的句法结构信息
，损失


了句法结构信息
。
ＫＥＭＣ
－ＧＣＮｗ／ｏＳｅｍａｎｔｉｃ
－ＧＣＮ的
Ａｃｃ
．和
Ｆ
１性能指标分别下


降
１
．９
１％和
１
．６
１％
。
这表明缺乏语义信息模块
，
使得模型不能很好地捕获句子的


语义信息
，特别是不能很好地捕获没有语法结构的短文本数据的词与词的语义关


系
ｃ


ＫＥＭＣ
－ＧＣＮｗ／ｏＫｎｏｗｌｅｄｇｅ
－ＧＣＮ模型在Ａｃｃ
．和Ｆ
１指标分别下降了３
．５７％


和
３
．４７％
。这验证了模型与基于知识增强的知识通道信息相结合可以获得比较好


的性能
。
模型在本组实验中
，
性能下降最多
，
这充分说明了Ｋｎｏｗｌｅｄｇｅ
－ＧＣＮ通


道的有效性
。也进
一步说明
，使用外部知识库建立方面类别词与句子中词与关系


的重要性
。
ＫＥＭＣ
－ＧＣＮｗ／Ｂｅｒｔ模型在各个数据上各项指标都有明显提升
，
这充


分说明了基于大量语料库的预训练语言模型ＢＥＲＴ包含了更丰富的语义知识
。


５４


第四章
基于知识增强的多通道图卷积神经网络的方面类别情感分析研宄


４
．６
．７
图卷积层数影响


为了进
一步研究
ＫＥＭＣ
－ＧＣＮ模型中
ＧＣＮ
层数对模型的影响
，
本文在


Ｒｅｓｔａｕｒａｎｔ
－
１５的数据集上对模型进行评估
，并将
ＧＣＮ
层数设置在
｛
１
，２
，３
，４
，５
，６
，７
｝


层的范围内
ｔ
将三个通道的ＧＣＮ层数作为参数进行参数组合
，
初始层数都设置


为
１
。


８５
－ｍ
Ｓｙｎｔａｃｔ
ｉｃ
－ＧＣＮ


｜
７０
＇＼


＜６５
＼


６０
－＼


５５
－
■


５０
１２３４５６７


ＮｕｍｂｅｒｏｆＳｙｎｔａｃｔ
ｉｃ
－ＧＣＮ
ｌａｙｅｒｓ


图
４
－６Ｓｙｎｔａｃｔ
ｉｃ
－ＧＣＮ层数对模型Ａｃｃ
．的影响


８５
－＋Ｓｅｍａｎｔ
ｉｏＧＣＮ


Ｉ
７。
．


＜６５
－＼


６０
－＼


５５＼＿＿＿．


５０
１２３４５６７


ＮｕｍｂｅｒｏｆＳｅｍａｎｔ
ｉｃ
－ＧＣＮ
ｌａｙｅｒｓ


图
４
－７Ｓｅｍａｎｔｉｃ
－ＧＣＮ层数对模型Ａｃｃ
．的影响


实验结果如图
４
－６
，图
４
－７
和图
４
－８所示
。根据实验结果
，２层
Ｓｙｎｔａｘ
－ＧＣＮ
、


２层
Ｓｅｍａｎｔ
ｉｃ
－ＧＣＮ和
Ｉ层Ｋｎｏｗ
ｌｅｄｇｅ
－ＧＣＮ是本模型最佳的参数组合
。实验结果


显示
，
当模型层数过多或过少时都不能达到最优性能
。


当图神经网络层数太少时
，
信息无法通过图结构聚合远距离节点的信息
，
当


前节点只能捕获周围邻居节点的信息
，图中节点和结构的信息捕获太少
。当ＧＣＮ


层数层数太多
，
结构过深时
，
图中的每个节点都将聚合图中大多数节点的信息
，


这会导致大部分节点聚合了过多的噪声信息
，
表示变得趋于相似
。
对于


５５


北京邮电大学工程硕士学位论文


Ｋｎｏｗ
ｌｅｄｇｅ
－ＧＣＮ而言
，
１层就能达到最好效果
，
因为该通道基于外部知识库
，
己


经具备先验知识
，
层数太深反而会引入噪声
。


ｇｇ
－ａ
－Ｋｎｏｗ
ｌｅｄｇｅ
－ＧＣＮ


－＼


ｆ
７０
＇
＼


〇
＼


＜
６５
－


６０
－＼


＼


５５
－


５０
－


１２３４５６７


ＮｕｍｂｅｒｏｆＫｎｏｗｌｅｄｇｅ
－ＧＣＮ
ｌａｙｅｒｓ


图
４
－８Ｋｎｏｗｌｅｄｇｅ
－ＧＣＮ层数对模型Ａｃｃ
．的影响


４
．６
．８案例分析


１
）注意力可视化


Ｆｏｏｄ
－


ＨＨＩＨＢ



■■


Ｓｅｒｖ
ｉｃｅ
－


ＩＶ
ｉ
ｉ
ｉ
ｉ
ｉ
Ｉ


ＺＺ＾＾＾＾
’９
°〇
＜


０
００
．
２０
．
４
．０
６０８
１
．
０


图
４
－９
注意力可视化


为了定性地展示方面类别相关词在句子中对情感分析的引导作用
，本文通过


注意力可视化的方法进行展示
。
如图
４
－９所示
，
以句子
“
Ｔｈｅｐ
ｉｚｚａ
ｔａｓｔｅｓｇｏｏｄｂｕｔ


ｔｈｅｓｅｒｖ
ｉｃｅｉｓｐｏｏｒ．
”
为例
，
句子中每个词的权重的大小可以通过颜色的深浅进行


表示
，
颜色越深表示权重越大
。
从图中可以观察得到
，
ＫＥＭＣ
－ＧＣＮ模型的注意


力可视化结果相对稀疏
，权重集中在方面类别相关词和情感观点词上
。对于方面


类别
“
ｆｏｏｄ
” 而言
，
模型更加关注句子中
“
ｐ
ｉｚｚａ
” 和
“
ｇｏｏｄ
”
；
而对于方面类别


“
ｓｅｒｖ
ｉｃｅ
”而言
，
模型则和关注到了
“
ｓｅｒｖ
ｉｃｅ
” 和
“
ｐｏｏｒ
”
。
因此
，
本文的ＫＥＭＣ
－ＧＣＮ


模型准确预测出方面类别
“
ｆｏｏｄ
” 和
“
ｓｅｒｖ
ｉｃｅ
” 的正确情感极性
。


２
）
案例分析


５６


第四章
基于知识增强的多通道图卷积神经网络的方面类别情感分析研究


表
４
－４展示了３
个从ＭＡＭＳ测试集中挑选的例子
，
ｐｏｓｉｔｉｖｅ
、
ｎｅｇａｔｉｖｅ和


ｎｅｕｔｒａｌ分别表示正向
、
负向情感极性和中性情感极性
。
表中用（预测值
，
标签值）


来对模型预测结果和真实结果进行对比
＝
从表中可以看出
，
本文提出的模型


ＫＥＭＣ
－ＧＣＮ能很好地对多个不同方面的情感极性做出准确的预测
。


表
４
－４
案例分析


案例句子
｜方面类别
｜
ＫＥＭＣ
－ＧＣＮ


ｆｏｏｄ
（ｐｏｓｉｔｉｖｅ
，


１
．ｔｈｅｓｅｒｖ
ｉｃｅｗａｓｈｏｒｒｉｂｌｅｔｈｅｎａｎｄｔｈｅ
ｐｏｓ
ｉｔｉｖｅ）


ｆｏｏｄｗａｓｄｅｃｅｎｔ
．
．（ｎｅｇａｔｉｖｅ
，


ｓｅｒｖ
ｉｃｅ
．
、



ｎｅｇａｔｉｖｅ）


＿
．（ｎｅｕｔｒａｌ
，


ｆｏｏｄ


２
．０ｕｒｗａｉｔｒｅｓｓｋｅｐ
ｔｆｏｒｇｅｔｔｉｎｇｏｕｒｄｒｉｎｋｓ
．


；



ｓｔａｆ⑴聊腦
、



ｎｅｇａｔｉｖｅ
）


ｆｏｏｄ
（ｐｏｓｉｔｉｖｅ
，


３
．Ｔｈｅｏｎｌｙｓａｖｉｎｇｇｒａｃｅｉｓｔｈｅｓｗｅｅｔｔｅａ
－
ｐｏｓｉｔｉｖｅ）


ｂｕｔｅｖｅｎｔｈａｔｉｓｎ
＇
ｔｗｏｒｔｈｔｈｅｗａｉｔ
．
．（ｎｅｇａｔｉｖｅ
，


ｓｅｒｖｉｃｅ
．
、


ｎｅｇａｔｉｖｅ
）


４
．７本章小结


本章对
ＡＣＳＡ任务与其存在的问题进行了研宄
。
提出了
一种基于知识增强


的多通道学习图卷积神经网络模型框架ＫＥＭＣ
－ＧＣＮ
。
随后详细介绍了该模型的


核心模块
，基于依存句法分析的语法图通道
、基于自注意力机制的语义图通道以


及基于知识增强的知识图通道
。随后
，介绍了基于注意力机制的多通道特征融合


模块
。


基于依存句法分析的语法图通道采用图卷积神经网络作为编码器
，将基于依


存句法分析矩阵作为图结构
，对图中的单词节点进行特征聚合与更新
，得到包含


增强的句法信息的特征向量
。基于自注意力机制的语义图通道通过图卷积神经网


络提取出单词包含知识语义的特征向量
。多通道特征融合模块对这两种特征进行


融合
，帮助模型利用两种通道之间的特征互补性
。基于外部知识增强的知识图通


道通过外部知识库扩展方面类别的语义
，能有效地捕捉方面类别与句中下位概念


的语义相似度
，
解决了方面类别不显式出现在句子中的问题
。


实验部分介绍了ＡＣＳＡ任务实验的主要环境
、实验所使用的数据集
、评估指


标和基线模型方法
。
然后
，
通过定量实验
、消融对比实验和案例分析展示等
，
验


证了本文提出的模型ＫＥＭＣ
－ＧＣＮ在方面词情感分析任务上的有效性
。


５７


北京邮电大学工程硕士学位论文


５８


第五章
面向用户评论的方面级情感分析系统设计与实现


第五章
面向用户评论的方面级情感分析系统设计与实现


本章将详细介绍面向用户评论的方面级情感分析系统的设计思想与实现技


术
。根据本文研宄的方面词情感分析和方面类别情感分析任务
，
系统需要实现用


户管理模块
、模型训练模块
、情感分析与可视化模块等具体的功能模块
。本章内


容主要包括系统的需求分析
、系统概要设计
、系统详细设计以及系统的实现与测


试
。


５
．
１
系统需求分析


越来越多的互联网巨头开始在本地生活领域进行布局
，直播电商
、兴趣电商


开始发力
，
在存量市场中争抢市场份额
＝对于本地生活类应用来讲
，
相关技术己


经积累得相当成熟
，
不存在显著的行业壁垒
。在技术水平相当的前提下
，
精细化


运营正成为争抢用户的关键
。精细化运营需要数据支撑
，特别是用户的评论数据
。


用户评论是分析用户行为
，构建用户画像的关键数据
，
同样也是后续构建个性化


推荐的基础
。


对于接入本地生活的商户而言
，其用户范围从实体门店的
一隅之地变为
一个


区
，甚至
一个城市
。每家商户都面临着机遇与挑战
，他们的经营范围变大的同时


必然面临着市场相互重叠
。要从众多商户激烈的竞争中脱颖而出
，必须时刻根据


用户的反馈做出策略调整
。用户反馈的评论数据蕴含着用户的情感喜恶
，具有十


分重要的应用价值
。
对这些数据的分析
，
可以帮助商家更容易地从菜品
、
服务
、


到环境等方方面面进行改善
，
从而提高顾客黏性
，
实现长期的销售数据增长
。


本系统的目标是设计和实现
一个面向评论的方面级情感分析系统
，该系统可


以同时供模型训练管理员和运营人员使用
。


５
．
１
．
１
功能性需求


（
１
）
用户管理模块


用户管理的主要功能保证用户只有完成注册并登录后
，
才能正常使用本情感


分析系统
，从而保证使用者和系统的数据安全
＝如图
５
－
１所示
，具体功能需求为
：


１
）注册功能
：
在登录前
，
用户需要先完成注册
，
成为本系统的用户
＝


２
）
登陆功能
：
系统的注册用户需要通过登陆功能来进入系统
，
完成后续操


作
。


３
）个人信息管理功能
：
用户可以对个人信息进行管理
，
包括查看和修改个


人信息等
，
如
：
用户名
、
邮箱
、
手机号
、
登录密码等
。


５９


北京邮电大学工程硕士学位论文


Ｔ＼＜Ｔ
＾Ｏ，
￣
＾


甩户
＜
ｉｎｃ
ｌｕｄｅ＞


＜
ｉｎｃ
ｌｕｄｅ＞


个人＾）


图
５
－
丨
用户管理模块用例图


（２
）模型管理模块


模型训练模块可供系统管理员使用
，
如图
５
－２所示
，
主要包括以下功能
：


１
）数据管理功能
：
包括数据的上传
，
下载等
。


２
）数据预处理功能
：
将上传数据处理成模型需要的格式
。


３
）模型可视化参数配置与训练功能
：在训练模型时
，可直接使用前端的参数


配置表单页输入参数进行参数调整和模型训练
。


系统管理员＼

Ｖ

、
＾
＜

ｉｎｃ
ｌｕｄｅ〉
、’


〈
ｉｎｃ
ｌｕｄｅ〉


图
５
－２模型管理模块用例图


（３
）情感分析与可视化模块


情感分析与可视化模块可供系统运营人员使用
，
如图
５
－３所示
，
主要包括以


下功能
：


１
）数据输入功能
：
用于数据的输入
。


２
）情感分析功能
：本模块基于前述的算法模型
，这些模型被封装成功能函数


或者网络ＡＰＩ
，
来提供情感分析的能力
。


３
）评论结果可视化功能
：
通过不同的输出形式
，
来展示情感分析的结果
。


６０


第五章
面向用户评论的方面级情感分析系统设计与实现



＾
＾情感分析


人＼
￣


用户、


结果可视少


图
５
－３
情感分析与可视化模块用例图


５
，
１
．２非功能性需求


（
１
）
易用性需求


本系统面向用户
，
在进行功能设计时
，应将提高用户使用效率
、方便用户使


用为作为功能设计的前提
，
从而保证用户可以获得最佳的体验效果
。
同时
，
应保


证系统的各模块内部的操作流程应符合普通用户的使用习惯
，应保证界面设计的


简单和简约
，
应保证本系统可方便用户快速部署和使用
。


（２
）可扩展性需求


在设计本系统时
，
应充分考虑可扩展性需求
。
应充分考虑到
，
随着系统使用


人数的变化而导致的后期数据库服务器
、逻辑服务器硬件后续的扩展
，
同时在进


行代码实现时
，要考虑要把当前功能封装成无状态服务
，方便后期的部署与微服


务的扩缩容
＝


（３
）兼容性需求


本系统的前端页面需要适配
ＩＥ、
Ｆ
ｉｒｅｆｏｘ和
Ｃｈｒｏｍｅ等各种主流浏览器
。


（４
）
安全性需求


本系统在安全性方面应该满足以下几个方面
：
用户密码等用户信息需要使用


算法加密方式进行存储
，保证用户信息的安全性
。当用户对系统数据进行操作前
，


都应该核对用户的身份信息
，对用户的操作权限等进行验证
，保证数据操作的安


全性
。本系统需要通过日志功能
，
记录用户对系统的各种操作行为
，
用于后期的


问题定位以及用户画像构建与用户行为安全性分析
。


５
．２系统概要设计


根据系统的需求分析
，本节将对面向用户评论的方面级情感分析进行总体功


能设计与系统架构设计
。


６
１


北京邮电大学工程硕士学位论文


５
．２
．
１总体功能设计


本节根据上
一节对本系统进行的需求分析
，对面向用户评论的方面级情感分


析系统进行模块化设计
。
如图
５
－４所示
，
该系统分为用户管理
、模型管理和情感


分析与可视化三大部分
；其中
，模型管理又细分为数据管理
、数据预处理
、模型


可视化参数配置与训练等模块
；
每个模块又细分为若干更小的模块
。


面向评论的细粒度情感分析系统


■
Ｉ
；
￣￣１
￣—
；
１
■


用户
模块
￣￣
１
丨
輕麵模块
１
丨
情感分析胡视髓块


ｉＩｉｉｉ１ｉ１Ｉｉ


ｒｒｉｒＥ
￣
ｉｒ＾ｎｒｎ门ｒｗ


用
用
人ｆｓ｜Ｉ数
情２


？ｊＳ￡ＩＩｉ｜｜Ｉ


ｉＩｉ和处
数
练可


册
录｜下
理
配Ａ挤
视


＿
｜
｜＿＿
｜Ｌ＾Ｊ［ｍｊＬ＿Ｊ
｜＿ｉｊ
｜＿
｜
｜＿
｜
｜＿
｜
｜＿Ｓ
＿


图
５
？４
总体功能设计图


５
．２
．２系统架构设计


系统的整体架构从上到下
，
分别是应用层
、模型层和数据层
。应用层主要服


务于运营用户和开发人员
，提供数据分析以及可视化分析等功能
。模型层为开发


人员提供了可视化调参和训练模型等功能
，同时训练好的模型可以为应用层提供


方面级情感分析的能力
。数据层面向运营用户和开发人员
，
用于对用户的输入数


据及经过系统产生的结果数据进行保存
，同时为应用层的数据上传下载提供接口
。


如图
５
－５所示
，
具体功能如下
：


（
１
）应用层


应用层是本系统数据业务逻辑处理和结果展示层
，应用层接受用户输入数据
，


并对数据做出逻辑处理
，将分析结果在前端以各种图表的方式进行展示
，帮助用


户做出基于数据的分析与决策
。


（２
）模型层


模型层为本系统的核心层
。
本层不同的功能有着不同的权限
＾
开发人员可以


使用模型训练功能通过调参进行训练模型
。开发人员只需在前端页面输入需要调


整的参数
，保存参数
，
点击训练按钮后即可启动模型的训练
。
同时
，
该层的模型


６２


第五章
面向用户评论的方面级情感分析系统设计与实现


推理功能也为运营用户开放
，为运营用户的细粒度情感分析需求提供算法能力支


撑
。


（３
）数据层


数据层是整个系统的基石
。功能主要是保存用户上传的待细粒度情感分析的


评论数据
，
保存评论数据经情感分析后的结果
。系统提供了文件上传功能
，
数据


可以以文件的形式进行上传
。
同时
，
分析结果会保存在系统的数据库中
。


应用层细粒度情感分析
结果可视化



ｆ



模型层
鐵配置
藝川练模型麵


Ｚ


觀层
文件
麵库


图
５
－５
系统层次架构设计图


５
．３
系统详细设计


５
．３
．
１用户管理模块


用户管理模块主要包含用户注册
、
用户登录等功能
，
当用户在进入系统前
，


首先在来到登录页面
，进行用户信息的填写
，
如果用户已经注册
，
则直接进入系


统
；
否则直接进行注册
。
逻辑流程图如图
５
－６所示
。


６３


北京邮电大学工程硕士学位论文






Ｖｅｓ
—


—


＾
＾


注
户登录成功


图
５
－６用户登录流程图


５
．３
．２模型训练模块设计


模型训练模块主要面向系统管理员
，用户只需要按照表单进行选择或在下拉


框中进行选择
，
即可完成模型训练参数的可视化配置
。然后开启模型训练任务即


可
。
逻辑流程图如图
５
－７所示
。


Ｉ


ｚｎ：


總§酬里训练輕


丰


选择模型
？
髮藍参数


图
５
－７模型训练流程图


５
．３
．３情感分析与可视化模块设计


情感分析与可视化模块主要面向运营人员
，用户只需选择合适的任务与模型
，


输入需要进行细粒度情感分析的文本
，
点击确认后就会拿到分析好的结果
。逻辑


流程图如图
５
－８所示
。


６４


第五章
面向用户评论的方面级情感分析系统设计与实现


１


输
＾
结束Ｊ



１
Ｘ


Ｉ
Ｉ


情感分祈
？
结果展示


图
５
－８情感分析流程图


５
．３
．４数据库设计


在本系统中
，包含了多张关系数据表
，其中包括
：用户信息表
（ｕｓｅｒｊｎｆ
ｏ表
）
、


训练任务表
（ｔｒａｉｎ
＿ｔａｓｋ表
）
、情感分析任务表
（ａｎａ
ｌｙｓ
ｉｓ
＿ｔａｓｋ表
）等
。


表
５
－
１
用户信息表


名称
｜
数据类型
｜
约束类型
｜是否创建索引
｜
是否非空
｜字段注释


ｔａｓｋ
＿
ｉｄＩｎｔｅｇｅｒ主键

ｎｏｔｎｕｌ
ｌ唯
一ＩＤ


ｔａｓｋｎａｍｅＳｔｒｉｎｇ（２０）
－
晏ｎｏｔｎｕｌ
ｌ任务名


ｐａｒａｍｅｔｅｒｓＳｔｒｉｎｇ（２０
）
－
Ｓ

ｎｏｔｎｕｌ
ｌ训练参数


ｕｓｅｒ
＿
ｉｄＳｔｒｉｎｇ（３０
）
－
ｍ
ｎｕｌ
ｌ
用户ＩＤ


ｃｒｅａｔｅｄ
＿
ｔｉｍｅＤａｔｅｔｉｍｅ
－否ｎｕｌ
ｌ创建时间


表
５
－２
训练任务表


名称
｜
数据类型
｜
约束类型
｜是否创建索引
｜
是否非空
｜字段注释


ｔａｓｋ
＿
ｉｄＩｎｔｅｇｅｒ主键是ｎｏｔｎｕ
ｌ
ｌ唯
一ＩＤ


ｔａｓｋｎａｍｅＳｔｒｉｎｇ（２０）
－
＾
ｎｏｔｎｕｌ
ｌ任务名


ｐａｒａｍｅｔｅｒｓＳｔｒｉｎｇ（２Ｑ）
－
？ｎｏｔｎｕｌｌ训练参数


ｕｓｅｒ
＿
ｉｄＳｔｒｉｎｇ（３０）
－
＾ｎｕｌｌ
用户ＩＤ


ｃｒｅａｔｅｄ
＿
ｔｉｍｅＤａｔｅｔ
ｉｍｅ
－否ｎｕｌ
ｌ创建时间


如表
５
－
１所示
，
用户信息表
（ｕｓｅｒ
＿
ｉｎｆｏ表
）
，
用于存储用户信息
，
支持用户注


册
、
登录
、
修改信息等功能
。
如表
５
－２所示
，
训练任务表
（ｔｒａｉｎ
＿
ｔａｓｋ表
）
，
用于


存储训练参数
，
训练结果等信息
，
支持参数提交
，
模型训练结果查询等功能
■＝
如


６５


北京邮电大学工程硕士学位论文


表
５
－３所示
，
情感分析任务表
（ａｎａｌｙｓ
ｉｓ
＿
ｔａｓｋ表
）
，
用于存储前端用户上传数据
，


以及相应的情感分析结果
，
用于支持数据可视化和数据分析功能
。数据表的设计


及各个字段如各表所示
。


表
５
＿３
情感分析任务表


＿
名称
｜＾据类型１约束类型
｜是否创建索引
｜
是否非空了字段注＾


ａｎａｉｙｓ
ｉｓ
＿
ｉｄＩｎｔｅｇｅｒ主键是ｎｏｔｎｕｌｌ唯
一ＩＤ


ａｎａｌｙｓ
ｉｓ
＿ｎａｍｅＳｔｒｉｎｇ（２０）＾
ｎｏｔｎｕｌ
ｌ任务名


ｔｅｘｔ

Ｓｔｒ
ｉｎｇ（
ｌＯＯ）Ｓｎｏｔｎｕ
ｌ
ｌ分析文本


ｒｅｓｕ
ｌｔ

Ｓｔｒｉｎｇ（３０）＾
ｎｕ
ｌ
ｌ
结果


ｕｓｅｒ
ｉｄ

Ｉｎｔｅｇｅｒ＾
ｎｕｌ
ｌ
仓
丨
ｊ建人


ｃｒｅａｔｅｄ
—
ｔｉｍｅＤａｔｅｔｉｍｅＳｎｕｌｌ
创建时间


５
．４系统实现


５
．４
．
１
系统实现环境


表
５４
系统实现环境与技术组件


系统组件工具名称


Ｖｕｅ
，
单页面应用框架
，
交互式项目脚手架


＾丄山柜加Ａｘｉｏｓ
，
一个基于
ｐｒｏｍｉｓｅ
的ＨＴＴＰ库


１０＾ａｍｄ
，
前端组件库


Ｅｃｈａｒｔ
，
前端组件库


后端框架Ｆｌａｓｋ
，
基于
ｐｙｔｈｏｎ的轻量Ｗｅｂ应用框架


数据库管理系


统
ＳＱＬ
ｉｔｅ


Ｐｙｔｈｏｎｗｓｇ
ｉ应用服务器
：
Ｇｕｎｉｃｏｒｎ


杀现部者Ｗｅｂ服务器
：
Ｎｇ
ｉｎｘ


版本控制
：
ｇ
ｉｔ／ｇ
ｉｔｌａｂ


其他工具系统文档
：
ｍｉｎｄｏｃ


深度学习框架
：
Ｐｙｔｏｒｃｈ


本系统前后端交互采用
Ｂ／Ｓ的架构方式
，在实际开发过程中
，采用目前主流


的前后端分离技术进行同步实现
。
如表
５
－４所示
，
前端使用Ｖｕｅ的单页Ｗｅｂ应


用框架技术
，
后端采用了支持
Ｐｙｔｈｏｎ语言的
Ｆ
ｌａｓｋＷｅｂ框架
，
同时数据库选择


了Ｐｙ
ｔｈｏｎ内置的轻量级数据库
ＳＱＬｉｔｅ
。此外
，
算法部分中采用
Ｐｙ
ｔｏｒｃｈ深度学


习框架
，
实现模型的训练和推理等工作
。


６６


第五章
面向用户评论的方面级情感分析系统设计与实现


５
．４
．２
用户管理模块实现


如图
５
－９所示
，
未注册过本系统账户的新用户在使用本系统时
，
需要在注册


页中的表单填写相关信息进行注册
。


ｍ


图
５
－９
系统注册页


如图
５
－
１０所示
，在完成注册之后
，
用户可以跳转到登录页面
，输入用户名和


密码进行登录
＝


ｍｍ


图
５
－
１０
系统登录页


５
，４
．３模型训练模块实现


如图
５
－
１
１
所示
，
进入系统后
，
通过系统的侧边导航栏
，
可以打开
一级菜单


６７


北京邮电大学工程硕士学位论文


“ 模型管理
” 下的二级菜单项
“ 数据管理
”
，
选择数据格式
，
进行数据的上传
。


Ｍ
ｔ？Ｍ
ＵＭＷ＜


｜供
４＊？ｃｃ？ｗ？ｒ
＊？？
ｕ＊ｒ
？Ｋ，
＊＊
？
５ｊｎｅ？Ｆ


ｍｏｔｓ
ｇ
ｉ
ｃｇ
）Ｍｎｗｔｆ
ｉｎ


图
５
－
１
１数据管理页


如图
５
－
１２所示
，
进入系统后
，
通过系统的侧边导航栏
，
可以打开
一级菜单


“ 模型管理
” 下的二级菜单项
“ 模型训练
”
，
在表单中填写完整的训练参数
，
点


击保存按钮可以保存配置数据
；
点击训练按钮可以创建并开启
一个训练任务
。


〇：ｒ
ｆ，ｗ＾８


＊＊ｗ
－


ｖｉＸｔ＾ａ


■＿Ｓ


图
５
－
１２模型管理页


５
．４
．４情感分析与可视化模块实现


如图
５
－
１３
所示
，
进入系统后
，
通过系统的侧边导航栏
，
可以打开
一级菜单


６８


第五章
面向用户评论的方面级情感分析系统设计与实现


“ 情感分析
” 下的二级菜单项
“ 方面词情感分析
”
，
在输入框中填写待分析的文


本
，点击提交按钮可以创建
一个情感分析任务
。分析结果会显示在下方的文本框
。


０Ｉｓｔ
ｉＶ
ｍＭ咖
？
、


〇＊？
？
？

ｓ
ｉｃ
－
■
ＢＱ


ｆｏｅ＊
＊
ｉ＞０ｒｖ
ａｃ？


图
５
－
１３
方面词情感分析页


如图
５
－
１４
所示
，
进入系统后
，
通过系统的侧边导航栏
，
可以打开
一级菜单


“ 情感分析
” 下的二级菜单项
“ 方面类别情感分析
”
，
在输入框中填写待分析的


文本
，
点击提交按钮可以创建
一个方面类别情感分析任务
＝分析结果会显示在下


方的文本框
。


〇：－
．ｗ
＊ｍ他＊
？
敖


？Ｖ
ｒ
ｊｒ
＜
５


ＢＱ


图
５
－
１４
方面类别情感分析页


如图
５
－
１５所示
，
进入系统后
，
通过系统的侧边导航栏
，
可以打开
一级菜单


“ 可视化
” 下的二级菜单项
“ 图表可视化
”
。
评论分析的结果会以图表的形式显


示在右侧页面
＝


６９


北京邮电大学工程硕士学位论文


參ｓｒａ


＊＿驟
Ｉ
Ｉ
Ｉ
－
，
ｌ＾ｋ


ｉＬｌＬｉｅＩ，？
＇
－


图
５
－
１５
可视化展示页


５
．５
系统测试


５
．５
．
１
功能性测试


为了确保开发出的系统与系统需求分析中的要求与设计保持
一致
，达到验收


标准
，
在完成系统功能需求开发后
，
需要对照需求分析文档对系统的功能
、性能


及界面等进行冒烟测试
、
黑盒与白盒测试等
。
系统的测试结果如表
５
＿５
所示
。


表
５
－５
系统功能测试表


工力能模块功能测试结果具体信肩、


注册通过注册功能正常


用户管理




ａａ登录功能正常


数据上传通过数据上传功能正常


＿＿数据下载通过数据下载功能正常


模型管理




参数配置通过参数配置功能正常


模型训练通过模型训练功能正常


模型选择通过模型选择功能正常


一
，
、一
、
，
，文本输入
Ｍ］±
文本输入功能正常


、心
一可视文本情感分析通过情感分析功能正常



｜
文本结果展示通过
｜
结果展示功能正常


５
．５
＿２非功能性测试


在非功能测试中
，
本小节主要针对易用性和兼容性需求进行测试
。测试结果


７０


第五章
面向用户评论的方面级情感分析系统设计与实现


如下
：本系统的在不同环境下部署便捷性
；在不同的浏览器可以支持前端界面正


常显示
。


５
．６本章小结


本章详细介绍了面向用户评论的方面级情感分析系统的设计与实现工作
。首


先分析了系统的功能与非功能性需求
，然后介绍了系统的总体功能设计
、架构设


计和每个功能模块的详细功能设计
。接下来对系统具体实现进行了简要介绍
，
对


系统进行了展示
。
最后介绍了系统的功能性测试与非功能测试
。


７
１


北京邮电大学工程硕士学位论文


７２


第六章
总结与展望


第六章
总结与展望


６
．
１
工作总结


方面级情感分析旨在以方面粒度为分析单位
，
预测出文本中各个方面的情感


倾向
，
具有重要的学术价值和广泛的应用价值
，
在自然语言处理领域方兴未艾
。


本文主要围绕方面词情感分析
（ＡＴＳＡ
）和方面类别情感分析
（ＡＣＳＡ
）这两


个任务展开
，分别针对两个任务及当前遇到的问题进行研宄
，具体研究工作如下
：


１
．针对方面词情感分析任务
，提出
一种情感感知的双通道图卷积神经网络模


型
，其中双通道分别为语法通道和语义通道
。在语法通道中
，
本文首先利用依存


句法分析方法捕获句子的语法结构
，
得到原始的句法依存树矩阵
，
然后利用


ＳｅｎｔｉｃＮｅｔ中词的情感数值构建句子的情感值矩阵
，以对原始矩阵增强情感知识
，


另外使用词性知识构建词性知识矩阵
，对句法依存树矩阵进
一步增强
；在语义通


道中
，
本文将基于
Ｃｏｎｃｅｐ
ｔＮｅｔ知识库训练的词向量与经过模型编码的隐向量融


合
，
对词语进行知识语义增强
。最终解决了语法
、语义通道对外部知识利用不足


的问题
。


２
．针对方面类别情感分析任务
，提出
一种知识增强的多通道图卷积神经网络


模型
，
其中多通道分别为知识通道
、语法通道和语义通道
。在知识通道中
，
本文


利用基于ＷｏｒｄＮｅｔ的相似度函数计算方面类别与句子上下文之间的语义相似度
，


进而得到与方面类别相关的相似度矩阵
，解决了方面类别与句中相关概念词关系


的捕获和利用不合理的问题
；在语法和语义通道中
，本文分别使用句法依存分析


和自注意力机制构建对应的邻接矩阵
，设计基于注意力的特征融合模块
，融合语


法和语义通道的特征
，解决了模型对句法结构和语义关系互补性的建模缺失问题
＝


３
．根据对方面词情感分析和方面类别情感分析任务的研宄
，本工作设计并实


现了
一套面向用户评论的方面级情感分析系统
。该系统的搭建过程包括需求分析
、


功能设计
、功能实现等阶段
。主要功能包括用户管理功能
、模型管理功能以及情


感分析及可视化功能
。该系统不仅可以用于可视化模型训练
，
同时也可以用于模


型推理和预测
，
即使用训练好的模型对文本进行方面级情感分析
，并对分析结果


进行可视化展示
。


７３


北京邮电大学工程硕士学位论文


６
．２工作展望


本文对方面词情感分析和方面类别情感分析两个任务及当前遇到的问题进


行了研宄
，
提出了相应的
ＳＡＤＣ
－ＧＣＮ和ＫＥＭＣ
－ＧＣＮ模型
，
取得了
一些进展
，


但依然存在探索和改进的空间
。
具体如下
：


１
）探索更有效的建立方面类别与句中相关词关系的方法
。
本文引入外部知


识库
，利用基于知识库的相似度函数来建模方面类别与句中相关词关系
，
同时结


合多通道特征图捕获句子的语法和语义特征
。这种基于外部知识库的单
一相似度


函数并不能充分地表达方面类别与句中相关词的关系
。未来
，可以考虑多种相似


度函数融合的方法
，
对单
一相似度函数的结果进行完善
。


２
）探索隐式情感数据的情感预测方法
。
评论数据中存在少量的隐式情感样


本
，
即样本中没有出现明显的情感观点词
，本文提出的模型不能准确地预测此类


样本的情感极性
，
从而在性能指标上有局限性
。
未来
，可以探索使用预训练模型


在针对含有隐式情感表达的数据集上
，构造无监督对比学习或其他相关任务
，进


行领域自适应的预训练
，
帮助大模型学习到更多包含隐式情感的句子表达方式
。


然后
，
使用大模型更高效地进行情感分析
。


７４


参考文献


参考文献


［
１
ＪＬ
ｉｕＢ
：Ｓｅｎｔｉｍｅｎｔａｎａ
ｌｙｓｉｓａｎｄｏｐ
ｉｎ
ｉｏｎｍ
ｉｎｉｎｇ［Ｊ
］
．／／Ｓｙｎｔｈｅｓ
ｉｓｌｅｃｔｕｒｅｓｏｎｈｕｍａｎ


ｌａｎｇｕａｇｅｔｅｃｈｎｏ
ｌｏｇ
ｉｅｓ
，２０
１２
．５（
１
）
：１
－
１６７
．


［２
］ＢｅｈｄｅｎｎａＳ
，ＢａｒｉｇｏｕＦ
，Ｂｅ
ｌａ
ｌｅｍＧ
．Ｄｏｃｕｍｅｎｔｌｅｖｅ
ｌｓｅｎｔｉｍｅｎｔａｎａｌｙｓｉｓ
：ａ


ｓｕｒｖｅｙ
［Ｊ
］
．／／ＥＡＩＥｎｄｏｒｓｅｄＴｒａｎｓａｃｔｉｏｎｓｏｎＣｏｎｔｅｘｔ
－ａｗａｒｅＳｙｓｔｅｍｓａｎｄ


Ａｐｐ
ｌｉｃａｔ
ｉｏｎｓ
，２０
１８
．４
（
１
３
）
：ｅ２
－ｅ２
．


［３
］ＭｅｅｎａＡ
，ＰｒａｂｈａｋａｒＴＶ．Ｓｅｎｔｅｎｃｅｌｅｖｅ
ｌｓｅｎｔｉｍｅｎｔａｎａｌｙｓ
ｉｓｉｎｔｈｅｐｒｅｓｅｎｃｅｏｆ


ｃｏｎｊｕｎｃｔｓｕｓ
ｉｎｇ
ｌ
ｉｎｇｕｉｓｔｉｃａｎａ
Ｉｙｓｉｓ
［Ｃ
］
．／／Ｅｕｒｏｐｅａｎｃｏｎｆｅｒｅｎｃｅｏｎｉｎｆｏｒｍａｔｉｏｎ


ｒｅｔｒｉｅｖａｌ
．２００７
：５７３
－５８０
．


［４
］ＺｈａｎｇＷ，Ｌ
ｉＸ
，ＤｅｎｇＹ
，ｅｔａｌ
．ＡＳｕｒｖｅｙｏｎＡｓｐｅｃｔ
－ＢａｓｅｄＳｅｎｔｉｍｅｎｔＡｎａｌｙｓｉｓ
：


Ｔａｓｋｓ
，Ｍｅｔｈｏｄｓ
，ａｎｄＣｈａｌ
ｌｅｎｇｅｓ
［Ｊ
］
．／／２０２２
．


［５
］ＲｈａｎｏｕｉＭ
，ＭｉｋｒａｍＭ
？ＹｏｕｓｆｌＳ
，ｅｔａｌ
．ＡＣＮＮ
－Ｂ
ｉＬＳＴＭｍｏｄｅｌｆｏｒｄｏｃｕｍｅｎｔ
－


ｌｅｖｅ
ｌｓｅｎｔｉｍｅｎｔａｎａｌｙｓ
ｉｓ
［Ｊ
］
．／／Ｍａｃｈ
ｉｎｅＬｅａｒｎ
ｉｎｇａｎｄＫｎｏｗ
ｌｅｄｇｅＥｘｔｒａｃｔｉｏｎ
，２０
１９
，


１
（３
）
：８３２
— ８４７
．


［６
］ＹａｎｇＢ
，ＣａｒｄｉｅＣ
．Ｃｏｎｔｅｘｔ
－ａｗａｒｅｌｅａｒｎｉｎｇｆｏｒｓｅｎｔｅｎｃｅ
－
ｌｅｖｅｌｓｅｎｔｉｍｅｎｔａｎａｌｙｓｉｓ


ｗｉｔｈ
ｐｏｓｔｅｒｉｏｒｒｅｇｕｌａｒｉｚａｔｉｏｎ
［Ｃ
］
．／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ５２ｎｄＡｎｎｕａｌＭｅｅｔｉｎｇｏｆｔｈｅ


ＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔ
ｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ（Ｖｏｌｕｍｅ１
：ＬｏｎｇＰａｐｅｒｓ）
．２０
１４
：３２５
－


３３５
．


［７
］
陈晓东
．
基于情感词典的中文微博情感倾向分析研宄
［Ｊ
］
．／／华中科技大学
．


硕士学位论文
，
２０
１２
．


［８
］ＮｅｅｔｈｕＭＳ
，Ｒａ
ｊａｓｒｅｅＲ
．Ｓｅｎｔｉｍｅｎｔａｎａｌｙｓｉｓｉｎｔｗｉｔｔｅｒｕｓｉｎｇｍａｃｈｉｎｅｌｅａｒｎ
ｉｎｇ


ｔｅｃｈｎ
ｉｑｕｅｓ
［Ｃ
］

．／／２０
１３ｆｏｕｒｔｈｉｎｔｅｒｎａｔｉｏｎａｌｃｏｎｆｅｒｅｎｃｅｏｎｃｏｍｐｕｔｉｎｇ，


ｃｏｍｍｕｎｉｃａｔ
ｉｏｎｓａｎｄｎｅｔｗｏｒｋｉｎｇ
ｔｅｃｈｎｏ
ｌｏｇ
ｉｅｓ（ＩＣＣＣＮＴ）
．２０
１３
：１
－５
．


［９
］Ｈ
ｉｎｔｏｎＧＥ
．Ｄ
ｉｓｔｒｉｂｕｔｅｄｒｅｐｒｅｓｅｎｔａｔ
ｉｏｎｓ
［Ｊ
］
．／／
１９８４
．


［
１０
］Ｂｅｎｇ
ｉｏＹ．ＤｕｃｈａｒｍｅＲ，ＶｉｎｃｅｎｔＰ．Ａｎｅｕｒａｌｐｒｏｂａｂ
ｉｌｉｓｔｉｃｌａｎｇｕａｇｅｍｏｄｅｌ
［Ｊ
］
．／／


Ａｄｖａｎｃｅｓｉｎｎｅｕｒａｌｉｎｆｏｒｍａｔｉｏｎ
ｐｒｏｃｅｓｓ
ｉｎｇｓｙｓｔｅｍｓ
．２０００
，
１３
．


［
１
１
］ＫｉｍＹ．Ｃｏｎｖｏ
ｌｕｔｉｏｎａｌＮｅｕｒａｌＮｅｔｗｏｒｋｓｆｏｒＳｅｎｔｅｎｃｅＣ
ｌａｓｓ
ｉｆ
ｉｃａｔｉｏｎ
［Ｊ
］
．／／２０
１４
．


［
１２
］ＸｕｅＷ．Ｌ
ｉＴ．ＡｓｐｅｃｔＢａｓｅｄＳｅｎｔｉｍｅｎｔＡｎａｌｙｓｉｓｗｉｔｈＧａｔｅｄＣｏｎｖｏ
ｌｕｔｉｏｎａｌ


Ｎｅｔｗｗｋｓ
［Ｃ
］
．／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ５６ｔｈＡｎｎｕａｌＭｅｅｔｉｎｇｏｆｔｈｅＡｓｓｏｃ
ｉａｔｉｏｎｆｏｒ


ＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ（Ｖｏ
ｌｕｍｅ１
：ＬｏｎｇＰａｐｅｒｓ）
．２０
１８
：２５
１４
－２５２３
．


［
１３
］ＤｏＨＨ
５ＰｒａｓａｄＰＷＣ
．ＭａａｇＡ
．ｅｔａｌ
．Ｄｅｅｐ
ｌｅａｒｎ
ｉｎｇｆｏｒａｓｐｅｃｔ
－ｂａｓｅｄｓｅｎｔｉｍｅｎｔ


ａｎａ
ｌｙｓ
ｉｓ
；ａｃｏｍｐａｒａｔｉｖｅｒｅｖ
ｉｅｗ
［Ｊ
］
．／／Ｅｘｐｅｒｔｓｙｓｔｅｍｓｗ
ｉｔｈａｐｐ
ｌ
ｉｃａｔｉｏｎｓ
，２０
１９
？１
１８
：


７５


北京邮电大学工程硕士学位论文


２７２
－２９９
．


［
１４
］ＷａｎｇＹ．ＨｕａｎｇＭ
？Ｚｈｕｘｉａｏｙａｎ
．ｅｔａｌ
．Ａｔｔｅｎｔｉｏｎ
－ｂａｓｅｄＬＳＴＭｆｏｒＡｓｐｅｃｔ
－
ｌｅｖｅ
ｌ


ＳｅｎｔｉｍｅｎｔＣ
ｌａｓｓ
ｉｆ
ｉｃａｔｉｏｎ
ｆＣ
］

．／／Ｐｒｏｃｅｅｄ
ｉｎｇｓｏｆｔｈｅ２０
１６ＣｏｎｆｅｒｅｎｃｅｏｎＥｍｐ
ｉｒｉｃａｌ


ＭｅｔｈｏｄｓｉｎＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ
．２０
１６
：６０６
—６
１５
．


［
１５
］ＭａＤ
．Ｌ
ｉＳ
，ＺｈａｎｇＸ
，ｅｔａｌ
．ＩｎｔｅｒａｃｔｉｖｅＡｔｔｅｎｔｉｏｎＮｅｔｗｏｒｋｓｆｏｒＡｓｐｅｃｔ
－Ｌｅｖｅ
ｌ


Ｓｅｎｔ
ｉｍｅｎｔＣ
ｌａｓｓ
ｉｆ
ｉｃａｔｉｏｎ
［Ｊ
］
．／／２０
１７
．


［
１６
］ＣｈｅｎＰ
，ＳｕｎＺ
，Ｂ
ｉｎｇＬ
，ｅｔａｌ
．Ｒｅｃｕｒｒｅｎｔａｔｔｅｎｔｉｏｎｎｅｔｗｏｒｋｏｎｍｅｍｏｒｙｆｏｒａｓｐｅｃｔ


ｓｅｎｔｉｍｅｎｔａｎａ
！ｙｓ
ｉｓ
［Ｃ
］
．／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆ
ｔｈｅ２０
１７ｃｏｎｆｅｒｅｎｃｅｏｎｅｍｐ
ｉｒ
ｉｃａ
ｌｍｅｔｈｏｄｓ


ｉｎｎａｔｕｒａｌｌａｎｇｕａｇｅ
ｐｒｏｃｅｓｓ
ｉｎｇ
．２０
１７
：４５２
－４６
１
．


［
１７
］ＦａｎＦ
，ＦｅｎｇＹ
，ＺｈａｏＤ
．Ｍｕｌｔｉ
－
ｇｒａｉｎｅｄａｔｔｅｎｔｉｏｎｎｅｔｗｏｒｋｆｏｒａｓｐｅｃｔ
－
ｌｅｖｅ
ｌｓｅｎｔｉｍｅｎｔ


ｃ
ｌａｓｓｉｆ
ｉｃａｔｉｏｎ
［Ｃ
］
．／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２０
１８ｃｏｎｆｅｒｅｎｃｅｏｎｅｍｐ
ｉｒ
ｉｃａｌｍｅｔｈｏｄｓｉｎ


ｎａｔｕｒａｌｌａｎｇｕａｇｅ
ｐｒｏｃｅｓｓｉｎｇ
．２０
１８
：３４３３
－３４４２
．


［
１８
］Ｋ
ｉｐｆＴＮ
；Ｗｅｌｌ
ｉｎｇＭ
．Ｓｅｍ
ｉ
－ｓｕｐｅｒｖｉｓｅｄｃｌａｓｓ
ｉｆ
ｉｃａｔｉｏｎｗｉｔｈｇｒａｐｈｃｏｎｖｏｌｕｔｉｏｎａｌ


ｎｅｔｗｏｒｋｓ
［Ｊ
］
．／／ａｒＸ
ｉｖ
ｐｒｅｐｒｉｎｔａｒＸｉｖ
：１６０９
．０２９０７
，２０
１６
．


［
１９
］ＺｈａｎｇＣ
．Ｌ
ｉＱ
，ＳｏｎｇＤ
．Ａｓｐｅｃｔ
－ｂａｓｅｄｓｅｎｔｉｍｅｎｔｃｌａｓｓｉｆ
ｉｃａｔｉｏｎｗ
ｉｔｈａｓｐｅｃｔ
－ｓｐｅｃｉｆ
ｉｃ


ｇｒａｐｈｃｏｎｖｏ
ｌｕｔｉｏｎａｌｎｅｔｗｏｒｋｓ
［Ｊ
］
．／／ａｒＸ
ｉｖ
ｐｒｅｐｒｉｎｔａｒＸ
ｉｖ
：１９０９
．０３４７７
，２０
１９
．


［２０
］ＳｕｎＫ
，ＺｈａｎｇＲ
，ＭｅｎｓａｈＳ
，ｅｔａｌ
．Ａｓｐｅｃｔ
－Ｌｅｖｅ
ｌＳｅｎｔｉｍｅｎｔＡｎａｌｙｓ
ｉｓＶｉａ


ＣｏｎｖｏｌｕｔｉｏｎｏｖｅｒＤｅｐｅｎｄｅｎｃｙＴｒｅｅ
［Ｃ
］
．／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２０
１９Ｃｏｎｆｅｒｅｎｃｅｏｎ


Ｅｍｐ
ｉｒｉｃａｌＭｅｔｈｏｄｓｉｎＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇａｎｄｔｈｅ９ｔｈＩｎｔｅｒｎａｔｉｏｎａｌＪｏｉｎｔ


ＣｏｎｆｅｒｅｎｃｅｏｎＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ（ＥＭＮＬＰ
－
ＩＪＣＮＬＰ）
．２０
１９
：５６７８
－


５６８７
，


［２
１
］ＨｕａｎｇＢ
．ＣａｒｌｅｙＫ
．Ｓｙｎｔａｘ
－ＡｗａｒｅＡｓｐｅｃｔＬｅｖｅｌＳｅｎｔｉｍｅｎｔＣ
ｌａｓｓ
ｉｆｉｃａｔｉｏｎｗ
ｉｔｈ


ＧｒａｐｈＡｔｅｎｔｉｏｎＮｅｔｗｏｒｋｓ
［Ｃ
］
．／／Ｐｒｏｃｅｅｄ
ｉｎｇｓｏｆｔｈｅ２０
１９ＣｏｎｆｅｒｅｎｃｅｏｎＥｍｐ
ｉｒｉｃａ
ｌ


ＭｅｔｈｏｄｓｉｎＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓ
ｉｎｇａｎｄｔｈｅ９ｔｈＩｎｔｅｒｎａｔｉｏｎａ
ｌＪｏ
ｉｎｔ


ＣｏｎｆｅｒｅｎｃｅｏｎＮａｔｕｒａ
ｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓ
ｉｎｇ（ＥＭＮＬＰ
－
ＩＪＣＮＬＰ）
．２０
１９
：５４６８
－


５４７６
．


［２２
］ＷａｎｇＫ
，ＳｈｅｎＷ．ＹａｎｇＹ．ｅｔａ
ｌ
．Ｒｅ
ｌａｔｉｏｎａ
ｌＧｒａｐｈＡｔｅｎｔｉｏｎＮｅｔｗｏｒｋｆｏｒＡｓｐｅｃｔ
－


ｂａｓｅｄＳｅｎｔｉｍｅｎｔＡｎａｌｙｓ
ｉｓ
［Ｊ
］
．／／２０２０
．


［２３
］Ｌ
ｉＲ
，ＣｈｅｎＨ
，ＦｅｎｇＦ
，ｅｔａｌ
．Ｄｕａｌ
ｇｒａｐｈｃｏｎｖｏ
ｌｕｔｉｏｎａｌｎｅｔｗｏｒｋｓｆｏｒａｓｐｅｃｔ
－ｂａｓｅｄ


ｓｅｎｔｉｍｅｎｔａｎａｌｙｓ
ｉｓ
［Ｃ
］
．／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ５９ｔｈＡｎｎｕａｌＭｅｅｔｉｎｇｏｆｔｈｅ


Ａｓｓｏｃ
ｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａ
ｌＬ
ｉｎｇｕｉｓｔ
ｉｃｓａｎｄｔｈｅ１
１
ｔｈＩｎｔｅｒｎａｔｉｏｎａ
ｌＪｏ
ｉｎｔ


ＣｏｎｆｅｒｅｎｃｅｏｎＮａｔｕｒａ
ｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓ
ｉｎｇ（Ｖｏ
ｌｕｍｅ１
：ＬｏｎｇＰａｐｅｒｓ
）
．２０２
１
：


６３
１９
－６３２９
．


［２４
］Ｌ
ｉａｎｇＳ
，Ｗｅ
ｉＷ
；ＭａｏＸ
－Ｌ
，ｅｔａｌ
．Ｂ
ｉＳｙｎ
－ＧＡＴ＋
：Ｂ
ｉ
－ＳｙｎｔａｘＡｗａｒｅＧｒａｐｈＡｔｅｎｔｉｏｎ


７６


参考文献


ＮｅｔｗｏｒｋｆｏｒＡｓｐｅｃｔ
－ｂａｓｅｄＳｅｎｔｉｍｅｎｔＡｎａｌｙｓｉｓ
［Ｃ
］
．／／ＦｉｎｄｉｎｇｓｏｆｔｈｅＡｓｓｏｃ
ｉａｔｉｏｎ


ｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕ
ｉｓｔｉｃｓ
：ＡＣＬ２０２２
．２０２２
：１８３５
－
１８４８
．


［２５
］ＨｕＭ
，ＺｈａｏＳ
．ＺｈａｎｇＬ
；ｅｔａｌ
．ＣＡＮ
：ＣｏｎｓｔｒａｉｎｅｄＡｔｔｅｎｔｉｏｎＮｅｔｗｏｒｋｓｆｏｒＭｕｌｔｉ
－


ＡｓｐｅｃｔＳｅｎｔｉｍｅｎｔＡｎａｌｙｓｉｓ
［Ｃ
］
．／／Ｐｒｏｃｅｅｄ
ｉｎｇｓｏｆ
ｔｈｅ２０
１９ＣｏｎｆｅｒｅｎｃｅｏｎＥｍｐ
ｉｒｉｃａｌ


ＭｅｔｈｏｄｓｉｎＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇａｎｄｔｈｅ９ｔｈＩｎｔｅｒｎａｔｉｏｎａｌＪｏｉｎｔ


ＣｏｎｆｅｒｅｎｃｅｏｎＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ（ＥＭＮＬＰ
－
ＩＪＣＮＬＰ）
．２０
１９
：４６００
－


４６０９
．


［２６
］Ｌ
ｉＹ
，ＹｉｎＣ
，ＺｈｏｎｇＳ
，ｅｔａ
ｌ
．Ｍｕｌｔｉ
－
ＩｎｓｔａｎｃｅＭｕ
ｌｔ
ｉ
－Ｌａｂｅ
ｌＬｅａｒｎ
ｉｎｇＮｅｔｗｏｒｋｓｆｏｒ


Ａｓｐｅｃｔ
－ＣａｔｅｇｏｒｙＳｅｎｔｉｍｅｎｔＡｎａｌｙｓ
ｉｓ
［Ｊ
］
．／／２０２０
．


［２７
］ＺｈｏｕＪ
，ＨｕａｎｇＪＸ
，ＨｕＱＶ
，ｅｔａｌ
．Ｓｋ
－
ｇｃｎ
：Ｍｏｄｅ
ｌ
ｉｎｇｓｙｎｔａｘａｎｄｋｎｏｗｌｅｄｇｅｖ
ｉａ


ｇｒａｐｈｃｏｎｖｏ
ｌｕｔｉｏｎａｌｎｅｔｗｏｒｋｆｏｒａｓｐｅｃｔ
－
ｌｅｖｅ
ｌｓｅｎｔｉｍｅｎｔｃ
ｌａｓｓｉｆ
ｉｃａｔｉｏｎ
［Ｊ
］
．／／


Ｋｎｏｗｌｅｄｇｅ
－ＢａｓｅｄＳｙｓｔｅｍｓ
，２０２０
，２０５
：１０６２９２
．


［２８
］ＺｈａｎｇＺ
，ＨａｎＸ
．ＬｉｕＺ
，ｅｔａｌ
．ＥＲＮＩＥ
：ＥｎｈａｎｃｅｄＬａｎｇｕａｇｅＲｅｐｒｅｓｅｎｔａｔｉｏｎｗｉｔｈ


ＩｎｆｏｒｍａｔｉｖｅＥｎｔｉｔｉｅｓ
［Ｊ
］
．／／２０
１９
．


［２９
］ＬｉａｎｇＢ
，ＳｕＨ
，Ｇｕ
ｉＬ
，ｅｔａｌ
．Ａｓｐｅｃｔ
－ｂａｓｅｄｓｅｎｔｉｍｅｎｔａｎａｌｙｓ
ｉｓｖｉａａｆｆｅｃｔｉｖｅ


ｋｎｏｗｌｅｄｇｅｅｎｈａｎｃｅｄｇｒａｐｈｃｏｎｖｏｌｕｔｉｏｎａｌｎｅｔｗｏｒｋｓ
［Ｊ
］
．／／Ｋｎｏｗｌｅｄｇｅ
－Ｂａｓｅｄ


Ｓｙｓｔｅｍｓ
，２０２２
．２３５
：１０７６４３
．


［３０
］Ｌ
ｉａｎｇＢ
，ＳｕＨ
，ＹｉｎＲ
ｓｅｔａｌ
．ＢｅｔａＤｉｓｔｒｉｂｕｔｉｏｎＧｕ
ｉｄｅｄＡｓｐｅｃｔ
－ａｗａｒｅＧｒａｐｈｆｏｒ


ＡｓｐｅｃｔＣａｔｅｇｏｒｙＳｅｎｔｉｍｅｎｔＡｎａｌｙｓｉｓｗｉｔｈＡｆｅｃｔｉｖｅＫｎｏｗ
ｌｅｄｇｅ
［Ｃ
］
．／／Ｐｒｏｃｅｅｄｉｎｇｓ


ｏｆｔｈｅ２０２
１ＣｏｎｆｅｒｅｎｃｅｏｎＥｍｐ
ｉｒｉｃａｌＭｅｔｈｏｄｓｉｎＮａｔｕｒａｌＬａｎｇｕａｇｅ


Ｐｒｏｃｅｓｓ
ｉｎｇ
．２０２
１
：２０８
－２
１８
．


［３
１
］Ｍ
ｉｋｏ
ｌｏｖＴ．ＣｈｅｎＫ
，ＣｏｒｒａｄｏＧ
．ｅｔａｌ
．Ｅｆｆ
ｉｃ
ｉｅｎｔＥｓｔｉｍａｔｉｏｎｏｆＷｏｒｄ


ＲｅｐｒｅｓｅｎｔａｔｉｏｎｓｉｎＶｅｃｔｏｒＳｐａｃｅ
［Ｊ
］
．／／２０
１３
．


［３２
］ＰｅｎｎｉｎｇｔｏｎＪ
，ＳｏｃｈｅｒＲ
，ＭａｎｎｉｎｇＣＤ
．Ｇ
ｌｏｖｅ
：Ｇ
ｌｏｂａｌｖｅｃｔｏｒｓｆｏｒｗｏｒｄ


ｒｅｐｒｅｓｅｎｔａｔｉｏｎ
［Ｃ
］
．／／Ｐｒｏｃｅｅｄ
ｉｎｇｓｏｆｔｈｅ２０
１４ｃｏｎｆｅｒｅｎｃｅｏｎｅｍｐ
ｉｒｉｃａｌｍｅｔｈｏｄｓｉｎ


ｎａｔｕｒａｌｌａｎｇｕａｇｅ
ｐ
ｒｏｃｅｓｓｉｎｇ（ＥＭＮＬＰ）
．２０
１４
：１５３２
－
１５４３
．


［３３
］ＰｅｔｅｒｓＭＥ
，ＮｅｕｍａｎｎＭ
，ＩｙｙｅｒＭ
，ｅｔａｌ
．Ｄｅｅｐｃｏｎｔｅｘｔｕａｌｉｚｅｄｗｏｒｄ


ｒｅｐｒｅｓｅｎｔａｔｉｏｎｓ
［Ｊ］
．／／２０
１８
．


［３４
］ＥｌｍａｎＪＬ
＝Ｆ
ｉｎｄｉｎｇｓｔｒｕｃｔｕｒｅｉｎｔ
ｉｍｅ［Ｊ
］
．／／Ｃｏｇｎ
ｉｔｉｖｅｓｃ
ｉｅｎｃｅ
，
１９９０
？
１４
（２
）
：１７９
－２
１
１
．


［３５
］ＧｒａｖｅｓＡ
．Ｌｏｎｇｓｈｏｒｔ
－ｔｅｒｍｍｅｍｏｒ＞
７
［Ｊ
］
．／／Ｓｕｐｅｒｖｉｓｅｄｓｅｑｕｅｎｃｅｌａｂｅｌｌｉｎｇｗｉｔｈ


ｒｅｃｕｒｒｅｎｔｎｅｕｒａｌｎｅｔｗｏｒｋｓ
，２０
１２
：３７
－４５
．


［３６
］ＢａｈｄａｎａｕＤ
；ＣｈｏＫ
．Ｂｅｎｇ
ｉｏＹ．Ｎｅｕｒａｌｍａｃｈｉｎｅｔｒａｎｓ
ｌａｔ
ｉｏｎｂｙｊｏ
ｉｎｔｌｙ
ｌｅａｒｎ
ｉｎｇｔｏ


ａｌｉｇｎａｎｄｔｒａｎｓｌａｔｅ＾］
．／／ａｒＸｉｖ
ｐ
ｒｅｐｒｉｎｔａｒＸ
ｉｖ
：１４０９
．０４７３
，２０
１４
．


［３７
］Ｖａｓｗａｎ
ｉＡ
．ＳｈａｚｅｅｒＮ
，ＰａｒｍａｒＮ
．ｅｔａｌ
．Ａｔｔｅｎｔｉｏｎｉｓａ
ｌ
ｌ
ｙｏｕｎｅｅｄ
［Ｊ
］
．／／Ａｄｖａｎｃｅｓｉｎ


７７


北京邮电大学工程硕士学位论文


ｎｅｕｒａｌｉｎｆｏｒｍａｔｉｏｎ
ｐｒｏｃｅｓｓ
ｉｎｇｓｙｓｔｅｍｓ
，２０
１７
，３０
．


［３８
］ＲａｄｆｏｒｄＡ
，ＮａｒａｓｉｍｈａｎＫ
？Ｓａｌ
ｉｍａｎｓＴ
，ｅｔａｌ
．Ｉｍｐｒｏｖ
ｉｎｇ
ｌａｎｇｕａｇｅｕｎｄｅｒｓｔａｎｄ
ｉｎｇ


ｂｙｇｅｎｅｒａｔｉｖｅ
ｐｒｅ
－
ｔｒａｉｎｉｎｇ［Ｊ
］
．／／２０
１８
．


［３９
］ＤｅｖｌｉｎＪ
？ＣｈａｎｇＭ
－Ｗ
，ＬｅｅＫ
，ｅｔａｌ
．Ｂｅｒｔ
：Ｐｒｅ
－ｔｒａｉｎｉｎｇｏｆｄｅｅｐｂｉｄ
ｉｒｅｃｔｉｏｎａ
ｌ


ｔｒａｎｓｆｏｒｍｅｒｓｆｏｒｌａｎｇｕａｇｅｕｎｄｅｒｓｔａｎｄ
ｉｎｇ［Ｊ
］
．／／ａｒＸ
ｉｖｐｒｅｐｒｉｎｔａｒＸ
ｉｖ
：１８
１０
．０４８０５
，


２０
１８
．


［４０
］ＬｉｕＹ
，ＯｔｔＭ
，ＧｏｙａｌＮ
，ｅｔａｌ
．Ｒｏｂｅｒｔａ
：Ａｒｏｂｕｓｔｌｙｏｐ
ｔｉｍ
ｉｚｅｄｂｅｒｔｐｒｅｔｒａ
ｉｎ
ｉｎｇ


ａｐｐｒｏａｃｈ
［Ｊ
］
．／／ａｒＸ
ｉｖ
ｐｒｅｐｒｉｎｔａｒＸ
ｉｖ
：
１９０７
．
１
１６９２
，２０
１９
．


［４
１
］ＳｕｎＹ
，ＷａｎｇＳ
．Ｌ
ｉＹ．ｅｔａｌ
．Ｅｒｎ
ｉｅ
：Ｅｎｈａｎｃｅｄｒｅｐｒｅｓｅｎｔａｔｉｏｎｔｈｒｏｕｇｈｋｎｏｗ
ｌｅｄｇｅ


ｉｎｔｅｇｒａｔｉｏｎ
［Ｊ
］
．／／ａｒＸ
ｉｖ
ｐｒｅｐｒｉｎｔａｒＸ
ｉｖ
：１９０４
．０９２２３
，２０
１９
．


［４２
］Ｊ
ｉＳ
，ＰａｎＳ
，ＣａｍｂｒｉａＥ
，ｅｔａｌ
．Ａｓｕｒｖｅｙｏｎｋｎｏｗｌｅｄｇｅｇｒａｐｈｓ
：Ｒｅｐｒｅｓｅｎｔａｔｉｏｎ
，


ａｃｑｕｉｓ
ｉｔｉｏｎ
，ａｎｄａｐｐ
Ｉ
ｉｃａｔｉｏｎｓ
［Ｊ
］
．／／ＩＥＥＥｔｒａｎｓａｃｔｉｏｎｓｏｎｎｅｕｒａ
ｌｎｅｔｗｏｒｋｓａｎｄ


ｌｅａｒｎｉｎｇｓｙｓｔｅｍｓ
，２０２
１
，３３（２）
：４９４
－５
１４
．


［４３
］ＢｏｒｄｅｓＡ
，ＵｓｕｎｉｅｒＮ
，Ｇａｒｃｉａ
－ＤｕｒａｎＡ
．ｅｔａｌ
．Ｔｒａｎｓ
ｌａｔｉｎｇｅｍｂｅｄｄ
ｉｎｇｓｆｏｒｍｏｄｅｌ
ｉｎｇ


ｍｕｌｔｉ
－ｒｅｌａｔｉｏｎａ
ｌｄａｔａ
［Ｊ
］
．／／Ａｄｖａｎｃｅｓｉｎｎｅｕｒａｌｉｎｆｏｒｍａｔｉｏｎｐｒｏｃｅｓｓｉｎｇｓｙｓｔｅｍｓ
，


２０
１３
，
２６
．


［４４
］ＷａｎｇＺ
，ＺｈａｎｇＪ
５ＦｅｎｇＪ
？ｅｔａｌ
．Ｋｎｏｗｌｅｄｇｅｇｒａｐｈｅｍｂｅｄｄｉｎｇｂｙｔｒａｎｓ
ｌａｔｉｎｇｏｎ


ｈｙｐｅｒｐ
ｌａｎｅｓ
［Ｃ
］

．／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＡＡＡＩｃｏｎｆｅｒｅｎｃｅｏｎａｒｔｉｆｉｃｉａ
ｌ


ｉｎｔｅｌｌｉｇｅｎｃｅ
．２０
１４
，２８（
１
）
．


［４５
］Ｎ
ｉｃｋｅ
ｌＭ
，ＴｒｅｓｐＶ
，Ｋｒｉｅｇｅ
ｌＨ
－Ｐ．Ａｔｈｒｅｅ
－ｗａｙｍｏｄｅｌｆｏｒｃｏ
ｌ
ｌｅｃｔｉｖｅｌｅａｒｎｉｎｇｏｎ


ｍｕ
ｌｔｉ
－ｒｅ
ｌａｔｉｏｎａ
ｌｄａｔａ
．
［Ｃ
］
．／／Ｉｃｍ
ｌ
．２０
１
１
，１
１
（
１０
．５５５５
）
：３
１０４４８２
－３
１０４５８４
．


［４６
］ＹａｎｇＢ
，ＹｉｈＷ
，ＨｅＸ
，ｅｔａｌ
．ＥｍｂｅｄｄｉｎｇＥｎｔ
ｉｔｉｅｓａｎｄＲｅｌａｔ
ｉｏｎｓｆｏｒＬｅａｒｎｉｎｇａｎｄ


ＩｎｆｅｒｅｎｃｅｉｎＫｎｏｗｌｅｄｇｅＢａｓｅｓ
［Ｊ
］
．／／２０
１５
．


［４７
］ＳｐｅｅｒＲ
，ＣｈｉｎＪ
，Ｈａｖａｓ
ｉＣ
．Ｃｏｎｃｅｐ
ｔｎｅｔ５
．５
：Ａｎｏｐｅｎｍｕ
ｌｔｉ
ｌ
ｉｎｇｕａ
ｌ
ｇｒａｐｈｏｆ
ｇｅｎｅｒａ
ｌ


ｋｎｏｗＩｅｄｇｅ
［Ｃ
］
．／／Ｔｈ
ｉｒｔｙ
－ｆｉｒｓｔＡＡＡＩｃｏｎｆｅｒｅｎｃｅｏｎａｒｔｉｆ
ｉｃ
ｉａｌｉｎｔｅ
ｌ
ｌ
ｉｇｅｎｃｅ
．２０
１７
．


［４８
］ＣａｍｂｒｉａＥ
，ＰｏｒｉａＳ
，ＨａｚａｒｉｋａＤ
，ｅｔａｌ
．ＳｅｎｔｉｃＮｅｔ５
：Ｄ
ｉｓｃｏｖｅｒｉｎｇＣｏｎｃｅｐｔｕａｌ


Ｐｒｉｍ
ｉｔ
ｉｖｅｓｆｏｒＳｅｎｔｉｍｅｎｔＡｎａｌｙｓ
ｉｓｂｙＭｅａｎｓｏｆＣｏｎｔｅｘｔＥｍｂｅｄｄｉｎｇｓ
［Ｊ
］
．／／


Ｐｒｏｃｅｅｄ
ｉｎｇｓｏｆｔｈｅＡＡＡＩＣｏｎｆｅｒｅｎｃｅｏｎＡｒｔｉｆ
ｉｃ
ｉａｌＩｎｔｅｌ
ｌ
ｉｇｅｎｃｅ
，２０
１８
？３２（
１
）
．


［４９
］ＳｅｎｔｉｃＮｅｔ６
：ＥｎｓｅｍｂｌｅＡｐｐ
ｌｉｃａｔｉｏｎｏｆＳｙｍｂｏｌ
ｉｃａｎｄＳｕｂｓｙｍｂｏｌ
ｉｃＡｌｆｏｒ


ＳｅｎｔｉｍｅｎｔＡｎａｌｙｓｉｓ
｜Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２９ｔｈＡＣＭＩｎｔｅｒｎａｔｉｏｎａ
ｌＣｏｎｆｅｒｅｎｃｅｏｎ


Ｉｎｆｏｒｍａｔｉｏｎ＆ＫｎｏｗｌｅｄｇｅＭａｎａｇｅｍｅｎｔ
［ＥＢ／ＯＬ
］
．／／
［２０２３
－０６
－


０５
］
．ｈｔｐｓ
：／／ｄｌ
．ａｃｍ
．ｏｒｇ
／ｄｏｉ／ａｂｓ／１０
．
１
１４５／３３４０５３
１
．３４
１２００３
．


［５０
］ＧｒａｖｅｓＡ
？ＳｃｈｍｉｄｈｕｂｅｒＪ
．Ｆｒａｍｅｗｉｓｅｐｈｏｎｅｍｅｃ
ｌａｓｓ
ｉｆｉｃａｔｉｏｎｗｉｔｈｂｉｄｉｒｅｃｔｉｏｎａｌ


ＬＳＴＭｎｅｔｗｏｒｋｓ
［Ｃ
］
．／／Ｐｒｏｃｅｅｄｉｎｇｓ
．２００５ＩＥＥＥＩｎｔｅｒｎａｔｉｏｎａ
ｌＪｏｉｎｔＣｏｎｆｅｒｅｎｃｅｏｎ


７８


参考文献


ＮｅｕｒａｌＮｅｔｗｏｒｋｓ
，２００５
．２００５
，４
：２０４７
－２０５２
．


［５
１
］Ｎ
ｉｖｒｅＪ
．Ａｎｅｆｆｉｃｉｅｎｔａｌｇｏｒｉｔｈｍｆｏｒｐｒｏｊｅｃｔｉｖｅｄｅｐｅｎｄｅｎｃｙ


ｐａｒｓｉｎｇ［Ｃ
］

．／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅｅｉｇｈｔｈｉｎｔｅｒｎａｔｉｏｎａｌｃｏｎｆｅｒｅｎｃｅｏｎｐａｒｓｉｎｇ


ｔｅｃｈｎｏｌｏｇ
ｉｅｓ
．２００３
：１４９
－
１６０
．


［５２
］Ｊ
ｉａｎｇＱ
，ＣｈｅｎＬ
？ＸｕＲ
，ｅｔａｌ
．ＡＣｈａｌ
ｌｅｎｇｅＤａｔａｓｅｔａｎｄＥｆｆｅｃｔｉｖｅＭｏｄｅ
ｌｓｆｏｒ


Ａｓｐｅｃｔ
－ＢａｓｅｄＳｅｎｔｉｍｅｎｔＡｎａｌｙｓ
ｉｓ
［Ｃ
］
．／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２０
１９Ｃｏｎｆｅｒｅｎｃｅｏｎ


Ｅｍｐ
ｉｒｉｃａ
ｌＭｅｔｈｏｄｓｉｎＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓ
ｉｎｇａｎｄｔｈｅ９ｔｈＩｎｔｅｒｎａｔｉｏｎａ
ｌＪｏ
ｉｎｔ


ＣｏｎｆｅｒｅｎｃｅｏｎＮａｔｕｒａ
ｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ（ＥＭＮＬＰ
－ＩＪＣＮＬＰ）
．２０
１９
：６２７９
－


６２８４
．


［５３
］Ｍ
ｉ
ｌ
ｌｅｒＧＡ
．ＷｏｒｄＮｅｔ
：ａｌｅｘｉｃａｌｄａｔａｂａｓｅｆｏｒＥｎｇ
ｌｉｓｈ
［Ｊ
］
．／／Ｃｏｍｍｕｎ
ｉｃａｔｉｏｎｓｏｆｔｈｅ


ＡＣＭ
，
１９９５
５３８
（
１
１
）
：３９＾
１
．


［５４
］赵福强
．基于词嵌入和ＷｏｒｄＮｅｔ的词汇相似度计算模型
［Ｊ
］
．／／２０２
１
．


［５５
］刘宏哲
，须德
．
基于本体的语义相似度和相关度计算研宄综述
［Ｊ
］
．／／计算机


科学
，
２０
１２
，３９（２
）
：８
－
１３
．


［５６
］Ｌ
ｉｎＤ
．Ａｎｉｎｆｏｒｍａｔｉｏｎ
－ｔｈｅｏｒｅｔｉｃｄｅｆ
ｉｎ
ｉｔｉｏｎｏｆｓｉｍｉ
ｌａｒｉｔｙ
．
［Ｃ
］
．／／Ｉｃｍ
ｌ
．
ｌ９９８
，９８（
１９９８）
：


２９６
－３０４
．


［５７
］ＷａｎｇＸ
，
ＺｈｕＭ
，
ＢｏＤ
，
ｅｔａｌ
．Ａｍ
－
ｇｅｎ
：Ａｄａｐｔ
ｉｖｅｍｕｌｔｉ
－ｃｈａｎｎｅｌｇｒａｐｈ


ｃｏｎｖｏｌｕｔｉｏｎａｌｎｅｔｗｏｒｋｓ
［Ｃ
］

．／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆ
ｔｈｅ２６ｔｈＡＣＭＳＩＧＫＤＤＩｎｔｅｒｎａｔｉｏｎａｌ


ｃｏｎｆｅｒｅｎｃｅｏｎｋｎｏｗｌｅｄｇｅｄ
ｉｓｃｏｖｅｒｙ＆ｄａｔａｍ
ｉｎｉｎｇ
．２０２０
：１２４３
－
１２５３
．


［５８
］ＫｉｎｇｍａＤＰ
，ＢａＪ
．Ａｄａｍ
：Ａｍｅｔｈｏｄｆｏｒｓｔｏｃｈａｓｔ
ｉｃｏｐ
ｔｉｍ
ｉｚａｔｉｏｎ
［Ｊ
］
．／／ａｒＸ
ｉｖ
ｐｒｅｐｒｉｎｔ


ａｒＸ
ｉｖ
：
１４
１２
．６９８０
，２０
１４
．


［５９
］ＴａｎｇＤ
ｓＱ
ｉｎＢ
，ＦｅｎｇＸ
５ｅｔａｌ
．ＥｆｅｃｔｉｖｅＬＳＴＭｓｆｏｒｔａｒｇｅｔ
－ｄｅｐｅｎｄｅｎｔｓｅｎｔ
ｉｍｅｎｔ


ｃ
ｌａｓｓｉｆ
ｉｃａｔｉｏｎ
［Ｊ
］
．／／ａｒＸｉｖ
ｐｒｅｐｒｉｎｔａｒＸ
ｉｖ
：
１５
１２
．０
１
１００
；２０
１５
．


［６０
］ＷａｎｇＹ．ＳｕｎＡ
，ＨｕａｎｇＭ
，ｅｔａｌ
．Ａｓｐｅｃｔ
－
ｌｅｖｅ
ｌｓｅｎｔｉｍｅｎｔａｎａｌｙｓ
ｉｓｕｓｉｎｇａｓ
－


ｃａｐｓｕ
！ｅｓ
［Ｃ
］
，／／Ｔｈｅｗｏｒｌｄｗ
ｉｄｅｗｅｂｃｏｎｆｅｒｅｎｃｅ
．２０
１９
：２０３３
－２０４４
．


［６
１
］ＺｈｏｕＴ
，ＬａｗＫＭＹ．ＳｅｍａｎｔｉｃＲｅ
ｌａｔｅｄｎｅｓｓＥｎｈａｎｃｅｄＧｒａｐｈＮｅｔｗｏｒｋｆｏｒａｓｐｅｃｔ


ｃａｔｅｇｏｒ
｝
＾ｓｅｎｔｉｍｅｎｔａｎａｌｙｓ
ｉｓ
［Ｊ
］
．／／ＥｘｐｅｒｔＳｙｓｔｅｍｓｗｉｔｈＡｐｐ
ｌｉｃａｔｉｏｎｓ
．２０２２
，１９５
：


１
１６５６０
．


７９


