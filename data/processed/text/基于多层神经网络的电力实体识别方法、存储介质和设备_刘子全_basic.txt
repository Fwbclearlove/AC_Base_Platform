(19)国家知识产权局

(12)发明专利

(10)授权公告号 (45)授权公告日

(21)申请号202011337566.5

(22)申请日2020.11.25

(65)同一申请的已公布的文献号

申请公布号CN 112560486 A

(43)申请公布日2021.03.26

(73)专利权人国网江苏省电力有限公司电力科

学研究院 地址211103 江苏省南京市江宁区帕威尔

路1号 专利权人国家电网有限公司

北京邮电大学 国网江苏省电力有限公司 江苏省电力试验研究院有限公司

(72)发明人刘子全 李睿凡 王泽元 胡成博 熊永平 朱雪琼

(74)专利代理机构南京纵横知识产权代理有限

公司32224 专利代理师丁朋华

(51)Int.Cl.

G06F 40/295(2020.01) G06N 3/045(2023.01)

(56)对比文件

CN 110413779 A,2019.11.05 CN 111708868 A,2020.09.25

(54)发明名称

基于多层神经网络的电力实体识别方法、存 储介质和设备 (57)摘要

本发明公开了一种基于多层神经网络的电 力实体识别方法、存储介质和设备，将待识别的 电力语料输入至预先构建的BERT电力实体识别 模型中，得到电力实体标签的哈夫曼编码，通过 哈夫曼编码映射得到实体标签，进而得到识别出 的实体。通过语言模型训练语料对BERT语言模型 进行预训练；对电力语料数据标注电力实体标 签，构建电力实体识别语料；根据电力实体标签 在电力实体识别语料中的数量构建电力实体标 签的哈夫曼编码；在预训练得到的BERT语言模型 后增加分类层构成BERT电力实体识别模型，通过 电力实体识别语料对BERT电力实体识别模型进 行再次训练，得到训练好的BERT电力实体识别模 型。提高了电力领域中文命名实体识别的精度。

权利要求书2页 说明书7页 附图3页

CN 112560486 B 2025.03.04

CN 112560486 B

1.一种基于多层神经网络的电力实体识别方法，其特征在于： 将待识别的电力语料输入至预先构建的BERT电力实体识别模型中，得到电力实体标签 的哈夫曼编码，通过哈夫曼编码映射得到实体标签，进而得到识别出的实体；

所述BERT电力实体识别模型的构建步骤包括： 提取海量文本语料库，对海量本文语料库进行数据预处理，得到语言模型训练语料； 通过语言模型训练语料对BERT语言模型进行预训练； 对电力语料数据标注电力实体标签，构建电力实体识别语料； 根据电力实体标签在电力实体识别语料中的数量构建电力实体标签的哈夫曼编码； 在预训练得到的BERT语言模型后增加分类层构成BERT电力实体识别模型，通过电力实 体识别语料对BERT电力实体识别模型进行再次训练，得到训练好的BERT电力实体识别模 型；

所述分类层包括串接的全连接层和Sigmoid激活函数，分类层的输入为BERT语言模型 的输出，分类层的输出为预测的电力实体标签的哈夫曼编码；

训练BERT电力实体识别模型，包括： 将电力实体识别语料输入BERT电力实体识别模型，输出为预测的电力实体标签的哈夫 曼编码，通过哈夫曼编码映射得到对应的电力实体标签；

采用交叉熵损失计算电力实体识别语料上的真实标签和BERT电力实体识别模型输出 标签的差异，并通过AdamW优化器训练BERT电力实体识别模型，当电力实体识别语料验证集 上模型的损失不再下降时，停止训练，保存模型参数，得到训练好的BERT电力实体识别模 型。

2.根据权利要求1所述的一种基于多层神经网络的电力实体识别方法，其特征是：所述 对海量本文语料库进行数据预处理过程包括：

对文本分句并构建句子对，句子对用设定的连接标签进行连接，句子对头部加设定的 头部标签，句子对尾加设定的尾部标签；其中，原始文本相连的句子构成的句子对为正样 本，未连接的句子作为负样本；构建上下句关系预测任务的语料；

在每一个句子中，随机遮住部分的字用于预测；对于被遮住的字，其一部分用设定字符 串标签代替，一部分用随机的字来替换，剩余部分保持词不变，构成用于字预测任务的语 料；

根据遮住位置的真实词生成词标签，根据句子对的关系生成上下句关系标签，从而得 到语言模型训练语料。

3.根据权利要求2所述的一种基于多层神经网络的电力实体识别方法，其特征是：所述 通过语言模型训练语料对BERT语言模型进行预训练，包括步骤：

BERT语言模型的输入为经过预处理后的文本，输出为字标签和上下句关系标签； 计算BERT语言模型的输出和真实标签的损失值，将字标签的损失值和上下句关系的损 失值加和得到最终的损失值，根据最终损失值采用AdamW优化器训练BERT语言模型，当验证 集上模型的损失值不再下降时，停止训练，保存模型参数，得到BERT语言模型。

4.根据权利要求1所述的一种基于多层神经网络的电力实体识别方法，其特征是：所述 对电力语料数据标注电力实体标签，构建电力实体识别语料，包括：人工标注部分电力语料 数据，得到电力实体的知识库；使用该知识库对剩余的电力语料进行非人工电力实体标签

权 利 要 求 书 1/2 页

CN 112560486 B

标注，得到电力实体识别语料。

5.根据权利要求4所述的一种基于多层神经网络的电力实体识别方法，其特征是：所述 非人工电力实体标签标注，包括：

采用BMEO的标记形式构建得到电力实体识别语料，若一个字符单元是一个实体词的开 始，则标记为B‑实体类别；若一个字符单元是一个实体词的结束，则标记为E‑实体类别；若 一个字符单元是一个实体词的非开始非结束字符，则标记为M‑实体类别；若一个字符不属 于实体词则标注为O。

6.一种存储一个或多个程序的计算机可读存储介质，其特征在于：所述一个或多个程 序包括指令，所述指令当由计算设备执行时，使得所述计算设备执行根据权利要求1至5任 一一项所述的基于多层神经网络的电力实体识别方法。

7.一种计算设备，其特征在于：包括， 一个或多个处理器、存储器以及一个或多个程序，其中一个或多个程序存储在所述存 储器中并被配置为由所述一个或多个处理器执行，所述一个或多个程序包括用于执行根据 权利要求1至5任一一项所述的基于多层神经网络的电力实体识别方法中的指令。

权 利 要 求 书 2/2 页

CN 112560486 B

基于多层神经网络的电力实体识别方法、存储介质和设备

[0001] 本发明涉及电力实体识别技术领域，具体涉及一种基于多层神经网络的电力实体 识别方法、存储介质和设备。

[0002] 命名实体识别(NER)(也称为实体识别、实体分块和实体提取)是信息提取的一个 子任务，用于识别输入文本中的人名、地名、组织机构名或者根据特定需求划分的命名实 体，旨在将输入文本中的命名实体定位并分类为预先定义的类别。传统命名实体识别涉及 包括3大类(实体、时间和数字)和7小类(人名、地名、时间、数值、货币和百分比)的识别任 务。传统的命名实体识别方法可以分为基于词典的命名实体识别方法，基于规则的命名实 体识别方法以及基于传统机器学习的命名实体识别的方法。

[0003] 早期的研究基于规则方法，制定规则、维护规则的人力成本很高。基于机器学方 法，其中条件随机场模型(CRF)通过建立对数似然模型进行特征学习，但是训练代价较大， 训练速度较慢。基于深度学习模型，可以自动学习特征，长短期记忆网络模型(LSTM)可以通 过门控单元学习到长距离特征，注意力机制模型(Attention)可以在众多的输入信息中聚 焦于对NER任务更为关键的信息。

[0004] 大多数现有的NER方法基于数据驱动来实现，即数据量越大，模型的学习效果越 好。但在一些特定领域难以建立足够多的标注语料，模型的效果也会大打折扣。目前构建电 力领域命名实体识别工具存在标注语料不充足的问题，此外命名实体识别任务常出现标签 不平衡问题，即不同实体出现的频次差异较大，基于该数据训练的模型会导致模型偏向于 预测成频次多的标签，语料不足也加大了不平衡的问题处理难度。而人工标注需要有电力 领域专业知识，普通人难以直接准确识别电力领域实体，会造成成本高昂、标注较为缓慢问 题。

[0005] 为解决现有技术中的不足，本发明提供一种基于多层神经网络的电力实体识别方 法、存储介质和设备，解决了电力实体识别标签不平衡、识别不准确、人工标注慢的问题。

[0006] 为了实现上述目标，本发明采用如下技术方案：一种基于多层神经网络的电力实 体识别方法，包括步骤：将待识别的电力语料输入至预先构建的BERT电力实体识别模型中， 得到电力实体标签的哈夫曼编码，通过哈夫曼编码映射得到实体标签，进而得到识别出的 实体。

[0007] 进一步的，所述BERT电力实体识别模型的构建步骤包括：

[0008] 提取海量文本语料库，对海量本文语料库进行数据预处理，得到语言模型训练语 料；

[0009] 通过语言模型训练语料对BERT语言模型进行预训练；

[0010] 对电力语料数据标注电力实体标签，构建电力实体识别语料；

说 明 书 1/7 页

CN 112560486 B

[0011] 根据电力实体标签在电力实体识别语料中的数量构建电力实体标签的哈夫曼编 码；

[0012] 在预训练得到的BERT语言模型后增加分类层构成BERT电力实体识别模型，通过电 力实体识别语料对BERT电力实体识别模型进行再次训练，得到训练好的BERT电力实体识别 模型。

[0013] 进一步的，所述对海量本文语料库进行数据预处理过程包括：

[0014] 对文本分句并构建句子对，句子对用设定的连接标签进行连接，句子对头部加设 定的头部标签，句子对尾加设定的尾部标签；其中，原始文本相连的句子构成的句子对为正 样本，未连接的句子作为负样本；构建上下句关系预测任务的语料；

[0015] 在每一个句子中，随机遮住部分的字用于预测；对于被遮住的字，其一部分用设定 字符串标签代替，一部分用随机的字来替换，剩余部分保持词不变，构成用于字预测任务的 语料；

[0016] 根据遮住位置的真实词生成词标签，根据句子对的关系生成上下句关系标签，从 而得到语言模型训练语料。

[0017] 进一步的，所述通过语言模型训练语料对BERT语言模型进行预训练，包括步骤：

[0018] BERT语言模型的输入为经过预处理后的文本，输出为字标签和上下句关系标签；

[0019] 计算BERT语言模型的输出和真实标签的损失值，将字标签的损失值和上下句关系 的损失值加和得到最终的损失值，根据最终损失值采用AdamW优化器训练BERT语言模型，当 验证集上模型的损失值不再下降时，停止训练，保存模型参数，得到BERT语言模型。

[0020] 进一步的，所述对电力语料数据标注电力实体标签，构建电力实体识别语料，包 括：人工标注部分电力语料数据，得到电力实体的知识库；使用该知识库对剩余的电力语料 进行非人工电力实体标签标注，得到电力实体识别语料。

[0021] 进一步的，所述非人工电力实体标签标注，包括：

[0022] 采用BMEO的标记形式构建得到电力实体识别语料，若一个字符单元是一个实体词 的开始，则标记为B‑实体类别；若一个字符单元是一个实体词的结束，则标记为E‑实体类 别；若一个字符单元是一个实体词的非开始非结束字符，则标记为M‑实体类别；若一个字符 不属于实体词则标注为O。

[0023] 进一步的，所述分类层包括串接的全连接层和Sigmoid激活函数，分类层的输入为 BERT语言模型的输出，分类层的输出为预测的电力实体标签的哈夫曼编码。

[0024] 进一步的，训练BERT电力实体识别模型，包括：

[0025] 将电力实体识别语料输入BERT电力实体识别模型，输出为预测的电力实体标签的 哈夫曼编码，通过哈夫曼编码映射得到对应的电力实体标签；

[0026] 采用交叉熵损失计算电力实体识别语料上的真实标签和BERT电力实体识别模型 输出标签的差异，并通过AdamW优化器训练BERT电力实体识别模型，当电力实体识别语料验 证集上模型的损失不再下降时，停止训练，保存模型参数，得到训练好的BERT电力实体识别 模型。

[0027] 一种存储一个或多个程序的计算机可读存储介质，所述一个或多个程序包括指 令，所述指令当由计算设备执行时，使得所述计算设备执行根据前述的任一一项基于多层 神经网络的电力实体识别方法。

说 明 书 2/7 页

CN 112560486 B

[0028] 一种计算设备，包括，

[0029] 一个或多个处理器、存储器以及一个或多个程序，其中一个或多个程序存储在所 述存储器中并被配置为由所述一个或多个处理器执行，所述一个或多个程序包括用于执行 根据前述的任一一项基于多层神经网络的电力实体识别方法中的指令。

[0030] 本发明所达到的有益效果：

[0031] 1、本发明中实体标签的哈夫曼编码能通过哈夫曼树结构有效缓解了电力领域实 体标签不平衡的问题，提高了电力领域中文命名实体识别的精度；

[0032] 2、本发明中伪标注的数据标注方法，能够有效减少实体识别文本标注的人力成 本；

[0033] 3、本发明通过BERT预训练模型增强字的语义表示，通过微调的方式减少了训练参 数，节省了训练时间，数据量较小的情况下模型性能良好。

[0034] 图1为本发明具体实施方式中的数据标注流程图；

[0035] 图2为本发明具体实施方式中的电力实体标签分布示意图；

[0036] 图3为本发明具体实施方式中的BERT电力实体识别模型训练流程图；

[0037] 图4为本发明具体实施方式中的BERT电力实体识别模型结构示意图；

[0038] 图5为本发明具体实施方式中的哈夫曼编码示意图。

具体实施方式

[0039] 下面结合附图对本发明作进一步描述。以下实施例仅用于更加清楚地说明本发明 的技术方案，而不能以此来限制本发明的保护范围。

[0040] 实施例1：

[0041] 一种基于多层神经网络的电力实体识别方法，包括以下步骤：

[0042] 下述步骤1～5为BERT电力实体识别模型训练流程，流程图如图3所示；

[0043] 步骤1：从维基百科、百度百科爬取海量文本，构成海量文本语料库；

[0044] 步骤2：对海量本文语料库进行数据预处理，形成语言模型训练语料；

[0045] 对海量本文语料库进行数据预处理的流程主要包括以下三部分：

[0046] (1)对海量文本语料进行字符级别切分；

[0047] (2)对切分字符后的文本进行分句并构建句子对，对超过预设长度max‑num‑ tokens的句子对进行截断，句子对用设定的连接标签[SEP]进行连接，句子对头部加设定的 头部标签[CLS]，句子对尾加设定的尾部标签[SEP]；其中原始文本相连的句子构成的句子 对为正样本，未连接的句子作为负样本；构建上下句关系预测任务的语料；

[0048] (3)在每一个句子中，随机遮住15％的字用于预测；对于被遮住的字，其80％用设 定字符串[MASK]代替，10％用随机的一个字来替换，10％保持词不变；构成用于字预测任务 的语料；

[0049] (4)根据(2)中遮住位置的真实词生成词标签，根据(3)中句子对的关系生成上下 句关系标签；

[0050] (5)对于上述方式构建好的语言模型训练语料，采用比例为9：1的方式划分训练集

说 明 书 3/7 页

CN 112560486 B

[0051] 步骤3：使用语言模型训练语料对BERT语言模型进行预训练；

[0052] BERT(双向变压器编码)语言模型为现有的模型，其中存在多层神经网络；它将文 本序列首先通过嵌入层构成向量序列，然后通过层变压器(transformer)编码器对上下文 进行编码，可以实现文本序列到标签序列或是文本序列到单个标签的转换。

[0053] 语言模型预训练的任务包括两部分：字预测任务和上下句关系预测任务；

[0054] 在字预测任务中，输入为经过预处理后的文本，输出为经过BERT语言模型的 [MASK]标签位置的预测字；在上下句关系预测任务中，输入同样为经过预处理后的文本，输 出为经过BERT语言模型的[CLS]位置的上下句关系。由于两个任务的输入相同，因此对同一 条输入数据，可以得到两种预测标签，包括字标签和上下句关系标签。

[0055] 通过计算BERT语言模型对应位置的输出和真实标签的损失优化模型，损失包括： 字标签的损失和上下句关系的损失，损失计算采用交叉熵公式进行计算。通过将字标签的 损失值和上下句关系的损失值加和得到最终的损失值，根据最终损失值采用动量和权重衰 减的优化器(AdamW)优化器基于训练集训练BERT语言模型。当验证集上模型的损失值不再 下降时，模型停止训练，保存模型参数。

[0056] 步骤4：对电力语料数据标注电力实体标签(即进行BMEO标注)，构建电力实体识别 语料，并对电力实体识别语料进行预处理；

[0057] 待标注的电力语料采用国家电网公司出台的变电检修的规章制度的文本，该文本 包括：国家电网公司变电验收、运维、检测、评价、检修通用管理规定和反事故措施(以下简 称《五通一措》)。该文本包括大量电力命名实体名词，通过格式转换、清洗和去重操作得到 纯文本数据。

[0058] 从国家电网《五通一措》语料数据中随机抽取长度较长的1/4的数据(大约9800条) 组成标注池。通过设定一套电力命名实体的标注标准得到如表1所示的实体知识库统计信 息。图2中展示了电力命名实体标签的统计分布，可以看出电力命名实体标签具有的不平衡 性。注意到，实体标签为“O”(表示字符不属于实体词)的数量为322,331，远高于其他的标签 类。

[0059] 表1：电力实体库的统计信息

[0060]

说 明 书 4/7 页

CN 112560486 B

[0061]

[0062] 该步骤中需对电力语料进行预处理后方可对BERT模型进行微调。如图1所示，电力 语料的标注流程包括：

[0063] (1)标注部分电力语料数据(如抽取1/4电力语料数据)，得到电力实体的知识库， 知识库中存储二元组(实体类型和实体名)；

[0064] (2)使用该知识库对其他电力语料进行非人工伪标注，非人工伪标注指机器按照 电力实体知识库为无标签电力语料数据进行电力实体标签标注，得到电力实体识别语料， 即伪标注语料；

[0065] 对电力实体识别语料进行预处理，包括：

[0066] 对电力实体识别语料进行字符级切分，在句子头部加设定的头部标签[CLS]，句子 尾加设定的尾部标签[SEP]。(电力实体识别语料包括若干个单独的句子)。

[0067] 对预处理后的电力实体识别语料，采用比例为8：2的方式划分训练集和验证集；

[0068] 标注时采用BMEO的标记形式构建得到伪标注语料。具体而言，若一个字符单元是 一个实体词的开始，则标记为B‑实体类别；若一个字符单元是一个实体词的结束，则标记为 E‑实体类别；若一个字符单元是一个实体词的非开始非结束字符，则标记为M‑实体类别；若 一个字符不属于实体词则标注为O。例如句子“对串联绝缘子的电抗器进行检查”的标注结 果为“对O、串B‑1、联M‑1、绝M‑1、缘M‑1、子E‑1、的O、电B‑1、抗M‑1、器E‑1、进O、行O、检O、查 O”。采用该种方式能够极大减少人力成本。

[0069] 步骤4：根据电力实体标签在电力实体识别语料中的数量构建电力实体标签的哈 夫曼编码；

[0070] 在上述的电力命名实体标签中，采用哈夫曼树编码表示实体标签中的所有实体标 签类型，实体标签类型包括“B‑0”到“B‑8”， “M‑0”到“M‑8”， “E‑0”到“E‑8”和“O”等共28个实 体标签，0～8分别为表1中的9个编号。根据这28个实体标签在电力实体识别语料中的数量， 构建哈夫曼树。这28个实体标签存储于哈夫曼树的叶子节点。对于每个标签的哈夫曼编码 是根节点到该叶子节点的路径，定义路径中走向左子树为0，右子树为1。例如图5中叶子节 点X的哈夫曼编码为[1 ,1 ,1]，但由于不同的叶子节点有不同的长度，对路径长度小于最长 路径的叶子节点，对它的哈夫曼编码填充0至最长的路径，例如最长路径为5，上述标签X所 属的叶子节点的哈夫曼编码为[1,1,1,0,0]。

[0071] 通过对实体标签进行哈夫曼编码，能够缓解类别不平衡造成的问题。这是因为使 用模型对标签进行预测时，输出为它的哈夫曼编码路径，根据哈夫曼编码路径使用前向最 大匹配的方式进行标签映射。由于哈夫曼编码构建的特点，每次都是选择数量最少的两个

说 明 书 5/7 页

CN 112560486 B

实体标签构成子树，该子树根节点上路径的0和1分别指向选择的两个实体，因此标签中0和 1分布通常是较为平衡的。通过预测哈夫曼编码的路径，能够有效针对实体类别标签预测的 不平衡问题。

[0072] 步骤5：在预训练得到BERT语言模型后增加分类层构成BERT电力实体识别模型，通 过电力实体识别语料对BERT电力实体识别模型进行再次训练，得到BERT电力实体识别模 型；

[0073] 在预训练得到BERT语言模型后增加分类层构成BERT电力实体识别模型。如图4所 示，输入文本经过BERT电力实体识别模型时，先经过嵌入层变成文本向量序列E[1]到E [12]，然后经过多层编码器得到BERT语言模型的输出T[1]到T[12]，最后经过分类得到实体 标签。分类层包括依次连接的全连接层和Sigmoid激活函数，分类层的输入为BERT语言模型 的输出，分类层的输出为预测的电力实体标签的哈夫曼编码，通过哈夫曼编码映射得到对 应的电力实体标签。采用交叉熵损失计算电力实体识别语料上的真实标签和BERT电力实体 识别模型输出标签的差异，并通过AdamW优化器训练BERT电力实体识别模型。当电力实体识 别语料验证集上模型的损失不再下降时，模型停止训练，保存模型参数。

[0074] 步骤6：将待识别语料输入至BERT电力实体识别模型中，得到电力实体标签的哈夫 曼编码，通过哈夫曼编码映射得到实体标签，进而得到语料中预先定义9个类别的实体。

[0075] 通过将待识别语料逐句输入到BERT电力实体识别模型，得到句子中每个字实体标 签的哈夫曼编码；随后，通过哈夫曼编码和实体标签的映射关系，将待识别语料句子中每个 字实体标签的哈夫曼编码映射成原始的实体标签。通过判断实体标签组成的序列是否满足 “B‑X”、 “B‑X,E‑X”,“B‑X,M‑X,E‑X”和“B‑X,M‑X,M‑X,E‑X”等设定的实体类型的模板，其中X 表示实体类别(0～9)， “M‑X”可以重复多次，以满足不同长度的实体要求；最后将满足该模 板的子序列对应的字组合成实体作为最终的预测实体。

[0076] 实施例2：

[0077] 一种存储一个或多个程序的计算机可读存储介质，所述一个或多个程序包括指 令，所述指令当由计算设备执行时，使得所述计算设备执行根据前述的基于多层神经网络 的电力实体识别方法中的任一方法。

[0078] 一种计算设备，包括，

[0079] 一个或多个处理器、存储器以及一个或多个程序，其中一个或多个程序存储在所 述存储器中并被配置为由所述一个或多个处理器执行，所述一个或多个程序包括用于执行 根据前述的基于多层神经网络的电力实体识别方法中的任一方法的指令。

[0080] 本领域内的技术人员应明白，本申请的实施例可提供为方法、系统、或计算机程序 产品。因此，本申请可采用完全硬件实施例、完全软件实施例、或结合软件和硬件方面的实 施例的形式。而且，本申请可采用在一个或多个其中包含有计算机可用程序代码的计算机 可用存储介质(包括但不限于磁盘存储器、CD‑ROM、光学存储器等)上实施的计算机程序产 品的形式。

[0081] 本申请是参照根据本申请实施例的方法、设备(系统)、和计算机程序产品的流程 图和/或方框图来描述的。应理解可由计算机程序指令实现流程图和/或方框图中的每一流 程和/或方框、以及流程图和/或方框图中的流程和/或方框的结合。可提供这些计算机程序 指令到通用计算机、专用计算机、嵌入式处理机或其他可编程数据处理设备的处理器以产

说 明 书 6/7 页

CN 112560486 B

生一个机器，使得通过计算机或其他可编程数据处理设备的处理器执行的指令产生用于实 现在流程图一个流程或多个流程和/或方框图一个方框或多个方框中指定的功能的装置。

[0082] 这些计算机程序指令也可存储在能引导计算机或其他可编程数据处理设备以特 定方式工作的计算机可读存储器中，使得存储在该计算机可读存储器中的指令产生包括指 令装置的制造品，该指令装置实现在流程图一个流程或多个流程和/或方框图一个方框或 多个方框中指定的功能。

[0083] 这些计算机程序指令也可装载到计算机或其他可编程数据处理设备上，使得在计 算机或其他可编程设备上执行一系列操作步骤以产生计算机实现的处理，从而在计算机或 其他可编程设备上执行的指令提供用于实现在流程图一个流程或多个流程和/或方框图一 个方框或多个方框中指定的功能的步骤。

[0084] 以上所述仅是本发明的优选实施方式，应当指出，对于本技术领域的普通技术人 员来说，在不脱离本发明技术原理的前提下，还可以做出若干改进和变形，这些改进和变形 也应视为本发明的保护范围。

说 明 书 7/7 页

CN 112560486 B

说 明 书 附 图 1/3 页

CN 112560486 B

说 明 书 附 图 2/3 页

CN 112560486 B

说 明 书 附 图 3/3 页

CN 112560486 B