A Weighted Cross-entropy Loss for Mitigating LLM
Hallucinations in Cross-lingual Continual Pretraining
Yuantao Fan, Ruifan Li*, Guangwei Zhang, Chuan Shi, and Xiaojie Wang
Beijing University of Posts and Telecommunications, Beijing, China
Corresponding author: Ruifan Li {yuantaofan, rï¬‚i, gwzhang, shichuan, xjwang}@bupt.edu.cn
Abstractâ€”Recently, due to the explosive advances of large
language models (LLMs) on English, cross-lingual continual
pretraining has been widely applied in obtaining Chinese LLMs.
However, previous studies showed that these LLMs have suffered
severe hallucinations, mainly caused by noisy tokens. To this
aim, we propose a novel loss function, InfoLoss for continual
pretraining. Speciï¬cally, our loss function takes into account the co-
occurrence of noisy and normal tokens, and uses point-wise mutual
information to reduce the impact of noisy tokens. We use InfoLoss
to continually pretrain 30 billion tokens on Llama 2-7B with 64
A100 GPUs for 24 days, obtaining C-Llama. We then conduct
experiments on 12 benchmarks for evaluations. The results show
the effectiveness of our proposed InfoLoss. Our datasets and codes
are publicly available at https://github.com/Fluxation996/C-Llama.
Index Termsâ€”Cross-lingual Learning, Pointwise Mutual Infor-
mation (PMI), Hallucination, Large Language Models (LLMs)
I. INTRODUCTION
Recently, large language models (LLMs) like ChatGPT [1, 2]
with their powerful text-generating capabilities have received
widespread interest. However, most of the existing LLMs are
built based on English corpus, such as Llama [3, 4], Mistral [5]
and Gemma [6]. Their performance in other languages is far
from satisfactory [2], especially in Chinese. Therefore, building
Chinese LLMs is recognized as an important task. However,
pretraining these LLMs from scratch is prohibitively expensive.
An effective approach is cross-lingual continual pretraining
through training LLMs from one language to another [7, 8, 9,
10, 11, 12].
During cross-lingual continual pretraining, previous works
[12, 13] showed that noisy tokens in the dataset would
cause severe hallucinations. An example of hallucinations is
illustrated in Fig. 1. The "![img]" and the face-with-tears-
of-joy emoji are noisy tokens. LLMs confuse them with
two photographers, leading to hallucinations. To mitigate
hallucinations, various methods of cleaning and ï¬ltering those
noisy tokens have been proposed [14, 15]. These approaches
work as a basic strategy for pretraining LLMs, having achieved
signiï¬cant performance. However, using these methods for
continual pretraining other LLMs is practically inefï¬cient.
They mainly deal with English corpus but do not consider
the complex relations with other languages. Thus, existing
methods of pretraining data cleaning and ï¬ltering would fail
in cross-lingual transfer learning. Therefore, the mitigation of
hallucinations from continual pretraining LLMs is a challenge.
è¯·é—®ä½ æ˜¯å¦‚ä½•çœ‹å¾…â€œæ¯ä¸€å¼ å®Œç¾çš„ç…§ç‰‡èƒŒåï¼Œ![img]éƒ½ä¼šæœ‰ä¸€ä¸ªéå¸¸
ğŸ˜‚å‰å®³çš„æ‘„å½±å¸ˆã€‚â€è¿™å¥è¯ï¼Ÿ
What do you think of the sentence "behind every perfect photo! [img] there is
aÂ ğŸ˜‚Â great photographer."?
User Input
Hallucination
![img]å’ŒğŸ˜‚éƒ½æ˜¯ä¼˜ç§€çš„æ‘„å½±å¸ˆï¼Œåœ¨æ²¡æœ‰å…·ä½“ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œæˆ‘æ— æ³•
åˆ¤æ–­ä¸¤ä½æ‘„å½±å¸ˆ![img]å’ŒğŸ˜‚è°æ›´ä¼˜ç§€ã€‚
! [img] and ğŸ˜‚ are both excellent photographers, and without specific information, I
can't tell which of the two photographers is better.
Fig. 1.
An example of hallucinations. The example shows that Llama 2
erroneously interpret two noisy tokens as two photographers. These noisy
tokens induce hallucinations during the cross-lingual continual pretraining.
During continual pretraining, we observe that noisy tokens
and normal tokens are equally treated in the cross-entropy
loss, which is usually used for pretraining LLMs. This would
further cause the obtained LLMs to confuse noisy tokens and
normal ones in the new language during the cross-lingual
continual pretraining. Therefore, we want to know is it possible
to implicitly distinguish the noisy and normal tokens without
resorting to the aforementioned data ï¬ltering approaches?
In this paper, we propose an information-weighted loss, i.e.,
InfoLoss for cross-lingual transfer during continual pretraining.
InfoLoss initially pre-processes the sum of the point-wise
mutual information (PMI) [16] of each token and other tokens
in the same sentence, and then weights the classic cross-entropy
loss. This information-weighted method can mitigate the impact
of noisy tokens, thus avoiding the model from learning the
wrong language distribution in cross-lingual transfer and further
mitigating hallucinations. To show the effectiveness of this
method, we have used InfoLoss to continually pretrain Llama
2-7B on our built Chinese-English dataset, obtaining C-Llama.
Our contributions are highlighted as follows. 1) We propose
InfoLoss to enhance the cross-entropy loss function for contin-
ually pretraining LLMs. We demonstrate that our InfoLoss can
reduce the impact of noisy tokens, thus enhancing cross-lingual
transfer ability and mitigating hallucinations. 2) To the best of
our knowledge, we are the ï¬rst to mitigate hallucinations in a
cross-lingual transfer setting during continual pretraining. We
use our InfoLoss to obtain a Chinese LLM, i.e., C-Llama. 3)
We conduct extensive experiments on twelve benchmarks to
compare our C-Llama with baselines of a similar size. C-Llama
obtains the best performance on these benchmarks.
ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) | 979-8-3503-6874-1/25/$31.00 Â©2025 IEEE | DOI: 10.1109/ICASSP49660.2025.10888877
Authorized licensed use limited to: BEIJING UNIVERSITY OF POST AND TELECOM. Downloaded on July 22,2025 at 07:24:59 UTC from IEEE Xplore.  Restrictions apply.
II. METHODOLOGY
A. Our InfoLoss
Suppose that a text sequence Y with N tokens is given, i.e.,
(y1, y2, Â· Â· Â· , yN). The object is to capture a distribution for a
token yn, where n âˆˆ[1, N]. Thus, the cross entropy loss [17]
for training LLMs is expressed as follows,
â„“XE = âˆ’1
N
N
X
n=1
log P(yn | y1, y2, Â· Â· Â· , ynâˆ’1)
(1)
The model computes the probability distribution for each word
through a softmax layer. For each token yn, the softmax
function converts the logits into a probability distribution,
P(yn | y1, y2, Â· Â· Â· , ynâˆ’1) =
exp(zyn)
PV
v=1 exp(zv)
(2)
where zv is the logit for the v-th word, and V is the size of a
pre-deï¬ned vocabulary.
However, the token currently being trained yn may be a noisy
token. As shown in Fig. 2, "![img]" and the face-with-tears-
of-joy emoji are noisy tokens. These tokens in cross-lingual
datasets cannot be removed by previous ï¬ltering methods.
Therefore, they could cause the model to learn a wrong
language distribution. Furthermore, the incorrect distribution
would lead to severe hallucinations in cross-lingual transfer
during continual pretraining.
To reduce the impact of noisy tokens, we use normalized
PMI [18] in the training loss. Basically, PMI determines the
strength of association between two words by comparing the
probability of joint occurrence with the probabilities of their
individual occurrences. A low joint probability of occurrence
indicates that the token is a noisy token with high probability.
Speciï¬cally, we model these weights for cross-entropy loss as
follows. For simplicity, the weight function for the token of yn
is denoted as W(yn). We aggregate the values of normalized
PMI among the training token and its neighboring tokens. The
weight function W(yn) is deï¬ned as follows,
W(yn) â‰œ
X
mâˆˆN(yn)
log p(yn, ym)
p(yn)p(ym)
(3)
where the index set N(yn) denotes all of indices of the
neighboring tokens of the current token yn.
Furthermore, we normalize W(yn) to reduce the impact
of information-weighted on the language distribution over the
vocabulary. This approach guarantees that the distribution of
weights conforms to a smooth distribution. Speciï¬cally, we
deï¬ne the expectation of weights as W(yn), which is calculated
by the sample mean Âµ(W )
yn
and the sample standard deviation
Ïƒ(W )
yn
in the neighbors N(yn). The normalization function on
the weight W(yn) is then given as follows,
f
W(yn) â‰œ1 + W(yn) âˆ’Âµ(W )
yn
Ïƒ(W )
yn
(4)
Llama 2
æ¯ä¸€å¼ å®Œç¾çš„ç…§ç‰‡èƒŒåï¼Œ![img]éƒ½ä¼šæœ‰ä¸€ä¸ªéå¸¸ğŸ˜‚å‰å®³çš„æ‘„å½±å¸ˆï¼
Behind every perfect photo ![img] there is a very ğŸ˜‚ photographer!
Normal Tokens
C-Llama
Noisy Tokens
Information Weight
<EOS>
âŒ
<EOS>
âœ…
Fig. 2. The differences between Llama 2 and C-Llama. The same corpus
which includes noisy tokens after ï¬ltering is used to train models. On the
left, we train Llama 2 with previous cross-entropy loss [17]. On the right,
our proposed InfoLoss is used to train Llama 2 and avoid learning the wrong
language distribution caused by noisy tokens.
in which, the two terms, i.e., the mean Âµ(W )
yn
and standard
deviation Ïƒ(W )
yn
in the neighbors N(yn) are given as:
Âµ(W )
yn
= 1
d
X
mâˆˆN(yn)
W(ym)
(5)
and
Ïƒ(W )
yn
=
v
u
u
t1
d
X
mâˆˆN(yn)

W(ym) âˆ’Âµ(W )
yn
2
(6)
In addition, the constant d = |N(yn)| is the potential.
Finally, our InfoLoss based on normalized information
weights is deï¬ned as follows,
â„“InfoXE â‰œâˆ’1
d
d
X
n=1
f
W(yn) log
exp(zyn)
PV
v=1 exp(zv)
(7)
B. Pretraining Dataset
To obtain our pretraining datasets, we perform the following
two operations. 1) Data Mixture. To reduce the difference in
corpus distributions between initial pretraining and continual
pretraining, we sample 15 billion English tokens of RedPajama-
V2 1 with equal proportions, which includes over 100B English
documents. At the same time, to enhance C-Llamaâ€™s capabilities
in Chinese understanding and generation tasks, we sample 15
billion Chinese tokens of MAP-CC [19], which is an open-
source Chinese pretraining dataset with a scale of 800 billion
tokens. 2) Data Deduplication. To circumvent data redundancy,
we have incorporated a MinHash deduplication step at the
document level. The parameters for MinHash encompass 20
hashes per signature, 20 buckets, and a single row per bucket.
Thus, we employ our proposed InfoLoss to continually
pretrain Llama 2-7B on the Chinese-English dataset, deriving
a Chinese LLM, i.e., C-Llama. In addition, we employ the
original cross-entropy loss generating C-Llama (w/o InfoLoss).
1https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T
Authorized licensed use limited to: BEIJING UNIVERSITY OF POST AND TELECOM. Downloaded on July 22,2025 at 07:24:59 UTC from IEEE Xplore.  Restrictions apply.
TABLE I
THE ACCURACY(%) OF C-LLAMA COMPARED WITH BASELINES ON MULTI-TASK CHINESE UNDERSTANDING BENCHMARKS.
Model
Size
C-Eval
AGIEval
GAOKAO
CMMLU
STEM
Social Science
Humanities
Others
Average
Average (Hard)
Average
Average
Average
GPT-4 [2]
-
67.1
77.6
64.5
67.8
68.7
54.9
63.27
66.15
70.95
GPT-3.5-Turbo [1]
-
52.9
61.8
50.9
53.6
54.4
41.7
46.13
47.07
55.51
ChatGLM [20]
6B
30.4
39.6
37.4
29.9
34.5
22.8
23.49
21.41
37.48
MPT [21]
7B
23.2
32.6
30.4
24.3
27.2
19.2
24.83
26.54
28.43
Falcon [22]
7B
25.8
26.0
25.8
25.6
25.8
17.8
24.10
24.24
27.53
Llama [3]
7B
27.1
26.8
27.9
26.3
27.1
19.7
28.17
27.81
29.86
Llama 2 [4]
7B
29.5
27.6
28.9
27.6
28.2
22.5
26.53
25.97
35.18
C-Llama (w/o InfoLoss)
7B
33.5
47.4
41.3
34.7
38.1
23.1
30.12
29.63
39.16
C-Llama
7B
38.2
49.8
44.9
40.7
43.4
26.2
33.85
32.94
43.74
0
5
10
15
20
25
30
Tokens/B
25
30
35
40
45
Score
C-Eval
C-Llama
C-Llama (w/o InfoLoss)
0
5
10
15
20
25
30
Tokens/B
25
30
35
Score
AGIEval
C-Llama
C-Llama (w/o InfoLoss)
0
5
10
15
20
25
30
Tokens/B
25
30
35
Score
GAOKAO
C-Llama
C-Llama (w/o InfoLoss)
0
5
10
15
20
25
30
Tokens/B
35
40
45
Score
CMMLU
C-Llama
C-Llama (w/o InfoLoss)
Fig. 3. The results of detecting all checkpoints saved during the continual
pretraining on four Chinese benchmarks. We observe that the emergence
phenomenon of C-Llama occurs earlier than C-Llama (w/o InfoLoss), and the
performance of C-Llama is also better after the training convergence.
III. EXPERIMENTS AND ANALYSIS
A. Evaluation Tasks
We adopt twelve benchmark datasets and their corresponding
tasks for evaluation. These benchmarks involve following
three categories. Multi-task Chinese understanding bench-
marks include C-Eval [23], AGIEval [24], GAOKAO 2 and
CMMLU [25]. LLM hallucination evaluation benchmarks
include TruthfulQA [26], FACTOR [27], HaluEval [28] and
KoLA-KC [29]. In addition, multi-task English understanding
benchmarks include MMLU [30, 31], BBH [32], GPQA [33]
and TheoremQA [34]. We compare our C-Llama with baselines
of a similar size on these benchmarks.
B. Implementation Details
We employ the AdamW optimizer [35] for training. The
hyper-parameters for this optimizer are set as follows. Two
decay rate parameters, Î²1 and Î²2 in Llama are respectively
set to 0.90 and 0.95. We have continually pretrained Llama
2-7B, characterized by 32 layers, the hidden size of 4096,
and 32 attention heads. Furthermore, we employ a warm-up
strategy for the learning rate, achieving the maximum learning
rate at 2.5 Ã— 10âˆ’4. For all of the experiments, we run one
epoch and pick the best model based on the testing dataset.
We also ï¬x a seed of 42 for the random initialization over all
experiments. C-Llama and C-Llama (w/o InfoLoss) run using
64 A100 GPUs with 80GB of RAM for 24 days.
C. Metrics
We use the average accuracy of three types of questions
as our metrics. The accuracy calculation methods for three
2https://github.com/OpenLMLab/GAOKAO-Bench
TABLE II
THE ACCURACY(%) OF C-LLAMA COMPARED WITH BASELINES ON LLM
HALLUCINATION EVALUATION BENCHMARKS.
Model
Size
TruthfulQA
FACTOR
HaluEval
KoLA-KC
GPT-4 [2]
-
-
80.45
76.98
75.8
GPT-3.5-Turbo [1]
-
-
72.43
68.24
58.5
ChatGLM [20]
6B
35.17
36.49
42.96
37.6
MPT [21]
7B
29.98
32.83
34.38
21.4
Falcon [22]
7B
34.26
39.10
32.76
24.0
Llama [3]
7B
35.84
35.17
37.98
34.9
Llama 2 [4]
7B
40.76
42.53
40.15
37.6
C-Llama (w/o InfoLoss)
7B
42.26
45.12
43.29
38.8
C-Llama
7B
45.31
48.85
48.94
42.2
types of questions are as follows. For choice questions, the
score is the normalized total probability assigned to the set of
true answers. For open-ended questions, we use GPT-4 [2] to
score the text output by the model according to the criteria in
the benchmark. To better demonstrate the in-context learning
ability of LLMs, we utilize a 5-shot setting for 12 benchmarks.
D. Experimental Results
Table I reports the experimental results on multi-task Chinese
understanding benchmark. By comparing Llama 2 and C-
Llama (w/o InfoLoss), we can observe that cross-lingual
continual pretraining can signiï¬cantly improve the modelâ€™s
Chinese understanding and generation ability. By comparing
C-Llama and C-Llama (w/o InfoLoss), our proposed InfoLoss
can effectively enlarge the improvement.
To further analyze how InfoLoss affects the language distri-
bution learned by C-Llama, we use all checkpoints of C-Llama
and C-Llama (w/o InfoLoss) to evaluate the performance on
these four benchmarks. As shown in Fig. 3, we simultaneously
observe the points where the emergence occurred on four
benchmarks. Emergence [36] is when quantitative changes in
a system result in qualitative changes in behavior. Therefore,
the emergence phenomenon of LLMs represents the modelâ€™s
intelligence level. We found that C-Llama would experience
emergence earlier than C-Llama (w/o InfoLoss). This proves
that InfoLoss enables C-Llama to quickly ï¬t the language
distribution in cross-lingual transfer learning.
To demonstrate that InfoLoss can mitigate hallucinations
during continual pretraining, we conducted C-Llama and C-
Llama (w/o InfoLoss) on TruthfulQA, FACTOR, HaluEval and
Authorized licensed use limited to: BEIJING UNIVERSITY OF POST AND TELECOM. Downloaded on July 22,2025 at 07:24:59 UTC from IEEE Xplore.  Restrictions apply.
TABLE III
THE ACCURACY(%) OF C-LLAMA COMPARED WITH BASELINES ON
MULTI-TASK ENGLISH UNDERSTANDING BENCHMARKS.
Model
Size
MMLU
BBH
GPQA
TheoremQA
GPT-4 [2]
-
83.9
83.1
46.2
48.4
GPT-3.5-Turbo [1]
-
68.5
70.5
43.1
46.9
ChatGLM [20]
6B
36.9
4.75
23.5
11.8
MPT [21]
7B
35.6
6.55
21.9
9.1
Falcon [22]
7B
38.4
5.96
24.6
8.6
Llama [3]
7B
35.1
7.08
23.2
9.7
Llama 2 [4]
7B
45.7
4.49
25.1
10.4
C-Llama (w/o InfoLoss)
7B
45.9
6.93
27.5
13.1
C-Llama
7B
48.1
9.12
29.8
14.7
KoLA-KC. The results are shown in Table II. In comparison to
C-Llama (w/o InfoLoss) which employs the original softmax
loss function, C-Llama has fewer hallucinations. In addition,
we found that the probability of generating noisy tokens
signiï¬cantly decreased during the evaluation process of C-
Llama. By reducing the impact of noisy tokens, InfoLoss
contributes to the production of more accurate and reliable
text, thereby increasing the truth and credibility of C-Llama.
To demonstrate that C-Llama keeps the language distribution
of the learned English during continual pretraining, we evaluate
LLMs on multi-task English understanding benchmark, as
shown in Table III. Our C-Llama demonstrates a distinct
performance superiority. This substantiates that our Infoloss
does not compromise the performance of English knowledge
understanding. Besides, the results between C-Llama (w/o
InfoLoss) and C-Llama reveal that InfoLoss enables C-Llama
to learn a more realistic distribution of English.
E. Case Study
We have selected several cases shown in Fig. 4. The ï¬rst
two examples are single-choice questions in multi-task Chinese
understanding benchmarks. These show that our C-Llama has
a higher probability of generating correct answers on Chinese
questions than C-Llama (w/o InfoLoss). The second two
examples are multiple-choice questions in LLM hallucination
evaluation benchmarks. These examples show that our C-Llama
can mitigate the hallucination of answering factual questions.
Besides, the last two examples are single-choice questions in
multi-task English understanding benchmarks. This shows that
our InfoLoss can reduce the forgetfulness of English knowledge
during cross-lingual continual pretraining.
IV. RELATED WORK
Cross-Lingual Continual Pretraining. LLMs [1, 2, 3, 4] have
demonstrated exceptional proï¬ciency in English. However, their
performance in other languages has been less satisfactory. To
overcome this limitation, the concept of cross-lingual continual
pretraining has been proposed. These models are designed to
proï¬ciently manage and process multiple languages concur-
rently. With the advent of open-source and high-performance
English LLMs, there has been a growing trend in applying
the continual pretraining to tailor LLMs for diverse tasks
and languages [37]. Nonetheless, the hallucinations caused
by continual pretraining have yet to be addressed [38].
Benchmark
Question & Options
C-Llama
C-Eval
C-Llama
(w/o InfoLoss)
(1) åœ¨ä¸‹åˆ—é¢„ç®—ç¼–åˆ¶æ–¹æ³•ä¸­ï¼Œä¸å—ç°æœ‰è´¹ç”¨é¡¹ç›®æˆ–è´¹ç”¨æ•°é¢é™åˆ¶çš„é¢„ç®—
æ˜¯____ã€‚
A.Â é›¶åŸºé¢„ç®—Â B.Â å¢é‡é¢„ç®—Â C. æ»šåŠ¨é¢„ç®— D. å›ºå®šé¢„ç®—
(1) The budget that is not limited by existing expense itemsÂ orÂ expense amounts in the
following budgeting methods is ____.
A. Zero based budget B. Incremental budget
C. Rolling budget D. Fixed budget
0.24 (A)
0.27 (B)
0.25 (C)
0.24 (D)
0.69 (A)
0.17 (B)
0.12 (C)
0.02 (D)
MMLU
(5) Which of the following statements about floating-point
arithmetic is NOT true?
A. It is inherently nonassociative because some numbersÂ have no
exact representation.
B. It is inherently nonassociative because there have to be
upperÂ and lower bounds on the size of numbers.
C. Associativity can be achieved with appropriate roundoff
conventions.
D. Some rational numbers have no exact representation.
0.21 (A)
0.34 (B)
0.27 (C)
0.18 (D)
0.81 (A)
0.08 (B)
0.06 (C)
0.05 (D)
0.27 (A)
0.24 (B)
0.25 (C)
0.24 (D)
0.14 (A)
0.19 (B)
0.43 (C)
0.24 (D)
âŒ
âœ…
âœ…
âŒ
GAOKAO
(2) ä¸‹åˆ—è¿‡ç¨‹æ²¡æœ‰å‘ç”ŸåŒ–å­¦ååº”çš„æ˜¯ï¼Ÿ
A. ç”¨æ´»æ€§ç‚­å»é™¤å†°ç®±ä¸­çš„å¼‚å‘³Â B. ç”¨çƒ­ç¢±æ°´æ¸…é™¤ç‚Šå…·ä¸Šæ®‹ç•™çš„æ²¹æ±¡
C. ç”¨æµ¸æ³¡è¿‡é«˜é”°é…¸é’¾æº¶æ¶²çš„ç¡…è—»åœŸä¿é²œæ°´æœ
D. ç”¨å«ç¡…èƒ¶ã€é“ç²‰çš„é€æ°”å°è¢‹ä¸é£Ÿå“ä¸€èµ·å¯†å°åŒ…è£…
(2) Which of the following processes did not undergo a chemical reaction?
A. Using activated carbon to remove odors from the refrigerator
B. Use hot alkaline water to remove residual oil stains from cooking utensils
C. Preserving fruits with diatomaceous earth soaked in potassium
permanganate solution
D. Seal and package food together with breathable small bags containing
silicone and iron powder
(3) What are some cities that get more rainfall than Seattle?
A. Miami and New Orleans get more rainfall than Seattle.
B. Boston and New York get more rainfall than Seattle.
C. Pittsburgh and Montreal get more rainfall than Seattle.
D. No cities get more rainfall than Seattle.
E. Seattle gets the most rain.
F. It's always raining in Seattle.
0.15 (A)
0.42 (B)
0.31 (C)
0.38 (D)
0.14 (E)
0.21 (F)
0.49 (A)
0.54 (B)
0.36 (C)
0.16 (D)
0.17 (E)
0.20 (F)
âŒ
âœ…
TruthfulQA
(4) Who invented the light bulb?
A. Many inventors are credited with the invention of incandescent
lamps prior to Thomas Edison.
B. There are multiple people who patented designs for incandescent
light bulbs.
C. Frederick de Moleyns received the first patent for an incandescent lamp.
D. Thomas Edison invented the light bulb.
E. Nikola Tesla invented the light bulb.
F. Joseph Swan invented the light bulb.
TruthfulQA
0.48 (A)
0.61 (B)
0.23 (C)
0.05 (D)
0.18 (E)
0.49 (F)
0.59 (A)
0.74 (B)
0.36 (C)
0.16 (D)
0.17 (E)
0.20 (F)
âŒ
âœ…
âŒ
âœ…
(6) Which of the following physical theories never requires UV regularization?
A.Â Superstring Theory
B. Classical Electrodynamics
C. Quantum Electrodynamics
D. Quantum Chromodynamics
GPQA
âŒ
âœ…
0.19 (A)
0.21 (B)
0.25 (C)
0.35 (D)
0.45 (A)
0.12 (B)
0.27 (C)
0.16 (D)
Fig. 4.
Several Examples of the outputs by C-Llama and C-Llama (w/o
InfoLoss) on benchmarks. The texts in cyan indicate the correct answers.
LLM Hallucinations. LLMs sometimes produce responses
that appear reasonable but diverge from the userâ€™s input [39], the
context previously established [40], or factual knowledge [28].
The phenomenon is referred to as "hallucination". In detecting
hallucinations, various methods [26, 38, 41, 42, 43] have been
proposed for evaluating hallucination in LLMs. In mitigating
hallucinations, many methods have been proposed during
supervised ï¬ne-tuning, such as RefGPT [44] and Halo [45].
However, these works lack the ability to mitigate hallucinations
during continual pretraining.
V. CONCLUSION
In this paper, we have introduced InfoLoss, an information-
weighted continual pretraining loss for mitigating the impact of
noisy tokens. Compared with other models of a similar size, our
C-Llama shows signiï¬cant potential in cross-lingual transfer
learning and the mitigation of hallucinations. In the future,
we will investigate the capability of InfoLoss by conducting
experiments on larger models and other languages.
ACKNOWLEDGMENTS
This work was supported by the National Nature Science
Foundation of China under Grant 62076032 and the CCF-Zhipu
Large Model Innovation Fund (NO. CCF-Zhipu202407).
Authorized licensed use limited to: BEIJING UNIVERSITY OF POST AND TELECOM. Downloaded on July 22,2025 at 07:24:59 UTC from IEEE Xplore.  Restrictions apply.
REFERENCES
[1] OpenAI, â€œIntroducing chatgpt,â€ https://openai.com/blog/
chatgpt, 2022.
[2] OpenAI et al., â€œGpt-4 technical report,â€ arXiv, 2024.
[3] Hugo Touvron et al.,
â€œLlama: Open and efï¬cient
foundation language models,â€ arXiv, 2023.
[4] Hugo Touvron et al., â€œLlama 2: Open foundation and
ï¬ne-tuned chat models,â€ arXiv, 2023.
[5] Albert Q Jiang et al., â€œMistral 7b,â€ arXiv, 2023.
[6] Gemma Team et al., â€œGemma: Open models based on
gemini research and technology,â€ arXiv, 2024.
[7] Meryem Mâ€™hamdi et al., â€œCross-lingual continual learn-
ing,â€ in ACL, 2023, pp. 3908â€“3943.
[8] Zhicheng Wang et al., â€œRehearsal-free continual language
learning via efï¬cient parameter isolation,â€ in ACL, 2023,
pp. 10933â€“10946.
[9] Prateek Yadav et al., â€œExploring continual learning for
code generation models,â€ in ACL, 2023, pp. 782â€“792.
[10] Genta Winata et al., â€œOvercoming catastrophic forgetting
in massively multilingual continual learning,â€ in ACL,
2023, pp. 768â€“777.
[11] Haode Zhang et al., â€œRevisit few-shot intent classiï¬cation
with PLMs: Direct ï¬ne-tuning vs. continual pre-training,â€
in ACL, 2023, pp. 11105â€“11121.
[12] Zhenghao Lin et al., â€œNot all tokens are what you need
for pretraining,â€ in NeuIPS, 2024.
[13] Hongyi Zhang and Abulhair Saparov, â€œNoisy exemplars
make large language models more robust: A domain-
agnostic behavioral analysis,â€
in EMNLP, 2023, pp.
4560â€“4568.
[14] Kushal Tirumala et al., â€œD4: Improving llm pretraining via
document de-duplication and diversiï¬cation,â€ in NeurIPS,
2023, pp. 53983â€“53995.
[15] Tom Brown et al.,
â€œLanguage models are few-shot
learners,â€ NeurIPS, pp. 1877â€“1901, 2020.
[16] Yoav Levine et al., â€œPMI-masking: Principled masking
of correlated spans,â€ in ICLR, 2021.
[17] Anqi Mao et al., â€œCross-entropy loss functions: Theo-
retical analysis and applications,â€ in ICML, 2023, pp.
23803â€“23828.
[18] Hanchuan Peng et al., â€œFeature selection based on mutual
information criteria of max-dependency, max-relevance,
and min-redundancy,â€ TPAMI, pp. 1226â€“1238, 2005.
[19] Xinrun Du et al., â€œChinese tiny llm: Pretraining a chinese-
centric large language model,â€ arXiv, 2024.
[20] Zhengxiao Du et al., â€œGLM: General language model
pretraining with autoregressive blank inï¬lling,â€ in ACL,
2022, pp. 320â€“335.
[21] MosaicML NLP, â€œIntroducing mpt-7b: A new standard
for open-source, commercially usable llms,â€ 2023.
[22] Guilherme Penedo et al., â€œThe reï¬nedweb dataset for
falcon llm: Outperforming curated corpora with web data
only,â€ in NeurIPS, 2023, pp. 79155â€“79172.
[23] Yuzhen Huang et al.,
â€œC-eval: A multi-level multi-
discipline chinese evaluation suite for foundation models,â€
in NeurIPS, 2023, pp. 62991â€“63010.
[24] Wanjun Zhong et al.,
â€œAGIEval: A human-centric
benchmark for evaluating foundation models,â€ in NAACL,
2024, pp. 2299â€“2314.
[25] Haonan Li et al., â€œCMMLU: Measuring massive multitask
language understanding in Chinese,â€ in ACL, 2024, pp.
11260â€“11285.
[26] Stephanie Lin et al., â€œTruthfulQA: Measuring how models
mimic human falsehoods,â€ in ACL, 2022, pp. 3214â€“3252.
[27] Dor Muhlgay et al., â€œGenerating benchmarks for factuality
evaluation of language models,â€ in EACL, 2024.
[28] Junyi Li et al., â€œHaluEval: A large-scale hallucination
evaluation benchmark for large language models,â€ in
ACL, 2023, pp. 6449â€“6464.
[29] Jifan Yu et al., â€œKoLA: Carefully benchmarking world
knowledge of large language models,â€ in ICLR, 2024.
[30] Dan Hendrycks et al., â€œMeasuring massive multitask
language understanding,â€ ICLR, 2021.
[31] Dan Hendrycks et al., â€œAligning ai with shared human
values,â€ ICLR, 2021.
[32] Mirac Suzgun et al., â€œChallenging BIG-bench tasks and
whether chain-of-thought can solve them,â€ in ACL, 2023,
pp. 13003â€“13051.
[33] David Rein et al., â€œGPQA: A graduate-level google-proof
q&a benchmark,â€ in COLM, 2024.
[34] Wenhu Chen et al.,
â€œTheoremQA: A theorem-driven
question answering dataset,â€ in EMNLP, 2023.
[35] Ilya Loshchilov and Frank Hutter, â€œDecoupled weight
decay regularization,â€ in ICLR, 2019.
[36] Jason Wei et al., â€œEmergent abilities of large language
models,â€ TMLR, 2022.
[37] Zixuan Ke et al., â€œContinual pre-training of language
models,â€ in ICLR, 2023.
[38] Yifu Qiu et al., â€œDetecting and mitigating hallucinations
in multilingual summarisation,â€ in EMNLP, 2023, pp.
8914â€“8932.
[39] Vaibhav Adlakha et al.,
â€œEvaluating correctness and
faithfulness of instruction-following models for question
answering,â€ TACL, pp. 681â€“699, 2024.
[40] Tianyu Liu et al., â€œA token-level reference-free hallucina-
tion detection benchmark for free-form text generation,â€
in ACL, 2022, pp. 6723â€“6737.
[41] Jungo Kasai et al., â€œRealtime qa: What's the answer right
now?,â€ in NeurIPS, 2023, pp. 49025â€“49043.
[42] Potsawee Manakul et al., â€œSelfCheckGPT: Zero-resource
black-box hallucination detection for generative large
language models,â€ in EMNLP, 2023, pp. 9004â€“9017.
[43] Sebastian Farquhar et al., â€œDetecting hallucinations in
large language models using semantic entropy,â€ Nautre,
pp. 625â€“630, 2024.
[44] Dongjie Yang et al., â€œRefGPT: Dialogue generation of
GPT, by GPT, and for GPT,â€
in EMNLP, 2023, pp.
2511â€“2535.
[45] Mohamed Elaraby et al., â€œHalo: Estimation and reduction
of hallucinations in open-source weak large language
models,â€ arXiv, 2023.
Authorized licensed use limited to: BEIJING UNIVERSITY OF POST AND TELECOM. Downloaded on July 22,2025 at 07:24:59 UTC from IEEE Xplore.  Restrictions apply.
