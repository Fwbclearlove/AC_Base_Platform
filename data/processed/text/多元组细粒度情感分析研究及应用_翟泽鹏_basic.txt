密级 ： 公开  

避耆啣ｆＡｆ  

硕士学位论文  

题目 ：多元组细粒度情感分析  

研究及应用  

学号 ： ２０２０１４０５３８  

姓名 ： 翟泽鹏  

学科专业 ：电子信息 （计算机技术方向 ）  

培养方式 ： 全曰制  

导师 ： 李睿凡  

学院 ： 人工智能学院  

２０２３年 ０５月 ２８ 日  

■北京  

密级 ： 公开  

分ｉ却ｔ大綮  

硕士学位论文 （专业学位）  

题目 ：多元组细粒度情感分析  

研究及应用  

学号 ：２０２０１４０５３８  

姓名 ：翟泽鹏  

学科专业 ： 电子信息 （计算机技术方向）  

培养方式 ：全日制  

导师 ：李窨凡  

学院 ：人工智能学院  

２０２３年 ５月 ２８曰  

ＳｅｃｒｅｔＬｅｖｅｌ ：Ｐｕｂｌｉｃ  

Ｂｅｉ ｊ ｉｎｇＵｎｉｖｅｒｓｉｔｙｏｆＰｏｓｔｓａｎｄ  

Ｔｅｌｅｃｏｍｍｕｎｉｃａｔｉｏｎｓ  

ＴｈｅｓｉｓｆｏｒＭａｓｔｅｒＤｅｇｒｅｅ  

ＴＩＴＬＥ ：ＲＥＳＥＡＲＣＨＡＮＤＡＰＰＬＩＣＡＴＩＯＮＯＦ  

ＭＵＬＴＩ －ＴＵＰＬＥＦＩＮＥ －ＧＲＡＩＮＥＤＳＥＮＴＩＭＥＮＴ  

ＡＮＡＬＹＳＩＳ  

ＳｔｕｄｅｎｔＩＤ ：２０２０１４０５３８  

Ｃａｎｄｉｄａｔｅ ：ＺｅｐｅｎｇＺｈａｉ  

Ｓｕｂ ｊｅｃｔ ：ＥｌｅｃｔｒｏｎｉｃａｎｄＩｎｆｏｒｍａｔｉｏｎＥｎｇｉｎｅｅｒｉｎｇ  

ＴｒａｉｎｉｎｇＭｏｄｅ ：Ｆｕｌｌ －ｔｉｍｅ  

Ｓｕｐｅｒｖｉｓｏｒ：ＲｕｉｆａｎＬｉ  

Ｉｎｓｔｉｔｕｔｅ ：ＳｃｈｏｏｌｏｆＡｒｔｉｆｉｃｉａｌＩｎｔｅｌｌｉｇｅｎｃｅ  

Ｍａｙ２８ ，２０２３  

答辩委员会名单  

职务姓 名职 称工 作 单 位  

主席鲁鹏副教授北京邮电大学  

委员周延泉副教授北京邮电大学  

委员李睿凡副教授北京邮电大学  

委员李蕾教授北京邮电大学  

委员杜建凤高级工程师中国移动北京公司  

秘书刘咏彬讲师北京邮电大学  

答辩日期２０２３ ．５ ．２８  

多元组细粒度情感分析研究及应用  

摘 要  

互联网上众多平台包含大量的文本 ，其蕴含的情感信息具有重要  

的商业价值以及参考价值 ，而与传统的粗粒度情感分析不同 ，细粒度  

情感分析可以提取文本中方面词 、意见词等更具体的信息 。本文聚焦  

于两大前沿多元组细粒度情感分析任务 ： 方面级三元组抽取 （Ａｓｐｅｃｔ  

ＳｅｎｔｉｍｅｎｔＴｒｉｐ ｌｅｔＥｘｔｒａｃｔｉｏｎ ，ＡＳＴＥ ） 以及结构化情感分析 （Ｓｔｒｕｃｔｅｄ  

ＳｅｎｔｉｍｅｎｔＡｎａｌｙｓｉｓ ， ＳＳＡ ） 。 本文主要研究内容如下 ：  

、在方面级三元组抽取任务中 ，最近相关研究工作采用机器阅  

读理解架构通过多轮询问得到方面词以及对应的意见词和情感 ，并取  

得了令人印象深刻的结果 。然而这些方法在

一句话包含多个方面词时  

会遇到其他方面词造成干扰这

一问题 ，从而不能准确地鉴别各个方面  

词对应的情感信息 。为克服上述挑战 ，木文提出

一种基于掩码上下文  

的机器阅读理解 （ＣＯＭ

－ＭＲＣ ）框架 。 该方法包含三个部分 ： 掩码式  

数据增强 、交互式式判别模型以及阶段式推理方法 。在掩码式数据增  

强中 ，利用掩码方面词来增强训练数据 ， 设置相同的查询以及不同的  

掩码上下文作为输入 。在交互式判别模型中 ，各个子任务之间可以交  

互信息得到更准确的判别结果 。在阶段式推理方法中 ，设计先推理方  

面词再通过掩码无关方面词推理意见词和情感的两阶段式推理方法 。  

三部分协同工作 ， 实验结果表明 ， ＣＯＭ

－ＭＲＣ取得了先进的性能 ，验  

证了方法的有效性 。  

二 、在结构化情感分析任务中 ，最近相关的研究工作通常将其转  

化为双词汇依赖解析问题 。然而因为该任务存在重叠实体以及非连续  

实体问题 ， 故这些转化方法都是不等价的 。为克服上述挑战 ， 本文提  

一处理重叠实体以及非连续实体的双词汇依赖解析方  

法 。该方法分为两种类型的解析边 ： 关系预测以及单词提取 ， 分别对  

应解决重叠实体以及非连续实体问题 。并且将双词汇依赖解析方法转  

一的 ２维表格填充机制 ，称为ＵＳＳＡ机制 。该机制将表格划分  

为下半三角以及上半三角 ，分别对应关系预测以及单词提取 。最后设  

－个模型以适配ＵＳＳＡ机制 ，该模型中的双轴向注意力机制可以捕  

捉表格中关系类型的行列关联信息 ， 以进行更准确的识别 。实验结果  

表明 ， ＵＳＳＡ取得了先进的性能 ， 验证了方法的有效性 。  

三 、基于对方面级三元组抽取和结构化情感分析任务的研究 ，本   文开发了

一个细粒度情感分析系统 。该系统支持注册登陆 、用户管理、  

在线处理等多个功能 ， 简单易用 。  

关键词 ： 深度学习 方面级情感分析结构化情感分析注意力机制  

ＲＥＳＥＡＲＣＨＡＮＤＡＰＰＬＩＣＡＴＩＯＮＯＦ  

ＭＵＬＴＩ －ＴＵＰＬＥＦＩＮＥ －ＧＲＡＩＮＥＤ  

ＳＥＮＴＩＭＥＮＴＡＮＡＬＹＳＩＳ  

ＡＢＳＴＲＡＣＴ  

Ｔｈｅｉｎｔｅｒ ｎｅｔｉｓｆ ｉｌｌｅｄｗｉｔｈｃｏｐ ｉｏｕｓｔｅｘｔｕａｌｄａｔａ ，ｈａｒｂｏｒｉｎｇ ｉｎｖａｌｕａｂｌｅ  

ｓｅｎｔｉｍｅｎｔｉｎｆｏｒｍａｔｉｏｎｗｉｔｈｓｉｇｎｉｆｉｃａｎｔｃｏｍｍｅｒｃｉａｌａｎｄｒｅｆｅｒｅｎｔｉａｌｖａｌｕｅ ．  

Ｉｎｃｏｎｔｒａｓｔｔｏｔｒａｄｉｔ ｉｏｎａｌｃｏａｒｓｅ

－ ｇｒａｉｎｅｄｓｅｎｔｉｍｅｎｔａｎａｌｙｓｉｓ ，ｆｉｎｅ － ｇｒａｉｎｅｄ  

ｓｅｎｔｉｍｅｎｔａｎａｌｙｓｉｓｅｘｔｒａｃｔｓｍｏｒｅｓｐｅｃｉｆ ｉｃｉｎｆｏｒｍａｔｉｏｎ ，ｓｕｃｈａｓａｓｐｅｃｔｗｏｒｄｓ  

ａｎｄｏｐ ｉｎｉｏｎｗｏｒｄｓ ．Ｔｈｉｓｄｉｓｓｅｒｔａｔｉｏｎｆｏｃｕｓｅｓｏｎｔｗｏｃｕｔｔｉｎｇ

－ｅｄｇｅｍｕｌｔｉ ？  

ｔｕｐ ｌｅｆｉｎｅ － ｇｒａｉｎｅｄｓｅｎｔｉｍｅｎｔａｎａｌｙｓｉｓｔａｓｋｓ ：ＡｓｐｅｃｔＳｅｎｔｉｍｅｎｔＴｒ ｉｐ ｌｅｔ  

Ｅｘｔｒａｃｔｉｏｎ （ＡＳＴＥ）ａｎｄＳｔｒｕｃｔｕｒｅｄＳｅｎｔｉｍｅｎｔＡｎａｌｙｓｉｓ （ＳＳＡ）

ｐｒ ｉｍａｒｙ  

ｒｅｓｅａｒｃｈａｒｅａｓａｒｅａｓｆｏｌｌｏｗｓ ：  

Ｆｉｒｓｔ ，ＩｎｔｈｅＡＳＴＥｔａｓｋ ，ｒｅｃｅｎｔｒｅｌｅｖａｎｔｓｔｕｄｉｅｓｈａｖｅｅｍｐ ｌｏｙｅｄ  

ｍａｃｈｉｎｅｒｅａｄｉｎｇｃｏｍｐｒｅｈｅｎｓｉｏｎａｒｃｈｉｔｅｃｔｕｒｅｓｔｏｏｂｔａｉｎａｓｐｅｃｔｗｏｒｄｓａｎｄ  

ｃｏｒｒｅｓｐｏｎｄｉｎｇｏｐ ｉｎｉｏｎｗｏｒｄｓａｎｄｓｅｎｔｉｍｅｎｔｓｔｈｒｏｕｇｈｍｕｌｔｉｐ ｌｅｒｏｍｉｄｓｏｆ  

ｑｕｅｓｔｉｏｎｉｎｇ，ａｃｈｉｅｖｉｎｇ ｉｍｐｒｅｓｓｉｖｅｒｅｓｕｌｔｓ ．Ｈｏｗｅｖｅｒ ，ｔｈｅｓｅｍｅｔｈｏｄｓｆａｃｅ  

ｄｉｆ ｉｃｕｌｔｉｅｓｗｈｅｎａｓｅｎｔｅｎｃｅｃｏｎｔａｉｎｓｍｕｌｔｉｐ ｌｅａｓｐｅｃｔｗｏｒｄｓｔｈａｔｉｎｔｅｒｆｅｒｅ  

ｗｉｔｈｅａｃｈｏｔｈｅｒ ，ｔｈｅｒｅｂｙｈｉｎｄｅｒｉｎｇａｃｃｕｒａｔｅｉｄｅｎｔｉｆｉｃａｔｉｏｎｏｆｓｅｎｔｉｍｅｎｔ  

ｉｎｆｏｒｍａｔｉｏｎｆｏｒｅａｃｈａｓｐｅｃｔ．Ｔｏｏｖｅｒｃｏｍｅｔｈｅｓｅｃｈａｌｌｅｎｇｅｓ ，ｔｈｅｄｉｓｓｅｒｔａｔｉｏｎ  

ｐｒｏｐｏｓｅｓａＣｏｎｔｅｘｔ －ｂａｓｅｄＭａｓｋｉｎｇＭａｃｈｉｎｅＲｅａｄｉｎｇＣｏｍｐｒｅｈｅｎｓｉｏｎ  

（ＣＯＭ －ＭＲＣ）ｆ ｒａｍｅｗｏｒｋ ，ｃｏｎｓｉｓｔｉｎｇｏｆｔｈｒｅｅｐａｒｔｓ ：ｍａｓｋｅｄｄａｔａ  

ａｕｇｍｅｎｔａｔｉｏｎ ， ｉｎｔｅｒａｃｔｉｖｅｄｉｓｃｒｉｍｉｎａｔｉｏｎｍｏｄｅｌｓ ，ａｎｄｓｔａｇｅｄｉｎｆｅｒｅｎｃｅ  

ｍｅｔｈｏｄｓ ．Ｂｙｓｙｎｅｒｇ ｉｚｉｎｇ ｔｈｅｓｅｔｈｒｅｅｃｏｍｐｏｎｅｎｔｓ ，ｔｈｅＣＯＭ

－ＭＲＣａｃｈｉｅｖｅｓ  

ａｄｖａｎｃｅｄ ｐｅｒｆｏｒｍａｎｃｅ ，ｖａｌｉｄａｔｉｎｇ ｉｔｓｅｆｆｅｃｔｉｖｅｎｅｓｓ ．  

Ｓｅｃｏｎｄ ，ｉｎｔｈｅＳＳＡｔａｓｋ ，ｒｅｃｅｎｔｒｅｌａｔｅｄｓｔｕｄｉｅｓｔｙｐ ｉｃａｌｌｙｃｏｎｖｅｒｔｔｈｅ  

ｐｒｏｂｌｅｍｉｎｔｏａｂｉ

－ ｌｅｘｉｃａｌｄｅｐｅｎｄｅｎｃｙｐａｒｓｉｎｇｐｒｏｂｌｅｍ ．Ｈｏｗｅｖｅｒ ，ｄｕｅｔｏ  

ｏｖｅｒｌａｐｐ ｉｎｇａｎｄｎｏｎ －ｃｏｎｔｉｇｕｏｕｓｅｎｔｉｔｉｅｓｉｎｔｈｅｔａｓｋ ，ｔｈｅｓｅｔｒａｎｓｆｏｒｍａｔｉｏｎ  

ｍｅｔｈｏｄｓａｒｅｉｎｅｑｕｉｖａｌｅｎｔ．Ｔｏｓｕｒｍｏｕｎｔｔｈｅｓｅｃｈａｌｌｅｎｇｅｓ ，ｔｈｅｄｉｓｓｅｒｔａｔｉｏｎ  

ｉｎｔｒｏｄｕｃｅｓａｂｉ － ｌｅｘｉｃａｌｄｅｐｅｎｄｅｎｃｙｐａｒｓｉｎｇｍｅｔｈｏｄｃａｐａｂｌｅｏｆｕｎｉｆｏｒｍｌｙ  

ａｄｄｒｅｓｓｉｎｇｏｖｅｒｌａｐｐ ｉｎｇａｎｄｎｏｎ

－ｃｏｎｔｉｇｕｏｕｓｅｎｔｉｔｉｅｓ ．Ｔｈｅｍｅｔｈｏｄ  

ｃｏｍｐｒｉｓｅｓｔｗｏ ｐａｒｓｉｎｇｅｄｇｅｔｙｐｅｓ ：ｒｅｌａｔｉｏｎ

ｐｒｅｄｉｃｔｉｏｎａｎｄｗｏｒｄｅｘｔｒａｃｔｉｏｎ ，  

ｗｈｉｃｈｃｏｒｒｅｓｐｏｎｄｔｏｔｈｅｒｅｓｏｌｕｔｉｏｎｏｆｏｖｅｒｌａｐｐ ｉｎｇａｎｄｎｏｎ

－ｃｏｎｔｉｇｕｏｕｓ  

ｅｎｔｉｔｙ ｉｓｓｕｅｓ ．Ｆｕｒｔｈｅｒｍｏｒｅ ，ｔｈｅｂｉ － ｌｅｘｉｃａｌｄｅｐｅｎｄｅｎｃｙｐａｒｓｉｎｇｍｅｔｈｏｄｉｓ  

ｔｒａｎｓｆｏｒｍｅｄｉｎｔｏａｕｎｉｆ ｉｅｄ２Ｄｔａｂｌｅ

－ｆｉｌｌｉｎｇｍｅｃｈａｎｉｓｍｃａｌｌｅｄｔｈｅＵＳＳＡ  

ｍｅｃｈａｎｉｓｍ．Ｔｈｉｓｍｅｃｈａｎｉｓｍｄｉｖｉｄｅｓｔｈｅｔａｂｌｅｉｎｔｏｌｏｗｅｒａｎｄｕｐｐｅｒ  

ｔｒ ｉａｎｇ ｌｅｓ ，ｃｏｒｒｅｓｐｏｎｄｉｎｇ ｔｏｒｅｌａｔｉｏｎｉｄｅｎｔｉｆｉｃａｔｉｏｎａｎｄｗｏｒｄｅｘｔｒａｃｔｉｏｎ ，  

ｒｅｓｐｅｃｔｉｖｅｌｙ

．Ｆｉｎａｌｌｙ，ａｍｏｄｅｌｃｏｍｐａｔｉｂｌｅｗｉｔｈｔｈｅＵＳＳＡｍｅｃｈａｎｉｓｍｉｓ  

ｄｅｓｉｇｎｅｄ ，ｗｈｏｓｅｂｉ －ａｘｉａｌａｔｅｎｔｉｏｎｍｅｃｈａｎｉｓｍｃａｐｔｕｒｅｓｒｏｗ －ｃｏｌｕｍｎ  

ｃｏｒｒｅｌａｔｉｏｎｉｎｆｏｒｍａｔｉｏｎｏｆｒｅｌａｔ ｉｏｎｔｙｐｅｓｉｎｔｈｅｔａｂｌｅｆｏｒｍｏｒｅａｃｃｕｒａｔ ｅ  

ｉｄｅｎｔｉｆｉｃａｔｉｏｎ ．ＴｈｅｅｘｐｅｒｉｍｅｎｔａｌｒｅｓｕｌｔｓｄｅｍｏｎｓｔｒａｔｅｔｈｅＵＳＳＡ

＇ ｓａｄｖａｎｃｅｄ  

ｐｅｒｆｏｒｍａｎｃｅａｎｄｅｆｅｃｔｉｖｅｎｅｓｓ ．  

Ｔｈｉｒｄ ，ｂａｓｅｄｏｎｔｈｅｒｅｓｅａｒｃｈｏｆＡＳＴＥａｎｄＳＳＡｔａｓｋｓ ，ａｆｉｎｅ － ｇｒａｉｎｅｄ  

ｓｅｎｔｉｍｅｎｔａｎａｌｙｓｉｓｓｙｓｔｅｍｈａｓｂｅｅｎｄｅｖｅｌｏｐｅｄ．Ｔｈｅｓｙｓｔｅｍｓｕｐｐｏｒｔｓ  

ｍｕｌｔｉｐ ｌｅｆｅａｔｕｒｅｓ ，ｓｕｃｈａｓｒｅｇ ｉｓｔｒａｔｉｏｎ

， ｌｏｇ ｉｎ ，ｕｓｅｒｍａｎａｇｅｍｅｎｔ ，ａｎｄｏｎｌｉｎｅ  

ｐｒｏｃｅｓｓｉｎｇ，ｍａｋｉｎｇ ｉｔｕｓｅｒ

－ｆ ｒｉｅｎｄｌｙａｎｄａｃｃｅｓｓｉｂｌｅ ．  

ＫＥＹＷＯＲＤＳ ：ｄｅｅｐｌｅａｒｎｉｎｇ，ａｓｐｅｃｔ

－ｂａｓｅｄｓｅｎｔｉｍｅｎｔａｎａｌｙｓｉｓ ，ｓｔｒｕｃｔｕｒｅｄ  

ｓｅｎｔｉｍｅｎｔａｎａｌｙｓｉｓ ，ａｔｅｎｔｉｏｎｍｅｃｈａｎｉｓｍ ．  

ＴＶ  

目 录  

一章绪论 ４  

１ ． １研究背景及其意义 ４  

１ ．２ 国内外研究现状 ６  

１ ．２ ． １ 情感分析发展历程 ６  

１ ．２ ．２方面级三元组抽取 ７  

１ ．２ ．３结构化情感分析 ８  

１ ．３本文研究内容与主要工作 ８  

１ ．３ ．１ 方面级三元组抽取 ８  

１ ．３ ．２结构化情感分析 １ １  

１ ．４本文的组织结构 １２  

１ ．５本章小结 ．  １４  

第二章 相关技术 １５  

２ ． １溁度学习模型 １５  

２ ． １ ．１循环神经网络 １５  

２丄２卷积神经网络 １７  

２ ． １ ．３注意力机制 １８  

２ ？１ ．４Ｔｒａｎｓｆｏｒｍｅｒ １９  

２ ．２深度学习技术 ２１  

２ ．２ ． １Ｄｒｏｐｕｔ ２１  

２ ．２ ．２Ｎｏｒｍａｌｉｚａｔｉｏｎ ２１  

２ ．２ ．３损失函数 ２２  

２ ．２ ．４优化器 ２３  

２ ．３ 本章小结 ２４  

第三章基于掩码上下文机器阅读理解框架的方面级三元组抽取方法    ２５  

３ ． １ 本章引论 ２５  

３ ．２基于掩码上下文的机器阅读理解框架 ２７  

３ ．２ ．１框架总览 ２７  

３ ．２ ．２掩码式数据增强 ２８  

３ ．２ ．３交互式判别模型 ２９  

３ ．２ ．４阶段式推理方法 ３ １  

３ ．３实验对比与分析 ３３  

３ ．３ ． １ 数据集介绍 ３３  

３ ．３ ．２实验参数与设置 ３４  

３ ．３ ．３评估指标 ３４  

３ ．３ ．４基线方法 ３５  

３ ．３ ．５实验结果 ３６  

３ ．３ ．６实验分析 ３８  

３ ．４木章小结  ４５  

第四章基十统

一表格填充机制端到端框架的结构化情感分析方法４６  

４ ． １ 本￥弓 丨论 ４６  

？的表格填充机制ＵＳＳＡ ４９  

４ ．２ ． １表格填充 ５０  

４ ．２ ．２意见元组解码 ５１  

４ ．３模型架构 ５３  

４ ．３ ． １ 编码层 ５３  

４ ．３ ．２词对表亦层 ， ５３  

４ ．３ ．３细化策略 ５４  

４ ．３ ．＂页测层 ５５  

４ ．３ ．５损失函数 ５５  

４ ．４实验对比和分析 ５６  

４ ．４ ． １ 数据集介绍 ５６  

４ ．４ ．２实验参数与设置 ５７  

４ ．４ ．３评估指标 ， ５８  

４ ．４ ．４基线方法 ５９  

４ ．４ ．５实验结果 ５９  

４ ．４ ．６实验分析 ６ １  

４ ．５本章小结  ６３  

第五章 多元组细粒度情感分析系统 ６４  

５ ． １ 系统需求分析 ６４  

５ ．２ 系统整体设计 ６５  

５ ．３前端部分设计 ６６  

５ ．４后端部分设计 ６８  

５ ．５系统工作流程 ６９  

５ ．６本章小结 ７０  

第六章 总结与展望 ７１  

６ ． １ 工作总结 ７１  

６ ．２未来工作展望 ７２  

参考文献 ７３  

北京邮电大学电子信息硕士学位论文  

第 一章绪论  

Ｕ研究背景及其意义  

随着互联网的普及和发展 ，越来越多的信息以文本形式进行传播 ，如社交媒  

体上的微博 、微信朋友圈 、评论区 、新闻报道 、产品评价等 。这些文本信息不仅  

包含了人们对事物的描述 ，还表达了人们对事物的情感 、态度和观点 ，对于各个  

领域的应用都有着重要的价值和作用 。  

情感分析作为自然语言处理领域的

一个重要研究方向 ，是对文本中情感信息  

进行自动化提取和分析的技术手段 。情感分析技术的发展不仅能够帮助人们更好  

地理解和分析文本信息 ，而且还能够为商业 、政府 、学术等各个领域提供有价值  

的信息支持 。例如 ，在商业领域 ，情感分析可以帮助企业了解消费者对其产品或  

服务的评价和意见 ， 优化产品设计和营销策略 ， 提高市场竞争力 ； 在政府领域 ，  

情感分析可以帮助政府了解公众对政策的反应和态度 ，为政策制定和危机管理提  

供决策依据 ；在学术领域 ，情感分析作为自然语言处理的经典任务 ， 其研究结果  

可以为自然语言处理技术的进

一步发展提供支持 。  

基石 ＼ｍｍｍ＼文语言麵

情感词偏移器情感损失ＢＥＲＴ   句子主观性分析（Ｗｉｅｂｅ ，１９９４） （Ｔａｎｇ ，２０１４）（Ｄｅｖｌ ｉｎ ，２０１９ ）  

（Ｗ ｉｅｂｅ ，１９９４） 观点摘要语义和情感ＵＬＭＦｉＴ  

｛Ｈｕ＆Ｕｕ ，２００４） （Ｍａａｓ ，２０１ １ ）（Ｈｏｗａｒｄ ， ２０１８ ）   八

． ．Ａ＾Ａ入 …，   ：  

ＳｅｎｔｉＷｏｒｄＮｅｔＲＮＴＮＧＰＴ３  

词袋和语法规则 （Ｅｓｕ 丨 丨 ， ２００６） （Ｓｏｃｈｅｒ

，２０１３ ） （Ｂｒｏｗｎ

，２０２０ ）  

（Ｙｕｒｎｅｙ ，２００２）ＳＯ －ＣＡＬＣＮＮＣｈａｔＧＰＴ 、 ＧＰＴ４  

（Ｔａｂｏａｄａ ，２０１ １ ）（Ｋｉｍ ，２０１４ ） （ＯｐｅｎＡ Ｉ ，２０２３）  

评论情感分析

］ ？厂深度学习＼ ［ 大模型  

图 １ １ 情感分析里程碑  

情感分析从粒度来说大体上可分为粗粒度与细粒度 。粗粒度情感分析包括篇  

章级情感分析 、句子级情感分析等 。篇章级情感分析旨在判别

一篇文章整体的情  

感极性 ，例如分析

一篇包含多个句子或段落的长篇微博的情感倾向 ，或者分析媒  

一章 绪论  

体对于社会热点事件发布的

一篇议论长文整体是褒义还是贬义 ，然而整篇文章可  

能对不同人物或事件可能有不同的情感倾向 ，故有时将

一篇文章判定为

一种情感  

极性并不合理 。 由此句子级情感分析是粒度更加细的

一种 ，其旨在分析

一句话整  

体呈现的情感极性 ， 但同样道理 ，

一句话也可能包括不同的情感极性 。例如

“ 这  

里环境不错 ，但服务

” 的情感极性是积极的 ，但对于

的情感倾向是消极的 ７ 故在

一些情况下分析

一句话整体的情感也不合理 。 此外 ，  

粗粒度的情感分析会丢失

一些关键的细节信息 ，例如仅知道情感极性是哪

一种并  

不足够 ，可能还需要知道对哪个事物的情感 、判断依据是什么以及谁表达的情感 。  

故近年来 ， 细粒度情感分析应运而生 。 由于细粒度情感分析会包含诸如方面词 、  

意见词 、情感等多个元素 ，这些元素经常以元组的形式组织在

一起 ， 故这些元组  

抽取式任务也可称为多元组细粒度情感分析 。常见的多元组细粒度情感分析有方  

面意见词联合抽取 （ＡｓｐｅｃｔａｎｄＯｐ ｉｎｉｏｎＴｅｒｍＣｏ

－Ｅｘｔｒａｃｔｉｏｎ ， ＡＯＴＥ ）

［２ １ 、 方面情  

感对抽取 （Ａｓｐｅｃｔ

－ＳｅｎｔｉｍｅｎｔＰａｉｒＥｘｔｒａｃｔｉｏｎ ，ＡＳＰＥ ）

［３ １ 、方面级三元组抽取 （Ａｓｐｅｃｔ  

ＳｅｎｔｉｍｅｎｔＴｒｉｐ ｌｅｔＥｘｔｒａｃｔｉｏｎ ， ＡＳＴＥ ）

［４６ ］ 、 意见词挖掘 （Ｏｐ ｉｎｉｏｎＭｉｎｉｎｇ

， ＯＭ ） 网  

以及结构化情感分析 （ＳｔｒｕｃｔｅｄＳｅｎｔｉｍｅｎｔＡｎａｌｙｓｉｓ ， ＳＳＡ ）

［３６ ］等 。其中前三个任务  

是方面级情感分析 （Ａｓｐｅｃｔ

－ＢａｓｅｄＳｅｎｔｉｍｅｎｔＡｎａｌｙｓｉｓ ，ＡＢＳＡ ）

［ １ ］的子任务 ，而ＡＳＴＥ  

任务包含了ＡＯＴＥ以及ＡＳＰＥ任务 ， ＳＳＡ任务也包含了０Ｍ任务 。 故本文选取  

相对来说更为复杂的方面级三元组抽取 （ＡＳＴＥ ）以及结构化情感分析 （ＳＳＡ）进  

行研究 。多元组细粒度情感分析也与自然语言处理多项任务有关 ，例如命名实体  

识别 （ＮａｍｅｄＥｎｔｉｔｙＲｅｃｏｇｎｉｔｉｏｎ ， ＮＥＲ）

［４ ］ 、 事件抽取 （ＥｖｅｎｔＥｘｔｒａｃｔｉｏｎ ， ＥＥ ）  

， 细粒度情感分析的研究也带动了这些任务的发展 。 此外 ， 细粒度情感分析在  

许多场景下应用广泛 ，例如在电商场景下分析评论文本 ，判别用户对产品的不同  

属性的喜好程度 ，从而更加清晰地了解产品各个方面的优劣 ；在社交媒体场景下  

分析事件相关的评论文本 ，可以清楚地知道人们对于事件中涉及的各个人物的观  

点 。故本文开展的多元组细粒度情感分析无论是在研究还是在应用上都具有重要  

的意义与价值 。接下来对本文研究的两个任务方面级情感分析以及结构化情感分  

析进行任务描述 。  

方面级情感分析旨在研究在文本方面词 （ＡｓｐｅｃｔＴｅｒｍ ） 级别上的情感分析 ，  

其主要包含方面词抽取 （ＡｓｐｅｃｔＴｅｒｍＥｘｔｒａｃｔｉｏｎ ， ＡＴＥ ）

＿８ ］ 、意见词抽取 （Ｏｐ ｉｎｉｏｎ  

ＴｅｒｍＥｘｔｒａｃｔｉｏｎ ， ＯＴＥ ）以及方面词情感判定 （ＡｓｐｅｃｔＳｅｎｔｉｍｅｎｔＣｌａｓｓｉｆ ｉｃａｔｉｏｎ ，  

ＡＳＣ ）

－ １４ ｌ三个基础的子任务 。假设句子为

“ 这里环境不错 ， 但服务

， ＡＴＥ  

旨在抽取句子中的方面词 ， 即

； ＯＴＥ旨在抽取句子中的意见  

； ＡＳＣ旨在根据方面词判定其对应的情感极性 ， 即对  

” 的情感为积极的 ， 对

” 的情感是消极的 。 最初三者是独立的子任  

北京邮电大学电子信息硕士学位论文  

务 ，但最近研究发现 ，分别完成三个子任务再组合会导致效率低下以及错误迭代  

传播问题 ，且三个子任务具有

一定的关联性 ，故将这三个任务结合起来形成

一个  

一的任务 ， 即方面级三元组抽取 （ＡＳＴＥ）

［４６】任务 ， 该任务是为了抽取所有的  

（方面词 、 意见词 、情感）三元组。  

结构化情感分析

［３６］是近年来新兴的细粒度情感分析任务 ， 其主要包括情感  

持有者抽取 （Ｓｅｎｔ ｉｍｅｎｔＨｏｌｄｅｒＥｘｔｒａｃｔｉｏｎ ， ＳＨＥ ） 、情感目标抽取 （ＳｅｎｔｉｍｅｎｔＴａｒｇｅｔ  

Ｅｘｔｒａｃｔｉｏｎ， ＳＴＥ） 、情感表达抽取 （Ｓｅｎｔ ｉｍｅｎｔＥｘｐｒｅｓｓｉｏｎＥｘｔｒａｃｔ ｉｏｎ ， ＳＥＥ） 以及情  

感极性判定 （Ｓｅｎｔ ｉｍｅｎｔＰｏｌａｒ ｉｔｙＣｌａｓｓｉｆ ｉｃａｔｉｏｎ， ＳＰＣ）这些子任务 。假设句子为

“ 一  

些人给这家餐厅五星好评

， ＳＨＥ旨在抽取句子中的情感持有者 ， 也就是判断是  

谁在表达情感 ， 即

； ＳＴＥ旨在抽取句子中表达情感的对象 ， 也就是判  

断对谁在表达情感 ， 即

“ 这家餐厅

； ＳＥＥ旨在抽取句子中表达的观点 ， 即

“ 五  

； ＳＰＣ旨在判别情感的极性 ， 即

。结构化情感分析将这些任务  

一的四元组抽取任务 ， 即抽取所有的 （持有者 ， 目标 ，表达 ，极  

性）四元组 。相比于方面级情感分析 ，该任务包含的信息更为丰富 ，也更具有挑  

战性 。  

本文主要聚焦于细粒度情感分析的两个重要任务 ，分别是方面级三元组抽取  

和结构化情感分析 ，并分别研究了机器阅读理解框架以及端到端框架在这两个任  

务的应用 。  

１ ．２国内外研究现状  

本节首先介绍情感分析的发展历程 ，然后介绍所研究的两个任务的研究现状  

并给出分析 。  

１ ．２．１情感分析发展历程  

情感分析任务最早起源于对句子主观性的分析 Ｗｉｅｂｅ ［１８ｌ将主观句子与  

说话者的私人状态相关联 ， 但这些状态并不对外开放 。情感分析研宄自 ２０００年  

一个活跃的领域 ，这主要是因为出现了大量评论资源文本

［１９ ＿２ １ 】

，其中  

一个开创性工作是面向情感的评论分类

， 该工作通过枚举

一些句法规则

〖２３ １完  

成短语级别的情感挖掘任务 ，并引入词袋 （ｂａｇ

－ｏｆ －ｗｏｒｄｓ）的概念进行情感标注 。  

一般来说 ，情感分析研究主要关注三种粒度

：文档级别 、句子级别和方面级  

别 。在文档级别的情感分析中 ，假设文档对实体 （例如产品 ）传达了唯

一的观点 ，  

其目标是推断对

一个文档的整体观点Ｐ６ ＂％ 。 Ｐａｎｇ等人 进行了最初的文档级别  

一章绪论  

情感分析研究 ，他们使用各种特征并训练简单的分类器 ，例如朴素贝叶斯分类器  

和支持向量机 ，从而对评论文档进行正／负极性分类 。除了分类和回归任务 ，还有  

其他诸如生成观点摘要

－３ １］等任务 。句子级别的情感分析主要分析单个句子  

这些句子可能是文档或对话的

一部分 ， 还有可能是在社交平台上存在的文本

】 。  

虽然文档级别和句子级别的情感分析都提供了总体情感倾向 ，但在大多数情况下 ，  

它们并不指明情感的目标 。它们有

一种隐含假设 ， 即文本跨度 （文档或句子 ）传  

达了对实体的单

一情感 ， 然而这种假设并不总是成立 。为克服这

一挑战 ，方面级  

情感分析 对每个实体 （连同其方面 ）进行情感分析 ， 由此同

一文档  

中不同的实体可能有不同的情感 。结构化情感分析 是近年来新兴的任务 ，其粒  

度相比方面级情感分析来说更加精细 ， 也更具有挑战性 。  

１ ．２ ．２方面级三元组抽取  

方面级情感分析通常包含方面词抽取 ＣＡｓｐｅｃｔＴｅｒｍＥｘｔｒａｃｔｉｏｎ

，ＡＴＥ ）意  

见词抽取 （Ｏｐ ｉｎｉｏｎＴｅｒｍＥｘｔｒａｃｔｉｏｎ ，ＯＴＥ ）

－ １ １ ］以及方面词情感判定 （Ａｓｐｅｃｔ  

ＳｅｎｔｉｍｅｎｔＣｌａｓｓｉｆ ｉｃａｔｉｏｎ ，ＡＳＣ ）

－ １４ 】三个子任务 ，后来研宄发现各个子任务之间具  

一定的关联性 ，并且具有相互促进的作用 。故研究人员将不同任务组合起来形  

成新的任务 ， 例如方面

－意见词联合抽取 （ＡｓｐｅｃｔａｎｄＯｐ ｉｎｉｏｎＴｅｒｍＣｏ

－Ｅｘｔｒａｃｔｉｏｎ ，  

ＡＯＴＥ ）

－４ １ １

－情感对抽取 （Ａｓｐｅｃｔ

－ＳｅｎｔｉｍｅｎｔＰａｉｒＥｘｔｒａｃｔｉｏｎ

，ＡＳＰＥ ）

４５ ］ 。但是这两个任务仍不够全面 ： ＡＯＴＥ任务缺少对方面词的情感极性判定 ， 因  

此尽管获得对于每个方面词的观点 ，但缺少情感极性的判别也就缺乏对该方面词  

持何态度 ； ＡＳＰＥ任务缺少对意见词的判定 ， 尽管知道对该方面词的情感极性 ，  

但并不知道为何持有这种情感 。 因此Ｐｅｎｇ等人

［４６ ］将三个子任务全部合并形成方  

面级三元组抽取任务 ，通过对句子进行分析不仅可以得到方面词和其对应的情感  

极性 ，还可以感知为何是这种情感极性 ， 即意见词 ，最后形成 （方面词 、意见词 、  

情感极性 ）这样的三元组 ， 全面概括了句子中包含的情感信息 。  

针对方面级三元组抽取任务 ， Ｐｅｎｇ等人 首先提出

一种两阶段式方法 ， 第  

一阶段鉴别带有情感的方面词 ，第二阶段抽取意见词 ，最后将它们配对形成三元  

组 。然而阶段式方法会造成错误累计传播问题 ， 且开销较大 。 由此Ｘｕ等人

［４７ ］提  

一种采用位置感知序列标注方案的端到端模型 ，充分利用三元组中各元素的关  

联性 ；Ｗｕ等人 放弃了

一维的序列标注 ，而是设计了

一种二维的表格填充机制 ；  

［Ｍ等人提出

一种多任务学习框架联合提取方面词与意见词 ， 并利用双仿射  

评分器来解析它们之间的情感依赖关系 ； Ｃｈｅｎ等人 利用图神经网络提取词对  

之间的语义和句法信息 ，并用 ＬＳＴＭ对原始句子建模 ； Ｙａｎ等人 提出将该任务  

一的生成式问题 ， 并设计了

一种基于预训练生成式模型 ＢＡＲＴ 的端到  

端框架 。  

 北京邮电大学电子信息硕士学位论文  

除了端到端框架 ，另

一种有效的框架是机器阅读理解框架 。Ｍａｏ等人

［５巧１过  

两阶段式询问分别得到方面词与其对应的意见词和情感 ； Ｃｈｅｎ等人

［５３］则设计了  

一种双向的机器阅读理解框架 ，并设计了三种不同的查询来捕获各元素之间的关  

联性 。然而这些方法都忽略了

一句话包含多个方面词时其他方面词带来的千扰问  

题 ，让模型更准确地识别各个方面词以及对应附属物可有效提升三元组抽取性能 。  

１ ．２ ．３结构化情感分析  

结构化情感分析可以划分为多个子任务 ，包括提取实体、决定实体相关的类  

型以及赋予情感极性 。

一些以往在Ｏｐ ｉｎｉｏｎＭｉｎｉｎｇ（ＯＭ）任务的研宄专注于提  

取持有者 、 目标 、表达以及鉴别它们之间的关系 ， 主要是在ＭＰＱＡ数据集

［５４］上  

进行 。 ＡｒｚｏｏＫａｔ ｉｙａｒ和 ＣｌａｉｒｅＣａｒｄｉｅ ［５５ ］应用 ＢｉＬＳＴＭ －ＣＲＦ模型去预测词级别的  

意见角色标签 ， 并鉴别它们的关系 ； Ｑｕａｎ等人

［５６］应用

一种端到端的基于 ＢＥＲＴ  

的模型 ； Ｚｈａｎｇ等人

［５７］使用预定的动作集设计

一种基于转变的方法 ； Ｘｋ等人

［５８］  

一的基于跨度的模型去解决重叠问题 。然而所有这些方法均忽视了  

情感极性的判别 。 在如 １ ．２ ．２部分所述的方面级情感分析的方法也忽视了持有者  

的提取 ， 并且其主要是抽取扁平的实体 ， 然而 ＳＳＡ任务存在重叠实体以及非连  

续实体的难题 。  

Ｂａｒ ｎｅｓ等人

［３６］首次提出 ＳＳＡ任务然后将其转化为双词汇依赖解析问题 ， 并  

提出 ｈｅａｄ －ｆ ｉｒｓｔ以及ｈｅａｄ －ｆｉｎａｌ两种方法 ，然而这种转化是有损的 ， 因为其并不能  

正确鉴别两个部分重叠的实体 。为解决这

一问题 ， Ｓｈｉ等人

［５９］提出另外

一个依赖  

解析方法并利用图神经网络进行建模 ，然而不幸的是 ，其不能鉴别非连续的实体 。  

Ｓ＿ｅｌ等人

［６Ｇ］注意到嵌套实体问题并提议直接从文本解析情感图 。 因此目前尚  

没有可以同时解决重叠实体以及非连续实体两大难题的双词汇依赖解析方法 ，而  

同时解决这两大问题对提升整个结构化情感分析的性能尤为关键 。  

１ ．３本文研究内容与主要工作  

接下来针对方面级三元组抽取以及结构化情感分析两个多元组细粒度情感  

分析任务来阐述本文研宄内容 。  

１ ．３ ．１方面级三元组抽取  

方面级三元组抽取任务的目标是抽取其中所有的 （方面词 、 意见词 、 情感 ）  

三元组 。如图１ ． ２所示 ，给定句子为

“ Ｎｉｃｅａｍｂｉｅｎｃｅ ，ｂｕｔｈｉｇｈｌｙｏｖｅｒｒａｔｅｄ

ｐ ｌａｃｅ

其三元组有 （ａｍｂｉｅｎｃｅ ，Ｎｉｃｅ，ｐｏｓｉｔ ｉｖｅ） 以及 （ｐ ｌａｃｅ ，ｏｖｅｒｒａｔｅｄ ，ｎｅｇａｔ ｉｖｅ） 。 在该任  

务上目前主流的深度学习方法主要分为阶段式、端到端式以及机器阅读理解式方  

一章绪论  

法 ， 相对于阶段式方法 ， 后两者是更为先进的方法 ，对于该任务本文采用机器阅  

读理解框架 。  

ｐｏｓ ｉｔ ｉｖｅ ：积级的ｎｅｇａｔｉｖｅ ：消极的  

， ， —

ｉ   Ｎｉｃｅ

ｉ ｉａｍｂｉｅｎｃｅ，ｂｕｔｈｉｇｈｌｙｏｖｅｒｒａｔｅｄ ： ｐ ｌａｃｅ．  

三元组 ：｛ ｉｍｉｂ ｉｅｎｃｅ ，Ｎｉｃｅ ，ＰＯＳ｝ ｛ｐ

ｌａｃｅ ，ｏｖｅｒｒａｔｅｄ ，

ＩＶＥＣ， ｝

｜    ＞  

图 １ ． ２ ： 方面级三元组抽取样例  

机器阅读理解方法主要是要给预训练语言模型 （例如ＢＥＲＴ ）设置

一个查询  

（ｑｕｅｒｙ ） 以及

一个上下文 （ｃｏｎｔｅｘｔ） 。 查询通常是人为设定的所要完成的任务目  

标 ， 上下文通常是原始的语句 。将查询以及上下文同时作为输入 ，模型通过查询  

所带来的提示可以更好地完成任务的目标 。 如图 １ ． ２所示 ， 原始语句为

“ Ｎｉｃｅ  

ａｍｂｉｅｎｃｅｌ ，ｂｕｔｈｉｇｈｌｙｏｖｅｒｒａｔｅｄ

ｐ ｌａｃｅ ．

， 假设需要提取该句中的方面词 ， 可以设置  

“ Ｆｉｎｄｔｈｅａｓｐｅｃｔｓｉｎｔｈｅｔｅｘｔ ．

， 并将原始语句作为上下文 ， 然后联合作为  

输入 ， 模型会更准确地识别所有的方面词为

“ ａｍｂｉｅｎｃｅ

ｐ ｌａｃｅ

针对方面级三元组抽取 ，前沿的机器阅读理解方法通常是使用多轮的问答来  

得到最终的三元组 。在这多轮问答中 ， 查询是不同的 ，但上下文是相同的 。具体  

来说 ， 机器阅读理解方法通常包含两个阶段 。 第

一阶段为方面词推理 （Ａｓｐｅｃｔ  

Ｉｎｆｅｒｅｎｃｅ ， ＡＩ ）阶段 ， 在该阶段中 ， 查询设置为找到句子中所有的方面词 ， 例如  

“ Ｗｈａｔａｓｐｅｃｔｓ？

，而上下文设置为原始语句 ，故模型与第

一阶段可以找到句子中  

所有的方面词 。第二阶段为方面词附属物推理 （ＡｓｐｅｃｔＡｃｃｅｓｓｏｒｙ Ｉｎｆｅｒｅｎｃｅ ，ＡＡＩ ）  

阶段 ， 方面词附属物即方面词对应的意见词和情感 。在该阶段中 ， 查询设置为找  

到某个方面词对应的意见词和情感 ， 例如

“ Ｗｈａｔｏｐ ｉｎｉｏｎｓａｎｄｓｅｎｔｉｍｅｎｔ ｇ ｉｖｅｎｔｈｅ  

ａｓｐｅｃｔａｍｂ ｉｅｎｃｅ？

’ ’ 或

Ｗｈａｔｏｐ ｉｎｉｏｎｓａｎｄｓｅｎｔｉｍｅｎｔ ｇ ｉｖｅｎｔｈｅａｓｐｅｃｔ ｐ ｌａｃｅ？

， 上下  

文同样设置为原始语句 。因此对于

一个原始语句 ，在第

一个查询找到  

所有方面词 ，在第二阶段设置多个查询分别对每个方面词找到其对应的意见词和  

情感 ， 由此得到最终的多个三元组 。  

表 卜 １方面级三元组抽取基准数据集中多方面词句子以及其包含的三元组数量占比  

多方面词Ｒｅｓｔ１４Ｌａｐ１４Ｒｅｓｔ１５Ｒｅｓｔ１６  

句子４２ ．７％２９ ，４％２９ ．２％２８ ．７％  

三元组６２ ．９％４８ ． １％４７ ．２％４６ ．６％  

北京邮电人学电子信息硕士学位论文  

尽管前沿的机器阅读理解方法的表现令人印象深刻 ，然而当

一句话同时存在  

多个方面词时 ，这些方法会面临严重的其他方面词带来的干扰问题 。首先从直觉  

一句话包含的方面词数量越多 ， 句子中包含的情感信息就越丰富 ，方面  

词对应的意见词和情感也通常就越难判别 。如图 １ ．２所示 ， 当句子中同时存在两  

“ ａｍｂｉｅｎｃｅ

ｐ ｌａｃｅ

” 时 ， 模型可能会错误地将

“ ｏｖｅｒｒａｔｅｄ

” 视为  

“ ａｍｂｉｅｎｃｅ

” 对应的意见词 。而且 ，训练后的模型因为其内部的 ｔｒａｎｓｆｏｒｍｅｒ  

架构自带注意力机制 ，模型会潜在地捕捉方面词与意见词的对应关系 。如果模型  

注意到其他的方面词 ，那么也就会关注到这些方面词对应的意见词 ，进而对目前  

方面词的意见词推理造成干扰 。 如图 １ ． ２所示 ， 对于方面词

“ ａｍｂｉｅｎｃｅ

， 如果  

“ ｐ ｌａｃｅ

” 仍存在 ， 模型可能会关注到错误的意见词

“ ｏｖｅｒｒａｔ ｅｄ

上 。需要注意的是表 １

－ １显示多方面词句子在整体中占比很多 ，这些句子包含的  

三元组数量更是占比接近

一半 ，因此如何从多方面词句子中排除其他方面词的干  

扰以更准确地鉴别情感信息是非常有意义的 。基于以上的观察 ， 本文提出了掩码  

一核心思想 。掩码方面词不仅可以直接去除对无关方面词的注意力 ，而  

且基于此可以进行简单有效的数据增强 ，这恰好又符合

“ 一句话包含方面词数量  

越多 ，所以情感信息量越大 ，所以应该将这句话增强为多个训练样本

”这个直觉 。  

为此 ， 本文提出了

一种基于掩码上下文的机器阅读理解 （ＣＯｎｔｅｘｔ

－Ｍａｓｋｅｄ  

ｍａｃｈｉｎｅｒｅａｄｉｎｇｃｏｍｐｒｅｈｅｎｓｉｏｎ ，ＣＯＭ

－ＭＲＣ ）框架 ， 该方法打破了传统机器阅读  

“ 不同的查询 、相同的上下文

一理念 ， 转而采用

“ 相同的查询 ， 不同  

” 并实现了先进的效果 。该框架包括掩码式数据增强 、交互式判别模型  

以及阶段式推理方法三个部分 。在掩码式数据增强中 ，本文针对原始输入句子中  

每个方面词都会考虑是否掩码 ，故假设

一句话包含 ｔ个方面词 ， 数据增强后变为  

＾个句子 。 将这些增强后的句子作为上下文 ， 并设置固定的

一句话作为查询 ，  

目的是找到该句中首个方面词以及对应的意见词和情感 ，因此模型会减少无关方  

面词带来的干扰 ，更好地分辨来自不同方面词的情感信息 。在交互式判别模型中 ，  

为了更加充分地利用三元组个元素的关联关系 ，本文设计了四个模块 ，并且允许  

各模块之间交流信息 。这四个模块分别是方面词提取模块 、 意见词提取模块 、情  

感判别模型以及方面词探测模块 ，其中最后

一个模块是为了探测掩码上下文中是  

否仍存在方面词 。 在阶段式推理方法中 ， 本文设计了两个阶段 ， 分别为方面词推  

理阶段以及方面词附属物推理阶段 。在方面词推理阶段中 ，通过逐步掩码方面词  

将所有的方面词提取出来 ；在方面词附属物推理阶段中 ，所有无关的方面词都会  

被掩码以排除它们带来的干扰 ，由此可以针对每个提取出来的方面词识别对应的  

意见词和情感 ；最后综合两个阶段形成三元组 。在基准数据集上的大量实验结果  

证明了本文提出的ＣＯＭ

－ＭＲＣ框架的有效性 。  

１０  

一章绪论  

本文在方面级情感分析任务中的研究内容总结如下 ：  

１ ）首次提出利用掩码方面词的思想进行简单有效的数据增强 ，提供了

一种与  

传统机器阅读理解思路不同的 、 用于解决多方面词干扰问题的研究新视角 。  

２ ）提出完整的基于掩码上下文的机器阅读理解框架 ，该框架包括掩码式数据  

增强 、 交互式判别模型以及阶段式推理方法三个部分 。  

３）在基准数据集上做了大量实验 ， 结果表明了ＣＯＭ

－ＭＲＣ方法的有效性 。  

１ ．３ ．２结构化情感分析  

结构化情感分析的目的是抽取句子中所有的 （持有者 ， 目标 ， 表达 ， 情感 ）  

意见元组 。 如图 １ ．３所示 ， 假设给定句子为

“ Ｍｙ ｅｍｐ ｌｏｙｅｒ ｆｏｕｎｄｙｏｕｒｗｏｒｄｓ ｔｏｂｅ  

ｒｅｌａｌｌｙｒｕｄｅａｎｄｌｏｎｇｗｉｎｄｅｄ ．

， 其意见元组为（Ｍｙｅｍｐ

ｌｏｙｅｒ ， ｙｏｕｒｗｏｒｄｓ ， ｒｅａｌｌｙ  

ｒｕｄｅ ，ｎｅｇａｔｉｖｅ）以及（Ｍｙｅｍｐ ｌｏｙｅｒ ，ｙｏｕｒｗｏｒｄｓ ，ｒｅａｌｌｙ ｌｏｎｇｗｉｎｄｅｄ ，ｎｅｇａｔｉｖｅ） 〇  

一一 Ｎｅｇａｔｉｖｅ（消极的 ）  

Ｈｏ ｌｄｅｒ（持有者）

｜ Ｔａｒｇｅｔ（ Ｉ ！标）Ｅｘｐｒｅｓｓ丨ｏｎ（表达＞  

Ｍｙｅｍｐｌｏｙｅｒｆｏｕｎｄｙｏｕｒｗｏｒｄｓｔｏｂｅｒｅａｌｌｙｒｕｄｅａｎｄｌｏｎｇｗｉｎｄｅｄ  

Ｈｏ ｌｄｅｒ（持丫Ｍ ）Ｔａｒｇｅｔ（ ｌ ｉｆ ｅ）Ｅｘｐｒ ｅｓｓｉｏｎ７表达 ）  

Ｎｅｇａｔｉｖｅ（消极的 ）  

图 １ ． ３ ： 结构化情感分析样例  

－２ 结构化情感分析数据集中包含重叠以及非连续情况的元组數量与占比  

ＯｖｅｒｌａｐＤｉｓｃｏｎｔｉｎｕｉｔｙ   Ｄａｔａｓｅｔ  

 ＃％ ＃ ％  

ＮｏＲｅＣｐ ｉｎｅ２ １７８１９ ．６１０８０９ ．７  

ＭｕＩｔｉＢｎｕ〇０１６４７ ． １  

ＭｕｌｔｉＢｃＡ３０ ． １１ １３４ ， １  

ＭＰＱＡ４０３１ ．４００  

ＤＳｕｎ ｉｓ

１８１ ．７ 

１０２ ９ ．９  

目前主流的结构化情感分析方法通常将其视为双词汇依存解析问题 。双词汇  

依赖解析是将每个单词视为节点 ，然后在其中

一些点与点之间连接有向边 。通常  

１ １  

北京邮电大学电子信息硕士学位论文  

连接的边是自定义多种类型的其中

一种 ，表明了这两个节点之间的关系 。通过对  

节点之间是否连边以及连边后边的类型进行判别 ，最后形成图 ，也称为双词汇依  

赖解析图 。然而 ，现有的双词汇依存解析转化方法有缺陷的 ， 因为其不能同时解  

决实体重叠和不连续的问题 。例如 ，在

“ Ｍｙｅｍｐ ｌｏｙｅｒｆｏｕｎｄ

ｙｏｕｒｗｏｒｄｓｔｏｂｅｒｅｌａｌｌｙ  

ｒｕｄｅａｎｄ ｌｏｎｇｗｉｎｄｅｄ ，

这句话中存在两个表达 ： ｛ ｒｅａｌｌｙ ， ｒｕｄｅ ｝和｛ ｒｅａｌｌｙ ， ｌｏｎｇ ，  

ｗｉｎｄｅｄ 丨 。 这两个表达中有重叠的单词

ｒｅａｌｌｙ

， 并且后者是不连续的 ， 但目前  

一种双词汇依赖解析方法可以同时解决这两个问题 。 然而如表 １

－ ２所示 ，  

基准数据集中包含重叠以及非连续问题的意见元组占比不可忽略 ，因此如何同时  

解决实体重叠和非连续问题是非常有意义的 ， 也充满挑战性 。  

为了解决结构化情感分析中的实体重叠和非连续问题 ，本文提出了

一种双词  

汇依赖解析方法 。该方法对应的双词汇依赖解析图包含两种类型的有向边 ：关系  

预测 （Ｒｅｌａｔ ｉｏｎＰｒｅｄｉｃｔｉｏｎ ， ＲＰ）和单词提取 （ＴｏｋｅｎＥｘｔｒａｃｔ ｉｏｎ ， ＴＥ） 。 ＲＰ主要处  

理实体边界的判定和实体间关系的预测 ， 解决了重叠问题 ； ＴＥ主要识别给定实  

体边界内的所有 ｔｏｋｅｎ ， 解决了非连续问题 。然后 ， 本文将这种双词汇依赖解析  

一的表格填充机制 ， 即ＵＳＳＡ（ＵｎｉｆｉｅｄＴａｂｌｅＦｉｌｌｉｎｇＳｃｈｅｍｅｆｏｒ  

ＳｔｒｕｃｔｕｒｅｄＳｅｎｔｉｍｅｎｔ Ａｎａｌｙｓｉｓ） 。在ＵＳＳＡ机制中 ，每条边的起始位置作为ｘ坐标 ，

终止位置作为ｙ坐标 ，边的类型作为表格单元格的标签类型 ，所有的标签共有 １３  

种 。故整个表格被划分为左下半三角和右上半三角两部分 ，分别对应ＲＰ和ＴＥ。  

最后 ，基于ＵＳＳＡ机制 ，本文进

一步提出了适用于该机制的模型 ，并利用双轴注  

意力模块来有效捕捉表格中行与列的关联信息 。在基准数据集上进行的大量实验  

结果证明了本文提出的ＵＳＳＡ的有效性 。  

本文在结构化情感分析任务中的研宄内容总结如下 ：  

１）首次提出同时可以解决实体重叠以及非连续问题的双词汇依赖解析方法 ，  

然后将该方法其转化为统

一的表格填充机制ＵＳＳＡ 。  

２）提出有效的模型适配ＵＳＳＡ机制 ，其中设计了双轴注意力模块有效地捕捉  

表格中不同标签的联系 。  

３）在基准数据集上做了大量实验 ， 结果表明了ＵＳＳＡ机制的有效性 。  

１ ．４本文的组织结构  

本论文总共包含五个章节 ，章节结构如图 １ ．４所示 ，每

一章的重点内容如下 ：  

一章 ， 绪论 。 首先介绍了多元组细粒度情感分析任务的研究背景与意义 ，  

然后介绍了国内外研究现状 ，并主要对方面级三元组抽取以及结构化情感分析两  

１２  

一章 绪论  

个任务的研究现状进行详述 ，最后介绍了针对两个多元组细粒度情感分析任务本  

文的研究内容与主要工作 。  

第二章 ，相关技术 。简要介绍了本文涉及的深度学习模型以及深度学习技术 。  

首先介绍循环神经网络 、 卷积神经网络 、 注意力机制以及Ｔｒ ａｎｓｆ ｏｒｍｅｒ等深度学  

习模型 ， 然后介绍了Ｄｒｏｐｏｕｔ 、 Ｎｏｒｍａｌｉｚａｔｉｏｎ 、 损失函数以及优化器等深度学习  

技术 。  

第三章 ，基于掩码上下文机器阅读理解框架的方面级三元组抽取方法 。首先  

介绍了本文设计基于掩码上下文机器阅读理解框架ＣＯＭ －ＭＲＣ ，并介绍了所提出  

框架中掩码式数据增强 、交互式判别模型以及阶段式推理方法的技术细节 。最终  

通过实验验证以及对比分析 ， 验证了所提出的框架的有效性 。  

第四章 ，基于统

一表格填充机制端到端框架的结构化情感分析方法 。首先介  

绍了本文提出的统

一的表格填充机制ＵＳＳＡ ，然后进

一步介绍了配备ＵＳＳＡ机制  

的模型架构 。 最终通过实验验证以及对比分析 ， 验证了所提出方法的有效性 。  

第五章 ， 多元组细粒度情感分析系统 。首先介绍了系统的需求分析以及整体  

设计 ， 然后介绍了系统的前端以及后端设计 ， 最后介绍了系统的工作流程 。  

第六章 ， 总结与展望 。对全文的研究工作进行了总结 ，并对未来工作进行了  

展望 ， 提出了可以进

一步开展的研宄方向 。  

ｒ ＞  

一章绪论  

＼＞  

第二簞相关技术  

／  

！  

＇ ｉ   

？＼／Ｎ／Ｎ／＼；  

： 挑战 １

： 多方面词存挑战２ ： 各模块之间挑战３

： 重叠实体问挑战４ ： 非连续实体

丨在干扰缺乏交互题问题 丨  

：Ｉ／Ｌ

： Ｊ＼ ／Ｖ＾）

Ｉ １

、 （ １  １ ；   ＇

第三章基于掩码上下文 １

＾第四章基于统 一表格１ ！  

ｉ机器阅读理解框架填充机制端到端框架

的方面级三元组抽取方法的结构化情感分析方法 ｉ  

Ｖ／Ｖ  ／ ；  

第五章多元组绽粒度   情感分析系统  

＼ Ｖ  

第六章总结与展望  

图 １ ． ４ ： 本论文的组织结构  

１３  

北京邮电大学电子信息硕士学位论文  

１ ．５本章小结  

本章首先给出了多元组细粒度情感分析的研宄背景和意义 ，然后介绍了研宄  

背景 ，然后介绍了本文主要的两大研究内容 ：方面级三元组抽取以及结构化情感  

分析 。 最后介绍了本文的组织架构 。  

１４  

第二章相关技术  

第二章相关技术  

本文实验主要使用的是深度学习方法 ， 故本节首先介绍各种深度学习模型 ，  

然后介绍相关的深度学习技术 。  

２ ． １深度学习模型  

２ ． １ ． １循环神经网络  

循环神经网络 （ＲｅｃｕｒｒｅｎｔＮｅｕｒａｌＮｅｔｗｏｒｋ ， ＲＮＮ）

［６１ ］是

一种深度学习神经网络 ，  

可以处理序列数据 ， 例如文本和语音等 。 相比于传统的前馈神经网络 ， ＲＮＮ具  

有记忆性 ， 可以利用先前的信息来帮助处理后续的数据 。  

ＲＮＮ 的核心是循环单元 （ＲｅｃｕｒｒｅｎｔＵｎｉｔ） ， 它由

一个隐藏状态向量和

一个输  

入向量组成 ，通过 一个非线性函数将它们组合起来 。隐藏状态向量是网络的记忆 ，  

可以存储先前的信息并传递给下

一个时间步 ， 其前向传播公式如下 ：  

ｈ ｔ ＝

ｆ（Ｗｈｈ ｔ＾＋Ｗｘｘｔ＋ｂ）（２ － １）  

其中 ， ￣是时间步 ｔ的隐藏状态向量 ， ；＾是时间步 ｔ的输入向量 ，％和％ 分  

别是隐藏状态和输入的权重矩阵 ， ＆是偏置向量 ， ／是非线性函数 ， 例如 ｔａｎｈ  

或者ＲｅＬＵ等 。  

ｔａｎｈ激活函数表达式如下 ：  

恤财 ＝ －

ｅｘ＋ｌ＾（２ － ２）  

ＲｅＬＵ激活函数的计算公式如下 ：  

（ａ：） ＝ｍａｘ（ｘ ，０）（２ — ３）  

ＲＮＮ 的反向传播算法使用了误差反向传播算法 （Ｂａｃｋｐｒｏｐａｇａｔ ｉｏｎＴｈｒｏｕｇｈ  

Ｔｉｍｅ ，ＢＰＴＴ） ， 在每个时间步计算梯度并更新权重参数 。但是 ， 由于 ＲＮＮ计算  

时存在大量的连乘操作 ，随着时间步数的增加 ，梯度会出现梯度消失或梯度爆炸  

的问题 。  

长短时记忆网络 （ＬｏｎｇＳｈｏｒｔ －ＴｅｒｍＭｅｍｏｒｙ ， ＬＳＴＭ）

［６１ ］是

一种特殊的循环  

神经网络 ，能够有效地处理序列数据 ，并且在解决了ＲＮＮ中梯度消失和梯度爆  

炸的问题后 ，成为了当时处理序列数据的主流模型 。ＬＳＴＭ 的关键在于引入了门  

控机制 （Ｇａｔｅ） ， 可以控制信息的流动和存储 ， 从而避免了梯度消失和梯度爆炸  

的问题 ， 同时也能更好地存储和利用长期记忆 。  

１５  

北京邮电大学电子信息硕士学位论文  

ＬＳＴＭ 的结构包含了

一个记忆单元 （ＭｅｍｏｒｙＣｅｌｌ） ， 三个门控单元 （Ｉｎｐｕｔ  

Ｇａｔｅ、 ＦｏｒｇｅｔＧａｔｅ、 ＯｕｔｐｕｔＧａｔｅ） ， 以及

一个非线性激活函数 。其中 ， 记忆单元是  

网络的主要组成部分 ，用于存储当前时刻的状态信息 ，三个门控单元则用于控制  

信息的流动和存储 。前向传播公式如下 ：  

ｆｔ＝ｆ（Ｗｆ［ｈ ｔ ．ｖｘｔ］＋ｂ ｆ）  

ｉｔ＝ｃｒＣＷ ｉ ｌｈ＾ ．Ｘｔ］＋ｂ ｉ）  

〇 ｔ ＝＾（Ｗ〇 ［ｈ ｔ

＿１ ，ｘｔ＼＋ｂ０）（２ －４）   Ｃ ｔ ＝ｔａｎｈＣＷｃ ｔＶｉ ，＾］＋ｂｃ）  

Ｃ ｔ ＝ ｆｔ ＊ Ｑ －！＋ｉ ｔ ＊ Ｑ  

ｈ ｔ ＝ｏ ｔ ＊ｔａｎｈ（Ｃ ｔ）  

其中 ， Ａ是输入向量 ， ｈ ｔ是隐藏状态向量 ， Ｃ ｔ是记忆单元 ， ／ｔ是遗忘门 ， ＾是  

输入门 ， ｏ ｔ是输出门 ， ＆是候选记忆单元 ， 〇

？是 ｓｉｇｍｏｉｄ函数 ， ＊是逐元素相乘 。  

ｓｉｇｍｏｉｄ激活函数表达式如下 ：  

ｓｉｇｍｏｉｄ〇）＝〇

？ 〇〇 ＝

－ｘ（２ － ５）  

在前向传递过程中 ，输入向量 ＆和前

一时刻的隐藏状态 经过门控单元和非  

线性激活函数的处理 ，得到遗忘门Ａ 、输入门 ｉ ｔ 、输出门＆和候选记忆单元Ｇ ，  

然后根据遗忘门和输入门的值来更新记忆单元Ｃ ｔ ， 最后根据输出门的值来计算  

本时刻的隐藏状态￣ 。  

ＬＳＴＭ可以应用于很多领域 ，例如语音识别 自然语言处理

［６３ ］ 、 图像描述  

［６４ ］等任务 。ＬＳＴＭ的优点是可以很好地处理长期依赖性 ，而缺点是计算量较  

大 ， 训练时间较长 。 另外 ，在

一些场景下 ， ＬＳＴＭ也会出现过拟合的问题 ， 需要  

使用正则化等方法进行处理 。  

双向长短时记忆网络 （Ｂｉｄｉｒｅｃｔ ｉｏｎａｌＬｏｎｇＳｈｏｒｔ －ＴｅｒｍＭｅｍｏｒｙ ，ＢｉＬＳＴＭ）

［６５ ］  

一种能够同时利用过去和未来信息的循环祌经网络 ， 是对传统 ＬＳＴＭ模型的  

一种扩展 。 ＢｉＬＳＴＭ能够对

一个序列进行双向建模 ，利用前向和后向的信息来预  

测当前时刻的输出 ， 提高了序列建模的准确性和鲁棒性。  

ＢｉＬＳＴＭ模型包含两个独立的ＬＳＴＭ ， 一个前向 ＬＳＴＭ和

一个后向ＬＳＴＭ。  

前向 ＬＳＴＭ从序列的第

一个元素开始处理 ， 依次向后处理 ， 后向 ＬＳＴＭ则从序  

一个元素开始处理 ，依次向前处理 。最终 ，将两个ＬＳＴＭ的输出拼接起  

一个综合的表示 ， 用于进行后续的任务 ，例如情感分析 命名实体识  

［６７１ 、 语音识别＾等 。 在训练时 ， ＢｉＬＳＴＭ的前向ＬＳＴＭ和后向 ＬＳＴＭ分别进  

行反向传播 ，最终得到的梯度是两者梯度的加和 。由于双向ＬＳＴＭ能够利用过去  

和未来的信息 ，因此相对于传统ＬＳＴＭ模型 ， ＢｉＬＳＴＭ能够更好地处理长期依赖  

性和序列中的复杂模式 。  

１６  

第二章相关技术  

门控循环单元 （ＧａｔｅｄＲｅｃｕｒｒｅｎｔＵｎｉｔ， ＧＲＵ）

一种类似于ＬＳＴＭ的循环  

神经网络结构 ，它由两个门控单元和 一个更新门组成 ，可以有效地解决长期依赖  

问题 ，并且具有比ＬＳＴＭ更少的参数和计算量 ， 是对ＬＳＴＭ的

一种简化和改进 。  

ＧＲＵ只使用

一个记忆单元来存储过去的状态信息 ，并使用两个门控单元 ，即Ｒｅｓｅｔ  

Ｇａｔ ｅ和ＵｐｄａｔｅＧａｔｅ来控制记忆单元的更新和信息的流动 。其前向传播公式如下 ：  

ｒｔ ＝ａ｛Ｗｒｘｔ＋＋ｂｒ）  

ｚ ｔ ＝ａ（Ｗ２ｘ ｔ＋Ｖｚｈ ｔ＾＋ｂｚ）  

ｈ ｔ ＝ｔａｎｈ（Ｖ／ｘｔ＋Ｕ（ｒ ｔ〇／ｉｔ －ｉ）＋ｂ

＾）  

ｈ ｔ ＝ （ｌ － ｚｔ）〇／ｉ ｔ ＿ａ＋ｚｔＱｈ ｔ  

其中 ， Ａ是输入向量 ， ／ｉｔ是隐藏状态向量 ， ；ｒｔ是重置门 ，＆是更新门 ，＆是临时  

隐藏状态向量 ， Ｖ、 ｉ／和 ６分别是权重矩阵和偏置向量 ， 〇是 ｓｉｇｍｏｉｄ函数 ， 〇  

表示逐元素相乘。在前向传递过程中 ，输入向量 ；＾和前

一时刻的隐藏状态  

经过重置门和更新门的处理 ， 得到重置门 ｒ ｔ 、 更新门ｚｔ ， 然后利用重置门更新  

一时刻的隐藏状态 ，再利用更新门更新临时隐藏状态 ，最后根据更新门和前

时刻的隐藏状态计算本时刻的隐藏状态 。  

２ ． １ ．２卷积神经网络  

卷积神经网络 （ＣｏｎｖｏｌｕｔｉｏｎａｌＮｅｕｒａｌＮｅｔｗｏｒｋ ， ＣＮＮ ）

［７＜）］是

一种可以自动提  

取图像 、音频 、文本等数据的特征的神经网络结构 ，具有优秀的特征提取和分类  

能力 ，是计算机视觉和自然语言处理领域最常用的模型之

。卷积神经网络的基  

本组成部分包括卷积层 、池化层和全连接层 。卷积层是卷积神经网络的核心部分 ，  

它通过卷积操作提取输入数据的特征 ，池化层则用于降低卷积层的输出数据的空  

间尺寸 ，减少网络参数 ，全连接层则用于将卷积层和池化层的输出特征映射到分  

类结果 。  

卷积神经网络中的卷积操作可以看作是

一种特殊的加权求和操作 ，它能够自  

动学习图像中的特征 。在卷积操作中 ， 卷积核在输入数据的不同位置进行滑动 ，  

对每个位置上的数据进行卷积计算 ，得到对应的特征图 。卷积神经网络中的每个  

卷积层可以包含多个卷积核 ，每个卷积核都可以提取不同的特征 。卷积层的前向  

传递公式如下 ：  

ａ ｉ ｉＪ ＝ ＣＷ＊ｘ）ｔｊ＋ｂ  

ｈｊ ＝ ｆ｛ａ ｔ ＞ｊ）  

１７  

北京邮电大学电子信息硕士学位论文  

其中 ＊是卷积运算 ， Ｘ是输入数据 ， 撕是卷积核 ， ＆是偏置项 ， 是卷积层在  

位置 （ ｉｊ）的卷积加权求和结果 ， ｂ是卷积层在位置 （ ｉｊ）的激活值 ， ／

？是激活  

函数 。  

在卷积神经网络中 ，每个卷积层后面通常都会加上 一个池化层 ， 以便减少特  

征图的维度和数量 ， 同时也能够减少模型的计算复杂度 。池化层通常使用最大池  

化或平均池化的方式对卷积层的特征图进行下采样操作 。  

除了基本的卷积神经网络 ，还有

一些变种的卷积神经网络 ，例如ＲｅｓＮｅｔ ［１Ｇ２］ 、  

ＧｏｏｇＬｅＮｅｔ

［ １Ｍ］ 、ＶＧＧ

［ １Ｇ４ ］等 ，它们在不同的任务和场景下都取得了很好的效果 。  

２． １ ．３注意力机制  

注意力机制 （Ａｔｅｎｔｉｏｎ）ＰＩ是

一种可以根据输入的序列自适应地给不同部分  

分配不同的权重的机制 ， 常用于序列到序列 （Ｓｅｑｕｅｎｃｅ －ｔｏ －Ｓｅｑｕｅｎｃｅ ， Ｓｅｑ２Ｓｅｑ）  

模型和自然语言处理任务中 ， 能够有效提升模型的表现。  

注意力机制可以视为

一种加权求和操作 ，它会为输入序列中的每个元素分配  

一个权重 ，然后利用这些权重对输入序列中的不同部分进行加权求和 。常见的注  

意力机制包括点积注意力 （Ｄｏｔ －Ｐｒ ｏｄｕｃｔＡｔｅｎｔ ｉｏｎ） 、 加性注意力 （Ａｄｄｉｔ ｉｖｅ  

Ａｔｅｎｔ ｉｏｎ） 、乘性注意力 （Ｍｕｌｔｉｐ ｌｉｃａｔｉｖｅＡｔｅｎｔｉｏｎ）等 。不同于传统的加权求和方  

法 ，注意力机制能够自适应地计算权重 ，使得模型能够专注于输入序列中的重要  

部分 。在自然语言处理中 ，其通常被用于机器翻译

［７２］ 、文本摘要

［７３］ 、对话生成

［７４］  

等任务中 ，其中输入序列通常是

一段文本 。注意力机制能够帮助模型  

找到输入序列中最重要的部分 ， 提高模型的翻译 、摘要和生成等能力 。  

自注意力机制 （Ｓｅｌｆ －Ａｔｅｎｔｉｏｎ）是

一种特姝的注意力机制 ， 它可以对序列中  

的每个元素计算

一个权重 ，用于自适应地聚焦序列中的不同部分 。在自注意力机  

制中 ，输入序列中的每个元素都可以看作是

一个查询向量、键向量和值向量 。 自  

注意力机制根据查询向量和键向量计算每个元素之间的相似度 ，然后将相似度转  

化为权重 ，再根据权重对每个元素的值向量进行加权求和 。这样 ， 自注意力机制  

可以自适应地学习到不同元素之间的关系 ，从而提高模型的表现 。其计算过程如  

下所示 ：  

Ａｔｔｅｎｔｉｏｎ（Ｑ ，Ｋ ，Ｖ）＝ｓｏｆｔｍａｘ＾（２ － ８）  

其中 ， Ｑ 、 Ｋ和Ｖ分别是输入序列中的查询向量、键向量和值向量 ， ｄｆ ｃ是键向量  

的维度 ， ｓｏｆ ｔｍａｘ函数将相似度转化为权重 。具体来说 ，ｇ是查询向量和键向量  

１８  

第二章相关技术  

之间的相似度矩阵 ， ｓｏｆｔｍａｘ函数将相似度矩阵转化为权重矩阵 ， 然后将权重矩  

阵乘以值向量 ， 得到每个元素的加权和 。  

自注意力机制主要具有以下优点 ：  

１ ）可以自适应地聚焦序列中的不同部分 ， 提高模型的表现 。  

２ ）可以处理不定长的输入序列 ， 不需要固定序列长度 。  

３ ）可以并行计算 ， 加速模型的训练和推理。  

多头注意力机制 （Ｍｕｌｔ ｉ －ＨｅａｄＡｔｅｎｔｉｏｎ）

一种改进的自注意力机制 ， 它  

可以同时从多个不同的表示空间中学习特征 ，提高模型的表现 。多头注意力机制  

最初被用于 Ｔｒａｎｓｆｏｒｍｅｒ模型中 ， 目前已经成为自然语言处理中

一种重要的技术 。  

在多头注意力机制中 ， 输入序列中的每个元素都可以看作是

一个查询向量、  

键向量和值向量 。多头注意力机制将输入序列分别投影到多个不同的表示空间中 ，  

然后在每个表示空间中计算注意力 ，最后将多个注意力结果合并起来 。这样 ， 多  

头注意力机制可以从不同的角度学习到输入序列中的信息 ，提高模型的表现。其  

计算过程如下所示 ：  

ＭｕｌｔｉＨｅａｄ （Ｑ ，Ｋ，Ｖ） ＝Ｃｏｎｃａｔ （ｈｅａｄｈｅａｄ ２ ＞？ｈｅａｄ ＾Ｗ ０  

其中 ， ０ 、 Ａ：和Ｆ分别是输入序列中的查询向量、键向量和值向量 ， ｈｅａｄ ｆ是第  

ｉ个头部的注意力 ， ／Ｉ是头部的数量 ， Ｃｏｎｃａｔ函数将多个头部的注意力拼接起来 ，  

州 ０是输出层的权重矩阵 。  

每个头部的注意力计算过程和自注意力机制类似 ，只不过是在不同的表示空  

间中进行计算 。具体来说 ，每个头部都有自己的查询向量、键向量和值向量 ， 可  

以使用不同的权重矩阵对输入序列进行投影 ，然后计算注意力 。多头注意力机制  

还可以使用残差连接和层归

一化等技术 ， 进

一步提高模型的表现。  

在实际应用中 ， 多头注意力机制已经被广泛地应用于自然语言处理领域 。例  

如 ， Ｔｒａｎｓｆｏｒｍｅｒ模型就是 一种使用多头注意力机制的神经网络模型 ， 它在机器  

翻译 、文本生成等任务中取得了很好的性能 。另外 ， 多头注意力机制还可以用于  

ｆ７５Ｋ 语音识别 等领域 ， 提高模型的表现 。  

２ ． １ ．４Ｔｒａｎｓｆｏｒｍｅｒ  

Ｔｒａｎｓｆｏｒｍｅｒ

—种基于自注意力机制的神经网络模型 ， 由Ｇｏｏｇ ｌｅ在２０１７  

年提出 ， 用于处理自然语言处理任务 ， 如机器翻译、文本生成等 。其核心思想是  

将自注意力机制和前馈神经网络结合在

一起。自注意力机制用于学习输入序列中  

不同位置的依赖关系 ， 前馈神经网络用于学习特征之间的非线性关系 。  

１９  

北京邮电大学电子信息硕士学位论文  

Ｔｒ ａｎｓｆｏｒｍｅｒ模型中还采用了残差连接和层归

一化等技术 ， 进

一步提高模型的表  

现 。  

在编码器中 ，输入序列首先通过

一个嵌入层将每个单词表示为

一个固定维度  

的向量 ，然后输入到多层的自注意力模块和前馈神经网络模块中 。 自注意力模块  

用于学习输入序列中不同位置之间的依赖关系 ，前馈神经网络模块用于学习特征  

之间的非线性关系 。多层的自注意力模块和前馈神经网络模块之间还加入了残差  

一化 ， 提高模型的表现 。  

在解码器中 ，输出序列首先通过

一个嵌入层将每个单词表示为

一个固定维度  

的向量 ， 然后输入到多层的自注意力模块、 编码器 －解码器注意力模块和前馈神  

经网络模块中 。 编码器

－解码器注意力模块用于将解码器的输出与编码器的表示  

相结合 ， 以便更好地学习源语言和目标语言之间的对应关系 。  

Ｔｒａｎｓｆｏｒｍｅｒ模型的优点在于能够处理不定长的输入序列 ，避免了ＲＮＮ在处  

理长序列时出现的梯度消失和梯度爆炸问题 。此外 ，Ｔｒａｎｓｆｏｒｍｅｒ

？模型采用了并行  

计算的方法 ，可以加快训练和推理的速度 。在机器翻译等任务中 ， Ｔｒａｎｓｆｏｍｉｅｉ

？模  

型已经取得了很好的性能 ， 也超过了之前使用 ＲＮＮ 的神经机器翻译模型

［７７】 。  

ＢＥＲＴ（Ｂｉｄｉｒｅｃｔ ｉｏｎａｌＥｎｃｏｄｅｒＲｅｐｒｅｓｅｎｔａｔｉｏｎｓｆ ｒｏｍＴｒａｎｓｆｏｒｍｅｒｓ）

一种预  

训练的语言模型 ， 由Ｇｏｏｇ ｌｅ在 ２０１８年提出 ， 采用 Ｔｒａｎｓｆｏｒｍｅｉ

？模型结构 ， 以非  

监督学习的方式从大规模文本数据中学习通用的语言表示 。 ＢＥＲＴ当时在多个自  

然语言处理任务中取得了先进的结果 ，包括文本分类、命名实体识别 、语义匹配  

等 。  

该模型的特点在于采用了双向编码器 ，即同时考虑输入序列中前向和后向的  

上下文信息 。结构由多层Ｔｒ ａｎｓｆ ｏｒｍｅｒ编码器组成 。在训练时 ， ＢＥＲＴ模型先在大  

规模无标注文本数据上进行预训练 ，然后在特定任务上进行微调 。在微调时 ，可  

以针对具体的任务和数据集选择不同的输出层和损失函数 ， 以达到最好的性能 。  

在预训练过程中 ， ＢＥＲＴ模型采用了基于掩码的语言模型和下

一句预测两种任务 ，  

以学习通用的语言表示 。其中 ，基于掩码的语言模型的任务是预测输入序列中

部分被掩盖的单词 ， 以鼓励模型学习输入序列中的上下文信息 。下

一句预测的任  

务是判断两个句子是否相邻 ， 以鼓励模型学习语言之间的关系 。  

该模型最大的贡献在于提出了

“ 预训练＋微调

” 的方法 ， 即在大规模无标注  

文本数据上预训练语言模型 ， 然后在特定任务上进行微调 ， 以获得更好的性能 。  

预训练的语言模型可以学习到通用的语言表示 ，然后可以在不同的任务中进行微  

调 ， 以适应特定任务和数据集 。  

２０  

第二章相关技术  

２ ．２深度学习技术  

２ ．２．１Ｄｒｏｐｏｕｔ  

Ｄｒ〇Ｐ〇ｕｔ ［７９］是

一种常用的正则化技术 ，通过在训练时随机删除

一部分神经元 ，  

以减少模型中神经元之间的复杂关系 ， 从而防止过拟合 。具体来说 ， Ｄｒｏｐｏｕｔ技  

术可以看作是随机失活

一部分神经元 ，以防止神经元之间产生强烈的相互依赖关  

系 。在训练时 ，每个神经元有

一定的概率被随机丢弃 ，不参与前向传播和反向传  

播过程 ，这样可以减少网络中某些特征之间的依赖关系 ，从而降低过拟合的风险 。  

在测试时 ，为了保证模型的稳定性 ， 需要将所有的神经元都保留下来 ，但是需要  

按照失活概率重新调整所有神经元的输出 ， 以保证输出的期望值不变 。  

该技术的优点在于简单易用 ，不需要额外的参数和模型结构 ，可以在很多深  

度学习模型中使用 。需要注意的是 ， Ｄｒｏｐｏｕｔ技术可能会导致模型在训练时收敛  

速度变慢 ， 因为随机失活会减少网络的有效容量 。 因此 ，在实际应用中 ， 需要根  

据具体的任务和数据集进行调参和优化 ， 以提高模型的性能 。  

２ ．２ ．２Ｎｏｒｍａｌｉｚａｔｉｏｎ  

Ｎｏｒｍａｌｉｚａｔｉｏｉｉ ［８Ｇ －８３】技术是深度学习中常用的

一种正则化技术 ，可以防止模型  

过拟合并提高模型的泛化能力 。该技术的主要思想是对输入数据进行归

一化 ， 以  

消除输入数据之间的差异 ， 从而增强模型的鲁棒性 。 其可以分为 Ｂａｔｃｈ  

Ｎｏｒｍａｌｉｚａｔ ｉｏｎ ＾８０］ 、 ＬａｙｅｒＮｏｒｍａｌｉｚａｔ ｉｏｎ间 、 ＧｒｏｕｐＮｏｒｍａｌｉｚａｔ ｉｏｎ＾ 、 Ｉｎｓｔａｎｃｅ  

Ｎｏｒｍａｌｉｚａｔｉｏｎ＾等多种不同的方法 ，具体使用哪种方法需要根据不同的场景和任  

务进行选择 。接下来对最为常用的ＬａｙｅｒＮｏｒｍａｌｉｚａｔｉｏｎ以及ＢａｔｃｈＮｏｒｍａｌｉｚａｔ ｉｏｎ  

做简单介绍 。  

ＬａｙｅｒＮｏｒｍａｌｉｚａｔｉｏｎ是

一种常用的正则化技术 ，可以防止深度神经网络中的  

梯度消失和梯度爆炸问题 ，其通过对每层神经元的输出进行归

一化 ， 以增强模型  

的鲁棒性。具体来说 ， ＬａｙｅｒＮｏｒｍａｌｉｚａｔｉｏｎ技术可以看作是对每个样本和每个神  

经元的输出进行归

一化 ， 以消除每个神经元的输出和样本的均值和方差的影响 。  

这样可以増强模型的鲁棒性 ， 并且可以缩小不同样本和不同神经元之间的差异 ，  

从而增强模型的泛化能力 。  

在具体实现时 ， ＬａｙｅｒＮｏｒｍａｌｉｚａｔ ｉｏｎ技术对每个样本和每个神经元的输出进  

一化 ， 可以看作是对输入ｘ进行以下操作 ：  

ＬａｙｅｒＮｏｒｍＣ＾）＝ ｙ

－ ｐ＝＝＋ ／ ？（２ — ９）   Ｖｃｒ ２ ＋ｅ  

２１  

北京邮电大学电子信息硕士学位论文  

其中 ， 和 Ｃ分别表示输入Ｘ的均值和方差 ， ７和０分别表示归

一化后的输出的  

缩放系数和偏置系数 ， ｅ是

一个很小的常数 ， 用于避免分母为零 。  

在训练时 ， ＬａｙｅｒＮｏｒｍａｌｉｚａｔ ｉｏｎ技术可以増强模型的鲁棒性 ， 防止模型出现  

梯度消失和梯度爆炸的问题 。在测试时 ， 由于样本和神经元的数量较少 ，可以使  

用整个数据集的均值和方差进行归

一化 ， 以提高模型的稳定性 。  

一种Ｎｏｒｍａｌｉｚａｔ ｉｏｎ技术ＢａｔｃｈＮｏｒｍａｌｉｚａｔ ｉｏｎ则可以看作是对每个批次的  

神经元的输出进行归

一化 ，以消除每个神经元的输出和批次的均值和方差的影响 。  

Ｘ — Ｕｎ   Ｂｎ｛ｘ） ＝Ｙ

ｒ＝＝＋ Ｐ（２ － １０）   ４４＋ｅ  

其中 ，办和 分别表示当前批次Ｓ中输入 ；ｃ的均值和方差 ， ｙ和沒分别表示归  

一化后的 出的缩放系数和偏置系数 ， ｅ是

一个很小的常数 ，  

２．２ ．３损失函数  

损失函数是深度学习中的

一个重要概念 ，用于衡量模型预测结果和真实结果  

之间的差异 ，所以通常使用损失函数来计算模型的预测误差 ，并且通过最小化损  

失函数来训练模型 。常见的损失函数有均方误差损失 （ＭｅａｎＳｑｕａｒｅｄＥｒｒｏｒＬｏｓｓ ，  

ＭＳＥＬｏｓｓ） 闕 、 交叉熵损失 （ＣｒｏｓｓＥｎｔｒｏｐｙＬｏｓｓ ， ＣＥＬｏｓｓ）间 、 ＫＬ散度损失  

（Ｋｕｌｌｂａｃｋ －ＬｅｉｂｌｅｒＤｉｖｅｒｇｅｎｃｅＬｏｓｓ ， ＫＬＬｏｓｓ）

［８６］等 。本节主要介绍实验中用到  

的交叉熵损失函数 。  

在深度学习中 ，交叉熵损失通常用于多分类问题中 ，可以通过最小化交叉熵  

损失来训练模型 。 而在分类问题中 ， 通常将每个类别都看作是

一个独立的事件 ，  

每个类别的概率之和等于 １ 。 因此 ， 可以使用多项式分布来描述每个类别的概率  

分布 ， 可以将真实结果表示为

一个概率分布 ７＝ ０＾ｙ２ 凡） ， 其中乃表示第 ｉ  

个类别的概率 。对于模型的预测结果 ，可以使用 ｓｏｆｔｍａｘ 函数将输出转换为

一个  

概率分布 ＝ （义 ，夕２ ， 九） ，其中负表示模型预测为第 Ｚ个类别的概率 。交叉熵  

损失可以用于衡量两个概率分布之间的差异 ， 定义为 ：  

７１  

ＬｃｓＣｙ－ｙ） ＝－

＾ ｙｉ ｉ〇ｇｙｉ（２ － １１）  

ｔ＝ｌ  

其中 ， ｙ表示真实结果的概率分布 ， ５

？表示模型的预测结果的概率分布 。交  

叉熵损失越小 ， 表示模型的预测结果越接近真实结果 。  

２２  

第二章相关技术  

２ ．２ ．４优化器  

在深度学习中 ，在训练深度学习模型时 ，通常需要最小化损失函数 ，而优化  

器是用于更新模型参数的

一种算法 。 常见的优化器有随机梯度下降 （Ｓｔ ｏｃｈａｓｔ ｉｃ  

ＧｒａｄｉｅｎｔＤｅｓｃｅｎｔ ， ＳＧＤ）間 、Ａｄａｍ（ＡｄａｐｔｉｖｅＭｏｍｅｎｔＥｓｔｉｍａｔｉｏｎ）网 、ＡｄａｍＷ网 、  

人＜＾§＾（１ ［９０ ］等 。本节主要介绍实验中涉及到的Ａｄａｍ以及ＡｄａｍＷ优化器 。  

Ａｄａｍ优化器是

一种自适应学习率的优化算法 ，可以根据梯度的

一阶矩估计  

和二阶矩估计来自适应地调整学习率 。它在处理稀疏梯度时表现良好 ， 同时也适  

用于大规模数据集和深层网络 。其是通过计算梯度的

一阶矩估计和二阶矩估计来  

更新模型参数 ， 定义为 ：  

ｍｔ ＝＋（１ －

ＰＯｇｔ  

ｖｔ ＝ ｈｖ ｔ －ｘ＋（１ －

Ｐ２）９ｔ  

八饥 ｔ  

（２ － １２）  

Ｖ ｔ ＝ Ｔ＾Ｍ  

０ ｔ＋１ ＝Ｑ ｔ

－＝ｍｔ  

其中 ， 办表示梯度 ， ｍｔ和巧分别表示梯度的

一阶矩估计和二阶矩估计 ， 馬和馬  

一阶矩和二阶矩的衰减系数 ， ｅ 是

一个很小的常数 ， 用于避免分母为  

零 。  

在实际应用中 ， Ａｄａｍ优化器可以自适应地调整学习率 ， 同时也适用于大规  

模数据集和深层网络 ，因此得到了广泛的应用 。需要注意的是 ，在使用 Ａｄａｍ优  

化器时 ， 需要选择合适的学习率和衰减系数 ， 以达到最好的训练效果 。  

ＡｄａｍＷ是 Ａｄａｍ优化器的

一种变体 ， 相较于 Ａｄａｍ优化器 ， 它添加了权  

重衰减 （ＷｅｉｇｈｔＤｅｃａｙ） 的正则化项 ， 用于避免模型过度拟合训练集 。在 Ａｄａｍ  

优化器中 ，权重衰减是通过在损失函数中添加正则化项来实现的 ，但是这种方式  

会使得学习率对于不同的参数具有不同的影响 ， 导致训练结果不稳定 。 所以  

ＡｄａｍＷ优化器在 Ａｄａｍ优化器的基础上 ，将权重衰减移到了更新步骤中 ，以使  

学习率对于所有参数都有相同的影响 。 ＡｄａｍＷ优化器定义为 ：  

２３  

北京邮电大学电子信息硕士学位论文  

＝▽０ｔＬ（０ ｔ）  

Ａｗｉ ｔ －ｉ＋（１ －

Ｐｘ）３ｔ  

ｖｔ＝＋（１ －

＾２）９ｔ  

八ｍ ｔ   ｍｔ＝

１ － Ｐｉ （２ － １３）  

八 ＿   Ｖｔ ＝ Ｔ＾Ｍ  

ｅ ｔ＋１＝ｅ ｔ

— （负 ｔ＋Ａ０ ｔ）  

ｙＪＶｔ＋６  

其中 ，Ａ 表示权重衰减系数 ，它可以控制正则化的强度 。相比于 Ａｄａｍ优化器 ，  

ＡｄａｍＷ优化器能够更好地避免模型过度拟合 ，尤其是在大规模数据集和深层网  

络中表现更加优异 。 因此 ， ＡｄａｍＷ优化器在深度学习的实践中得到了广泛的应  

用 。  

２ ．３本章小结  

本章主要介绍了与深度学习有关的模型以及技术 ，模型方面包括循环神经网  

络 、 卷积神经网络 、 注意力机制以及 Ｔｒａｎｓｆｏｒｍｅｒ ； 技术方面包括 Ｄｒｏｐｏｕｔ、  

Ｎｏｒｍａｌｉｚａｔｉｏｎ、损失函数以及优化器 。本文实验所用的模型主要以Ｔｒａｎｓｆｏｒｍｅｒ为  

主 ， 并应用多种深度学习技术 。  

２４  

第三章基于掩码上下文机器阅读理解框架的方面级三元组抽取方法  

第三章基于掩码上下文机器阅读理解框架的方面级三元组  

抽取方法  

针对当前方面级情感三元组抽取任务 ， 当前主流的机器阅读理解 （Ｍａｃｈｉｎｅ  

ＲｅａｄｉｎｇＣｏｍｐｒｅｈｅｎｓｉｏｎ ， ＭＲＣ）方法是设置不同的查询 （ｑｕｅｒｙ）和相同的上下  

文 （ｃｏｎｔｅｘｔ）并通过多轮问答 （ＱｕｅｓｔｉｏｎＡｎｓｗｅｒ， ＱＡ）解决 。 ＭＲＣ方法通常包  

含两个阶段 ， 第

一阶段是方面词推理 （Ａｓｐｅｃｔｌｎｆｅｒ ｅｎｃｅ ， ＡＩ）阶段 ， 其通常是构  

一个关于方面词的查询来提取所有的方面词 ；第二阶段是方面词附属推理  

（Ａｓｐｅｃｔ Ａｃｃｅｓｓｏｒｙ Ｉｎｆｅｒｅｎｃｅ ， ＡＡＩ ）阶段 ， 目的是构造

一个关于意见词和情感的  

查询去提取出所有的意见词和情感 。尽管之前基于ＭＲＣ的方法取得了很好的效  

果 ，然而仍存在模块间缺乏交互、训练数据量不足以及多个方面词之间存在干扰  

的问题 。为此 ，本文设计了

一种基于掩码上下文的机器阅读理解算法 ，有效地解  

决了这几个问题。实验结果表明 ，相对于现有的其他方法 ，模型可以更精准地抽  

取情感三元组 ， 达到了目前先进的效果 。  

３ ．１本章引论  

首先 ， 给出方面级三元组抽取的定义 ： 输入带有ｎ个 ｔｏｋｅｎ的

一个句子Ｓ＝  

｛ｖｖｐＷｉ

． ． ． ，ｗｎ｝ ，ＡＳＴＥ任务的目标是提取所有的三元组（ａ ，ｏ，ｓ） ，其中 ａ代表方面  

词 ， 〇代表ａ对应的意见词 ， ｓ代表情感极性 ，其中 ｓｅ ｛ｐｏｓｉｔ ｉｖｅ ，ｎｅｕｔｒａｌ ，ｎｅｇａｔ ｉｖｅ｝ 。  

例如 ， 给定句子为

“ Ｎｉｃｅａｍｂｉｅｎｃｅ ，ｂｕｔｈｉｇｈｌｙｏｖｅｒｒａｔｅｄｐ ｌａｃｅ ”

， 其三元组有  

（ａｍｂｉｅｎｃｅ ，Ｎｉｃｅ，ｐｏｓｉｔｉｖｅ） 以及 （ｐ ｌａｃｅ ，ｏｖｅｒｒａｔｅｄ ，ｎｅｇａｔｉｖｅ） 。 以下为三元组中各  

个元素的具体定义 ：  

鲁 方面词 （ａｓｐｅｃｔ）：ａ是情感分析中评判的对象 ， 在方面级三元组抽取任  

务中其总是显式出现在文本中 。例如 ， “ Ｎｉｃｅａｍｂｉｅｎｃｅ ，ｂｕｔｈｉｇｈｌｙｏｖｅｒｒａｔｅｄ  

ｐ ｌａｃｅ ． ” 中的

ａｍｂｉｅｎｃｅ ” 以及

ｐ ｌａｃｅ

” 都是方面词 。  

籲 意见词 （ｏｐ ｉｎｉｏｎ）：ｏ是情感分析中对于评判对象ａ的观点 ， 在方面级三  

元组抽取任务中其总是依附于方面词存在 。例如 ，在

“ Ｎｉｃｅａｍｂｉｅｎｃｅ ，ｂｕｔ  

ｈｉｇｈｌｙｏｖｅｒｒａｔｅｄ

ｐ ｌａｃｅ． ”

中 ， “Ｎｉｃｅ ”

ａｍｂｉｅｎｃｅ 

的意见词 ，

ｏｖｅｒｒａｔｅｄ ”  

“ ｐ ｌａｃｅ

” 的意见词 。  

＊ 情感极性 （ｓｅｎｔｉｍｅｎｔｐｏｌａｒ ｉｔｙ）：ｓ是情感分析中对于评判对象ａ的情感极  

性 ，其通常是积极的 （ｐｏｓｉｔｉｖｅ ， ＰＯＳ ） 、消极的 （ｎｅｇａｔｉｖｅ ， ＮＥＧ） 、 中性  

２５  

北京邮电大学电子信息硕士学位论文  

的 （ｎｅｕｔｒａｌ ，ＮＥＵ ）其中

一种 。例如 ，在

“ Ｎｉｃｅａｍｂｉｅｎｃｅ ， ｂｕｔｈｉｇｈｌｙｏｖｅｒｒａｔｅｄ  

ｐ ｌａｃｅ ．

” 中 ， 方面词

“ ａｍｂｉｅｎｃｅ

” 的情感极性是积极的 ， 方面词

ｐ ｌａｃｅ

的情感极性是消极的 。  

需要注意的是在该任务中方面词与意见词可以是

一对多 、多对

，甚至是多对多  

的关系 ， 且总是由单个 ｔｏｋｅｎ或者多个连续 ｔｏｋｅｎ组成 ， 不存在非连续情况 。  

传统的ＭＲＣ方法通常是通过多轮的问答方式来进行 ， 这些问答通常是基于  

不同的查询 （ｑｕｅｒｙ ）以及相同的上下文 （ｃｏｎｔｅｘｔ）模式 。通常问答包含两个阶段 ，  

分别为方面词推理 （Ａｓｐｅｃｔ Ｉｎｆｅｒｅｎｃｅ ， ＡＩ ）阶段以及方面词附属物推理 （Ａｓｐｅｃｔ  

Ａｃｃｅｓｓｏｒｙ Ｉｎｆｅｒｅｎｃｅ ， ＡＡＩ ）阶段 。 在方面词推理阶段 ， 将原始输入句子作为上下  

文 ， 查询是关于查找方面词的自定义提示 ，例如

“ Ｗｈａｔａｓｐｅｃｔｓ？

。在方面词附属  

物推理阶段 ， 同样将原始输入句子作为上下文 ， 查询是关于方面词附属物 ， 即方  

面词对应意见词以及情感的提不 ， 例如

“ Ｗｈａｔｏｐ ｉｎｉｏｎｓａｎｄｓｅｎｔｉｍｅｎｔｇ ｉｖｅｎｔｈｅ  

ａｓｐｅｃｔａｍｂｉｅｎｃｅ？

ｐｏｓ ｉｔｉｖｅ ｎｅｇａｔｉｖｅ  

ｊＮｉｃｅ ｉ ｉａｍｂｉｅｎｃｅ

］ ，ｂｕｔｈｉｇｈｌｙｊｏｖｅｒｒａｔｅｄ ｜ｐ ｌａｃｅ

厂    

｜Ｔｒｉｐ ｌｅｔｓ ：｛ａｍｂｉｅｎｃｅ ，Ｎｉｃｅ ，ＰＯＳ｝｛ｐ ｌａｃｅ ，ｏｖｅｒｒａｔｅｄ，ＮＥＧ｝ ｜    ｌ  

＇ Ｍｅｒｅｎｃｒｓｔａａ＾ ！ａｍｂｉｅｎｃｅ

—＾Ｎ ｉｃｅ，ＰＯＳｐ ｌａｃｅ— ｏｖｅｒｒａｔｅｄ，、￡Ｇ

Ｔｒａｄｉｔｉｏｎａｌ ｜ｑｕｅｒｙｃｏｎｔｅｘｔ ｑｕｅｒｙｃｏｎｔｅｘｔ   ＭＲＣａｂｏｕｔａｍｈｉｅｔｕｅｖｖｌｌｌＣＡＣａｂｏｕｔ ｐ ｌａｃｅＷｌｌｌＣＡＸ  

ｉＬ太 士 吟心

’Ｊｖ．又

、ｖ ？ ？

＂ Ｊ［  

蠢 ｉ  

ｉＣＯ＼１ －＼ＩＲＣ ｃｏｎｔｅｘｔ ｃｏｎｔｅｘｔ  

（Ｏｌｉｒｓ ）

Ｃ ＪＵＣｒｊｍａｓｋｅｄ

ｐ ｌａｃｅＱ ＩＩ６ｍａｓｋｅｄｔｔｍｂｉｅｎｃｅ

ｔ ＿   Ｊ  

１ ： 传统的 ＭＲＣ方法与本文方法的不同  

尽管传统的ＭＲＣ方法取得了优异的效果 ， 但却会受到其他方面词造成的干  

扰问题 。 如图 ３ － １所示 ， 在 ＡＡＩ阶段处理

“ ａｍｂｉｅｎｃｅ

ｐ ｌａｃｅ

” 可能会造成干  

扰 ， 在处理

ｐ ｌａｃｅ

“ ａｍｂｉｅｎｃｅ

” 也同样可能造成千扰 。 训练所用的基础模型  

会配备注意力机制 ，该机制会潜在地捕捉方面词与意见词的联系 ， 如果模型在处  

理该方面词时注意到其他方面词 ， 那么也会注意到其他方面词对应的意见词 ，进  

而对本方面词的意见词推理造成干扰 。而且

一般来说 ，

一句话的方面词越多 ， 那  

２６  

第三章基于掩码上下文机器阅读理解框架的方面级三元组抽取方法  

么三元组的数量越多 ，提取三元组的难度也会变高 。故本文认为如果可以将

一句  

包含多个方面词的句子视为多条样本 ，将会加强模型学习的效果 。 由此本文提出  

一种基于掩码上下文的机器阅读理解框架 ，如图 ３ －１所示 。 ＣＯＭ

－ＭＲＣ与传统  

ＭＲＣ最大的不同是 ：传统ＭＲＣ方法基于不同的ｑｕｅｒｙ和相同的ｃｏｎｔｅｘｔ ；而ＣＯＭ －  

ＭＲＣ方法是基于相同的ｑｕｅｒｙ和不同的 ｃｏｎｔｅｘｔ。其中不同的 ｃｏｎｔｅｘｔ是基于掩码  

方面词进行数据增强 ，从而将包含多个方面词的句子扩充为多条样本 ，进而提升  

模型效果 。  

本章后续内容安排如下 ：  

首先介绍 ＣＯＭ －ＭＲＣ框架的具体细节 ， 其次介绍主实验的数据集情况以及  

实验结果 ， 最后介绍消融与对比实验证明该方法的有效性 。  

３ ．２基于掩码上下文的机器阅读理解框架  

３ ．２ ． １框架总览  

为了利用机器阅读理解方法精准地提取三元组 ，本文提出了

一种基于掩码上  

下文的机器阅读理解 （ＣＯｎｔｅｘｔｅｄＭａｓｋｅｄＭａｃｈｉｎｅＲｅａｄｉｎｇＣｏｍｐｒｅｈｅｎｓｉｏｎ， ＣＯＭ －  

ＭＲＣ）框架 ，如图 ３ －２所示 。 ＣＯＭ －ＭＲＣ框架包含三个部分 ，分别是掩码式数据  

增强方法 、交互式判别模型以及阶段式推理算法 ，这三部分分别解决了训练数据  

量不足、各模块之间没有交互以及推理时其他方面词存在干扰问题 ，并且这三部  

分可以协同工作 ， 相互促进 ， 实现了较好的效果 。  

具体而言 ， 在 ＣＯＭ －ＭＲＣ框架中 ， 掩码式数据增强方法通过掩码方面词思  

想实现简单有效的数据增强 ，有效扩充了训练的数据量 ；交互式判别模型中利用  

ＢＥＲＴ为主干网络 ， 并包含方面词提取模块、意见词提取模块 ， 情感极性判别模  

块以及方面词存在判定模块 ，这四个模块之间有效交互信息实现更好的性能 ；两  

阶段式推理方法分为方面词推理、方面词附属物（即方面词对应的意见词 、情感）  

推理两个阶段 ，在方面词附属物推理阶段通过掩码其他方面词从而减少它们带来  

的干扰 。  

２７  

北京邮电大学电子信息硕士学位论文  

Ｍｏｄｅｌ Ｔｒａ ｉｎ ｉｎｇ Ｉｎｆｅｒｅｎｃｅ  

Ａｓｐｅｃｔ  ｉｎｆ ｅｒｅｎｃｅ  

ｉ ｆＳＶＡ

，Ｆｉｎｄｔｈｅ ｆ ｉｒｓ ｔ ａｓｐｅｃｔ ｔｅｒｍａｎｄ

ｆｆｆｆ

，Ｔ＾Ｐ 〇 ？ ｄ Ｓｎ Ｆ５Ｐｌ ｎ

１ ° ｔｈ ｆ ｔｃｘ ｔ（Ｃ＾Ｗ＾ｏｖｅｒｍｃｄ ｐｂｃｏ  

Ｆ．ｘｋｔｅｎｃｅＳｉｅｎ ｉ ＼ｍｔｎ ｉ １ＡｓｐｅｃｔＯｐ

ｉｎ ｉｏａ １ 

ｆ ｓ

： ＵｙｅｒＬ？声ｊＬａ＞ｅｒＬａｙｔｒ ｜Ｃｏｎｔｅｘｔ．ｖ

，Ａ ａｍｂ ，ｃｎｃｃ１ ｒｕｅ  ；  

＾ Ａｕｇｍｅｎｔａｔｉｏｎ＾＾＾£ ３＾ ４二 一 ＇  

ＱＣ２ｂｍ Ｕｊ

＞ｈ＾ｏｖｅｒｒａｔｅｄ

ｐ ｌａｃｅ   

［Ａｒ ｉｄ ＆Ｎ〇ｒｍ

＾ ＾Ｃｉ

＇Ｎ ｉｃｅ ａｍｂ ｉｅｎｃｅ ．ｂｕ ｔ ｈ ｉｓｂ ｌｙ ｏ ＼ｅｒｒａ ｔｅｄ ｐ

Ａｐ ｌａｃｅ ； ＆ Ｔ ｒｕｅ —  

ｙＴＶＡＷｏｏｔｉｏｎ ］ 一 ＩＥｋＳ？ＯＳ＼— ｃ〇＼ ＞ｃｃＱ

ｂｕ ｔ ｈｆ ｅｋ＾＾ＬＺ：

－   ／ 

髟Ｕ ｌＪ  

＇— Ｎｉｃｅ 

．ｂｕ ｌ ｂｉｇｈ ｉｙｏ ％ｃｍｔｅｄ ｐｂｅｖ ． —

． ， ２；Ａｓｐｅｃｔ Ａｃｃｅｓｓｏｒｙ 

Ｉｎｆｅｒｅｎｃｅ  

ｆｘｓｐｅｎｌｔｖｐｌ ［Ｏｐｈ ｉ ｉｏａＫｅｐ

＊ — ＊ＢＴｍｅＳＳＥＧＡｅ ｔ ｌｉｃｅ〇〇＼ｃｍａｃｄ   ？

－＝－１ｆ运   □Ｋｓ

， ］Ｑ

？ｂｕ ｔ ｈ ｉｇｈ ｌｙ ｏｖｃｒｒａｔｅｄ 

：：＾＿ ｐ〇ｓ

驪 Ｎ ｉｗ

＂ Ｕ  

１ ｔｔｔｔｔｔ

、— ＊ｇＴｎ ＾ ｃＳＰＯＳ Ａａｍｂ ｉｅｎｔ〇Ｓ ．ｃｃ 、  

丁 Ｑ０２ｂｕ ｔ ｈ ｉｇ ｉ ｉＫ ｏｖｗｒａｕ －ｎｉ ｐ ｌ？ｃｃ ．  

： ：Ｔ ｆ

— Ｔｌｆｉ

一￣ ； ， １ｆ ｔ

— ￣ ｆ °ａ＾？Ｌ＿＾

．＾？ ？？ｗ ° ｖ＾ ［＝ｊ

－ ｈ  

□Ｌ －Ｗ－ Ｊ□Ｌ ． ， ｊｄ

＂＾ ＿ｉＬ＾

ｊＷｐＫ？Ｈｍｂ．＾Ｎｔ ｅ ．Ｔ ＯＳＫ

ｆＣＬＳ】ｑ

＾ＳＥＰ ＾ｆｃ

＇ ＾ＳＥＰ ： ＼ｐ

ｌａｃｅ ， ｏｖｅｒｒａｔｅｄ， ＮＥＧ ｝＜—  

ＱＱｕｃ ｒ＞Ｃ ｊＭａ ｓｋｅｄ  （ｏ ｔｉ ｉｏｘ ｉＭａｘｐｏｏ ｌ ｉｎｇ？  Ｃｏｎｃａｉｃｎａ ｉｃ ｊ＾ ｊＭａｓｋ  

Ｅ＼ ｓｐｅｃ ｔＥｘ ｉｓｔｅｎｃｅＳＳｅｎ ｉ ｉｍｃｎ ： ＡＡ ｓ ．ｐｃｃ ｉＱ 

ｉｍｏｎ  

图 ３ －２ ：ＣＯＭ

－ＭＲＣ框架总览  

３ ．２ ．２掩码式数据增强  

掩码式数据增强是本文提出的

一种简单有效的数据增强方法 ，该方法通过将  

相同的查询以及增强的掩码上下文组合作为训练样本 ，相对于传统的机器阅读理  

解的输入构造方法 ， 该方法显著扩充了数据量 。  

首先 ， 本研究整个模型是使用固定的查询以及不同的掩码上下文作为输入 ，  

并使用 ＢＥＲＴ作为句子编码器来表征语义 。具体来说 ， 对于固定的查询 ，本研究  

主要鉴别最左侧的没有被掩码的方面词以及其对应的意见词和情感 ， 如下所示 ：  

ｑ ＝Ｆｉｎｄｔｈｅ ｆ ｉｒｓｔａｓｐｅｃｔｔｅｒｍａｎｄ  

ｃｏｒｒｅｓｐｏｎｄｉｎｇｏｐ ｉｎｉｏｎｔｅｒｍｓ

＾」  

对于不同的掩码上下文 ，本文首先介绍掩码式数据增强方法 。假设

一个句子  

■有 ｔ个方面词 ，对于每个方面词本文应用两种操作 ： ｍａｓｆ ｃｉＸｇ或 ｎｏｔｍａｓｆ ｃｉｎｇ 。  

由此可以获得 个实例作为上下文 。这

一过程由图 ３

－２形象描述 。掩码某个方面  

词意味着将该方面词的所有 ｔｏｋｅｎ进行掩码 。假设为了掩码第 ｋ个 ｔｏｋｅｎ ， 本文会  

将对该 ｔｏｋｅｎ的注意力分值设为 ０ 。 定义掩码矩阵Ｍ如下所示 ：  

，ｉｆ； ＝Ａ：   Ｍ＂ ＝

，ｏｔｈｅｒｗｉｓｅ （３ － ２）  

然后本研宄将该矩阵应用到 ＢＥＲＴ中的注意力模块中 。 给定 ｑｕｅｒｙ

（？ 、 ｋｅｙＫ以  

及 ｖａｌｕｅＩ／ ， 应用掩码矩阵的注意力模块Ｚ为 ：  

２８  

第三章基于掩码上下文机器阅读理解框架的方面级三元组抽取方法  

Ａ（Ｑ ，Ｋ ，Ｖ） ＝ｓｏｆｔｍａｘ

－ ＋ｂ＾

Ｖ（３ － ３）  

其中ｄ为 或ＡＴ或Ｋ的维度 。  

给定固定的查询 （公式 ３ － １ ） 以及由掩码式数据增强方法生成的掩码上  

下文 Ｊｆ ， 本研究使用 ＢＥＲＴ 去表征语义 。 具体的输入给定为 ：  

［ＣＬＳ ］［Ｓ￡Ｐ ］Ｘ ［卿 ］

，其中＿］和＿］是ＢＥＲＴ中的特殊 ｔｏｋｅｎ ，分别是句子  

的起始与间隔标志 。假设ｑ包含ｍ个 ｔｏｋｅｎ ， ；》：的 ｔｏｋｅｎ数量与原句子 ｔｏｋｅｎ数量  

相同 ， 其包含ｎ个 ｔｏｋｅｎ ， 可以从 ＢＥＲＴ 中的最后

一层得到句子的语义表示ｈｅ  

ｊｊ＾ ｄｘ（ｍ＋ｎ＋３）

〇  

３ ．２ ．３交互式判别模型  

考虑到提取方面词 、提取意见词以及情感判定三部分是有关联的 ，例如对于  

“Ｎｉｃｅａｍｂｉｅｎｃｅ ，ｂｕｔｈｉｇｈｌｙｏｖｅｒｒａｔｅｄｐ ｌａｃｅ

’ ％已知对于 “ ａｍｂｉｅｎｃｅ ” 的意见词是  

“ Ｎｉｃｅ ”

，那么可以判定其情感是积极的 ， 因为

“ Ｎｉｃｅ

” 是 一个正向的表达积极情  

感的词汇 。故为了使提取方面词 、提取意见词 、判别情感等模块交互信息 ，本研  

一种交互式判别模型 。 该模型分为四个模块 ， 分别为方面词提取模块、  

意见词提取模块、 情感判别模块以及方面词探测模块 。  

３ ．２ ．３ ． １方面词提取模块  

为了得到首个没有被掩码的方面词 ，在得到句子的表示 ／Ｉ后 ，本文定义［ＣＬＳ ］  

ｔｏｋｅｎ对应的表示为ｈｅＺｓｅＭ ｄｘｌ

，掩码上下文对应的表示为 ｅＭ ｄｘｎ

。受 ｓｐａｎ －  

ｂａｓｅｄ方法

［＿的启发 ，本文设计了方面词提取模块 ，该模块可以得到目标词的开  

始与结束位置的概率 ， 如下所示 ：  

ｒａ ＝＾ａ，ｌＫ  

ａ＇ｓ＝ｓｏｆｔｍａｘ（Ｗａ２ｒａ）（３ －４）  

ｐ ａ＇ｅ＝ｓｏｆｔｍａｘ（Ｍ／

ａ ３ｒａ）  

其中％ ，１ ￡股

、＾２ ￡肢 １＞＾以及＾ ，３ £ ］１＾

￡４是可训练的参数 ， 仏代表方面词  

的表示 ， 是开始位ｉ的概率 ， ？ 是结束位置的概率 。对于开始和结束位置 ，  

本文使用交叉熵损失函数 ， 由此方面词提取损失Ａ定义如下 ：  

ｎｎ  

Ａ ＝ｙｆ

ｉ〇ｇｐｒｙｒ

＊ｅ（３ － ５）  

ｉ＝ｌ ｉ－ｌ  

２９  

北京邮电大学电子信息硕士学位论文  

其中ｙＷ ｅＲ ｎ以及ｙ＃ｅＲ ＂是最左侧的未掩码方面词的开始与结束位置的真实  

值 ， 下标ｉ代表第ｉ个 ｔｏｋｅｎ。  

３ ．２．３ ．２意见词提取模块  

为了得到首个未被掩码方面词对应的意见词 ，与方面词提取模块类似 ，本文  

设计意见词提取模块如下所示 ：  

ｒ〇 ＝Ｗ〇 ｉｈｘ  

ｏｓ＝ｓｏｆ ｔｍａｘ ｆＷ＾ｒ。）（３ － ６）  

〇 ，ｅ＿ ｓ〇ｆｔｍａｘ（Ｍ＾ ｏ３ｒ０）  

其中 以及％ ，３ ｅＥ ｌｘｄ是可训练的参数 ， ｒ。代表意见词  

ｐ ？是开始位金的概率 ，ｐａ是结束位置的概率 。与方面词提取模块类似 ，  

对开始和结束位置使用交叉熵损失函数 ， 故意见词提取损失＞＾定义如下 ：  

ｎｎ  

ｘ０ ＝ｆ ｌｏｇ＃ｙｆ

＊ｅ（３ － ７）  

ｉ－１ ｉ＝ｌ  

° ＞ｓ ｅＭ ｎ以及ｙ

° ｉｅ ｅ１＾是最左侧的未掩码方面词对应意见词的开始与结束  

位置的真实值 ， 下标 ｉ代表第 ｉ个 ｔｏｋｅｎ。  

３ ．２３ ．３情感判别模块  

直觉上来说 ，情感极性与方面词以及意见词具有很强的关联性 ，而且也与整  

句话的语义有关 ，故本文使用多头注意力机制去融合这些信息 。整个过程如下所  

示 ：  

ｒｓ ＝ＬＮ（ｈ＾＋ＭｕｌｔｉＨｅａｄＣ＾，ｒａ，ｒ０））  

ｇｓ ＝Ｍ？（ｒｓ） （３ －８）  

ｓ＝ｓｏｆｔｍａｘ（Ｗｓｇｓ＋ｂｓ）  

其中ＬＮ代表ＬａｙｅｒＮｏｒｍ、Ｍｕｌｔ ｉＨｅａｄ代表多头注意力机制 ，ＭＰ代表ｍａｘｐｏｏｌｉｎｇ 。  

ｒｓｅＲ ｄｘｎ和仇 ｅＲ ｄｘｌ是中间变量 ，呎 ｅ股 ３＞＾和￡）５是可训练的权重和偏置 。情感  

判别的交叉熵损失给定如下 ：  

＝－１ ｙｆ ｌｏｇｐｆ（３ － ９）  

ｉ＝ｌ  

ｓ ｅＭ ３是情感极性的标签 。  

３０  

第三章基于掩码上下文机器阅读理解框架的方面级三元组抽取方法  

３ ．２ ．３ ．４方面词探测模块  

该模块是为了探测在掩码的上下文中是否仍存在方面词 。对于所有方面词均  

被掩码的上下文 ，其标签为Ｆａｌｓｅ ，否则为Ｔｒｕｅ。该模块将 ［ＣＬＳ］ｔ ｏｋｅｎ的表示  

方面词表示仏以及意见词表示＆作为输入 ， 如下所示 ：  

ｒｅ ＝ Ｋｉｓ？ＭＰ（ｒ０） （ｇ ）ＭＰ（ｒａ）ｎ－１ｍ  

ｅ＝ｓｏｆｔｍａｘ（Ｍ４ｒｅ＋ｂｅ）

＾Ｊ  

其中 ｒｅ ｅＲ ３ｄｘｌ是中间变量 ， ％ ｅＲ ２ｘ３ｄ以及￣是可训练的权重以及偏置 。 符  

号 （Ｓ ）代表拼接操作 。对于该模块 ， 本研宄使用使用二元交叉熵损失给定如下 ：  

＾ ｙｆ ｌｏｇｐ！（３ － １１）  

ｉ＝ｌ  

３ ．２Ｊ ．５优化目标  

整体的优化目标是最小化如下损失函数 ：  

Ｌｔ ＝ａ ．ＬＡ＋ ＾Ｌ〇＋ｙ￡ｓ＋Ｓ￡ｅ（３ － １２）  

其中ａ 、 沒 、 ｙ以及５是超参数 ， 用于调节对应模块损失的影响因子 。  

３ ．２ ．４阶段式推理方法  

为了缓解其他方面词带来的干扰 ，本文提出了

一种阶段式推理方法如算法 １  

所示 。  

算法 １ ：ＣＯＭ －ＭＲＣ阶段式推理算法  

输入 ： 句子Ｓ以及查询ｑ  

输出 ： 三元组Ｊ＝ ｛（ａ，ｏ ，ｓ）｝ｗ  

１初始化 ＝  

／／方面词推理 （ＡＴ）阶段 ： 获得方面词探测标识 ｅ和方面词 ａ  

２ｗｈｉｌｅｅ＝Ｔｒｕｅｄｏ  

３ｃ／Ｚｃ／ＺＵ ｛ｆ ｌ｝  

４ｅ ，ａ ＊ｒ － ＧｅｔＡＩ｛ｑ，Ｓ．Ｍａｓｋ （Ａ））  

５ｅｎｄｗｈｉｌｅ  

／／方面词附属推理 （ＡＡＴ）阶段 ： 获得意见词集合０和情感ｓ  

６ｆｏｒａ ｉ ＥＡｄｏ  

７０ ，ｓ＜ｒ

－ ＧｅｔＡＡＩ ｛ｑ，Ｓ．Ｍａｓｋ｛ＪＬ —

｛ａ ｊ｝））  

３ １  

北京邮电大学电子信息硕士学位论文  

８ｆｏｒ〇 ｊＥ０ｄｏ  

９Ｔ＜ －ＴＵ ｛（ａ ￡，ｏ

，ｓ）｝  

１０ｅｎｄｆｏｒ  

１１ｅｎｄｆｏｒ  

１２ｒｅｔｕｒｎＴ  

该算法包含两个连续的阶段 ： 方面词推理 （Ａｓｐｅｃｔｌｎｆｅｒ ｅｎｃｅ ，ＡＩ）阶段以及  

方面词附属物推理 （ＡｓｐｅｃｔＡｃｃｅｓｓｏｒｙ Ｉｎｆｅｒｅｎｃｅ ， ＡＡＩ）阶段 。 ＡＩ阶段是为了提取  

所有的方面词 ， 而ＡＡＩ阶段是为了鉴别所有方面词对应的意见词和情感 。 整体  

的算法流程如图 ３ －２所示 。  

在ＡＩ阶段 ， 首先使用将查询ｇ以及句子Ｓ输入至训练后的模型中 ， 从而获  

得方面词探测标识 ｅ以及首个方面词ａ 。若探测标识 ｅ是Ｔｒｕｅ ，増加方面词 ａ至  

方面词集合 ｃ／ｌ中 ， 然后在句子Ｓ中将方面词 ａ掩码 。 然后将查询 ｑ以及掩码后  

的上下文输入至模型 ， 从而得到下

一个方面词探测标识 ｅ以及下

一个方面词ａ 。  

重复以上步骤直至方面词探测标识为Ｆａｌｓｅ 。最后 ，本研宄得到了方面词集合 ｃ／Ｚ 。  

在ＡＡＩ阶段 ， 对于某个方面词ａ ， 本文掩码所有除ａ以外的其他方面词作  

为掩码上下文 。将固定的查询 ｑ以及掩码后的上下文输入至模型中 ，本研究可以  

得到该方面词对应的意见词集合０以及情感极性 ｓ 。最后遍历意见词集合０，将  

每个意见词与方面词ａ 、情感极性ｓ组合形成三元组加入至集合Ｊ中 。算法 １中  

Ｓ．ＭａｓｋＧ／Ｚ）代表通过掩码集合 ｃ／Ｚ中所有方面词将句子Ｓ更新为掩码后的上下  

文 。  

图 ３ －２ 的推理方法部分给出了

一个样例 ， 对于

“Ｎｉｃｅａｍｂｉｅｎｃｅ ，ｂｕｔｈｉｇｈｌｙ  

ｏｖｅｒｒａｔｅｄ

ｐ ｌａｃｅ

， 在ＡＩ阶段 ， 通过输入固定的查询

Ｆｉｎｄｔｈｅｆｉｒｓｔａｓｐｅｃｔｔｅｒｍａｎｄ  

ｃｏｒｒｅｓｐｏｎｄｉｎｇｏｐ ｉｎｉｏｎｔｅｒｍｓｉｎｔｈｅｔ ｅｘｔ ” 以及将原始句子作为上下文 ， 模型首先判  

别其首个方面词为

“ ａｍｂｉｅｎｃｅ ”

，方面词存在标识为Ｔｒｕｅ ；紧接着将

“ ａｍｂｉｅｎｃｅ ”掩  

码 ，然后将查询以及掩码后的上下文输入至模型 ，得到第二个方面词

“ ｐ ｌａｃｅ

” 以  

及方面词存在标识Ｔｒｕｅ ；最后再将

ｐ ｌａｃｅ ” 掩码 ，将查询以及掩码后的上下文输  

入至模型后发现方面词存在标识变为Ｆａｌｓｅ ，方面词推理阶段结束 。在ＡＡＩ阶段 ，  

输入查询以及掩码

ｐ ｌａｃｅ

” 的上下文 ，模型得到

“ ａｍｂｉｅｎｃｅ

” 的意见词为

“ Ｎｉｃｅ

情感为Ｐｏｓｉｔｉｖｅ ；然后输入查询以及掩码

“ ａｍｂｉｅｎｃｅ

” 的上下文 ，模型得到

“ ｐ ｌａｃｅ

“ ｏｖｅｒｒａｔｅｄ ”

，情感为Ｎｅｇａｔｉｖｅ 。最后将两阶段结果汇总 ，得到两个三  

元组｛ａｍｂｉｅｎｃｅ ， Ｎｉｖｅ ， Ｐｏｓｉｔ ｉｖｅ｝以及｛ｐ ｌａｃｅ ， ｏｖｅｒｒａｔｅｄ ， Ｎｅｇａｔｉｖｅ｝ 。  

３２  

第三章基于掩码上下文机器阅读理解框架的方面级三元组抽取方法  

３ ．３实验对比与分析  

３ ．３ ． １数据集介绍  

 表 ３ － １ＡＳＴＥ数据集评论语句以及三元组数量统计   

Ｒｅｓｔ１４Ｌａｐ１４Ｒｅｓｔ１５Ｒｅｓｔｌ６  

｜一一扣 评触

丨 －』 评倾

｜一＾？ 评賴

｜一＾？   句句 句 句  

训练集１２５９２３５６８９９１４５２６０３１０３８８６３１４２１  

？ｉ验证集３１５５８０２２５３８３１５１２３９２１６３４８  

测试集４９３１００８３３２５４７３２５４９３３２８５２５  

训练集１２６６２３３８９０６１４６０６０５１０１３８５７１３９４  

Ｄ２验证集３１０５７７２１９３４６１４８２４９２１０３３９  

测试集４９２９９４３２８５４３３２２４８５３２６５１４  

实验对ＣＯＭ

－ＭＲＣ框架以及其他基线方法在两组ＡＢＳＡ数据集上的性能进  

行了评估 。其中 ，第

一组数据集 （记为２＾ ）是Ｗｕ等人标注的 第二组数据集  

（记为２） ２ ）是Ｘｕ等人基于 ： 正后的数据集ｍ 。这些数据集均来源于ＳｅｍＥｖａｌ ，

包括Ｒｅｓｔ１４、 Ｌａｐ １４、Ｒｅｓｔ１５ 、Ｒｅｓｔ１６这四个子数据集 。 Ｌａｐ是Ｌａｐ ｔｏｐ的缩写 ，  

表示笔记本领域 ； Ｒｅｓｔ是Ｒｅｓｔａｕｒａｎｔ的缩写 ，表示餐馆领域 ； 两组数据集的统计  

情况如表 ３ －１以及表 ３ －２所示 。  

表 ３ －２ＡＳＴＥ数据集多方面设定下句子与三元组词数量统计  

Ｒｅｓｔ１４Ｌａｐ１４Ｒｅｓｔ１５Ｒｅｓｔｌ６  

丨包含的 多方面

丨包含的 多方面

｜包含的 多方面

丨包含的  

词駒 词赖 词駒 三碰 词躺 三碰  

训练集５３６１４８５２５４６８５１９０５ １２８６３１４２１  

〇！验证集１ １９３２２７５２０８４２１０８６２１６２  

测试集２２８６６８１０３２６６８２２１３９３２３８  

训练集５３３１４４３２６５７０９１８３４８９２４４６５２  

？２验证集１２３３５２５９１５５４９１２５６５１６３  

测试集２２８６６２１０３２６６８２２１ １９１２３２  

３３  

北京邮电大学电子信息硕士学位论文  

３ ．３ ．２实验参数与设置  

本文采用开源深度学习框架ＰｙＴｏｒｃｈ实现了所提出的 ＣＯＭ －ＭＲＣ框架 ， 并  

在公开基准数据集上进行了训练。为验证其有效性 ，本文在基准数据集上进行了  

定性以及定量实验 ， 实验环境为Ｕｂｕｎｔｕ１８ ．０４服务器 ， ＧＰＵ为单张 ２４ＧＢ显存  

的ＮＶＩＤＩＡＧｅＦｏｒｃｅ３０９０显卡 。  

在训练阶段 ， 使用 Ｂｅｒｔ －Ｂａｓｅ －ＵｎｃａｓｅｄＥｎｇ ｌｉｓｈ作为句子编码器以及ＡｄａｍＷ  

作为优化器 。训练时应用ｗａｒｍｕｐ策略，学习率于前 １０％训练步骤内线性增长为  

９Ｘ１０＇ 于后 ９０％训练步骤内学习率 ｃｏｓｉｎｅ式下降为 ０ ， 并且应用梯度衰减  

（ｗｅｉｇｈｔｄｅｃａｙ） ，衰减因子设为 １〇

。批处理大小 （ｂａｔｃｈｓｉｚｅ）设为１５ ，ｄｒｏｐｏｕｔ  

设为 ０ ． １ 。考虑到预测的性能与方面词的关系较大 ，这是由于若方面词预测错误 ，  

那么预测其对应的意见词和情感也就失去了意义 。故将方面词模块的损失设置较  

大的权重 ， 故将公式３ — １２中的四个超参数ａ、 ／ ？ 、 ｙ以及５分别设为 ８ ．０、 ３ ．２、  

１ ．０以及 １ ．０ 。在推理过程中 ，本文使用启发式解码算法预测方面词以及意见词区  

间 ，对应的阈值根据不同数据集手动设定 。使用ＮＶＩＤＩＡＧｅＦｏｒｃｅ３０９０显卡训练  

模型 ， 平均每个数据集训练时间为 ０ ．８５ｈ。  

需要说明的是 ，上述模型的结构和超参数设置均通过验证集的表现进行选择 ，  

最后报道的模型表现结果是 ５个不同随机种子设置下的平均值。  

３ ．３ ．３评估指标  

实验采用精确率 （Ｐｒｅｃｉｓｉｏｎ， Ｐ）、 召回率 （Ｒｅｃａｌｌ ， Ｒ） 以及Ｆ１指标来衡量  

三元组抽取的性能 。  

１ ）Ｐｒｅｃｉｓｉｏｎ（精确率） ： 在该任务中三元组的精确率是指提取正确的三元组  

占所有预测的三元组的比例 。需要注意的是正确的三元组是指每个元素都必须匹  

配 ， 若有

一个不正确则视为整个三元组不正确 。 精确率ｐ表示如下 ：  

ｃｏｒｒｅｃｔｔｒｉｐ ｌｅｔｓｎｕｍｂｅｒ（３ — １３）  

ｐｒｅｄｉｃｔｔｒｉｐ ｌｅｔｓｎｕｍｂｅｒ  

２）Ｒｅｃａｌｌ（召回率） ： 在该任务中三元组的召回率是指提取正确的三元组占  

所有真实标注的三元组的比例 。 召回率Ｒ表示如下 ：  

ｃｏｒｒｅｃｔｔｒｉｐ ｌｅｔｓｎｕｍｂｅｒ＾  

ｌａｂｅｌｅｄｔｒｉｐ ｌｅｔｓｎｕｍｂｅｒ  

３ ）ＦＩ ：在该任务中三兀组的 ＦＩ是指精确率以及召回率的调和平均值 ， 以综  

合反映三元组的抽取性能 ， 是这三者中最重要的指标 。 Ｆ１表示如下 ：  

３４  

第三章基于掩码上下文机器阅读理解框架的方面级三元组抽取方法  

＝ （３ － １５）  

Ｒ  

３ ．３ ．４基线方法  

本节选取了近年来的方面级三元组抽取模型进行定量的性能对比 ，这些模型  

可划分为三种类型 ： 阶段式 （Ｐｉｐｅｌｉｎｅ） 、端到端式 （Ｅｎｄ － ｔｏ －ｅｎｄ） 、机器阅读理解  

式 （ＭＲＣ

－ｂａｓｅｄ） 。阶段式模型有ＣＭＬＡ＋、 ＲＩＮＡＮＴＥ＋、 Ｌｉ

－ｕｎｉｆｉｅｄ －Ｒ、 Ｐｅｎｇ

ｓｔａｇｅ、 Ｐｅｎｇ

－ｓｔａｇｅ＋ＩＯＧ以及ＩＭＮ＋ＩＯＧ ； 端到端式模型有 ０丁￡以１１＾、正１

ＢＥＲＴ、 ＧＴＳ、 Ｓ ３Ｅ ２

、 Ｕｎｉｆｉｅｄ、 ＳＰＡＮ －ＡＳＴＥ以及ＥＭＣ －ＧＣＮ ； 机器阅读理解式模  

型有ＢＭＲＣ 。 与上述研宄保持

一致 ， 。  

简要所对比的模型描述如下 ：  

１ ）ＣＭＬＡ＋ ［４６］模型 ： 该模型是ＣＭＬＡ的修改版本 ， 其利用多层注意力网络  

来提取方面词 、 意见词 ， 并且模块之间进行交互式学习 ， 是

一种端到端的框架 。  

２）ＲＩＮＡＮＴＥ＋ ｆ４６

：！模型 ： 该模型是ＲＩＮＡＮＴＥ的修改版本 ， 其利用

一种基于  

依赖解析结果自动从现有训练样例中挖掘提取规则的算法来标记大量辅助数据 ，  

然后利用ＢｉＬＳＴＭ和ＣＲＦ构建模型 。  

３ ）Ｌｉ －ｕｎｉｆ ｉｅｄ －Ｒ

［４６］模型 ： 该模型是

一种两层式循环神经网络 ， 上层神经网络  

一的标签 ，下层神经网络用来辅助目标边界预测 ，并利用门控机制保  

持方面词的情感

一致性 。  

４）Ｐｅｎｇ

－ｓｔａｇ＃ ６』模型 ：该模型是

一种两阶段式框架 ， 第

一阶段预测带有  

情感极性的方面词以及意见词 ， 第二阶段将两者配对构成最后的三元组 。  

５ ） Ｐｅｎｇ

－ｔｗｏ －ｓｔａｇｅ＋ＩＯＧ ［４８＾型 ： 该模型在Ｐｅｎｇ

－ｔｗｏ －ｓｔａｇｅ的基础上 ，将语义  

表示基底模型更换为外部 ＬＳＴＭ＋内部 ＬＳＴＭ＋全局 ＬＳＴＭ的模式 ， 综合考虑全  

局信息 。  

６）ＩＭＮ＋ＩＯＧＭｌ是将 ＩＭＮ ［９ １１与 ＩＯＧ结合的模型 ，其先用 ＩＭＮ去得到评论语  

句中的所有方面词及其对应的情感 ， 然后利用 ＩＯＧ去输出最后所有的三元组 。  

７）ＧＴＳＭ主要是利用二维网格标注方法来端到端的抽取方面情感三元组 。  

８）ＯＴＥ －ＭＴＬ

［４９ ］是 一种基于双仿射评分器的多任务学习框架 ， 以缓和之前方  

法在提取方面

－意见词对时缺乏情感极性为参照的问题 ，其可以联合抽取方面词 、  

意见词以及情感极性 。  

９）ＪＥＴ －ＢＥＲＴ ［４？１提出具有位置感知标记方案的端到端模型以联合提取三元组 ，  

其中具有位置感知的标记方案是

一种融入位置信息的序列标注方法 。  

３５  

北京邮电大学电子信息硕士学位论文  

３Ｅ ２ ［５ｆ ｌ］与ＧＴＳ相近 ， 同样利用二维网格标注的模式来端到端的抽取三  

元组 ， 其还利用图神经网络编码词与词之间的语法或语义关系 。  

１２）ＢＭＲＣ ［５３

一种机器阅读理解方法 ，其包括两种顺序的提问 ，先询问方  

面词再询问意见词 ，或先询问意见词再询问方面词 ，综合考虑两个方向的询问结  

果最后生成三元组 。  

１３ ）Ｕｎｉｆ ｉｅｄ ［５１＾ＡＳＴＥ任务转化为生成式任务 ， 并利用ＢＡＲＴ生成式模型  

作为基底模型进行实验 。  

１４）ＳＰＡＮ －ＡＳＴＥ ［９２３是

一种基于 ｓｐａｎ标注的方法 ， 其会枚举所有的单词作为  

开始或结束位置形成 ｓｐａｎ ， 然后对 ｓｐａｎ进行预测与配对 。  

１５ ）ＥＭＣ －ＧＣＮ ［９３

一种加强多通道的图神经网络 ， 其首先设计

一种二维的  

网格标注方案 ，然后利用该图神经网络对词与词之间的关系进行预测 ，并利用到  

词性、 语法等语言学信息进行特征加强 。  

３ ．３ ．５实验结果  

表 ３ －３和表 ３４展示了在仏数据集上的评测结果 ，表 ３ －５和表 ３ －６展示了在  

？２数据集上的评测结果 。  

表 ３ －３ 在 ＾＾数据集中Ｒｅｓｔ１４以及Ｌａｐ ｌ４的评测结果对比  

Ｒｅｓｔ１４Ｌａｐ１４   Ｍｏｄｅｌ  

ＰＲＦＩＰＲＦＩ  

Ｌｉ －ｕｎｉｆｉｅｄ －Ｒ４１ ．４４６８ ．７９５１ ．６８４２ ．２５４２ ．７８４２ ．４７  

－ｔｗｏ －ｓｔａｇｅ４４ ．１８６２ ．９９５１ ．８９４０ ．４０４７ ．２４４３ ．５０  

－ｔｗｏ －ｓｔａｇｅ＋ＩＯＧ５８ ．８９６０ ．４１５９ ．６４４８ ．６２４５ ．５２４７ ．０２  

ＩＭＮ＋ＩＯＧ５９ ．５７６３ ．８８６１ ．６５４９ ．２１４６．２３４７ ．６８  

Ｓ ３Ｅ ２６９ ．０８６４ ．５５６６ ．７４５９ ．４３４６ ．２３５２ ．０１  

ＧＴＳ －ＢｉＬＳＴＭ６７ ．２８６１ ．９１６４ ．４９５９ ．４２４５ ．１３５１ ．３０  

ＧＴＳ －ＣＮＮ７０ ．７９６１ ．７１６５ ．９４５５ ．９３４７ ．５２５１ ．３８  

ＧＴＳ －ＢＥＲＴ７０ ．９２６９ ．４９７０ ．２０５７ ．５２５ １ ．９２５４ ．５８  

ＢＭＲＣ －－７０ ．０１ －－５７ ．８３  

ＥＭＣ －ＧＣＮ ７１ ．８５７２ ． １２７１ ．９８６１ ．４６５５ ．５６５８ ．３２  

ＯｕｒＣＱＭ －ＭＲＣ ７６ ．４５６９ ．６７７２．８９６４ ．７３５６ ．０９６０．０９  

３６  

第三章基于掩码上下文机器阅读理解框架的方面级三元组抽取方法  

表 ３ －４在 ；Ｐｉ数据集中Ｒｅｓｔ１５以及Ｒｅｓｔ１６的评测结果对比  

Ｒｅｓｔ１５Ｒｅｓｔ１６   Ｍｏｄｅｌ  

ＰＲＦＩＰＲＦＩ  

Ｌｉ －ｕｎｉｆ ｉｅｄ －Ｒ４３ ．３４５０ ．７３４６ ．６９３８ ．１９５３ ．４７４４ ．５１  

－ｓｔａｇｅ４０．９７５４ ．６８４６ ．７９４６ ．７６６２．９７５３ ．６２  

－ｔｗｏ －ｓｔａｇｅ＋ＩＯＧ５１ ．７０４６ ．０４４８ ．７１５９ ．２５５８ ．０９５８ ．６７  

ＩＭＮ＋ＩＯＧ５５ ．２４５２ ．３３５３ ．７５ －－－  

Ｓ ３Ｅ ２６１ ．０６５６．４４５８ ．６６７１ ．０８６３ ．１３６６．８７  

ＧＴＳ －ＢｉＬＳＴＭ６３ ．２６５０．７１５６ ．２９６６ ．０７６５ ．０５６５ ．５６  

ＧＴＳ －ＣＮＮ６０．６９５３ ．５７５６．６４６２ ．６３６６ ．９８６４ ．７３  

ＧＴＳ －ＢＥＲＴ５９ ．２９５８ ．０７５８ ．６７６８ ．５８６６ ．６０６７ ．５８  

ＢＭＲＣ －？５８ ．７４ －－６７ ．４９  

ＥＭＣ －ＧＣＮ ５９ ．８９６１ ．０５６０ ．３８６５ ．０８７１ ．６６６８ ．１８  

ＯｕｒＣＯＭ －ＭＲＣ６８ ．５０５９．７４６３．６５７２ ．８０７０ ．８５７１．７９  

表 ３ －５在 数据集中Ｒｅｓｔ１４以及Ｌａｐ ｌ４的评测结果对比  

Ｒｅｓｔ１４Ｌａｐ１４   Ｍｏｄｅｌ   ＰＲＦＩＰＲＦＩ  

ＣＭＬＡ＋３９ ．１８４７ ．１３４２ ．７９３０ ．０９３６ ．９２３３ ．１６  

ＲＩＮＡＮＴＥ＋３ １ ．４２３９ ．３８３４ ．９５２１ ．７１１８ ．６６２０ ．０７  

－ｕｎｉｆｉｅｄ －Ｒ４１ ．０４６７ ．３５５１ ．００４０ ．５６４４ ．２８４２ ．３４  

－ｔｗｏ －ｓｔａｇｅ４３ ．２４６３ ．６６５１ ．４６３７ ．３８５０ ．３８４２ ．８７  

ＯＴＥ －ＭＴＬ６２ ．００５５ ．９７５８ ．７１４９ ．５３３９ ．２２４３ ．４２  

ＪＥＴ －ＢＥＲＴ７０ ．５６５５ ．９４６２ ．４０５５ ．３９４７ ．３３５１ ．０４  

ＧＴＳ －ＢＥＲＴ６８ ．０９６９ ．５４６８ ．８１５９ ．４０５ １ ．９４５５ ．４２  

Ｕｎｉｆｉｅｄ６５ ．５２６４ ．９９６５ ．２５６１ ．４１５６，１９５８ ．６９  

ＢＭＲＣ７５ ．６１６１ ．７７６７ ，９９７０ ．５５４８ ．９８５７ ．８２  

ＳＰＡＮ －ＡＳＴＥ７２ ．８９７０ ．８９７１ ．８５６３ ．４４５５ ．８４５９ ．３８  

－ＧＣＮ ７１ ．２１７２ ．３９７１ ．７８６１ ．７０５６ ．２６５８ ．８ １  

ＯｕｒＣＯＭ －ＭＲＣ ７５ ．４６６８ ．９１７２．０１６２ ．３５５８ ．１６６０．１７  

３７  

北京邮电大学电子信息硕士学位论文  

表 ３ －６在 ２）２数据集中Ｒｅｓｔ１５以及Ｒｅｓｔ１６的评測结果对比  

Ｒｅｓｔ１５Ｒｅｓｔ１６   Ｍｏｄｅｌ   ＰＲＦＩＰＲＦＩ  

ＣＭＬＡ＋３４ ．５６３９ ．８４３７ ．０１４１ ．３４４２ ． １０４１ ．７２  

ＲＩＮＡＮＴＥ＋２９ ．８８３０ ．０６２９ ．９７２５ ．６８２２ ．３０２３ ．８７  

Ｌｉ －ｕｎｉｆｉｅｄ －Ｒ４４ ．７２５１ ．３９４７．８２３７．３３５４ ．５１４４ ．３ １  

－ｔｗｏ －ｓｔａｇｅ４８ ．０７５７ ．５ １５２ ．３２４６ ．９６６４ ．２４５４ ．２１  

ＯＴＥ －ＭＴＬ５６ ．３７４０ ．９４４７． １３６２．８８５２ ．１０５９ ．９６  

ＪＥＴ －ＢＥＲＴ６４ ．４５５１ ．９６５７ ．５３７０ ．４２５８ ．３７６３ ．８３  

－ＢＥＲＴ５９ ．２８５７ ．９３５８ ．６０６８ ．３２６６ ．８６６７．５８  

Ｕｎｉｆ ｉｅｄ５９ ．１４５９ ．３８５９ ．２６６６ ．６０６８ ．６８６７ ．６２  

ＢＭＲＣ６８ ．５ １５３ ．４０６０ ．０２７１ ．２０６１ ．０８６５ ．７５  

ＳＰＡＮ －ＡＳＴＥ６２ ．１８６４ ．４５６３ ．２７６９ ．４５７１ ． １７７０ ．２６  

ＥＭＣ －ＧＣＮ ６１ ．５４６２ ．４７６１ ．９３６５ ．６２７１ ．３０６８ ．３３  

ＯｕｒＣＯＭ －ＭＲＣ ６８ ．３５６１ ．２４６４，５３７１ ．５５７１ ．５９７１ ．５７  

可以看到在 ：０１和 ：０２数据集 ，基于ＦＩ指标 ，本文的ＣＯＭ

－ＭＲＣ全面超越了  

所有阶段式、端到端式以及ＭＲＣ式的方法 。值得注意的是在 ：０２数据集上 ， 本  

文框架同样超越了现有最好的端到端式方法 ＳＰＡＮ

－ＡＳＴＥ。 可以看出相对于阶段  

式方法 ， 端到端式方法和ＭＲＣ式方法是更具竞争力的 ， 这是因为他们减缓了错  

误传播问题并且构建了子任务之间的关联性 。更进

一步来说 ，对比另外

一个强有  

力的ＭＲＣ式方法 ， 即ＢＭＲＣ ， 本文所提出的ＣＯＭ －ＭＲＣ方法在仏和＾数据  

集上分别超越了其 ３ ．５９％以及 ４ ． １８％ 的 Ｆ１ 指标。 该提升归功于本文的 ＣＯＭ －  

ＭＲＣ可以通过上下文增强策略、判别式模型以及推理方法有效地缓解千扰问题 。  

此外 ， 为了体现本研究实验结果的可信性 ， 本文针对Ｆ １指标做了ｔ检验 ， 该检  

验是在％和２）２数据集上对比ＣＯＭ －ＭＲＣ于ＢＭＲＣ以及ＥＭＣ

－ＧＣＮ而完成 ，所  

有的ｐ值均小于 ０ ．０５ ， 证明了对应实验结果具有统计学显著性 。  

３ Ｊ ．６实验分析  

为了进 一步验证 ＣＯＭ －ＭＲＣ框架的有效性 ， 本节分别从上下文增强策略、  

判别式模型 、推理算法、查询的设置 、注意力可视化和案例研究进行了定性或定  

量分析 。  

３８  

第三章基于掩码上下文机器阅读理解框架的方面级三元组抽取方法  

３ ．３ ．６．１对上下文增强策略的定量分析  

本节将应用于 ＣＯＭ －ＭＲＣ的指数级数据增强策略与另外两个策略 ， 即线性  

级策略和空策略进行了对比 。 首先介绍这三种策略 ：  

表 ３ －７在 ２）２数据集中采用不同数据增强方法的评测结果对比  

Ｒｅｓｔ１４Ｌａｐ１４Ｒｅｓｔ１５Ｒｅｓｔ１６  

Ｓｔｒａｔ ｅｇｙ  

ＦＩ＃ＦＩ＃ＦＩ＃ＦＩ＃  

Ｅｘｐ７２．０１５２５８６２．１７３０２２６４．５３２０４４７１．５７２７４６  

Ｌｉｎｅａｒ６９ ．０８４１０２５６ ．５２２５６２６２ ．６５１７２４６９ ．３７２３９６  

ＮＯＰ５０ ．４９１２６６５０ ．４０９０６５１ ．２５６０５５５ ．９５８５７  

１）指数级策略 （Ｅｘｐ ） ：对于

一句包含 ｔ个方面词的输入 ，指数级数据增强策  

略的思路是将每个方面词都进行掩码或不掩码两种操作 ， 由此会生成２ １个样本。  

该策略应用于ＣＯＭ －ＭＲＣ框架中 。  

２）线性级策略 （Ｌｉｎｅａｒ） ：对于 一句包含 ｔ个方面词的输入 ，线性级策略仅考  

虑应用于推理阶段的样本 。推理阶段的方面词推理阶段和方面词附属物推理阶段  

分别包含 ｔ以及 ｔ＋１个样本 ， 两阶段会有

一个样本 （掩码除了最后

一个方面词  

其他所有方面词）完全相同 ， 故会生成２ｔ个样本 。  

３）空策略 （ＮＯＰ） ．

一句输入 ， 空策略不进行任何数据增强 ， 直接将该  

输入作为训练样本 ， 由此只有 １个样本。  

在 ＣＯＭ －ＭＲＣ框架中的模型与推理方法不变的情况下 ， 分别应用以上三种  

上下文增强策略进行对比实验 ， 指标Ｆ１结果以及样本数量如表 ３ ＿７所示。 可以  

看出对比线性级策略以及空策略 ，指数级策略实现了显著的性能提升 。总的来说 ，  

样本数量越多 ，模型表现越好 ，但指数级策略对比空策略平均增加约 ２ ．５倍的样  

本量 ， 并不会带来过多的计算负担 。  

此外还进行了在单方面词 （Ｓｉｎｇ ｌｅ －Ａｓｐｅｃｔ ，ＳＡ）以及多方面词 （Ｍｕｌｔ ｉ －Ａｓｐｅｃｔ ，  

ＭＡ）两个条件下的实验 ， 结果如表 ３ －８所示 。 可以看出对比其他策略 ， 指数级  

策略在多方面词设定下获得的增益明显高于单方面词下的增益 。总的来说 ，应用  

于ＣＯＭ －ＭＲＣ的指数级增强策略是很有效的 。  

３９  

北京邮电大学电子信息硕士学位论文  

表 ３ －８在ｇ２数据集中采用不同数据增强方法的在单／多方面词下的评测结果对比  

一Ｒｅｓｔ１４Ｌａｐ１４Ｒｅｓｔ１５Ｒｅｓｔ１６  

Ｓｔｒａｔｅｇｙ  

 ＳＡＭＡＳＡＭＡＳＡＭＡＳＡＭＡ  

Ｅｘｐ７３．４８７１．３１６０．４４５９．８６６４．８３６４．０９６９．８６７３．６６  

Ｌｉｎｅａｒ７１ ．３８６７ ．４８５７ ．０５５６ ．０２６４ ．２９６０ ．５１６８ ．３８７０ ．５４  

ＮＯＰ６４ ．３９４０ ．８０５６．６４４２ ．２１５９．１８３７ ．２１６２．８０４４ ．６９  

３ ．３ ．６ ．２对判别式模型的定量分析  

为验证判别式模型各模块的有效性 ， 本研宄基于Ｄ２数据集对各模块进行了  

消融研究 。 结果如表 ３ －９所示。  

－９在Ｄ２数据集上的消融实验结果  

ＭｏｄｅｌＣＯＭ －ＭＲＣ ６７．０７  

ｗ／ｏＡｓｐｅｃｔＲｅｐｒｅｓｅｎｔａｔｉｏｎ６６ ． １０（０ ．９７ｉ）  

ｗ／ｏＯｐ ｉｎｉｏｎＲｅｐｒｅｓｅｎｔａｔｉｏｎ６６ ． １６ （０ ．９ Ｕ）   Ｍｏｄｕｌｅ   ｗ／ｏＥｘｉｓｔｅｎｃｅＣｏｎｃａｔｅｎａｔｉｏｎ６６ ．６２（０ ．４５ｉ）  

ｗ／ｏＳｅｎｔｉｍｅｎｔＡｔｔｅｎｔ ｉｏｎ６５ ．５７ （ １ ．５０１）  

对于方面词和意见词提取模块来说 ，去除方面词或意见词的表示会带来

一些  

性能的下降 ； 对于方面词探测模块来说 ， 去除公式３ － ８中的拼接操作而仅仅使  

用 ［ＣＬＳ］的表示 会带来较小的影响 ；对于情感分类模块来说 ，去除公式３ － ８  

的注意力模块而仅仅使用上下文表示心有较大的影响 ，这会导致平均 １ ．５％的Ｆ１  

指标的下降 。这展现了本文所提出的注意力机制可以非常有效地融合方面词和意  

见词信息 ，其他模块中方面词的表示、意见词的表示以及拼接操作融合信息对于  

模型的性能提升都是有效的 。总的来说 ，所有模块对于该模型在ＡＳＴＥ任务的性  

能表现均有贡献。  

３ ．３ ．６ ．３对推理算法的分析  

为验证 ＣＯＭ －ＭＲＣ框架中推理方法的有效性 ， 本文展现了两种不同的推理  

方法 （如图 ３ －３所示 ） ， 这两种推理方法包含了相同的方面词推理阶段和不同的  

方面词附属物推理阶段 ， 并将这两种不同的方面词附属物推理阶段命名为ＡＡＩ １  

和ＡＡＩ ２ ， 详细描述如下 ：  

４０  

第三章基于掩码上下文机器阅读理解框架的方面级三元组抽取方法  

ＡＡＩ１ＡＡ Ｉ ２  

Ｉｆ， —

￣ Ｎｉｃｅａｍｂｉｅｎｃｅ， ’ ： 一；

￣ ＇Ｎｉｃｅａｍｂｉｅｎｃｅ ，（   ＱＣ１ｂｕｔｈｉｇｈｌｙｏｖｅｒｒａｔｅｄ ｐ ｌａｃｅ ．ＱＣ３ｂｕｔｈｉｇｈｌｙｏｖｅｒｒａｔｅｄＪ  

ａｂｏｉｒ ｔ

１＾Ａ ＰＯＳ Ｌ〇！＾ｍａｉｅｄ ｜ａｂｏｕｔ＾立ＰＯＳ！Ｎｉｃｅ！  

“ ａｍｂｉｅｎｃｅ

＂ ＂＂ ａｍｂｉｅｎｃｅ

＂  Ｊ  

— ． 、 ，？ ％Ｎｉｃｅ ，厂

广二 Ｎｉｃｅ ，  

ＱＣ２ｂｕｔｈｉｇｈｌｙｏｖｅｒｒａｔｅｄ ｐ ｌａｃｅ．ＱＣ２ｂｎｔｈｉｇｈｌｙｏｖｅｒｒａｔｅｄ ｐ ｌａｃｅ，  

ａｂｏｕｔ ＾ＳＮＥＧ

［０ｏｖｅｒｒａｔｅｄａｂｏｕｔ ＼＾ＳＮＥＧ０ｏｖｅｒｒａｔｅｄ  

＂ ｐ ｌａｃｅ

＂Ｌ ＂

＂： ＝   

Ｔｒｉｐ ｌｅｔｓ｛ａｍｂｉｅｎｃｅ ，Ｎｉｃｅ ，ＰＯＳ｝ Ｔｒｉｐ ｌｅｔｓ  

ｔ ＿■麵 丨 丨 ■ ■ ■ ■■ 丨 — ■ 丨 丨 丨咖 ■ 丨圆

｛ａｍＤｉｅｉｌＣｅ ，ＮｌＣＣ ，Ｐ〇Ｓ｝  

Ｊ｛ａｍｂｉｅｎｃｅ ，ｏｖｅｒｒａｔｅｄ

，ＰＯＳ｝ｘ

｜  

ａｎｍ＜ｗｇ？ａ ａｒ＿ｎｍ ｉｍｍｆＢ ｉＨａｍ？ｗＭｗ？Ｂｓｍｆ … Ｔｍｍ而ｎｕＭｇＢｍｒＷＴｒ

＊＾ Ｊ ｌ  

｛ｐ ｌａｃｅ ，ｏｖｅｒｒａｔｅｄ ，ＮＥＧ ｝

， ，  

－３ ：不同推理方法对比  

１ ）ＡＡＩ １ ： 这是

一种朴素的方面词附属物推理算法 。 固定的查询设置为找到  

最左侧的方面词 ，然后为得到方面词对应的意见词和情感 ，该算法仅从左到右逐  

步掩码最左侧的方面词作为掩码上下文 ， 由此获得各个方面词的附属物 。然而当  

前的方面词处理阶段会受到后续方面词的干扰 。如图 ３

－２所示 ，方面词

“ ａｍｂｉｅｎｃｅ ”  

ｐ ｌａｃｅ

” 的千扰 ， 由此

“ ｏｖｅｒｒａｔｅｄ

” 会被错误地识别为

“ ａｍｂｉｅｎｃｅ

” 的意  

见词 。  

２）ＡＡＩ２ ： 这是应用于ＣＯＭ －ＭＲＣ的方面词附属物推理算法 。 固定的查询同  

样设置为找到最左侧的方面词 ，然后为得到各个方面词对应的意见词和情感 ，该  

算法会将除了该方面词的其他所有方面词进行掩码 ，从而得到各个方面词的附属  

物 。  

基于 ＣＯＭ －ＭＲＣ框架 ， 本文应用相同的模型以及指数级掩码上下文增强策  

略 ， 然后应用不同的推理算法并在？ ２数据集上进行实验对比探究 ， 结果如表 ３ －  

１０所示 。  

－ １０在 ：０ ２数据集中采用不同推理方法的评测结果对比  

ＭｏｄｅＩｎｆｅｒｅｎｃｅＲｅｓｔ１４Ｌａｐ１４Ｒｅｓｔ１５Ｒｅｓｔ１６  

ＳＡＡＡＩ１／２７３ ．４８６０ ．４４６４ ．８３６９ ．８６  

ＡＡＩ１６８ ．８０５７ ． １３６０ ．３２７ １ ．５０  

ＭＡＡＡＩ２７１ ．３１５９ ．８６６４ ．０９７３ ．６６  

Ａ  ＋２ ．５ １＋２ ．７３＋３ ．７７＋２ ． １６  

４ １  

北京邮电大学电子信息硕士学位论文  

在基于模型判定的单方面词 （Ｓｉｎｇ ｌｅ －Ａｓｐｅｃｔ ， ＳＡ）模式下 ，算法ＡＡＩ １和ＡＡＩ  

２具有相同的表现 ，这是因为在单方面词设定下 ， 两个算法完全

一致 ； 而在多方  

面词 （Ｍｕｌｔ ｉ －Ａｓｐｅｃｔ ，ＭＡ）模式下 ，算法ＡＡＩ２对于ＡＡＩ１具有明显的性能提升 ，  

最大可在Ｒｅｓｔ１５数据集上提升 ３ ．７７％ 。 这表明在推理阶段其他方面词会造成较  

大的干扰 ，掩码其他方面词会有效地缓解这种干扰 。需要注意的是为何不直接掩  

码意见词的原因是

一个意见词可能会匹配多个方面词 ，因此掩码

一个意见词可能  

会影响其他方面词的意见词判定过程 。总的来说 ，实验结果表明了应用于ＣＯＭ －  

ＭＲＣ的方面词附属物推理方法 ， 即ＡＡＩ２的有效性 。  

３ ．３ ．６ ．４对查询的定量分析  

－ １ １ 在 ；０２数据集中采用不同查询的评测结果对比  

ＱｕｅｒｙＲｅｓｔ１４Ｌａｐ１４Ｒｅｓｔ１５Ｒｅｓｔ１６  

ＲｅｇｕｌａｒＱｕｅｒｙ

： “ Ｆｉｎｄｔｈｅｆｉｒｓｔ  

ａｓｐｅｃｔｔｅｒｍａｎｄｃｏｒｒｅｓｐｏｎｄｉｎｇ７２．０１６０．１７６４．５３７１．５７  

ｏｐ ｉｎｉｏｎｔｅｒｍｓｉｎｔｈｅｔｅｘｔ ”  

ＩｍｐｒｏｐｅｒＱｕｅｒｙ ： “ Ｆｉｎｄｔｈｅａｓｐｅｃｔ７０ ．４４５９ ．４４６２ ．９７７０ ．４０  

ｔｅｒｍａｎｄｃｏｒｒｅｓｐｏｎｄｉｎｇｏｐｉｎｉｏｎ  

－１ ．５７）（

－０ ．７３ ）（

－ １ ．５６）（

－１ ， １７）  

ｔｅｒｍｓｉｎｔｈｅｔｅｘｔ ”  

７０ ．２３５７ ．８６６０ ．７８６９ ．０２  

Ｎｕｌｌ  

－１ ．７８）（

－２ ．３１ ）（

－３ ．７５）（

－２ ．５５）  

为验证 ＣＯＭ －ＭＲＣ设置的固定的查询的有效性 ， 本文构造了其他三种类型  

一作对比 ， 这三种查询如下所述 ：  

１ ）Ｒｅｇｕｌａｒ Ｑｕｅｒｙ ：

＇＇ Ｆｉｎｄｔｈｅｆｉｒｓｔａｓｐｅｃｔｔｅｒｍａｎｄｃｏｒｒｅｓｐｏｎｄｉｎｇｏｐ ｉｎｉｏｎｔｅｒｍｓｉｎ  

ｔｈｅｔｅｘｔ ” 是应用在ＣＯＭ －ＭＲＣ 内的查询语句 ， 该查询旨在引导模型找至

ｌ ｊ文本中  

出现的首个方面词和对应的意见词 ， 与模型本身的目标

一致 。  

２ ） Ｉｍｐｒｏｐｅｒ Ｑｕｅｒｙ ：

（（ Ｆｉｎｄｔｈｅａｓｐｅｃｔｔｅｒｍａｎｄｃｏｒｒｅｓｐｏｎｄｉｎｇｏｐ ｉｎｉｏｎｔｅｒｍｓｉｎｔｈｅ  

一个不合适的语句 ， 相比前者少了关键词

“ ｆ ｉｒｓｔ ”

， 与模型本身的目标不  

一致。  

３）Ｎｕｌｌ ： 代表不设置任何查询 。  

４２  

第三章基于掩码上下文机器阅读理解框架的方面级三元组抽取方法  

实验结果如表 ３ － １ １所示 ， 相对于ＲｅｇｕｌａｒＱｕｅｒｙ

， 设置 ＩｍｐｒｏｐｅｒＱｕｅｒｙ会导  

致Ｆ １指标平均在每个数据集上下降 １ ．２６％ ，而Ｎｕｌｌ会让模型性能下降更为明显 ，  

平均下降 ２ ．６％ 。 这体现了本研究所设置的 ｑｕｅｒｙ的有效性 。  

３Ｊ ．６ ．５注意力可视化  

为展现 ＣＯＭ －ＭＲＣ框架可以有效地缓解千扰问题 ， 本文可视化了注意力矩  

阵 。在固定的查询以及掩码上下文作为输入的设置下 ，该注意力矩阵蕴含着首个  

方面词对应的意见词信息 。  

ｉｈｈｈｌ＾ｓ  

ｆｏｏｄ｜  

Ｓ ｒｃａ ｔ ｏｒｃａ ｔｇ  

ｃｕ ｓ ｔｏｍｅｒ  

－４ ： 是否掩码除首个方面词以外其他方面词后的注意力分布示例  

＾＾＾＾＾＾＾＾Ｖｆ ｃ＾一＃一户

？ 、？＃＃  

ｄｅｃｏｒ  

ｃｕ ｓＫ ｉｍｃｒｌ  

ｂａｄ ｂａｄ  

ｍａｎａｇｅｒ

■  

－５ ： 是否掩码其他方面词后的注意力分布示例  

－４和图 ３

－５所示 ，考虑包含两个对立极性的句子 ：

“ ｇｏｏｄｆｏｏｄ ，ｂａｄｄｅｃｏｒ ，  

ｇｒｅａｔｃｕｓｔｏｍｅｒｓｅｒｖｉｃｅ ，ｂａｄｍａｎａｇｅｒ

， 如图３ －４所示 ， 为鉴别

“ ｆｏｏｄ

” 的意见词 ，  

４３  

北京邮电大学电子信息硕士学位论文  

两个子图都展示出主要的注意力都聚集在正确的意见词

ｇｏｏｄ ” 上 ，然而 ， 左侧  

的子图表明仍有不可忽视的注意力在错误的意见词 ， 尤其是

” 上 。 相对比  

之下 ，如右侧子图所示 ， 当其他所有方面词被掩码后 ，对于错误意见词的注意力  

被显著降低 。类似地 ， 如图 ３ ＿５所示 ， 为鉴别

“ ｄｅｃｏｒ

” 的意见词 ， 两个子图都展  

示出主要的注意力都聚集在正确的意见词

” 上 ， 然而 ， 左侧的子图表明仍  

有不可忽视的注意力在错误的意见词 ，尤其是

“ ｇｒｅａｔ ” 上 ， 而当其他所有方面词  

被掩码后 ， 对于错误意见词的注意力被显著降低 。  

３ ．３ ．６．６案例研究  

－ １２所示 ， 本文展示了

一下包含多个反面词的案例以与当前最先进的  

机器阅读理解方法ＢＭＲＣ作对比 。对于来自于１ ？ ２的Ｒｅｓｔ ｌ６数据集的例子 “ Ｎｉｃｅ  

ａｍｂｉｅｎｃｅ ，ｂｕｔｈｉｇｈｌｙｏｖｅｒｒａｔｅｄｐ ｌａｃｅ

， 两个模型都正确的预测出了方面词  

“ ａｍｂｉｅｎｃｅ ”

ｐ ｌａｃｅ

” 以及它们分别对应的意见词

“ Ｎｉｃｅ

“ ｏｖｅｒｒａｔｅｄ ”

。 然而 ，  

ＢＭＲＣ未能正确地鉴别

“ ｐ ｌａｃｅ

” 的情感极性 ，这可能是因为

“ ａｍｂｉｅｎｃｅ ” 的情感  

极性为积极的 ， 从而影响了模型对

“ ｐ ｌａｃｅ

” 情感极性的判断 。对于来自于１） ２的  

Ｌａｐ １４数据集的例子

ｐｒ ｉｃｅｆｒｅｅｓｈｉｐｐ ｉｎｇｗｈａｔｅｌｓｅｃａｎｉａｓｋｆｏｒ ！ ！ ”

， ＢＭＲＣ提  

一个错误的三元组 （ｓｈｉｐＰ ｉｎｇ，ｇｒｅａｔ ，ＰＯＳ） ， 这可能是因为另外

一个方面词  

“ ｐｒ ｉｃｅ ” 的意见词是

“ ｇｒｅａｔ ”

， 模型错误受到了这个方面词的干扰 。  

表 ３ － １２案例探究  

ＳｅｎｔｅｎｃｅＧｒｏｕｎｄＴｒｕｔｈＢＭＲＣ ＯｕｒＣＯＭ －ＭＲＣ  

Ｎｉｃｅａｍｂｉｅｎｃｅ ，ｂｕｔ（ａｍｂｉｅｎｃｅ ，Ｎｉｃｅ，（ａｍｂｉｅｎｃｅ，Ｎｉｃｅ，（ａｍｂｉｅｎｃｅ ，Ｎｉｃｅ ，  

ｈｉｇｈｌｙｏｖｅｒｒａｔｅｄＰＯＳ）ＰＯＳ）ＰＯＳ）  

ｐ ｌａｃｅ ．（ＦｒｏｍＲｅｓｔ１６（ｐ ｌａｃｅ ，ｏｖｅｒｒａｔｅｄ ，（ｐ ｌａｃｅ ，ｏｖｅｒｒａｔｅｄ ，（ｐ ｌａｃｅ ，ｏｖｅｒｒａｔｅｄ ，  

ｏｆＤ２ ）ＮＥＧ）ＰＯＳ）ＸＮＥＧ）  

ｇｒｅａｔ ｐｒｉｃｅｆｒｅｅ（ｐｒｉｃｅ ，ｇｒｅａｔ ，（ｐｒ ｉｃｅ ，ｇｒｅａｔ ，（ｐｒｉｃｅ，ｇｒｅａｔ ，  

ｓｈｉｐｐ ｉｎｇｗｈａｔｅｌｓｅＰＯＳ）ＰＯＳ）ＰＯＳ）  

ｃａｎｉａｓｋｆｏｒ ！ ！（Ｆｒｏｍ（ｓｈｉｐｐ ｉｎｇ，ｆ ｒｅｅ，（ｓｈｉｐｐ ｉｎｇ，ｆ ｒｅｅ ，（ｓｈｉｐｐ ｉｎｇ，ｆ ｒｅｅ ，  

Ｌａｐ １４ｏｆＤ２）ＰＯＳ）ＰＯＳ）ＰＯＳ）  

（ｓｈｉｐｐ ｉｎｇ，ｇｒｅａｔ ，  

 ＰＯＳ）ｘ  

４４  

第三章基于掩码上下文机器阅读理解框架的方面级三元组抽取方法  

３ ．４本章小结  

本章开展了基于掩码上下文机器阅读理解框架的方面级三元组抽取方法的  

研宄工作 。本章首先提出了基于掩码上下文机器阅读理解框架并作详细介绍 ，该  

框架包含三部分 ，分别为掩码式数据增强、交互式判别模型以及阶段式推理方法 。  

随后 ，本章介绍了实验的基准数据集、实验参数与设置 、评估指标、基线方法和  

实验结果 ，验证了方法的可行性以及有效性 。最后 ，本章分别对上下文增强方法、  

判别式模型模块、推理算法以及查询进行定量分析 ，并且进行了注意力可视化和  

案例研宄 ，从多方面多角度验证了方法里各部分的有效性 ，也证明了其他方面词  

会带来干扰问题 ，而本章提出的方法有效地缓解了该问题 ，进

一步验证了方法的  

完备性以及可行性 。  

４５  

北京邮电大学电子信息硕士学位论文  

第四章基于统 一表格填充机制端到端框架的结构化情感分  

析方法  

当前主流的结构化情感分析 （ＳｔｒｕｃｔｕｒｅｄＳｅｎｔｉｍｅｎｔ Ａｎａｌｙｓｉｓ ， ＳＳＡ）方法是将  

该任务视为双词汇依赖解析问题 ，然而这些方法均不能同时处理重叠和非连续问  

题 ，而这两个问题的同时存在也是该任务的主要难点 。本文提出了

一种针对性的  

解决方法 ，首先构造

一种新颖的双词汇依赖解析图 ，该图包含两类有向边 ，即

“ 关  

（ＲｅｌａｔｉｏｎＰｒｅｄｉｃｔ ｉｏｎ， ＲＰ ） 以及

“ 单词提取

（ＴｏｋｅｎＥｘｔｒａｃｔ ｉｏｎ ， ＴＥ） ，  

它们分别对应解决了重叠和非连续问题 。然后 ，将这种双词汇依赖解析图转化为  

一的表格填充机制 ， 将其命名为ＵＳＳＡ（Ｕｎｉｆ ｉｅｄＴａｂｌｅＦｉｌｌｉｎｇＳｃｈｅｍｅｆｏｒ  

ＳｔｒｕｃｔｕｒｅｄＳｅｎｔｉｍｅｎｔＡｎａｌｙｓｉｓ） 。在该表格填充机制里 ， ＲＰ与ＴＥ分别对应表格的  

左下三角部分以及右上三角部分 。最后 ， 为了很好地适配ＵＳＳＡ ， 本文构造了

个端到端模型进行训练和预测 。在该模型中 ，本文提出

一种双轴注意力模块去有  

效地捕捉表格中的行与列的关联信息 。实验结果表明 ，本文提出的方法可以有效  

地解决重叠和非连续问题 ， 达到了当前最先进的效果 。  

４ ． １本章引论  

结构化情感分析旨在鉴别给定的

一句话中所有的意见元组 （ｏｐ ｉｎｉｏｎｔｕｐ ｌｅ） ，  

一个意见元组 （ ／ｉ ， ｔ，ｅ ，ｐ）代表持有者 （ｈｏｌｄｅｒ ， ／０对意见目标 （ｔａｒｇｅｔ ， ｔ）发表了  

一种带有情感极性 （ｐｏｌａｒｉｔｙ ， ｐ ）的表达 （ｅｘｐｒｅｓｓｉｏｎ ， ｅ） 。例如对于

“ Ｍｙｅｍｐ ｌｏｙｅｒ  

ｆｏｕｎｄ ｙｏｕｒｗｏｒｄｓｔｏｂｅｒｅｌａｌｌｙｒｕｄｅａｎｄｌｏｎｇｗｉｎｄｅｄ．

， 意见元组为（Ｍｙｅｍｐ ｌｏｙｅｒ ，  

ｙｏｕｒｗｏｒｄｓ ， ｒｅａｌｌｙｒｕｄｅ ，ｎｅｇａｔｉｖｅ ）以及（Ｍｙｅｍｐ ｌｏｙｅｒ ，ｙｏｕｒｗｏｒｄｓ ， ｒｅａｌｌｙ ｌｏｎｇｗｉｎｄｅｄ ，  

ｎｅｇａｔ ｉｖｅ ） 。结构化情感分析任务定义如下 ：给定

一个包含 ｉＶ个 ｔｏｋｅｎ的句子Ｘ＝  

｛你１ ，说２ ，

， １％｝ ， 目的是提取所有的意见元组７

＇ ＝ ｛（ ／以，６ ，０）？１￡１；１

，情感极性？ ３£ 

｛ｐｏｓｉｔ ｉｖｅ ，ｎｅｕｔｒａｌ ，ｎｅｇａｔ ｉｖｅ １ 。 以下为意见元组中各个元素的具体定义 ：  

＊ 持有者 （ｈｏｌｄｅｒ）：／Ｉ是结构化情感分析中发表观点的主体 ，通常会显式  

出现在文本中 ， 也有可能为空 。例如 ，

“ Ｍｙｅｍｐ ｌｏｙｅｒｆｏｕｎｄｙｏｕｒｗｏｒｄｓｔｏ  

ｂｅｒｅｌａｌｌｙｒｕｄｅａｎｄｌｏｎｇｗｉｎｄｅｄ． ” 中的

“ Ｍｙｅｍｐ ｌｏｙｅｒ

” 是持有者 ，

？“ Ｏｈ ，  

’ ｓ ｇｒｅａｔ ． ” 中没有持有者 。  

４６  

第四章基于统

一表格填充机制端到端框架的结构化情感分析方法  

？ 目标 （ｔａｒｇｅｔ） ：ｔ是结构化情感分析中发表观点的对象 ， 通常会显式出  

现在文本中 ， 也有可能为空 。例如 ，

“ Ｍｙｅｍｐ ｌｏｙｅｒｆｏｕｎｄ ｙｏｕｒｗｏｒｄｓｔｏｂｅ  

ｒｅｌａｌｌｙｒｕｄｅａｎｄｌｏｎｇｗｉｎｄｅｄ ：

” 中的

ｙｏｕｒｗｏｒｄｓ

” 是目标 ，

“ Ｓｏｍｅｏｔｈｅｒｓ  

ｇ ｉｖｅｔｈｅｎｅｗＵＭＵＣ５ｓｔａｒｓ ．

” 中的

ｔｈｅｎｅｗＵＭＵＣ

” 是目标 。  

＊ 表达 （ｅｘｐｒｅｓｓｉｏｎ） ：ｅ是结构化情感分析中带有观点的语句 ， 总是会显  

示出现在文本中 ， 可以认为其为意见元组的主键 。 例如 ，

“ Ｍｙｅｍｐ ｌｏｙｅｒ  

ｆｏｕｎｄ ｙｏｕｒｗｏｒｄｓｔｏｂｅｒｅｌａｌｌｙｒｕｄｅａｎｄｌｏｎｇｗｉｎｄｅｄ ．

’ ’ 中的

ｒｅｉａｌｌｙｒｕｄｅ

ｒｅｌａｌｌｙ

ｌｏｎｇｗｉｎｄｅｄ

”是表达 ， “

Ｓｏｍｅｏｔｈｅｒｓ ｇ ｉｖｅｔｈｅｎｅｗＵＭＵＣ５ｓｔａｒｓ ．

“ ５ｓｔａｒｓ

” 是表达 。  

＊ 极性 （ｐｏｌａｒｉｔｙ） ：ｐ是结构化情感分析中对于ｅ带有的情感极性 ， 属于  

｛ｐｏｓｉｔｉｖｅ ， ｎｅｇａｔｉｖｅ ， ｎｅｕｔｒａｌ｝的其中

一种 。例如 ，

“ Ｍｙｅｍｐ ｌｏｙｅｒｆｏｕｎｄ ｙｏｕｒ  

ｗｏｒｄｓｔｏｂｅｒｅｌａｌｌｙｒｕｄｅａｎｄｌｏｎｇｗｉｎｄｅｄ ，

” 中对于表达

ｒｅｌａｌｌｙｒｕｄｅ

” 和  

ｒｅｌａｌｌｙ ｌｏｎｇｗｉｎｄｅｄ

的情感极性都是消极的 ，

“ Ｓｏｍｅｏｔｈｅｒｓｇ ｉｖｅ ｔｈｅｎｅｗ  

ＵＭＵＣ５ｓｔａｒｓ ．

，， 中对于表达

“ ５ｓｔａｒｓ

，， 的情感极性是积极的 。  

本文将该任务中的 ／Ｉ 、 ｔ 、 ｅ统称为实体 ， 需要注意的是实体 ／ｉ 、 ｔ和 ｅ各自都可  

以是非连续的 ，体现在其在原句中不

一定是连续的片段 ；并且不同的意见元组的  

一实体可能是部分重叠的 。   

Ｎｅｇａｔｉｖｅ  

ＨｏｌｄｅｒＴａｒｇｅｔＥｘｐ ｒｅｓｓ ｉｆＭｉ  

Ｍｙｅｍｐｌｏｙｅｒｆｏｕｎｄｙｏｕｒｗｏｒｄｓｔｏｂｅｒｅａｌｆ ｙ ｊｒｕｄｅａｎｄｌｏｎｇｗｉｎｄｅｄ  

ＨｏｌｄｅｒＴａｒｇｅｔ Ｅｘｐｒｅｓｓ ｉｏｎ  

—一Ｎｅｇａｔｉｖｅ  

（ａ）  

Ｈｏ ｌｄｅｒＥｘｐ

：ＮｅｇＥｘｐ Ｎｅｇ  

—Ｊ 、、、、 ／ Ｎｅｇ  

Ｍｙｅｍｐｌｏｙｅｒｆｏｕｎｄｙｏｕｒｗｏｒｄｓｔｏｂｅｒｅａｌｌｙｒｕｄｅａｎｄｌｏｎｇｗｉｎｄｅｄ  

Ｈｏ ｌｄｅｒＴａｒｇｅｔ （ｂ）  

－ＳＨ －Ｅ  

一   

Ｍｙｅｍｐｌｏｙｅｒｆｏｕｎｄｖｏｕｒｗｏｒｄｓｔｏｂｅｒｅａｌｌｙｒｕｄｅａｎｄｌｏｎｇｗｉｎｄｅｄ  

ｉ＾Ｅ  

（ｃ ）  

１ ：ａ） 结构化情感分析示例 ； ｂ） 传统的双词汇依赖解析方法

， 但不能解决重叠  

实体问题 ； ｃ） 本文的双词汇依赖解析方法 ；  

４７  

北京邮电大学电子信息硕士学位论文  

大多数现有的方法将结构化情感分析任务视为双词汇依赖解析问题 ，但不幸  

的是 ，这种转化是有损的 ，这是因为无法同时解决重叠以及非连续问题 。如图 ４ －  

１ａ ）所不 ，

Ｍｙｅｍｐ ｌｏｙｅｒｆ ｏｕｎｄ ｙｏｕｒｗｏｒｄｓｔｏｂｅｒｅｌａｌｌｙｒｕｄｅａｎｄｌｏｎｇｗｉｎｄｅｄ ，

” 有两  

个表达 ： ｛ｒｅａｌｌｙ ，ｒｕｄｅ｝和 ｛ｒｅａｌｌｙ ，ｌｏｎｇ ， ｗｉｎｄｅｄ｝ ， 这两者之间有重叠的单词  

ｒｅａｌｌｙ

， 且后者是非连续的 。 如图 ４

－ １ｂ）所示 ， Ｂａｒ ｎｅｓ等人

［３６ ］提出

一种 ｈｅａｄ ？  

ｆ ｉｒｓｔ依赖解析方法 ，然而将该方法应用于该句时 ，解析图中出现了内在的模糊性 ，  

其会错误地判别两个重叠的表达 ｛ ｒｅａｌｌｙ ， ｒｕｄｅ 丨和 丨 ｒｅａｌｌｙ

，ｌｏｎｇ ，ｗｉｎｄｅｄ 丨为单独  

的表达 ｛ ｒｅａｌｌｙ

， ｒｕｄｅ ， ｌｏｎｇ

，ｗｉｎｄｅｄ ｝ 。也就是说 ， 对于结构化情感分析任务 ， 这  

种方法并不能识别两个重叠的实体 。 另外

一种先进的依赖解析方法ＴＧＬＳ由 Ｓｈｉ  

［５９ ］提出 ， 该方法旨在鉴别实体的头尾边界 ，但其不能识别非连续的实体 ，这  

是因为由于实体的非连续性 ， 根据实体的边界并不能直接确定实体本身 。  

为突破结构化情感分析任务中的核心瓶颈 ， 即同时存在重叠以及非连续问题 ，  

一种新颖的双词汇依赖解析图 ， 如图 ４

＿ ｌ ｃ）所示 。 这种双词汇依赖解  

析图包含两种类型的有向边 ： 关系预测 （ＲｅｌａｔｉｏｎＰｒｅｄｉｃｔｉｏｎ ， ＲＰ ） 以及单词提取  

（ＴｏｋｅｎＥｘｔｒａｃｔｉｏｎ ， ＴＥ ） 。 ＲＰ 主要处理实体本身边界判定以及实体间的关系预  

测 ， 其解决了重叠问题 。 具体来说 ， Ｅ

－ＰＯＳ／Ｅ

－ＮＥＧ／Ｅ －ＮＥＵ类型的边由表达的尾  

部连接至头部 ， 并将其作为带有 ｐｏｓｉｔｉｖｅ／ｎｅｇａｔｉｖｅ／ｎｅｕｔｍｌ情感极性的 ｅ 。 如图 ４ －  

１ｃ ）所示 ， — ｒｅａＷｙ是Ｅ －ＮＥＧ的边 ， 代表表达 ｛ｒｅａｌｌｙ

，ｌｏｎｇ

， ｗｉｎｄｅｄ ｝  

的头尾分别是

ｒｅａｌｌｙ

“ ｗｉｎｄｅｄ

，情感极性为 ｎｅｇａｔｉｖｅ 。 Ｈ

－Ｅ类型的边标  

记了／ｉ和 ｅ的起始／结束位置 ，例如 ｒｅａＵｙ— ｍｙ和 ｗｉｎｄｅｄ— ｅｍｐ ｌｏｙｅｒ分别是  

Ｈ －Ｓ 和 Ｈ

－Ｅ类型的边 ， 代表表达

｛ ｒｅａｌｌｙ ， ｌｏｎｇ

，ｗｉｎｄｅｄ ｝对应的持有者 ｛ｍｙ

ｅｍｐ ｌｏｙｅｒ ｝的起始点为

， 结束点为

。 同理 ， Ｔ

－Ｓ／Ｔ －Ｅ类型的边标  

记了／ｉ和 ｔ的起始／结束位置 ，例如 ｒｅａＷｙ— ｙｏｕｒ和 Ｈａｎｄｅｄ— ｗｏｒｄｓ

■分别是Ｔ －  

Ｓ和 Ｔ －Ｅ类型的边 ，代表表达 ｛ｒｅａｌｌｙ

，ｌｏｎｇ

，ｗｉｎｄｅｄ ｝对应的持有者 丨 ｙｏｕｒ ， ｗｏｒｄｓ｝  

， 结束点为

“ ｗｏｒｄｓ

。 ＴＥ主要鉴别给定实体边界后内部所有  

的 ｔｏｋｅｎ ，其解决了非连续问题 。具体来说 ， Ｈ

－ＮＷ类型的边表明下  

一个单词是什么 ，意为这两个单词是实体内部的

一个片段 ，是组成该实体的

一部  

分 。  

一步将依赖解析图转化为

一的 ２Ｄ表格填充机制 ， 并将其命名  

为ＵＳＳＡ 。 如图 ４

－２所示 ， 将依赖解析图每条边的起始位置视为 ｘ坐标 ， 终止位  

置视为 ｙ坐标 ，边的类型视为表格中单元格的标签类型 。因此整个表格被划分为  

左下半三角以及右上半三角部分 ， 分别对应ＲＰ和ＴＥ 。  

基于ＵＳＳＡ机制 ， 本文进

一个模型进行适配 。 首先 ， 基于 ２Ｄ表  

格中每个单元格对应的词对 ， 多语言 ＢＥＲＴ和 Ｂ ｉＬＳＴＭ被用来作为词对的语义  

４８  

第四章基于统

一表格填充机制端到端框架的结构化情感分析方法  

编码器 。然后如图 ４ －２所示 ， 可以看出表格中许多标签的横纵坐标具有很强的关  

联性 。基于这样的观察 ，本文提出

一个双轴注意力模块去有效地捕捉这些关联信  

息 。 最后应用

一个预测头去决定词对的关系类型 。  

１ ｙ   Ｈｏｌｄｅｒ  —

― －￣￣￣ ｒ

￣＾  

Ｍｙｈ－ｎｗ ／  

ｌｏｙｅｒ ２  

ｆｏｕｎｄ ３  

Ｔａｒｇｅｔ    

ｙｏｕｒｔ －ｎｗ ｖ  

ｗｏｒｄｓ ５  

ｔｏ ６  

ｂｅ ７  

ｒｅａ ｌ ｌｙｈ －ｓｔ －ｓ ｜

￡ －ｎｗＥ －ｍｖ？  

ｒｕｄｅＨ －ＥＴ －ＥＥ －ＮＥＧ ９  

ａｎｄ ｉ〇  

ｌｏｎｇ Ｅ －ＮＷｕ  

ｗ ｉｎｄｅｄＨ －ＥＴ －Ｅｅ －ｎｅｇ＾  

ｘ＼ Ｉ２３４５６７８９１０］Ｊ１２  

－ ２ ： 本文提出的双词汇依赖解析方法转化后的 ２Ｄ表格填充机制  

实验在ＮｏＲｅＣＦ ｉｎｅ

［ １０６］ 、 ＭｕｌｔｉＢＥｕ

［ １０７ ］ 、 Ｍｕｌｔ ｉＢｃＡ

［ １０７］ 、 ＭＰＱＡ

［ １０８］以及  

〇３１；１＾

１ （）９］这 ５个基准数据集上进行 ， 结果表明了本文提出的框架的有效性 ， 达  

到了先进的性能 。  

４ ．２统 一的表格填充机制ＵＳＳＡ  

本节首先介绍ＵＳＳＡ这

一表格填充机制 ，然后展示根据这

一机制如何在各种  

不同情形下进行解码 。  

４９  

北京邮电大学电子信息硕士学位论文  

４．２ ．１表格填充  

如表４ － １所示 ， 为解决结构化情感分析任务 ， ＵＳＳＡ为词对 构建了  

１３种类型的关系 。 如图４ －２所示 ， 整个表格被划分为左下三角以及右上三角部  

分 ， 分别对应关系预测以及单词提取部分 ， 接下来详述这两种关系类型 ：  

表４ －１ＵＳＳＡ机制中 １３种不同类型的关系  

Ｔｙｐｅ ＃ＲｅｌａｔｉｏｎＭｅａｎｉｎｇｏｆ ｗｏｒｄ ｐａｉｒ（ｗ ｔ，ｗ ，）  

１Ｅ －ＰＯＳ  

ｂｏｕｎｄａｉｙｗｏｒｄｓｏｆｅｘｐｒｅｓｓｉｏｎｗｉｔｈ  

２Ｅ －ＮＥＧ  

ｐｏｓｉｔｉｖｅ／ｎｅｇａｔｉｖｅ／ｎｅｕｔｒａｌｐｏｌａｒｉｔｙ  

３Ｅ －ＮＥＵ  

Ｒｅｌａｔｉｏｎ４Ｈ

－Ｓ  

ｓｔａｒｔｉｎｇｏｒｅｎｄｉｎｇｂｏｕｎｄａｒｙ   Ｐｒｅｄｉｃｔｉｏｎ５Ｈ －Ｅ  

ｏｆｈｏｌｄｅｒａｎｄｃｏｒｒｅｓｐｏｎｄｉｎｇｅｘｐｒｅｓｓｉｏｎ  

（ＲＰ）６Ｈ －ＳＥ  

７Ｔ －Ｓ  

ｓｔａｒｔｉｎｇｏｒｅｎｄｉｎｇｂｏｕｎｄａｒｙ  

８Ｔ －Ｅ  

ｏｆｔａｒｇｅｔａｎｄｃｏｒｒｅｓｐｏｎｄｉｎｇｅｘｐｒｅｓｓｉｏｎ  

 ９Ｔ －ＳＥ   

Ｔｏｋｅｎ１０Ｅ －ＮＷ  

ｓｐｅｃｉｆｉｃｔｏｋｅｎｓｏｆｅｘｐｒｅｓｓｉｏｎ／ｈｏｌｄｅｒ／ｔａｒｇｅｔ   Ｅｘｔｒａｃｔｉｏｎ１１Ｈ －ＮＷ  

ｂｙｉｎｄｉｃａｔｉｎｇｗ ，

－ｉｓｔｈｅＮｅｘｔＷｏｒｄｏｆＷｉ  

（ＴＥ）１２Ｔ －ＮＷ

１３丄ｎｏａｂｏｖｅｒｅｌａｔｉｏｎｓ  

鲁 关系预测 （ＲｅｌａｔｉｏｎＰｒｅｄｉｃｔｉｏｎ， ＲＰ） 旨在鉴别实体之间的关系并且赋予情  

感极性 。 具体来说 ， Ｅ －ＰＯＳ／Ｅ －ＮＥＧ／Ｅ －ＮＥＵ蕴含了表达ｅ的开始和结束边  

界 ， 并且赋予了ｐｏｓｉｔ ｉｖｅ／ｎｅｇａｔ ｉｖｅ／ｎｅｕｔｒａｌ情感极性 。 Ｈ －Ｓ／Ｈ －Ｅ／Ｈ －ＳＥ表明了表  

达ｅ对应的持有者ｆ ｔ的边界位置 ， 其中 Ｓ和Ｅ分别代表开始和结束位置 。  

ＳＥ说明该实体仅包含

一个 ｔｏｋｅｎ， 因此开始与结束位置是相同的 。 为了确  

保该词对对应的单元格位于表格左下三角部分 ， 需要将位置中较大者设为ｘ  

坐标 ， 较小者设为ｙ坐标 。 Ｔ －Ｓ／Ｔ －Ｅ／Ｔ －ＳＥ与Ｈ －Ｓ／Ｈ －Ｅ／Ｈ －ＳＥ类似 ， 表明了表  

达 ｅ对应的目标 ｔ的边界位置 。  

籲 单词提取 （ＴｏｋｅｎＥｘｔｒａｃｔｉｏｎ，ＴＥ） 旨在在由ＲＰ给定实体边界后来提取实  

体内部具体的单词 。 Ｅ －ＮＷ／Ｈ －ＮＷ／Ｔ －ＮＷ表明ｅｘｐｒｅｓｓｉｏｎＴｈｏｌｄｅｒ／ｔａｒｇｅｔ的下

个单词是什么 ， 意为这对单词被连续拼接成为实体的组成部分 。  

５０  

第四章基于统 一表格填充机制端到端框架的结构化情感分析方法  

４ ．２ ．２意见元组解码  

整体的解码算法是首先基于表格的左下三角部分鉴别每个 ｈｏｌｄｅｒ、 ｔａｒｇｅｔ 以  

及 ｅｘｐｒｅｓｓｉｏｎ的边界单词 ， 然后根据表格的右上三角部分鉴别具体单词 。 首先 ，  

－ＰＯＳ 、 Ｅ －ＮＥＧ 、 Ｅ －ＮＥＵ ｝找到 ｅｘｐｒｅｓｓｉｏｎ的边界单词 ； 其次 ， 根据 彳Ｈ

－Ｓ 、  

Ｈ －Ｅ 、 Ｈ

－ＳＥ ｝和 ｛Ｔ －Ｓ 、 Ｔ －Ｅ 、 Ｔ －ＳＥ ｝分别鉴别该ｅｘｐｒｅｓｓｉｏｎ对应的ｈｏｌｄｅｒ和ｔａｒｇｅｔ的  

边界单词 ；最后 ，在己知 ｈｏｌｄｅｒ、 ｔａｒｇｅｔ、 ｅｘｐｒｅｓｓｉｏｎ的边界单词后 ，根据 丨Ｅ

－ＮＷ 、  

Ｈ －ＮＷ 、 Ｔ

－ＮＷ ｝来提取ｈｏｌｄｅｒ 、 ｔａｒｇｅｔ 、 ｅｘｐｒｅｓｓｉｏｎ的内部具体的单词 ， 由此便形成  

意见元组（ｈ ，ｔ

，ｅ ，ｐ） 。  

Ｗ＼Ｗ２Ｗ４ＷｑＷｊＶＪ＼Ｗ２ＷｓＷ４ＷｑＷｒ  

１ ＿＿＿＿一 ■＿＿

■，  ■ ． －－－－■ ． ｎ  

Ｈ－ＮＷ切１Ｈ －ｍｖ  

Ｗ ：２Ｗ２  

Ｌ ．－ ．． ＿ １纖娜  

Ｔ －ｆｍ切３Ｔ －ＮＷ  

Ｉ＾４ ，  

切５Ｈ－ＳＴ －Ｓ￡ ＞娜也５Ｈ

－ＳＨ －ＥＴ －ＳＴ

－ １ＳＷ  

切６Ｈ －ＥＴ

－ＥＥ －Ｎ

－ ＥＧＷＱＨ －ＥＴ －ＥＥ －ＮＥＧ  

ＷｊＷｊ  

（｛ｗ；！ ，ｗ；２｝ ，｛＾３ ，Ｗ４｝ ，｛ｗ５ ，＾６｝ ，ＮＥＧ ） （｛ ２＾１ ，＾２｝ ，｛＾３ ，＾４｝ ，｛ｗｂ｝ ，ＮＥＧ ）  

ＨｏｌｄｅｒＴａｒｇｅｔＥｘｐｒｅｓｓｉｏｎ （｛ｗｉ ＾Ｗｉ｝ ，｛＾３ ，＾４＞ ，｛ｗ －

０ ，ｗ６｝ ，ＮＥＧ ）  

（ａ） （ｂ ）  

ＶＪ＼Ｗ２Ｗ４１Ｕ５Ｗｑｌ〇７ＷｉＶ〇２ＷｓＷ４ＷｑＷ７  

；Ｗ＼麵  

＼乂 ＞ ｘ

Ｗ２Ｗ２  

切３Ｈ －ＳＥＥ －ＰＯＳ切３ｔｉ －ＳＥ－ＮＷ  

＾４ Ｈ ＾ＳＥ

－？ｆＸＶ  

＾５ｍｘｗ＾５Ｅ －ＮＷＥ

ＷＱ财￡棚＿   ＾７ｊＴ－Ｅ＾７Ｈ －ＥＥ－ＮＥＨ  

（｛ｗ．Ｕ＾Ｕ＾ｈＮＥＧ ）

（｛ｍｈｎｕｌＬ ｛ｗ３ ，ｗ， ，ｗ７｝ ，ＮＥＵ ）   ＵＵ，Ｘ

． ／ａＵ ，）（｛Ｗｌ ，Ｗ２｝ ，ｎｕｌｌ

， ｛ｗ４ ，ｗ５ ，ｗ６｝ ，ＮＥＵ ）  

（ｃ） （ｄ ）  

－ ３ ： 四个解码示例  

－３展示了对于句子Ｚ＝的四个解码样例 ， 详述如下 ：  

５ １  

北京邮电大学电子信息硕士学位论文  

ａ） 扁平实体样例 ： 对于（｛ １＾１ ， １４／ ２｝， ｛＼＾ ＼＾４｝， ｛＞５ ， ＼＾６｝ ，况沉）这个意见元组 ， 首先  

根据坐标为 （ｗ６，ｗｓ）的Ｅ －ＮＥＧ鉴别ｅｘｐｒ ｅｓｓｉｏｎ的边界单词为ｗｓ和 ｖｖ６ ， 然  

后根据坐标为 （ｗｓ，ｗｊ的Ｈ －Ｓ和坐标为 （ｗ６ ，ｗ２）的Ｈ －Ｅ ， 可以鉴别该  

ｅｘｐｒｅｓｓｉｏｎ对应的ｈｏｌｄｅｒ为 和ｗ２ ， 同理根据坐标为 （ｗ５ ，ｖｖ３）的Ｔ －Ｓ和坐  

标为〇ｖ６ ，ｗ３）的Ｔ －Ｅ ， 可以鉴别该 ｅｘｐｒｅｓｓｉｏｎ对应的 ｔａｒｇｅｔ为ｗ３和ｗ４ 。然  

后根据坐标为 （ＷｐＵ＾）的Ｈ －ＮＷ、 坐标为 （ｗ３，ｗ４）的Ｔ －ＮＷ以及坐标为  

（ｗｓ ，ｗ６）的Ｅ －ＮＷ标签 ， 可以探测出三个路径ｗ ！

－ ＞ｗ２ 、 ｗ３

－ ＞ｗ４和ｗｓ—  

ｗ６ ， 最后便形成该意见元组 。  

１＞） 重金实体样例 ： 两个意见元组（｛１１／ １ ， １＾２｝，｛说３ ， １＾４｝，｛ １／１／ ５｝，况￡（；） 以及  

（｛Ｗｉ，ｗ２｝，｛ｗ３，ｖｖ４｝，｛ｗｓ ，ｗ６｝ ，中的ｅｘｐｒｅｓｓｉｏｎ ｛ｗｓ｝和 ｛ｗｓ ，ｖｖ６｝是部分重  

叠的 。 为鉴别这两个意见元组 ， 首先根据坐标为 （ｗｓ ， ｖｖ５）的Ｅ －ＮＥＧ鉴别其  

一个ｅｘｐｒｅｓｓｉｏｎ的起始和结束位置都是 ｛ ｉｖｓ｝ ， 然后根据坐标为 （ＷｐｗＯ  

的Ｈ －Ｓ和坐标为 （ｗ５ ，ｗ２）的Ｈ －Ｅ ， 可以鉴别该ｅｘｐｒｅｓｓｉｏｎ对应的ｈｏｌｄｅｒ为  

叫和ｗ２ ， 同理根据坐标为 （ｗ５ ，ｗ／ ３）的Ｔ －Ｓ和坐标为 （ｗ５ ，ｗ４）的Ｔ －Ｅ ， 可以  

鉴别该 ｅｘｐｒ ｅｓｓｉｏｎ对应的 ｔａｒｇｅｔ为ｗ３和ｗ４ ， 由此再根据ＮＷ类型的关系识  

别具体单词 ， 故第

一个意见元组可以识别出来 。 同理另

一个Ｅ －ＮＥＧ坐标为  

（ｗ５ ，ｗ６） ， 再根据Ｈ －Ｓ／Ｈ －Ｅ鉴别ｈｏｌｄｅｒ ， 根据Ｔ －Ｓ／Ｔ －Ｅ鉴别 ｔａｒｇｅｔ， 最后根  

据ＮＷ类型标签识别具体单词形成第二个意见元组 。 因此 ， ＲＰ类型的关系  

有助于解决重叠实体问题 。  

ｃ） 非连续实体样例 ： 意见元组 （｛叫｝ ，｛ｗ５ ，ｗ７｝，｛ｗ３｝，ｉＶ￡Ｇ）中的 ｔａｒｇｅｔ ｛ｗｓ，ｗ７）  

是非连续的 ， 这可以根据位于（ｍ／ ５ ，ｗ７）的Ｔ －ＮＷ识别 。 因此 ， ＴＥ类型的关  

系有助于处理非连续实体问题 。  

ｄ） 复杂实体样例 ： 考虑

一个复杂且罕见的样例 ， 两个意见元组  

（｛ｗｊ，ｎｕＨ，｛ｗ３ ，ｗ５ ，ｗ７｝，ｉＶＥｉ／）和 （｛ １％ ，ｗ２｝ ，ｎｕｌｌ ，｛ｗ４ ，ｗ５ ，ｖｖ６｝，中的  

ｅｘｐｒｅｓｓｉｏｎ ｛ｗ３，ｗｓ ，ｕ／ ７］和 ｛ｍ／４ ，ｗ５ ，ｕ／ ６｝是部分重叠的 ， 且前者是非连续的 。  

如果仅使用ＲＰ类型关系 ， 非连续的 ｅｘｐｒ ｅｓｓｉｏｎ｛ｗ３ ，ｕ／ ｓ ，ｗ７｝将会错误鉴别为  

连续的实体 ｛＾３ ， １＾４ ， １＾５ ，说６， １１／７｝ ； 如果仅使用 丁五类型关系 ， 则不可能正确  

地鉴别正确的 ｅｘｐｒｅｓｓｉｏｎ ， 这是因为可以找到四个路径 （ｗ３

－？ｗ５ 、 ｗ４—  

ｗｓ 、 ｗ５ ％和ｗｓ

－？ｗ７ ） ， 而由于没有ＲＰ指定实体的边界由此无法获知  

这些路径的组合关系 。 因此 ， 只有同时使用ＲＰ和ＴＥ才能正确鉴别这个复  

杂的意见元组样例 。  

５２  

第四章 基于统

一表格填充机制端到端框架的结构化情感分析方法  

４ ，３模型架构  

本节展示了适配ＵＳＳＡ的模型架构 ， 该模型结构如图 ４

－４所示 ， 主要由四  

个部分组成 ， 分别为 ： 编码层 、 词对表示层 、 细化策略以及预测层 。  

Ｗ ，双轴注意力模块  

Ｗ，词对表示模块

 １   

＝ ＝ ｎ  

－－ｎ，

＿ —ｇ  

－＊？ ：

—？ —？Ｈ －？

— ？？

？？＊ —？

ｗ ｉ乌 —

１＿ Ｉ？

ｒ ｉＪ Ｉｉ

｜   ＇？ｈ ｊＷｏｒｄ － ｐａｉｒ ＾ｊ  

？ ？Ｒｅｐｒ ｅ？？ｎｔ？Ｈｏｉ ｉ ｊ雄 Ｊ  

Ｗｙ ▼  

 ： ＪＤｅｃｏｄｉｎｇ  

Ｄｉｓ ｔａｎｃｅ Ｆｅａｔｕｒｅ  

编码层 ｜词对表示层 Ｉ细化策略 预测层 ｍ，  

－４ ： 结构化情感分析模型架构  

４ ．３ ， １编码层  

给定包含 ｉＶ个 ｔｏｋｅｎ 的输入句子义 ＝ ｛ １＾ ， １＾２ ， －

， １＾｝ ， 编码层利用多语言  

ＢＥＲＴ与ＢｉＬＳＴＭ构成基础的语义编码器形成该序列的表示 ／／＝ ，  

如下所示 ：  

Ｈ＝ＢｉＬＳＴＭ （ｍＢＥＲＴ（Ｘ））（４ － １）  

４ ．３ ．２词对表不层  

为得到 ２Ｄ表格中每个单元格词对的表示 ， 本文提出

一种词对表示模块 。  

显然在ＵＳＳＡ机制中词对表示并没有对称性 受Ｗａｎｇ等  

［９４ 】的启发 ， 本文利用条件层归

一化 （ＣｏｎｄｉｔｉｏｎａｌＬａｙｅｒＮｏｒｍａｌｉｚａｔｉｏｎ ，  

ＣＬＮ ） 去建模这种条件化词对表示Ｒ ，  

ｒＵｊ ＝ＣＬＮ （ ／ｉ ， ，ｈ ｊ） ＝ＹｉＯ＾（４ － ２）  

其中＇；代表位于 （叫 ，％ ；）单元格的表示 ， 缩放因子ｎ和转移因子小蕴含了额  

外的语境信息 ， ＾和＾分别是ｂ的均值和标准差 ，  

— Ｗ ｙ

／ｉ ＾＋ｂ ｙ

ｊ ＝＋ｂｘ（４ — ３）  

５３  

北京邮电大学电子信息硕士学位论文  

Ｍ ＝ 互工 ？７＾ ＝（￣ｆ ｃ

２（４ －４）  

ｆ ｃ＝ｌ＾ ｋ＝ｌ  

其中 ／ｉ＃表示ｂ的第ｆ ｃ维 ， Ｗ ｙ

，ＷＡ，？和 ６Ａ是可学习的参数 。  

４．３ ．３细化策略  

４ ．３ ．３ ．１双轴注意力模块  

ＵＳＳＡ的关系在横纵坐标上呈现很强的关联性 。 如图 ４

－２所示 ， 对于位于  

（ １２ ，８）的Ｅ －ＮＥＧ ， 那么其对应的Ｈ －Ｓ／Ｈ －Ｅ／Ｔ －Ｓ／Ｔ －Ｅ如果存在 ， 那么其

一定位于  

第 ８行或第 １２行或第 ８列或第 １２列 ， 而且必定会有Ｅ －ＮＷ关系位于第 ８行以  

及位于第 １２列 。 受轴向注意力

［９５ ＿９７ ］在计算机视觉的成功的启发 ， 本文提出应用  

双轴注意力模块去捕获关系之间的关联性并且确保全局的连接 。 首先定义

一个  

单向注意力如下 ：  

ａ Ｕｊ ＝ＭｕｌｔｉＨｅａｄ ｛ｎｊ

，ｒｏｗ ｆ，ｒｏｗ ；）＋（４ — ５）   ＭｕｌｔｉＨｅａｄ （ｒ ｆ ｙ

，ｃｏｌ ｙ

，ｃｏｌ ｙ）  

其中ＭｕｌｔｉＨｅａｄ 、 ｒｏｗ ｊ以及 ｃｏｌ ；

？分别代表多头注意力机制 、 词对表不Ｒ的第 ｆ  

？ 列 。 然后本研究利用另

一个对称的轴向注意力以及词对本身的表示  

去构造语义表示Ｃ如下 ：  

ｃｕ ＝ａＵ？ｎ ．ｊ？？

／ ， ｉ（４ 一 ６）  

４ ．３ ．３ ．２特征加强  

一步加强表示 ， 本文引入如图４

－４所示的相对距离特征 。 这是因为  

ＵＳＳＡ的关系对相对距离是比较敏感的 ， 例如ＮＷ的跨度越大 ， 中间间隔的距  

离就越大 。此外 ， 其还可以帮助鉴别词对是位于左下半角还是右上半角区域 ，  

故最终的表示Ｖ如下 ：  

ｖ ｉ ，ｊ ＝ｃ ｉ ，ｊ？ｄ ｉ ￣ ｊ（４ 一 ７）  

其中木 ＿ ；

？是相对距离特征 。  

５４  

第四章基于统

一表格填充机制端到端框架的结构化情感分析方法  

４ ．３ ，４预测层  

为得到表格中单元格词对的标签概率分布 ， 将细化表示Ｖ输入至前馈神  

经网络 （ｆｅｅｄ －ｆｏｒｗａｒｄｎｅｔｗｏｒｋ， ＦＦＮ）并将序列化输入表示心 ，；

？送至双仿射预测  

头 （ｂｉａｆ ｉｎｅ ｐｒｅｄｉｃｔｏｒ）作为增强。  

４．３ ．４ ． １ＦＦＮ预测头  

对于词对表示 ， 利用前馈神经网络得到关系的预测分值 ：  

ｓ｛ｊ ＝ＦＦＮ＾Ｖｙ） （４ －８）  

其中 ｅ是关系分值 ， ｍ是关系类型的数量。  

４．３ ．４．２Ｂｉａｆ ｉｎｅ预测头  

双仿射注意力 （Ｂｉａｆ ｉｎｅａｔｔ ｅｎｔｉｏｎ）机制己经在依赖解析任务中证明了有效  

， 而且基于以往的研宄

， 它可以与ＦＦＮ协同工作 。 故在本研究模型中使  

用Ｂｉａｆ ｉｎｅ模块去得到关系分值＜；作为增强 ， 如下所示 ，  

ｈｆ ＝ＦＮＮａ  

ｈｆ ＝ＦＮＮｂ（ｈ ｊ） （４ －９）  

ｓ匕 ＝ ｈｆ ｌ＾ｈｆ＋Ｕ２（吋十ｈｆ）＋ｂ  

其中ｕｌ ｆ以及ｈ是可训练的权重和偏置项 ， 最后关系分值 ｅＲ？被获得 。  

最后的关系标签概率分布由

—起计算得到 ， 即 ：  

Ｐｉｊ ＝ｓｏｆｔｍａｘ （ａｓ（｝

．＋（１ —（４ — １０）  

其中ａ是被用来调整各部分权重的超参数 。  

４ ．３ ．５损失函数  

最后的目标是最小化如下交叉摘损失 ：  

ＮＮ   —ｍ：ＫＶｉｊ ＝ｒ） ｌｏｇ（ＰＵ ！ｒ）（４ ￣ １１）  

ｉｊｒＧＸ  

其中Ｗ是句子中ｔ ｏｋｅｎ的数量 ， 灭是ＵＳＳＡ预定义的关系集合 。  

５５  

北京邮电大学电子信息硕士学位论文  

４ ．４实验对比和分析  

４ ．４． １数据集介绍  

本实验选用ＮｏＲｅＣＦｉｎｅ

［＿ 、 ＭｕｌｔｉＢＥｕ

［ １Ｇ７］ 、 ＭｕｌｔｉＢＣＡ

［ １Ｑ７］ 、 ＭＰＱＡ

［１ ＜）８］以及  

〇８１１１＾

［１（）９］这五个数据集 。ＮｏＲｅＣｆｉｎｅ是

一个挪威语的专业评论数据集 ；ＭｕＭＢｅｕ和  

Ｍｕｌｔ ｉＢＣＡ分别是Ｂａｓｑｕｅ和 Ｃａｔａｌａｎ语言的关于旅馆的数据集 ； ＭＰＱＡ是英文的  

新闻语料 ； ＤＳＵｎｉｓ是在线的英文的关于大学的语料。 表４ －２展示了数据集中训练  

集、验证集以及测试集的句子 、持有者、 目标 、表达以及情感极性分别为积极的 、  

中性的 、 消极的四元组的数量 ， 表４ －３展示了重叠与非连续实体数量 。  

表 ４ －２结构化情感分析基准測试集数据统计   数据集 划分 句子ｈｔｅｍｍ 中性 消极  

训练集８６３４８９８６７７８８４４８５６８４０２７５６  

ＮｏＲｅＣＦ ｉｎｅ验证集１５３１１２０１ １５２１４３２９８８０４４３  

测试集１２７２１１０９９３１２３５８７５０３５８  

训练集１０６４２０５１２８５１６８４１４０６０２７８  

ＭｕｌｔｉＢｅｕ验证集１５２３３１５３２０４１６８０３６  

测试集３０５５８３３７４４０３７５０６５  

训练集１ １７４１６９１６９５１９８１１２７２０７０８  

ＭｕＷＢｃａ验证集１６８１５２１ １２５８１５１０１０７  

测试集３３６５２４３０５１８３１３０２０４  

训练集５８７３１４３ １１４８７１７５ １６７１３３７６９８  

ＭＰＱＡ验证集２０６３４１４５０３５８１２２３１２６２１６  

测试集２１ １２４３４４６２５１８１５９８２２２３  

训练集２２５３６５８３６８３６３４９１０４３８３  

ＤＳｕｎｉｓ验证集２３２９１０４１０４３ １１６５７  

测试集３ １８１２１４２１４２５９１２７ １  

５６  

第四章基于统

一表格填充机制端到端框架的结构化情感分析方法  

表４ ＊３ 结构化情感分析基准測试集非连续实体以及重叠实体数量统计 （ｏｖｅｒ．表示重叠 ， ｄｉｓ．  

表示非连续）  

ＨｏｌｄｅｒｓＴａｉ＾ｅｔｓＥｘｐｒｅｓｓｉｏｎｓ   数据集划分  

ａｌｌｏｖｅｒ．ｄｉｓ．ａｌｌｏｖｅｒ．ｄｉｓ ．ａｌｌｏｖｅｒ．ｄｉｓ ．  

训练集８９８００６７７８０３９８４４８１６６５７８１  

ＮｏＲｅＣＦｉｎｅ验证集１２０００１ １５２０５１４３２２６１１３ １  

测试集１１０００９９３０６１２３５２６２１２５  

训练集２０５０４１２８５０２３１６８４０９１  

Ｍｕｌｔ ｉＢＥＵ验证集３３００１５３０１２０４０１５  

测试集５８０６３３７０４４４００２３  

训练集１６９０１１６９５０２３１９８１０６１  

ＭｕｌｔｉＢｃＡ验证集１５００２１１０１２５８３６  

测试集５２００４３００８５１８０１８  

训练集１４３ １００１４８７２４１０１７１５６０  

ＭＰＱＡ验证集４ １４００５０３８００５８１２０  

测试集４３４００４６２８００５１８００  

训练集６５００８３６１６０８３６０８２  

ＤＳｕｎｉｓ验证集９００１０４００１０４０８  

测试集１２００１４２２０１４２０１２  

４ ．４ ．２实验参数与设置  

本文采用开源深度学习框架ＰｙＴｏｒ ｃｈ实现了所提出的ＵＳＳＡ机制以及模型 ，  

并在公开基准数据集上进行了训练 。为验证其有效性 ，本文在基准数据集上进行  

了定性以及定量实验 ， 实验环境为Ｕｂ＿１８ ．０４服务器 ， ＧＰＵ为单张 ２４ＧＢ显  

存的ＮＶＩＤＩＡ Ａ００显卡 。  

在训练阶段 ， 使用固定权重的预训练的 ＢＥＲＴ

－ｍｕｌｔｉｌｉｎｇｕａｌ －ｂａｓｅ 以及可训练  

的 ４层输出维度为 ７６８的ＢｉＬＳＴＭ作为句子编码器 ， 这样设置是为了与其他方  

法进行公平对比 。优化器选用ＡｄａｍＷ ， 对于前 １０％的训练步骤应用ｗａｒｍｕｐ策  

略 。 相对距离特征维度设为 １００ ， 批处理大小 （ｂａｔｃｈｓｉｚｅ）设为 １６ ，ｄｒｏｐｏｕｔ设  

为 ０ ． １ ， ｅｐｏｃｈ数量设为 ６０。采用梯度累积策略 ， 步长设为 ２ 。 出了ＭｕＷＢｃａ使  

学习率设为 ｌｅ －３ ，其他数据集学习率设为 ２ｅ －３ 。ＮｏＲｅＣＦｉｎｅ、ＭｕｌｔｉＢＣＡ以及ＤＳＵｎｉｓ  

５７  

北京邮电大学电子信息硕士学位论文  

中超参数ａ设为０ ．６５ ，ＭｕｌｔｉＢＥＵ中ｃｔ设为０ ．５ ，ＭＰＱＡ中ａ设为０ ．７２５。采用ＮＶＩＤＩＡ  

Ａ１００显卡训练模型 ， 平均每个数据集训练时间为 ０．７５ｈ。  

需要说明的是 ，上述模型的结构和超参数设置均通过验证集的表现进行选择 ，  

最后报道的模型表现结果是 ５个不同随机种子设置下的平均值 。  

４ ．４．３评估指标  

实验采用持有者ＦＩ（ＨｏｌｄｅｒＦｌ ） 、 目标ＦＩ（ＴａｒｇｅｔＦｌ ） 、表达ＦＩ（Ｅｘｐｒｅｓｓｉｏｎ  

Ｆ１ ）、情感图ＦＩ（Ｓｅｎｔ ｉｍｅｎｔＧｒａｐｈＦｌ ， ＳＦ１ ） 以及无极性情感图ＦＩ（Ｎｏｎｐｏｌａｒｉｔｙ  

ＳｅｎｔｉｍｅｎｔＧｒａｐｈＦＩ ， ＮＳＦ１ ）来衡量三元组抽取的性能 。接下来详细介绍各个指  

标 ：  

１ ）持有者Ｈ（ＨｏｌｄｅｒＦ１ ）指标是持有者精确率 （Ｐｒｅｃｉｓｉｏｎ ， Ｐ）和持有者召  

回率 （Ｒｅｃａｌｌ ， Ｒ） 的调和平均 ， 目标Ｆ１和表达Ｆ １也是类似的 。持有者Ｆ１如下  

所示 ：  

ｃｏｒｒｅｃｔｈｏｌｄｅｒｎｕｍｂｅｒ   ＾ｈｏｌｄｅｒｐｒｅｄｉｃｔｅｄｈｏｌｄｅｒｎｕｍｂｅｒ ４＾  

ｃｏｒｒｅｃｔｈｏｌｄｅｒｎｕｍｂｅｒ   ＾ｈｏｌｄｅｒｌａｂｅｌｅｄｈｏｌｄｅｒｎｕｍｂｅｒ ＾  

＾ｈｏｌｄｅｒ ＝－

ｉ ￣（４ － １４）  

１ ｉ   ＾ｈｏｌｄｅｒ＾ｈｏｌｄｅｒ  

２）情感图 ＦＩ（ＳＦ１ ）指标是综合评判意见四元组的抽取效果 ， 该指标与

一般的Ｈ是当四元组每个元素都正确才算四元组正确 ，但 ＳＦ１  

会考虑预测的四元组和真实标签的四元组之间的重合度 。 该指标计算四个元素  

（Ｈｏｌｄｅｒ ， Ｔａｒｇｅｔ ， Ｅｘｐｒｅｓｓｉｏｎ ， Ｐｏｌａｒ ｉｔｙ）的预测片段和真实标签片段的平均重合  

程度 （其中 Ｐｏｌａｒｉｔｙ只有匹配和不匹配两种情况 ） ，若有多个匹配的意见四元组 ，  

则取平均重合度最大的元组 。意见元组标签的Ｈｏｌｄｅｉ

？或者Ｔａｒｇｅｔ元素可以为空 ，  

此时 ， 相应的要求预测的Ｈｏｌｄｅｒｓ或者Ｔａｒｇｅｔｓ元素也要为空 ， 否则不算成功匹  

配 。首先定义加权精确率 （ＷｅｉｇｈｔｅｄＰｒｅｃｉｓｉｏｎ ， ＷＰ ） 以及加权召回率 （Ｗｅｉｇｈｔｅｄ  

Ｒｅｃａｌｌ ， ＷＲ）如下所示 ：  

ｏｖｅｒｌａｐｐｅｄｃｏｒｒｅｃｔｔｕｐ ｌｅｓ ，   ＷＰ＝ — －￣－ ｚ ￣

—（４ － １５）   ｐｒｅｄｉｃｔｅｄｔｕｐ ｌｅｓｎｕｍｂｅｒ  

ｏｖｅｒｌａｐｐｅｄｃｏｒｒｅｃｔｔｕｐ ｌｅｓ ，   ＷＲ＝——（４ －１６）  

ｌａｂｅｌｅｄｔｕｐ ｌｅｓｎｕｍｂｅｒ  

５８  

第四章基于统

一表格填充机制端到端框架的结构化情感分析方法  

情感图ＦＩ（ＳＦ１ ）指标是两者的调和平均值 ：  

ＳＦ１＝ ｘ

－ ２ －

－ （４ － １７）  

Ｗｒ  

３ ）无极性情感图 ＦＩ（ＮＳＦ１ ）指标类似情感图 Ｆｌ ， 但是不考虑情感极性 ，  

只考虑 （Ｈｏｌｄｅｒ ， Ｔａｒｇｅｔ ， Ｅｘｐｒｅｓｓｉｏｎ）三个元素的抽取情况 。  

４ ．４ ．４基线方法  

本文选取了５个先进的模型进行对比 ，分别为ＲＡＣＬ －ＢＥＲＴ、Ｈｅａｄ －ｆ ｉｒｓｔ、Ｈｅａｄ

ｆ ｉｎａｌ 、ＦｒｏｚｅｎＰＥＲＩＮ以及ＴＧＬＳ 。  

简要所对比的模型描述如下 ：  

１ ）ＲＡＣＬ －ＢＥＲＴＭ％这是关系感知协作框架 ，允许结构化情感分析中各个子  

任务之间协同工作 。 Ｂａｒ ｎｅｓ等人 应用该方法作为结构化情感分析的基  

线模型 。  

２）Ｈｅａｄ －ｆｉｒｓｔ和Ｈｅａｄ －ｆ ｉｎａｌ ［３６］

： 这是由Ｂａｒｎｅｓ等人提出的两个不同的依赖解  

析方法 ， 并且使用了复现的神经解析器 。  

３ ）ＦｒｏｚｅｎＰＥＲＩＮ ［６Ｑ］ ： 其应用 ＰＥＲＩＮ ［１Ｑ １＾用

一个固定的ＸＬＭ －Ｒ作为基底建  

模图特征的超集 。  

４）ＴＧＬＳ ［５Ａ 这是

一种最新的双词汇依赖解析方法并应用图注意力模块 。  

４ ．４ ．５实验结果  

本文对ＵＳＳＡ与 ５个先进的方法使用多个指标进行对比 ，结果如表４ －４所示 。  

本研究发现本研究的ＵＳＳＡ在 ＳＦ１指标评测下比其他基线模型表现更好 ， 平均  

比次优方法 １ ．４７％的Ｈ指标 。它包含了

一些较大的提升 ， 诸如在ＭＰＱＡ数据集  

提取 ｔａｒｇｅｔ有 ７ ．２％的Ｈ分值提升 ， 以及在ＮｏＲｅＣＦ ｉｎｅ数据集提取ｈｏｌｄｅｒ有 ５ ．４％  

的Ｆ１分值提升 。ＵＳＳＡ在ＮｏＲｅＣＦｉｎｅ数据集对于提取 ｔａｒｇｅｔ略弱 ，有 ０ ．５％的Ｆ１  

指标的下降 。考虑情感图指标 ，该指标是很重要的对于全面评价实体、关系以及  

情感极性的预测 。本文的ＵＳＳＡ方法无论是在ＮＳＦ１指标还是 ＳＨ指标都全面超  

越了其他方法 。对比另外

一个强有力的基线ＴＧＬＳ ，尽管并没有使用 ＰＯＳ、 ｌｅ＿ａ  

或字符 ｅｍｂｅｄｄｉｎｇ ，本文的ＵＳＳＡ超越其表现平均 ３ ．４８％ＮＳＦ１分值以及３ ．１４ＳＦ１  

分值。这种提升归功于本文的ＵＳＳＡ可以有效的解决重叠实体以及非连续实体 。  

５９  

北京邮电大学电子信息硕士学位论文  

－４ 结构化情感分析实验结果对比  

＿ ＿ＳｐａｎＳｅｎｔ．Ｇｒａｐｈ   ＤａｔａｓｅｔＭｏｄｅｌ  

ＨｏｌｄｅｒＦＩＴａｒｇｅｔＦ ＩＥｘｐ

．Ｆ ＩＮＳＦ ｌｔＳＦ １Ｔ  

ＲＡＣＬ －ＢＥＲＴ ＿４７ ，２５６ ．３＿＿  

－ｆｉｒｓｔ５ １ ． １５０ ， １５４ ．４３７ ．０２９ ．５  

－ｆ ｉｎａｌ６０ ．４５４．８５５ ，５３９ ．２３ １ ．２   ＮｏＲｃＣＦ ｉｎｅ  

ＦｒｏｚｅｎＰＥＲＩＮ４８ ．３５ １ ．９５７ ．９４ １ ．８３５ ．７  

ＴＧＬＳ６０ ．９５３ ．２６ １ ，０４６ ．４３７ ．６  

ＵＳＳＡ ６６３ ５４ ．３６１ ．４４７．７３９．６  

ＲＡＣＬ －ＢＥＲＴ＿５９ ．９７２ ，６＿＿  

－ｆｉｒｓｔ６０ ．４６４ ．２７３ ，９５８ ．０５４ ．７  

Ｈｅａｄ －ｆｉｎａｌ６０ ．５６４ ．０７２ ． １５８ ．２５４ ．７   ＭｕｌｔｉＢＥｕ  

ＦｒｏｚｅｎＰＥＲＩＮ５５ ．５５８ ．５６８ ，８５３ ． １５ １ ．３  

ＴＧＬＳ６２ ．８６５ ．６７５ ．２６ １ ． １５８ ．９  

ＵＳＳＡ ６３Ａ  ６６．９７５ ．４６３ ．５６０．４  

－ＢＥＲＴ＿６７ ．５７０ ．３＿＿  

－ｆ ｉｒｓｔ４３ ．０７２ ．５７ １ ． １６２ ．０５６ ．８  

－ｆｉｎａｌ３７ ． １５４ ．８６７ ． １５９ ．７５３ ．７   ＭｕｌｔｉＢＣＡ  

ＦｒｏｚｅｎＰＥＲＩＮ３９ ．８６９ ．２６６ ．３６０ ．２５７ ．６  

ＴＧＬＳ４７ ．４７３ ．８７ １ ．８６４ ．２５９ ．８  

ＵＳＳＡ ４７

＾ ５  ７４．２７２．２６７．４６１ ．０  

－ＢＥＲＴ＿２０ ．０３ １ ．２＿ ＿  

－ｆ ｉｒｓｔ４３ ．８５ １ ．０４８．１２４ ．５１７ ．４  

－ｆｉｎａｌ４６ ．３４９ ．５４６ ．０２６ ． １１８ ．８   ＭＰＱＡ  

ＦｒｏｚｅｎＰＥＲＩＮ４４ ．０４９ ．０４６ ．６３０ ．７２３ ． １  

ＴＧＬＳ４４ ， １５ １ ．７４７ ．８２８ ．２２ １ ．６  

ＵＳＳＡ ４７Ｊ  ５８ ．９４８ ．０３６ ．８３０．５  

－ＢＥＲＴ＿４４ ．６３８ ．２ ＿＿  

－ｆｉｒｓｔ２８ ．０３９ ．９４０ ．３３ １ ．０２５ ．０  

－ｆｉｎａｌ３７ ．４４２ ． １４５ ．５３４ ．３２６ ．５  

ＤＳｕｎ ｉｓ  

ＦｒｏｚｅｎＰＥＲＩＮ１３ ．８３７ ．３３３ ．２２４ ．５２ １ ．３  

ＴＧＬＳ４３ ．７４９ ．０４２ ．６３６ ． １３ １ ． １  

ＵＳＳＡ ４４２  ５０ ．２４６ ．６３８．０３３ ．２  

６０  

第四章基于统

一表格填充机制端到端框架的结构化情感分析方法  

４．４ ．６实验分析  

４ ．４ ．６ ．１对于模型各模块的分析  

为了对模型各模块进行分析 ， 本节做了消融实验 ， 结果如 ４ －５所示 。 实验结  

果展现了双轴向注意力模块对于性能的提升是有效的 ，它的去处会导致在 ５个数  

据集上明显的性能下降 。另

一方面 ，替代双轴向注意力为单轴注意力模块以及去  

除距离特征对于性能表现有微弱的下降 。进

一步来说 ， 本研究发现当 ＦＦＮ预测  

一个重要的角色 ， ｂｉａｆ ｉｎｅ预测头也有积极的影响 ，平均有 ０ ．４７％的性能提  

升 。 最后 ， 完全去除ＮＷ类型关系会在所有数据集上有明显的 ＳＦ１分值下降 ，  

尤其是在ＮｏＲｅＣＦ ｉｎｅＵ１ ．２９％）和ＤＳＵｎｉｓＵ１ ．０１％） ， 这是因为这些数据集包含非  

连续实体的比例更高 ， 如果去除ＮＷ类型关系 ， 这些实体会被错误鉴别为连续  

的 。总的来说 ，实验结果表明每个模块的有效性并且证明了ＮＷ关系的重要性 。  

表 ４ －５ 结构化情感分析消融研究  

ＮｏＲｅＣＦｉｎｅＭｕｌｔｉＢＥＵＭｕＩｔｉＢｃＡＭＰＱＡＤＳｕｎｉｓ  

ＦｕｌｌＭｏｄｅｌ ３９．５８ ６０．３９６１ ．０２３０．４６３３．１９  

３８ ．６７５９．６１６０ ．０３２９ ．８３３２ ．１７   ｗ／ｏｂｉ －ａｘｉａｌａｔｔ．  

－０ ．９１）（

－０ ．７８）（

－０．９９）（

－０．６３）（１ ．０２）  

３９ ．３２５９ ．９１６０ ．４４２９ ．９８３２．８８   ｗ／ｓｉｎｇｌｅ －ａｘｉａｌａｔｔ．  

－０ ．２６）（

－０ ．４８）（

－０ ．５８）（

－０ ．４８）（

－０ ．３ １）  

３９ ．４１６０ ．０１６０ ．６７３０．２１３２ ．９９   ｗ／ｏｄｉｓｔａｎｃｅ  

－０ ．１７ ）（

－０ ．３８）（

－０ ．３５）（

－０ ．２５）（

－０ ．２０）  

３９ ．２２６０ ． １ １６０ ．８４２９ ．９９３３ ．０２   ｗ／ｏｂｉａｆ ｉＢｎｅ  

－０．３６）（

－０ ．２８）（

－０． １８）（

－０ ．４７）（

－０． １７ ）  

３７ ．７２５９．２３５９．６０２９ ．３６３１ ．５９   ｗ／ｏＦＦＮ  

－ １ ．８６）（

－ １ ． １６）（

－１ ．４２）（

－ １ ． １０）（

－ １ ．６０ ）  

３８ ．２９５９．８２６０ ．４１３０ ．４２３２ ．１８   ｗ／ｏ ＊ －ＮＷ    （

－１ ．２９）（

－０ ．５７）（

－０ ．６１）（

－０．０４）（

－ １ ．０ １）  

６１  

北京邮电大学电子信息硕士学位论文  

４Ａ６ ．２对于双轴注意力机制的分析  

？ＵＳＦ１  

ｓｆｉ   ■  

ｉｄｍ  

ＮｏＲｅｃｐ ｉｎｅＭｕ ｌｔ ｉＢｅｕＭｕ Ｉｔ ｉＢｃＡＭＰＱＡＤＳＵｎ ｉｓ  

－５ ： 将双轴注意力替换为卷枳神经网络时的性能下降幅度  

０ ．８  

０ ．６  

｜   ｍｅａｎｓ － ｅ －ｎｗ  

１ＩＨ  

ａ － 卜０ ．４  

ｄｉｐ ｌｏｍａ －ＩＪ

ｅ －ｎｗ Ｊ  

ｍｍｍｂ

０ ，２  

ＩＬ  

Ｉｔｉｓｂｙｎｏｍｅａｎｓａｄｉｐ ｌｏｍａｍ ｉｌｌ．  

￣６ ： 双轴注意力可视化分布  

６２  

第四章基于统

一表格填充机制端到端框架的结构化情感分析方法  

以往研宄已经论证了卷积神经网络在表格填充方法的有效性 ，然而当表格非  

常大时 ，ＣＮＮ对于快速捕获全局信息是有挑战性的 。将应用在过往研究

［９８ ］的ＣＮＮ  

以及本文提出的双轴向注意力机制进行了实验对比 ，结果如图 ４ －５所示 。结果表  

明替换为ＣＮＮ ， 其在所有五个数据集表现均有所下降 ，这可能是因为ＳＳＡ任务  

中许多句子是比较长的 。 此外如图 ４ －６所示 ， 对输入评论

Ｉｔｉｓｂｙｎｏｍｅａｎｓａ  

ｄｉｐ ｌｏｍａｍｉＵ ． ” 可视化了应用在Ｅ －ＰＯＳ标签所在单元格的双轴向注意力分值 ，结  

果展现了在Ｅ

－ＰＯＳ相关的标签 ， 如 Ｔ －Ｓ和 Ｔ －Ｅ会有较高的注意力分值 ， 这体现  

了双轴注意力机制可以很好地利用标签之间的关联性并有助于识别表格中的单  

元格类型 。  

４ ．５本章小结  

本章开展了基于统

一表格填充机制端到端框架的结构化情感分析方法研宄 ，  

首先介绍了统

一的表格填充机制ＵＳＳＡ ，该机制可以同时解决非连续实体以及重  

叠实体问题 。然后介绍了与该机制配备的模型架构 ，其中的双轴向注意力机制可  

以有效地捕捉表格中行与列的关联信息 。 最后在 ５个公开基准数据集上进行实  

验 ， 实验结果以及分析证明了方法的有效性 。  

６３  

北京邮电大学电子信息硕士学位论文  

第五章多元组细粒度情感分析系统  

基于第三章的针对方面级三元组抽取的机器阅读理解框架以及第四章的针  

对结构化情感分析的端到端框架 ，本章设计了多元组细粒度情感分析系统 ， 旨在  

可视化展示方法效果 ，并让更多人通过该任务了解自然语言处理 。本系统主要采  

用 Ｆｌｕｔｅｒ框架制作前端 ，采用 Ｆｌａｓｋ制作后端ＡＰＩ ，并可以同时发布至多个平台 。  

５ ．１系统需求分析  

文本细粒度情感分析系统的需求主要分为功能需求以及性能需求 。  

在功能需求中主要包含三点 ， 分别如下 ：  

１ ）用户管理 ： 作为

一个面向广大用户的系统 ， 用户管理十分关键 。对于普  

通用户提供了用户注册和用户登陆的功能 ；对于管理员提供了用户信息列表展示  

的功能 。 此外为保证使用体验 ， 还提供了游客模式 ， 可以不用登陆直接进入。  

２）情感分析 ： 该功能包含针对方面级三元组抽取的机器阅读理解方法以及  

针对结构化情感分析的端到端方法 ，也是本系统的核心功能 。用户选择相应的任  

务 ，输入相应句子 ，该模块会调用ＡＰＩ请求后端 ，返回结果后呈现至前端页面 。  

３ ）数据管理 ： 在系统中会产生非常多的数据 ， 对这些数据进行管理可以使  

整个系统更好地运作 。该模块有多个功能 ，包括用户的账号密码等信息会存入数  

据库 ， 系统管理员可以进行增删改查 。此外每个用户的使用历史记录会保存 ，也  

会存储至数据库中方便用户自身以及系统管理员查看 。  

在性能需求中主要包含三点 ， 分别如下 ：  

１ ）跨平台 ：本系统采用 Ｆｌｕｔｅｒ这

一跨平台框架 ， 除了广泛使用的Ｗｅｂ平台  

以外 ， 还发布至Ｍａｃ 、 Ａｎｄｒｏｉｄ平台 ， 故无论用户是桌面端还是移动端都可以更  

好地使用本系统 。  

２）用户体验好 ： 本系统界面简洁明了 ， ＵＩ设计美观并风格统

。 用户可以  

免登录直接进入系统 ，然后针对细粒度情感分析功能的输入都提供了示例 ，并且  

后端响应速度很快 ， 方面级三元组抽取的响应时间平均每条为 ０ ．３３秒 ， 结构化  

情感分析的响应时间平局每条为 ０ ．２４秒 。  

３）高性能 ： 后端采用 Ｆｌａｓｋ这

一款轻量级的 Ｐｙ ｔｈｏｎ后端框架 ， 其提供了简  

单易用的ＡＰＩ ， 使得开发后端变得更加快速和简单 。 并使用 Ｇｕｎｉｃｏｍ这 一基于  

Ｐｙｔｈｏｎ的ＷＳＧＩＨＴＴＰ服务器 ，其可以预派生子进程来处理ＨＴＴＰ请求 ， 可以充  

分利用多核ＣＰＵ的优势 ，从而提供高性能的服务 ，与 Ｆｌａｓｋ内置的开发服务器相  

６４  

第五章 多元组细粒度情感分析系统  

比 ， Ｇｕｎｉｃｏｒｎ能够更好地处理大量并发请求 。 此夕卜 ， 整个项目使用 Ｄｏｃｋｅｒ容器  

化部署 ，可以快速在任意环境下开箱即用 ，不受当前操作环境以及配置依赖的影  

响 。  

５ ．２系统整体设计  

用户登陆  

－一一＾  

广（ＳＱＬｉｔｅ）

／Ｓ户注册  

＿＿＿＿＿＿  

－＾＾］ 情感分析

＾   ＥＭｆ ｃＥＥＳ结Ｓ化情感 ！  

丨 分析ｐ

ｒ＼ ＿ Ｉ   

文本细粒度情感 ＿ 用户信息  

分析系统ｆ据管气  

ＪＬ（ＳＱＬｉｔｅ）  

—二一＾ Ｉ：Ｊ

：历史１Ｓ彔  

ｒ  方面级三元组抽取ＡＰ 丨    匕＝二 ：：： ：  

结构化情感分析ＡＰ Ｉ  

１ ： 系统整体设计  

本系统主要采用前后端分离的技术 ，前端部分采用 Ｆｌｕｔｅｒ框架 ，其使用内置  

渲染引擎来达到原生应用的性能效果 ，从而可以发布至多个平台并保持性能不俗 。  

前端以 Ｄａｒｔ为语言 ， 利用 Ｆｌｕｔｔ ｅｒ 内置组件构建整个前端页面 。 数据存储采用  

ｓｑ ｆｌｉｔｅ ，这是轻量级 ＳＱＬｉｔｅ数据库包 ，可以使得使得Ｆｌｕｔｔｅｒ应用程序可以在本地  

存储和管理数据 ， 并且也支持跨平台 ， 其可以在Ａｎｄｒｏｉｄ和 ｉＯＳ平台上运行 ， 也  

可以在桌面应用程序上运行 。 后端主要使用 Ｐｙ ｔｈｏｎ语言进行开发 ， 首先使用  

ＰｙＴｏｒｃｈ框架搭建深度学习模型 ， 并利用 Ｆｌａｓｋ以及Ｇｕｎｉｃｏｒｎ来将模型预测函数  

封装为ＡＰＩ方便前端调用 。  

６５  

北京邮电大学电子信息硕士学位论文  

Ｆｌｕｔｔｅｒ跨平台前端框架ｎａｓｋ轻量级ｐｙｔｈｏｎ后端框架  

Ｄａｒｔ作为ＦｈＭｅｒ前端语言后端处理ＨＴＴＰ并发请求  

Ｍｉｅｒｎｋｍ注意力机制Ｐｙ ｔｈａｒ＼作为后端语言    一— 一   

模型ＢＥＲＴ双 编码器数据库

（ ＳＱＬ＿

：轻量级数据库   １ … 」  

ＰｙＴｏｒｃｈ深度学习框架部署

＾ Ｄｏｃｋｅｒ容航觸    ｛ ）   

－２ ： 多元组细粒度情感分析技术架构图  

５ ．３前端部分设计  

用户初次登陆系统 ， 可以选择登陆 、注册 、游客模式这三个选项 ， 其中登陆  

功能会检测数据库看是否存在该用户 ， 若存在该用户且密码正确 ， 则登录成功 ；  

若用户存在但密码错误 ， 则登录失败 ； 若用户不存在则会自动创建用户 。注册功  

能会检测数据库 ，只有不存在该用户才会注册成功 。游客模式下会自动创建令牌 ，  

直接可以进入系统 。  

＇，  

－ ３ ： 注册登陆界面  

进入系统后主页如下 ：  

６６  

第五章 多元组细粒度情感分析系统  

？ｏ？ ｔｃａ ． ，

＜ ？ａＡ  

－４ ： 系统主页  

可以看到页面正中间有方面级情感分析以及结构化情感分析两大核心功能  

的按钮 ，此外上方的导航栏也有相关按钮 ，其中导航栏中的ＡＳＴＥ是指方面级三  

元组抽取 ， 也是方面级情感分析的

一种 ， ＳＳＡ是结构化情感分析 。 点击

“ 方面级  

” 按钮 ， 进入如下画面 ：  

ＡＳＴ￡ＳＳＡ   —   Ｎ ｉｃｅａｍｂｅｎｃｅ

，ｂｕｔｈ ｉｇｈ ｌｙｏｖｅｒｒａｔｅｄｐ ｌａｃｅ  

Ｂｍｉｊ  

ＩｉＩ  

－５ ： 方面级三元组抽取任务示例  

方面级三元组会抽取出句子中的 （方面词 、 意见词 、情感极性 ）这样的三元  

组 ， 输入样例

“ Ｎ ｉｃｅａｍｂｉｅｎｃｅ ，ｂｕｔｈｉｇｈｌｙｏｖｅｒｒａｔｅｄ ｐ ｌａｃｅ ．

， 点击解析按钮 ， 会得  

到三元组为 （ａｍｂｉｅｎｃｅ ，Ｎｉｃｅ ，ＰＯＳ ） 以及 （ｐ ｌａｃｅ ，ｏｖｅｒｒａｔｅｄ ，ＮＥＧ ） 。 点击上方的ＳＳＡ  

按钮或者返回上

一级重新点击结构化情感分析按钮 ， 进入如下页面 ：  

６７  

北京邮电大学电子信息硕士学位论文  

Ｈ〇＾５￡ＳＳＡ   Ｍ   Ｍｙ ｅｍｐ

ｌｏｙｅ ｒｆｏｕｎｄ

ｙｏｕ ｆｗ〇ｆｄｓ ｔｃ ｂｅ ｒｅ ｉａｎｙ

ｒｕｄｅ ￡  

Ｍｙ ｅｍｐ ｌｏｙｅ ｒ

ｙｏｕ ｒｗｃ ＾Ｏｂｎｅ ｒｇａｌｖｅ ｉ ，

＜＞   ＨＨ＾ｉｓｈｈｉｈｈｉｈ＾＾＾＾Ｉ  

－ ６ ： 结构化情感分析示例  

结构化情感分析会抽取出句子中的 （持有者 、 目标 、表达 、极性 ）这样的四  

元组 ，输入样例

Ｍｙｅｍｐ ｌｏｙｅｒｆｏｕｎｄ ｙｏｕｒｗｏｒｄｓｔｏｂｅｒｅｌａｌｌｙｒｕｄｅａｎｄｌｏｎｇｗｉｎｄｅｄ ．

点击解析按钮 ， 会得到四元组为 （Ｍｙｅｍｐ ｌｏｙｅｒ ，ｙｏｕｒｗｏｒｄｓ ，ｒｅａｌｌｙｒｕｄｅ ，  

ｎｅｇａｔｉｖｅ），（Ｍｙｅｍｐ ｌｏｙｅｒ ，ｙｏｕｒｗｏｒｄｓ，ｒｅａｌｌｙ ｌｏｎｇｗｉｎｄｅｄ ，ｎｅｇａｔｉｖｅ） 〇  

５ ．４后端部分设计  

后端系统主要包括数据处理 、 调用模型 、 结果输出等功能 。  

１ ）数据处理 ．

？ 后端接收到用户的输入数据后 ， 首先会进行数据的预处理 ，  

包括去除无用的空格 ，数据过长截断等 ，然后将输入语句进行分词转化为 ｔｏｋｅｎ ，  

最后变为文本输入向量 。  

２ ） 调用模型 ： 首先会经过

一个小的数据判别模型用来判断输入语句来源的  

领域 ，这个判别模型是利用不同数据集的训练语料训练而来 ， 目前对于方面级三  

元组抽取任务有 Ｒｅｓｔａｕｒａｎｔ和 Ｌａｐｔｏｐ两大领域 ， 对于结构化情感分析任务有旅  

馆 、新闻 、大学以及评论领域 。在确定是某个数据集后 ， 然后调用在与该领域最  

相近的数据集训练得到的模型作为输出 。  

３ ） 结果输出 ： 两个任务输出都是元组形式 ， 只不过单个元组内部元素数量  

不同 。例如针对方面级三元组抽取任务 ，输入

“ Ｎ ｉｃｅａｍｂｉｅｎｃｅ， ｂｕｔｈｉｇｈｌｙｏｖｅｒｒａｔｅｄ  

ｐ ｌａｃｅ ．

” 的三元组为 （ａｍｂｉｅｎｃｅ ，Ｎｉｃｅ ，ＰＯＳ） 以及 （ｐ ｌａｃｅ，ｏｖｅｒｒａｔｅｄ ，ＮＥＧ） ， 针对结构  

化情感分析任务 ， 输入

Ｍｙｅｍｐ ｌｏｙｅｒｆｏｕｎｄ ｙｏｕｒｗｏｒｄｓｔｏｂｅｒｅｌａｌｌｙｒｕｄｅａｎｄ［ｏｎｇ  

６８  

第五章 多元组细粒度情感分析系统  

ｗｉｎｄｅｄ ？

” 的四元组为（Ｍｙｅｍｐ ｌｏｙｅｒ ，ｙｏｕｒｗｏｒｄｓ ， ｒｅａｌ ｌｙｒｕｄｅ ，ｎｅｇａｔｉｖｅ） ，（Ｍｙｅｍｐ ｌｏｙｅｒ ，  

ｙｏｕｒｗｏｒｄｓ ， ｒｅａｌｌｙ ｌｏｎｇｗｉｎｄｅｄ ，ｎｅｇａｔｉｖｅ） 。  

５ ．５系统工作流程  

将之前所描述的流程总结如下 ：首先用户通过登录 、注册以及游客模式来进  

入细粒度情感分析系统 ，期间用户的信息会存入数据库中 。该系统主要包含两个  

功能 ，方面级三元组抽取以及结构化情感分析两个功能 ，这两个功能会分别调用  

后端的方面级三元组抽取ＡＰＩ以及结构化情感分析ＡＰＩ ，期间的操作历史也会存  

入数据库中 。在后端中 ，通过数据处理 、模型调用以及结果输出三部分构建方面  

级三元组抽取ＡＰＩ以及结构化情感分析ＡＷ用于给前端调用 。 整体流程图如下  

所示 ：  

用户注册★ 用户登录ｈ＞ 多元 情感  

？Ｌ— 一 丁

． ■ … — Ｊ  

＊§  

： 广＾＾ ＾ ； ，  

游客模式

；方面级三元组 结构化情感  

一 １＊抽取分析  

；  ＩＪ  

？个个  

用户信息检测

＇   ｆ ＼   １ ；  

一一一＾操作历史存储与展示  

ＳＱＬｉｔｅ？  操作历史存储与展示  

Ｖ＿Ｊ＜ －   

方面级三元组结构化情感   柚取ＡＰ Ｉ ＞ 分析ＡＰ ！  

、－＇ ■， 令  

： 数据处理 ｜  模型调用 １ 结果＿出 ｜  

、匕一一 Ｊ 匕― — Ｊ匕 ― ． ． ． －— －＾  

－７ ： 多元组细粒度情感分析系统流程  

６９  

北京邮电大学电子信息硕士学位论文  

５ ．６本章小结  

本章基于第三章的针对方面级三元组抽取的机器阅读理解框架以及第四章  

的针对结构化情感分析的端到端框架设计了文本细粒度情感分析系统 。对于两个  

任务 ，在前端部分用户都只需输入

一句话 ，后端算法会自动检测输入语句所在领  

域并进行分析并自动化选择合适的模型进行预测 。本章重点介绍了系统的需求分  

析、前后端模块设计以及系统整体的工作流程 。  

７０  

第六章总结与展望  

第六章总结与展望  

６ ． １工作总结  

本文开展了基于掩码上下文机器阅读理解框架的方面级三元组抽取以及基  

于统 一表格填充机制端到端框架的结构化情感分析方法研宄 ，方面级三元组抽取  

以及结构化情感分析两个任务都是情感分析领域内流行的两个多元组细粒度情  

感分析任务 。  

方面级三元组抽取任务旨在提取句子中的 （方面词 、意见词 、情感极性）三  

元组 ，现有的深度学习方法主要分为阶段式、端到端式以及机器阅读理解式 ，而  

端到端式以及机器阅读理解式方法表现相对更好 。现有的机器阅读理解方法仍存  

在模块间缺乏交互、训练数据量不足以及多个方面词之间存在干扰的问题 ，本文  

分别设计了掩码式数据增强、交互式判别模型以及阶段式推理方法来分别解决以  

上问题 ， 统称为基于掩码上下文的机器阅读理解 （ＣＯｎｔｅｘｔｅｄＭａｓｋｅｄＭａｃｈｉｎｅ  

ＲｅａｄｉｎｇＣｏｍｐｒｅｈｅｎｓｉｏｎ ， ＣＯＭ －ＭＲＣ）框架 。掩码式数据増强是通过设置固定的  

查询以及掩码方面词后的上下文进行简单有效的数据增强 ；交互式判别模型包括  

方面词提取模块、意见词提取模块、情感判别模块以及方面词探测模块 ，并允许  

各模块之间交互信息 ；阶段式推理方法首先推理方面词 ，然后推理方面词对应的  

意见词和情感 ，推理时均通过掩码方面词进行 ，从而可以减少无关方面词的千扰 。  

实验在两组ＡＢＳＡ数据集上进行 ， 实验结果证明了ＣＯＭ －ＭＲＣ框架的有效性 。  

结构化情感分析任务旨在提取句子中的 （持有者、 目标、表达、极性 ） 四元  

组 ，现有的深度学习方法主要是将该任务视为双词汇依赖解析问题 ，然而这些方  

法并不能同时处理重叠以及非连续问题 。本文首先构造了

一种双词汇依赖解析图 ，  

该图包含两类有向边， 即

“ 关系预测

” 以及 “ 单词提取

， 它们分别对应解决了  

重叠和非连续问题 。然后本文将双词汇依赖解析转化为

一的表格填充机制 ，  

命名为ＵＳＳＡ（Ｕｎｉｆ ｉｅｄＴａｂｌｅＦｉｌｌｉｎｇＳｃｈｅｍｅｆｏｒＳｔｒｕｃｔｕｒｅｄＳｅｎｔｉｍｅｎｔ Ａｎａｌｙｓｉｓ） 。在  

该表格填充机制里 ，ＲＰ与ＴＥ分别对应表格的左下三角部分以及右上三角部分 。  

最后 ， 为了很好地适配ＵＳＳＡ ，本文构造了

一个端到端模型进行训练和预测 。在  

该模型中 ，本文提出

一种双轴注意力模块去有效地捕捉表格中的行与列的关联信  

息 。实验在ＮｏＲｅＣＦｉｎｅ、ＭｕｌｔｉＢＥＵ、ＭｕｌｔｉＢｃＡ、ＭＰＱＡ以及０８触五个基准测试集  

上进行 ， 实验结果证明了该方法的有效性 。  

７１  

北京邮电大学电子信息硕士学位论文  

６ ．２未来工作展望  

尽管本文对方面级三元组抽取和结构化情感分析任务分别应用机器阅读理  

解和端到端框架开展研宄并取得

一定成果 ， 但这两种方法仍有

一定不足之处 ：  

１ ）在方面级三元组抽取任务中如何利用机器阅读理解方法进行端到端推理？  

本文构建的掩码式机器阅读理解方法尽管在训练时是端到端的 ，但在推理时  

仍然是阶段性的 ，存在错误累计传播、计算开销大等何题 。具体来说推理分为两  

？方面词推理阶段以及方面词附属物推理阶段 ，在方面词推理阶段时若未  

推理出正确的方面词 ，那么在方面词附属物推理阶段推理得到的意见词和情感极  

性也毫无意义 。此外因为推理是分阶段的 ，并非

一步到位 ， 因此推理计算开销相  

对于端到端推理开销更大 。如何利用机器阅读理解方法进行端到端的推理在当今  

研究中仍具挑战 。  

２ ）在结构化情感分析任务中如何构建完全等价的双词汇依赖解析图？  

本文所提出的基于统

一表格填充机制端到端框架是基于所构建的双词汇依  

赖解析图 ，该图可以解决大部分重叠以及非连续实体问题 ，但在

一些极特殊情况  

下仍无法等价地构建该图 。 例如如果在

一句话中同时存在 ｂｅ，ｐ）以及  

（ｈ２ ， ｔ２ ，ｅ ，ｐ）

， 由此构建的双词汇依赖解析图可能与其本身并不等价 。如何在存在  

非连续以及重叠实体的困难下来构建完全等价的双词汇依赖解析图仍具挑战 。  

除了本文开展的研宄工作外 ，方面级三元组抽取任务和结构化情感分析任务  

仍有很多有价值且具有挑战性的方向值得深入研究 ：  

１）如何利用大规模语言模型统

一解决细粒度情感分析问题？  

近期诸如ＣｈａｔＧＰＴ、 ＧＰＴ４等大规模语言模型进入大家的视野 ， 这些大模型  

预先在海量文本上进行过预训练 ，甚至在人类交互下进行过强化学习 ，表现让人  

非常惊艳 。那么如何借助这

一类大模型统 一解决包括方面级三元组抽取以及结构  

化情感分析在内的细粒度情感分析任务？如何减少大模型使用的时间成本以及  

金钱成本？如何对大模型进行压缩便于线上推理？这些都是非常有希望有前途  

的研宄方向 。  

多元组细粒度情感分析任务是自然语言处理领域内新兴的研宄方向之

，是  

非常重要且极具挑战性的任务 ， 需要大量尝试以及不断探索 。  

７２  

参考文献  

参考文献  

［１ ］Ｋ．ＳｃｈｏｕｔｅｎａｎｄＦ．Ｆｒａｓｉｎｃａｒ ，

＾ Ｓｕｒｖｅｙｏｎａｓｐｅｃｔ － ｌｅｖｅｌｓｅｎｔｉｍｅｎｔａｎａｌｙｓｉｓ，

ＩＥＥＥ  

Ｔｒａｎｓ．Ｋｎｏｗｌ ．ＤａｔａＥｎｇ

． ｓｖｏｌ ．２８ ，ｎｏ ．３ ，ｐｐ

．８１３ －８３０ ，２０１６ ．  

［２］ＷｅｎｙａＷａｎｇ，ＳｉｎｎｏＪｉａｌｉｎＰａｎ ？ＤａｎｉｅｌＤａｈｌｍｅｉｅｒ ，ａｎｄＸｉａｏｋｕｉＸｉａｏ ，２０１６ａ．  

Ｒｅｃｕｒｓｉｖｅｎｅｕｒａｌｃｏｎｄｉｔｉｏｎａｌｒａｎｄｏｍｆ ｉｅｌｄｓｆｏｒａｓｐｅｃｔ －ｂａｓｅｄｓｅｎｔｉｍｅｎｔａｎａｌｙｓｉｓ ．Ｉｎ  

Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２０１６ＣｏｎｆｅｒｅｎｃｅｏｎＥｍｐ ｉｒｉｃａｌＭｅｔｈｏｄｓｉｎＮａｔｕｒａｌＬａｎｇｕａｇｅ  

Ｐｒｏｃｅｓｓｉｎｇ，ｐａｇｅｓ６１６ －６２６ ，Ａｕｓｔｉｎ ，Ｔｅｘａｓ．ＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌ  

Ｌｉｎｇｕｉｓｔｉｃｓ．  

［３ ］ＤｅｈｏｎｇＭａ ，Ｓｕｊ ｉａｎＬｉ ，ａｎｄＨｏｉｘｆｅｎｇＷａｎｇ

．２０１８ ．Ｊｏｉｎｔｌｅａｒｎｉｎｇｆｏｒｔａｒｇｅｔｅｄ  

ｓｅｎｔｉｍｅｎｔａｎａｌｙｓｉｓ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２０１８ＣｏｎｆｅｒｅｎｃｅｏｎＥｍｐ ｉｒｉｃａｌＭｅｔｈｏｄｓ  

ｉｎＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ，ｐａｇｅｓ４７３７

－４７４２ ，Ｂｒｕｓｓｅｌｓ ，Ｂｅｌｇ ｉｕｍ．Ａｓｓｏｃｉａｔｉｏｎ  

ｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ．  

［４ ］Ｄ ．Ｎａｄｅａｕ ，Ｓ ．Ｓｅｋｉｎｅ ，Ａｓｕｒｖｅｙｏｆｎａｍｅｄｅｎｔｉｔｙｒｅｃｏｇｎｉｔｉｏｎａｎｄｃｌａｓｓｉｆｉｃａｔｉｏｎ ，ｉｎ：  

ＬｉｎｇｖｉｓｔｉｃａｅＩｎｖｅｓｔｉｇａｔｉｏｎｅｓ ，２００７ ．  

［５ ］ＬａｉＶＤ ．ＥｖｅｎｔＥｘｔｒａｃｔｉｏｎ：ＡＳｕｒｖｅｙ［Ｊ ］

．ａｒＸｉｖ ｐｒｅｐｒ ｉｎｔａｒＸｉｖ：２２１０ ．０３４１９ ，２０２２ ．  

［６ ］ＱｉｕＧ ，ＬｉｕＢ，ＢｕＪ ？ｅｔａｌ．Ｏｐ ｉｎｉｏｎｗｏｒｄｅｘｐａｎｓｉｏｎａｎｄｔａｒｇｅｔｅｘｔｒａｃｔｉｏｎｔｈｒｏｕｇｈ  

ｄｏｕｂｌｅ ｐｒｏｐａｇａｔｉｏｎ．Ｃｏｍｐｕｔａｔｉｏｎａｌｌｉｎｇｕｉｓｔｉｃｓ，２０１ １ ，３７（１）

：９ －２７．  

［７ ］ＬｉＸ ，ＬａｍＷ．Ｄｅｅｐｍｕｌｔｉ －ｔａｓｋｌｅａｒｎｉｎｇｆｏｒａｓｐｅｃｔｔｅｒｍｅｘｔｒａｃｔｉｏｎｗｉｔｈｍｅｍｏｒｙ  

ｉｎｔｅｒａｃｔｉｏｎ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆ ｔｈｅ２０１７ｃｏｎｆｅｒｅｎｃｅｏｎｅｍｐ ｉｒｉｃａｌｍｅｔｈｏｄｓｉｎｎａｔｕｒａｌ  

ｌａｎｇｕａｇｅｐｒｏｃｅｓｓｉｎｇ，２０１７ ：２８８６ －２８９２ ．  

［８ ］ＬｉＸ ，ＢｉｎｇＬ ？ＬｉＰ ？ｅｔａｌ ．Ａｓｐｅｃｔｔｅｒｍｅｘｔｒａｃｔｉｏｎｗｉｔｈｈｉｓｔｏｒｙａｔｔｅｎｔｉｏｎａｎｄｓｅｌｅｃｔｉｖｅ  

ｔｒａｎｓｆｏｒｍａｔｉｏｎ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２７ｔｈＩｎｔｅｒ ｎａｔｉｏｎａｌＪｏｉｎｔＣｏｎｆｅｒｅｎｃｅｏｎ  

ＡｒｔｉｆｉｃｉａｌＩｎｔｅｌｌｉｇｅｎｃｅ ，Ｓｔｏｃｋｈｏｌｍ ，Ｓｗｅｄｅｎ ；ＡＡＡＩＰｒｅｓｓ ．２０１８ ：４１９４—４２００ ．  

［９ ］ＦａｎＺ ，ＷｕＺ ，ＤａｉＸ，ｅｔａｌ ．Ｔａｒｇｅｔ －ｏｒ ｉｅｎｔｅｄｏｐ ｉｎｉｏｎｗｏｒｄｓｅｘｔｒａｃｔｉｏｎｗｉｔｈｔａｒｇｅｔ －  

ｆ ｉｉｓｅｄｎｅｕｒａｌｓｅｑｕｅｎｃｅｌａｂｅｌｉｎｇ

．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２０１９ＣｏｎｆｅｒｅｎｃｅｏｆｔｈｅＮｏｒｔｈ  

ＡｍｅｒｉｃａｎＣｈａｐｔｅｒｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ：Ｈｍｎａｎ  

ＬａｎｇｕａｇｅＴｅｃｈｎｏｌｏｇ ｉｅｓ ，Ｖｏｌｕｍｅ１（ＬｏｎｇａｎｄＳｈｏｒｔＰａｐｅｒｓ），２０１９ ：２５０９ －２５１８ ．  

［ １０］ＹａｎｇＢ ，ＣａｒｄｉｅＣ ．Ｅｘｔｒａｃｔｉｎｇｏｐ ｉｎｉｏｎｅｘｐｒｅｓｓｉｏｎｓｗｉｔｈｓｅｍｉ －ｍａｒｋｏｖｃｏｎｄｉｔｉｏｎａｌ  

ｒａｎｄｏｍｆ ｉｅｌｄｓ ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２０１２ＪｏｉｎｔＣｏｎｆｅｒｅｎｃｅｏｎＥｍｐ ｉｒｉｃａｌＭｅｔｈｏｄｓ  

ｉｎＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇａｎｄＣｏｍｐｕｔａｔｉｏｎａｌＮａｔｕｒａｌＬａｎｇｕａｇｅＬｅａｒｎｉｎｇ，  

２０１２ ：１３３５ －１３４５ ．  

７３  

北京邮电大学电子信息硕士学位论文  

［１ １ ］ＹａｎｇＢ ，ＣａｒｄｉｅＣ ．Ｊｏｉｎｔｉｎｆｅｒｅｎｃｅｆｏｒｆｉｎｅ － ｇｒａｉｎｅｄｏｐ ｉｎｉｏｎｅｘｔｒａｃｔｉｏｎ．Ｉｎ  

Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ５１ｓｔＡｎｎｕａｌＭｅｅｔｉｎｇｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌ  

Ｌｉｎｇｕｉｓｔｉｃｓ （Ｖｏｌｕｍｅ１ ：ＬｏｎｇＰａｐｅｒｓ），２０１３ ：１６４０ －１６４９ ．  

［ １２］ ＤｏｎｇＬ ，ＷｅｉＦ ，ＴａｎＣ ，ｅｔａｌ ．Ａｄａｐｔｉｖｅｒｅｃｕｒｓｉｖｅｎｅｕｒａｌｎｅｔｗｏｒｋｆｏｒｔａｒｇｅｔ －ｄｅｐｅｎｄｅｎｔ  

ｔｗｉｔｔｅｒｓｅｎｔｉｍｅｎｔｃｌａｓｓｉｆｉｃａｔｉｏｎ ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ５２ｎｄａｎｎｕａｌｍｅｅｔｉｎｇｏｆｔｈｅ  

ａｓｓｏｃｉａｔｉｏｎｆｏｒｃｏｍｐｕｔａｔｉｏｎａｌｌｉｎｇｕｉｓｔｉｃｓ（ｖｏｌｕｍｅ２ ：Ｓｈｏｒｔ

ｐａｐｅｒｓ），２０１４ ：４９ －５４ ．  

［１３ ］ＭａＤ ，ＬｉＳ ，ＺｈａｎｇＸ ５ｅｔａｌ ．Ｉｎｔｅｒａｃｔｉｖｅａｔｔｅｎｔｉｏｎｎｅｔｗｏｒｋｓｆｏｒａｓｐｅｃｔ － ｌｅｖｅｌ  

ｓｅｎｔｉｍｅｎｔｃｌａｓｓｉｆｉｃａｔｉｏｎ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆ ｔｈｅ２６ｔｈＩｎｔｅｒ ｎａｔｉｏｎａｌＪｏｉｎｔＣｏｎｆｅｒｅｎｃｅ  

ｏｎＡｒｔ ｉｆ ｉｃｉａｌＩｎｔｅｌｌｉｇｅｎｃｅ ，Ｍｅｌｂｏｕｒ ｎｅ ，Ａｕｓｔｒａｌｉａ ；ＡＡＡＩＰｒｅｓｓ ．２０１７ ；４０６８—４０７４ ．  

［１４］ ＷａｎｇＹ ？ＨｕａｎｇＭ ，ＺｈｕＸ ５ｅｔａｌ ．Ａｔｔｅｎｔｉｏｎ －ｂａｓｅｄＬＳＴＭｆｏｒａｓｐｅｃｔ － ｌｅｖｅｌｓｅｎｔｉｍｅｎｔ  

ｃｌａｓｓｉｆｉｃａｔ ｉｏｎ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２０１６ｃｏｎｆｅｒｅｎｃｅｏｎｅｍｐ ｉｒｉｃａｌｍｅｔｈｏｄｓｉｎ  

ｎａｔｕｒａｌｌａｎｇｕａｇｅ ｐｒｏｃｅｓｓｉｎｇ，２０１６ ：６０６ －６１５ ．  

［ １５ ］Ｗｉｅｂｅ，Ｊ．Ｍ ． ，Ｂｒｕｃｅ ，Ｒ．Ｆ． ？ａｎｄＯ

＾Ｈａｒａ ，Ｔ．Ｐ．Ｄｅｖｅｌｏｐｍｅｎｔａｎｄｕｓｅｏｆａｇｏｌｄ －  

ｓｔａｎｄａｒｄｄａｔａｓｅｔｆｏｒｓｕｂ ｊｅｃｔｉｖｉｔｙｃｌａｓｓｉｆｉｃａｔｉｏｎｓ ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆ ｔｈｅ３７ｔｈａｎｎｕａｌ  

ｍｅｅｔｉｎｇｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔ ｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓｏｎＣｏｍｐｕｔａｔｉｏｎａｌ  

Ｌｉｎｇｕｉｓｔｉｃｓ ，ｐｐ

．２４６ －２５３ ．ＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ，１９９９．  

［１６ ］ Ｗｉｅｂｅ ，Ｊ ．Ｌｅａｒｎｉｎｇｓｕｂｊｅｃｔｉｖｅａｄ ｊｅｃｔｉｖｅｓｆｒｏｍｃｏｒｐｏｒａ．ＩｎＫａｕｔｚ ，Ｈ ．Ａ ．ａｎｄＰｏｒｔｅｒ ，  

Ｂ ．Ｗ．（ｅｄｓ ． ）？ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＳｅｖｅｎｔｅｅｎｔｈＮａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＡｒｔ ｉｆｉｃｉａｌ  

ＩｎｔｅｌｌｉｇｅｎｃｅａｎｄＴｗｅｌｆ ｔｈＣｏｎｆｅｒｅｎｃｅｏｎｏｎＩｎｎｏｖａｔｉｖｅＡｐｐ ｌｉｃａｔｉｏｎｓｏｆＡｒｔｉｆｉｃｉａｌ  

Ｉｎｔｅｌｌｉｇｅｎｃｅ ＾Ｊｕｌｙ３０ － Ａｕｇｕｓｔ３ ，２０００ ，Ａｕｓｔｉｎ ，Ｔｅｘａｓ ，ＵＳＡ ，ｐｐ

．７３５ －７４０ ．ＡＡＡＩ  

Ｐｒｅｓｓ／ＴｈｅＭＩＴＰｒｅｓｓ ，２０００ ．  

［１７ ］Ｈａｔｚｉｖａｓｓｉｌｏｇ ｌｏｕ，Ｖ．ａｎｄＷｉｅｂｅ，Ｊ ．Ｍ ．Ｅｆｆ ｅｃｔｓｏｆａｄｊｅｃｔｉｖｅｏｒ ｉｅｎｔａｔｉｏｎａｎｄ  

ｇｒａｄａｂｉｌｉｔｙｏｎｓｅｎｔｅｎｃｅｓｕｂｊｅｃｔｉｖｉｔｙ

．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ１８ｔｈｃｏｎｆｅｒｅｎｃｅｏｎ  

Ｃｏｍｐｕｔａｔｉｏｎａｌｌｉｎｇｕｉｓｔｉｃｓ －Ｖｏｌｕｍｅ１ ，ＰＰ

－２９９ －３０５ ．ＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌ  

Ｌｉｎｇｕｉｓｔｉｃｓ ，２０００．  

［１８ ］Ｍｅｂｅ ，Ｊ．Ｍ ．Ｔｒａｃｋｉｎｇｐｏｉｎｔｏｆｖｉｅｗｉｎｎａｒｒａｔｉｖｅ ．ＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ，  

２０（２） ：２３３ －２８７？１９９４ ．  

［１９ ］Ｔｏｎｇ，Ｒ．Ｍ ．Ａｎｏｐｅｒａｔｉｏｎａｌｓｙｓｔｅｍｆｏｒｄｅｔｅｃｔｉｎｇａｎｄｔｒａｃｋｉｎｇｏｐ ｉｎｉｏｎｓｉｎｏｎ

－ ｌｉｎｅ  

ｄｉｓｃｕｓｓｉｏｎ．ＩｎＷｏｒｋｉｎｇＮｏｔｅｓｏｆｔｈｅＡＣＭＳＩＧＩＲ２００１ＷｏｒｋｓｈｏｐｏｎＯｐｅｒａｔｉｏｎａｌ  

ＴｅｘｔＣｌａｓｓｉｆｉｃａｔ ｉｏｎ，ｖｏｌｕｍｅ１ ，２００１ ．  

［２０ ］Ｍｏｒｉｎ＾ａ，Ｓ ． ，Ｙａｍａｎｉｓｈｉ ，Ｋ． ？Ｔａｔｅｉｓｈｉ ５Ｋ． ，ａｎｄＦｕｋｕｓｈｉｍａ ，Ｔ．Ｍｉｎｉｎｇｐｒｏｄｕｃｔ  

ｒｅｐｕｔａｔ ｉｏｎｓｏｎｔｈｅｗｅｂ ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅｅｉｇｈｔｈＡＣＭＳＩＧＫＤＤｉｎｔｅｒ ｎａｔｉｏｎａｌ  

ｃｏｎｆｅｒｅｎｃｅｏｎＫｎｏｗｌｅｄｇｅｄｉｓｃｏｖｅｒｙａｎｄｄａｔａｍｉｎｉｎｇ，ｐｐ

－３４９ ．ＡＣＭ ，２００２ ．  

７４  

参考文献  

［２ １］Ｎａｓｕｋａｗａ，Ｔ．ａｎｄＹｉ ，Ｊ．Ｓｅｎｔｉｍｅｎｔａｎａｌｙｓｉｓ ：Ｃａｐｔｕｒｉｎｇｆａｖｏｒａｂｉｌｉｔｙｕｓｉｎｇｎａｔｕｒａｌ  

ｌａｎｇｕａｇｅｐｒｏｃｅｓｓｉｎｇ

．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２ｎｄｉｎｔｅｒ ｎａｔｉｏｎａｌｃｏｎｆｅｒｅｎｃｅｏｎ  

Ｋｎｏｗｌｅｄｇｅｃａｐ ｔｕｒｅ ，ｐｐ

－７７ ．ＡＣＭ ，２００３ ．  

［２２］Ｔｕｍｅｙ，Ｐ．Ｄ ．Ｔｈｕｍｂｓｕｐｏｒｔｈｕｍｂｓｄｏｗｎ？ｓｅｍａｎｔｉｃｏｒ ｉｅｎｔａｔ ｉｏｎａｐｐ ｌｉｅｄｔｏ  

ｕｎｓｕｐｅｒｖｉｓｅｄｃｌａｓｓｉｆｉｃａｔｉｏｎｏｆ ｒｅｖｉｅｗｓ ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆ ｔｈｅ４０ｔ ｉｈＡｎｎｕａｌＭｅｅｔｉｎｇ  

ｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ，Ｊｕｌｙ６

－ １２ ，２００２ ？Ｐｈｉｌａｄｅｌｐｈｉａ ，  

ＰＡ ５ＵＳＡ ，ｐｐ

．４１７ －４２４ ．ＡＣＬ ，２００２．  

［２３］Ｈａｔｚｉｖａｓｓｉｌｏｇ ｌｏｕ？Ｖ．ａｎｄＭｃＫｅｏｗｎ ，Ｋ ．Ｒ．Ｐｒｅｄｉｃｔｉｎｇｔ ｉｉｅｓｅｍａｎｔｉｃｏｒ ｉｅｎｔａｔｉｏｎｏｆ  

ａｄｊｅｃｔ ｉｖｅｓ ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ３５ｔｈａｎｎｕａｌｍｅｅｔｉｎｇｏｆｔ ｉｉｅａｓｓｏｃｉａｔｉｏｎｆｏｒ  

ｃｏｍｐｕｔａｔｉｏｎａｌｌｉｎｇｕｉｓｔｉｃｓａｎｄｅｉｇｈｔｈｃｏｎｆｅｒｅｎｃｅｏｆｔｈｅｅｕｒｏｐｅａｎｃｈａｐｔｅｒｏｆｔｈｅ  

ａｓｓｏｃｉａｔｉｏｎｆｏｒｃｏｍｐｕｔａｔｉｏｎａｌｌｉｎｇｕｉｓｔｉｃｓ ，ｐｐ

－ １８ １ ．Ａｓｓｏｃｉａｔｉｏｎｆｏｒ  

ＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ，１９９７ ．  

［２４］Ｌｉｕ ，Ｂ ．ＳｅｎｔｉｍｅｎｔＡｎａｌｙｓｉｓａｎｄＯｐ ｉｎｉｏｎＭｉｎｉｎｇ

．ＳｙｎｔｈｅｓｉｓＬｅｃｔｕｒｅｓｏｎＨｕｍａｎ  

ＬａｎｇｕａｇｅＴｅｃｈｎｏｌｏｇ ｉｅｓ ．Ｍｏｒｇａｎ＆ＣｌａｙｐｏｏｌＰｕｂｌｉｓｈｅｒｓ，２０１２ ．  

［２５］Ｌｉｕ，Ｂ ．Ｓｅｎｔｉｍｅｎｔａｎａｌｙｓｉｓａｎｄｓｕｂｊｅｃｔｉｖｉｔｙ

．ＩｎＩｎｄｕｒｋｈｙａ，Ｎ ．ａｎｄＤａｍｅｒａｎ，Ｆ．Ｊ ．  

（ｅｄｓ ．）５ＨａｎｄｂｏｏｋｏｆＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ，ＳｅｃｏｎｄＥｄｉｔｉｏｎ ，ｐｐ

．６２７ －６６６．  

ＣｈａｐｍａｎａｎｄＨａｌｌ／ＣＲＣ ５２０１０．  

［２６ ］Ｐａｎｇ，Ｂ ．ａｎｄＬｅｅ ，Ｌ ．Ａｓｅｎｔｉｍｅｎｔａｌｅｄｕｃａｔｉｏｎ：Ｓｅｎｔｉｍｅｎｔａｎａｌｙｓｉｓｕｓｉｎｇｓｕｂ ｊｅｃｔｉｖｉｔ ｙ  

ｓｕｍｍａｒｉｚａｔｉｏｎｂａｓｅｄｏｎｍｉｎｉｍｕｍｃｕｔｓ ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ４２ｎｄａｎｎｕａｌｍｅｅｔｉｎｇ  

ｏｎＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ，ｐｐ

．２７１ ．Ａｓｓｏｃｉａｔｉｏｎｆｏｒ  

Ｃｏｍｐｕｔａｔ ｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ，２００４ ．  

［２７ ］Ｇｌｏｒｏｔ ，Ｘ ． ，Ｂｏｒｄｅｓ，Ａ ． ，ａｎｄＢｅｎｇ ｉｏ ，Ｙ．Ｄｏｍａｉｎａｄａｐｔａｔｉｏｎｆｏｒｌａｒｇｅ －ｓｃａｌｅｓｅｎｔｉｍｅｎｔ  

ｃｌａｓｓｉｆｉｃａｔｉｏｎ；Ａｄｅｅｐ ｌｅａｒｎｉｎｇａｐｐｒｏａｃｈ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２８ｔ ｆｉｉｎｔｅｒ ｎａｔｉｏｎａｌ  

ｃｏｎｆｅｒｅｎｃｅｏｎｍａｃｈｉｎｅｌｅａｒｎｉｎｇ（ＩＣＭＬ －１ １ ），ｐｐ

．５ １３ －５２０ ，２０ １１ ．  

［２８ ］Ｍｏｒａｅｓ，Ｒ． ，Ｖａｌｉａｔｉ ，Ｊ．Ｆ． ５ａｎｄＮｅｔｏ，Ｗ．Ｐ．Ｇ ．Ｄｏｃｕｍｅｎｔ －ｌｅｖｅｌｓｅｎｔｉｍｅｎｔ  

ｃｌａｓｓｉｆｉｃａｔ ｉｏｎ：Ａｎｅｍｐ ｉｒ ｉｃａｌｃｏｍｐａｒ ｉｓｏｎｂｅｔｗｅｅｎｓｖｍａｎｄａｎｎ ．Ｅｘｐｅｒ ｔＳｙｓｔｅｍｓ  

ｗｉｔｈＡｐｐ ｌｉｃａｔｉｏｎｓ，４０（２）

：６２１ －６３３ ？２０１３ｂ ．  

［２９ ］Ｐａｎｇ，Ｂ ． ５Ｌｅｅ ，Ｌ ． ，ａｎｄＶａｉｔｈｙａｎａｔｈａｎ ，Ｓ ．Ｔｈｕｍｂｓｕｐ？ ：ｓｅｎｔｉｍｅｎｔｃｌａｓｓｉｆｉｃａｔｉｏｎ  

ｕｓｉｎｇｍａｃｈｉｎｅｌｅａｒｎｉｎｇ ｔｅｃｈｎｉｑｕｅｓ ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔ ｉｉｅＡＣＬ －０２ｃｏｎｆｅｒｅｎｃｅｏｎ  

Ｅｍｐ ｉｒｉｃａｌｍｅｔｈｏｄｓｉｎｎａｔｕｒａｌｌａｎｇｕ＾ｅｐｒｏｃｅｓｓｉｎｇ

－Ｖｏｌｕｍｅ１０ ， ｐｐ

－８６．  

ＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ，２００２ ．  

［３０］Ｐａｎｇ？Ｂ ． ｓＬｅｅ ，Ｌ ． ？ａｎｄＶａｉｔｈｙａｎａｔｈａｎ ，Ｓ ．Ｔｈｕｍｂｓｕｐ？：ｓｅｎｔｉｍｅｎｔｃｌａｓｓｉｆｉｃａｔｉｏｎ  

ｕｓｉｎｇｍａｃｈｉｎｅｌｅａｒｎｉｎｇｔｅｃｈｎｉｑｕｅｓ ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＡＣＬ －０２ｃｏｎｆｅｒｅｎｃｅｏｎ  

７５  

北京邮电大学电子信息硕士学位论文  

Ｅｍｐ ｉｒｉｃａｌｍｅｔｈｏｄｓｉｎｎａｔｕｒａｌｌａｎｇｕａｇｅｐｒｏｃｅｓｓｉｎｇ

－Ｖｏｌｕｍｅ１０ ， ｐｐ

－８６．  

ＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ，２００２ ．  

［３１ ］Ｌｌｏｒｅｔ ，Ｅ ． ？Ｂａｌａｈｕｒ ，Ａ ． ５Ｐａｌｏｍａｒ ，Ｍ． ？ａｎｄＭｏｎｔｏｙｏ ，Ａ ．Ｔｏｗａｒｄｓｂｕｉｌｄｉｎｇａ  

ｃｏｍｐｅｔｉｔｉｖｅｏｐ ｉｎｉｏｎｓｕｍｍａｒ ｉｚａｔｉｏｎｓｙｓｔｅｍ ：Ｃｈａｌｌｅｎｇｅｓａｎｄｋｅｙｓ．ＩｎＨｕｍａｎ  

ＬａｎｇｕａｇｅＴｅｃｈｎｏｌｏｇ ｉｅｓ：ＣｏｎｆｅｒｅｎｃｅｏｆｔｈｅＮｏｒｔｈＡｍｅｒ ｉｃａｎＣｈａｐｔｅｒｏｆｔｈｅ  

ＡｓｓｏｃｉａｔｉｏｎｏｆＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ，Ｐｒｏｃｅｅｄｉｎｇｓ ，Ｍａｙ３１ －Ｊｕｎｅ５ ，２００９ ，  

Ｂｏｕｌｄｅｒ ，Ｃｏｌｏｒａｄｏ ，ＵＳＡ ，ＳｔｕｄｅｎｔＲｅｓｅａｒｃｈＷｏｒｋｓｈｏｐａｎｄＤｏｃｔｏｒａｌＣｏｎｓｏｒｔｉｕｍ，  

．７２ －７７ ．ＴｈｅＡｓｓｏｃｉａｔ ｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ，２００９ ．  

［３２ ］Ｙｕ ，Ｈ ．ａｎｄＨａｔｚｉｖａｓｓｉｌｏｇ ｌｏｕ ，Ｖ．Ｔｏｗａｒｄｓａｎｓｗｅｒｉｎｇｏｐ ｉｎｉｏｎ ｑｕｅｓｔｉｏｎｓ ：Ｓｅｐａｒａｔｉｎｇ  

ｆａｃｔｓｆ ｒｏｍｏｐ ｉｎｉｏｎｓａｎｄｉｄｅｎｔｉｆ ｙ ｉｎｇｔｈｅｐｏｌａｒｉｔｙｏｆｏｐ ｉｎｉｏｎｓｅｎｔｅｎｃｅｓ．Ｉｎ  

Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２００３ｃｏｎｆｅｒｅｎｃｅｏｎＥｍｐ ｉｒｉｃａｌｍｅｔｈｏｄｓｉｎｎａｔｕｒａｌｌａｎｇｕａｇｅ  

ｐｒｏｃｅｓｓｉｎｇ，ｐｐ

－ １３６ ．ＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ，２００３ ．  

［３３ ］Ｋｉｍ ，Ｓ ．ｍｉｄＨｏｖｙ５Ｅ ．Ｈ．Ｄｅｔｅｒｍｉｎｉｎｇｔｈｅｓｅｎｔｉｍｅｎｔｏｆ ｏｐ ｉｎｉｏｎｓ ．ＩｎＣＯＬＩＮＧ２００４ ，  

２０ｔｈＩｎｔｅｒ ｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔ ｅｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ，Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ  

Ｃｏｎｆｅｒｅｎｃｅ，２３ －２７Ａｕｇｕｓｔ２００４ ，Ｇｅｎｅｖａ，Ｓｗｉｔｚｅｒｌａｎｄ ，２００４ ．  

［３４ ］Ｋｏｕｌｏｕｍｐ ｉｓ ，Ｅ” Ｗｉｌｓｏｎ ，Ｔ” ａｎｄＭｏｏｒｅ ， Ｊ ．Ｄ ．Ｔｗｉｔｅｒｓｅｎｔｉｍｅｎｔａｎａｌｙｓｉｓ ：Ｔｈｅ ｇｏｏｄ  

ｔｈｅｂａｄａｎｄｔｈｅｏｍｇ

！Ｉｃｗｓｍ ，１１（５３８

－５４１） ：１６４，２０１１ ．  

［３５ ］Ｈｕ ？Ｍ ．ａｎｄＬｉｕ ５Ｂ ，Ｍｉｎｉｎｇａｎｄｓｕｍｍａｒ ｉｚｉｎｇｃｕｓｔｏｍｅｒｒｅｖｉｅｗｓ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆ  

ｔｈｅｔｅｎｔｈＡＣＭＳＩＧＫＤＤｉｎｔｅｒ ｎａｔｉｏｎａｌｃｏｎｆｅｒｅｎｃｅｏｎＫｎｏｗｌｅｄｇｅｄｉｓｃｏｖｅｒｙａｎｄ  

ｄａｔａｍｉｎｉｎｇ，ｐｐ

－ １７７ ．ＡＣＭ ５２００４ｂ ．  

Ｐ ６ ］ＪｅｒｅｍｙＢａｒｎｅｓ ，ＲｏｂｉｎＫｕｒｔｚ ，ＳｔｅｐｈａｎＯｅｐｅｎ ，Ｌｉｌ ｊ ａ０ｖｒｅｌｉｄ ，ａｎｄＥｒｉｋＶｅｌｌｄａＬ２０２１？  

Ｓｔｒｕｃｔｕｒｅｄｓｅｎｔｉｍｅｎｔａｎａｌｙｓｉｓａｓｄｅｐｅｎｄｅｎｃｙｇｒａｐｈ ｐａｒｓｉｎｇ

．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ  

５９ｔｈ ＡｎｎｕａｌＭｅｅｔｉｎｇｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓａｎｄｔｈｅ１ １ｔｈ  

Ｉｎｔｅｒ ｎａｔ ｉｏｎａｌＪｏｉｎｔＣｏｎｆｅｒｅｎｃｅｏｎＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ（Ｖｏｌｕｍｅ１ ：Ｌｏｎｇ  

Ｐａｐｅｒｓ），ｐａｇｅｓ３３８７ －３４０２ ，Ｏｎｌｉｎｅ ．Ａｓｓｏｃｉａｔ ｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ．  

［３７ ］ＷａｎｇＷ ｓＰａｎＳＪ ９ＤａｈｌｍｅｉｅｒＤ ，ｅｔａｌ ．ＲｅｃｕｒｓｉｖｅＮｅｕｒａｌＣｏｎｄｉｔｉｏｎａｌＲａｎｄｏｍＦｉｅｌｄｓ  

ｆｏｒＡｓｐｅｃｔ －ｂａｓｅｄＳｅｎｔｉｍｅｎｔＡｎａｌｙｓｉｓ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２０１６ｃｏｎｆｅｒｅｎｃｅｏｎ  

ｅｍｐ ｉｒ ｉｃａｌｍｅｔｈｏｄｓｉｎｎａｔｕｒ ａｌｌａｎｇｕａｇｅ ｐｒｏｃｅｓｓｉｎｇ，Ａｕｓｔ ｉｎ ，Ｔｅｘａｓ ：Ａｓｓｏｃｉａｔｉｏｎｆｏｒ  

ＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ，２０１６：６１６ －６２６ ．  

［３ ８ ］ＷａｎｇＷ ５ＰａｎＳＪ ？ＤａｈｌｍｅｉｅｒＤ ，ｅｔａｌ ．Ｃｏｕｐ ｌｅｄｍｕｌｔｉ － ｌａｙｅｒａｔｅｎｔｉｏｎｓｆｏｒｃｏ ？  

ｅｘｔｒａｃｔ ｉｏｎｏｆａｓｐｅｃｔａｎｄｏｐ ｉｎｉｏｎｔｅｒｍｓ ．ＩｎＴｈｉｒｔｙ

－ＦｉｒｓｔＡＡＡＩＣｏｎｆｅｒｅｎｃｅｏｎ  

ＡｒｔｉｆｉｃｉａｌＩｎｔｅｌｌｉｇｅｎｃｅ ，２０１７ ．  

７６  

参考文献  

［３ ９ ］ＤａｉＨ ｓＳｏｎｇＹ．ＮｅｕｒａｌＡｓｐｅｃｔａｎｄＯｐ ｉｎｉｏｎＴｅｒｍＥｘｔｒａｃｔｉｏｎｗｉｔｈＭｉｎｅｄＲｕｌｅｓａｓ  

ＷｅａｋＳｕｐｅｒｖｉｓｉｏｎ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２０１９ｃｏｎｆｅｒｅｎｃｅｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒ  

ＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ，Ｆｌｏｒｅｎｃｅ，Ｉｔａｌｙ，２０１９ ：５２６８ －５２７７ ．  

［４０］ＷａｎｇＷ ，ＰａｎＳＪ 〇Ｔｒａｎｓｆｅｒａｂｌｅｉｎｔｅｒａｃｔｉｖｅｍｅｍｏｒｙｎｅｔｗｏｒｋｆｏｒｄｏｍａｉｎａｄａｐ ｔａｔｉｏｎ  

ｉｎｆｉｎｅ － ｇｒａｉｎｅｄｏｐ ｉｎｉｏｎｅｘｔｒａｃｔｉｏｎ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＡＡＡＩＣｏｎｆｅｒｅｎｃｅｏｎ  

ＡｒｔｉｆｉｃｉａｌＩｎｔｅｌｌｉｇｅｎｃｅ ，２０１９：７１９２ －７１９９ ．  

［４１ ］ＣｈｅｎＳ ５ＬｉｕＪ ，ＷａｎｇＹ５ｅｔａｌ ．ＳｙｎｃｈｒｏｎｏｕｓＤｏｕｂｌｅ －ｃｈａｎｎｅｌＲｅｃｕｒｒｅｎｔＮｅｔｗｏｒｋｆｏｒ  

Ａｓｐｅｃｔ －Ｏｐ ｉｎｉｏｎＰａｉｒＥｘｔｒａｃｔ ｉｏｎ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆ ｔｈｅ５８ｔｈＡｎｎｕａｌＭｅｅｔｉｎｇｏｆ ｔｈｅ  

ＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ，２０２０：６５１５ －６５２４ ．  

［４２ ］ＭａＤ ，ＬｉＳ ，ＷａｎｇＨ ．Ｊｏｉｎｔｌｅａｒｎｉｎｇｆｏｒｔａｒｇｅｔｅｄｓｅｎｔｉｍｅｎｔａｎａｌｙｓｉｓ ．ＩｎＰｒｏｃｅｅｄｉｎｇｓ  

ｏｆｔｈｅ２０１８ＣｏｎｆｅｒｅｎｃｅｏｎＥｍｐ ｉｒｉｃａｌＭｅｔｈｏｄｓｉｎＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ，  

２０１８ ：４７３７ －４７４２．  

［４３］ ＨｅＲ ，ＬｅｅＷＳ ，ＮｇＨＴ ３ｅｔａｌ ．ＡｎＩｎｔｅｒａｃｔｉｖｅＭｕｌｔｉ －ＴａｓｋＬｅａｒｎｉｎｇＮｅｔｗｏｒｋｆｏｒ  

Ｅｎｄ －ｔｏ

－ＥｎｄＡｓｐｅｃｔ －ＢａｓｅｄＳｅｎｔｉｍｅｎｔＡｎａｌｙｓｉｓ ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２０１９  

ｃｏｎｆｅｒｅｎｃｅｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ，Ｆｌｏｒｅｎｃｅ ，Ｉｔａｌｙ，  

２０１９ ：５０４ －５１５ ．  

［４４ ］ＬｉＸ ，ＢｉｎｇＬ ，ＬｉＰ ？ｅｔａｌ ．Ａｕｎｉｆｉｅｄｍｏｄｅｌｆｏｒｏｐ ｉｎｉｏｎｔａｒｇｅｔｅｘｔｒａｃｔｉｏｎａｎｄｔａｒｇｅｔ  

ｓｅｎｔｉｍｅｎｔｐｒｅｄｉｃｔｉｏｎ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＡＡＡＩＣｏｎｆｅｒｅｎｃｅｏｎＡｒｔ ｉｆｉｃｉａｌ  

Ｉｎｔｅｌｌｉｇｅｎｃｅ，２０１９ ：６７１４ －６７２１ ．  

［４５］ＬｉＸ ，ＢｉｎｇＬ ，ＺｈａｎｇＷ ？ｅｔａｌ ．Ｅｘｐ ｌｏｉｔｉｎｇＢＥＲＴｆｏｒＥｎｄ －ｔｏ

－ＥｎｄＡｓｐｅｃｔ

－ｂａｓｅｄ  

ＳｅｎｔｉｍｅｎｔＡｎａｌｙｓｉｓ ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２０１９ｃｏｎｆｅｒｅｎｃｅｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒ  

ＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ，ＨｏｎｇＫｏｎｇ，Ｃｈｉｎａ ，２０１９ ：３４ －４１ ．  

［４６ ］ＰｅｎｇＨ ？ＸｕＬ ，ＢｉｎｇＬ ？ｅｔａｌ ．Ｋｎｏｗｉｎｇｗｈａｔ ，ｈｏｗａｎｄｗｈｙ

：Ａｎｅａｒｃｏｍｐ ｌｅｔｅ  

ｓｏｌｕｔｉｏｎｆｏｒａｓｐｅｃｔ －ｂａｓｅｄｓｅｎｔｉｍｅｎｔａｎａｌｙｓｉｓ，ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＡＡＡＩ  

ＣｏｎｆｅｒｅｎｃｅｏｎＡｒｔｉｆｉｃｉａｌＩｎｔｅｌｌｉｇｅｎｃｅ ，２０２０ ：８６００ －８６０７ ．  

［４７ ］ＬｕＸｕ ５ＨａｏＬｉ ，ＷｅｉＬｕ ，ａｎｄＬｉｄｏｎｇＢｉｎｇ

，２０２０ ．Ｐｏｓｉｔｉｏｎ －ａｗａｒｅｔａｇｇ ｉｎｇｆｏｒａｓｐｅｃｔ  

ｓｅｎｔｉｍｅｎｔｔｒ ｉｐ ｌｅｔｅｘｔｒａｃｔｉｏｎ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２０２０ＣｏｎｆｅｒｅｎｃｅｏｎＥｍｐ ｉｒｉｃａｌ  

ＭｅｔｈｏｄｓｉｎＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ（ＥＭＮＬＰ ），ｐ＾ｅｓ２３３９ －２３４９ ，Ｏｎｌｉｎｅ．  

ＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ．  

［４８ ］ＺｈｅｎＷｉｌ

，ＣｈｅｎｇｃａｎＹｉｎｇ，ＦｅｉＺｈａｏ，ＺｈｉｆａｎｇＦａｎ，ＸｉｎｙｕＤａｉ ，ａｎｄＲｕｉＸｉａ．２０２０ ．  

Ｇｒ ｉｄｔａｇｇ ｉｎｇｓｃｈｅｍｅｆｏｒａｓｐｅｃｔ －ｏｒ ｉｅｎｔｅｄｆｉｎｅ － ｇｒａｉｎｅｄｏｐ ｉｎｉｏｎｅｘｔｒａｃｔｉｏｎ．Ｉｎ  

ＦｉｎｄｉｎｇｓｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ：ＥＭＮＬＰ２０２０ ，ｐａｇｅｓ  

２５７６ －２５８５ ，Ｏｎｌｉｎｅ ．ＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ．  

７７  

北京邮电大学电子信息硕士学位论文  

［４９ ］ＣｈｅｎＺｈａｎｇ，ＱｉｕｃｈｉＬｉ ，ＤａｗｅｉＳｏｎｇ？ａｎｄＢｅｎｙｏｕＷａｎｇ

．２０２０ ．Ａｍｕｌｔｉ －ｔａｓｋ  

ｌｅａｒｎｉｎｇｆ ｒａｍｅｗｏｒｋｆｏｒｏｐ ｉｎｉｏｎｔｒ ｉｐ ｌｅｔｅｘｔｒａｃｔｉｏｎ．ＩｎＦｉｎｄｉｎｇｓｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎ  

ｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ：ＥＭＮＬＰ２０２０ ，ｐａｇｅｓ８１９ －８２８ ，Ｏｎｌｉｎｅ ．Ａｓｓｏｃｉａｔｉｏｎ  

ｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ．  

［５０］ＺｈｅｘｕｅＣｈｅｎ，ＨｏｎｇＨｕａｎｇ，ＢａｎｇＬｉｕ，ＸｕａｎｈｉｘａＳｈｉ ，ａｎｄＨａｉＪｉｎ．２０２１ｂ，Ｓｅｍａｎｔｉｃ  

ａｎｄｓｙｎｔａｃｔｉｃｅｎｈａｎｃｅｄａｓｐｅｃｔｓｅｎｔｉｍｅｎｔｔｒｉｐ ｌｅｔｅｘｔｒａｃｔｉｏｎ．ＩｎＦｉｎｄｉｎｇｓｏｆｔｈｅ  

ＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ：ＡＣＬ －ＩＪＣＮＬＰ２０２１ ，ｐａｇｅｓ１４７４— １４８３ ，  

Ｏｎｌｉｎｅ．ＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ．  

［５１ ］ ＨａｎｇＹａｎ ，Ｊｕｎｑ ｉＤａｉ ，ＴｕｏＪｉ ？ＸｉｐｅｎｇＱｉｕ ，ａｎｄＺｈｅｎｇＺｈａｎｇ

．２０２ＬＡｕｎｉｆ ｉｅｄ  

ｇｅｎｅｒａｔｉｖｅｆ ｒａｍｅｗｏｒｋｆｏｒａｓｐｅｃｔ －ｂａｓｅｄｓｅｎｔｉｍｅｎｔａｎａｌｙｓｉｓ ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ  

５９ｔｈＡｎｎｕａｌＭｅｅｔｉｎｇｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓａｎｄｔｈｅ１ １ｔｈ  

Ｉｎｔｅｒ ｎａｔｉｏｎａｌＪｏｉｎｔＣｏｎｆｅｒｅｎｃｅｏｎＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ（Ｖｏｌｕｍｅ１ ：Ｌｏｎｇ  

Ｐａｐｅｒｓ），ｐａｇｅｓ２４１６ －２４２９ ，Ｏｎｌｉｎｅ ．ＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔ ｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ．  

［５２］ＹｕｅＭａｏ，ＹｉＳｈｅｎ ，ＣｈａｏＹｕ，ａｎｄＬｏｎｇｊｕｎＣａｉ ．２０２１ ．Ａ

ｊｏｉｎｔｔｒａｉｎｉｎｇｄｕａｌ －ｍｒｃ  

ｆ ｒａｍｅｗｏｒｋｆｏｒａｓｐｅｃｔｂａｓｅｄｓｅｎｔｉｍｅｎｔａｎａｌｙｓｉｓ ．ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｉｉｅＡＡＡＩ  

ＣｏｎｆｅｒｅｎｃｅｏｎＡｒｔｉｆ ｉｃｉａｌＩｎｔｅｌｌｉｇｅｎｃｅ ，３５（１５） ： １３５４３

－ １３５５１ ．  

［５３ ］ＳｈａｏｗｅｉＣｈｅｎ ，ＹｕＷａｎｇ，ＪｉｅＬｉｕ ，ａｎｄＹｕｅｌｉｎＷａｎｇ

．２０２１ａ．Ｂｉｄｉｒｅｃｔｉｏｎａｌｍａｃｈｉｎｅ  

ｒｅａｄｉｎｇｃｏｍｐｒｅｈｅｎｓｉｏｎｆｏｒａｓｐｅｃｔｓｅｎｔｉｍｅｎｔｔｒｉｐ ｌｅｔｅｘｔｒａｃｔｉｏｎ．Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ  

ＡＡＡＩＣｏｎｆｅｒｅｎｃｅｏｎＡｒｔ ｉｆｉｃｉａｌＩｎｔｅｌｌｉｇｅｎｃｅ ，３５（１４ ）

：１２６６６

－ １２６７４ ．  

［５４ ］ＡｎｄｒｅａＥｓｕｌｉ ？Ｆａｂｒ ｉｚｉｏＳｅｂａｓｔｉａｎｉ ， ａｎｄＩｌａｒｉａＵｒｃｉｕｏｌｉ ．２００８ ．Ａｎｎｏｔａｔｉｎｇ  

ｅｘｐｒｅｓｓｉｏｎｓｏｆｏｐ ｉｎｉｏｎａｎｄｅｍｏｔｉｏｎｉｎｔｈｅＩｔａｌｉａｎｃｏｎｔｅｎｔａｎｎｏｔａｔｉｏｎｂａｎｋ．Ｉｎ  

ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＳｉｘｔｈＩｎｔｅｒ ｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＬａｎｇ ｘｘａｇｅＲｅｓｏｕｒｃｅｓａｎｄ  

Ｅｖａｌｕａｔｉｏｎ（ＬＲＥＣ

５ ０８），Ｍａｒｒａｋｅｃｈ ，Ｍｏｒｏｃｃｏ．ＥｕｒｏｐｅａｎＬａｎｇｕａｇｅＲｅｓｏｕｒｃｅｓ  

Ａｓｓｏｃｉａｔ ｉｏｎ （ＥＬＲＡ）

［５５ ］ＡｒｚｏｏＫａｔｉｙａｒａｎｄＣｌａｉｒｅＣａｒｄｉｅ ．２０１６ ．ＩｎｖｅｓｔｉｇａｔｉｎｇＬＳＴＭｓｆｏｒ

ｊｏｉｎｔｅｘｔｒａｃｔｉｏｎ  

ｏｆ ｏｐ ｉｎｉｏｎｅｎｔｉｔｉｅｓａｎｄｒｅｌａｔｉｏｎｓ ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆ ｔｈｅ５４ｔｈＡｎｎｕａｌＭｅｅｔｉｎｇｏｆ ｔｈｅ  

ＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ（Ｖｏｌｕｍｅ１ ：ＬｏｎｇＰａｐｅｒｓ），ｐａｇｅｓ９１９

９２９ ，Ｂｅｒｌｉｎ ，Ｇｅｒｍａｎｙ

．ＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ．  

［５６ ］Ｗｅｉ Ｑｕａｎ ，ＪｉｎｌｉＺｈａｎｇ，ａｎｄＸｉａｏｈｕａＴｏｎｙＨｕ ．２０１９．Ｅｎｄ －ｔｏ

ｊｏｉｎｔｏｐ ｉｎｉｏｎｒｏｌｅ  

ｌａｂｅｌｉｎｇｗｉｔｈｂｅｒｔ．Ｉｎ２０１９ＩＥＥＥＩｎｔｅｒ ｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＢｉｇＤａｔａ （ＢｉｇＤａｔａ） ，  

ｐａｇｅｓ２４３８ －２４４６ ．  

［５７ ］ＢｏＺｈａｎｇ，ＹｕｅＺｈａｎｇ，ＲｕｉＷａｎｇ，ＺｈｅｎｇｈｕａＬｉ ，ａｎｄＭｉｎＺｈａｎｇ

．２０２０ａ．Ｓｙｎｔａｘ －  

ａｗａｒｅｏｐ ｉｎｉｏｎｒｏｌｅｌａｂｅｌｉｎｇｗｉｔｈｄｅｐｅｎｄｅｎｃｙｇｒａｐｈｃｏｎｖｏｌｕｔｉｏｎａｌｎｅｔｗｏｒｋｓ ．Ｉｎ  

７８  

参考文献  

Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ５８ｔｈＡｎｎｕａｌＭｅｅｔｉｎｇｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌ  

Ｌｉｎｇｕｉｓｔｉｃｓ ，ｐａｇｅｓ３２４９ －３２５８ ，Ｏｎｌｉｎｅ ．ＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ．  

［５８ ］Ｑ ｉｎｇｒｏｎｇＸｉａ，ＢｏＺｈａｎｇ，ＲｕｉＷａｎｇ，ＺｈｅｎｇｈｕａＬｉ ，ＹｕｅＺｈａｎｇ，ＦｅｉＨｕａｎｇ，ＬｕｏＳｉ ？  

ａｎｄＭｉｎＺｈａｎｇ

．２０２１ ．Ａｕｎｉｆｉｅｄｓｐａｎ －ｂａｓｅｄａｐｐｒｏａｃｈｆｏｒｏｐ ｉｎｉｏｎｍｉｎｉｎｇｗｉｔｈ  

ｓｙｎｔａｃｔｉｃｃｏｎｓｔｉｔｕｅｎｔｓ ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２０２１ＣｏｎｆｅｒｅｎｃｅｏｆｔｈｅＮｏｒｔｈ  

ＡｍｅｒｉｃａｎＣｈａｐ ｔｅｒｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ：Ｈｕｍａｎ  

ＬａｎｇｕａｇｅＴｅｃｈｎｏｌｏｇ ｉｅｓ，ｐａｇｅｓ１７９５ － １８０４ ５Ｏｎｌｉｎｅ．Ａｓｓｏｃｉａｔ ｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌ  

Ｌｉｎｇｕｉｓｔｉｃｓ ．  

［５９］ＷｅｎｘｕａｎＳｈｉ ，ＦｅｉＬｉ ，ＪｉｎｇｙｅＬｉ ，ＨａｏＦｅｉ ，ａｎｄＤｏｎｇｈｏｎｇＪｉ ．２０２２ ．Ｅｆｆｅｃｔｉｖｅｔｏｋｅｎ  

ｇｒａｐｈｍｏｄｅｌｉｎｇｕｓｉｎｇａｎｏｖｅｌｌａｂｅｌｉｎｇｓｔｒａｔｅｇｙｆｏｒｓｔｒｕｃｔｕｒｅｄｓｅｎｔｉｍｅｎｔａｎａｌｙｓｉｓ ．  

ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ６０ｔｈＡｎｎｕａｌＭｅｅｔｉｎｇｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔ ｉｏｎａｌ  

Ｌｉｎｇｕｉｓｔｉｃｓ（Ｖｏｌｕｍｅ１ ：ＬｏｎｇＰａｐｅｒｓ），ｐａｇｅｓ４２３２－４２４１ ，Ｄｕｂｌｉｎ，Ｉｒｅｌａｎｄ．  

ＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ．  

［６０ ］ＤａｖｉｄＳａｍｕｅｌ ，ＪｅｒｅｍｙＢａｍｅｓ５ＲｏｂｉｎＫｕｒｔｚ，ＳｔｅｐｈａｎＯｅｐｅｎ ，Ｌｉｌｊａ０ｖｒｅｌｉｄ ？ａｎｄ  

ＥｒｉｋＶｅｌｌｄａｌ．２０２２ ．Ｄｉｒｅｃｔ ｐａｒｓｉｎｇ ｔｏｓｅｎｔｉｍｅｎｔ ｇｒａｐｈｓ ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ６０ｔｈ  

ＡｎｎｕａｌＭｅｅｔｉｎｇｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ （Ｖｏｌｕｍｅ２ ：Ｓｈｏｒｔ  

Ｐａｐｅｒｓ），ｐａｇｅｓ４７０ －４７８，Ｄｕｂｌｉｎ，Ｉｒｅｌａｎｄ．Ａｓｓｏｃｉａｔ ｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌ  

Ｌｉｎｇｕｉｓｔｉｃｓ ．  

［６１ ］ＳｈｅｒｓｔｉｎｓｋｙＡ ．Ｆｕｎｄａｍｅｎｔａｌｓｏｆｒｅｃｕｒｒｅｎｔｎｅｕｒａｌｎｅｔｗｏｒｋ（ＲＮＮ）ａｎｄｌｏｎｇｓｈｏｒｔ ？  

ｔｅｒｍｍｅｍｏｒｙ（ＬＳＴＭ）ｎｅｔｗｏｒｋ ［Ｊ ］

．ＰｈｙｓｉｃａＤ ：ＮｏｎｌｉｎｅａｒＰｈｅｎｏｍｅｎａ，２０２０ ，４０４ ：  

１３２３０６ ．  

［６２ ］ＣｈｉｕＣＣ ，ＳａｉｎａｔｂＴＮ ，ＷｕＹ ，ｅｔａｌ ．Ｓｔａｔｅ －ｏｆ －ｔｈｅ －ａｒｔｓｐｅｅｃｈｒｅｃｏｇｎｉｔｉｏｎｗｉｔｈ  

ｓｅｑｕｅｎｃｅ －ｔｏｓｅｑｕｅｎｃｅｍｏｄｅｌｓ ［Ｃ ］／／２０１８ＩＥＥＥｉｎｔｅｒｎａｔｉｏｎａｌｃｏｎｆｅｒｅｎｃｅｏｎ  

ａｃｏｕｓｔｉｃｓ ，ｓｐｅｅｃｈａｎｄｓｉｇｎａｌ ｐｒｏｃｅｓｓｉｎｇ（ＩＣＡＳＳＰ）

．ＩＥＥＥ ，２０１８ ：４７７４ －４７７８ ．  

［６３ ］ＷａｎｇＳ ５ＪｉａｎｇＪ ．ＬｅａｒｎｉｎｇｎａｔｕｒａｌｌａｎｇｕａｇｅｉｎｆｅｒｅｎｃｅｗｉｔｈＬＳＴＭ［Ｊ ］

．ａｒＸｉｖ

ｐｒｅｐｒｉｎｔ  

ａｒＸｉｖ：１５１２ ．０８８４９ ９２０１５ ．  

［６４ ］ＷａｎｇＣ ？ＹａｎｇＨ ，ＢａｒｔｚＣ ５ｅｔａｌ ．Ｉｍａｇｅｃａｐｔｉｏｎｉｎｇｗｉｔｈｄｅｅｐｂｉｄｉｒｅｃｔｉｏｎａｌ  

ＬＳＴＭｓ［Ｃ ］ ／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２４ｔｈＡＣＭｉｎｔｅｒ ｎａｔｉｏｎａｌｃｏｎｆｅｒｅｎｃｅｏｎＭｕｌｔｉｍｅｄｉａ，  

２０１６：９８８

－９９７ ．  

［６５ ］ＳｃｈｕｓｔｅｒＭ ３ＰａｌｉｗａｌＫＫ．Ｂｉｄｉｒｅｃｔｉｏｎａｌｒｅｃｕｒｒｅｎｔｎｅｕｒａｌｎｅｔｗｏｒｋｓ ［Ｊ ］

．ＩＥＥＥ  

ｔｒａｎｓａｃｔｉｏｎｓｏｎＳｉｇｎａｌＰｒｏｃｅｓｓｉｎｇ， １９９７ ，４５（ １ １ ） ；２６７３ －２６８１ ．  

［６６ ］ＸｕＧ ？ＭｅｎｇＹ ５Ｑ ｉｕＸ ５ｅｔａｌ ．Ｓｅｎｔｉｍｅｎｔａｎａｌｙｓｉｓｏｆｃｏｍｍｅｎｔｔｅｘｔｓｂａｓｅｄｏｎ  

ＢｉＬＳＴＭ ［Ｊ］

．ＩｅｅｅＡｃｃｅｓｓ ，２０１９ ，７ ：５１５２２ －５１５３２ ．  

７９  

北京邮电大学电子信息硕士学位论文  

［６７ ］ＰａｎｃｈｅｎｄｒａｒａｊａｎＲ ，ＡｍａｒｅｓａｎＡ．ＢｉｄｉｒｅｃｔｉｏｎａｌＬＳＴＭ －ＣＲＦｆｏｒｎａｍｅｄｅｎｔｉｔｙ  

ｒｅｃｏｇｎｉｔ ｉｏｎ ［Ｃ］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ３２ｎｄＰａｃｉｆｉｃＡｓｉａｃｏｎｆｅｒｅｎｃｅｏｎｌａｎｇｕａｇｅ ，  

ｉｎｆｏｒｍａｔｉｏｎａｎｄｃｏｍｐｕｔａｔｉｏｎ．２０１８ 〇  

［６８ ］ＷｕＴＷ ？ＣｈｅｎＩＦ ？ＧａｎｄｈｅＡ ．ＬｅａｒｎｉｎｇｔｏｒａｎｋｗｉｔｈＢＥＲＴ －ｂａｓｅｄｃｏｎｆ ｉｄｅｎｃｅ  

ｍｏｄｅｌｓｉｎＡＳＲｒｅｓｃｏｒｍｇ［Ｊ ］

．２０２２ ．  

［６９］ＣｈｏＫ ，ＶａｎＭｅｒｒｉｇｎｂｏｅｒＢ ，ＧｕｌｃｅｈｒｅＣ ，ｅｔａｌ ．Ｌｅａｒｎｉｎｇｐｈｒａｓｅｒｅｐｒｅｓｅｎｔａｔｉｏｎｓ  

ｕｓｉｎｇＲＮＮｅｎｃｏｄｅｒ －ｄｅｃｏｄｅｒｆｏｒｓｔａｔｉｓｔｉｃａｌｍａｃｈｉｎｅｔｒａｎｓｌａｔｉｏｎ ［Ｊ］

．ａｒＸｉｖ ｐｒｅｐｒｉｎｔ  

ａｒＸｉｖ：１４０６ ．１０７８ ５２０１４ ．  

［７０ ］Ｏ

’ ＳｈｅａＫ ，ＮａｓｈＲ ．Ａｎｉｎｔｒｏｄｕｃｔ ｉｏｎｔｏｃｏｎｖｏｌｕｔ ｉｏｎａｌｎｅｕｒａｌｎｅｔｗｏｒｋｓ［Ｊ ］

．ａｒＸｉｖ  

ｐｒｅｐｒｉｎｔａｒＸｉｖ：１５１１ ．０８４５８ ，２０１５ ．  

［７１ ］ ＶａｓｗａｎｉＡ ， ＳｈａｚｅｅｒＮ ，ＰａｒｍａｒＮ ， ｅｔａｌ ．Ａｔｅｎｔｉｏｎｉｓａｌｌ ｙｏｕｎｅｅｄ ［Ｊ］

．Ａｄｖａｎｃｅｓｉｎ  

ｎｅｕｒａｌｉｎｆｏｒｍａｔｉｏｎ ｐｒｏｃｅｓｓｉｎｇｓｙｓｔｅｍｓ ，２０１７ ，３０ ，  

［７２ ］ＢａｈｄａｎａｕＤ，ＣｈｏＫ ？Ｂｅｎｇ ｉｏＹ．Ｎｅｕｒａｌｍａｃｈｉｎｅｔｒａｎｓｌａｔｉｏｎｂｙｊｏｉｎｔｌｙ ｌｅａｒｎｉｎｇｔｏ  

ａｌｉｇｎａｎｄｔｒａｎｓｌａｔｅ［Ｊ］

．ａｒＸｉｖ ｐｒｅｐｒ ｉｎｔａｒＸｉｖ；１４０９ ．０４７３ ，２０１４ ．  

［７３ ］Ｎａｌｌａｐａｔ ｉＫＺｈｏｕＢ ， ＧｕｌｃｅｈｒｅＣ ， ｅｔａｌ ．Ａｂｓｔｒａｃｔｉｖｅｔｅｘｔｓｕｍｍａｒｉｚａｔ ｉｏｎｕｓｉｎｇ  

ｓｅｑｕｅｎｃｅ

－ｓｅｑｕｅｎｃｅｍｎｓａｎｄｂｅｙｏｎｄ ［Ｊ ］

．ａｒＸｉｖ

ｐｒｅｐｒｉｎｔａｒＸｉｖ ：１６０２．０６０２３ ，２０１６．  

［７４ ］ ＸｉｎｇＣ，ＷｕＷ ９ＷｕＹ ，ｅｔａｌ ．Ｔｏｐ ｉｃａｗａｒｅｎｅｕｒａｌｒｅｓｐｏｎｓｅ  

ｇｅｎｅｒａｔｉｏｎ ［Ｃ］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆ ｔｈｅ ＡＡＡＩｃｏｎｆｅｒｅｎｃｅｏｎａｒｔ ｉｆ ｉｃｉａｌｉｎｔｅｌｌｉｇｅｎｃｅ．２０１７ ？  

３ １（１）

［７５ ］ ＸｕＫ ，ＢａＪ ，ＫｉｒｏｓＲ ，ｅｔａｌ ．Ｓｈｏｗ，ａｔｔｅｎｄａｎｄｔｅｌｌ ：Ｎｅｗａｌｉｍａｇｅｃａｐｔｉｏｎ ｇｅｎｅｒａｔｉｏｎ  

ｗｉｔｈｖｉｓｕａｌａｔｔｅｎｔｉｏｎ［Ｃ ］ ／／Ｉｎｔｅｍａｔｉｏｎａｌｃｏｎｆｅｒｅｎｃｅｏｎｍａｃｈｉｎｅｌｅａｒｎｉｎｇ

．ＰＭＬＲ ，  

２０１５ ：２０４８

－２０５７ ．  

［７６ ］ＣｈｏｒｏｗｓｋｉＪＫ ，ＢａｈｄａｎａｕＤ ？ＳｅｒｄｙｕｋＤ ？ｅｔａｉ ．Ａｔｔｅｎｔｉｏｎ －ｂａｓｅｄｍｏｄｅｌｓｆｏｒｓｐｅｅｃｈ  

ｒｅｃｏｇｎｉｔｉｏｎ［Ｊ ］

．Ａｄｖａｎｃｅｓｉｎｎｅｕｒａｌｉｎｆｏｒｍａｔｉｏｎ ｐｒｏｃｅｓｓｉｎｇｓｙｓｔｅｍｓ ，２０１５，２８ ．  

［７７ ］ ＡｎＥｍｐ ｉｒｉｃａｌＣｏｍｐａｒｉｓｏｎｏｆＳｅｑｕｅｎｃｅ －ｔｏ

－ＳｅｑｕｅｎｃｅＭｏｄｅｌｓｆｏｒＣｈｉｎｅｓｅ －ｔｏ －  

Ｅｎｇ ｌｉｓｈＭａｃｈｉｎｅＴｒａｎｓｌａｔｉｏｎ  

［７８ ］ ＤｅｖｌｉｎＪ ，ＣｈａｎｇＭＷ ，ＬｅｅＫ ，ｅｔａｌ ．Ｂｅｒｔ：Ｐｒｅ －ｔｒａｉｎｉｎｇｏｆｄｅｅｐｂｉｄｉｒｅｃｔｉｏｎａｌ  

ｔｒａｎｓｆｏｒｍｅｒｓｆｏｒｌａｎｇｕａｇｅｕｎｄｅｒｓｔａｎｄｉｎｇ ［Ｊ ］

．ａｒＸｉｖｐｒｅｐｒｉｎｔａｒＸｉｖ：１８１０ ．０４８０５ ，  

２０１８ ．  

［７９ ］ＳｒｉｖａｓｔａｖａＮ ，ＨｉｎｔｏｎＧ ，Ｋｒ ｉｚｈｅｖｓｋｙＡ ， ｅｔａｌ ．Ｄｒｏｐｏｕｔ：ａｓｉｍｐ ｌｅｗａｙｔｏｐｒｅｖｅｎｔ  

ｎｅｕｒａｌｎｅｔｗｏｒｋｓｆ ｒｏｍｏｖｅｒｆ ｉｔｔｉｎｇ［Ｊ ］

．Ｔｈｅ ｊｏｕｒ ｎａｌｏｆｍａｃｈｉｎｅｌｅａｒｎｉｎｇｒｅｓｅａｒｃｈ ，  

２０１４ ， １５（１ ）

：１９２９ －１９５８ ．  

８０  

参考文献  

［８０ ］Ｉｏｆ ｌｆ ｅＳ ，ＳｚｅｇｅｄｙＣ ．Ｂａｔｃｈｎｏｒｍａｌｉｚａｔｉｏｎ：Ａｃｃｅｌｅｒａｔｉｎｇｄｅｅｐｎｅｔｗｏｒｋｔｒａｉｎｉｎｇｂｙ  

ｒｅｄｕｃｉｎｇ ｉｎｔｅｒ ｎａｌｃｏｖａｒ ｉａｔｅｓｈｉｆ ｔ ［Ｃ］／／Ｉｎｔｅｍａｔｉｏｎａｌｃｏｎｆｅｒｅｎｃｅｏｎｍａｃｈｉｎｅｌｅａｒｎｉｎｇ

ｐｍｌｒ ，２０１５ ：４４８ －４５６ ．  

］ ＢａＪＬ ， ＫｉｒｏｓＪＲｊＨｉｎｔｏｎＧＥ ．Ｌａｙｅｒｎｏｒｍａｌｉｚａｔ ｉｏｎ［Ｊ］

．ａｒＸｉｖｐｒｅｐｒ ｉｎｔ  

ａｒＸｉｖ： １６０７ ．０６４５０ ？２０ １６．  

［８２ ］ＷｕＹ ？ＨｅＫ．Ｇｒｏｕｐｎｏｒｍａｌｉｚａｔｉｏｎ［Ｃ ］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆ ｔｈｅＥｕｒｏｐｅａｎｃｏｎｆｅｒｅｎｃｅｏｎ  

ｃｏｍｐｕｔｅｒｖｉｓｉｏｎ （ＥＣＣＶ）

．２０１８ ：３ － １９ ．  

［８３ ］ＵｌｙａｎｏｖＤ ５ＶｅｄａｌｄｉＡ ，Ｌｅｍｐ ｉｔｓｋｙＶ．Ｉｎｓｔａｎｃｅｎｏｒｍａｌｉｚａｔｉｏｎ：Ｔｈｅｍｉｓｓｉｎｇ  

ｉｎｇｒｅｄｉｅｎｔｆｏｒｆａｓｔｓｔｙ ｌｉｚａｔｉｏｎ［Ｊ ］

．ａｒＸｉｖ ｐｒｅｐｒｉｎｔａｒＸｉｖ：１６０７ ．０８０２２ ，２０１６ ．  

［８４ ］ＷａｎｇＺ ？ＢｏｖｉｋＡＣ ．Ｍｅａｎｓｑｕａｒｅｄｅｒｒｏｒ：Ｌｏｖｅｉｔｏｒｌｅａｖｅｉｔ？Ａｎｅｗｌｏｏｋａｔｓｉｇｎａｌ  

ｆ ｉｄｅｌｉｔｙｍｅａｓｕｒｅｓ［Ｊ ］

．ＩＥＥＥｓｉｇｎａｌ ｐｒｏｃｅｓｓｉｎｇｍａｇａｚｉｎｅ，２００９ ，２６ （１） ：９８ －１ １７ ．  

［８５ ］ＦｅｎｇＬ ？ＳｈｕＳ ？ＬｉｎＺ ，ｅｔａｌ ．Ｃａｎｃｒｏｓｓｅｎｔｒｏｐｙ ｌｏｓｓｂｅｒｏｂｕｓｔｔｏｌａｂｅｌ  

ｎｏｉｓｅ？ ［Ｃ ］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＴｗｅｎｔｙ

－ＮｉｎｔｈＩｎｔｅｒ ｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎ  

Ｉｎｔｅｒ ｎａｔｉｏｎａｌＪｏｉｎｔＣｏｎｆｅｒｅｎｃｅｓｏｎＡｒｔｉｆｉｃｉａｌＩｎｔｅｌｌｉｇｅｎｃｅ ．２０２１ ：２２０６ －２２１２ ．  

［８６ ］ＫｉｍＴ ，ＯｈＪ ？ＫｉｍＮＹ

，ｅｔａｌ ．Ｃｏｍｐａｒｉｎｇｋｕｌｌｂａｃｋ －ｌｅｉｂｌｅｒｄｉｖｅｒｇｅｎｃｅａｎｄｍｅａｎ  

ｓｑｕａｒｅｄｅｒｒｏｒｌｏｓｓｉｎｋｎｏｗｌｅｄｇｅｄｉｓｔｉｌｌａｔｉｏｎ ［Ｊ］

．ａｒＸｉｖ ｐｒｅｐｒｉｎｔａｒＸｉｖ ：２１０５ ．０８９１９ ？  

２０２１ ．  

［８７ ］ＲｕｄｅｒＳ ．Ａｎｏｖｅｒｖｉｅｗｏｆｇｒａｄｉｅｎｔｄｅｓｃｅｎｔｏｐｔ ｉｍｉｚａｔｉｏｎａｌｇｏｒ ｉｔｈｍｓ［Ｊ］

．ａｒＸｉｖ  

ｐｒｅｐｒｉｎｔａｒＸｉｖ：１６０９ ．０４７４７ ，２０１６．  

［８８ ］ＫＬｉｎｇｍａＤＰ ５ＢａＪ，Ａｄａｍ ：Ａｍｅｔｈｏｄｆｏｒｓｔｏｃｈａｓｔｉｃｏｐｔｉｍｉｚａｔｉｏｎ ［Ｊ ］

．ａｒＸｉｖ ｐｒｅｐｒｉｎｔ  

ａｒＸｉｖ： １４１２ ．６９８０ ？２０１４－  

［８９ ］ＬｏｓｈｃｈｉｌｏｖＩ ？ＨｕｔｔｅｒＦ．Ｆｉｘｉｎｇｗｅｉｇｈｔｄｅｃａｙｒｅｇｕｌａｒｉｚａｔｉｏｎｉｎａｄａｍ［Ｊ ］

．２０１７．  

［９０ ］ＤｕｃｈｉＪ ，ＨａｚａｎＥ ，ＳｉｎｇｅｒＹ．Ａｄａｐ ｔｉｖｅｓｕｂｇｒａｄｉｅｎｔｍｅｔｈｏｄｓｆｏｒｏｎｌｉｎｅｌｅａｒｎｉｎｇａｎｄ  

ｓｔｏｃｈａｓｔｉｃｏｐ ｔｉｍｉｚａｔｉｏｎ［Ｊ ］

，Ｊｏｕｒ ｎａｌｏｆｍａｃｈｉｎｅｌｅａｒｎｉｎｇｒｅｓｅａｒｃｈ ，２０１ １ ， １２ （７）

［９ １ ］ＨｅＲ，ＬｅｅＷＳ ，ＮｇＨＴ ，ｅｔａｌ ．ＡｎＩｎｔｅｒａｃｔｉｖｅＭｕｌｔｉ －ＴａｓｋＬｅａｒｎｉｎｇＮｅｔｗｏｒｋｆｏｒ  

Ｅｎｄ －ｔｏ －ＥｎｄＡｓｐｅｃｔ －ＢａｓｅｄＳｅｎｔｉｍｅｎｔＡｎａｌｙｓｉｓ［Ａ ］ ／／Ｆｌｏｒｅｎｃｅ ，Ｉｔａｌｙ

：Ａｓｓｏｃｉａｔｉｏｎ  

ｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ，２０１９ ：５０４ －５１５ ．  

［９２ ］ ＸｕＬ ？ＣｈｉａＹＫ ？ＢｉｎｇＬ ．Ｌｅａｒｎｉｎｇｓｐａｎ － ｌｅｖｅｌｉｎｔｅｒａｃｔｉｏｎｓｆｏｒａｓｐｅｃｔｓｅｎｔｉｍｅｎｔ  

ｔｒ ｉｐ ｌｅｔｅｘｔｒａｃｔｉｏｎ ［Ｊ ］

．ａｒＸｉｖ ｐｒｅｐｒｉｎｔａｒＸｉｖ ：２１０７ ．１２２１４ ，２０２１ ．  

［９３ ］ＣｈｅｎＨ ，ＺｈａｉＺ ，ＦｅｎｇＦ ，ｅｔａｌ ．Ｅｎｈａｎｃｅｄｍｕｌｔｉ －ｃｈａｎｎｅｌｇｒａｐｈｃｏｎｖｏｌｕｔｉｏｎａｌ  

ｎｅｔｗｏｒｋｆｏｒａｓｐｅｃｔｓｅｎｔｉｍｅｎｔｔｒｉｐ ｌｅｔｅｘｔｒａｃｔｉｏｎ［Ｃ ］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆ ｔｈｅ６０ｔｈＡｎｎｕａｌ  

Ｍｅｅｔｉｎｇｏｆ ｔ ｉｉｅＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ（Ｖｏｌｕｍｅ１ ：ＬｏｎｇＰａｐｅｒｓ）

２０２２ ：２９７４ －２９８５ ．  

８ １  

北京邮电大学电子信息硕士学位论文  

［９４ ］ＹｕｃｈｅｎｇＷａｎｇ５ＢｏｗｅｎＹｕ ，ＨｏｎｇｓｏｎｇＺｈｕ ５ＴｉｎｇｗｅｎＬｉｕ ？ＮａｎＹｕ，ａｎｄＬｉｍｉｎＳｕｎ ．  

２０２１ ．Ｄｉｓｃｏｎｔｉｎｕｏｕｓｎａｍｅｄｅｎｔｉｔｙｒｅｃｏｇｎｉｔｉｏｎａｓｍａｘｉｍａｌｃｌｉｑｕｅｄｉｓｃｏｖｅｒｙ

．Ｉｎ  

Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ５９ｔｈＡｎｎｕａｌＭｅｅｔｉｎｇｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌ  

Ｌｉｎｇｕｉｓｔｉｃｓａｎｄｔｈｅ１ １ｔｈＩｎｔｅｒ ｎａｔｉｏｎａｌＪｏｉｎｔＣｏｎｆｅｒｅｎｃｅｏｎＮａｔｕｒａｌＬａｎｇｕａｇｅ  

Ｐｒｏｃｅｓｓｉｎｇ（Ｖｏｌｕｍｅ１ ：ＬｏｎｇＰａｐｅｒｓ），ｐａｇｅｓ７６４ －７７４ ，Ｏｎｌｉｎｅ ．Ａｓｓｏｃｉａｔｉｏｎｆｏｒ  

ＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ．  

［９５ ］ＪｏｎａｔｈａｎＨｏ，ＮａｌＫａｌｃｈｂｒｅｎｎｅｒ ，ＤｉｒｋＷｅｉｓｓｅｎｂｏｍ，ａｎｄＴｉｍＳａｌｉｍａｎｓ．２０１９．Ａｘｉａｌ  

ａｔｔｅｎｔｉｏｎｉｎｍｕｌｔ ｉｄｉｍｅｎｓｉｏｎａｌｔｒａｎｓｆｏｒｍｅｒｓ ．ａｒＸｉｖ ｐｒｅｐｒｉｎｔａｒＸｉｖ ：１９１２ ．１２１８０ ．  

［９６ ］ＨｕｉｙｕＷａｎｇ，ＹｕｋｕｎＺｈｕ，ＢｒａｄｌｅｙＧｒｅｅｎ ，ＨａｘｔｗｉｇＡｄａｍ ，ＡｌａｎＬｏｄｄｏｎＹｕｉｌｌｅ，ａｎｄ  

－ＣｈｉｅｈＣｈｅｎ．２０２０ａ．Ａｘｉａｌ －ｄｅｅｐ ｌａｂ ：Ｓｔａｎｄ －ａｌｏｎｅａｘｉａｌ

－ａｔｔｅｎｔｉｏｎｆｏｒ

ｐａｎｏｐ ｔｉｃ  

ｓｅｇｍｅｎｔａｔｉｏｎ．ＩｎＥｕｒｏｐｅａｎＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎ．  

［９７ ］ ＺｉｌｏｎｇＨｕａｎｇ， ＸｉｎｇｇａｎｇＷａｎｇ， ＬｉｃｈａｏＨｕａｎｇ，ＣｈａｎｇＨｕａｎｇ，ＹｕｎｃｈａｏＷｅｉ ，  

ＨｕｍｐｈｒｅｙＳｈｉ ，ａｎｄＷｅｎｙｕＬｉｕ．２０１８ ．Ｃｃｎｅｔ ：Ｃｒｉｓｓ －ｃｒｏｓｓａｔｔｅｎｔｉｏｎｆｏｒｓｅｍａｎｔｉｃ  

ｓｅｇｍｅｎｔａｔｉｏｎ．２０１９ＩＥＥＥ／ＣＶＦＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎ  

（ＩＣＣＶ ），ｐａｇｅｓ６０３ －６１２ ．  

［９８ ］ＪｉｎｇｙｅＬｉ ，ＨａｏＦｅｉ ？ＪｉａｎｇＬｉｎ，Ｓｈｅｎｇｑ ｉｏｎｇＷｕ ？ＭｅｉｓｈａｎＺｈａｎｇ，ＣｈｏｎｇＴｅｎｇ，  

ＤｏｎｇｈｏｎｇＪｉ，ａｎｄＦｅｉＬｉ ．２０２２ ．Ｕｎｉｆ ｉｅｄｎａｍｅｄｅｎｔｉｔｙｒｅｃｏｇｎｉｔｉｏｎａｓｗｏｒｄ －ｗｏｒｄ  

ｒｅｌａｔｉｏｎｃｌａｓｓｉｆｉｃａｔｉｏｎ．ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＡＡＡＩＣｏｎｆｅｒｅｎｃｅｏｎＡｒｔｉｆ ｉｃｉａｌ  

Ｉｎｔｅｌｌｉｇｅｎｃｅ ，３６（１０）

：１０９６５ － １０９７３ ．  

［９９ ］ＪｉｎｇｙｅＬｉ ，ＫａｎｇＸｕ ，ＦｅｉＬｉ ，ＨａｏＦｅｉ ，ＹａｆｅｎｇＲｅｎ ，ａｎｄＤｏｎｇｈｏｎｇＪｉ ．２０２ＬＭＲＮ：  

Ａｌｏｃａｌｌｙａｎｄｇ ｌｏｂａｌｌｙｍｅｎｔｉｏｎ －ｂａｓｅｄｒｅａｓｏｎｉｎｇｎｅｔｗｏｒｋｆｏｒｄｏｃｕｍｅｎｔ － ｌｅｖｅｌ  

ｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎ．ＩｎＦｉｎｄｉｎｇｓｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ：  

ＡＣＬ －ＩＪＣＮＬＰ２０２１ ，ｐａｇｅｓ１３５９ － １３７０ ，Ｏｎｌｉｎｅ ．ＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌ  

Ｌｉｎｇｕｉｓｔｉｃｓ ．  

［ １００］ＺｈｘｉａｎｇＣｈｅｎａｎｄＴｉｅｙｕｎＱ ｉａｎ．２０２０ ．Ｒｅｌａｔｉｏｎ －ａｗａｒｅｃｏｌｌａｂｏｒａｔｉｖｅｌｅａｒ ｎｉｎｇ  

ｆｏｒｕｎｉｆ ｉｅｄａｓｐｅｃｔ －ｂａｓｅｄｓｅｎｔｉｍｅｎｔａｎａｌｙｓｉｓ ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ５８ｔｈＡｎｎｕａｌ  

ＭｅｅｔｉｎｇｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ，ｐａｇｅｓ３６８５ －３６９４ ，  

Ｏｎｌｉｎｅ ．ＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ．  

［ １０１ ］ＤａｖｉｄＳａｍｕｅｌａｎｄＭｉｌａｎＳｔｒａｋａ．２０２０ ．ＵＦＡＬａｔＭＲＰ２０２０ ：Ｐｅｒｍｕｔａｔｉｏｎ －  

ｉｎｖａｒｉａｎｔｓｅｍａｎｔｉｃｐａｒｓｉｎｇ ｉｎＰＥＲＩＮ ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＣｏＮＬＬ２０２０Ｓｈａｒｅｄ  

Ｔａｓｋ ：Ｃｒｏｓｓ －ＦｒａｍｅｗｏｒｋＭｅａｎｉｎｇＲｅｐｒｅｓｅｎｔａｔｉｏｎＰａｒｓｉｎｇ，ｐａｇｅｓ５３ －６４ ，Ｏｎｌｉｎｅ ．  

ＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ．  

８２  

参考文献  

［ １０２］ＨｅＫ ，ＺｈａｎｇＸ ５ＲｅｎＳ ５ｅｔａｌ．Ｄｅｅｐｒｅｓｉｄｕａｌｌｅａｒｎｉｎｇｆｏｒｉｍａｇｅ  

ｒｅｃｏｇｎｉｔｉｏｎ［Ｃ ］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＩＥＥＥｃｏｎｆｅｒｅｎｃｅｏｎｃｏｍｐｕｔｅｒｖｉｓｉｏｎａｎｄ  

ｐａｔｔｅｒ ｎｒｅｃｏｇｎｉｔｉｏｎ．２０１６ ：７７０ －７７８ ．  

［１０３］ＳｚｅｇｅｄｙＣ ，ＬｉｕＷ ，ＪｉａＹ ，ｅｔａｌ ．Ｇｏｉｎｇｄｅｅｐｅｒｗｉｔｈ  

ｃｏｎｖｏｌｕｔｉｏｎｓ

［Ｃ ］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＩＥＥＥｃｏｎｆｅｒｅｎｃｅｏｎｃｏｍｐｕｔｅｒｖｉｓｉｏｎａｎｄ  

ｐａｔｔｅｒｎｒｅｃｏｇｎｉｔｉｏｎ．２０１５ ：ｌ －９ 〇  

［ １０４ ］ＳｉｍｏｎｙａｎＫ，ＺｉｓｓｅｒｍａｎＡ．Ｖｅｒｙｄｅｅｐｃｏｎｖｏｌｕｔｉｏｎａｌｎｅｔｗｏｒｋｓｆｏｒｌａｒｇｅ －ｓｃａｌｅ  

ｉｍａｇｅｒｅｃｏｇｎｉｔｉｏｎ ［Ｊ ］

．ａｒＸｉｖ ｐｒｅｐｒｉｎｔａｒＸｉｖ：１４０９．１５５６ ，２０１４．  

［ １０５ ］ＭｉｎｇｈａｏＨｕ，ＹｕｘｉｎｇＰｅｎｇ，ＺｈｅｎＨｕａｎｇ，ＤｏｎｇｓｈｅｎｇＬｉ ５ａｎｄＹｉｗｅｉＬｖ．２０１９．  

Ｏｐｅｎ －ｄｏｍａｉｎｔａｒｇｅｔｅｄｓｅｎｔｉｍｅｎｔａｎａｌｙｓｉｓｖｉａｓｐａｎ －ｂａｓｅｄｅｘｔｒａｃｔｉｏｎａｎｄ  

ｃｌａｓｓｉｆ ｉｃａｔｉｏｎ ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ５７ｔｈＡｎｎｕａｌＭｅｅｔｉｎｇｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒ  

ＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ，ｐａｇｅｓ５３７

－５４６ ， Ｆｌｏｒｅｎｃｅ，Ｉｔａｌｙ

．Ａｓｓｏｃｉａｔｉｏｎｆｏｒ  

Ｃｏｍｐｕｔａｔ ｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ，  

［１０６］Ｌｉｌ ｊａ０ｖｒｅｌｉｄ ５ＰｅｔｅｒＭａｓｈｌｕｍ，ＪｅｒｅｍｙＢａｍｅｓ，ａｎｄＥｒｉｋＶｅｌｌｄａｌ ．２０２０ ．Ａｆｉｎｅ ？  

ｇｒａｉｎｅｄｓｅｎｔｉｍｅｎｔｄａｔａｓｅｔｆｏｒＮｏｒｗｅｇｉａｎ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆ ｔｈｅＴｗｅｌｆ ｔｈＬａｎｇｕａｇｅ  

ＲｅｓｏｕｒｃｅｓａｎｄＥｖａｌｕａｔｉｏｎＣｏｎｆｅｒｅｎｃｅ ，ｐａｇｅｓ５０２５ －５０３３ ，Ｍａｒｓｅｉｌｌｅ，Ｆｒａｎｃｅ ．  

ＥｕｒｏｐｅａｎＬａｎｇｕａｇｅＲｅｓｏｕｒｃｅｓＡｓｓｏｃｉａｔｉｏｎ．  

［ １０７］ＪｅｒｅｍｙＢａｍｅｓ，ＴｏｎｉＢａｄｉａ，ａｎｄＰａｔｒｉｋＬａｍｂｅｒｔ．２０１８ ．ＭｕｌｔｉＢｏｏｋｅｄ：Ａ  

ｃｏｒｐｕｓｏｆＢａｓｑｕｅａｎｄＣａｔａｌａｎｈｏｔｅｌｒｅｖｉｅｗｓａｎｎｏｔａｔｅｄｆｏｒａｓｐｅｃｔ －ｌｅｖｅｌｓｅｎｔｉｍｅｎｔ  

ｃｌａｓｓｉｆｉｃａｔｉｏｎ．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＥｌｅｖｅｎｔｈＩｎｔｅｒ ｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎ  

ＬａｎｇｕａｇｅＲｅｓｏｕｒｃｅｓａｎｄＥｖａｌｕａｔｉｏｎ（ＬＲＥＣ２０１８），Ｍｉｙａｚａｋｉ ，Ｊａｐａｎ．Ｅｕｒｏｐｅａｎ  

ＬａｎｇｕａｇｅＲｅｓｏｕｒｃｅｓＡｓｓｏｃｉａｔｉｏｎ （ＥＬＲＡ）

［ １０８ ］Ｊａｎｙｃｅ＼Ｗｅｂｅ ？ＴｈｅｒｅｓａＷｉｌｓｏｎ，ａｎｄＣｌａｉｒｅＣａｒｄｉｅ ．２００５ ．Ａｎｎｏｔａｔｉｎｇ  

ｅｘｐｒｅｓｓｉｏｎｓｏｆｏｐ ｉｎｉｏｎｓａｎｄｅｍｏｔｉｏｎｓｉｎｌａｎｇｕａｇｅ ．ＬａｎｇｕａｇｅＲｅｓｏｕｒｃｅｓａｎｄ  

Ｅｖａｌｕａｔ ｉｏｎ，３９ ：１６５ －２１０ ．  

［ １０９ ］ＣｉｇｄｅｍＴｏｐｒａｋ ，ＮｉｋｌａｓＪａｋｏｂ ，ａｎｄＩｒｙｎａＧｕｒｅｖｙｃｈ．２０１０ ．Ｓｅｎｔｅｎｃｅａｎｄ  

ｅｘｐｒｅｓｓｉｏｎｌｅｖｅｌａｎｎｏｔａｔｉｏｎｏｆ ｏｐ ｉｎｉｏｎｓｉｎｕｓｅｒ － ｇｅｎｅｒａｔｅｄｄｉｓｃｏｕｒｓｅ．ＩｎＰｒｏｃｅｅｄｉｎｇｓ  

ｏｆｔｈｅ４８ｔｈＡｎｎｕａｌＭｅｅｔｉｎｇｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ，  

ｐａｇｅｓ５７５ －５８４ ，Ｕｐｐｓａｌａ，Ｓｗｅｄｅｎ．ＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ ．  

８３  