密级
：
公开


避ｉｆｆＡ＃


硕士学位论叉


＠


题目
：
面向复杂文本的指称表达理解研究


学号
：２０２１１１１２４５


姓名
：陆明聪


学科专业
：智能科学与持太


培养方式
：全曰制


导师
：
李睿凡


学院
：人工智能学院


２０２４年
０５月
２８
日


中国
■北京


密级
：
公开


分名仰耄大聲


硕士学位论文
（学术学位）


＠


题目
：
面向复杂文本的指称表达理解研究


学号
：２０２１１１１２４５


姓名
：陆明聪


学科专业
：智能科学与技术


培养方式
：全日制


导师
：李睿凡


学院
：人工智能学院


２０２４年
５月
２８
日


？
ＢＥＩＪＩＮＧＵＮＩＶＥＲＳＩＴＹＯＦ


ＰＯＳＴＳＡＮＤ


ＴＥＬＥＣＯＭＭＵＮＩＣＡＴＩＯＮＳ


ＭａｓｔｅｒＴｈｅｓｉｓ


ＣｏｍｐｌｅｘＴｅｘｔ
－ｏｒｉｅｎｔｅｄＲｅｆｅｒｒｉｎｇＥｘｐｒｅｓｓｉｏｎ


ＣｏｍｐｒｅｈｅｎｓｉｏｎＲｅｓｅａｒｃｈ


ＳｔｕｄｅｎｔＩＤ
：２０２１１１１２４５


Ａｕｔｈｏｒ
：ＭｉｎｇｃｏｎｇＬｕ


Ｓｕｂ
ｊｅｃｔ
：ＩｎｔｅｌｌｉｇｅｎｔＳｃｉｅｎｃｅａｎｄＴｅｃｈｎｏｌｏｇｙ


Ｓｕｐｅｒｖｉｓｏｒ
：ＲｕｉｆａｎＬｉ


Ｉｎｓｔｉｔｕｔｅ
：ＳｃｈｏｏｌｏｆＡｒｔｉｆ
ｉｃｉａｌＩｎｔｅｌｌｉｇｅｎｃｅ


Ｄａｔｅ２８Ｍｏｎｔｈ５Ｙｅａｒ２０２４






摘
要


指称表达理解（ＲｅｆｅｒｒｉｎｇＥｘｐｒｅｓｓｉｏｎＣｏｍｐｒｅｈｅｎｓｉｏｎ
，ＲＥＣ
）旨在根


据给定的自然语言描述定位图像中的目标物体
，属于图文多模态领域


重要的研究方向之
一
。指称表达通常需要包含物体的外观
、属性
、位


置
、与其他物体的相互作用关系
、上下文场景知识等关键信息来指定


目标物体
，
因而指称表达文本具备较高的多样性和复杂性
。本文重点


聚焦于复杂文本场景下的指称表达理解研究
。


目前
，
主流的研究方法存在以下问题亟待解决
：
首先
，
当前方法


在进行跨模态推理时通常依托于简单的特征序列拼接和自注意力机


制
，造成文本特征淹没于图像特征中
，
导致模型无法有效提取文本中


的关键线索进行准确的定位
。此外
，
当指称表达需要结合上下文场景


知识共同定位目标物体时
，当前工作缺乏对复杂场景知识的有效处理


方案
，造成跨模态推理的准确性较差
。为此
，
本文提出充分发挥语言


信息在跨模态推理中的引导作用
，并结合大语言模型强大的通用理解


能力简化外部场景知识
，以提高模型面对复杂文本时的跨模态推理和


定位能力
。
具体而言
，
本文开展的研宄工作如下
：


一
、针对现有工作中的文本信息淹没问题
，本文设计了
一个语言


引导的ＲＥＣ推理网络（ＬａｎｇｕａｇｅＧｕｉｄｅｄＲｅａｓｏｎｉｎｇＮｅｔｗｏｒｋ
，ＬＧＲ
－


ＮＥＴ
）
，充分发挥语言的引导作用
。网络在跨模态推理过程中分离输入


的图文模态并引入跨注意力机制反复融入文本特征
，同时逐步引导图


像模态的对齐
，进而提高跨模态推理效果
。结合本文提出的坐标编码


和跨模态对齐损失
，
可以进
一步增强图文对齐的监督信号和效果
。


二
、针对指称表达中包含的复杂场景知识问题
，本文基于大语言


模型强大的指令遵循能力设计指令模板
，用于过滤无关物体的描述进


而简化复杂的场景知识
，减少对跨模态推理的干扰
。此外
，
本文进
一


步提出基于场景知识的推理网络（ＳｃｅｎｅＫｎｏｗｌｅｄｇｅＲｅａｓｏｎｉｎｇＮｅｔｗｏｒｋ
，


ＳＫＲＮ
）
，
分别提取指称表达和场景知识特征
，充分利用两者进行联合


推理
，
提高了模型处理复杂场景知识的能力以及最终的定位准确率
。


基于以上两种方法
，本文在ＲｅｆＣＯＣＯ
、
ＲｅｆＣＯＣＯ＋
、
ＲｅｆＣＯＣＯｇ
、


ＳＫ
－ＶＧ等多个公开的指称表达理解基准数据集上开展了大量实验
。


通过定量分析和定性对比
，
充分验证了所提方法的可行性和有效性
。


关键词
：
图文多模态
；
指称表达理解
；
注意力机制
；
大语ｓ
ｔ吴型


Ｉ


北京邮电大学硕士学位论文


ＡＢＳＴＲＡＣＴ


ＲｅｆｅｒｒｉｎｇＥｘｐｒｅｓｓｉｏｎＣｏｍｐｒｅｈｅｎｓｉｏｎ
（ＲＥＣ）ａｉｍｓｔｏｌｏｃａｔｅｔｈｅｔａｒｇｅｔ


ｏｂｊｅｃｔｉｎｉｍａｇｅｓａｃｃｏｒｄｉｎｇｔｏｎａｔｕｒａｌｌａｎｇｕａｇｅｄｅｓｃｒｉｐ
ｔｉｏｎｓ
，ｓｅｒｖｉｎｇａｓａ


ｃｒｕｃｉａｌｔａｓｋｉｎｔｈｅｖｉｓｉｏｎ
－ａｎｄ
－
ｌａｎｇｕａｇｅｄｏｍａｉｎ
．Ｔｈｅｒｅｆｅｒｒｉｎｇｅｘｐｒｅｓｓｉｏｎ


ｕｓｕａｌｌｙｎｅｅｄｓｔｏｃｏｎｔａｉｎｋｅｙ
ｉｎｆｏｒｍａｔｉｏｎｓｕｃｈａｓｔｈｅａｐｐｅａｒａｎｃｅ
，ａｔｔｒｉｂｕｔｅｓ
，


ｓｐａｔｉａｌｐｏｓｉｔｉｏｎ
，ｒｅｌａｔｉｏｎｓｈｉｐｗｉｔｈｏｔｈｅｒｏｂｊｅｃｔｓ
，ｃｏｎｔｅｘｔｕａｌｓｃｅｎｅ


ｋｎｏｗｌｅｄｇｅｔｏｓｐｅｃｉｆｙ
ｔｈｅｔａｒｇｅｔｏｂｊｅｃｔ
，
ｌｅａｄｉｎｇ
ｔｏｈｉｇｈｄｉｖｅｒｓｉｔｙａｎｄ


ｃｏｍｐ
ｌｅｘｉｔｙ
．ＴｈｉｓｔｈｅｓｉｓｆｏｃｕｓｅｓｏｎｔｈｅｒｅｓｅａｒｃｈｏｆＲＥＣｔａｓｋｓｉｎｃｏｍｐ
ｌｅｘ


ｔｅｘｔｓｃｅｎａｒｉｏｓ
．


Ｃｕｒｒｅｎｔｌｙ，ｍａｉｎｓｔｒｅａｍｍｅｔｈｏｄｓｓｈａｒｅｔｈｅｆｏｌｌｏｗｉｎｇｐｒｏｂｌｅｍｓ
：Ｆｉｒｓｔｌｙ，


ｅｘｉｓｔｉｎｇｍｅｔｈｏｄｓｕｓｕａｌｌｙｕｔｉｌｉｚｅｖｉｓｕａｌａｎｄｔｅｘｔｕａｌｆｅａｔｕｒｅｓｉｎａｃｏｎｎｅｃｔｅｄ
－


ａｔｔｅｎｔｉｏｎｗａｙｗｈｅｎｐｅｒｆｏｒｍｉｎｇｃｒｏｓｓ
－ｍｏｄａｌｒｅａｓｏｎｉｎｇ
．Ｔｈｉｓｗｉｌｌｌｅａｄｔｏ


ｔｅｘｔｕａｌｉｎｆｏｒｍａｔｉｏｎｏｖｅｒｗｈｅｌｍｅｄｂｙｖｉｓｕａｌｉｎｆｏｒｍａｔｉｏｎ
，ｍａｋｉｎｇ
ｉｔｄｉｆ
ｉｃｕｌｔ


ｆｏｒｍｏｄｅｌｔｏｅｆｆｅｃｔｉｖｅｌｙｅｘｔｒａｃｔｋｅｙｃｌｕｅｓｆｒｏｍｔｈｅｔｅｘｔｆｏｒａｃｃｕｒａｔｅ


ｌｏｃａｌｉｚａｔｉｏｎ
．Ｓｅｃｏｎｄｌｙ，ｗｈｅｎｔｈｅｍｏｄｅｌｎｅｅｄｓｔｏｃｏｍｂｉｎｅｒｅｆｅｒｒｉｎｇ


ｅｘｐｒｅｓｓｉｏｎａｎｄｃｏｎｔｅｘｔｕａｌｓｃｅｎｅｋｎｏｗｌｅｄｇｅｔｏｌｏｃａｔｅｔｈｅｔａｒｇｅｔｏｂｊｅｃｔ
，


ｃｕｒｒｅｎｔｗｏｒｋｓｌａｃｋｅｆｆｅｃｔｉｖｅｓｏｌｕｔｉｏｎｓｆｏｒｃｏｍｐ
ｌｅｘｓｃｅｎｅｋｎｏｗｌｅｄｇｅ
．Ｔｏ


ａｄｄｒｅｓｓｔｈｅｓｅｃｈａｌｌｅｎｇｅｓ
，ｗｅｐｒｏｐｏｓｅｔｏｌｅｖｅｒａｇｅｔｈｅｇｕｉｄｉｎｇｒｏｌｅｏｆｔｈｅ


ｌａｎｇｕａｇｅｉｎｃｒｏｓｓ
－ｍｏｄａｌｒｅａｓｏｎｉｎｇ
．Ｉｎａｄｄｉｔｉｏｎ
，ｗｅ
ｐｒｏｐｏｓｅｔｏｕｔｉｌｉｚｅｌａｒｇｅ


ｌａｎｇｕａｇｅｍｏｄｅｌｓｔｏｓｉｍｐ
ｌｉｆ
ｙｃｏｍｐ
ｌｅｘｓｃｅｎｅｄｅｓｃｒｉｐ
ｔｉｏｎｓ
，ｔｈｅｒｅｂｙ


ｅｎｈａｎｃｉｎｇ
ｔｈｅｃｒｏｓｓ
－ｍｏｄａｌｒｅａｓｏｎｉｎｇａｂｉｌｉｔｉｅｓａｎｄｌｏｃａｌｉｚａｔｉｏｎａｃｃｕｒａｃｙ
．


Ｓｐｅｃｉｆｉｃａｌｌｙ，ｏｕｒｗｏｒｋｓａｒｅａｓｆｏｌｌｏｗｓ
：


Ｆｉｒｓｔｌｙ，ｔｏａｄｄｒｅｓｓｔｈｅｉｓｓｕｅｏｆｔｅｘｔｉｎｆｏｒｍａｔｉｏｎｏｖｅｒｗｈｅｌｍ
，ｗｅｄｅｓｉｇｎ


ｏｕｒＬＧＲ
－ＮＥＴ
：ＬａｎｇｕａｇｅＧｕｉｄｅｄＲｅａｓｏｎｉｎｇＮｅｔｗｏｒｋｆｏｒＲｅｆｅｒｒｉｎｇ


ＥｘｐｒｅｓｓｉｏｎＣｏｍｐｒｅｈｅｎｓｉｏｎ
．Ｔｈｅｍｏｄｅｌｆｕｌｌｙｅｘｐ
ｌｏｉｔｓｔｈｅｇｕｉｄａｎｃｅｏｆ


ｌａｎｇｕａｇｅｂｙｓｅｐａｒａｔｉｎｇ
ｔｗｏｍｏｄａｌｉｔｉｅｓａｎｄｌｅｖｅｒａｇｅｃｒｏｓｓ
－ａｔｔｅｎｔｉｏｎ


ｍｅｃｈａｎｉｓｍｓｔｏｉｔｅｒａｔｉｖｅｌｙ
ｉｎｃｏｒｐｏｒａｔｅｔｅｘｔｕａｌｆｅａｔｕｒｅｓ
，ｐｒｏｇｒｅｓｓｉｖｅｌｙ


ｇｕｉｄｉｎｇ
ｔｈｅａｌｉｇｎｍｅｎｔｏｆｉｍａｇｅｍｏｄａｌｉｔｉｅｓ
．Ｆｕｒｔｈｅｒ
，ｗｅｄｅｓｉｇｎａｎｏｖｅｌ


ｃｏｏｒｄｉｎａｔｅｅｍｂｅｄｄｉｎｇａｎｄｃｒｏｓｓ
－ｍｏｄａｌａｌｉｇｎｍｅｎｔｌｏｓｓｔｏｓｔｒｅｎｇ
ｔｈｅｎｔｈｅ


ｆ
ｉｎｅ
－
ｇｒａｉｎｅｄａｌｉｇｎｍｅｎｔｂｅｔｗｅｅｎｉｍａｇｅａｎｄｔｅｘｔ
．


Ｓｅｃｏｎｄｌｙ，ｔｏｔａｃｋｌｅｔｈｅｐｒｏｂｌｅｍｏｆｃｏｍｐ
ｌｅｘｓｃｅｎｅｄｅｓｃｒ
ｉｐ
ｔｉｏｎｓｉｎ


ｒｅｆｅｒｒｉｎｇｅｘｐｒｅｓｓｉｏｎｓ
，ｗｅｄｅｓｉｇｎｐｒｏｍｐ
ｔｔｅｍｐ
ｌａｔｅｓｔｏｓｉｍｐ
ｌｉｆｙｓｃｅｎｅ


ＩＩ


ｍｍ


ｋｎｏｗｌｅｄｇｅｂａｓｅｄｏｎＬａｒｇｅＬａｎｇｕａｇｅＭｏｄｅｌｓ
．Ｔｈｅｓｉｍｐ
ｌｉｆｉｅｄｓｃｅｎｅ


ｋｎｏｗｌｅｄｇｅｒｅｔａｉｎｓｏｎｌｙ
ｔｈｅｄｅｓｃｒｉｐ
ｔｉｏｎｓｒｅｌｅｖａｎｔｔｏｔｈｅｒｅｆｅｒｒｅｄｏｂｊｅｃｔ
，


ｒｅｄｕｃｉｎｇ
ｉｎｔｅｒｆｅｒｅｎｃｅｉｎｃｒｏｓｓ
－ｍｏｄａｌｒｅａｓｏｎｉｎｇ
．Ｆｕｒｔｈｅｒ
，ｗｅｄｅｓｉｇｎａＳｃｅｎｅ


ＫｎｏｗｌｅｄｇｅＲｅａｓｏｎｉｎｇＮｅｔｗｏｒｋ
（ＳＩＣＲＮ）ｔｏｓｅｐａｒａｔｅｒｅｆｅｒｒ
ｉｎｇｅｘｐｒｅｓｓｉｏｎｓ


ａｎｄｓｃｅｎｅｋｎｏｗｌｅｄｇｅｔｏｆｕｌｌｙｕｔｉｌｉｚｅｔｈｅｍｆｏｒ
ｊｏｉｎｔｒｅａｓｏｎｉｎｇ，ｉｔｉｍｐｒｏｖｅｓ


ｔｈｅｍｏｄｅｌ
’
ｓａｂｉｌｉｔｙ
ｔｏｈａｎｄｌｅｃｏｍｐ
ｌｅｘｓｃｅｎｅｋｎｏｗｌｅｄｇｅａｎｄｌｏｃａｌｉｚａｔｉｏｎ


ａｃｃｕｒａｃｙ
．


Ｉｎｔｈｉｓｔｈｅｓｉｓ
，ａｌａｒｇｅｎｕｍｂｅｒｏｆｅｘｐｅｒｉｍｅｎｔｓａｒｅｃｏｎｄｕｃｔｅｄｏｎｓｅｖｅｒａｌ


ｐｕｂｌｉｃＲＥＣｂｅｎｃｈｍａｒｋｄａｔａｓｅｔｓ
，ｉｎｃｌｕｄｉｎｇＲｅｆＣＯＣＯ
，ＲｅｆＣＯＣＯ＋
，


ＲｅｆＣＯＣＯｇ，ａｎｄＳＫ
－ＶＧ．Ｔｈｒｏｕｇｈｑｕａｎｔｉｔａｔｉｖｅｃｏｍｐａｒｉｓｏｎｓａｎｄ


ｑｕａｌｉｔａｔｉｖｅａｎａｌｙｓｅｓ
，ｔｈｅｆｅａｓｉｂｉｌｉｔｙａｎｄｅｆｆｅｃｔｉｖｅｎｅｓｓｏｆｏｕｒｐｒｏｐｏｓｅｄ


ｍｅｔｈｏｄｓａｒｅｔｈｏｒｏｕｇｈｌｙｖａｌｉｄａｔｅｄ
．


ＫＥＹＷＯＲＤＳ：ｖｉｓｉｏｎａｎｄｌａｎｇｕａｇｅ
；ｒｅｆｅｒｒｉｎｇｅｘｐｒｅｓｓｉｏｎ


ｃｏｍｐｒｅｈｅｎｓｉｏｎ
；ａｔｔｅｎｔｉｏｎｍｅｃｈａｎｉｓｍ
；ｌａｒｇｅｌａｎｇｕａｇｅｍｏｄｅｌｓ


ｍ


目
录


第
一章绪论
１


１
．
１研究背景及其意义
１


１
．２研宄现状及分析
２


１
．２
．１
指称表达理解
２


１
．２
．２
图像场景知识
６


１
．３研究内容和贡献
７


１
．３
．
１现存问题
７


１
．３
．２研究内容
８


１
．３
．３贡献及创新点
８


１
．４本文的组织结构
９


１
．５本章小结
１０


第二章基础知识
１１


２
．
１文本编码器
１
１


２
．１
．１
词向量
１
１


２
．１
．２Ｔｒ
ａｎｓｆｏｒｍｅｒ网络
１２


２
．
１
．３预训练语言模型
１４


２
．２视觉编码器
１６


２
．２
．１卷积神经网络
１６


２
．２
．２视觉Ｔｒ
ａｎｓｆ
ｏｒｍｅｒ网络


１７


２
．２
．３
目标检测模型
１９


２．３指称表达理解范式
２０


２
．３
．１基于提议的模型
２１


２
．３
．２基于锚框的模型
２１


２
．３
．３基乎Ｔｒａｎｓｆ
ｏｒｍｅｒ的模型
２２


２
．４数据集及评测指标
２２


２
．４
．１
指称表达理解基准数据集
２２


２
．４
．２指称表达理解任务评价指标
２３


２
．５本章小结

２４


第三章语言引导的指称表达理解研宄
２５


３
．
１本章引论
２５


３
．２语言引导的指称表达理解推理网络
２６


３
．２
．
１框架总览
２６


３
．２
．２文本提取模块
２７


３
．２
．３视觉提取模块
２７


３
．２
．４文本特征扩展模块
２７


３
．２
．５跨模态推理与定位框预测模块
２８


３
．２
．６损失函数
２９


３
．３实验对比与分析
３０


３
．３
．
１
实验参数弓设置
３０


３
．３
．２评测指标对比与分析
３１


３
．３
．３
消融实验
３７


３
．３
．４口
Ｊ
．
视化分析
４０


３
．３
．５
定性分析
４３


３
．４本章小结
４４


第四章基于场景知识的指称表达理解研究
４５


４
．
１本章引论
４５


４
．２复杂场景知识的处理方法
４７


４
．２
．
１基于大语言模型的数据生成方法

４７


４
．２
．２生成数据的评估方案
４９


４
．３面向场景知识的指称表达理解推理网络
５２


４
．３
．
１
模型总览
５２


４
．３
．２结合场景知识的指称表达推理模块
５２


４
．３
．３模型损失函数
５３


４
．４实验对比与分析
５４


４
．４
．
１实验参数与设置
５４


４
．４
．２评测指标对比与分析
５４


４
．４
．３
消融实验
５６


４
．４
．４定性分析
５７


４
．５本章小结
５８


第五章
总结与展望
，
５９


５
．
１
工作总结
５９


５
．２未来工作展望
６０


参考文献
６１


第
一章绪论


第
一章绪论


１
．１研究背景及其意义


近年来
，
基于深度学习
［
１】的人工智能技术发展迅猛
，
在计算机视觉
，
自然语


言处理
，
以及语音信号处理等领域都取得了突破性的进展
。这些技术发展也带来


了巨大的社会变革和经济收益
，如提高安防效率的人脸识别系统
，
比肩专业从业


者的高水平机器翻译
，各种智能语音助理
，
以及最近
一年来流行的大模型技术及


其应用
。


２０１２年ＡｌｅｘＮｅｔ
ｆ
２］在当年的ＩｍａｇｅＮｅｔＬａｒｇｅＳｃａｌｅＶｉｓｕａｌＲｅｃｏｇｎｉｔ
ｉｏｎ


Ｃｈａｌｌｅｎｇｅ（ＩＬＳＶＲＣ）上取得了突破性的成绩
，大幅提高了图像分类任务的准确率
。


从此开启了人工智能快速发展的时代
，计算机在处理图像分类、
目标检测
、语义


分割等多种计算机视觉任务上的性能不断被刷新
。


自然语言处理（ＮＬＰ）从早期的探索词嵌入表示到后来的利用循环神经网络进


行序列建模
，
再到最近以
Ｔｒａｎｓｆ
ｏｒｍｅｒ为代表的新型架构增强自然语言的建模表


征能力
。
如今的大模型更是极大提升了自然语言处理技术的应用价值
。


随着互联网的发展
，如今以图文多模态形式呈现的内容越来越多
，
占比也越


来越高
。与此同时
，对于多模态数据的利用和分析也越来越迫切
，
相比于单
一模


态的处理
，
多模态信息处理不仅需要对各自模态的准确表征
，
同时要求相应的系


统对于模态之间的关系具备良好的建模能力
。基于多模态的应用也与日俱増
，如


图文检索
，
多模态知识图谱推理等。综上所述
，开展多模态相关技术的研究具备


重大的学术价值和社会效益
。


指称表达理解
（Ｒｅｆｅｒｒ
ｉｎｇＥｘｐｒｅｓｓｉｏｎＣｏｍｐｒｅｈｅｎｓｉｏｎ）
，
旨在根据自然语言形


式的指称表达定位图片中的目标物体
，是图文多模态领域
一个重要的任务。针对


指称表达理解的研究有助于推动多种下游图文多模态应用的发展
，
如图文匹配
，


密集图像描述
，视觉导航等
另外
，
作为
一个细粒度图文对齐任务
，
其


研究进展体现了模型对图文内容的通用理解能力
，
并且也是具身智能（Ｅｍｂｏｄｉｅｄ


Ｉｎｔｅｌｌｉｇｅｎｃｅ）发展所依赖的重要能力之
一
。与传统基于封闭的
、预定义的类别标签


的目标检测任务不同
，指称表达理解在推理时接受以自然语言描述的自由文本作


为输入
，要求模型同时结合文本和图像进行联合的跨模态推理
，才能从包含多个


物体的图片中识别和定位出文本指定的目标物体
，比目标检测这
一传统计算机视


觉（ＣＶ）任务更具有挑战性
。指称表达（Ｒｅｆｅｒｒ
ｉｎｇＥｘｐｒｅｓｓｉｏｎ）是指描述场景中特定


１


北京邮电大学硕士学位论文


物体的自然语言表达
，人们经常使用指称表达区分场景中的多个物体
，
如
“ 穿着


蓝色衣服的人
”
、
“ 左边的那只狗
”
。


指称表达
：
Ａｍａｎｂｅｎｄｉｎｇｏｖｅｒ
、模型定位结果
＇


ｔｏｕｃｈｉｎｇａｈｏｒｓｅｆｏｏｔ


图
１
－
１
指称表达理解任务


在现实应用中
，
一个场景通常包含多个不同的物体
，它们对应着不同的类别
，


包含不同的属性
，具备不同的空间位置关系
，并且不同物体之间也有丰富的相对


关系
。指称表达通常包含这些描述
，
并据此唯
一确定图片中的目标物体
，
因此指


称表达理解要求模型根据自由多变的指称表达定位图片中的物体
。此外还会包含


丰富的场景知识
，进
一步提高指称表达的文本长度
。这
一方面造成指称表达文本


的复杂性较高
，
另
一方面对模型的跨模态理解能力提出了更高的要求
。这种理解


能力对于人工智能在现实世界中的应用至关重要
，因为它模拟了人类如何通过语


言和视觉信息进行交流和理解
。从应用角度来看
，
在ＶＲ领域
，
能够理解自然语


言描述并将其与虚拟环境中的对象关联起来
，对于提升用户体验和交互效率至关


重要
，
ＲＥＣ技术可以帮助用户更自然地与虚拟世界互动
。在机器人导航中
，理解


周围环境的自然语言描述对于任务执行是有必要的
，
ＲＥＣ技术可以帮助此类系


统更好地理解人类指令和环境信息
。另外
，对于视障人士
，
可以根据指称表达理


解技术开发出辅助工具
，帮助他们通过语言描述识别和理解周围环境
，提高生活


质量
。


因此
，开展面向复杂文本的指称表达理解的研宄不仅能推动人工智能领域的


技术进步
，
也能为多个行业和日常生活带来实际的应用价值
。


１
．２研究现状及分析


本节将介绍指称表达理解任务的国内外研究现状
，并补充关于图像场景知识


的指称表达介绍
。


１
．２
．
１指称表达理解


指称表达理解（ＲｅｆｅｒｒｉｎｇＥｘｐｒｅｓｓｉｏｎＣｏｍｐｒｅｈｅｎｓｉｏｎ）任务
，
通常又称为视觉定


位（ＶｉｓｕａｌＧｒｏｕｎｄｉｎｇ）任务
，
旨在根据给定图像和句子形式的指称表达定位目标物


体
，
目标物体唯
一并且通常情况下定位的标注形式为矩形框的形式
。
另外
，存在


２


第
一章
绪论


多种输入输出形式类似的多模态任务
，本小节首先对它们进行简单的梳理
。从图


像的角度来看对目标物体的标注包含矩形框和像素级掩码两种方式
，后者对应指


称表达分割（ＲｅｆｅｒｒｉｎｇＥｘｐｒｅｓｓｉｏｎＳｅｇｍｅｎｔａｔｉｏｎ
，ＲＥＳ）任务
。
从文本角度来看包括


句子形式和短语形式的标注
，
后者对应短语定位（ＰｈｒａｓｅＧｒ
ｏｕｎｄｉｎｇ）任务
，
并且通


常会定位多个物体
。为说明本文研究内容的特点
，在此将上述任务以及传统的目


标检测任务统
一称为视觉定位任务
，
具体任务形式和区别如图
１
－２所示
。本文重


点关注方框标注的两个任务
，
由于输入文本的复杂度更高
，它们对模型的跨模态


推理能力要求更高
。


定ｃａｔ（多个）
：定
：定
个
）
定
个）
｜


．… …亍
—
．
．亍
．
〒
－
．


１＊？（视觉）輕觀＋语言）？（觀
＋语言
）模型
（願＋语言
）


ｌ
．１
＾
Ｉ
；
广丄」
：
―Ｌ：
＇
…丄―
Ｉ
＇


ｓｓ
ｉ
ｉＭ
ｉ
ｉ
：＊
｜
Ｉ


图片图片图片
丨
丨
表达
｜
丨图片＋
；
＇
？
－… 乂
场景
ｉ


Ｌ…
… …
？
？
？
？
？
．
，
：Ｉ
Ｌ？
、
．
？…
？
．… ｊＩ
，
？… …
？
？
？
？
．
．」
Ｉ
？
．
…ｍ
目标检测
短语接地指称表达理解基于场景知识的指称表


（Ｏｂ
ｊｅｃｔＤｅ
ｔｅｃｔ
ｉｏｎ
）
（ＰｈｒａｓｅＧｒｏｕｎｄ
ｉｎｇ
）
（Ｒｅｆｅｒｒ
ｉｎｇＥｘｐ
ｒｅｓｓ
ｉｏｎ达理解


Ｃｏｍｐｒｅｈｅｎｓ
ｉｏｎ
）
（Ｒｅｆｅｒｒ
ｉｎｇＥｘｐ
ｒｅｓｓ
ｉｏｎ


Ｃｏｍｐ
ｒｅｈｅｎｓ
ｉｏｎＧ
ｉｖｅｎ


ＳｃｅｎｅＫｎｏｗ
ｌｅｄｇｅ
）


图
１
－２视觉定位任务分类


此外
，
从模型训练的监督信号来源可以区分为有监督
（ｓｕｐｅｒｖｉｓｅｄ）和弱监督


（ｗｅａｋｌｙｓｕｐｅｒｖｉｓｅｄ
）两类任务
。
前者在模型训练时包含（图像
，
指称表达
，
定位框）


完整的数据标注
，可以有效监督模型的学习
。弱监督指称表达理解在训练时只提


供图像和指称表达标注
，
由于缺乏来自定位框的监督信号
，通常需要构造伪标签


来监督模型的训练
，
包括梯度热力图损失
，
文本重建损失等
。综上所述
，
指称表


达理解作为
一个重要的图文多模态任务
，与其类似的任务多种多样
，并且根据模


型训练时监督信号的不同也有多种不同的子任务
。本文关注对跨模态推理要求更


高
，
监督信号明确的有监督指称表达理解任务
。


指称表达理解至今已经经历了较长的发展时间
，其中出现的方法繁多
。现有


方法可以划分为基于提议的（Ｐｒｏｐｏｓａｌ
－ｂａｓｅｄ）方法
，基于锚框的
（Ａｎｃｈｏｒ
－ｂａｓｅｄ
）方法
，


以及基于Ｔｒａｎｓｆｏｒｍｅｒ的（Ｔｒａｎｓｆｏｒｍｅｒ
－ｂａｓｅｄ
）方法
。


基于提议的方法通常可以分成两个阶段
。首先
，第
一阶段会对给定的图像生


成可能包含目标物体的区域提议（Ｒｅｇ
ｉｏｎＰｒｏｐｏｓａｌｓ）集合并提取其特征
，
然后第二


阶段对候选区域和给定的指称表达文本特征进行联合建模并预测最终的目标区


域
。其中第
一阶段生成区域提议的过程通常会使用
一些现成的方案
，例如Ｒｅｇ
ｉｏｎ


ＰｒｏｐｏｓａｌＮｅｔｗｏｒｋ（ＲＰＮ
）
［８］
，ＳｅｌｅｃｔｉｖｅＳｅａｒｃｈ（ＳＳ）
［９
］等
。
大部分基于提议的方法都


３


北京邮电大学硕士学位论文


在针对第二阶段的图文联合建模做优化
，
这些联合建模方法分为联合编码方法
、


模块化网络方法
、
以及图方法
。
联合编码（ＪｏｉｎｔＥｍｂｅｄｄｉｎｇ）方法通常使用相应的


编码器将图像
、
文本分别编码映射至相同的向量空间
。
Ｒｏｈｒｂａｃｈ等人
［
１Ｑ
］使用


ＬＳＴＭ和ＣＮＮ分别对文本和图像进行编码
，
利用排序损失（ＲａｎｋｉｎｇＬｏｓｓ）获取候


选提议与自然语言特征的匹配关系预测目标物体
。Ｎａｇａｒａ
ｊａ等人Ｍ考虑所提取的


候选提议之间的关系
，
利用
ＬＳＴＭ将图像区域对的特征与文本特征融合进行预


测
。
类似地
，
Ｙｕ等Ｍ提出了
一种学习物体之间关系的方法
，
利用多个候选区域


结合指称表达进行目标物体的预测定位
，此外他们还在该工作中公开了三个常用


的基准数据集
：
ＲｅｆＣＯＣＯ
、
ＲｅｆＣＯＣＯ＋和
ＲｅｆＣＯＣＯｇ
。
模块化网络
（Ｍｏｄｕｌａｒ


Ｎｅｔｗｏｒｋ）方法通过对指称表达进行细粒度的区分
，提升模型对于物体关系描述的


理解以及图像上下文的关注
。
Ｈｕ等人
［
１３
］将指称表达拆解为主语
、
宾语和关系
，


计算每个候选区域对于主语
、
宾语
、
关系的分数
，
综合分数结果确定目标区域
。


Ｙｕ等人
在上述基础上增加对位置信息处理的模块
，
此外通过引入注意力机制


提升对指称表达各部分的建模效果
，提高了定位准确率
。
图方法通常以物体作为


节点
，
物体间关系作为边构造图
，
通过在构造的图上推理进行目标物体的预测
。


Ｗａｎｇ等人
［
１
５
］构造的语言引导的图注意力网络包括语言自注意力模块
，
语言引导


注意力模块以及匹配模块
，其中第
一个模块通过在文本上做自注意力将其分解为


物体本身
、
同类物体间关系和不同种类物体间关系三部分描述
，
以此引导物体节


点间关系的构建进行推理
。
Ｙａｎｇ等人
［
１６
」提出动态图注意力网络
，
通过将文本信


息融入以图片物体为节点构建的图中
，以及在图上逐步推理实现目标物体的定位
。


总体而言
，基于提议的方法具有推理方法丰富
，
定位相对准确的优点
。然而


由于其第
一阶段需要生成大量候选提议
，且通常需要结合非极大化抑制（ＮＭＳ）去


除重复区域
，导致计算开销大
，推理速度较慢
。因此
，近年来无需生成候选提议
、


推理速度快的基于锚框的方法也引起了关注
。基于锚框的方法通常利用基于锚框


的目标检测器
，如ＹＯＬＯｖ３
［
１
７
］等
，进行图像特征的提取
，并在这个过程中融入文


本表示
，
最终得到样本的多模态特征表示并直接预测目标物体
。
Ｙａｎｇ等人
［
１８
］基


于ＹＯＬＯｖ３
目标检测器构建了
一阶段框架
，他们利用ＤａｒｋＮｅｔ骨干网络（Ｂａｃｋｂｏｎｅ
）


提取图片多尺度特征
，
得到多层不同分辨率的特征图
。对于每层特征图
，提取自


ＢＥＲＴ的文本特征与其进行拼接并输入预测头进行目标区域的预测
。Ｌｉａｏ等人
［
１９
］


提出
一个实时跨模态相关滤波方法
，
将
ＲＥＣ
任务重定义为相关滤波过程


（ＣｏｒｒｅｌａｔｉｏｎＦｉｌｔｅｒｉｎｇ）
，
通过计算指称表达与图像之间的相关图（ＣｏｒｒｅｌａｔｉｏｎＭａｐ）


进行目标物体中心的预测
，再预测定位框的宽和高
，最终确定目标物体
。Ｙａｎｇ等


人
［２Ｇ
］在工作
［
１
８
］的基础上通过引入多轮递归子查询提升模型的推理能力
，
以及面


对复杂指称表达时的定位准确率
。
Ｓｕｎ等人
［２
１
］将ＲＥＣ建模成
一个逐步调整预测


４


第
… 章
绪论


框的序列推理过程
，开始时预测框包含整张图片
，通过逐步调整预测框的大小和


位置实现目标物体的定位
。通过引入马尔可夫过程和深度强化学习来完成模型的


学习和训练
。


然而
，
无论是基于提议还是基于锚框的方法都依托于现成的目标检测模型
，


在此基础上构建的ＲＥＣ模型性能受限于所依赖的目标检测准确率
。
以基于提议


的方法为例
，若模型在第
一阶段生成的候选提议集合就不包含目标物体
，
那么第


二阶段将很难预测争取的目标物体
。
因此
，
最近的ＲＥＣ方法都基于Ｔｒａｎｓｆｏｒｍｅｒ


进行跨模态推理
，
同时直接预测目标物体
。


最近几年
Ｔｒａｎｓｆｏｒｍｅｒ不仅在
ＮＬＰ领域发展迅速
，
在计算机视觉（ＣＶ
）领域


同样取得了重大的进展
。
ＶｉｓｉｏｎＴｒａｎｓｆｏｒｍｅｒ（ＶｉＴ
）
［２２
］证明基于注意力机制的


Ｔｒ
ａｎｓｆｏｒｍｅｒ架构在图像特征的提取上能比肩当前最好的卷积神经网络
，
并且在


下游的图像分类任务上也与性能最好的卷积神经网络模型相当
。
Ｓｗｉｎ


Ｔｍｎｓｆ
ｏｒｍｅｉ＾
ｌ保留卷积神经网络对图像的局部性的先验
，
改进了ＶｉＴ
的注意力


机制而引入窗口注意力
，
提高了Ｔｒａｎｓｆｏｒｍｅｒ对高分辨率图像特征的建模能力
，


使得
Ｔｒａｎｓｆｏｒｍｅｒ骨架也能胜任密集型图像任务如目标检测
、
实例分割等
。


ＤＥＴＲ
［２４
］及其后续工作＿胃７
１
］将目标检测任务视作集合预测任务
，并使用
一套基


于
Ｔｒａｎｓｆｏｒｍｅｒ的框架直接预测目标物体框坐标
。
避免引入复杂的候选提议或者


预定义的锚框集合
，也因此无需使用非极大化抑制等后处理操作
，真正意义上实


现了目标检测任务的端到端实现
，开创了目标检测的新范式
。在上述工作的基础


上
，
指称表达理解的方法也得到了进
一步扩展
。
Ｄｅｎｇ等人
〖２５
］首次构建了基于


Ｔｒａｎｓｆｏｒｍｅｒ的
ＲＥＣ框架
，
使用
Ｔｒａｎｓｆｏｒｍｅｒ中的自注意力机制融合提取的两个


模态的特征并直接输出目标物体的框坐标
。
在此基础上
，
Ｄｅｎｇ等人
［２６
］进
一步调


整模型架构
，直接在ＶｉＴ基础上添加额外的注意力模块来融入文本特征
，进而实


现纯Ｔｒａｎｓｆｏｒｍｅｒ架构的ＲＥＣ模型
。
Ｙｅ等人
［２６
］在工作＿的基础上在将文本信息


引入图像特征提取器进而优化图像特征的表示
。
Ｙａｎｇ等人
［２８项
１
」是在模态融合端


引入额外的图文交互模块
，针对提取的图像特征先进行增强与优化
，再进行后续


的定位框预测
。提高预测准确率
。Ｌｉ＾］和
Ｓｕ
ｆＷ等人通过同时引入ＲＥＣ和ＲＥＳ两


个任务的标注数据进行多任务联合训练
，
ＲＥＣ和ＲＥＳ的区别在于定位目标物体


时的输出粒度
，
在跨模态融合和推理的角度两者是高度相似的
。
因此
，
此类方法


通过统
一的模型推理架构和不同的输出预测头即可实现多任务的联合学习
，额外


的标注数据也促进了模型的学习效果和最终的定位性能
。


最近
，
受到
Ｔｒａｎｓｆ
ｏｒｍｅｒ在自然语言处理和计算机视觉领域的启发
，
有许多


工作
琎
一步开展了视觉
＿语言预训练
。通常来说
，这些模型使用


大规模的图像
－文本对来提高模型在多模态理解方面的性能
，
并将这种能力转移


５


北京邮电大学硕士学位论文


到下游任务
，
如指称表达理解和视觉问答
。
ＣＬＩＰ
［３５
］收集了４亿图像
－文本对进行


预训练
，
并通过对比学习实现了显著的图像
－文本对齐性能
。
ＯＦＡ
［３２
］通过
一个简


单的序列到序列（Ｓｅｑｕｅｎｃｅｔｏｓｅｑｕｅｎｃｅ
）框架统
一了不同的跨模态任务
。
ＢＬＩＰ２
［３７
］


和ＭｉｎｉＧＰＴ４
［，＃大型语言模型的通用能力迁移到多模态场景中
。


综上所述
，
一方面指称表达理解的发展过程与目标检测领域的发展具备
一定


的相关性
，
另
一方面由于文本模态的加入
，
涌现出更加丰富多样的跨模态推理方


法
，
各种ＲＥＣ范式相互促进
，
螺旋式发展
。
总体来看
，
最近的Ｔｒａｎｓｆｏｒｍｅｒ架构


在指称表达理解任务上展现了强大的性能
，
是未来的
一个重要方向
。


１
．２
．２图像场景知识


图像场景知识通常指的是对图像内容的理解
，包括但不限于识别图像中的物


体
、场景
、动作以及它们之间的关系
。这种知识使得计算机能够识别图像中的元


素
，并理解它们在特定上下文中的意义
。例如
，
图像场景知识不仅可以帮助计算


机识别出图像中的
“ 人
”
、
“ 狗
”
、
“ 树
” 等对象
，
以及它们的位置
、
大小
、颜色等


属性
，
甚至能够理解这些对象之间的互动
，
如
“ 狗在追逐球
”
。
计算机视觉的目


标是使计算机能够像人类
一样
“ 看
” 和理解图像
。
这涉及图像分类
、
物体检测
、


图像分割
、
目标跟踪
、场景理解等任务
。
例如
，
图像分类任务旨在将图像分配到


预定义的类别中
，
如识别图像中的猫
、狗或汽车
；
物体检测则进
一步确定图像中


特定物体的位置和类别
；
图像分割则将图像分割成多个部分
，并对每个部分进行


精确的类别标注
；场景理解则是对整个场景进行分析
，理解场景中的物体如何相


互作用
。在图像场景知识的应用中
，ＮＬＰ也可以帮助计算机理解与图像相关的文


本描述
，
如图像标题
、
标签或注释
。
这涉及到图像描述（ＩｍａｇｅＣａｐ
ｔｉｏｎｉｎｇ）
、
视觉


问答（ＶｉｓｕａｌＱｕｅｓｔｉｏｎＡｎｓｗｅｒ
ｉｎｇ，ＶＱＡ
）和多模态检索等任务
。
图像描述任务要求


模型生成描述图像内容的自然语言句子
；视觉问答则是回答关于图像内容的问题
；


多模态检索则是根据文本查询在图像数据库中找到相关图像
。


不同于目标检测任务仅仅要求定位物体并识别类别
，在指称表达理解任务中
，


常见的指称表达都会包含图片中的场景信息如物体类别
，
位置
，
大小
，
颜色
，
状


态
，
以及与其他对象的相对关系
［３９
］
，并结合这些场景信息进行丰富的组合来指定


目标物体
。这要求模型对指称表达中的场景知识具备良好的理解能力
，
才能结合


图片定位出正确的物体对象
。


一方面图像场景知识提供了识别图像中对象的基础
，而指称表达需要在此基


础上精确定位这些对象
，
并用语言描述它们
。另
一方面
，
图像场景知识有助于理


解图像的上下文
，这对于指称表达的标注至关重要
，指称表达还需要包括对象之


间的关系
，
以便模型根据对象之间的相互关系识别目标物体
。
最近
，
Ｃｈｅｎ等人


［４Ｑ
］提出基于场景知识的指称表达理解任务（ＳＫ
－ＲＥＣ
）
，
不同于传统指称表达理解


６


第
一章
绪论


任务仅仅依靠指称表达即可实现对目标物体的定位
，
ＳＫ
－ＲＥＣ要求模型根据指称


表达以及外部的复杂场景知识进行联合推理才能确定目标物体
。这对模型的复杂


外部场景知识理解能力以及跨模态推理能力提出了更高的要求
。


综上所述
，
图像场景知识常常蕴含于指称表达中
，对指称表达理解任务的完


成有重要的作用
。此外
，
有时候仅仅依靠指称表达还不足以确定目标物体
，
而需


要进
一步借助外部的场景知识作为补充进行联合推理才能确定图片中的对象
。


１
．３研究内容和贡献


尽管研宄者们在指称表达理解任务中进行了各种方向的探索
，也取得了许多


阶段性的结果
，但是当前模型面对相对长而复杂的指称表达文本
，尤其当需要结


合复杂的场景知识进行联合推理时
，
对目标物体的定位效果还有待提升
。


１
．３
．
１现存问题


通过对相关研究工作的梳理
，
可以发现以下亟待解决的问题
：


１）基于
Ｔｒａｎｓｆｏｒｍｅｒ的ＲＥＣ模型在跨模态推理过程中存在文本信息淹没


问题
，
导致对文本特征利用不足


指称表达理解作为
一个图文多模态任务
，其性能表现非常依赖模型的跨模态


推理效果
。
当前基于Ｔｒａｎｓｆｏｒｍｅｒ的ＲＥＣ方法通常使用
ｔｒａｎｓｆｏｒｍｅｒｅｎｃｏｄｅｒ结构


中的自注意力机制对拼接的图文特征序列进行模态融合和推理
。然而
，这种简单


的特征拼接方式会导致文本信息被淹没在图像信息中
。具体来说
，
由于文本模态


和图像模态的抽象层级存在较大的差异
。文本通常具备更高的抽象层次
，
一个指


称表达包含的单词数量最多为几十个
。而图像往往会具备更多的细节信息
，会包


含目标物体
，
其他相关物体
，
背景等丰富的视觉信息以及上下文语义信息
。
以常


见的指称表达模型实现举例
，通常设置文本最大
ｔｏｋｅｎ数量为４０
，
图像分辨率为


６４０ｘ６４０
，
经过常见的图像编码器如ＲｅｓＮｅｔ
［４
１域者
ＳｗｉｎＴｒａｎｓｆｏｒｍｅｒ
［２３＿３２倍


降采样后
，
图像特征数量为
２０ｘ２０
，
即图像ｔ
ｏｋｅｎ数量为
４００
。
面对这种模态特


征规模高度不对称的情况
，采用简单的特征拼接和自注意力方式进行模态融合会


导致文本信息淹没于图像特征中
，
具体分析可见
３
．３
．４小节
。
这导致模型在进行


跨模态推理时对文本信息的利用不足
，难以有效捕捉指称表达中对目标物体的关


键描述信息
，
进而导致模型定位不准确
。


２
）ＲＥＣ模型缺乏有效的针对复杂场景知识的推理方案


面对基于场景知识的指称表达理解任务时
，需要结合指称表达和复杂的外部


场景知识才能定位目标物体
。比如指称表达为
“ 杰克的帽子
”
，而
“ 杰克是谁？
”


这
一问题则需要根据给定的外部场景知识才能确定
。在这种情况下由于需要同时


结合指称表达和外部场景知识进行联合推理
，并且给定的外部场景知识通常还包


７


北京邮电大学硕士学位论文


含许多关于其他相关物体的描述
，这导致整体文本的长度和复杂性大幅提高
，增


大了模型的推理难度
。现有的方法通常结合简单的模板将指称表达和场景知识进


行拼接来进行联合推理
。这种方案
一方面难以避免场景知识中其他无关描述对模


型推理造成的干扰
，
另
一方面会导致拼接后的文本长度过长
，进
一步提高模型的


推理难度
，
导致对目标物体的定位效果不佳
。


１
．３
．２研究内容


为了解决上述问题
，本文开展了面向复杂文本的指称表达理解研究
。
一方面


从模型架构角度避免文本信息的淹没问题
，充分利用语言特征引导模型的跨模态


推理过程
。另
一方面从数据角度结合大模型强大的指令遵循能力降低场景知识的


复杂度
，
过滤无关的场景描述
，
减少其他物体的干扰
。


本文的主要研究内容如下
：


１）语言引导的指称表达理解研宄


针对问题
一
，
本文提出
ＬＧＲ
－ＮＥＴ模型充分利用语言来指导
ＲＥＣ任务中的


跨模态推理过程
。分别通过以下两个方向实现对语言信息的充分利用
，
一是避免


问题
一中提到的文本淹没问题
。本文摒弃了此前工作中利用自注意力机制融合图


文特征的方案而改用跨注意力机制融入语言信息
，这样对模态进行分离可以避免


文本信息淹没在视觉信息中
。二是在图文跨模态推理过程中从多个角度
，多个层


次引入不同抽象层次的文本信息进行引导
，包括指称表达中关于目标物体的空间


位置描述
、全体的指称表达信息
、
以及全局的句子信息
，
分别用于跨模态推理过


程的多个阶段
。综上
，
本文提出
一种语言引导的指称表达理解推理网络
，充分利


用语言特征
，
提高模型的推理效果
。


２）基于场景知识的指称表达理解研宄


针对问题二
，
基于场景知识的ＲＥＣ任务要求模型结合指称表达和外部场景


知识才能确定目标物体
。而外部场景知识通常冗长而复杂
，本文从数据和模型两


个角度解决这
一问题
。从数据层面
，本文结合最近
一年来取得重大突破的大语言


模型技术简化场景知识
，通过大模型强大的指令遵循能力构建提示模板
，过滤场


景知识中与指称物体无关的描述
，减少推理过程中的干扰
。
从模型层面
，将输入


视作图片
、指称表达
、场景知识三元组
，
分别对其进行编码和特征提取
。通过注


意力机制建模三者之间的关联
，提取场景知识中关键的描述信息
，提高模型的推


理效果
。


１
．３
．３贡献及创新点


本文的主要贡献及创新点总结如下
：


１
）在指称表达理解任务中
，
本文提出了
一个语言引导的指称表达理解推理


网络
，通过跨注意力机制分离图文模态避免了文本信息淹没于图像信息中
。另外
，


８


第
一章绪论


在图文跨模态推理过程中从多个角度引入不同抽象层次的文本特征用于引导推


理过程
，其中包含目标物体空间信息的特征编码为坐标向量
，
增强预测
ｔｏｋｅｎ的


空间表征
；蕴含全体指称表达信息的文本特征被交替用于引导跨模态对齐和融合


过程
；包含指称表达全局句子信息的句子向量用于构建新颖的图文对齐损失
，增


强推理过程中的模态对齐约束
，充分实现语言特征的利用
，提高模型推理的效果
。


２
）在基于场景知识的指称表达理解任务中
，
本文从模型和数据两个层面提


出了针对外部场景知识复杂冗长的解决方案
。在数据层面
，结合大模型强大的指


令遵循能力设计提示模板简化场景知识
，过滤无关物体的描述
，减少模型推理过


程的干扰
。在模型层面
，提出面向场景知识的指称表达理解模型
，分离输入的指


称表达和场景知识
，并结合注意力机制根据指称表达动态关注场景知识中的关于


指称物体的有效描述
，
提高模型推理的性能和最终的定位准确率
。


３）在ＲｅｆＣＯＣＯ、ＲｅｆＣＯＣＣＨ、ＲｅｆＣＯＣＯｇ
、Ｆｌｉｃｋｒ３０ＫＥｎｔｉｔｉｅｓ、ＲｅｆｅｒｌｔＧａｍｅ、


以及
ＳＫ
－ＶＧ这六个基准数据集上设计了完备的实验分析流程
，
包括性能指标对


比
、模型组件消融、可视化分析
、案例研究、
以及关键超参数分析等
。在传统指


称表达理解和基于场景知识的指称表达理解两个任务上对比了目前主要的基线


方法
，
通过全面的定量
、
定性实验充分证明了本文提出的方案的有效性
。


１
．４本文的组织结构


本文
一共包含五个章节
，
章节结构如图
１
－３所示
，
每
一章的重点内容如下
：


第
一章
，绪论
。首先介绍了指称表达理解任务的研究背景和应用价值
，对相


关工作进行了梳理和总结
。进
一步分析和总结了当前指称表达理解任务面临的主


要问题
，
最后对本文的研究内容和贡献做了提炼
。


第二章
，基础知识
。该章节详细介绍了本文提出方法所涉及的图文多模态基


础理论知识
。包括常见的图像、文本编码器
，
目标检测方法
，
随后总结了当前主


流指称表达理解方法的范式。最后对指称表达理解基准数据集以及指称表达理解


任务的评估指标进行了总结
。


第三章
，语言引导的指称表达理解研宄
。首先介绍了语言引导的指称表达推


理网络
，以语言引导的跨模态推理为主线
，分别介绍了各模块设计的动机和思路
，


以及
一些方法的具体细节
。最后通过大量的实验分析验证了该方法的有效性
。


第四章
，基于场景知识的指称表达理解研宄
。该章节首先介绍了基于场景知


识的ＲＥＣ任务特点
，并指出从数据和模型两个角度来处理复杂场景知识的问题
。


从大模型的利用方案
，
到指令模板的设计
，再到生成数据的评估
，逐步介绍了数


据层面的处理细节
。模型层面优化输入表示
，
介绍了各模块的设计细节和思路
。


最后通过实验分析
，
分别验证了数据简化和模型的有效性
。


９


北京邮电大学硕士学位论文


第五章
，
总结与展望
。对本文的研宄工作进行了总结
，并对未来工作进行了


展望
，
提出了可以进
一步深入研究的方向
。


第
一章绪论


ｉ


第二章基础知识


挑战
１
：ｋ称表达
１ｆ
挑战２
：
细粒度ｆ挑战３
：
场景知
＇


线索利用不充分融合对齐困难识冗长复杂


问题１
：
跨模态推理存问题２
：
缺乏针对复杂


在文本信息淹没问题场景知识的推理方案


Ｖ／＼
／


ｉｉ


第三章语言引导的第四章基于场景知识


指称表达理解研宄的指称表达理解研宄


第五章
总结


图
１
－３
本文的组织结构


１
．５本章小结


本章首先对指称表达理解的研宄背景与意义
、研究现状进行了详细的介绍与


分析
，
接着提出传统ＲＥＣ存在文本信息淹没和语言特征利用不充分的问题以及


基于场景知识的ＲＥＣ中存在外部场景知识冗长复杂的问题
，
针对这些问题依次


阐述了本文的主要研究内容与创新点
，并以这两个研究方向作为脉络
，
列出了本


文各章节的核心要点及组织架构
。


１０


第二章基础知识


第二章基础知识


２
．
１文本编码器


在自然语言处理领域
，文本编码器（ＴｅｘｔＥｎｃｏｄｅｒ
）是
一种将文本数据转换为计


算机可以理解和处理的数值形式的工具
。这种转换过程通常涉及将单词
、句子或


段落映射为向量
，这些向量能够捕捉文本的语义信息
。文本编码器在多种ＮＬＰ任


务中扮演着核心角色
，
如机器翻译
、
文本分类
、
情感分析
、
问答系统等
。


文本编码器早期依赖词向量的发展
，
如今以
Ｔｒａｎｓｆｏｒｍｅｒ为模型架构的预训


练语言模型是更优的文本编码方案
。
本节主要对这两部分进行介绍
。


２
．
１
．
１词向量


最基本的文本编码方案是独热编码
［４２
］
（〇ｎｅ
－ＨｏｔＥｎｃｏｄｉｎｇ）
，
其编码思想就是


将每个词编码成
一个固定长度的二进制向量
，
其中只有
一个位置的值为
１
，
其余


位置都为
０
，
向量长度为词表大小
。
例如
，
一个词表包含三个单词
：
｛
“
ａｐｐ
ｌｅ
”
，


“
ｂａｎａｎａ
”
，
“
ｄｏｇ
”
｝
，那么对于
“
ａｐｐ
ｌｅ
”
，其Ｏｎｅ
－Ｈｏｔ向量可以是
［
１
，０
，０
］
，
“
ｂａｎａｎａ
”


对应
［０
，
１
，０
］
，
“
ｄｏｇ
” 对应
［０
，０
，
１
］
。独热编码的优势在于实现简单
，易于理解
。然而
，


由于其编码特性导致其词向量维度在词表增大时会同步增大
，进
一步增加向量的


稀疏性
，
导致维度灾难
。此外
，
Ｏｎｅ
－Ｈｏｔ编码无法捕捉单词之间的相似性或关联


性
，
因为每个单词的向量都是独立的
。


为解决Ｏｎｅ
－Ｈｏｔ编码造成的维度灾难问题
，
Ｍｉｋｏｌｏｖ等人提出利用稠密向量


进行单词的特征表示
，
即Ｗｏｒｄ２Ｖｅｃ（ＷｏｒｄｔｏＶｅｃｔｏｒ
）
。
其核心思想是通过单词的


上下文得到词语的向量化表示
，
按照训练目标的不同可分为连续词袋模型


（ＣＢＯＷ
）和跳字模型（Ｓｋ
ｉｐ
－Ｇｒａｍ
）
，
如图
２
－
１所示
：


ＩＮＰＵＴＰＲＯＪＥＣＴ
ＩＯＮＯＵＴＰＵＴ
ＩＮＰＵＴＰＲＯＪＥＣＴ
ＩＯＮＯＵＴＰＵＴ


ｗ
（
ｔ
－２
）＇
４
ｗ
（
ｔ
－２
＞


ｒ＼／
：


ｗ｛ｔ
－
１
）、＼
／＜ｗ
（
ｔ
－１
）


＼ＳＵＭ／


—？ｗ
（
ｔ
）ｗ｛ｔ）
—
？


￥


Ｗ
（
｛＋１
）
’／
＼Ａｗ
（
ｔ＋
１
）


ｗ（ｔ＋２
）
＇
＼ｗ
（
ｔ＋２
）


ＣＢＯＷＳｋ
ｉｐ
－
ｇｒａｍ


图
２
－
１Ｗｏｒｄ２Ｖｅｃ词向量模型
［４３
］


１
１


北京邮电大学硕士学位论文


ＣＢＯＷ模型的核心就是利用目标词的上下文预测目标词
。
具体而言
，
对于


目标中心词ｗ
（ｔ）
，
输入其上下文窗口Ｃ＝
｛ｗ（ｔ
—
ｃ）
，ｗ（ｔ
—
ｃ＋１）
，
…
，ｗ（ｔ＋ｃ
—


Ｉ）
，ｕ／（ｔ＋ｃ）｝的平均词向量＆
，
其中Ｃ为窗口大小
。
投影层则利用嵌入矩阵Ｗ
ｅ


ｉｒ
ｘｎ
（ｖ为词典大小
，
ｎ为特征维度）与向量
进行乘法运算
，从而获得隐藏状态


向量
；
输出层则将该隐藏向量与对应的权重矩阵
相乘
，
并经过
一个


Ｓｏｆ
ｔｍａｘ层获得当前词在词表上的条件概率分布
，
公式如下所示
，
模型选取最大


概率对应的词作为中心词的预测输出
。


… 一、
ｅｘｐ（ＱＷ
ＴＸ
ｔ）


（Ｗ（
）
Ｉ）
＿
＾
ｅｘｐ（ＱＷ
＾Ｘ〇
（°


Ｓｋｉｐ
－Ｇｒａｍ模型是ＣＢＯＷ的逆向过程
，
即利用目标词预测上下文
。


由于Ｗ〇ｒｄ２ＶｅＣ模型中的
Ｓｏｆ
ｔｍａｘ函数需要对词表中的所有词进行指数运算


与归
一化操作
，
因而在词表大小扩增的情况下会使得模型的计算复杂度陡增
。为


了解决此问题
，
出现了层次
Ｓｏｆ
ｔｍａｘ（ＨｉｅｒａｒｃｈｉｃａｌＳｏｆｔｍａｘ）和负采样
（Ｎｅｇａｔｉｖｅ


Ｓａｍｐ
ｌｉｎｇ）等改进思路以
［４４
］降低计算复杂度
。
层次
Ｓｏｆ
ｔｍａｘ
的核心是将词表构建


为
一棵霍夫曼树
［４５
］
，每个叶子节点代表
一个词
，每个内部节点代表
一个二分类器
，


计算任何
一个词的概率都只需要沿着该二叉树的路径逐次进行逻辑回归判断
，而


不需要遍历词表中的所有词
。负采样策略是将多分类问题转化为二分类问题
，
即


给定
一个中心词和
一个上下文背景词
，判断两者是否共现
，为了训练这个分类器
，


需要从噪声分布中利用词频进行带权采样以获取负样本
。


然而
，由于
＼ＶＯ
Ｔｄ２Ｖｅｃ模型训练得到的嵌入矩阵是聚焦于词的局部上下文的
，


使得词向量表示缺乏整体的语义联系
，并且无法根据不同的上下文情况进行动态


调整
，
此外Ｗｏｒｄ２ＶｅＣ模型在面对如机器翻译
、机器阅读理解等深层次的自然语


言理解任务时仍然存在较大的性能瓶颈
。后续Ｐｅｎｎｉｎｇ
ｔｏｎ等人提出的Ｇｌｏｖｅ词嵌


入模型
［４６
］
（ＧｌｏｂａｌＶｅｃｔｏｒｓｆｏｒＷｏｒｄＲｅｐｒｅｓｅｎｔａｔｉｏｎ）则在局部上下文窗口的基础上
，


引入全局的词共现统计信息来学习词向量表示
，从而在
一定程度上缓解了局部上


下文等问题
，
并在多类下游文本任务上取得显著的性能提升
。


２
．
１
．２Ｔｒａｎｓｆｏｒｍｅｒ网络


Ｖａｓｗａｎｉ等人于
２０
１７年提出
Ｔｒａｎｓｆｏｒｍｅｒ网络
，
网络结构包含完整的编解码


器如图
２
－２所示
，
用于解决机器翻译问题
。
在此之前
，
循环神经网络
（ＲＮＮ）
常


常被用于建模文字
，语音等序列数据
。然而因为循环神经网络计算时依赖前
一时


刻的输出来更新隐藏状态（ＨｉｄｄｅｎＳｔａｔｅ）
，
因此无法进行并行计算
，
导致训练效率


无法提高
。
此外
ＬＳＴＭ
［４８
］等循环神经网络结构仍然存在梯度消失和梯度爆炸问


题
。
为此
Ｔｒａｎｓｆｏｒｍｅｒ架构提出使用多头注意力机制实现序列上下文之间依赖关


系的捕捉和建模
，
由于注意力机制的特性
，
Ｔｒａｎｓｆｏｒｍｅｒ模型架构允许并行计算加


速训练
，
已经称为文本处理的新范式
，
同时大大推动了文本预训练模型的发展
。


１２


第二章
基础知识


本节主要介绍
Ｔｒａｎｓｆｏｒｍｅｒ模型的架构细节
。
如图
２
－２所示
，
Ｔｒａｎｓｆｏｒｍｅｒ架


构主要包括两部分
，
编码器
（Ｅｎｃｏｄｅｒ）和解码器（Ｄｅｃｏｄｅｒ）
，
其中编码器负责将输


入序列编码为特征序列
，解码器负责从隐藏状态序列中生成输出序列
，它们各自


均由多个结构相同的模块（ｂｌｏｃｋ
）堆叠而成
。


Ｏｕｔｐｕｔ


Ｐｒｏｂａｂ
ｉ
ｌ
ｉｔ
ｉｅｓ


ｔ


ＩＳｏｆｔｍａｘ）


會


＼Ｌ
ｉｎｅａｒ
￣
１


，
ｔｓ


ＩＡｄｄ＆Ｎｏ
ｒｍ


Ｆｅｅｄ


Ｆｏｒｗａｒｄ


ｔ＿Ｊ＝＝＝＝＝ｒ＾


／
＊
Ｉ
＂Ｎ
ｊＡｄｄ＆Ｎ〇
ｒ
ｒ？
ｉ


ｎ
Ａａｄ—
ｏ
ｒｍ


ＦｅｅｄＡｔｔｅｎｔ
ｉｏｎ


ＦｏｒｗａｒｄａａａＮｘ


．
．
ＩＡｄｄ
＆Ｎｏｒｍ


Ｍａｒｋｅｄ


Ｍｕｆ
ｔ
ｉ
－ＨｅａｄＭｕ
！ｔ
ｉ
－Ｈｅａｄ


Ａｔｔｅｎｔ
ｉｃｎＡｔｔｅｎｔ
ｉｏｎ


Ｐｏｓ
ｉ
ｔ
ｉｏｎａ
ｌＰｏｓｉｔ
ｉｏｎａ
ｌ


Ｅｎｃｏｄ
ｉｎｇＹＷＥｎｃｏｄ
ｉｎｇ


ＩｎｐｕｔＣＨｊ
ｔｐｕｔ


Ｅｍｂｅｄｄ
ｉｎｇＥｍｂｅｄｄ
ｉｎｇ


「ｔ


ＩｎｐｕｔｓＯｕｔｐｕｔｓ


（ｓｈ
ｉｆ
ｔｅｄｒ
ｉｇｈｔ
）


图２
－２Ｔｒａｎｓｆｏｒｍｅｒ模型架构
［４７
］


以编码器为例
，每个模块包含多头自注意力层
，残差及归
一化层
，全连接层
。


多头自注意力层是Ｔｒａｎｓｆｏｒｍｅｒ的核心所在
，
其核心在于多头自注意力机制
。


首先
，
定义注意力Ａｔｅｎｔｉｏｎ操作如下
：


Ａｔｔｅｎｔｉｏｎ（Ｑ
，Ｋ
，１＾
）
＝Ｓｏｆｔｍａｘ＾（２
—
２）


其中ＱＥＲ
ｉｘｄｆ
ｃ
，
尺
ｅ？
，
１／ｅＥ
ｉｘ？分别对应查询向量Ｑｕｅｒｙ
，键值向量


Ｋｅｙ以及值向量Ｖａｌｕｅ
。
Ｌ为序列长度
，
分别为查询向量和值向量的维度
。


将原始的Ｑ
，１Ｖ先投影到维度
，
分别进行
／Ｉ个独立注意力操作
，将最终得


到的结果拼接
，
便得到了多头注意力机制如下
：


Ｍｕｌｔｉｈｅａｄ（Ｑ
，Ｋ
，Ｖ）
＝Ｃｏｎｃａｔ｛ｈｅａｄ
ｘ
，ｈｅａｄ２
，：
．
，ｈｅａｄｈ）Ｗ
°（２
—
３）


ｈｅａｄ
ｔ
＝Ａｔｔｅｎｔｉｏｎ＾ＱＷ＾

，ＫＷ＾
，ＶＷ＾）


其中
ｅＥ知ｘ‘
，
ｖｔ／ｙｅＥ
ｄ” ｘｄｍ
，
ｅ。此外
，根据Ｑ
，Ｋ
，


Ｖ来源的不同可以区分为自注意力和跨注意力
，
自注意力即Ｑ
，Ｋ
，
Ｖ都来自同


一个向量序列
，
比如上述
Ｔｒａｎｓｆｏｒｍｅｒ模型架构中左侧
Ｅｎｃｏｄｅｒ和右侧
Ｄｅｃｏｄｅｒ


下方的注意力层
。若Ｑ
，Ｋ
，Ｖ来自不同的向量序列
，通常Ｑ来自
一个向量序列
，


Ｋ
，
Ｖ来自另
一个向量序列
，
那么该注意力称为跨注意力机制
，
如图
２
－２右侧


Ｄｅｃｏｄｅｒ上方的注意力层
。
另外
，
在Ｔｒ
ａｎｓｆｏｒｍｅｒ架构的
Ｅｎｃｏｄｅｒ和Ｄｅｃｏｄｅｒ中
，


１３


北京邮电大学硕士学位论文


自注意力还存在掩码矩阵
（Ｍａｓｋ
）的差异
，
由于
Ｄｅｃｏｄｅｒ应用于目标语言解码过


程
，
在
：Ｔ时刻预测时
，
其只能与历史时刻（ｔ＜
７〇的输入进行注意力计算
，
需要对


Ａｔｅｎｔｉｏｎ操作过程中生成的注意力矩阵加上
一个上三角的掩码矩阵
，以此来实现


上述操作
，
即ＭａｓｋｅｄＭｕｌｔｉ
－ＨｅａｄＡｔｅｎｔｉｏｎ
。而Ｅｎｃｏｄｅｒ用于对源语言进行编码而


无需添加掩码矩阵
。


残差及归
一化层作用于多头注意力层和全连接层之后
，通过对向量序列进行


归
一化可以避免数据偏移
，并且残差连接操作可以缓解模型堆叠过程中的梯度消


失问题
。


此外
，
由于上述Ａｔｅｎｔｉｏｎ操作是对称的
，
即无法感知序列位置的差异
，
而


位置特征在文本序列中又是不可或缺的
，
因此Ｔｒａｎｓｆｏｒｍｅｒ通过在词向量中加入


额外的位置编码区分不同位置的词向量
。
具体来说
，
其位置编码定义如下
：


｜
ＰＥ
（Ｐ〇ｓ
，２
ｌ）
＝ｓｉｎ（ｐｏｓ／１００００
２＾
）


｛Ｐ＾ｐ〇ｓ
，２
ｉ＋Ｄ
＝ｃｏｓ（ｐｏｓ／１００００
２
ｉ／ｄ－
）
Ｃ｝


其中
，
ｐｏｓ为词向量在序列中的位置下标
，
ｉ为词向量对应维度的下标值
。


相比于传统的时间序列模型如ＬＳＴＭ或ＧＲＵ
，
Ｔｒａｎｓｆｏｒｍｅｒ架构通过注意力


机制实现长距离关系建模的同时
，大大提高了模型训练的并行性
，使得大规模预


训练成为了可能
。


２
．
１
．３预训练语言模型


得益于上述Ｔｒａｎｓｆｏｒｍｅｒ架构的提出
，
后续以Ｇｏｏｇ
ｌｅ的ＢＥＲＴ和ＯｐｅｎＡＩ的


ＧＰＴ
［５２
］为代表的预训练语言模型开启了自然语言处理的新时代
。


如图
２
－３
［４９
］所示
，
ＢＥＲＴ模型仅利用
Ｔｒａｎｓｆｏｒｍｅｒ架构中的编码器进行双向的


多层堆叠
，并通过设置掩码预测
、句子连续预测等预训练任务在大规模语料上进


行预训练
。
模型内部的多头自注意力机制可以捕捉到每个词完整的上下文语义
，


最终的输出层将输入序列转换成高位的语义特征表示
。如此ＢＥＲＴ可以充当
一个


高质量的文本特征提取器
。
此外
ＢＥＲＴ开启的预训练
－微调方案也成为了自然语


言处理领域的新范式
，
即首先通过大规模预料预训练
一个强大的基座模型
，再在


这基础上结合特定下游任务进行微调
，
实现模型能力的迁移
。


在预训练阶段
，
主要通过设置
ＭＬＭ（ＭａｓｋｅｄＬａｎｇｕａｇｅＭｏｄｅｌ
ｉｎｇ）和
ＮＳＰ


（ＮｅｘｔＳｅｎｔｅｎｃｅＰｒｅｄｉｃｔｉｏｎ）两个任务对模型进行预训练
。
具体来说
，
在ＭＬＭ中
，


模型随机选取输入文本中的
一些单词进行掩码操作
，
其中被选中的单词以
８０％


的概率被替换成
［ＭＡＳＫ
］这
一特殊掩码符号
、
以
１０％的概率被替换成词表中的其


他单词
，
以
１０％的概率保持原始词不变
。然后让模型对被选中的单词进行预测恢


复
。这个任务可以使得模型学会对输入的文本进行双向建模
。而在ＮＳＰ任务中
，


ＢＥＲＴ模型的预测对象是输入的两个句子之间是否存在上下相邻的逻辑关系
，这


１４


第二章
基础知识


个任务的目的是让ＢＥＲＴ实现句子层面的逻辑关系
，提高模型对文本的理解层次


和泛化能力
。


在微调阶段
，通常会在ＢＥＲＴ顶部添加
一个任务相关的输出层
，这些层负责


处理特定任务的输出
，
如分类
、
问答
、命名实体识别
。微调过程中
，
预训练部分


的权重会被保留
，
而新添加的层会在下游任务上重新训练
。


｜
＇
｜
ｙ｜
；Ｊ｜
］
ＷＳＰＭ３ＳＫ
ＩＭ
ＵＮＵｌｉＥＲＳＱｕＡＤｇ
ｌｙ１＾＾Ｓｐａｎ


ＩＢＥＲＴ
ＢＥＲＴ


：
１ＳＢＺＵＥ１Ｓ５Ｅ
－ＳＤｍＫＷｍ
，
’


！
＊－ｅ
￣￣ｃＤ—ａ￣ｃｃ
— １￣— 
＊－〇
—ｏ

Ｑ
ｃ￣Ｄｃ


（
Ｔｒｍ
ｙ

ｉ
ｓＴｒｍ
｝
…（
Ｔ
ｉｍ
；３０孕
ＣＥＩＣＨＬ
互
［
＂巴」５ＥＳｊ


＇
Ｓ
＇
十
＾
］
Ｍａｓｋｅａ
Ｓｅｎ
ｉｅｎｃｅＡＭａｓＫｅＯ
Ｓｅｎ
ｔｅｎｃｅ
ＢＱｕｅｓｔｏｎ會
Ｐａｆａｓ
ｒａ
ｊｊｈ


—
—
，
；Ｕｎｔ
ｅＢｅ
ｉｅ
ｃ
ＪＳｅｒ＾Ａｅ
ｒ
．ａＢＰａ＊乂Ｖ／
、
、
．
、
．
ＱｕｅｓｂｏｎＰＷ



－Ｊ— — －１—
！
！— ＿２Ｌ一Ｊ
ＩＰｒｅ
－
ｔｒａ
ｉｎ
ｉｎｇ
Ｆ
ｉｎｅ
－Ｔｕｎ
ｉｎｇ


图
２
－３ＢＥＲＴ网络结构和预训练
－微调方案
［４９
］


ＢＥＲＴ的成功之处它的双向结构和先进的预训练方案
，这使得ＢＥＲＴ在短时


间内成为
一个强大而通用的语言模型并得到广泛应用
。后续在ＢＥＲＴ的基础上衍


生出很多改进工作
，
２０
１９年由
ＦａｃｅｂｏｏｋＡＩ提出的ＲｏＢＥＲＴａ
［５Ｇ
］使用更多的训练


数据
，
更长的训练时间
，
将ＭＬＭ任务改为动态掩码并移除了ＮＳＰ任务
；
后续


Ｇｏｏｇ
ｌｅ提出的
Ａｌｂｅｒ
ｔ
［５
１＾ｊ从模型轻量化的角度利用参数共享的训练策略大大降


低了模型的参数和训练成本
。


预训练语言模型另
一个重要的分支则是以
ＯｐｅｎＡＩ提出的
ＧＰＴ为代表的单


向语言模型
。
与ＢＥＲＴ不同
，
ＧＰＴ通过堆叠Ｔｒａｎｓｆｏｒｍｅｒ中的Ｄｅｃｏｄｅｒ层实现单


向的语言模型
。如图
２
－４所示
，
ＧＰＴ通过在大规模预料上采用无监督训练的方法


进行下
一个单词的预测对模型进行预训练
。
由于模型架构和预训练任务的不同
，


相比
ＢＥＲＴ模型
，
ＧＰＴ具备更好的自然语言生成能力
，
能够生成自然流畅的文


本
，
适用于各种文本生成任务
。
此外
，
ＯｐｅｎＡＩ在单向预训练语言模型这
一研究


路线上不断增大模型和数据的规模
，
并在后续提出
ＧＰＴ
－２
［５３］
，
ＧＰＴ
－３
［５４
］
，


ＩｎｓｔｒｕｃｔＧＰＴ
［５５
］等系列模型
，并开启了如今的大模型
（ＬａｒｇｅＬａｎｇｕａｇｅＭｏｄｅ
ｌ）时代
。


以ＣｈａｔＧＰＴ为代表的大模型通常会经过预训练
、
监督微调（ＳＦＴ）
、
人类反馈


的强化学习
（ＲＬＨＦ
）三个训练阶段
［５５
］
。
在这个过程中
，
大模型会学习到现实世界


丰富的知识
，
同时获得良好的指令跟随能力和人类偏好对齐能力
。基于此可以构


建多样的应用
，
如会议纪要生成
，
文档总结
。
此外
，
Ｌｌａｍａ
［５６
］
、
Ｌｌａｍａ
－２Ｐ］
、


ＣｈａｔＧＬＭ
［５８
］等国内外知名开源大模型也大大提高了开源大模型社区的活跃程度


与技术水平
。


１５


北京邮电大学硕士学位论文


ＯｐｅｎＡ
ＩＧＰＴ


了
１〕
［：
２
：
：ｊ
．
．
．丁Ｎ


（
Ｔｒｍ丁ｒｍ）（Ｔｒｍ
；


（
ＴｒｍＶＴｒｍ
｝
…
（
Ｔｒｍｊ


ｅ
２
？
？
？ｅ
ｎ


图
２
－４ＧＰＴ网络架构
［５２
］


总体而言
，
以ＢＥＲＴ和ＧＰＴ为代表的Ｔｒａｎｓｆｏｒｍｅｒ架构的预训练语言模型是


ＮＬＰ领域
一个重要的里程碑
。开创了崭新的自然语言研究方向
，
同时也极大提高


了ＮＬＰ的应用价值
。


２
．２视觉编码器


视觉编码器负责图像的处理和信息提取
，实现从原始的像素信息到高层视觉


语义信息的转换
。视觉编码理论基于计算机视觉方法的发展
，从早期的
ｓｉｆｔ
［５Ａ


Ｈａｒｒｉｓ特征点
［６Ｇ
］到如今的深度学习主导的特征表示
。本节主要介绍提取视觉特征


的骨干网络（Ｂａｃｋｂｏｎｅ）以及和指称表达理解比较相关的目标检测模型
。


２
．２
．
１卷积神经网络


卷积神经网络（ＣｏｎｖｏｌｕｔｉｏｎａｌＮｅｕｒａｌＮｅｔｗｏｒｋｓ
，ＣＮＮ）是
一种深度学习模型
，它


在图像和视频识别
、
语音信号等领域表现出色
。
ＣＮＮ
的核心特点是能够自动学


习输入数据的局部特征
，并且具有平移不变性
，这意味着网络能够识别出图像中


的物体
，
即使物体在图像中的位置发生变化
。其核心的卷积思想受到生物视觉系


统的启发
，采用共享的卷积操作代替全连接的矩阵运算
。早期主要被用于图像分


类领域
，
后来逐渐也称为目标检测
，
语义分割模型的重要组件
。


随着硬件性能和数据规模的发展
，
卷积神经网络架构的发展经历了早期的


ＬｅＮｅｔ
－５
［６
１
］
，
到
２０
１２年在
ＩｍａｇｅＮｅｔ大规模视觉识别竞赛大火
，
并开启深度学习


新时代的ＡｌｅｘＮｅｔ
［２
］
，
再到之后的ＶＧＧＮｅｔ
［６２
］
，ＧｏｏｇＬｅＮｅｔ
［６３
］
，
ＲｅｓＮｅｔ
［４
１
］
，


。尽管如此
，卷积神经网络如今仍然在快速迭代
，提升模型的性能


或者计算效率
。


各种卷积神经网络结构都有独特的设计
，但它们的基本都包括以下几个关键


部分
：


１
．
卷积层
（ＣｏｎｖｏｌｕｔｉｏｎａｌＬａｙｅｒ）
：


卷积层是ＣＮＮ的核心
，它通过卷积操作提取输入数据（如图像）的局部特征
。


卷积操作涉及将
一个可学习的滤波器（或称为卷积核）在输入数据上滑动
，
计算滤


１６


第二章
基础知识


波器与输入数据的局部区域的点积
，生成特征图（ＦｅａｔｕｒｅＭａｐ）
。以二维图像／为例
，


给定二维的卷积核模板欠
，
卷积操作计算过程如下
：


＝
０
＊＝＼ＶＩ（
ｉ＋ｍ，ｊ＋ｎ）ＫＣｍ
，ｎ）（２
－５）


＇ｎ


其中ｍ，ｎ分别为卷积核的长
、
宽
。
此外
，
卷积层可以有多个卷积核
，
每个


卷积核负责提取
一种特定的特征
，
如边缘
、
角点
、纹理等
。
因此生成的特征图通


常有很多通道数
。


２
．激活层
（ＡｃｔｉｖａｔｉｏｎＬａｙｅｒ）
：


激活层通常跟在卷积层之后
，
用于引入非线性
，使得网络能够学习复杂的模


式
。
常用的激活函数包括ＲｅＬＵ（Ｒｅｃｔｉｆ
ｉｅｄＬｉｎｅａｒＵｎｉｔ
）
、
Ｓｉｇｍｏｉｄ
、
Ｔａｎｈ等
。


３
．池化层
（ＰｏｏｌｉｎｇＬａｙｅｒ）
：


池化层用于降低特征图的空间维度
，减少计算量和防止过拟合
。最常见的池


化操作是最大池化（ＭａｘＰｏｏｌｉｎｇ）
，
它在特征图的局部区域内取最大值
。平均池化


（ＡｖｅｒａｇｅＰｏｏｌｉｎｇ）也是常用的池化方法
。


２
．２
．２视觉Ｔｒ
ａｎｓｆｏｒｍｅｒ网络


视觉Ｔｒａｎｓｆｏｒｍｅｒ
（ＶｉｓｉｏｎＴｒａｎｓｆｏｒｍｅｒ
，ＶｉＴ
［２２
］
）最早由Ｇｏｏｇ
ｌｅＲｅｓｅａｒ
ｃｈ于２０２０


年提出
，
旨在探索
Ｔｒａｎｓｆ
ｏｒｍｅｒ架构替代卷积神经网络成为视觉特征编码器的可


能
。如图
２
－５所示
，
ＶｉＴ通过将原始图片分割为固定大小的图片块
（Ｐａｔｃｈ）
，再将


分割后的图片块映射到固定的特征维度后直接用类似
ＢＥＲＴ
的
Ｔｒａｎｓｆｏｒｍｅｒ


Ｅｎｃｏｄｅｒ架构提取图片特征
。类似地
，ＶｉＴ引入了
一个额外的可学习的
“ 分类
ｔｏｋｅｎ
”
，


用于在最后
一层通过多层感知机进行分类
。类似的
，
由于注意力机制的位置对称


性
，在ｐａｔｃｈｔｏｋｅｎ输入ＶｉＴ之前也添加了位置编码用于区分不同
ｐａｔｃｈ在原图像


中不同的区域
。此外
，
与ＮＬＰ不同的是
，
ＶｉＴ不包含
一个固定的词表
，
而是直接


将图片分块
，
映射后的图片特征作为隐层特征序列进行建模
。


ＶｉｓｉｏｎＴｒａｎｓｆｏｒｍｅｒ
（ＶｉＴ）
＊ＴｒａｎｓｆｏｒｍｅｒＥｎｃｏｄｅｒ


１Ｊ—


Ｃｌａｓｓ
｜Ｌ
ｘ／ＴＮ


盟
一＝
，ｖ
￣
１


Ｏｆ
，ＭＬＰ



薩本
＂


Ｔｒａｎｓｆｏｒｍｅｒ
Ｅｎｃｏｄｅｒ
｜
［
］


！
｜
Ｍ
ＡＳ？
ｄ

｜


＊
Ｆ．ｘ
ｉ
ｒ
ｉ
ｉ

ｉｃａｒｎａｈｈ
－
ｆ
？
｜
＾
．
？
１


ｉ
ｃｌａｓ
ｓ
］ｅｍｂｅｄｄ
ｉｎｇＬｉｎｅａｒＰｒｏｊｅｃ
ｔ
ｉｏｎｏｆ
ＦｌａｔｔｅｎｅｄＰａ
ｌｃｈｅｓ？
个
牟


ｓｓａ１ｒｎ
￣￣
ｎ
ｉ
￣
ｒｎ
￣
ｒ！
丨
丄
丨


Ｉ
Ｉ
Ｅｍｂｅｄｄｅｄ
￣￣
Ｊ


ｉＰａｔｃｈｅｓ


图
２
－５ＶｉＴ模型架构
［２２
］


１７


北京邮电大学硕士学位论文


与卷积神经网络相比
，
由于注意力机制的特点
，
ＶｉＴ能够捕捉图像中的全局


依赖关系
，
在某些长距离依赖的视觉任务上比卷积神经网络更具有优势
。
此外
，


ＶｉＴ在设计上比
ＣＮＮ减少对图像的局部性
、
二维邻域结构和平移等归纳偏置的


依赖
。另
一方面
，
由于注意力机制具备序列长度平方级别的计算复杂度
，
当图片


分辨率较高时会导致生成的
ｐａｔｃｈｔｏｋｅｎ数量过多从而大大提高模型的计算复杂


度
。
以６４０ｘ６４０分辨率的图像为例
，
将其分成１６ｘ
１６的
ｐａｔｃｈ
，
将会生成长达


４０ｘ４０＝１６００的视觉
ｔｏｋｅｎ序列
，
在此基础上进行自注意力的计算将导致复杂


度激增
。


基于此
，微软亚洲研究院于
２０２
１年提出
ＳｗｉｎＴｒａｎｓｆｏｒｍｅｄ
２３
］
，其核心是通过


改进ＶｉＴ中的全局注意力
，使用窗口注意力机制来降低模型的计算复杂度
，基于


此可以使得Ｔｒａｎｓｆ
ｏｒｍｅｒ架构的视觉编码器可以处理分辨率更高的图片
，
进而应


用于目标检测
，实例分割等密集型视觉预测任务
，而不仅仅局限于对图片分辨率


要求不高的视觉任务
，
如图片分类等
。


如图
２
－６所示
，
ＳｗｉｎＴｒａｎｓｆｏｒｍｅｒ通过对图片进行窗口划分
，
通过只在窗口


内部进行注意力计算来降低计算复杂度
，同时结合移动窗口机制实现跨窗口注意


力交互
，
实现图片局部和全局关系的同时建模
。


ｓｅｇｍｅｎｔａｔｉｏｎ


ｃ
ｌａｓｓｉｆ
ｉｃａ
ｔ
ｉｏｎｄｅｔｅｃｔ
ｉｏｎ…ｃ
ｌａｓｓｉ
ｆ
ｉｃａｔｉｏｎ
Ｉａｖｅｒ１Ｌａｖｅｒ１＋１


￣￣
ｎｒ
￣￣
ｎｐｎｎ厂


Ｚｎ，Ｚ乙，
，
￣
｜
｜
Ｈ
￣￣Ａ
ＵＫ
－
ａ
ｌ＾ｄ＾
．０


－ｗ
：

Ｉ
Ｉ



ｚ
．ＨｈＨＨ
Ａ—


ｎ一’Ｌ＿
！
Ｌ＿－ＪＬ
ｉ
ｌ
Ｌ— Ｊ


（ａ）ＳｗｉｎＴｒａｎｓｆｏｍｉｅｒ
ｆｏｕｒｓ
）
（ｂ
）
ＶｉＴ


图
２
－６ＳｗｉｎＴｒａｎｓｆｏｒｍｅｒ与Ｖｉ丁注意力对比间


具体来说
，
窗口注意力（ＷｉｎｄｏｗＭｕｌｔｉ
－ｈｅａｄＳｅｌｆＡｔｔｅｎｔｉｏｎ
，Ｗ
－ＭＳＡ
）和全局注


意力复杂度对比如公式
２
－５所示
：


ｒｎ（ＭＳＡ）
＝４ｈｗＣ
２
＋２（ｈｗ）
２Ｃ


ｌｎ（Ｗ
－
ＭＳＡ）
＝４ｈｗＣ
２
＋２Ｍ
２ｈｗＣ
＾Ｊ


其中
／
分别为图片分块后的分辨率
，
／ｉｗ即为特征序列长度
，
Ｃ为向量维度
，


Ｍ为窗口数量
，
此处指定图片按窗口划分后得到的窗口大小为Ｍ
ｘＭ
。
简要推导


如下
：
在ＭＳＡ中
，
假设注意力头数量为ｄ
，
则单个注意力头的输入（Ｑ
，
Ｋ
，
Ｖ）维


度都为／ｉｗＸ
＆这个过程中分别需要维度为Ｃｘ
＾的投影矩阵
，该部分计算复杂度


为３ｘｈｗｘ
｜ｘＣ
，此外计算
得到注意力分数以及通过注意力分数和Ｖ计算输


ａ


Ｔ


出
（忽略
Ｓｏｆｔｍａｘ
｝的过程中分别对应
（
Ｚｉｕ／ｘ
吾）
（
／ｉｗｘ
￡
）
和（／ｕｖｘＺｉｗＯ
（
ＺｉＭ／ｘ
吾）两


１
８


第二章
基础知识


个矩阵乘法
，
它们的复杂度分别为／ｉｗＸ／ＩＷＸ
■
＾和ｈｗＸ
＾Ｘ／ＩＷ
。
因此所有注意力


ａａ


头复杂度之和为ｄｘ
＾
３／ｍ＾＋２（
／ｉｗ）
２
￡）
＝３
／ｉｗＣ
２
＋２（ｈｗ）
２Ｃ
，最后再加上融合


多个注意力头的矩阵运算（
Ｚｉｗ
ｘ
Ｃ）
（Ｃｘ
Ｃ）
，
复杂度为ｈｗＣ
２
，
由此得到ＭＳＡ总


体复杂度为４／ｉｗＣ
２
＋２（／ｉｗ）
２Ｃ
。而在Ｗ
－ＭＳＡ中
，
窗口大小为Ｍ
ｘＭ
，
则窗口数


为兰
窗口内部复杂度可根据上述
ＭＳＡ
复杂度计算
，
得到最终结果为


４ｈｗＣ
２
＋２Ｍ
２
／ｉｗＣ
■
。
可以发现ＭＳＡ复杂度为／ｉｗ的平方级别
，
而Ｗ
－ＭＳＡ为ｈｖｖ


的线性级别
（Ｍ为常数）
。
因此使用
ＳｗｉｎＴｒａｎｓｆｏｒｍｅｒ可以处理更高分辨率的图


片
，
提高了此类视觉特征提取器的适用范围
。


此外
，
为了实现窗口间的注意力
，
ＳｗｉｎＴｒａｎｓｆｏｒｍｅｒ通过移动窗口注意力


（Ｓｈｉｆ
ｔｅｄｗｉｎｄｏｗＭＳＡ
，ＳＷ
－ＭＳＡ
）实现跨窗口交互
。
如图
２
－７所示
，
通过对原来的


窗口划分进行左右
、上下的循环移位实现窗口间的信息交互
，
同时设计了高效的


掩码机制避免因移位导致原来不相邻的图片区域之间计算注意力
。
同时参考


ＣＮＮ的方案
，
ＳｗｉｎＴｒａｎｓｆｏｒｍｅｒ中的
ＰａｔｃｈＭｅｒｇ
ｉｎｇ操作逐步对图片特征进行降


采样
，
进
一步提高模型的全局建模能力并降低计算复杂度
。


Ｘ
－！— －—
ｒ
｜
Ｉｍａｓｋｅｄ
ｌ］
Ａｃ


ＭＳＡ


ＢＢ
Ｂ
；Ｂ


？
■
■
■

— 



■
■
？
■
■
ｎ
ＭＷ


Ｉ
Ｉ
；




＾ｍａｓｋｅｄ
ｉ

，


ｗ
ｉｎｄｏｗｐａｒｔ
ｉｔ
ｉｏｎ＾ａＭＳＡ


ｃｙｃ
ｌ
ｉｃｓｈ
ｉｆｔ
ｒｅｖｅｒｓｅｃｙｃ
ｌ
ｉｃｓｈ
ｉｆｔ


■疆


Ｗ
ｉｎ
ｔＪｏｗＯＷ
ｉｎｄｏｗｌ


■ｌ驪


Ｗｌｎｄｏｗ２Ｗ（ｎｄｏｗ３


Ａｔｔｎ
Ｍａｓｋ


图
２
－７ＳｗｉｎＴｒａｎｓｆｏｒｍｅｒ的移动窗〇注意力及其掩码机制
［２３
］


后续在
ＳｗｉｎＴｒａｎｓｆｏｒｍｅｒ的基础上发展的
ＳｗｉｎＶ２
［６５
］在跨窗口连接
，
多尺度


特征融合等多个角度进行了改进
，进
一步提升了模型的性能
。综上所述
，
相比传


统的卷积神经网络
，
如今
Ｔｒａｎｓｆｏｒｍｅｒ架构在图像特征提取上也具备强大的竞争


力
，
并逐渐成为视觉表征的
一个更佳的方案
。


２
．２
．３
目标检测模型


目标检测模型是
一种神经网络模型
，旨在从图像中同时定位物体并预测其类


另
Ｉ
Ｊ
。模型接受
一张图像作为输入
，
并输出
一个检测目标集合
。每个目标包含
一个


矩形边界框的空间坐标
，描述了检测到的物体的位置
，
以及对该物体类别的预测


１９


北京邮电大学硕士学位论文


结果
。
目前
，
主流的目标检测模型包括两阶段模型（如Ｆａｓｔ
ｅｒＲ
－ＣＮＮ
［８
］和ＭａｓｋＲ
－


ＣＮＮ
［６６
］
）和
一阶段模型（如ＹＯＬＯ系列
［
１７
］
［６７
］
）
。
以ＦａｓｔｅｒＲ
－ＣＮＮ为例
，该模型使用


卷积神经网络作为骨干网络
，
从输入图像中提取特征图
。然后
，通过区域提议网


络（ＲＰＮ
）基于预定义的锚点框对可能包含物体的区域进行预测
，
生成候选区域
。


接着
，利用Ｒ〇ｒ池化将这些区域的特征映射到固定大小的特征图上
。最后
，这些


区域的特征经过分类器进行分类和边界框回归
，
以得到最终的检测结果
。


此外
，
Ｍｅｔａ于
２０２０年提出的基于
Ｔｒａｎｓｆｏｒｍｅｒ的目标检测模型ＤＥＴＲ
［２４Ｗ


创了目标检测的新范式
。
与此前依赖于候选提议或者锚框的目标检测方法不同
，


它将目标检测任务转化成
一个集合预测问题
，
利用
Ｔｒ
ａｎｓｆｏｒｍｅｒ来预测图像中的


目标物体
。如图
２
－８所示
，
ＤＥＴＲ利用可学习的目标查询（ｏｂ
ｊｅｃｔ
ｑｕｅｒ
ｉｅｓ）捕捉目标


物体的视觉特征
，结合ＴｒａｎｓｆｏｒｍｅｒＤｅｃｏｄｅｒ架构逐步调整
，最后预测目标物体的


框坐标和类别
。
模型整体根据二分图匹配算法
［６８Ｈ十算损失函数进行优化
。


；ｂａｃｋｂｏｎｅ
ｊ
｜ｅｎｃｏｄｅｒ
！
｜ｄｅｃｏｄｅｒ
Ｉ「ｐｒｅｄ
ｉｃｔ
ｉｏｎｌｉｅａｄｓ
Ｉ


｜ｓｅｔｏｆ
ｉｍａｇｅｆｅａｔｕｒｅｓ
！
＇
ｊ
｜
ｓ
＂＂ｉ
．


ｉｐＴｒｉｆ
ｆ
—
＾＇


ｔｒａｎｓｆｏｒｍｅｒ
ｔｒａｎｓｆｏｒｍｅｒｎ
★
ｏｂ
ｊｅｃｔ
Ｉ
”


－斤
― ｄｅｒｄｅｃｏｄｅｒ


Ｎ６ＭＭｑ
ｉ
ｉ““


ｉ

＿？Ｄ５ｐ＾ｉｎｓ



丨


图
２
－８ＤＥＴＲ模型架构
［２４
］


ＤＥＴＲ的优势在于它的端到端结构
，
使得模型更简单直观
，
同时减少了训练


和推理的复杂度
。在ＤＥＴＲ的设计结构中对目标物体的预测是
一对
一的
，
因此无


需生成大量的提议或者锚框
，
同时不需要ＮＭＳ等后处理工作
，
降低了模型的计


算开销
。在某些情况下取得了与传统方法相媲美甚至更好的性能
，尤其在处理大


量目标和遮挡较多的图像时表现出色
。在ＤＥＴＲ的基础上
，涌现出了大量改进工


作
，
ＣｏｎｄｉｔｉｏｎａｌＤＥＴＲ
［６９＿ＤＡＢＤＥＴＲ
［７（）
］向目标查询中引入位置先验
，
使模型更


能准确地捕捉目标的空间位置信息
，加快模型的收敛速度并且提高模型检测性能
。


ＤｅｆｏｒｍａｂｌｅＤＥＴＲ
［７
１
］将卷积神经网络中的形变思想引入ＤＥＴＲ模型中
，使用可形


变注意力机制
（ＤｅｆｏｒｍａｂｌｅＡｔｅｎｔｉｏｎＭｅｃｈａｎｉｓｍ）
降低模型的复杂度
。


２
．３指称表达理解范式


指称表达理解至今已经经历了较长的发展时间
，其中出现的方法繁多
。
由于


输出形式与目标检测任务比较相似
，因此指称表达理解方法的发展和演变与目标


检测发展路线具备
一定的相关性
。经过上
一小节介绍可知
，
目标检测模型经历了


２０


第二章
基础知识


基于候选提议
、
基于锚框
、
基于
Ｔｒａｎｓｆ
ｏｒｍｅｉ
？的发展历程
。
类似的
，
主流的支撑


表达理解方法也可分为这三类
。本小节将总结这三种范式的基本思路和模型框架
。


Ａｍａｎｂｅｎｄｉｎｇｏｖｅｒ
＾语言编码器基子福议的方法


ｔｏｕｃｈｉｎｇａｈｏｒｓｅｆｏｏｔ



Ｒｅｇ
ｉｏｎＰｒｏｐｏｓａｌｓ


爾一－到一一
■


Ａｍａｎｈａｎｄ－
ｏｖｅｒ二— ：基于麵的方法
ｔｏｕｃｈｉｎｇａｈｏｒｓｅｆｏｏｔ
一—
＿… 識器


Ｄｅｎｓｅ
Ａｎｃｈｏｒｓ


视■码器
— ？视觉
－语言融合
— ？


基于Ｔｒａｎｓｆｏｒｍｅｒ


Ａｍａｎｂｅｎｄｉｎｇｏｖｅｒ
— ：语言编码器
ｔｏｕｃｈｉｎｇａｈｏｒｓｅｆｏｏｔ



臓
－语言


ｒ
ｎｆ
ｍ
— ＾视■码器，


图
２
－９
指称表达理解范式总结


２
．３
．
１基于提议的模型


如图
２
－９所示
，基于提议的指称表达理解通常模型依赖基于提议的目标检测


器
，如ＦａｓｔｅｒＲＣＮＮ等
。首先利用目标检测器提取图片特征并生成候选提议集合


（Ｒｅｇ
ｉｏｎＰｒｏｐｏｓａｌｓ）
，然后将ＲＥＣ任务转化成图文检索任务
，
即根据指称表达从候


选提议中选择匹配得分最高的提议作为最终的预测结果
。匹配得分的计算方式从


简单的向量相似度计算到结合树
、
图等高级数据结构的匹配方案
。


２
．３
．２基于锚框的模型


如图
２
－９所示
，基于锚框的指称表达理解方法会利用现有的基于锚框的目标


检测模型
，
如
Ｙ０Ｌ０系列模型
。
通过对此类目标检测模型进行扩展
，
使其在进


２
１


北京邮电大学硕士学位论文


行图像特征提取的过程中融入来自指称表达的文本线索信息
，最后根据锚框的置


信度分数选择最高的作为最终的预测结果
。


２
．３
．３基于Ｔｒａｎｓｆｏｒｍｅｒ的模型


可以发现
，基于提议的ＲＥＣ方法和基于锚框的ＲＥＣ方法都依赖现成的目标


检测模型
，
检测模型的效果是
ＲＥＣ任务性能的瓶颈
。
例如目标检测模型没有生


成包含指称物体的提议或者锚框
，
那么将很难对指称物体进行正确的定位
。基于


此
，基于Ｔｒａｎｓｆｏｒｍｅｒ的ＲＥＣ范式参考ＤＥＴＲ检测模型中目标查询（Ｏｂ
ｊｅｃｔ
ｑｕｅｒｙ）


的思想
，
通过设置可学习的查询向量
，
结合
Ｔｒａｎｓｆｏｒｍｅｉ
？跨模态通用的建模能力


构建新的
ＲＥＣ范式
。
如图
２
－９所示
，
基于
Ｔｒａｎｓｆｏｒｍｅｒ
的方法使用视觉语言


Ｔｒａｎｓｆｏｒｍｅｒ进行跨模态推理并直接预测指称物体位置
，
无需候选提议或者锚框


集合
。
模型整体设计高效优雅
，
是如今ＲＥＣ任务的重要发展方向
。


２
．４数据集及评测指标


指称表达理解包含多个常用的基准数据集
，它们有各自的特点
。本节主要介


绍这些数据集以及ＲＥＣ任务的性能评估指标
。


２
．４
．
１指称表达理解基准数据集


本文使用的指称表达理解数据集包括
ＲｅｆｅｒｌｔＧａｍｅ网
，
ＲｅｆＣＯＣＯ〃２
］
，


ＲｅｆＣＯＣＯ＋
［
１２
］
，ＲｅｆＣＯＣＯｇ
［７２
］
，
Ｆｌｉｃｋｒ３０ＫＥｎｔｉｔｉｅｓ
［７３］
，
以及ＳＫ
－ＶＧ
［４０
］
。
前五个数


据集用于评估传统指称表达理解任务
，
其中
ＲｅｆＣＯＣＯ／＋／ｇ
中的图片来自
ＭＳ
－


ＣＯＣＯ
［７４
］
。
最后
一个数据集用于评估基于场景知识的指称表达理解任务
。


ＲｅｆｅｒｌｔＧａｍｅ数据集由Ｋａｚｅｍｚａｄｅｈ等人在
２０
１４年的ＥＭＮＬＰ会议上首次提


出
。
包含来自
ＳＡＩＡＰＲ
－
１２数据集
［７５
］的
２００００张图片
，
每张图片包含若干个指称


表达
。
数据集收集自
一个名为ＲｅｆｅｒｌｔＧａｍｅ的双人游戏
，
旨在通过玩家之间的互


动来生成和验证描述图像中物体的自然语言表达式
。具体而言
，玩家
一根据给定


的图片和目标物体编写自然语言描述
，玩家二根据图片和来自玩家
一的自然语言


描述定位目标物体
，
若定位正确
，
则他们得分并交换角色
。
否则重新选取图片
。


ＲｅｆＣＯＣＯ和ＲｅｆＣＯＣＯ＋数据集由Ｙｕ等人于
２０
１６年提出
。
这两个数据集也


是通过ＲｅｆｅｒｌｔＧａｍｅ这
一游戏进行标注
。
其中
ＲｅｆＣＯＣＯ数据集对玩家编写描述


没有限制
，
而ＲｅｆＣＯＣＯ＋禁止使用绝对空间位置相关的描述
，
诸如
“ 左边
”
，
“ 右


上角
” 等
，
这要求根据目标物体本身的外观
、
属性等角度进行标注
。这两个数据


集被均被划分成训练集
、
验证集
、
测试集Ａ和测试集Ｂ
。
其中测试集Ａ只包含


目标物体是人的标注
，
测试集Ｂ包含其他类型的目标物体标注
。


ＲｅｆＣＯＣＯｇ由Ｍａｏ等人于
２０
１６年ＣＶＰＲ上提出
。与上述数据集标注方式不


同
，
ＲｅｆＣＯＣＯｇ旨在标注
一个指称表达复杂的数据集
，
因此未采用上述交互式的


２２


第二章基础知识


游戏标注方法
，而是直接请标注人员根据给定的图片和指定的物体编写准确的指


称描述
。
因此ＲｅｆＣＯＣＯｇ中的指称表达相对更复杂
，
平均句子长度达到
８
．４３个


单词
，而在ＲｅｆＣＯＣＯ和ＲｅｆＣＯＣＯ＋中
，指称表达的平均长度分别为
３
．６１和
３
．５３


个词
。ＲｅｆＣＯＣＯｇ通常有两种划分方式
，分别对应ＲｅｆＣＯＣＯ
－
ｇｏｏｇ
ｌｅ和ＲｅｆＣＯＣＯｇ
－


ｕｍｄ。前者将指称表达切分为
８５４７４／９５３６分别用于训练和验证
，后者将其切分为


８０５１２／４８９６／９６０２
，
分别对应训练、验证以及测试
。


Ｆｌｉｃｋｒ３０ＫＥｎｔｉｔｉｅｓ由Ｐｌｕｍｍｅｒ等人于
２０１７年提出
。其图片来自
Ｆｌｉｃｋｒ３０Ｋ数


据集
［７６］
，并扩充了实体级别的标注
，即图像中每个实体与句子描述中对应短语的


对应关系
。


ＳＫ
－ＶＧ由Ｃｈｅｎ等人于
２０２３年ＣＶＰＲ提出
。与上述数据集不同
，
ＳＫ
－ＶＧ面


向基于场景知识的ＲＥＣ任务
，
数据输入不仅仅包含图片和指称表达
，
还有
一段


复杂的场景知识描述。数据标注保证仅仅根据指称表达无法定位图片中的目标物


体
，还需要结合场景知识进行联合推理才能确定指称物体
，对模型的复杂推理能


力要求更高
，
也是对传统ＲＥＣ任务的
一次扩充
。


各数据集的统计信息如表２
－
１所示
。


表
２
－
１ＲＥＣ基准数据集统计


Ｄａｔａｓｅｔ
ＩｍａｇｅｓＥｘｐｒｅｓｓｉｏｎｓ
Ｉｎｓｔａｎｃｅｓ



ＲｅｆｅｒｌｔＧａｍｅ２０，０００１２０，０７２１９
，９８７


ＲｅｆＣＯＣＯ１９
，９９４１４２
，２１０５０
，０００


ＲｅｆＣＯＣＯ＋１９
，９９２１４１
，５６４４９
，８５６


ＲｅｆＣＯＣＯｇ２５
，７９９９５
，０１０４９
，８２２


Ｆｌｉｃｋｒ３０ＫＥｎｔｉｔｉｅｓ３
１
，７８３４２７
，０００４２７
，０００


ＳＫ
－ＶＧ
４
，０００
３９
，５４２
－



２
．４
．２指称表达理解任务评价指标


指称表达理解任务中
，
一个数据样本包括
丨
，分别对应图片
，指称表达


理解
，
和目标物体框坐标
。其中目标物体框坐标为５＝
〇＾九巧
，ｙ２｝
，
分别对应


标注框在图像中左上角的横纵坐标和右下角的横纵坐标
。针对每个样本
，模型会


输出预测框的坐标５


参考现有工作
［
１４
］
［
１８
］［２Ｇ
］［２５
］
［２６
］
［２７
］的主流评估准则
，
当预测框否和标注框５的交


并比
（Ｉｎｔｅｒａｃｔ
ｉｏｎｏｖｅｒＵｎｉｏｎ
，ＩｏＵ）大于
０
．５时视作预测正确
，
以此计算测试集的


正确率作为模型的性能评估指标
，
Ｉ〇Ｕ公式如
２
－６所示
。


＾ＡｒｅａｏｆＯｖｅｒｌａｐ（Ｂ
，Ｂ）


ｌｏＵ（Ｂ
，Ｂ）
＝－￣￣
Ｊ
．／
（２
－６）


ＡｒｅａｏｆＵｎｉｏｎ
（５
，Ｂ
）


２３


北京邮电大学硕士学位论文


２
．５本章小结


本章主要介绍了支撑表达理解任务相关的基础知识
。以常见的指称表达理解


模型必要的组件为线索
，首先介绍了文本
、图像编码器
，然后总结了如今主流ＲＥＣ


方法的模型范式
，
重点介绍了以Ｔｒａｎｓｆｏｒｍｅｒ为架构的方案
。接着介绍了主流的


用于评估指称表达模型性能的基准数据集
，包括数据集来源
，标注方法
，标注特


点
，
规模
。最后介绍了ＲＥＣ任务的评估方法
。


２４


第三章语言引导的指称表达理解研宄


第三章语言引导的指称表达理解研究


３
．
１本章引论


指称表达理解
（ＲＥＣ
）是图文多模态领域的
一个重要任务
，
旨在根据自然语


言表达定位图像区域
。
ＲＥＣ要求模型捕捉文本中的关键线索并进行准确的跨模


态推理
。
最近的趋势是采用基于
Ｔｒａｎｓｆｏｒｍｅｒ的架构来解决这个问题
。
然而
，
如


图
３
－
１所示
，这些方法通常以简单的模态特征拼接结合自注意力的方式进行图文


特征的融合和跨模态推理
。此外
，他们仅仅利用全体的词向量特征而没有考虑诸


如指称表达中关于目标物体的空间位置信息
、全局句子特征等其他抽象层次的语


言信息
。这主要导致如下的问题
：
一方面
，
由于语言的抽象层次更高
，
侧重于概


念等高层语义信息
，而图像的抽象层次更低
，会包含更多底层
，细节的视觉信号
。


面对这种图文模态间不对称的抽象层次
，直接对图文模态特征进行拼接并使用自


注意力机制将会导致文本信息淹没于图像信息中
，损害模型的推理性能
。另
一方


面
，
这些模型对文本特征的不充分利用也进
一步导致了次优的结果
。


■

，
＊

［
—
ｊ
— ＇
ｉ｜
￣
；
；Ｈｏｍｏｇｅｎｅｏｕｓ


ｔｈｅ
ｏｒａｕｇｅ
ｉｎ加
５ｕ
丨＾ｐ
：Ｕ
丨
？⑶麵如


ｏ
＊
ｃｌｏｃｋ
ｐｏｓ
ｉｔｉｏｎ
ｊ
：Ｌ
．
．
；Ｌ

：




ａ
）Ｐｒｅｖｉｏｕｓｔｒａｎｓｆｏｒｍｅｒ
－ｂａｓｅｄｆｒａｍｅｗｏｒｋｓ


ｉｔ
ｌｗ
ｏｒａ＾ｅ
ｉｎｔｈｅ５
；ｒ
＝
ｊ
ｔ

．ａｎｇ
ｉ
！ａｇｅ
ｇｕ
ｉｄｅｄ


ｏ
．
ｃｌｏｃｋｐｏｓｉｔ
ｉｏｎ

；
：
？
ｒ＾
ｒｅ＾ｏａ
ｉｎ
ｆ：
ｊ


ｄ
ｊＲ
－＾ｆ．１
，
… ＡＭ
＼
霞參ｌ


ｂ
）ＯｕｒＬＧＲ
－ＮＥＴ


图
３
－
１
本文的
ＬＧＲ
－ＮＥＴ模型与之前基于Ｔｒａｎｓｆｏｒｍｅｒ的ＲＥＣ方法框架的对比


因此
，本章提出了
一种语言引导的ＲＥＣ推理网络（ＬａｎｇｕａｇｅＧｕｉｄｅｄＲｅａｓｏｎｉｎｇ


Ｎｅｔｗｏｒｋ
，
ＬＧＲ
－ＮＥＴ）
，
以充分捕捉和利用指称表达中的关键描述
。参考之前的工


作
，
模型通过
一个预测标记
（Ｐｒｅｄｉｃｔｉｏｎｔｏｋｅｎ）来捕获跨模态特征并定位目标物


体
。
此外
，
为了避免文本信息的淹没问题
，
ＬＧＲ
－ＮＥＴ没有使用之前的拼接注意


力融合方案（Ｃｏｎｎｅｃｔｅｄ
－ａｔｔｅｎｔｉｏｎ
）而改用跨注意力机制融入文本特征
。为充分利用


文本特征引导跨模态推理
，
文本特征扩展器（ＴｅｘｔｕａｌＦｅａｔｕｒｅＥｘｔｅｎｄｅｒ
，
简称
ＴＦＥ）


２５


北京邮电大学硕士学位论文


从三个方面扩展了它们
。
首先
，
模型设计了
一种基于文本特征的新型坐标嵌入


（Ｃｏｏｒｄｉｎａｔ
ｅＥｍｂｅｄｄｉｎｇ）
。这种坐标嵌入被整合到预测标记中
，
以促进其捕获与语


言相关的视觉特征
。其次
，使用提取的文本特征交替进行文本引导的跨模态对齐


（Ｔｅｘｔ
－
ｇｕｉｄｅｄＣｒｏｓｓ
－ｍｏｄａｌＡｌｉｇｎｍｅｎｔ
，简称ＴＣＡ）和融合（Ｆｕｓｉｏｎ
，简称ＴＣＦ
）
。第三
，


设计了
一种新型的跨模态损失函数
，以增强指称表达与可学习预测标记之间的跨


模态对齐
。


３
．２语言引导的指称表达理解推理网络


本小节全面介绍ＬＧＲ
－ＮＥＴ的整体模型框架和设计思路
，
以及各组件的设计


细节
。


３
．２
．
１框架总览


ＬＧＲ
－ＮＥＴ整体模型框架如图
３
－２所示
，主要包括文本提取模块
（３
．２
．２小节）
，


视觉提取模块
（３
．２
．３小节）
，
文本特征扩展模块
（３
．２
．４小节）
，
跨模态推理与定位


框预测模块
（３
．２
．５小节）
，
以及模型的损失函数设计
（３
．２
．６小节）
。



ｔｈｅｏｒａｎｇｅｉｎｔｈｅ５？涵龛Ｔ
ｉ


ｏ
＇
ｃｌｏｃｋ
ｐｏｓｉｔ
ｉｏｎ
￣
ｇｉｓ
｜
＾
—



’
“
…
，
－
－
．
＇
、




、


□
—
０
：


Ｑ皿
丨
讅Ｕ
Ｋ
：□
丨１坐标编码


□
ＴＣＡＨＩ
：
議４
：ｇ
ｉＡ
句子向量
．
．
．
．
．
．
．
：


｜

！ＴＣＦ￣
＾



：
：

视觉ｔｏｋｅｎ


Ｉ
細键
＇
值ｊ
跨模态換失
：
一
＿ｗｋｅｎ
ｉ


图
３
－２ＬＧＲ
－ＮＥＴ模型框架总览


具体而言
，文本提取和视觉提取模块分别对输入的图片和指称表达进行特征


提取
。其中提取的文本特征首先经过文本特征扩展模块从三个不同的角度进行扩


展
，
生成坐标向量
，
词向量和句子向量
。这三种文本特征随后通过被反复送入跨


模态推理模块中的ＴＣＡ和ＴＣＦ模块以及参与跨模态对齐损失的计算来进行跨模


态推理
。同时
，预测
ｔｏｋｅｎ也得到充分学习
。最后
，定位框预测模块使用预测
ｔｏｋｅｎ


生成所指对象的边界框
。


２６


第三章
语言引导的指称表达理解研究


３
．２
．２文本提取模块


本文使用ＢＥＲＴ作为文本特征提取器
。在经过词表映射后的文本向量前后分


别添加了
［ＣＬＳ
］和
ｔｏｋｅｎ
。
设置最大句子长度为
然后经过ＢＥＲＴ的特征


提取
，
得到最终的文本特征
？
。


３
．２
．３视觉提取模块


如图
３
－３所示
，输入
一张ＲＧＢ图片／ＧＭ
３ｘｔ
ｆｘｌｖ
，
本文使用
ＳｗｉｎＴｒａｎｓｆｏｒｍｅｒ


提取其图像特征
。


／Ｖ
ｉｓｕａ
ｌＥｘｔｒａｃｔｏｒ
＼


＾Ｃｏｎｃａｔｅｎａｔｅ


ｌ
：麗
— —


ＭｅａｎＰｏｏ
ｌ
ｉｎｇ＆ＡｄｄＭａｘＰｏｏｌｉｎｇ


图
３
－３视觉提取器


首先通过图像块分割模块得到初始图像特征图Ｆ。
ｅ
，
然后通过四个


降采样阶段（４倍
、
８倍
、
１６倍
、
３２倍）分别生成四个特征图
［ＦｐＦｈＦｉ
／ｙ
。
然后


使用卷积核大小为１ｘ１的卷积神经网络统
一四个特征图的通道为Ｄｖ
。
通过在相


邻的特征图上逐步使用
ｍｅａｎｐｏｏｌ
ｉｎｇ和平均操作最终得到
一个聚合所有层次特


征的统
一特征图巧
ｅＭ？ｘｉ
ｘｉ
。进
一步地
，为了提升大目标物体的特征提取效果
，


本文还在特征图校的基础上通过２ｘ２的
ｍａｘ
－
ｐｏｏｌｉｎｇ操作生成Ｆ
５
＊
６


最后
，将上述提到的两个特征图Ｇ和Ｇ进行
一维展平并拼接
，得到最终的图像特


征？
已股？叫
，
其中特征长度ｉＶｖ
＝吾ｘ告＋
￡ｘ畀
。


３２３２６４６４


在提取完图文特征后
，
本文分别使用两个全连接神经网络（ＦＦＮ）将图文特征


投射到同
一个特征空间Ｄ
，
最终得到图像特征＆ｅＲ
ＤＸ？和文本特征Ｆ
ｔｅＥ
ＤＸ？


用于后续的模块输入
。


３
．２
．４文本特征扩展模块


为了充分利用文本特征进行跨模态推理
，
本文设计了文本特征扩展（ＴＦＥ）模


块来扩展文本特征
。
ＴＦＥ生成了包含目标物体空间特征的坐标向量（Ｃｏｏｒｄｉｎａｔｅ


Ｅｍｂｅｄｄｉｎｇ）
，
代表全体文本特征的词向量序列
，
以及句子全局特征的句子向量
。


前两者分别用于跨模态推理中的ＴＣＡ和ＴＣＦ模块
，最后
一个用于跨模态损失计


算
。


２７


北京邮电大学硕士学位论文


首先
，
ＴＦＥ根据原始语言特征生成
一个新颖的坐标向量
，
以增强预测
ｔｏｋｅｎ


的空间表示
。指称表达通常包括位置词或对象之间的空间关系
，
比如
“前面的
”或


“
５点钟位置
”
。
这些空间信息提供了目标物体在图片中的位置先验
，
利用它们可


以有效地增强用于定位的预测
ｔｏｋｅｎ的空间表示
。具体来说
，使用
［ＣＬＳ
］ｔ
ｏｋｅｎ通


过多层感知机（ＭＬＰ）生成
一个二维坐标
。
该过程的公式如下
：


（ｃｏｏｒｄ＝ｓｉｇｍｏｉｄ｛ＦＦＮｃ（ｈｃｌｓ））（３
－
１）


ＩＶｃｏｏｒｄ＝ＰＥ
（ｃｏｏｒｄ）


ＦＦＮ包含两层线性层和
一个
ＲｅＬＵ激活函数
。
是［ＣＬＳ
］
ｔｏｋｅｎ的向量表


示
，
ｃｏｏｒｄ是归
一化的二维坐标
，
函数将二维坐标转化为
一个Ｄ维的正弦位置


编码
，
类似原始Ｔｒａｎｓｆｏｒｍｅｒ架构中的位置向量计算过程（见公式２
－２）
。


然后ＴＦＥ直接输出文本特征巧作为词向量用于后续的ＴＣＦ模块
。此外
，ＴＦＥ


还生成了
一个句子向量用于跨模态损失计算
，句子向量来自
［ＣＬＳ
］
ｔｏｋｅｎ的向量表


示
，
公式如下
：


ｆｓｅｎｔ
＝ＦＦＮｓ（ｈｃｌｓ）
（３
－
２）


３
．２
．５跨模态推理与定位框预测模块


为明确跨模态推理模块中语言特征的引导过程
，本节将其分为语言引导的跨


模态对齐
（Ｔｅｘｔ
－
ｇｕｉｄｅｄＣｒｏｓｓ
－ｍｏｄａｌＡｌｉｇｎｍｅｎｔ
，ＴＣＡ
）组件和语言引导的跨模态融


合
（Ｔｅｘｔ
－
ｇｕｉｄｅｄＣｒｏｓｓ
－ｍｏｄａｌＦｕｓｉｏｎ
，ＴＣＦ
）组件
。


１
）文本引导的跨模态对齐（ＴＣＡ）


如图
３
－２所示
，首先在之前提取的图像特征＆前插入
一个预测词元／ｐｅＩＲ
Ｄｘ１


作为初始的跨模态表示义
１５
ｅＭ￣１＋？＼
即


ｘ
０
＝ｆｖ
〇ｍｖ（３
－
３）


ｖｉｓｕａｌ
ｆｅａｔｕｒｅｓ


本文使用自注意力机制对齐图文特征
。在那之前
，首先根据指称表达中目标


物体的位置先验信息增强预测
ｔｏｋｅｎ的空间表征
。具体来说
，在每层的预测
ｔｏｋｅｎ


中加入
３
．２
．４中介绍的ＴＦＥ生成的坐标向量？


ｆｖ
＝
ｆｐ＋
Ｐｃｏｏｒｄ
（３
－４）


后续使用多头自注意力机制和残差链接
、层归
一化用于更新
、对齐视觉特征
。


ｒ
Ｑａ＝ｗｌｘ＼Ｋａ
＝ｗｌｒ
，ｖａ
＝Ｗｊｊ
１


＇
Ｔ
ｌ＝ＬＮ
（
＾
ｓｏｆｔｍａｘｖａ＋
（３
－
５）


２８


第三章语言引导的指称表达理解研究


其中１＾
，
１４＾
，Ｍ／％分别是
ｑｕｅｒｙ
、
ｋｅｙ
、
ｖａｌｕｅ的映射权重矩阵
。
ＴＣＡ的输出


Ｐ即为对齐的图像特征
，
４是通道数
，
为层归
一化。


２
）文本引导的跨模态融合（ＴＣＦ）


本文使用跨模态注意力机制融合文本特征
。
使用对齐的图像特征炉作为


ｑｕｅｒｙ
，
文本特征巧作为ｋｅｙ和ｖａｌｕｅ。
文本引导的跨模态注意如下
：


ｒ
Ｑｆ
＝Ｗ＾Ｔ＼Ｋ
ｆ
＝Ｗｌｆ
Ｆ
ｔ
，Ｖ
ｆ
＝Ｗｊ
ｆ
Ｆ
ｔ


＾Ｔ
ｌ＝ＬＮ
（
＾
ｓｏｆｔｍａｘｖ
ｆ＋（３
－６）


、Ｘ
ｉ＋１＝ＬＮ（Ｔ
ｌ
＋ＦＦＮｆ〇Ｔ
１
））


其中
分别是
ｑｕｅｒｙ
、
ｋｅｙ
、
ｖａｌｕｅ的映射权重矩阵
。
此处
，
输出


的跨模态表示Ｘ
ｉ＋１捕获了跟指称物体相关的关键文本特征
。与此同时
，预测词元


聚合了视觉相关的文本特征用于后续轮次的ＴＣＡ过程。本小节堆叠了Ｎ层ＴＣＡ


和ＴＣＦ模块
，
它们交替工作
。


最终
，基于最后
一层输出的
，经过充分学习的预测词元／／
，通过
一个三层的


ＦＦＮ和
ｓｉｇｍｏｉｄ激活函数生成最终的预测框
，


（ｘ，ｙ，ｗ，ｈ）
＝ｓｉｇｍｏｉｄ
（
ｆＦＪＶｈ（／ｐ
ｗ
））
（３
－
７）


其中
，
Ａｙ分别代表预测框中心点的坐标
，
ｗ，ｈ表示预测框的宽和高
。


３
．２
．６损失函数


为了训练ＬＧＲ
－ＮＥＴ
，
本节设计了
一个包含两项的损失函数
。
前者为预测框


的回归损失
，
后者为跨模态对齐损失
。


ＮＮ


【
ｔｏｔａｌ
＝〉
：＾ｂｏｘ
ｉ２
＾ａｌｌｇｒｔ
ｉ（３
－
８）


ｉ＝ｌｉ
－ｌ


损失的第
一项用于帮助预测词元捕获所指对象的边缘特征
。第二项能促进其


捕获目标物体中与指称表达语义
一致的视觉特征
，
具体可见
３
．３
．５小节可视化分


析
。超参数；Ｉ用于平衡两者
。


具体来说
，
推理模块包括Ｎ层堆叠的ＴＣＡ和ＴＣＦ组件
，
将第ｉ层的预测框


表示为ｈ真实的标签为Ｓ＝
（元
；ｐ，设
，幻
，那么损失的第
一项计算


如下
，


＾ｂ〇Ｘ
ｉ
＝＾Ｇｉ〇ｖ｛ｈ，ｂ）＋ｔＬｌ｛ｐｉ
＞＊
）（３
－９）


其中
，
和仏分别是ＧＩｏＵ损失
［７７］和ＬＩ损失
。


２９


北京邮电大学硕士学位论文


此外
，为了增强指称目标和指称表达之间的对齐效果
，本节设计了
一项对比


形式的对齐损失
。将
一个批次里匹配的图文对视作正样本
，不匹配的视作负样本
。


对齐损失公式如下
：


！
＂ｅｘｐ
（
／〇ｉｙ
－％）


Ｍａｌｉｇｎ
＝ｌ〇９
７
￣
－
￣
（３
＿
１０）


片Ｅｆ＝１ｅｘｐ＾
．
－＾
Ｊ


其中ｆ
ｉ为批量大小
，
？代表内积
。其中Ｔ是可训练的温度参数
，
用于控制分布


的平滑度
。其中句子特征／ｓｅｎｔ来自
ＴＦＥ的输出
，
物体特征来自预测词元／ｐ
。


ｆｏｂｊ
＝ＦＦＮｏｊｂ（ｆｐ）（３
－
１１）


注意
，
为了简化公式表达
，
层数下标ｉ被忽略了
。


３
．３实验对比与分析


３
．３
．１实验参数与设置


本文设置输入图片的分辨率为６４０Ｘ
６４０
，
设置指称表达最大长度为４０。具


体来说
，
首先对图片进行
ｒｅｓｉｚｅ操作
，
使其更长的边为６４０
，
然后对短的
一边进


行填充操作
，
最终实现大小为６４０Ｘ
６４０
。对于超出限定长度的指称表达
，
对其


做尾部截断
，否则进行填充
。图片和文本的填充部分都会在
ａｔｅｎｔ
ｉｏｉｉ计算过程中


进行ｍａｓｋ操作
。本文设置层数ｉＶ＝６
，通道数Ｚ）＝２５６
，权重因子；Ｉ为２
。本文使


用
ＢＥＲＴ
－ｂａｓｅ
－ｕｎｃａｓｅｄ初始化文本特征提取器的参数
。视觉骨干初始化方法同


ＱＲＮｅｔ
［２７］
。
其他部分的参数进行随机初始化
。
本文使用权重衰减因子为１０
－４的


Ａｄａｍ优化器优化模型
。
ＢＥＲＴ和
ＳｗｉｎＴｒａｎｓｆｏｒｍｅｒ的初始学习率设置为１（Ｔ
Ｓ
，


其他部分参数的学习率设置为１ＣＴ
４
。设置可学习的温度参数ｔ＝０
．０７
。本文参考


之前工作
［２５］的做法进行了数据増强。本文训练模型
９０轮
，并且
６０轮后
，整体学


习率乘以
０
．１
。


针对预训练设置
，
本文首先使用ＶｉｓｕａｌＧｅｎｏｍｅ数据集
对ＬＧＲ
－ＮＥＴ模型


进行６轮的预训练
，然后在对应的基准数据集上微调
５０轮。ＶｉｓｕａｌＧｅｎｏｍｅ数据


集包含
１００Ｋ图片
，本文去除了其中出现在
５个基准数据集的验证集和测试集中


的图片
，
避免数据泄漏
。所有实验使用
８张ＮＶＩＤＩＡＶ１００显卡
，
ｂａｔｃｈｓｉｚｅ设置


为
１６
。对于所有的实验结果
，
本文使用不同的随机种子跑了５次实验
，
误差在


± ０
．５％之内
。


３０


第三章
语言引导的指称表达理解研究


３
．３
．２评测指标对比与分析


本节选取了指称表达理解最近几年主流的工作进行性能对比
，将其划分为基


于提议的
，基于锚框的
，基于Ｔｒ
ａｎｓｆ
ｏｒｍｅｒ的
，以及基于预训练的工作四个大类
。


简要说明对比的基线模型如下
：


ａ）基于提议的模型


１
）ＶＣ
［７９
］提出
一种变分贝叶斯（ＶａｒｉａｔｉｏｎａｌＢａｙｅｓｉａｎ）方法解决指称表达理解中


的复杂上下文建模问题
。


２
）ＭａｔｔＮｅｔ
［
１４
］利用模块化网络从目标物体的外观
，
位置
，
与其他物体的关系


三个方面建模ＲＥＣ中图像和文本的细粒度相似性
。


３）ＣＩＴＥ
［８Ｇ
］在
一个端到端的模型中学习多个文本条件向量
，
以实现短语的定


位
。


４）ＤＤＰＮ
［８
１
］专注于在生成提议时的多样性和区分性
，
并为ＲＥＣ提出了多样


化和具有区分性的提议网络
。


５）ＬＧＲＡＮＳ
［１５］提出图注意力机制建模目标物体及其邻居节点的关系
。


６）Ｓｉｍｉｌａｒ
ｉｔｙＮｅｔ
［Ｓ２
］提出编码网络和相似性网络共同用于解决ＲＥＣ任务
。


７）ＮＭ
－Ｔｒ
ｅｅ
［８３
］Ｕ句子的依存分析树出发解决ＲＥＣ问题
，
并基于树结构执行


细粒度的推理
。


８）ＤＧＡ
［１６］提出动态图注意力网络逐步执行多步的跨模态推理
。


９）Ｒｅｆ
－ＮＭＳ
［８４
］通过查询相关的非极大化抑制操作提高生成候选提议过程中


的对目标物体的召回率
，
从而提高模型的定位效果
。


１０）ＤＩＧＮ
［８５］关注场景图上下文中隐含的不同主题
，
并设计了
一种解耦的图


网络
，
以将主题感知的上下文信息整合到特征表示中
。


１
１）ＲｖＧ
－Ｔｒｅｅ
［８６］构建了
一个二叉树结构来解析指称表达
，
并以自下而上的方


式沿着树进行视觉推理。


ｂ）基＾｜ｇ的模型


１）ＦＡＯＡ
［
１８
］扩展ＹＯＬＯＶ３
，
引入文本特征获得锚框集合的跨模态特征表示
，


并选择得分最高的锚定框作为结果
。


２）ＲＣＣＦ
［Ｍ将指称表达理解重新定义为相关滤波过程
，
并在实时场景中取得


了良好的性能
。


３）ＲｅＳＣ
－Ｌａｒｇｅ
［２Ｇ？过递归子查询扩展了多轮交互改进
ＦＡＯＡ面对复杂文本


的定位性能
。


４）ＳＡＦＦ
［８７
］提出了
一个语义感知框架
，
利用查询的结构化知识和上下文敏感


表示来过滤视觉特征图
，
以更准确地定位指代对象
。


３
１


北京邮电大学硕士学位论文


５）ＬＢＹＬ
－Ｎｅｔ
［８－ｆ出了
一个地标特征卷积模块
，
该模块沿着不同方向传递视


觉特征
，
实现实时的定位效果
。


ｃ）基于Ｔｒａｎｓｆｏｒｍｅｒ的模型


１
）ＴｒａｎｓＶＧ
［２５
］首次提出使用
Ｔｒａｎｓｆｏｒｍｅｒ结构处理ＲＥＣ任务
，
并直接回归边


界框来定位目标物体
。


２
）ＶＬＴＶＧＰ８
］提出了
一个视觉
－语言验证模块
，
以增强视觉特征
，从而提升图


文对齐的效果
，
进而提高模型的性能
。


３）ＲｅｆＴＲ
［２９
］通过多任务学习
，
包括指称表达理解和指称表达分割
（Ｒｅｆｅｒｒｉｎｇ


ＥｘｐｒｅｓｓｉｏｎＳｅｇｍｅｎｔａｔｉｏｎ
，ＲＥＳ
）
，
以同时提高模型对两个任务的性能
。


４）（＾
１＾￥
２７
］通过
一个查询感知的动态注意力机制来増强视觉骨干网络
，使得


模型进行视觉特征提取时就将语言特征考虑到
，进而减少其他物体对视觉提取器


的干扰
。


５）Ｗｏｒｄ２Ｐｉｘ
［８９
］基于编码器
－解码器变换器架构
，
通过单词到像素的注意力机


制实现文本到视觉特征的对应学习
。


６）ＹＯＲＯ
［９Ｑ
］应用了
一种多模态变换器编码器架构用于
ＲＥＣ
，
实现了良好的


速度／准确度平衡
。


７）ＳｅｑＴＲ
［９
１
］提出了
一个通用网络用于视觉定位任务
，包括短语定位
、ＲＥＣ和


ＲＥＳ
〇


８）ＶＧ
－ＬＡＷ
［３Ｇ
］生成语言自适应权重用于视觉骨干网络
，
并通过多任务头直


接输出所指对象
，
无需额外的跨模态交互模块
。


９）ＴｒａｎｓＶＧ＋＋
［２６
］采用纯视觉变换器作为模型框架
，并用语言条件层替换了
一


些层以用于ＲＥＣ任务
。


１０
）ＤｙｎａｍｉｃＭＤＥＴＲ
［９２
］利用稀疏先验通过动态多模态变换器解码器加速视


觉定位过程
。


ｄ）基于预训练的模型


１
）ＶＵＢＥＲＴ
［９３
］将流行的ＢＥＲＴ架构扩展为
一个多模态双流模型
，分别处理视


觉和文本输入
，
并通过协同注意力变换器层进行交互
。


２
）ＵＮＩＴＥＲＭ＾出了
一种通用的图像
－文本表示学习
，
通过在四个图像
－文本


数据集和四个预训练任务上的大规模预训练学习
。


３）ＭＤＥＴＲ
［３
１
］是基于ＤＥＴＲ的端到端调制多模态检测器
，
通过基于查询的变


换器解码器来检测所有对象
。


４）ＯＦＡ
［３２
］将不同的视觉
－语言任务统
一到
一个
Ｓｅｑｕｅｎｃｅｔｏｓｅｑｕｅｎｃｅ框架中
，


并收集大量数据来预训练他们的框架
。


３２


第三章
语言引导的指称表达理解研宄


５）Ｓｈｉｋｒａ
［Ｍ］使多模态大型语言模型能够处理空间坐标的输入和输出
，并将其


应用于指代对话。


６）ＯＮＥ
－ＰＥＡＣＥ
［９７］构建了
一个面向任意模态的通用表示模型
，
可以无缝地在


视觉
、
音频和语言模态之间对齐和集成
。


７）ｍＰＬＵＧ
［９６］引入了
一种高效的视觉
－语言架构
，具有新颖的跨模态跳跃连接
，


并在大规模图像
－文本对上进行了端到端的预训练
，
同时具有区分性和生成性目


标
。


表
３
－
１
中展示了ＬＧＲ
－ＮＥＴ在三个基准数据集（ＲｅｆＣＯＣＯ
、
ＲｅｆＣＯＣＯ＋和


ＲｅｆＣＯＣＯｇ）上的实验结果
。可以发现ＬＧＲ
－ＮＥＴ在三个数据集上均取得了优越的


性能
。
与最佳的基于提议的方法Ｒｅｆ
－ＮＭＳ和基于锚框的方法ＬＢＹＬ
－Ｎｅｔ相比
，


ＬＧＲ
－ＮＥＴ在各方面都取得了显著的优势
。对于ＲｅｆＣＯＣＯ和ＲｅｆＣＯＣＣＨ数据集
，


ｔｅｓｔＢ上的改进尤为显著
。这表明
ＬＧＲ
－ＮＥＴ在涉及多样化的指称对象时实现了


更好的效果
。对于ＲｅｆＣＯＣＯｇ
，指称表达更为复杂
，
ＬＧＲ
－ＮＥＴ在ｖａｌ
－ｇ上仍然超


出ＬＢＹＬ
－Ｎｅｔ１２
．７８％
，
在ｖａｌ
－ｕ、
ｔｅｓｔ
－ｕ上平均超越Ｒｅｆ
－ＮＭＳ６
．３４％
。这表明模型


在面对复杂的引用表达时能够进行更准确的推理
。


此外
，
可以观察到
ＬＧＲ
－ＮＥＴ在表
３
－１
中仍然优于大部分最近提出的基于


Ｔｒａｎｓｆｏｒｍｅｒ的方法
。与ＴｒａｎｓＶＧ相比
，ＬＧＲ
－ＮＥＴ在使用ｒｅｓｎｅｔ
－１０
１作为ｂａｃｋｂｏｎｅ


时在ＲｅｆＣＯＣＯ、
ＲｅｆＣＯＣＯ＋和ＲｅｆＣＯＣＯｇ上分别实现了高达３
．７０％、
８
．０８％和


６
．５０％的绝对提升
。
这展示了ＬＧＲ
－ＮＥＴ相较于拼接注意力的跨模态推理方法的


优越性
。
与表现最好的ＶＬＴＶＧ相比
，
ＬＧＲ
－ＮＥＴ在相同的骨干网络上获得了可


比较的结果
，进
一步
，
当双方都使用更好的视觉骨干网络
ｓｗｉｎ
－ｓ时
，本文的方法


全面超越了ＶＬＴＶＧ
，
这意味着ＬＧＲ
－ＮＥＴ对更强骨干网络适应性更好
。此外
，


与那些具有更强视觉骨干网络的基线模型
，如ＶｉＴ
－Ｂ和ＣＬＩＰ
－Ｂ相比
，
ＬＧＲ
－ＮＥＴ


取得了有竞争力的结果
。尤其当面对指称表达更长
、
更复杂
ＲｅｆＣＯＣＯｇ数据集


时
，
ＬＧＲ
－ＮＥＴ取得了显著的性能优势
。
这进
一步证明了ＬＧＲ
－ＮＥＴ在面对复杂


文本时的推理性能
。


进
一步
，
表３
－２中展示了ＬＧＲ
－ＮＥＴ在ＲｅｆｅｒｌｔＧａｍｅ和Ｆｌｉｃｋｒ３０ＫＥｎｔｉｔｉｅｓ上


的实验结果
。可以发现ＬＧＲ
－ＮＥＴ明显优于基于提议和基于锚框的方法
。与最佳


的基于Ｔｒａｎｓｆｏｒｍｅｒ的方法相比
，
ＬＧＲ
－ＮＥＴ同样展现出有竞争力的结果
。值得注


意的是
，
ＲｅｆｅｒｌｔＧａｍｅ和Ｆｌｉｃｋｅｒ３０ＫＥｎｔｉｔ
ｉｅｓ中的指称表达主要是简单的名词性短


语
，
不适合展示ＬＧＲ
－ＮＥＴ面对复杂文本的推理能力
。
因此
，
在这两个数据集上


获得的提升小于在ＲｅｆＣＯＣＯｇ上的改进
。


３３


北京邮电大学硕士学位论文


表
３
－
１ＬＧＲ
－ＮＥＴ模型在ＲｅｆＣＯＣＯ
，ＲｅｆＣＯＣＯ＋
，
ＲｅｆＣＯＣＯｇ上的性能指标对比


ＲｅｆＣＯＣＯＲｅｆＣＯＣＯ＋ＲｅｆＣＯＣＯｇ


ＭｅｔｈｏｄＢａｃｋｂｏｎｅ


ｖａｌｔｓｔＡｔｅｓｔＢｖａｌｔｅｓｔＡｔｅｓｔＢｖａｌ
－ｇｖａｌ
－ｕｔｅｓｔ
－ｕ


Ｐｒｏｐｏｓａｌ
－ｂａｓｅｄ


ＶＣ
［７９
】ＶＧＧ
１６
－７３
．３３６７
．４４
－５８
．４０５３
．
１８６２
．３０
－
－


ＭａｔＮｅｔ
ｆ
１４
］ＲｅｓＮｅｔ
－
１０１７６．６５８１
．
１４６５
．３３７
１
．６２５６
．０２
－６６
．５８６７
．２７


ＬＧＲＡＮｓＭＶＧＧ１６
－７６．６０６６
．４０
－６４
．００５３
．４０６２
．５０
－
－


ＲｖＧ
－Ｔｒｅｅ岡ＲｅｓＮｅｔ
－
１０１７５
．０６７８
．６１６９
．８５６３
．５
１６７
．４５５６
．６６
－６６
．９５６６
．５１


ＮＭ
－Ｔｒｅｅ
丨８３
］ＲｅｓＮｅｔ
－
１０１７６
．４１８１
．２１７０
．０９６６
．４６７２
．０２５７
．５２６４
．６２６５
．８７６６
．４４


ＤＧＡ
【
１６】ＶＧＧ１６
－７８
．４２６５
．５３
－６９
．０７５
１
．９９
－
－６３
．２８


Ｒｅｆ
－ＮＭＳ＾ＲｅｓＮｅｔ
－
１０１８０
．７０８４．００７６．０４６８
．２５７３
．６８５９．４２
－７０
．５５７０
．６２


Ａｎｃｈｏｒ
－ｂａｓｅｄ


ＦＡＯＡ
［
１８］ＤａｒｋＮｅｔ
－５３７２
．５４７４
．３５６８
．５０５６
．８１６０
．２３４９
，６０５６
．
１２６
１
．３３６０
．３６


ＲＣＣＦＭＤＬＡ
－３４
－８
１
．０６７
１
．８５
－７０
．３５５６
．３２
－
－６５
．７３


ＲｅＳＣ
－Ｌａｉｇｅ
［２〇
ＪＤａｒｋＮｅｔ
－５３７７
．６３８０
．４５７２
．３０６３
．５９６８
．３６５６
．８１６３
．
１２６７
．３０６７
．２０


ＳＡＦＦ
［ｍＤａｒｋＮｅｔ
－５３７９
．２６８
１
．０９７６
．５５６４
．４３６８
．４６５８
．４３
－６８
．９４６８
．９
１


ＬＢＹＬ
－Ｎｅ＃
８
］ＤａｒｋＮｅｔ
－５３７９
．６７８２
．９
１７４
．
１５６８
．６４７３
．３８５９
．４９６２
．７０
－
－


Ｔｒａｎｓｆｏｒｍｅｒ
－ｂａｓｅｄ


Ｗｏｒｄ２Ｐｉｘ
［８９
］ＲｅｓＮｅｔ
－
１０１８１
．２０８４
．３９７８
．
１２６９
．４６７６
．８
１６１
．５７
－７０
．８
１７１
．３４


ＴｒａｎｓＶＧＰ５
］ＲｅｓＮｅｔ
－
１０１８
１
．０２８２
．７２７８
．３５６４
．８２７０
．７０５６
．９４６７
．０２６８
．６７６７
．７３


ＲｅｆＴＲ＾ＲｅｓＮｅｔ
－
１０１８２
．２３８５
．５９７６
．５７７
１
．５８７５
．９６６２
．
１６
－６９
．４
１６９
．４０


ＹＯＲ〇Ｐ°
】Ｌｉｎｅａｒ８２
．９０８５
，６０７７
．４０７３
．５０７８
．６０６４
．９０
－７３
．４０７４
．３０


ＳｅｑＴＲ
［９
１
】ＤａｒｋＮｅｔ
－５３８１
．２３８５
．００７６
．０８６８
．８２７５
．３７５８
．７８
－７１
．３５７１
．５８


ＱＲＮｅｔ
ｆ
２７
】Ｓｗｉｎ
－Ｓ８４
．０１８５
．８５８２
．３４７２
．９４７６
．１７６３
．８１７１
．８９７３
．０３７２
．５２


ＶＬＴＶＧ
ｔ２８
）ＲｅｓＮｅｔ
－
１０
１８４
．７７８７
．２４８０
．４９７４
．
１９７８
．９３６５
．
１７７２
．９８７６
．０４７４
．
１８


ＶＬＴＶＧ（Ｓｗｉｎ）Ｓｗｉｎ
－Ｓ８４
．６９８７
．５４８２
．３２７４
．２８７９
．２２６７
．９５７３
．９８７４
．８６７５
．
１
１


ＶＧ
－ＬＡＷ
［Ｍ
】ＶｉＴ
－Ｂ８６
．０６８８
．５６８２．８７７５．７４８０
．３２６６
．６９
－７５
．３
１７５
．９５


ＴｒａｎｓＶＧ＋＋
［２？ＶｉＴ
－Ｂ８６．２８８８
．３７８０
．９７７５
．３９８０
．４５６６
．２８７３
．８６７６
．
１８７６
．３０


ＤｙｎａｍｉｃＣＬＩＰ
－Ｂ８５
．９７８８．８２８０
．
１２７４
．８３８１
．７０６３
．４４７２
．２１７４．１４７４．４９


ＭＤＥＴＲ
【９２
］


Ｏｕｒｓ


ＬＧＲ
－ＮＥＴＲｅｓＮｅｔ
－
１０１８３
．６９８６
．４２７９
．２５７３
．５０７８
．３６６５
．０２７１
．３８７４
．
１４７４
．２３


ＬＧＲ
－ＮＥＴＳｗｉｎ
－Ｓ８５
．６３８８
．２４８２
．６９７５
．３２８０
．６０６８．３０７５．４８７６．８２７７．０３


３４


第三章语言引导的指称表达理解研究


表３
－２ＬＧＲ
－ＮＥＴ模型在ＲｅｆｅｒｌｔＧａｍｅ和Ｆｌｉｃｋｒ３０ＫＥｎｔｉｔｉｅｓ上的性能指标对比
，
ＬＧＲ
－ＮＥＴ＊


表示在ＶＧ数据集上进行了预训练


ＲｅｆｅｒｌｔＧａｍｅＦｌｉｃｋｒ３０Ｋ


ＭｅｔｈｏｄＢａｃｋｂｏｎｅ


ｔｅｓｔｔｅｓｔ


Ｐｒｏｐｏｓａｌ
－ｂａｓｅｄ


ＶＣ
［７９］ＶＧＧ１６３
１
．
１３
－


ＭａｔｔＮｅｔ
ｆ
１４
］ＲｅｓＮｅｔ
－
１０１２９
．０４
－


Ｓｉｍｉｌａｒ
ｉｔｙＮｅｔ
［８２
］ＲｅｓＮｅｔ
－
１０
１３４
．５４６０
．８９


ＣＩＴＥ
［８ｆ
ｌ
］ＲｅｓＮｅｔ
－
１０１３５
．０７６１
．３３


ＤＤＰＮ
［８
１
］ＲｅｓＮｅｔ
－１０１６３
．００７３
．３０


Ｄ１ＧＮ脚
ＶＧＧ１６
６５
．
１５
７８
．７３


Ａｎｃｈｏｒ
－ｂａｓｅｄ


ＦＡＯＡ
［
１８
］ＤａｒｋＮｅｔ
－５３６０
．６７６８
．７１


ＲＣＣＦ
［
１９］ＤＬＡ
－３４６３
．７９
－


ＲｅＳＣ
＿Ｌａｒｇｅ
ｆ２〇５ＤａｒｋＮｅｔ
－５３６４
．６０６９
．２８


ＳＡＦＦ
［８７
】ＤａｒｋＮｅｔ
－５３６６
．０１７０
．７１


ＬＢＹＬ
＿Ｎｅｔ
［８８］
ＤａｒｋＮｅｔ
－５３
６７
．４７
－



Ｔｒａｎｓｆｏｒｍｅｒ
－ｂａｓｅｄ


ＴｒａｎｓＶＧ
ｔ２５
！ＲｅｓＮｅｔ
－１０１７０
．７３７９
．
１０


ＲｅｆＴＲ
［２９
］ＲｅｓＮｅｔ
－
１０１７１
．４２７８
．６６


ＹＯＲＯ
［９０
］Ｌｉｎｅａｒ７１
．９０
－


ＳｅｑＴＲ
［９
１
］ＤａｒｋＮｅｔ
－５３６９
．６６８１
．２３


ＶＬＴＶＧ网ＲｅｓＮｅｔ
－
１０１７
１
．９８７９
．８４


ＶＬＴＶＧ（Ｓｗｉｎ）Ｓｗｉｎ
－Ｓ７０
．２６８０
．５７


ＱＲＮｅ＃
７】Ｓｗｉｎ
－Ｓ７４
．６１８
１
．９５


ＶＧ
－ＬＡＷ
［３０
］ＶｉＴ
－Ｂ７６．６０
－


ＴｒａｎｓＶＧ＋＋
ｔ２６
ｌＶｉＴ
－Ｂ７４
．７０８１
．４９


ＤｙｎａｍｉｃＭＤＥＴＲ
［９２
］
ＣＬＩＰ
－Ｂ
７０
．３７
８
１
．８９


Ｏｕｒｓ


ＬＧＲ
－ＮＥＴＲｅｓＮｅｔ
－１０１７１
．０３７９
．６１


ＬＧＲ
－ＮＥＴＳｗｉｎ
－Ｓ７４
．６４８１
．９７


ＬＧＲ
－ＮＥＴ＊

Ｓｗｉｎ
－Ｓ
７７．７８
８２．１８


此外
，
为了与基于预训练的方法进行比较
，
本文使用了３
．３
．
１小节中描述的


预训练策略来预训练ＬＧＲ
－ＮＥＴ模型
。表
３
－３中的预训练基线可以分为两类
。第


３５


北京邮电大学硕士学位论文


一类包括ＭＤＥＴＲ和ＲｅｆＴＲ
，它们的网络架构是专为检测或分割任务设计的
，并


只在这些任务的数据集如ＶＧ
、
ＭＳ
－ＣＯＣＯ和
Ｆｌｉｃｋｒ３０Ｋ上进行了预训练
。
由于


模型框架的限制
，可用的预训练数据集和下游任务相对较少
。其他基线属于第二


类
，
如
ＯＦＡ和ＯＮＥ
－ＰＥＡＣＥ
。
这些方法将不同类型的多模态任务统
一为序列到


序列任务
，
并应用统
一的框架来处理它们
。
这样
，
它们可以从
ＶＱＡ
、
图像描述


和ＲＥＣ等多种不同的多模态任务中收集数据集
。至于ＲＥＣ数据集
，
它们会将边


界框标签转换为类似
的
“ 句子
”
。因此
，这些方法可以利用


更多的数据集进行预训练
，如表
３
－３所示
。本文的预训练ＬＧＲ
－ＮＥＴ属于第
一类
，


因为它仅适用于ＲＥＣ
。
与
ＲｅｆＴＲ和ＭＤＥＴＲ相比
，
预训练
ＬＧＲ
－ＮＥＴ在相同甚


至更少的预训练数据上实现了更好的性能
，这展示了它的可扩展性
。与第二类方


法如ＯＮＥ
－ＰＥＡＣＥ或ｍＰＬＵＧ相比
，本文的方法性能略差
。可见
，
由多种数据集


预训练的通用跨模态框架拥有更高的性能上限
。


综上所述
，
上述实验结果表明了ＬＧＲ
－ＮＥＴ相比之前的基线方法的优势
，
尤


其是面对具备复杂指称表达的ＲｅｆＣＯＣＯｇ数据集
，
ＬＧＲ
－ＮＥＴ展现出显著的性能


优势
，
说明其面对复杂指称表达时优越的跨模态推理效果
。
在预训练设置下
，


ＬＧＲ
－ＮＥＴ超越了相同设置下的基线
，
证明了ＬＧＲ
－ＮＥＴ的可扩展性
。


表
３
－３ＬＧＲ
－ＮＥＴ模型在预训练设置下的性能对比


Ｐｒｅｔｒａ
ｉｎＲｅｆＣＯＣＯＲｅｆＣＯＣＯ＋ＲｅｆＣＯＣＯｇ


ＭｅｔｈｏｄＢａｃｋｂｏｎｅ


Ｉｍａｇｅｓｖａｌｔｅｓ
ｔ
－Ａｔｅｓｔ
－Ｂｖａ！ｔｅｓｔ
Ａｔｅｓｔ
－Ｂｖａｌ
－
ｇｖａｌ
ｕｔｅｓｉ
ｕ


ＶｉＬＢＥＲＴ州ＲｅｓＮｅｔ
－
１０
１３
．３Ｍ
－
－
－７２
．３４７８
．５２６２
．６
１
－
－
－


ＵＮＩＴＥＲ
Ｌ＿ＲｅｓＮｅｔ
－
１０
１４
．６Ｍ８
１
．４
１８７
．０４７４
．
１７７５
．９０８
１
．４５６６
．７０
－７４
８６７５
．７７


ＲｅｆＴＲ〇ＲｅｓＮｅｔ
－
１０
１
１００ｋ８５
．６５８８
．７３８
１
１６７７
．５５８２
．２６６８
．９９
－７９
．２５８０
．０
１


ＭＤＥＴＲ
【３
ｉ
］ＲｅｓＮｅｔ
－
１０
１２００ｋ８６
．７５８９
．５８８
１
．４
１７９
．５２８４
．０９７０
．６２
－８
１
．６４８０
．８９


ＯＦＡ
［３２
］ＲｅｓＮｅｔ
－
１５２２０Ｍ９０
．０５９２
．９３８５
．２６８５
．８０８９
．８７７９
．２２
－８５
８９８６
．５５


Ｓｈ
ｉｋｒａＰ
］ＶｉＴ
－Ｌ
－８７
．８３９
１
．
１
１８
１
．８
１８Ｚ８９８７
７９７４
．４
１
－８２
．６４８３
１６


ｍ
－ＰＬＵＧ％
ｌＶｉＴ
－Ｌ
１４Ｍ９２
．４０９４
．５１８８
．４２８６
．０２９０
．
１７７８
．
１
７８５
．８８８６
．４２


ＯＮＥ
－Ｖ
ｉＴ
－
ｇ
１
．５Ｂ９２
．５８９４
．
１
８８９
．２６８８
．７７９２
．２１８３
．２３
－８９
．２２８９．２７


ＰＥＡＣＥ
＇
４７
！


ＯｕｒＬＧＲ
－Ｓｗ
ｉｎ
－Ｓ
１００ｋ８８
．
１６９０
．０
１８４
．２６７８
．６６８３
．５
１７３
．
１８８０
．９４８２
．６０８２
．５８


ＮＥＴ
＊


３６


第三章语言引导的指称表达理解研究


３
．３
．３消融实验


为进
一步研究ＬＧＲ
－ＮＥＴ模型中各组件的有效性
，本节基于ＲｅｆＣＯＣＯｇ数据


集进行了详细的组件消融实验
。
由于ＲｅｆＣＯＣＯｇ包含更长而复杂的指称表达
，


选用它作为消融数据集能更清晰地解释ＬＧＲ
－ＮＥＴ各组件的效果
。


１）坐标编码


为了揭示ＬＧＲ
－ＮＥＴ中坐标编码
（ＣｏｏｒｄｉｎａｔｅＥｍｂｅｄｄｉｎｇ
，ＣＥ
）在不同数据集


上的影响
，本小节首先统计了不同数据集中包含空间位置词的指称表达比例
，结


果如表
３
－４所示
。
具体来说
，
如果
一个指称表达中包含
“
ｌｅｆｔ
”
，
“
ｒ
ｉｇｈｔ
”
，
“
ｔｏｐ
”
，


“
ｂｏｔｏｍ
”
，
“
ｍｉｄｄｌｅ
”
，
“
ｏ
’
ｃｌｏｃｋ
” 其中
一个或多个单词
，
则认为该指称表达包含


空间位置词
，
据此统计不同数据集中的占比
。


表
３
￣４
不同数据集中包含空间位置词的指称表达比例（％）统计


ＤａｔａｓｅｔｓＲｅｆＣＯＣＯＲｅｆＣＯＣＯ＋ＲｅｆＣＯＣＯｇＦｌｉｃｋｒ３０ＫＲｅｆｅｒｌｔＧａｍｅ


Ｐｒｏｐｏｒ
ｔｉｏｎ５４
．８２＾６５
１７
．９６
Ｌ０８
４５
．９２


在此基础上
，选取ＲｅｆＣＯＣＯ／＋／ｇ三个数据集进行关于ＣＥ的消融实验
。这三


个数据集中包含空间位置词的指称表达比例差异明显
，便于观察ＣＥ的消融效果
，


结果如表
３
－５所示
。可以发现
，
在ＲｅｆＣＯＣＯ＋中
，
由于空间指称表达比例不高
，


ＣＥ给ＬＧＲ
－ＮＥＴ带来的提升不明显
。
相比之下
，
ＣＥ在ＲｅｆＣＯＣＯ和ＲｅｆＣＯＣＯｇ


上给ＬＧＲ
－ＮＥＴ带来的性能提升显著
，在ＲｅｆＣＯＣＯ的
ｔｅｓｔＢ中
，ＣＥ带来了１
．７７％


的性能提升
。充分说明了ＣＥ在捕捉指称表达中关于物体空间位置描述的有效性
。


表
３
－５
不同数据集下ＣＥ的消融结果


ＲｅｆＣＯＣＯＲｅｆＣＯＣＯ＋ＲｅｆＣＯＣＯｇ


Ｍｏｄｅｌｓ


ｔｅｓｔＡｔｅｓｔＢｔｅｓｔＡｔｅｓｔＢｖａｌ
－ｕｔｅｓｔ
－ｕ


ＬＧＲ
－ＮＥＴ（ｗ／ｏＣＥ
）８７
．
１６８０
．９２８０
．
１
１６８
．３０７５
．４７７５
．９１


ＬＧＲ
－ＮＥＴ８８
．２４８２
．６９８０
．６０６８
．３０７６
．８２７７
．０３



Ａ
＋１
．０８＋１
．７７Ｈ
－Ｑ
．４９＋０＋１
．３５＋Ｕ２


此外
，
对坐标编码ＣＥ融入预测
ｔｏｋｅｎ的方式也进行了消融实验
，
探索不同


方式的优劣
。包括加和（ａｄｄ
）
，乘积（ｍｕｌｔｉｐ
ｌｙ）
，拼接（ｃｏｎｃａｔｅｎａｔｉｏｎ）三种
，其中拼接


操作之后会接
一个线性层对齐维度
，结果如表
３
－６所示
，可以发现使用加和操作


向预测
ｔｏｋｅｎ中融入坐标编码的效果最佳
。


３７


北京邮电大学硕士学位论文


表
３
－６
不同方法融入ＣＥ的消融结果


ＲｅｆＣＯＣＯｇ


Ｍｏｄｅｌｓ


ｖａｌ
－ｕ
ｔｅｓｔ
－ｕ


ＬＧＲ
－ＮＥＴ（ｃｏｎｃａｔｅｎａｔｉｏｎ）７６
．７６７６．９０


ＬＧＲ
－ＮＥＴ（ｍｕＩｔｉｐ
ｌｙ）７６
．６３７６
．４９



ＬＧＲ
－ＮＥＴ
７６．８２
７＾０３


２）跨模态损失


ＬＧＲ
－ＮＥＴ中跨模态损失（Ｃｒｏｓｓ
－ｍｏｄａｌＬｏｓｓ
，ＣＬ）采用的是对比学习的形式
，


由于对比学习效果受训练过程中的批次大小（ｂａｔｃｈｓｉｚｅ
，ｂｓ）影响明显
，该部分对此


进行消融实验
。消融结果如表
３
－７所示
，
可以发现
，
当增大ｂａｔｃｈｓｉｚｅ时
，
ＣＬ带


来的提升更大
，
当ｂａｔｃｈｓｉｚｅ为
１６时
，
ＣＬ在两个划分下只带来
０
．１２％的平均提


升
，
当
ｂａｔｃｈｓｉｚｅ提升至
１２８时
，
ＣＬ带来的性能提升达到
１
．１９％
。此外
，
当ｂａｔｃｈ


ｓｉｚｅ从
１６增大到
１２８的过程中
，
ＬＧＲ
－ＮＥＴ的性能提升比ＬＧＲ
－ＮＥＴ（ｗ／ｏＣＬ）更


显著
，前者平均来看提高了１
．９８％
，而后者仅仅提高了０
．９％
。因此消融结果不仅


验证了跨模态损失的有效性
，
而且验证了ｂａｔｃｈｓｉｚｅ大小对损失效果的影响
。


表
３
－７ｂａｔｃｈｓｉｚｅ大小对跨模态损失的影响


ＲｅｆＣＯＣＯｇ


Ｍｏｄｅｌｓ


ｖａｌ
－ｕ（ｂｓ
＝
１６
，
３２
，６４
，
１２８
）ｔｅｓｔ
－ｕ（ｂｓ＝
１６
，３２
，６４
，
１２８）


ＬＧＲ
－ＮＥＴ
（ｗ／ｏＣＬ）７４
．３８７５
．９４７５
．２８７５
．７１７５
．２９７５３２７５
．５９７５
．７６


ＬＧＲ
－ＮＥＴ
７４
．５７７６
．４７７６
．４３７６．８２７５
．３３７５
．９８７６
．６５７７．０３


此外
，跨模态损失是基于句子向量和预测
ｔｏｋｅｎ进行计算的
，而坐标编码（ＣＥ）


有助于预测ｔ
ｏｋｅｎ的学习
，
因此ＣＥ和ＣＬ
一定程度上具备互补的效果
。关于ＣＥ


和ＣＬ的联合消融结果如表
３
－８所示
，
可以发现ＣＥ和ＣＬ在ＲｅｆＣＯＣＯｇ的
ｖａｌ
－


ｕ上各自仅带来了０
．３
１％和
０
．０７％的提升
，
然而当另
一方生效时
，
ＣＥ和ＣＬ分别


带来了１
．３５％和
１
．
１
１％的性能提升
。
这不仅证明了ＣＥ和ＣＬ各自的有效性
，
还


进
一步证明了两者具备互相增强的效果。


表
３
－８ＣＥ和ＣＬ的联合消融结果


ＣｏｏｒｄｉｎａｔｅＣｒｏｓｓ
－ｍｏｄａｌＬｏｓｓＲｅｆＣＯＣＯｇ


Ｅｍｂｅｄｄｉｎｇｖａｌ
－ｕｔｅｓｔ
－ｕ


＾ｘ７５
．４０７５
．６３


ｘ？７５
．４７７５
．９１


？＾７５
．７
１７５
．７６


？？
７６．８２
ＴＬ０３


３）跨模态推理


３８


第三章语言引导的指称表达理解研究


ＬＧＲ
－ＮＥＴ跨模态推理模块中的
ＴＣＡ和
ＴＣＦ模块组合起来可以视作
一个拥


有自定义输入的
，标准的ＴｒａｎｓｆｏｒｍｅｒＤｅｃｏｄｅｒ模块
。然而
，
正是由于这样的自定


义输入方式
，
本文将其分为
ＴＣＡ和
ＴＣＦ模块
，
即文本模态以
ＴＣＦ中的
ｃｒｏｓｓ
－


ａｔｅｎｔｉｏｎ的ｋｅｙ和ｖａｌｕｅ的形式输入跨模态推理模块中
，而图像模态输入ＴＣＡ中
。


这样在堆叠模块时
，
文本特征能反复输入
，起到引导跨模态推理的效果
，
同时这


个过程中经过
ＴＣＡ
中的自注意力机制能逐渐调整预测
ｔｏｋｅｎ与图像特征之间的


注意力分布
，
与指称表达在语义层面上实现对齐
，
逐渐注意到指称物体
。
为此
，


本模块通过交换输入的图文模态进行关于ＴＣＡ和ＴＣＦ的消融实验
，
即交换跨模


态推理模块输入的图像文本
，
称之为ＬＧＲ
－ＮＥＴ
（ｔｅｘｔ
－ａｔｎ
－
ｉｍｇ）
。
消融结果如表
３
－


９所示
，
可以发现
，
交换输入的图文模态后
，
模型在两个划分上的性能都明显下


降
，
说明了ＴＣＡ和ＴＣＦ的有效性
，
进
一步的可视化分析见
３
．３
．４小节
。


表
３
－９ＴＣＡ和ＴＣＦ的消融结果


ＲｅｆＣＯＣＯｇ


Ｍｏｄｅ
ｌｓ


ｖａｌ
－ｕ
ｔｅｓｔ
－ｕ


ＬＧＲ
－ＮＥＴ
（ｔｅｘｔ
－ａｔｔｎ
－
ｉｍｇ）７５
．０６７５
．２
１



ＬＧＲ
－ＮＥＴ
７６
．８２
７７
．０３


４
）跨模态推理模块堆叠层数


本小节对跨模态推理模块中
ＴＣＡ和ＴＣＦ堆叠层数进行消融
。消融结果如图


３
－４所示
，
可以发现从
１层到
６层的过程中
，
模型在两个数据划分上都得到
一致


且明显的提升
，
当堆叠至
８层时
，
一方面在ＲｅｆＣＯＣＯｇ
ｔｅｓｔ
－ｕ上提升不够明显
，


另
一方面在ＲｅｆＣＯＣＯｇｖａｌ
－ｕ上不升反降
，
考虑到堆叠层数的同时会提高模型的


复杂度
，
因此ＬＧＲ
－ＮＥＴ最终选择堆叠
６层ＴＣＡ和
ＴＣＦ组件
。


７８
．０

｜

；


７７
．５


７７
．０
「


７６
．５


Ｉ
７６
＿〇
ｄ７５
．５Ｔ


＾Ｔ５
．０
－／


７４
．５
－／


ＲｅｆＣＯＣＯｇｖａｌ
－ｕ


ＲｅｆＣＯＣＯａｔｅｓｔ
－ｕ


７３０



＇
０
１
２
３
４
５
６
７
８
９


ＮｕｍｂｅｒｏｆＴＣＡａｎｄＴＣＦｌａｙｅｒｓ


图
３
－４ＴＣＡ和ＴＣＦ堆叠层数的消融结果


５
）跨模态损失权重又


３９


北京邮电大学硕士学位论文


ＬＧＲ
－ＮＥＴ的损失不仅包含之前方法常用的框回归损失
，
同时还有跨模态损


失
，
如公式
３
－８所示
，
后者通过权重因子Ａ进行平衡
。
本小节对权重因子进行消


融
，实验结果如图
３
－５所示
。可以发现
，
当权重２＝２时模型性能达到最佳
。
一方


面
，
当Ａ过小时
，
跨模态损失权重太小
，
对齐约束不足
。
另
一方面
，
当Ａ过大时
，


跨模态损失权重过大
，
导致模型对预测框的监督约束不充分
。两者都会降低模型


的收敛性能
。
消融结果说明了合理的权重因子可以促进跨模态损失的效果
。


７７
．４
，

＾
￣￣￣
；


７７
．１


＾
７６
．５＾


＾
７６
－２、


＜７５
．９
＾
／０


７５
．６
＾
７５３
Ｑ
－ＲｅｆＣＯＣＯｇｖａｌ
－ｕ


—Ａ
—
ＲｅｆＣＯＣＯｇ
ｔｅｓｔ
－ｕ


７５ｎ



＾
００
．５
１２
３
４
５
６
７
８
９１０


ＷｅｉｇｈｔｉｎｇｆａｃｔｏｒＡ


图
３
－５跨模态损失权重因子的消融结果


３
．３
．４可视化分析


为进
一步揭示ＬＧＲ
－ＮＥＴ关键组件对整体起的作用
，本小节针对跨模态推理


过程中的语言引导
、
坐标编码对文本中位置信息的捕捉
、
文本淹没问题
、
以及


ＴＣＡ和ＴＣＦ的推理过程进行了可视化分析
，
验证了它们的效果
。


１
）语言引导


为了揭示
ＬＧＲ
－ＮＥＴ
中文本特征扩展模块（ＴＦＥ
）输出的三类文本特征对跨模


态推理过程的引导效果
，
本小节进行了可视化分析
。
如图
３
－６所示
，
其中第
一列


可视化了生成的二维坐标在图片中的位置（以蓝点标记）
，
可以发现生成的二维坐


标准确命中了目标物体
，
以第二个样例为例
，
生成的
２维坐标为（０
．７７
，０
．７７）
，
可


以发现它正确命中了５点钟方向的橘子
，以此生成的坐标编码可以增强预测
ｔｏｋｅｎ


的空间表征
，
促进其捕获关键的视觉特征
。


第
２
－４列来自跨模态推理模块中
ＴＣＡ和ＴＣＦ的注意力可视化
。
在每个例子


中
，
３张相邻的二维注意力图分别对应第
２
，
４
，
６层预测
ｔｏｋｅｎ对视觉
ｔｏｋｅｎｓ的


注意力权重
，底下的
一维注意力图对应第
６层预测
ｔｏｋｅｎ对文本
ｔｏｋｅｎｓ的注意力


权重
。
可以发现
，
随着堆叠层数的增加
，
预测ｔ
ｏｋｅｎ逐步在关键文本线索的引导


下关注到目标物体相关的视觉特征
。
需要注意的是
，
在可视化预测
ｔｏｋｅｎ与文本


ｔｏｋｅｎｓ的权重时
，
为便于展示
，
对类似
“
ｏ
’
ｃｌｏｃｋ
” 这样会被分词器切分的单词进


行了聚合
，
使用它们的平均注意力作为合并后的注意力分数
。
以例子２为例
，预


４０


第三章
语言引导的指称表达理解研究


测
ｔｏｋｅｎ注意到关键的空间线索
“
５ｏ
’
ｃｌｏｃｋｐｏｓｉｔｉｏｎ
’
’
，
并逐渐将注意力集中到相


应位置的图片内容上
。


ＰＭＭＩ蘇，


ａｚｅｂｒａｉｎｆｒｏｎｔｏｆ
ｔｗｏｏｔｈｅｒｚｅｂｒａｓ


ｔｈｅｏｒａｎｇｅｉｎｔｈｅ５ｏ
＇
ｃ
ｌｏｃｋ
ｐｏｓｉｔｉｏｎ


图
３
－６ＬＧＲ
－ＮＥＴ
中语言引导的可视化


第
５
－６列展示了最后
一层的预测
ｔｏｋｅｎ与视觉
ｔｏｋｅｎｓ的
８个注意力头分别可


视化的结果
，其中第
５列来自其中
一个注意力头
，第
６列来自另外
４个注意力头


可视化的叠加
，
其他注意力头的结果类似
。
可以发现第五列
，
预测
ｔｏｋｅｎ关注目


标物体本身的
一些特征
，
第六列预测
ｔｏｋｅｎ关注目标物体
一些边缘特征
，
如第
一


个例子中斑马的脚
，
背等边缘信息
。
由此可以得知
，
预测
ｔｏｋｅｎ中捕获的关于目


标物体的视觉特征有两方面
，
一方面是物体本身与其他物体区分开的特征
，
另
一


方面是物体的边缘特征
，
用于定位
。后者可以通过损失函数中的框回归损失项进


行约束
，而前者可以由ＬＧＲ
－ＮＥＴ的跨模态对齐损失加强监督效果
，也说明了ＴＦＥ


生成的用于跨模态对齐损失计算的句子向量的有效性
。


２）坐标编码与指称表达


为进
一步说明坐标编码（ＣＥ）对指称表达中有关指称物体空间位置信息的捕


获
，
本小节可视化了ＣＥ在指称表达上的注意力权重
。
如图
３
．７所示
，
每个例子


展示了预测框（红色）和标注框（蓝色
坐标编码对文本的注意力权重
，
类似前文


所述
，
针对被分词器切分的单词做了聚合
；
以及二维坐标及其可视化（图片中蓝


色的点）
。
可以发现坐标编码确实可以捕捉指称表达中关于目标物体空间位置的


描述
，
如第
一个例子中的
“
ｍｉｄｄｌｅ
” 和第三个例子中的
“
ｂｏｔｏｍｒ
ｉｇｈｔ
ｃｏｍｅｒ
”
。
总


体来看
，
坐标编码可以促进预测
ｔｏｋｅｎ注意到文本中关于指称物体必要的空间位


置描述
，
并进
一步促进预测
ｔｏｋｅｎ捕获对应的图像特征
。


４
１


北京邮电大学硕士学位论文




ｉｙｖ
写


＂


隱


■麵■
— 
【涵
■！


ｔｈｅ
ｙｏｕｎｇｅｓｔ
ｇ
ｉｒａｆｆｅｏｕ
ｔｏｆ
ｔｈｒｅｅｉｎｔｈｅｍ
ｉｄｄｌｅｔｈｅｒ
ｉｇｈ
ｔｓｈｏｅｉｓｏｎｔｈｅｒｉｇｈｔｏｆ
ｔｈｅｂａｎａｎａ


（０３３
１５
，
０
．４８
１２）
（０
．８
１３７
．
０
．５
１６９）


■


ａｓｋ＼ｃｈｅｆｓｔｒｕｃｋｉｓｓｅｅｎｉｎｔｈｅｂｏｔｔｏｍｃｌｅｐｈａｏｔｏｏｌｅｆｔｓ
ｉｄｅ


＇
ｒｉｇ
，ｚ＾；；；？ｒ
ｅｃｎ
（鼠
。纖


图
３
－７
坐标编码对指称表达的可视化


３
）文本信息淹没


为进
一步说明
３
．
１节中提及的文本信息淹没问题
，
本小节分别对
ＬＧＲ
－ＮＥＴ


和ＱＲＮｅｔ中预测
ｔｏｋｅｎ与文本
ｔｏｋｅｎｓ的注意力可视化结果进行对比
。其中ＱＲＮｅｔ


采用拼接图文特征和自注意力机制进行图文特征融合和跨模态推理
，
如图
３
－
１ａ）


所示
。
具体来说
，
在
ＱＲＮｅｔ中进行跨模态融合推理时
，
先拼接图文特征序列得


到
［／ｐ
；怂
；￣］
，
分别为预测
ｔ〇ｋｅｎ／ｐ
，
文本特征序列圪
，
图像特征序列＆
，
其中文


本特征序列长度为
４０
，
远远小于图像特征序列的长度Ｈ／３２ｘＭ／
／３２＋Ｈ／６４ｘ


Ｗ／６４＝５００
，Ｈ＝Ｍ／＝６４０
。在此基础上通过自注意力机制进行融合
，
由于拼接


后整体特征序列长度很大
，
在这个过程中预测
ｔｏｋｅｎ对整体序列的注意力分数ｅ


Ｅ
１Ｘ
（１＋４Ｑ＋ＳＷ将被显著分散
，
导致注意力分数的值很小
。如图
３
－８所示
，
同样
的


例子中
，
ＱＲＮｅｔ里预测
ｔｏｋｅｎ对文本
ｔｏｋｅｎｓ的注意力分数远远小于ＬＧＲ
－ＮＥＴ中


的注意力分数
。如此
，
由于注意力分数被
“ 稀释
”
，预测
ｔｏｋｅｎ捕获的文本信息不


足
，
导致淹没问题
。在ＬＧＲ
－ＮＥＴ中
，
由于输入模态的分离
，
捕获的语言特征更


充分
，
更能发挥语言的引导作用
。


４）ＴＣＡ和ＴＣＦ的推理可视化


为进
一步揭示ＴＣＡ和
ＴＣＦ划分的原因和效果
，
本小节对
３
．３
．３节消融实验


３）做了进
一步的可视化分析
。
如图
３
－９所示
，
对ＬＧＲ
－ＮＥＴ跨模态推理模块的图


文模态输入进行交换
，
得到ＬＧＲ
－ＮＥＴ（ｔｅｘｔ
－ａｔｔｎ
－
ｉｍａｇｅ
）
。
具体来说
，
在ＬＧＲ
－ＮＥＴ


中对图片的注意力权重来自于自注意力层
，
而在
ＬＧＲ
－ＮＥＴ
（
ｔｅｘｔ
－ａｔｔｎ
－
ｉｍａｇｅ
）中
，


注意力权重来自于跨注意力层
。


４２


第三章
语言引导的指称表达理解研究


「
－
ＨｎＨｉｉ
…
－
；


ｍｍｉｍ


｜Ａ
－
、
０
－０Ｓ７
．
０
０５３
．０
．０５５
．０
０５８
．０
．０６３
．０
．０３９
．
０
．０５９


Ｑｕｅｒ＞
－
：ａ
ｇｒａｖｖｉｎａｂｏｗ
ｌｎｅａｒ
ｐｏｐｃｏｒｎ


…
—
！


０
．０５０
．
００５
］
，
０
．０５２
，０
．０４４
，０
．０３３
．０
．０７２
．０
０７８







一


图
３
－８ＱＲＮｅｔ和ＬＧＲ
－ＮＥＴ的文本可视化对比


通过对交换前后的模型进行可视化对比分析可以发现
，
交换后的模型
，
即


ＬＧＲ
－ＮＥＴ
（
ｔｅｘｔ
－ａｔｎ
－
ｉｍａｇｅ）在推理层数增加的过程中预测
ｔｏｋｅｎ难以调整对图片的


注意力分布
，
因此如果
一开始就关注错误的图像区域将无法调整
，从而降低了模


型的推理准确率
。
而在
ＬＧＲ
－ＮＥＴ
中
，
利用反复融入的文本特征作为引导信号
，


逐步调整预测
ｔｏｋｅｎ对图像
ｔｏｋｅｎｓ的注意力分布可以实现图像特征与文本线索的


有效对齐
。
因此
，
ＬＧＲ
－ＮＥＴ设计的跨模态推理模块中
ＴＣＡ和
ＴＣＦ的安排是合


理而有效的
。
ＨＰ


ｒ
—


肀
．
．
ＭｇｎｇＭ
．ａｇａｇＭ— ｉ—— ａ—
ａ
ｊ？
＞
ｉ
．ｒ
１：１


ｒ
ｉ
－
ｉ
一一―
＝ｒ＝＝ｒ


＾■■■＿■■


！
＞？
．

．
＜


ＬＧＲ＾ＥＴ


（
ｉｔ
ｒｘ
ｉ
－ａｎｎ
－
ｉｍｇ
）


ｆ
ｌａｙｅｒ
２
ｌａｙｅｒ
４
ｌａｙｅｒ６
ｌａｙｅｒ
２
ｌａｙｅｒ
４
ｌａｙｅｒ６


ｉ亭
１


嘩
舊


＾ｗｂｗｗｂ


ＭｅＬ，，
Ｉ


Ｉ
．ＧＲＮｆ．Ｔ


图３
－９ＬＧＲ
－ＮＥＴ和ＬＧＲ
－ＮＥＴ（ｔｅｘｔ
－ａｔｎ
－
ｉｍａｇｅ
）的可视化对比


３
．３
．５定性分析


本节通过对比ＬＧＲ
－ＮＥＴ和ＴｒａｎｓＶＧ
、
ＱＲＮｅｔ在不同例子中的预测结果来定


性分析
ＬＧＲ
－ＮＥＴ的特点
。
如图
３
－
１０所示
，
每个例子中有四个预测框
，
分别是


４３


北京邮电大学硕士学位论文


ＬＧＲ
－ＮＥＴ（红色）
、
ＴｒａｎｓＶＧ（白色）
、
ＱＲＮｅｔ（黄色）的预测结果以及
ｇｒｏｕｎｄｔｒｕｔｈ标


注框（蓝色）
。第
一行的四个结果只有ＬＧＲ
－ＮＥＴ预测正确
，其中第
一个例子ＬＧＲ
－


ＮＥＴ捕捉到了指称表达中完整的关键描述
“
ｆ
ｉｉｒｔｈｅｒｅｓｔａｗａｙｆ
ｒｏｍｗａｒｄｒｏｂｅ
” 并正


确定位了猫
，而不是仅通过部分描述
“
ｆ
ｉｉｒ
ｔｈｅｒｅｓｔ
”就定位距离镜头远
，而距离橱柜


近的那只干扰的猫
。说明ＬＧＲ
－ＮＥＴ对指称表达准确的理解能力
。第二行第
一个


例子只有使用ＲｅｓＮｅｔ为视觉
ｂａｃｋｂｏｎｅ的ＴｒａｎｓＶＧ预测正确
，说明面对图片中的


某些特殊场景如镜像等
，不同视觉提取器的适应能力可能存在
一定的差异
。对于


第二行后三个例子
，所有模型都预测错误
，说明在面对包含逻辑场景或者昏暗区


域的图片时
，
当前方法还有
一定的提升空间
。


ｉ


ａ
）
ｔｈｅｂｌａｃｋｃａｒ
ｆｕｒｔｈｅｒｅｓｔｂ）
ｔｈｅｏｒ
ａｎｇｅ
ｉｎｔｈｅ
５ｏ
ｃ
ｌｏｃｋｃ）
ｌｏｎｇｂ
ｌｏｎｄｈａ
ｉｒ
ｗｅａｒ
ｉｎｇａｌ
ｌｄ
）ｔｈｅｒ
ｉｇｈｔ
ｃｏｍｐｕｔｅｒ

ｉｎｔｈｅｒ
ｉｇｈ
ｔ


ａｗａｙｆｒｏｍｉｈｅｗａｒｄｒｏｂｅｐｏｓ
ｉｔ
ｉｏｎ
ｂｌａｃｋａｎｄａｋｎｅｅｂｒａｃｅｏｎｈａｎｄ
ｐ
ｉｃｔｕｒｅ


ｌｅｆ
ｔｋｎｅｅ


識圍國國


Ｃ
）
３
ｒ
，ｎｋ
ｐ＿．？
ｐｈ〇ｎｅ＝＝＝：


ｙｅ
ｌ
ｌｏｗ
ｐ
ｌａｔｅｗｒ
ｔｈｏｔｈｅｒ
ｐａｓｔｒ
．ｅｓ
ｉｈａ
ｉ
ｉｓｆａｃｉｎｇ

ｉｈｃ
ｌｅｆ
ｔ
ｉｂ
ｌ
ｌｏｗｅｄｂｙｍｏｒｅｂａｎａｎａ，


图
３
－
１０ＬＧＲ
－ＮＥ丁
、
ＴｒａｎｓＶＧ
、
ＱＲＮｅｔ的预测结果对比样例


３
．４本章小结


本章开展了基于语言引导的指称表达理解研究工作并提出了语言引导的推


理网络（ＬＧＲ
－ＮＥＴ
）
。首先
，本章从解决先前方法出现的文本信息淹没问题和跨模


态推理过程中语言特征利用不足的问题出发
，给出了ＬＧＲ
－ＮＥＴ模型的设计动机


和解决思路
。接下来
，
本章详细介绍了ＬＧＲ
－ＮＥＴ各个组件的设计细节
，
该模型


核心在于扩展语言特征并充分利用其进行跨模态推理的引导
。包括建模指称表达


中存在的关于目标物体在图片中的空间位置信息
；结合跨注意机制避免文本信息


的淹没
；
优化训练模型的损失函数
，
增强图文对齐约束
。
最后在实验验证部分
，


本章通过全面的定量性能对比
、详尽的组件有效性消融分析
、关键组件的可视化


展示
、
定性分析全方位地验证了所提方法的可行性和有效性
。


４４


第四章
基于场景知识的指称表达理解研宄


第四章基于场景知识的指称表达理解研究


４
．
１本章引论


基于场景知识的指称表达理解任务（ＳｃｅｎｅＫｎｏｗｌｅｄｇｅ
－ＲＥＣ
，
ＳＫ
－ＲＥＣ）是传统


指称表达理解的
一个变种
。其数据标注要求仅仅根据指称表达无法定位图片中的


目标物体
，
因为指称表达中不会直接包含关于目标物体的外观
，
属性
，方位等描


述信息
，而是会结合跟其他物体的关系进行指代
。
比如在传统的指称表达理解任


务中关于目标物体的描述可能是
“
Ｔｈｅｒｅｄｈａｔｗｏｒｎｂｙ
ｔｈｅ
ｐｅｒｓｏｎｏｎｔｈｅｌｅｆ
ｔｍｏｓｔ
”
，


而在
ＳＫ
－ＲＥＣ中的指称表达可能是
“
Ｂｏｂ
’
ｓｈａｔ
”
，
而如何确定
“ Ｂｏｂ
” 在图片中的


位置则需要结合场景知识（ＳｃｅｎｅＫｎｏｗｌｅｄｇｅ）进行联合推理
。
如图
４
－
１所示
，
传统


ＲＥＣ任务的指称表达为图中绿色部分
，
由于其本身对目标物体描述是准确而无


歧义的
，
因此仅仅依靠它就可以实现对目标物体的定位
。而在
ＳＫ
－ＲＥＣ中
，指称


表达如红色文本所示
，
图片中通常会存在多个相同类型的目标物体
，仅依靠指称


表达无法确定具体的实例
，从而需要结合给定的场景知识进行联合推理进行准确


的定位
。


：
Ｉｍａｇｅ
【
＞
（
－
■
：
－
－
）＜
１
？
：


ｒ
■？

—


：
Ｉｎａｃｌｏｔｈ
ｉｎｇｓｔｏｒｅ
，ｔｈｅｗｏｍａｎＴｏｍ
；
：
＇，
．
，
＼
：


：
ｗｅａｒｉｎｇａｂ
ｌｕｅｖｅｓｔｏｎｔｈｅｒ
ｉｇｈｔｓ
ｉｄｅｏｆ
ｔｈｅ
：
：
｜
＾
＇ｅａｃ
’
ｇａｓｓ（ｒａｉｔ
ｉｏｎａ）
ｉ

！ｉｍａｇｅａｎｄｈｅｒｙｏｕｎｇｅｒｓｉｓｔｅｒＭａｙｗｉｔｈ
；
！？
？


ｉｂ
ｌａｃｋｇ
ｌａｓｓｅｓｉｎｆｒｏｎｔｏｆｈｅｒａｒｅｃｈｏｏｓｉｎｇ
：
！：
．Ｑ
０
２
：Ｔｈ＂
＾＾
！


ｉｔｈｅｆａｂｒｉｃｏｆｔｈｅｉｒｃ
ｌｏｔｈｅｓ
．ＢｏｔｈＭａｙａｎｄ
丨
（
ｃｅｎｅｎｏｗｅｇｅ
－）


：
Ｔｏｍａｒｅｔｈ
ｉｎｋ
ｉｎｇｔｈａｔｔｈｅｆａｂｒｉｃｉｎｔｈｅｉｒ
；
：


；
ｈａｎｄｓｉｓｂｅａｕｔｉｆｕ
ｌ
．
ｉ
：


？
ＳｃｅｎｅＫｎｏｗ
ｌｅｄｇｅＱｕｅｒｙ


图
４
－
１ＳＫ
－ＲＥＣ和传统ＲＥＣ任务形式对比


事实上
，
在实际场景中往往会存在此类情况
。在某些场景下
，
若之前己经存


在
一定的场景描述上下文
，此时再通过指称表达指定目标物体时往往会简化对目


４５


北京邮电大学硕士学位论文


标物体的指称描述
。然而由于上下文通常比较长
，并且涉及多个目标物体的描述
，


在此类冗长复杂的数据上进行准确的图文跨模态推理给传统的指称表达理解模


型带来了
一定的挑战
。
具体来说
，
对
ＳＫ
－ＶＧ数据集中场景知识的长度统计如图


４
－２所示
，
可以发现
，
大部分场景知识包含
５０
－７０个单词
，
最多甚至有
１００个单


词以上
，
作为对比
，
传统ＲＥＣ任务的基准数据集中
，
ＲｅｆＣＯＣＯｇ拥有相对长而


复杂的指称表达
，平均也只有
８
．４３个单词
。因此
ＳＫ
－ＶＧ数据集相比传统ＲＥＣ的


数据集要复杂得多
。


２５００
－


２０００
－


Ｉ
１５００
？


ＫＫＫ）
－


５００
－


〇
ＪＶ聲４￣＾
＾
＾


２０
－３０３０
－４０４０
－５０５０
－６０６０
－７０７０
－８０８０
－９０９０
－
１００１００
－
１
１０１
１０
－
１２０


Ｋｎｏｗ
ｌｅｄｇｅｌｅｎｇｔｈ（ｗｏｒｄｓ）


图
４
－２ＳＫ
－ＶＧ数据集场景知识长度统计


之前的研究通常利用简单的模板对指称表达和场景知识进行拼接得到联合


的文本直接输入传统的指称表达理解模型执行跨模态推理
，
如
“
Ｑｕｅｒｙ
：Ｒｅｆｅｒｒｉｎｇ


Ｅｘｐｒｅｓｓｉｏｎ
．Ｋｎｏｗｌｅｄｇｅ
：ＳｃｅｎｅＫｎｏｗｌｅｄｇｅ
”
。
这种做法的问题在于场景知识相比于


指称表达的文本要复杂得多
，
一方面场景知识通常会包含图片中多个物体的详细


描述
，还包括对这些物体之间关系的描述
，
另
一方面场景知识的文本序列长度要


远远长于传统的指称表达文本
。其中关于其他物体的描述会对目标物体的推理和


定位造成干扰
，
并且传统的ＲＥＣ框架可能并不适用于处理如此长的复杂文本
。


因此
，
现有的直接利用传统ＲＥＣ的模型架构处理
ＳＫ
－ＲＥＣ任务无法取得良好的


效果
。


本章从数据和模型两个角度解决
ＳＫ
－ＲＥＣ任务中的推理干扰问题
。
一方面
，


当前大语言模型的意图理解能力和指令跟随能力己经实现了巨大的飞跃
，在此基


础上设计指令模板并结合上下文学习实现对
ＳＫ
＿ＲＥＣ
中复杂场景知识的简化
，


过滤场景知识中跟目标物体无关的描述
，降低跨模态推理过程中的千扰
。另
一方


面
，
构建面向场景知识的推理网络（ＳｃｅｎｅＫｎｏｗｌｅｄｇｅｂａｓｅｄＲｅａｓｏｎｉｎｇＮｅｔｗｏｒｋ
，


ＳＫＲＮ）通过分离指称表达和场景知识
，并分别对两者进行特征提取
，结合注意力


机制降低推理过程中场景知识中无关描述的干扰
。


４６


第四章
基于场景知识的指称表达理解研宄


本章节的后续内容安排如下
，首先介绍如何设计指令模板利用大模型简化场


景知识数据
，进
一步详细介绍
ＳＫＲＮ的设计动机和具体细节
，最终通过实验验证


所提方法的有效性
。


４
．２复杂场景知识的处理方法


本小节从数据角度解决场景知识过于复杂的问题
。最近
一年来自然语言处理


领域最大的突破莫过于大语言模型
，其强大的通用语言理解和指令跟随能力极大


提高了其可用性
。
利用大语言模型进行数据标注
逐渐成为人工标注合理的替


代方案
，
使用大模型对图像
、文本等数据进行初步标注
，然后通过人工进行校正


和完善
，
这样可以大幅提高数据标注的效率和准确性
。


４
．２
．
１基于大语言模型的数据生成方法


如图
４
－３所示
，
通过调用ＯｐｅｎＡＩ提供的ＣｈａｔＧＰＴＡＰＩ对原始数据集中的场


景知识进行简化
。具体而言
，综合数据规模
、指令长度
、以及ＡＰＩ价格
，选取ｇｐｔ
－


３
．５
－ｔｕｒｂｏ
－０
１２５接口进行数据简化的标注
。
设计的指令模板主要分为两部分
，
第


一部分为任务说明
，第二部分为若干人工标注的样例以提供上下文学习
［９９
］
。图
４
－


３仅展示了第
一部分
，
具体来说
，
要求模型根据指称查询中的指称物体从场景知


识中提取所有的描述
，
同时过滤原始场景知识中无关的描述
，
以此得到浓缩的场


景知识
，
降低来自其他物体描述的干扰
。第二部分选取若千人工标注的样例如表


４
－
１所示
，
每个标注样例包括原始的场景知识
、
指称查询
、
以及人工标注的推理


过程和最终的简化版场景知识
。


ＩＩＩＩＺ
—
？
简化的场景知识


指称查询
—
１
＾
１

Ｉ


简化指令




．
＿
＿


Ｈｅｒｅｉｓａｓｃｅｎｅｋｎｏｗｌｅｄｇｅａｎｄａｑｕｅｎ＾Ｈｅ
ｌｐｍｅｅｘｔｒａｃｔａｌ
ｌｉｎｆｏｒｍａｔｉｏｎ


ａｂｏｕｔｔｈｅｒｅｆｅｒｅｎｔｏｂｊｅｃｔｉｎｔｈｅｑｕｅｒｙｆｒｏｍｔｈｅｓｃｅｎｅｋｎｏｗ
ｌｅｄｇｅ
，ａｎｄ


ｆｉ
ｌｔｅｒｏｕｔｕｎｎｅｃｅｓｓａｒｙｄｅｓｃｒｉｐ
ｔ
ｉｏｎｓ
．


Ｔｈｅｒｅａｒｅｓｏｍｅｅｘａｍｐ
ｌｅｓａｎｄｒｅｆｅｒｅｎｃｅａｎｓｗｅｒｓ
：


图
４
－３
利用大语言模型简化场景知识


在标注推理过程时
，本文使用了设计大模型指令提示常用的思维链
［
１Ｍ］技巧
，


“
Ｌｅｔ
’
ｓ
ｔｈｉｎｋ
ｓｔｅｐｂｙ
ｓｔｅｐ
”［
ｉｍ
］
，
旨在让模型学习如何将
一个相对复杂的推理任务


拆解成多个小任务
，并通过逐步解决这些小任务来完成整体的任务
。值得注意的


是
，在例子
２中
，根据原始场景知识可知
，指称表达中的目标物体
“
ｍｅｄｉｃａｌ
ｒｅｃｏｒｄ
”


４７


北京邮电大学硕士学位论文


相对独立
，没有对其他物体有依赖
，
因此直接输出原始的指称表达即可
。通过对


不同类型的简化方案进行少量的人工标注
，并将这些标注案例拼接至图４
－３所示


的指令模板后面
，让大模型对任务本身理解更加准确
，提高场景知识的生成质量
。


表４
－
１
简化场景知识的人工标注样例展示


＂
样例序号
｜人工标注样例


Ｏｒ
ｉｇ
ｉｎａｌＳｃｅｎｅＫｎｏｗｌｅｄｇｅ
：Ｂｅｒｔｒａｍｉｓｔｈｅｍａｎｉｎｆｌｏｒａｌｃｌｏｔｈｅｓ


ｈｏｌｄｉｎｇａｂｏｔｌｅｏｆ
ｂｅｅｒｉｎｅａｃｈｈａｎｄ．Ｈｅｈａｓａ
ｐ
ｉｃｎｉｃｗｉｔｈｈｉｓｆ
ｒｉｅｎｄｓ


ｔｏｄａｙ
．ＴｈｅｃｈｅｆＢｅｖｉｓｉｓｓｔａｎｄｉｎｇｔｏｔｈｅｒ
ｉｇｊｉｔｏｆＢｅｒｔｒａｍ
，ｗｅａｒｉｎｇａ


ｗｈｉｔｅａｐｒｏｎ．Ｂｅｖｉｓｋ
ｇ
ｉｒｌｆ
ｒｉｅｎｄＣｅｌｅｓｔｅｗｅａｒｓａｅｏｌｏｒｆ
ｉｘｌｂｒａｃｅｌｅｔｏｎｈｅｒ


ｗｒｉｓｔａｎｄｉｓｃｈａｔｔｉｎｇｗｉｔｈＴｉｎａ
，ｗｈｏｗｅａｒｓａ
ｐ
ｉｎｋｄｒｅｓｓｏｎＢｅｒｔｒａｍ
＇
ｓ


ｌｅｆ
ｔ
．


Ｑｕｅｉｙ
：Ｔｈｅｗｏｍａｎｃｈａｔｔｉｎｇｗｉｔ
ｉｉＴｉｎａ．


１
Ｒｅａｓｏｎｉｎｇｐｒｏｃｅｓｓ
：


Ｌｅｔ
’
ｓｔｈｉｎｋｓｔｅｐｂｙｓｔｅｐ
．


Ｑｌ
．ＷｈｏｉｓＴｉｎａ？


Ａ１
．Ｔｉｎａ，ｗｈｏｗｅａｒｓａ
ｐ
ｉｎｋｄｒｅｓｓ
．


Ｑ２
．ＷｈｏｉｓｃｈａｔｉｎｇｗｉｔｈＴｉｎａ？


Ａ２
．Ｃｅｌｅｓｔｅｗｅａｒｓａｃｏｌｏｒｆｕｌｂｒａｃｅｌｅｔｏｎｈｅｒｗｒｉｓｔａｎｄｉｓｃｈａｔｔｉｎｇｗｉｔｈ


Ｔｉｎａ．


ＣｏｎｄｅｎｓｅｄＳｃｅｎｅＫｎｏｗｌｅｄｇｅ
：


ＧｅｌｅｓｔｅｗｅａｒｓａｃｏｌｏｒｆＵｌｂｒａｃｅｌｅｔｏｎｈｅｒｗｒ
ｉｓｔａｎｄｉｓｃｈａｔｉｎｇｗｉｔｈＴｉｎａ
．


Ｏｒｉｇ
ｉｎａｌＳｃｅｎｅＫｎｏｗｌｅｄｇｅ
：Ｔｏｍｉｓｓｉｃｋｉｎｔｈｅｈｏｓｐ
ｉｔａｌ
．Ｈｅｉｓｌｙ
ｉｎｇ
ｉｎ


ｔｈｅｈｏｓｐ
ｉｔａｌｂｅｄａｎｄｌｏｏｋｓａｌｉｔｔｌｅｗｅａｋ．Ｔｈｅｍｅｄｉｃａｌｒｅｃｏｒｄｈａｎｇ
ｉｎｇ


ａｔｔｈｅｅｎｄｏｆｔｈｅｂｅｄｒｅｃｏｒｄｓｔｈｅｄｅｔａｉｌｓｏｆＴｏｍ
＇
ｓｃｏｎｄｉｔｉｏｎ
．Ｈｉｓｗｉｆｅ


Ａｎａｓｉｔｓｏｎｔｈｅｓｔｏｏｌｏｎｔｈｅｌｅｆｔ
，ｈｏｌｄｉｎｇＴｏｍ
＇
ｓｈａｎｄａｎｄｌｏｏｋｉｎｇａｔ


ｈｉｍｗｏｒｒ
ｉｅｄｌｙ
．


Ｑｕｅｒｙ
：Ｔｈｅｍｅｄｉｃａｌｒｅｃｏｒｄｈａｎｇｉｎｇａｔｔｈｅｅｎｄｏｆｔｈｅｂｅｄ．


２


Ｒｅａｓｏｎｉｎｇｐｒｏｃｅｓｓ
：


Ｌｅｔ
’
ｓｔｈｉｎｋｓｔｅｐｂｙｓｔｅｐ
．


１
．Ｔｈｅｒｅｆｅｒｅｎｔｏｂｊｅｃｔ
＂ｍｅｄｉｃａｌｒｅｃｏｒｄ
＂
ｉｎｔｈｅ
ｑｕｅｒｙｒｅｌｉｅｓｏｎｎｏｂｏｄｙ
．


２
．Ｉｔ
＇
ｓｅｎｏｕｇｈｔｏｌｏｃａｔｅｔｈｅｔａｒｇｅｔｂａｓｅｄｓｏｌｅｌｙｏｎｔｈｅ
ｑｕｅｒｙ
．


ＣｏｎｄｅｎｓｅｄＳｃｅｎｅＫｎｏｗｌｅｄｇｅ
：


Ｔｈｅｍｅｄｉｃａｌｒｅｃｏｒｄｈａｎｇ
ｉｎｇａｔｔｈｅｅｎｄｏｆｔｈｅｂｅｄ．


４８


第四章
基于场景知识的指称表达理解研究


４
．２
．２生成数据的评估方案


为评估生成数据的质量
，本小节提出利用ＧＰＴ４
［
１Ｇ２
］对上述生成的数据进行评


估并结合人工核验保证最终的数据质量
。
具体方案是通过设计类似
４
．２
．
１小节中


的指令模板让
一个能力更强的大模型结合原始场景知识和指称查询对生成的简


化版场景知识进行评分
。
具体如图
４
－４所示
，
本小节使用更强的大语言模型对


４
．２
．
１小节生成的简化场景知识进行评估
，
使用的ＡＰＩ为ｇｐ
ｔ
－４
－０
１２５
－
ｐｒｅｖｉｅｗ
，
其


背后是ＯｐｅｎＡＩ最新的ＧＰＴ４模型
。


场景知识


指称查询

— 一￣
＾Ｍ


个


「简花涵景￥设
—


评估指令


Ｇｉｖｅｎａｎｏｒｉｇ
ｉｎａｌｓｃｅｎｅｋｎｏｗ
ｌｅｄｇｅ
，ａｑｕｅｒｙ
，ａｎｄａｃｏｎｄｅｎｓｅｄｓｃｅｎｅ


ｋｎｏｗｌｅｄｇｅ
．Ｊｕｄｇｅｔｈｅｓｃｏｒｅｏｆｔｈｅｃｏｎｄｅｎｓｅｄｋｎｏｗ
ｌｅｄｇｅａｎｄｇ
ｉｖｅｔｈｅ


ｒｅａｓｏｎ
．


Ｗｈｅｎｔｈｅｃｏｎｄｅｎｓｅｄｓｃｅｎｅｋｎｏｗｌｅｄｇｅｄｏｅｓｎ
＇
ｔｅｘｔｒａｃｔａｎｙｄｅｓｃｒ
ｉｐ
ｔ
ｉｏｎ


ｆｒｏｍｏｒｉｇ
ｉｎａｌｓｃｅｎｅｋｎｏｗｌｅｄｇｅａｂｏｕ
ｔｔｈｅｔａｒｇｅｔｏｂ
ｊｅｃｔｉｎｑｕｅｒｙ
，ｏｕｔｐｕｔ１
．


图
４
－４
利用ＧＰＴ４对生成的场景知识数据进行打分


评分说明如表４
－２所示
，
对生成的场景知识质量分数从低到高分为
４级
，
主


要依据生成的简化场景知识中与目标物体相关的描述以及其他无关描述的包含


情况
。
完全不包含和包含部分相关描述分别对应
１
，
２分
，
在完全包含的情况下


考虑是否还有关于其他物体的无关描述
，
若是
，
则评分为
３分
，
否则为
４分
。总


结来说
，将原始场景知识中的信息划分成与指称查询中目标物体相关或者不相关


两部分
，根据简化的场景知识中对这两部分信息的包含情况确定生成数据的质量
。


当充分包含相关描述的前提下完全过滤无关物体的描述时
，对应的简化版场景知


识是所需的高质量场景数据
，
可以有效降低模型的推理难度
，
提高模型性能
。


４９


北京邮电大学硕士学位论文


表
４
－２对生成数据的评估分类和说明


评分
＼ｍ


简化的场景知识完全不包含原始场景知识中对目标物体的描述
。


１（Ｔｈｅｃｏｎｄｅｎｓｅｄｓｃｅｎｅｋｎｏｗｌｅｄｇｅｄｏｅｓｎ
＇
ｔｅｘｔｒａｃｔａｎｙｄｅｓｃｒ
ｉｐ
ｔｉｏｎｆｒｏｍ


ｏｒ
ｉｇ
ｉｎａｌｓｃｅｎｅｋｎｏｗｌｅｄｇｅａｂｏｕｔｔｈｅｔａｒｇｅｔｏｂｊｅｃｔｉｎ
ｑｕｅｒｙ
．
）


简化的场景知识只包含原始场景知识对目标物体的部分描述
。


２（Ｔｈｅｃｏｎｄｅｎｓｅｄｓｃｅｎｅｋｎｏｗｌｅｄｇｅｏｎｌｙｅｘｔｒａｃｔｓａｐａｒｔｉａｌｄｅｓｃｒｉｐｔｉｏｎ


ｆｒｏｍｏｒｉｇ
ｉｎａｌｓｃｅｎｅｋｎｏｗｌｅｄｇｅａｂｏｕｔｔｈｅｔａｒｇｅｔｏｂｊｅｃｔｉｎ
ｑｕｅｒｙ
．）


简化的场景知识包含原始场景知识对目标物体的所有描述
，
同时


也包含其他无关的描述
。


３（Ｔｈｅｃｏｎｄｅｎｓｅｄｓｃｅｎｅｋｎｏｗｌｅｄｇｅｅｘｔｒａｃｔｓａｌｌｄｅｓｃｒ
ｉｐ
ｔｉｏｎｓｆｒｏｍ


ｏｒｉｇ
ｉｎａｌｓｃｅｎｅｋｎｏｗｌｅｄｇｅａｂｏｕｔｔｈｅｔａｒｇｅｔｏｂｊｅｃｔｉｎ
ｑｕｅｒｙ，ａｎｄｅｘｔｒａｃｔｓ


ｓｏｍｅｄｅｓｃｒｉｐ
ｔｉｏｎｓａｂｏｕｔｏｔｈｅｒｏｂｊｅｃｔｓ
．
）


简化的场景知识包含原始场景知识对目标物体所有描述
，并且不


包含其他无关的描述
。


４（Ｔｈｅｃｏｎｄｅｎｓｅｄｓｃｅｎｅｋｎｏｗｌｅｄｇｅｅｘｔｒａｃｔｓａｌｌｄｅｓｃｒ
ｉｐｔｉｏｎｓｆ
ｒｏｍ


ｏｒ
ｉｇ
ｉｎａｌｓｃｅｎｅｋｎｏｗｌｅｄｇｅａｂｏｕｔｔｈｅｔａｒｇｅｔｏｂｊｅｃｔｉｎ
ｑｕｅｒｙ，ａｎｄｄｏｅｓｎ
＇
ｔ


ｅｘｔｒａｃｔａｎｙｏｔｈｅｒｄｅｓｃｒ
ｉｐｔｉｏｎ
．
）


经过ＧＰＴ４评估后的数据
，
其中评分为
１和
２的数据被视作
“ 低质数据
”
，


对其重新调用
４
．２
．
１小节的标注过程进行重标
，
在这个过程中结合人工抽样核验


确保数据质量。
综合考虑ＡＰＩ调用过程中的时间成本和ＧＰＴ４的ＡＰＩ价格
，
这


个过程经历了两轮
，
流程如图４
－５所示
。


（
ＧＰＴ３
．５ｆ

（
＾


原始数据＾
生成数据＇

？？
高质量数据


ｖＪＩ
ｖＪ＼ＶＪ


ｒｐＴ
，
，ＧＰＴ４筛选


重新生成
Ｘ２十人工核＆


Ｖｒ）


＾低质数据Ｖ


ＶＪ


图
４
－５
低质数据筛选和重标流程


最终结合少量的人工核验和标注
，
评分为
３和
４的数据比例分别为
１２
．０３％


和
８７
．９７％
，
部分例子如表
４
－３所示
。


５０


第四章基于场景知识的指称表达理解研究


表４
－３
生成的简化场景知识及其评分样例


Ｔｈｅｍａｎｗｅａｒｉｎｇａ
ｐ
ｉｎｋｓｕｉｔａｎｄｈａｔｉｎｔｈｅｍｉｄｄｌｅｏｆｔｈｅ


ｉｍａｇｅｉｓＪｏｎａｈ．ＨｅｉｓｇｒａｂｂｅｄｂｙＬｏｇａｎ
，ａ
ｇ
ｉｒｌｗｅａｒｉｎｇａ


ｂｌａｃｋｈａｔｗｉｔｈａｃｌｏｗｎｆａｃｅｏｎｈｉｓｒｉｇｈｔ．Ｖｉｎｃｅ，ａｂｏｙｗｈｏ


ＳｃｅｎｅＫｎｏｗｌｅｄｇｅｈａｓｔｈｅｓａｍｅｃｌｏｗｎｆａｃｅｏｎＬｏｇａｎ
，
ｉｓｓｔａｎｄｉｎｇｏｎＪｏｎａｈ
＇
ｓ


ｌｅｆ
ｔａｎｄｓｔａｒｉｎｇｃｌｏｓｅｌｙａｔＪｏｎａｈ．Ｔｈｅｔｈｒｅｅｏｆｔｈｅｍａｒｅ


ｐｅｒｆｏｒｍｉｎｇｏｎｔｈｅｓｔｒｅｅｔ
，ａｔｔｒａｃｔｉｎｇｍａｎｙｐａｓｓｅｒｓ
－ｂｙｔｏ


＃１ｗａｔｃｈｔｈｅｉｒ
ｐｅｒｆｏｒｍａｎｃｅ．


ＱｕｅｒｙＴｈｅｈａｔｗｏｒｎｂｙＪｏｎａｈ


Ｊｏｎａｈ
，
ｔｈｅｍａｎｗｅａｒｉｎｇａ
ｐ
ｉｎｋｓｕｉｔａｎｄｈａｔｉｎｔｈｅｍｉｄｄｌｅ


ＣｏｎｄｅｎｓｅｄＳｃｅｎｅｏｆｔｈｅｉｍａｇｅ
．Ｌｏｇａｎ
，
ａｇ
ｉｒｌｗｅａｒｉｎｇａｂｌａｃｋｈａｔｗｉｔｈａ


ＫｎｏｗｌｅｄｇｅｃｌｏｗｎｆａｃｅｏｎＪｏｎａｈ
＇
ｓｒｉｇｈｔ．Ｖｉｎｃｅ
，ａｂｏｙｗｅａｒ
ｉｎｇａｈａｔ


ｗｉｔｈｔｈｅｓａｍｅｃｌｏｗｎｆａｃｅｏｎＪｏｎａｈ
’
ｓｌｅｆ
ｔ．


Ｓｃｏｒｅ３


Ｔｈｅｂａｌｄｗｈｉｔｅ
－ｈａｉｒｅｄｍａｎＲｅｘｉｓｐｕｌｌｉｎｇｈｉｓｄａｕｇｈｔｅｒ


Ａｎｎａｉｎｔｏｔｈｅｒｏｏｍ
，ｃｏｍｐ
ｌｅｔｅｌｙ
ｉｇｎｏｒｉｎｇｈｉｓｂａｒｅｆｏｏｔｗｉｆｅ


ＳｃｅｎｅＫｎｏｗｌｅｄｇｅ


Ｃａｔｈｙｂｅｈｉｎｄｈｉｍ．Ａｎｎａｗｅａｒｓａｗｈｉｔｅｓｕｉｔ
ｊａｃｋｅｔａｎｄｉｓ


ｗａｖｉｎｇｈｅｒｈａｎｄｔｏｃｏｏｌｄｏｗｎ
，ｓｅｅｍｉｎｇｔｏｆｅｅｌｖｅｒｙｈｏｔ．


＃２


ＱｕｅｒｙＴｈｅｍａｎｗｈｏｉｓｂａｌｄ


ＣｏｎｄｅｎｓｅｄＳｃｅｎｅＴｈｅｍａｎ
，Ｒｅｘ
，ｉｓｂａｌｄａｎｄｗｈｉｔｅ
－ｈａｉｒｅｄ．


Ｋｎｏｗｌｅｄｇｅ



Ｓｃｏｒｅ
４


最终在原始
ＳＫ
－ＶＧ数据集的基础上得到了新标注的ＣｏｎｄｅｎｓｅｄＳＫ
－ＶＧ（Ｃ
－


ＳＫ
－ＶＧ）数据集
。统计两者中场景知识长度结果对比如表４４所示
，可以发现
，本


章提出的数据简化方案有效地降低了场景知识的复杂度
。


表４４ＳＫ
－ＶＧ和Ｃ
－ＳＫ
－ＶＧ数据集中场景知识复杂度对比


ＤａｔａｓｅｔＭａｘＬｅｎｇ
ｔｈＡｖｅｒａｇｅＬｅｎｇｔｈ



ＳＫ
－ＶＧ
１０９５８
．２８


Ｃ
－ＳＫ
－ＶＧ
８２Ｕ￡
７


５
１


北京邮电大学硕士学位论文


４
．３面向场景知识的指称表达理解推理网络


本小节全面介绍
ＳＫＲＮ
的整体模型框架和设计思路
，
以及各组件的设计细


Ｈ
－Ｈ


Ｔ
Ｉ〇


４
．３
．
１模型总览


ＳＫＲＮ的整体模型框架如图
４
－６所示
，
主要包括语言编码器
，
图像编码器
，


以及场景知识推理模块
，
定位框预测模块
，
以及模型的损失函数设计
。其中语言


和图像编码器的具体实现
、定位框预测模块的设计跟第三章ＬＧＲ
－ＮＥＴ相应部分


相同
，
在此不再赘述
。模型首先通过各自的编码器提取指称查询
、场景知识
、
图


像的特征并得到特征序列
。
然后
，
本节将重点介绍场景知识推理模块
，


通过三个不共享的多头注意力分别对
进行注意力计算并提取特征
，在此


过程中预测
ｔｏｋｅｎ逐渐捕获相应模态中用于定位目标物体的特征
，
并在每
一层都


输出预测框坐标
。


Ｌａｎｇｕｒ＾／ｆ
Ｆｅ？Ｕｎ
．Ｅｉｍ．ｃ？ｒＳ


Ｅｎｃｏｄｅｒ


ＳｃｅｎｅＫｎｏｗｌｅｄｇｅ｜
Ｓｈａｒｅ


▼ＦｑＬｆ
ｌＵ
＾Ｅｎｃｍｋｒ＇


Ｌａｎｇｕａｇ
ｅ


＊Ｅ丄二

—


Ｑｕｅｒｙ


ＳＫＲｅａｓｏｎ
ｉｎｇ
．Ｍｏｄｕ
ｌｅ


Ｆ
．




｜
－
－＞
ｊ
Ｕｐｄａｔｅ
Ｉ
￣？Ｐ
ｑ
１？


４Ｆｑ
＼ＦｈＦｓｉｉ…
Ｆ
ｉ

＇
Ｆ，
＂




—
—
ｒ
— — —
■

Ｕｐｄａ
ｔｅ
— ？
■

＞


＼＼Ｌ＼
／


＿＿＾ＭＨＡ＾ＭＨＡ＾．
＞Ｑ
＞
：
ＦＦＮ—


— ？
｛１
Ｑ

＞


Ｐ
ｉ
ｘＮ


图
４
－６
面向场景知识的推理网络（ＳＫＲＮ
）模型框架


４
．３
．２结合场景知识的指称表达推理模块


为充分利用指称查询
、场景知识和图片三方面的特征进行联合的跨模态推理
，


本节设计了面向场景知识的推理模块
。
首先参考
ＬＧＲ
－ＮＥＴ设置
一个预测
ｔｏｋｅｎ


ｐ
ＥＲ
ｌｘｃ用于收集来自上述三方面的特征并预测定位框
，
ｃ为隐向量维度
。
在每


一层的推理过程中依次对指称查询
、场景知识
、
图片进行多头注意力运算
。首先


对指称查询进行注意力计算
，
如公式４
－
１所示
。


Ｐｑ
＝ＭＨＡ
（Ｑｐ
，Ｋ
ｑ
，Ｖ
ｑ）
（４
－
１）


其中
分别来自当前层的预测ｔ
ｏｋｅｎ，
当前层的指称表达特征￥
，


以及原始指称表达特征＆
。


类似的
，
再融入场景知识
，
如公式
４
－２所示
。


５２


第四章基于场景知识的指称表达理解研宄


Ｖｓｋ
＝ＭＨＡ
｛Ｑｐ？ＫＳＫ
ｉＶｓｋ）（４
－
２）


其中
？分别来自公式
４
－
１的预测
ｔｏｋｅｎ结果＆
，
当前层的场景知识


特征
，
以及原始场景知识特征


然后
，
再结合图片特征
，
如公式４
－３所示
。


Ｖｌ
＝ＭＨＡ
（Ｑｆ
，Ｋ
Ｉ
，Ｖ
Ｉ）（４
－
３）


其中
Ａ：
；
，Ｋ分别来自公式４
－２的预测ｔ
ｏｋｅｎ结果
当前层的图像特征Ｆ／
，


以及原始图像特征Ｆ
；
。
需要注意
，
在第
一层中
，
Ｆ／
＝Ｆ
；
，
另外两个特征序列情况


相同
。


最后
，
经过
一个全连接层和残差连接得到当前层更新后的预测
ｔｏｋｅｎ表示
，


如公式４
－４所示
。


（
Ｖ
＇＝ＬＮＱｐｉ＋ｐｊ）


［ｐ
ｉ＋１＝ＬＮ（ｐ
＇
＋ＦＦＮ（ｐ
＇
））
ＣＪ


此外
，
由于场景知识推理模块包含上述结构的多层叠加
，在推理迭代的过程


中
，
预测
ｔｏｋｅｎ会逐步更新
，
收集指称物体丰富的特征
。
因此对下
一轮次用于多


头注意力中
ｋｅｙ特征序列的指称查询
、场景知识
、
以及图像特征进行更新能有效


地调整注意力权重的分布
，
进而优化推理结果
。具体更新过程如图
４
－７所示
，
首


先利用更新后的预测
ｔｏｋｅｎｐ
ｉ＋１结合原始图片特征序列计算相似性分数
，然后利


用
Ｓｉｇｍｏｉｄ激活函数对其进行归
一化约束
，再乘以原始特征得到更适用于接下来


一个轮次的特征用于多头注意力中注意力权重矩阵的计算
。


Ｆ：


严


图
４
－７ＳＫＲｅａｓｏｎｉｎｇＭｏｄｕｌｅ中图像特征更新操作示意图


总结如公式
４
－５所示
。
指称查询特征￥和场景知识特征
的更新过程与公


式４
－５类似
，
在此不再赘述
。


Ｆ／
＋１＝Ｆ
，
■
５ｉ
＜ｇ
？ｎｏｉｄ（Ｆ
／（ｐ
ｌ＋１
）
Ｔ
）（４
－
５）


４
．３
．３模型损失函数


为训练
ＳＫＲＮ
，
本小节设计损失函数如下
：


５３


北京邮电大学硕士学位论文


Ｎ


＾
ｔｏｔａｌ
—
Ｉ
＾
ｇ
ｉｏｕ＾
ｇ
ｉｏｕｉｂ
．ｂ
１
）＋
（４
－
６）


ｉ＝ｌ


其中
Ｕ和
分别为ＧＩｏＵ损失和Ｌ
１损失
，
和Ａｔｌ为平衡两者


的超参数
。此外
，
一方面
ＳＫＲＮ模型框架更侧重结合指称查询和场景知识的联合


推理
，另
一方面由于
ＳＫ
－ＲＥＣ任务的场景知识中存在
一定的噪声
，
因此
ＳＫＲＮ的


损失函数没有沿用
ＬＧＲ
－ＮＥＴ中包含的跨模态对齐损失
，而是仅设置框回归损失


来监督模型的训练
。


４
．４实验对比与分析


４
．４
．
１实验参数与设置


关于
ＳＫＲＮ训练过程中的具体设置跟ＬＧＲ
－ＮＥＴ大体相同
。主要差异在于损


失函数的超参数设置
，
参考之前工作
［２８
］的经验
，
设置
分别为
２和
５
，


此外在模型训练的前
１０个
ｅｐｏｃｈ中冻结视觉和文本编码器的参数以稳定训练过


程
。
由于
ＳＫＲＮ对输入的指称表达和场景知识分别进行特征提取
，
因此结合表


４
－３的数据集文本长度统计
，
ＳＫＲＮ设置指称表达最大文本长度为４０
，ＳＫ
－ＶＧ和


Ｃ
－ＳＫ
－ＶＧ数据集下的场景知识最大文本长度分别为
１２０和
１００
。


４
．４
．２评测指标对比与分析


由于
ＳＫ
－ＲＥＣ任务以及
ＳＫ
－ＶＧ数据集比较新
，
可用于对比的
ｂａｓｅｌｉｎｅ不如


传统ＲＥＣ任务丰富
。
在此列出所有在该数据集评估过的工作作为对比基线
，
它


们分别是ＮＭ
－Ｔｒｅｅ
［８３
］
，ＣＭ
－Ａｔ
－Ｅｒａｓｅ
［
１
（）３
］
，
ＲｅＳＣ
－Ｌａｒｇｅ＿
，ＦＡＯＡ
［
１８
］
，ＭａｔＮｅｔ
［
１４
］
，


ＫｅＶｉＬＩ
［４０
］
。其中ＮＭ
－Ｔｒｅｅ
、
ＲｅＳＣ
－Ｌａｒｇｅ
、
ＦＡＯＡ
、
以及ＭａｔＮｅｔ在３
．３
．２节中己经


介绍过
，
在此不再赘述
。
首先简要介绍
一下ＣＭ
－Ａｔ
－Ｅｒａｓｅ和ＫｅＶｉＬＩ
：


１）ＣＭ
－Ａｔｔ
－Ｅｒａｓｅ
以
ＭａｔｔＮｅｔ作为基础模型
，
通过移除数据样本中最关键的


图文关联信息强迫模型学习到更细节
，容易被忽视的图文关联
，进而提升模型注


意力层的性能
。


２
）ＫｅＶｉＬＩ利用场景知识特征与图像特征进行跨注意力计算先对图像特征进


行增强
，
提高对场景知识提及物体的注意权重
，
再结合
３
．３
．
１小节提及的拼接注


意力方案对指称表达特征和增强后的图像特征进行跨模态融合
。


实验结果对比如表４
－５所示
，可以发现
ＳＫＲＮ取得了最佳的性能表现
，说明


了模型面对复杂场景知识的推理能力
。
此外
，
本节还对第三章提出的
ＬＧＲ
－ＮＥＴ


在ＳＫ
－ＶＧ数据集上进行了评估
，通过模板
“
Ｑｕｅｒｙ
：ｒｅｆｅｒｒｉｎｇｅｘｐｒｅｓｓｉｏｎ
．Ｋｎｏｗ
ｌｅｄｇｅ
：


ＳｃｅｎｅＫｎｏｗｌｅｄｇｅ
．
” 将
ＳＫ
－ＶＧ数据样本中的指称表达和场景知识拼接为
一个单
一


的文本序列
，
并将最大句子长度设置为
１４０
，
其他设置与
３
．３
．
１小节
一致
。
可以


５４


第四章基于场景知识的指称表达理解研宄


发现ＬＧＲ
－ＮＥＴ取得了次优的结果
，相比于ＫｅＶｉＬＩ的拼接注意力方案
，ＬＧＲ
－ＮＥＴ


超越了１０个百分点
，
进
一步说明
ＬＧＲ
－ＮＥＴ中跨模态推理方案相比拼接注意力


方案的优势
。另
一方面ＬＧＲ
－ＮＥＴ性能表现落后于
ＳＫＲＮ模型
，说明仅仅通过上


述模板对场景知识进行简单的拼接尚不足以应对如此复杂的跨模态推理要求
。


表
４
－５ＳＫＲＮ模型在
ＳＫ
－ＶＧ下的性能对比


ＭｅｔｈｏｄＡｃｃＭｅｔｈｏｄＡｃｃ


ＮＭ
－Ｔｒｅｅ
［８３］２５
．２４ＭａｔｔＮｅｔ
［
１４
］２５
．２８


ＣＭ
－Ａｔ
－Ｅｒａｓｅ
［
１０３
］２６
．０８ＫｅＶｉＬＩ
［４０
］３０
．０
１


ＲｅＳＣ
－Ｌａｒｇｅ
［２Ｇ
］３６
．６８ＬＧＲ
－ＮＥＴ４０
．４
１


ＦＡ０Ａ
［
１８］

１６
．３０
ＳＫＲＮ
４３
．０４


此外
，
ＳＫ
－ＶＧ数据集中的测试集包含推理难度的标注
，
分为简单（Ｅａｓｙ，
Ｅ）
，


中等（Ｍｉｄｉｕｍ
，Ｍ）
，
和困难（Ｈａｒｄ
，Ｈ
）
。
核心的标注依据为与场景知识相关程度越


高
，
视觉可分辨性越差的指称表达难度越高
。其中
“ 简单
” 的评判标准为指称表


达中包含目标物体明显的外观信息
，
与其他物体的关系或者其他视觉线索
；
“ 中


等
” 的评判标准为指称表达中仅仅提及不明显的视觉信息
；
“ 困难
” 的评判标准


为答案要求完全来源于场景知识
，
没有视觉偏向
。示例如图
４
－８所示
，
面对同
一


张图片和相同的场景知识
，由于不同指称表达指向物体对场景知识的依赖程度不


同
，
导致不同的难度
。


圍ＳｃｅｎｅＫｎｏｗ
ｌｅｄｇｅ


Ｉｎｔｈｅａｎ
ｔ
ｉｑｕｅｂｏｏｋｓ
ｔｏｒｅｗ
ｉ
ｔｈｈｕｇｅ


ｃｈａｎｄｅ
ｌ
ｉｅｒｓ
．Ｂｒ
ｉａｎ
，ａｎｏ
ｌｄｗｒ
ｉ
ｔｅｒ
ｉｎ


ａｔｏｐｈａｔｏｎｔｈｅｌｅｆｔｏｆｔｈｅｉｍａｇｅ
．


ｉｓｈａｎｇ
ｉｎｇｏｕ
ｔ
．ＷｈｅｎＣａｒｒ
ｉｅ
，ａ
ｆｆ
ｌＢ


ｓｈｏｒｔ
－ｈａ
ｉｒｅｄｆｅｍａ
ｌｅ加
，
ｓｅｅｓｈｅｒ


ｉｄｏ
ｌ
，ｓｈｅｎｅｒｖｏｕｓ
ｌｙｇ
ｉｖｅｓａｗａｙａ
ｌ
ｌ■


ｔｈｅｂｏｏｋｓｉｎｈｅｒｈａｎｄ
，ａｎｄｉｓ■


ｓｑｕａｔｔ
ｉｎｇｏｎｔｈｅｇｒｏｕｎｄｔｏｐ
ｉｃｋｕｐ獨


ｂｏｏｋｓｉｎａｐａｎｉｃ
．Ｔｈｅｏｌｄｗｒｉｔｅｒ


ｌｏｏｋｓａｔｈｅｒｗ
ｉｔｈｃｏｎｃｅｒｎ
．
Ｔｈｅｈｕｇｅｃｈａｎｄｅ
ｌ
ｉｅｒｓｉｎｔｈｅａｎｔｉｑｕｅｂｏｏｋｓｔｏｒｅ［Ｅａｓｙ
｜


ＡＲｅｆｅｒｒｉｎｇＥｘｐｒｅｓｓ
ｉｏｎｉｎＴｈｅｏ
ｌｄｗｒｉｔｅｒｌｏｏｋｉｎｇａｔＣａｒｒ
ｉｅ
丨Ｍｅｄ
ｉｕｍ


ｄ
ｉｆｆｅｒｅｎｔｄ
ｉｆｆｉｃｕ
ｌｔｙ
ｌｅｖｅ
ｌｓＢｒｉａｎ
＇
ｓｆａｎ
［
Ｈａｒｄ


图
４
－８
不同难度的指称表达样例


ＳＫ
－ＶＧ测试集中不同难度的指称表达比例统计如表４
－６所示
。在此基础上
，


本小节对模型在不同难度样本上的表现进行了进
一步的分析
。结果如表４
－７所示
，


对比ＬＧＲ
－ＮＥＴ和ＫｅＶｉＬＩ可以发现
，
ＬＧＲ
－ＮＥＴ主要在简单和中等难度的样本上


获得了明显的性能提升
，
而在困难样本上的提升相对不够显著
。
一方面
，这进
一


５５


北京邮电大学硕士学位论文


表
４
－６ＳＫ
－ＶＧ测试集中不同睢度样本比例
（％）


Ｄｉｆｆｉｃｕｌｔ
ｙ
ｌｅｖｅｌＥａｓｙＭｅｄｉｕｍＨａｒｄ


Ｐｒｏｐｏｒｔｉｏｎ４５
．９２７
．７２６
．４


步说明
ＬＧＲ
－ＮＥＴ指出的充分利用语言特征引导跨模态推理是正确而有效的
，
另


一方面也说明面对如此复杂的场景知识
，
ＬＧＲ
－ＮＥＴ的模型架构还有
一定的提升


空间
。通过观察ＳＫＲＮ和ＫｅＶｉＬＩ在不同难度样本上的性能差异还可以发现ＳＫＲＮ


的提升更显著而均衡
，在困难的样本上也获得了明显的改进
，这说明了ＳＫＲＮ推


理框架对场景知识的有效推理
。


表
４
－７模型在不同睢度数据上的性能对比


Ｄｉｆｆ
ｉｃｕｌｔｙ
－
ｌｅｖｅｌ


Ｍｅｔｈｏｄ
ＯｖｅｒａｌｌＡｃｃ


ＡｃｃｅＡｃｃｍＡｃｃｈ


ＫｅＶｉＬＩ
［４０
］３３
．７５２６
．５５２７
．
１４３０
．０
１


ＬＧＲ
－ＮＥＴ４８
．０５３５
．
１８３２
．６
１４０
．４
１


ＳＫＲＮ
４８
．８
１
３７
．６４
３８
．６９
４３
．０４


综上所述
，
上述实验结果不仅进
一步验证了ＬＧＲ
－ＮＥＴ的有效性
，
而且证明


了ＳＫＲＮ网络面对复杂场景知识进行跨模态推理的优势
。


４
．４
＿３消融实验


为说明利用大模型进行数据简化以及
ＳＫＲＮ
的组件有效性
，
本节分别设计


消融实验进行说明
。


１
）数据简化


本节使用
４
．２节生成的Ｃ
－ＳＫ
－ＶＧ数据集重新训练和评估
。
为跟之前模型进


行区分
，
Ｃ
－ＳＫ
－ＶＧ数据集上得到的模型记为ＬＧＲ
－ＮＥＴ＊和
ＳＫＲＮ
＊
，
两者性能对


比如表
４
－８所示
。
首先从总体正确率对比结果可以发现在Ｃ
－ＳＫ
－ＶＧ数据集上两


个模型都有显著的性能提升
，这说明了生成的简化版数据降低了原始复杂场景知


识对模型推理造成的干扰
，
降低了模型的学习难度
。
并且简化数据对
ＬＧＲ
－ＮＥＴ


的提升更明显
一些
，
得到
５
．３２％的提升
，
而给
ＳＫＲＮ带来的提升为
４
．４６％
。
这也


说明针对传统
ＲＥＣ任务的模型
，
直接简化输入文本的复杂性带来的收益是更显


著的
。此外
，
模型在不同难度样本下的性能提升程度也有差异
，
具体来说在困难


样本上简化数据带来的提升更明显
，
例如两模型在Ｚｃｑ上平均提升了８
．９９％
，
而


在
上的平均提升只有
２
．
１３％
。这说明在需要高度依赖场景知识进行联合推理


的困难样本中
，
简化的场景知识更能体现优势
。


表
４
－８
模型在
ＳＫ
－ＶＧ和Ｃ
－ＳＫ
－ＶＧ数据集上的性能对比


Ｄｉｆｆ
ｉｃｕｌｔｙ
－
ｌｅｖｅｌ


Ｍｏｄｅｌ
ＯｖｅｒａｌｌＡｃｃ


ＡｃｃｅＡｃｃｍＡｃｃｈ


５６


第四章基于场景知识的指称表达理解研究


ＬＧＲ
－ＮＥＴ４８
．０５３５
．
１８３２
．６１４０
．４
１


ＳＫＫＮ４８
．８１３７
．６４３８
．６９４３
．０４


ＬＧＲ
－ＮＥＴ＊５０
．４９４
１
．０２４２
．３７４５
．７３


ＳＫＲＮ
＊

５０
．６２
４２．８５
４６
．９２
４７．５０


２
）ＳＫＲＮ的特征更新模块


为说明
ＳＫＲＮ的场景知识推理模块中逐层更新模态特征的有效性
，
本小节


针对更新模块在
ＳＫ
－ＶＧ数据集上进行消融实验
，
分别消融对指称表达特征、
场


景知识特征
、
图像特征的更新操作
，结果如表４
－９所示
。可以发现对三个特征的


消融都带来了不同程度的性能下降
，
说明随着预测
ｔｏｋｅｎ的逐层更新
，
对ＭＨＡ


中参与注意力权重计算的特征进行同步更新有助于对关键特征的捕获
，并提升模


型推理的准确率。还可以发现移除对视觉特征的更新对模型性能的损害最大
，这


说明对视觉特征的逐层更新很有必要
，
一定程度上跟
３
．３
．４小节的结论是相符的
。


此外
，相比于指称表达特征
，对场景知识特征的更新对模型的表现影响更大
，
一


方面指称表达通常比较简单
，仅仅包含对目标物体的指向性描述
，因此预测ｔ
ｏｋｅｎ


对其注意力的分布相对集中
，对其进行更新操作带来的收益有限
。而场景知识更


复杂
，
包含图片中多个物体的描述
。
因此需要动态调整预测
ｔｏｋｅｎ对场景知识特


征的注意力分布
，逐渐增大对目标物体相关的描述的注意力而降低对其他描述的


注意力
。


表
４
－９ＳＫＲＮ中不同特征更新模块的消融结果


ＭｏｄｅｌＡｃｃ


ＳＫＲＮ
（ｗ／ｏｕｐｄａｔｅＦ
ｑ）４２
．８３


ＳＫＲＮ（ｗ／ｏｕｐｄａｔｅＦＳＫ）４１
．２６


ＳＫＲＮ
（ｗ／ｏｕｐｄａｔｅＦ
；）４０
．６７



ＳＫＲＮ
４３．０４


４．４
．４定性分析


本节通过对比
ＳＫＲＮ模型在
ＳＫ
－ＶＧ和Ｃ
－ＳＫ
－ＶＧ两个数据集的部分案例定


性分析简化的场景知识给模型推理带来的影响
。如图４
－９所示
，每个例子分别展


示了ＳＫ
－ＶＧ
中原始的场景知识
、
Ｃ
－ＳＫ
－ＶＧ
中简化版的场景知识
、
指称表达


（Ｑｕｅｒｙ）
、
图片
、
以及预测结果
。可以发现ＧＰＴ３
．５生成的简化版场景知识都过滤


了原始场景知识中的无关描述
，
生成了更简洁
，
更短的场景知识
。
在此基础上


ＳＫＲＮ模型在复杂场景下的错误预测转为简化场景下的正确预测
。此外可以发现


例子２中大模型还进
一步梳理场景知识中任务的逻辑关系
，指明
“ Ｇａｒｙ
ｉｓｔｈｅｆａｔｈｅｒ


ｏｆＰｈｉｌｉｐ
”
，
大大降低了定位目标物体的定位难度
。


５７


北京邮电大学硕士学位论文


發
：
ＢｉＳｃｅｎｅＫｎｏｗｌｅｄｇｅ


＾Ａｔｔｈｅｂａｓｅｉｎｔｈｅｄｅｓｅｒｔ
，ｏｎｅｏｆｔｈｅ


ｍｅｎ
．Ｐｈ
ｉ
ｌ
ｉｐ
，ｗ
ｉ
ｔｈａｐ
ｉｓ
ｔｏ
ｌ
ｉｎｌｉ
ｉｓ


ｈａｎｄ
，ｗｏｕｎｄｓＰｏｒｔｅｒ，ｗｈｏｉｓｌｙ
ｉｎｇ


一
…
＇
－
ｏｎｔｈｅｇｒｏｕｎｄ
．Ｐｈ
ｉ
ｌ
ｉｐ
ｌｏｏｋｓａ
ｔ



＇Ｈ
…
Ｃａｅｓａｒ，ｈｉｓｂ
ｌａｃｋｃｏ
ｌ
ｌｅａｇｕｅｉｎｄａｒｋ


蠢通
一
，ＳＩ
ｂ
ｌｕｅｔｏｈ
ｉｓｌｅｆｔ
，ｅｘｐ
ｌａ
ｉｎ
ｉｎｇ
ｔｈａｔ


ＨＨＨＨ
Ｐｏｒｔｅｒｗａｎ
ｔｓｔｏｓｔｅａ
ｌｓｅｃｒｅｔ


【
ｄｏｃｕｍｅｎｔｓａｎｄｈａｓｔｏｈｕｒｔｈ
ｉ
ｊｎ
．


Ａｎｄｔｈｅｓｕｒｐ
ｒ
ｉｓｅｄＭａｋｅｓｔａｎｄ
ｉｎｇ


ＨＨ
ｊ
ｏｎｔｈｅ
ｌｅｆｔｓ
ｉｄｅｏｆｔｈｅｉｍａｇｅｃａｎ
＇
ｔ



」Ｊ

＇，
：
汽
ｂｅ
ｌ
ｉｅｖｅｗｌｉａｔｈａｐｐｅｎｅｄ
．


圓
ＣｏｎｄｅｎｓｅｄＳｃｅｎｅＫｎｏｗ
ｌｅｄｇｅ


〇ｕｅｒ
ｙ
ｒ
：Ｍａｎｗ
ｉｔｈａｓｕｒｐｒｉｓｅｄｆａｃｅ
￣
＇
：
Ｍａｒｋ
，ｗｉｔｈａｓｕｒｐｒｉｓｅｄｆａｃｅ
，
ｉｓ


？
ｓｔａｎｄ
ｉｎｇｏｎｔｈｅｌｅｆｔｓ
ｉｄｅｏｆｔｈｅ


ｉｍａｇｅ
．


ｍ
Ｂ
ｉＳｃｅｎｅＫｎｏｗ
ｌｅｄｇｅ


Ｅｄｗａｒｄ，ｔｈｅｏｌｄｎｕｒｓｅａｔｔｈｅｈｏｓｐ
ｉｔａｌ，


ｉｓｖｅｒｙａｎｇ
ｒｙｂｅｃａｕｓｅｈｅｓｅｅｓｔｈｅ


ｎｅｗｄｏｃｔｏｒＲｏｂｅｒｔｒｏｕｇｈ
ｌｙｐ
ｉｃｋ
ｉｎｇ


ｕｐ
ｔｈｅｄａｒｋ
－ｓｋ
ｉｎｎｅｄｂａｂｙＰｈ
ｉ
ｌ
ｉｐ
．


Ｇａｒｙ
，ｔｈｅｆａ
ｔｍａｎｗ
ｉ
ｔｈｇ
ｌａｓｓｅｓ
，
ｌｏｏｋｓ


ｓｕｒｐｒ
ｉｓｅｄ
．Ｈｅｄｏｅｓｎ
＇
ｔａｇｒｅｅｗ
ｉｔｈｔｈｅ


ｄｏｃｔｏｒｈｏ
ｌｄ
ｉｎｇｕｐｈ
ｉｓｓｏｎＰｈ
ｉ
ｌ
ｉｐ
ｌ
ｉｋｅ


ｔｈ
ｉｓａｓｗｅ
ｌ
ｌ
．Ｈｅｔｈ
ｉｎｋｓｔｈａｔｔｈｅｄｏｃ
ｔｏｒ


ｓｈｏｕ
ｌｄｎ
＇
ｔｄｏｔｈ
ｉｓ
．


圓
ＣｏｎｄｅｎｓｅｄＳｃｅｎｅＫｎｏｗ
ｌｅｄｇｅ


（ＢｊｉＱｕｅｒｖ
ｒ
：Ｔｈｅｆａｔｈｅｒｗｈｏ
＇
ｓｓｏｎｉｓＰｈ
ｉ
ｌ
ｉｐ
—
Ｇａｒｙ
，ｔｈｅｆａｔｍａｎ从ｉｔｈ
ｇ
ｌａｓｓｅｓ
，
ｉｓｔｈｅ


？
ｆａｔｈｅｒｏｆ
Ｐｈ
ｉ
ｌ
ｉｐ
．


图
４
－９ＳＫＲＮ在
ＳＫ
－ＶＧ和Ｃ
－ＳＫ
－ＶＧ下的案例对比
。
图中可视化了ＳＫＪＲＮ在
ＳＫ
－ＶＧ下的


预测结果（红色框）
、
Ｃ
－ＳＫ
－ＶＧ下的预测结果（黄色框
）
，
以及
ｇｒｏｕｎｄｔｒｕｔｈ
（蓝色框）
。


４
．５本章小结


本章开展了基于场景知识的指称表达理解研究工作并提出从数据和模型两


个角度解决复杂场景知识带来的推理千扰问题
。首先
，本章从数据层面利用大模


型简化复杂的场景知识
，通过设计指令模板要求大模型根据指称表达和原始的场


景知识提取其中相关的描述并过滤无关的场景描述
。
进
一步利用更强的大模型


ＧＰＴ４对生成的数据进行评估打分
，
结合人工核验筛选低质数据进行重标
，
最终


在原始
ＳＫ
－ＶＧ数据集的基础上生成简化版场景知识的Ｃ
－ＳＫ
－ＶＧ数据集
。此外
，


本章提出
ＳＫＲＮ模型解决
ＳＫ
－ＲＥＣ任务
，
对输入的指称表达和场景知识分别编


码
，
在推理过程中预测
ｔｏｋｅｎ分别对其进行注意力计算
、
提取相应的特征
，
结合


逐层的特征更新实现注意力分布的调整
，提高模型推理的准确率
。最后
，本章在


实验验证部分通过全面的定量指标对比
，组件有效性分析
，
定性分析验证了数据


简化方案和
ＳＫＲＮ模型的可行性和有效性
。


５８


第五章
总结与展望


第五章总结与展望


５
．１工作总结


本文开展了面向复杂文本的指称表达理解研究
，旨在根据给定的文本定位图


像中的目标物体
，并解决这个过程中涉及的复杂跨模态推理问题
。
目前
，主流的


研究工作存在两个问题亟待解决
：
１）基于Ｔｒａｎｓｆｏｒｍｅｒ的ＲＥＣ模型在跨模态推


理过程中存在文本信息淹没问题
，
导致文本信息利用不足
；
２）当前的ＲＥＣ模型


缺乏有效的针对复杂场景知识的推理方案
。为此
，本文提出充分利用语言特征引


导模型的跨模态推理过程
，
同时结合大模型简化复杂的场景知识解决上述问题
。


针对问题
１
，
本文提出充分利用语言来引导跨模态推理
，
开展了基于语言引


导的指称表达理解算法研宄工作
。本文设计ＬＧＲ
－ＮＥＴ模型从两个方面实现对语


言信息的充分利用
。
一是避免文本淹没问题
，摒弃了自注意力机制融合拼接的图


文特征方案而改用跨注意力机制融入语言信息
，这样可以避免文本信息淹没在视


觉信息中
。二是在图文跨模态推理过程中从多个角度
，多个层次引入不同抽象层


次的文本信息
，包括指称表达中关于目标物体的空间位置描述信息
、全体的指称


表达信息
、
以及全局的句子信息
，分别用于跨模态推理过程的多个阶段
，如此引


入丰富的文本信息进行引导
。本文通过定量和定性实验
，验证了所提方法的可行


性与有效性
。


针对问题２
，本文提出利用大语言模型简化复杂场景知识
，
并进
一步优化推


理方案
，开展了基于场景知识的指称表达理解算法研宄工作
。本文从数据和模型


两个角度解决外部场景知识复杂的问题
。从数据层面
，本文结合大语言模型技术


简化场景知识
，通过大模型强大的指令遵循能力构建提示模板
，过滤场景知识中


与指称物体无关的描述
，减少推理过程中的干扰
。从模型层面
，提出
ＳＫＲＮ模型


将输入视作图片
、指称表达、场景知识三元组
，分别对其进行编码和融合
，
通过


注意力机制分别处理指称表达和场景知识
，提取场景知识中关键的描述信息
，提


高模型的推理效果
。本文通过定量和定性实验
，验证了所提方法的可行性与有效


性
。


综上所述
，
本文研宄工作的主要贡献总结如下
：


１
）提出了
一个语言引导的指称表达理解推理网络ＬＧＲ
－ＮＥＴ
。


２
）提出了结合大模型简化指称表达中复杂场景知识的技术方案
。


３）提出了
一个面向场景知识的指称表达理解推理网络ＳＫＲＮ
。


５９


北京邮电大学硕士学位论文


５
．２未来工作展望


尽管本文对面向复杂文本的指称表达理解开展了研究
，并取得了
一定的研究


成果
。经过对模型方法和实验结果进
一步的观察和分析发现
，本文的研究思路和


方法细节仍值得进
一步探索和挖掘
：


１
）
如何显式挖掘指称表达中的空间描述并与跨模态推理结合？


本文提出的ＬＧＲ
－ＮＥＴ模型提取了指称表达中的关于目标物体空间位置的描


述
，
并将其编码成坐标向量用于增强预测
ｔｏｋｅｎ的空间表征
。
然而由于缺乏显式


的监督信号
，这部分的学习依赖于模型整体的端到端训练
。这导致这种方法
一定


程度上还是偏黑箱性质的
，
可解释性不强
。
因此
，
显示挖掘指称表达中的空间描


述并对相应空间区域的图像进行特征强化
，再应用于跨模态推理中是
一个潜在的


改进方向
。


２
）
如何进
一步提升基于场景知识的指称表达理解任务性能？


通过对比
ＳＫ
－ＲＥＣ和传统ＲＥＣ各自的最佳方法
，
可以发现无论是在原始的


ＳＫ
－ＶＧ数据集或者是本文提出的简化版Ｃ
－ＳＫ
－ＶＧ数据集上
，
当前方法的表现还


有很大的提升空间
。基于
ＳＫ
－ＲＥＣ场景知识指代复杂
、
图片中多实体的特点
，
可


以结合ＮＬＰ领域的依存分析
、实体抽取
，
ＣＶ领域强大的检测或分割模型分别对


文本
、
图像进行更细粒度的特征提取
，
再结合树
、
图等适用于实体间关系建模的


方案设计更细致的推理方案是
一个潜在的优化方向
。


除了本文开展的研究工作之外
，指称表达理解任务仍然有很多有价值且具有


挑战性的方向值得研究
：


１）如何打造更通用的指称表达理解系统？


当前的指称表达理解工作都是基于基准数据集上的评估比较性能的优劣
，在


现实应用中的表现尚且未知
，
如何打造现实应用中通用的ＲＥＣ方法是
一个值得


研究的方向
。
从图像角度来看
，
如何在不同图像类型上实现准确定位
，
如动画
、


游戏
、草图等
。从文本角度来看
，
当前的方法都基于指称表达明确指向图片中某


个物体这
一前提
，
而当指称表达指向图片中的多个物体
，或者指称表达非法
，
没


有指向图片中的任何目标物体时
，如何设计模型同时解决此类场景
。如何打造更


通用的ＲＥＣ系统
，
上述关键问题是需要解决的
。


指称表达理解任务作为多模态人工智能的重要研究方向之
一
，其仍然是
一个


充满挑战的任务
，
需要科研工作者持续探索和不断尝试
。


６０


参考文献


参考文献


［
１
］ＬｅＣｕｎＹ
，Ｂｅｎｇ
ｉｏＹ
，ＨｉｎｔｏｎＧ
．Ｄｅｅｐ
ｌｅａｍｉｎｇ［Ｊ］
．Ｎａｔｕｒｅ
，２０１５
，５２１
（７５５３）
：４３６
－４４４
．


［２
］ＫｒｉｚｈｅｖｓｋｙＡ
，ＳｕｔｓｋｅｖｅｒＩ
，ＨｉｎｔｏｎＧＥ
．Ｉｍａｇｅｎｅｔｃｌａｓｓｉｆ
ｉｃａｔ
ｉｏｎｗｉｔｈｄｅｅｐｃｏｎｖｏｌｕｔｉｏｎａｌｎｅｕｒａｌ


ｎｅｔｗｏｒｋｓ
［Ｊ
］
．ＡｄｖａｎｃｅｓｉｎＮｅｕｒａｌＩｎｆｏｒｍａｔｉｏｎＰｒｏｃｅｓｓｉｎｇＳｙｓｔｅｍｓ
，２０
１２
，２５
：１
１０６
－
１
１
１４
．


［３
］徐守坤
，倪楚涵
，
吉晨晨
，李宁
．基于ＹＯＬＯｖ３的施工场景安全帽佩戴的图像描述［Ｊ］
．计


算机科学，２０２０
，４７（０８）
：２３３
－２４０．


［４
］沈佳敏
，鲍秉坤
．基于深度学习的广告布局图片美学属性评价［Ｊ
］
．计算机技术与发展
，


２０２１
，３１
（０３
）
：３９
－４４
．


［５
］陈悦
，郭宇
，谢圆琰
，等
．基于图像描述算法的离线盲人视觉辅助系统
［Ｊ
］
．
电信科学
，


２０２２
，３８（１
）
：６
１
－７２
．


［６
］杨二斌，王永刚，杨翠萍
．基于模糊逻辑的铁路货车标识喷涂机器人视觉导航研究
［Ｊ
］
．自动


化技术与应用
＞２０２４
，４３（０４
；）
：３８
－４２
．


［７
］张瑞
，蒋婉玥
．基于视觉跟踪与自主导航的移动机器人目标跟随系统
［Ｊ］
．工程设计学


报，２０２３
，３０（０６
）
：６８７
－６９６
．


［８
］ＲｅｎＳ
５ＨｅＫ
，ＧｉｒｓｈｉｃｋＲ
，ｅｔａｌ
．Ｆａｓｔｅｒｒ
－ｃｎｎ
：Ｔｏｗａｒｄｓｒｅａｌ
－ｔ
ｉｍｅｏｂｊｅｃｔｄｅｔｅｃｔｉｏｎｗｉｔｈｒｅｇ
ｉｏｎ


ｐｒｏｐｏｓａｌｎｅｔｗｏｒｋｓ
［Ｊ
］
．ＡｄｖａｎｃｅｓｉｎＮｅｕｒａｌＩｎｆｏｒｍａｔｉｏｎＰｒｏｃｅｓｓｉｎｇＳｙｓｔｅｍｓ
，２０１５
，２８
：９１
－９９
．


［９］Ｕｉ
ｊ
ｌｉｎｇｓＪＲＲ
，ＶａｎＤｅＳａｎｄｅＫＥＡ
，ＧｅｖｅｒｓＴ
，ｅｔａｌ
．Ｓｅｌｅｃｔｉｖｅｓｅａｒｃｈｆｏｒｏｂｊｅｃｔｒｅｃｏｇｎｉｔｉｏｎ
［Ｊ］
，


Ｉｎｔｅｒ
ｎａｔｉｏｎａｌＪｏｕｒｎａｌｏｆＣｏｍｐｕｔｅｒＶｉｓｉｏｎ
，２０１３
，１０４：１５４
－１７１
．


［
１０
］ＲｏｈｒｂａｃｈＡ
，ＲｏｈｒｂａｃｈＭ
，ＨｕＲ，ｅｔａｌ
．Ｇｒｏｕｎｄｉｎｇｏｆｔｅｘｔｕａｌｐｈｒａｓｅｓｉｎｉｍａｇｅｓｂｙ


ｒｅｃｏｎｓｔｒｕｃｔｉｏｎ
［Ｃ］／／ＥｕｒｏｐｅａｎＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎ．Ｓｐｒ
ｉｎｇｅｒ
，２０１６
：８１７
－８３４
．


［
１
１
］ＮａｇａｒａｊａＶＫ
，Ｍｏｒａｒ
ｉｕＶＩ
，ＤａｖｉｓＬＳ
．Ｍｏｄｅｌｉｎｇｃｏｎｔｅｘｔｂｅｔｗｅｅｎｏｂｊｅｃｔｓｆｏｒｒｅｆｅｒｒｉｎｇ


ｅｘｐｒｅｓｓｉｏｎｕｎｄｅｒｓｔａｎｄｉｎｇ［Ｃ］／／Ｅｍ
＊
ｏｐｅａｊａＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎ
．Ｓｐｒ
ｉｎｇｅｒ
，２０
１６
：７９２
－


８０７
．


［１２
］ＹｕＬ
，ＰｏｉｒｓｏｎＰ
，ＹａｎｇＳ
，ｅｔａｌ
．Ｍｏｄｅｌｉｎｇｃｏｎｔｅｘｔｉｎｒｅｆｅｒｒｉｎｇｅｘｐｒｅｓｓｉｏｎｓ
［Ｃ
］／／Ｅｕｒｏｐｅａｎ


ＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎ．Ｓｐｒｉｎｇｅｒ
，２０１６
：６９
－８５
．


［１３
］ＨｕＲ
？ＲｏｈｒｂａｃｈＭ
，ＡｎｄｒｅａｓＪ
９ｅｔａｌ
．Ｍｏｄｅｌｉｎｇｒｅｌａｔｉｏｎｓｈｉｐｓｉｎｒｅｆｅｒｅｎｔｉａｌｅｘｐｒｅｓｓｉｏｎｓｗｉｔｈ


ｃｏｍｐｏｓｉｔｉｏｎａｌｍｏｄｕｌａｒｎｅｔｗｏｒｋｓ［Ｃ
］
／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒ


ＶｉｓｉｏｎａｎｄＰａｔｅｒｎＲｅｃｏｇｎｉｔｉｏｎ
．２０１７
：４４１８
－４４２７
．


［
１４
］ＹｕＬ
，ＬｉｎＺ
，ＳｈｅｎＸ
，ｅｔａｌ
．Ｍａｔｔｎｅｔ
：Ｍｏｄｕｌａｒａｔｔｅｎｔｉｏｎｎｅｔｗｏｒｋｆｏｒｒｅｆｅｒｒｉｎｇｅｘｐｒｅｓｓｉｏｎ


ｃｏｍｐｒｅｈｅｎｓｉｏｎ［Ｃ
］
／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒ＼＾ｓｉｏｎａｎｄＰａｔｅｒｎ


Ｒｅｃｏｇｎｉｔｉｏｎ．２０
１８
：１３０７
－
１３１５
．


［
１５
］ＷａｎｇＰ
，ＷｕＱ，ＣａｏＪ
，
ｅｔａｌ
．Ｎｅｉｇｈｂｏｕｒｈｏｏｄｗａｔｃｈ
：Ｒｅｆｅｒｒ
ｉｎｇｅｘｐｒｅｓｓｉｏｎｃｏｍｐｒｅｈｅｎｓｉｏｎｖｉａ


６
１


北京邮电大学硕士学位论文


ｌａｎｇｕａｇｅ
－
ｇｕｉｄｅｄｇｒａｐｈａｔｔｅｎｔｉｏｎｎｅｔｗｏｒｋｓ
［Ｃ
］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎ


ＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄＰａｔｔｅｒｎＲｅｃｏｇｎｉｔｉｏｎ．２０１９
：１９６０
－１９６８
．


［
１６
］ＹａｎｇＳ
，ＬｉＧ，ＹｕＹ．Ｄｙｎａｍｉｃｇｒａｐｈａｔｔｅｎｔｉｏｎｆｏｒｒｅｆｅｒｒ
ｉｎｇｅｘｐｒｅｓｓｉｏｎ


ｃｏｍｐｒｅｈｅｎｓｉｏｎ［Ｃ
］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＩＥＥＥＩｎｔｅｒ
ｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎ
．


２０１９
：４６４４
－４６５３
．


［１７
］ＲｅｄｍｏｎＪ
，ＦａｒｈａｄｉＡ
．Ｙｏｌｏｖ３
：Ａｎｉｎｃｒｅｍｅｎｔａｌｉｍｐｒｏｖｅｍｅｎｔ［Ｊ］
．ａｒＸｉｖｐｒｅｐｒｉｎｔ


ａｒＸｉｖ：１８０４
．０２７６７
，２０１８
．


［
１８
］ＹａｎｇＺ
，ＧｏｎｇＢ
，ＷａｎｇＬ
，ｅｔａｌ
．Ａｆａｓｔａｎｄａｃｃｕｒａｔｅｏｎｅ
－ｓｔａｇｅａｐｐｒｏａｃｈｔｏｖｉｓｕａｌ


ｇｒｏｕｎｄｉｎｇ［Ｃ
］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＩＥＥＥＩｎｔｅｒ
ｎａｔ
ｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎ．２０１９
：


４６８３
－４６９３
．


［
１９
］ＬｉａｏＹ
，ＬｉｕＳ
，ＬｉＧ
，
ｅｔａｌ
．Ａｒｅａｌ
－ｔ
ｉｍｅｃｒｏｓｓ
－ｍｏｄａｌｉｔｙｃｏｒｒｅｌａｔｉｏｎｆｉｌｔｅｒ
ｉｎｇｍｅｔｈｏｄｆｏｒｒｅｆｅｒｒ
ｉｎｇ


ｅｘｐｒｅｓｓｉｏｎｃｏｍｐｒｅｌｉｅｎｓｉｏｎ［Ｃ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆ
ｔｈｅＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄ


ＰａｔｔｅｒｎＲｅｃｏｇｎｉｔ
ｉｏｎ．２０２０
：１０８８０
－１０８８９
．


［２０］ＹａｎｇＺ
，ＣｈｅｎＴ
，ＷａｎｇＬ
，ｅｔａｌ．Ｉｍｐｒｏｖｉｎｇｏｎｅ
－ｓｔａｇｅｖｉｓｕａｌ
ｇｒｏｕｎｄｉｎｇｂｙｒｅｃｕｒｓｉｖｅｓｕｂ
－
ｑｕｅｒｙ


ｃｏｎｓｔｒｕｃｔｉｏｎ［Ｃ］
／／ＥｕｒｏｐｅａｎＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎ．Ｓｐｒｉｎｇｅｒ
，２０２０；３８７
＊４０４
．


［２
１
］ＳｕｎＭ
，ＸｉａｏＪ
，ＬｉｍＥＧ
．Ｉｔｅｒａｔｉｖｅｓｈｒｉｎｋｉｎｇｆｏｒｒｅｆｅｒｒｉｎｇｅｘｐｒｅｓｓｉｏｎｇｒｏｕｎｄｉｎｇｕｓｉｎｇｄｅｅｐ


ｒｅｉｎｆｏｒｃｅｍｅｎｔｌｅａｍｉｎｇ［Ｃ
］
／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄ


ＰａｔｔｅｒｎＲｅｃｏｇｎｉｔｉｏｎ．２０２１
：１４０６０
－１４０６９
．


［２２］ＤｏｓｏｖｉｔｓｋｉｙＡ
，ＢｅｙｅｒＬ
，ＫｏｌｅｓｎｉｋｏｖＡ
，ｅｔａｌ
．Ａｎｉｍａｇｅｉｓｗｏｒｆ
ｌｉ１６ｘ１６ｗｏｒｄｓ
：Ｔｒａｎｓｆｏｒｍｅｒｓ


ｆｏｒｉｍａｇｅｒｅｃｏｇｎｉｔ
ｉｏｎａｔｓｃａｌｅ［Ｊ
］
，ａｒＸｉｖ
ｐｒｅｐｒｉｎｔａｒＸｉｖ：２０１０
．
１
１９２９
，２０２０
．


［２３
］ＬｉｕＺ
，ＬｉｎＹ
，ＣａｏＹ
，ｅｔａｌ
．Ｓｗｉｎｔｒａｎｓｆｏｒｍｅｒ：Ｈｉｅｒａｒｃｈｉｃａｌｖｉｓｉｏｎｔｒａｎｓｆｏｒｍｅｒｕｓｉｎｇｓｈｉｆ
ｔｅｄ


ｗｉｎｄｏｗｓ［Ｃ］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔ
ｉｉｅＩＥＥＥＩｎｔｅｒ
ｎａｔ
ｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎ．２０２
１
：


１００１２
－１００２２
．


［２４
］ＣａｎｏｎＮ
，ＭａｓｓａＦ
，ＳｙｎｎａｅｖｅＧ
，ｅｔａｌ．Ｅｎｄ
－ｔｏ
－ｅｎｄｏｂｊｅｃｔｄｅｔｅｃｔｉｏｎｗｉｔｈ


ｔｒａｎｓｆｏｎｎｅｒｓ［Ｃ
］／／ＥｕｒｏｐｅａｎＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎ．Ｓｐｒｉｎｇｅｒ
，２０２０
：２１３
－２２９
．


［２５
］ＤｅｎｇＪ
，
ＹａｎｇＺ
，ＣｈｅｎＴ
，
ｅｔａｌ
．Ｔｒａｎｓｖｇ
：Ｅｎｄ
－ｔｏ
－ｅｎｄｖｉｓｕａｌｇｒｏｕｎｄｉｎｇｗｉｔｈ


ｔｒａｎｓｆｏｒｍｅｒｓ［Ｃ］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＩＥＥＥＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒ＾ｓｉｏｎ．２０２１
：


１７６９
－
１７７９
．


［２６］ＤｅｎｇＪ
，ＹａｎｇＺ
５ＬｉｕＤ
，ｅｔａｌ
．ＴｒａｎｓＶＧ
－Ｈ
－
：Ｅｎｄ
－ｔｏ
－ＥｎｄＭｓｕａｌＧｒｏｕｎｄｉｎｇＷｉｔｈＬｍｇｕａｇｅ


ＣｏｎｄｉｔｉｏｎｅｄＶｉｓｉｏｎＴｒａｎｓｆｏｒｍｅｒ＾］
．ＩＥＥＥＴｒａｎｓａｃｔｉｏｎｓｏｎＰａｔｔｅｒｎＡｎａｌｙｓｉｓａｎｄＭａｃｈｉｎｅ


Ｉｎｔｅｌｌｉｇｅｎｃｅ
，２０２３
，４５（
１
１）
：１３６３６
＊１３６５２
．


［２７
］ＹｅＪ
９ＴｉａｎＪ
，ＹａｎＭ
，ｅｔａｌ
．Ｓｈｉｆｔｉｎｇｍｏｒｅａｔｔｅｎｔ
ｉｏｎｔｏｖｉｓｕａｌｂａｃｋｂｏｎｅ
：Ｑｕｅｒｙ
－ｍｏｄｕｌａｔｅｄ


ｒｅｆｉｎｅｍｅｎｔｎｅｔｗｏｒｋｓｆｏｒｅｎｄ
－ｔｏ
－ｅｎｄｖｉｓｕａｌ
ｇｒｏｕｎｄｉｎｇ［Ｃ］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＩＥＥＥＣｏｎｆｅｒｅｎｃｅ


ｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄＰａｔｔｅｒｎＲｅｃｏｇｎｉｔｉｏｎ
．２０２２
：１５５０２
－１５５
１２
．


６２


参考文献


［２８
］ＹａｎｇＬ
，ＸｕＹ
５ＹｕａｎＣ，ｅｔａｌ
．Ｉｍｐｒｏｖｉｎｇｖｉｓｕａｌ
ｇｒｏｕｎｄｉｎｇｗｉｔｈｖｉｓｕａｌ
－ｌｉｎｇｕｉｓｔｉｃｖｅｒｉｆｉｃａｔｉｏｎａｎｄ


ｉｔｅｒａｔｉｖｅｒｅａｓｏｎｉｎｇ［Ｃ
］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄＰａｔｔｅｒｎ


Ｒｅｃｏｇｎｉｔｉｏｎ
．２０２２
：９４９９
－９５０８
．


［２９
］ＬｉＭ
，ＳｉｇａｌＬ．Ｒｅｆｅｒｒｉｎｇｔｒａｎｓｆｏｒｍｅｒ：Ａｏｎｅ
－ｓｔｅｐａｐｐｒｏａｃｈｔｏｍｕｌｔｉ
－ｔａｓｋｖｉｓｕａｌ
ｇｒｏｕｎｄｉｎｇ［Ｊ
］
．


Ａｄｖａｎｃｅｓｉｎｎｅｕｒａｌｉｎｆｏｒｍａｔｉｏｎ
ｐｒｏｃｅｓｓｉｎｇｓｙｓｔｅｍｓ，２０２１
，
３４：
１９６５２
－
１９６６４
．


［３０
］ＳｕＷ
，ＭｉａｏＰ
，ＤｏｕＨ
，ｅｔａｌ
．Ｌａｎｇｕａｇｅａｄａｐｔｉｖｅｗｅｉｇｈｔｇｅｎｅｒａｔｉｏｎｆｏｒｍｕｌｔｉ
－
ｔａｓｋｖｉｓｕａｌ


ｇｒｏｕｎｄｉｎｇ［Ｃ
］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄＰａｔｔｅｒｎ


Ｒｅｃｏｇｎｉｔｉｏｎ
．２０２３
：１０８５７
－
１０８６６
．


［３
１
］ＫａｍａｔｈＡ
，ＳｉｎｇｈＭ
５ＬｅＣｕｎＹ
，ｅｔａｌ
．Ｍｄｅｔｒ
－ｍｏｄｕｌａｔｅｄｄｅｔｅｃｔｉｏｎｆｏｒｅｎｄ
－ｔｏ
－ｅｎｄｍｕｌｔｉ
－ｍｏｄａｌ


ｕｎｄｅｒｓｔａｎｄｉｎｇ［Ｃ
］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＩＥＥＥＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎ
．


２０２
１
：１７８０
－１７９０
．


［３２
］ＷａｎｇＰ
，ＹａｎｇＡ
，ＭｅｎＲ
，ｅｔａｌ
．Ｏｆａ：Ｕｎｉｆｙ
ｉｎｇａｒｃｈｉｔｅｃｔｕｒｅｓ
，ｔａｓｋｓ，ａｎｄｍｏｄａｌｉｔｉｅｓｔｈｒｏｕｇｈａ


ｓｉｍｐ
ｌｅｓｅｑｕｅｎｃｅ
－ｔｏ
－ｓｅｑｕｅｎｃｅｌｅａｒｎｉｎｇ
ｆｒａｍｅｗｏｒｋ
［Ｃ
］／／ＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＭａｃｈｉｎｅ


Ｌｅａｒｎｉｎｇ
．ＰＭＬＲ
，２０２２
：２３３
１８
－２３３４０
．


［３３
］ＳｕＷ
，ＺｈｕＸ
，ＣａｏＹ
，ｅｔａｌ
．Ｖｌ
－ｂｅｒｔ
：Ｐｒｅ
－ｔｒａｉｎｉｎｇｏｆｇｅｎｅｒ
ｉｃｖｉｓｕａｌ
－
ｌｉｎｇｕｉｓｔｉｃ


ｒｅｐｒｅｓｅｎｔａｔ
ｉｏｎｓ
［Ｃ
］／／ＩｎｔｅｍａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＬｅａｒｎｉｎｇＲｅｐｒｅｓｅｎｔａｔｉｏｎｓ
．２０１９
．


［３４
］ＬｉＬＨ
，ＹａｔｓｋａｒＭ
，Ｄ
，ｅｔａｌ
．Ｖｉｓｕａｌｂｅｒｔ：Ａｓｉｍｐ
ｌｅａｎｄ
ｐｅｒｆｏｒｍａｎｔｂａｓｅｌｉｎｅｆｏｒｖｉｓｉｏｎａｎｄ


ｌａｎｇｕａｇｅ
［Ｊ］
．ａｒＸｉｖ
ｐｒｅｐｒｉｎｔａｒＸｉｖ：
１９０８
．０３５５７，２０１９
．


［３５
］ＲａｄｆｏｒｄＡ
，ＫｉｍＪＷ
，ＨａｌｌａｃｙＣ
，ｅｔａｌ
．Ｌｅａｒｎｉｎｇｔｒａｎｓｆｅｒａｂｌｅｖｉｓｘｉａｌｍｏｄｅｌｓｆ
ｒｏｍｎａｔｕｒａｌ


ｌａｎｇｕａｇｅｓｕｐｅｒｖｉｓｉｏｎ
［Ｃ］／／ＩｎｔｅｍａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＭａｃｈｉｎｅＬｅａｒｎｉｎｇ
．ＰＭＬＲ
，２０２１
：８７４８
－


８７６３
．


［３６］ＬｉＪ
，ＬｉＤ
，ＸｉｏｎｇＣ
，ｅｔａｌ
．Ｂｌｉｐ
：Ｂｏｏｔｓｔｒａｐｐ
ｉｎｇ
ｌａｎｇｕａｇｅ
－
ｉｍａｇｅ
ｐｒｅ
－
ｔｒａｉｎｉｎｇｆｏｒｕｎｉｆ
ｉｅｄｖｉｓｉｏｎ
－


ｌａｎｇｕａｇｅｕｎｄｅｒｓｔａｎｄｉｎｇａｎｄｇｅｎｅｒａｔｉｏｎ
［Ｃ
］
／／ＩｎｔｅｍａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＭａｃｈｉｎｅＬｅａｒｎｉｎｇ
．


ＰＭＬＲ
，２０２２
：１２８８８
－１２９００
．


［３７
］ＬｉＪ
，ＬｉＤ
ｓＳａｖａｒｅｓｅＳ
，ｅｔａｌ
．Ｂｌｉｐ
－２
：Ｂｏｏｔｓｔｒａｐｐ
ｉｎｇ
ｌａｎｇｕａｇｅ
－
ｉｍａｇｅｐｒｅ
－ｔｒａｉｎｉｎｇｗｉｔｈｆ
ｒｏｚｅｎ


ｉｍａｇｅｅｎｃｏｄｅｒｓａｎｄｌａｒｇｅｌａｎｇｕａｇｅｍｏｄｅｌｓ［Ｊ
］
．ａｒＸｉｖ
ｐｒｅｐｒ
ｉｎｔａｒＸｉｖ
：２３０１
．１２５９７
，２０２３
．


［３８
］ＺｈｕＤ
，ＣｈｅｎＪ
，ＳｈｅｎＸ
５ｅｔａｌ
．Ｍｉｎｉｇｐｔ
－４
：Ｅｎｈａｎｃｉｎｇｖｉｓｉｏｎ
－
ｌａｎｇｕａｇｅｕｎｄｅｒｓｔａｎｄｉｎｇｗｉｔｈ


ａｄｖａｎｃｅｄｌａｒｇｅｌａｎｇｕａｇｅｍｏｄｅｌｓ［Ｊ］
．ａｒＸｉｖ
ｐｒｅｐｒｉｎｔａｒＸｉｖ
：２３０４
．
１０５９２
，２０２３
．


［３９
］ＫａｚｅｍｚａｄｅｈＳ
，ＯｒｄｏｎｅｚＶ
５ＭａｔｔｅｎＭ
，ｅｔａｌ
．Ｒｅｆｅｒ
ｉｔｇａｍｅ
：Ｒｅｆｅｒｒｉｎｇｔｏｏｂ
ｊｅｃｔｓｉｎ
ｐｈｏｔｏｇｒａｐｈｓ


ｏｆｎａｔｕｒａｌｓｃｅｎｅｓ［Ｃ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２０１４ＣｏｎｆｅｒｅｎｃｅｏｎＥｍｐ
ｉｒ
ｉｃａｌＭｅｔｈｏｄｓｉｎＮａｔｕｒａｌ


ＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ
．２０１４
：７８７
－７９８
．


［４０
］ＳｏｎｇＹ
，
ＺｈａｎｇＲ
ｊＣｈｅｎＺ
，
ｅｔａｌ
．ＡｄｖａｎｃｉｎｇＶｉｓｕａｌＧｒｏｕｎｄｉｎｇＷｉｔｈＳｃｅｎｅＫｎｏｗｌｅｄｇｅ
：


ＢｅｎｃｈｍａｒｋａｎｄＭｅｔｈｏｄ
［Ｃ
］
／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄ


ＰａｔｅｒｎＲｅｃｏｇｎｉｔｉｏｎ
．２０２３
：１５０３９
－１５０４９．


６３


北京邮电大学硕士学位论文


［４１
］ＨｅＫ
，ＺｈａｎｇＸ
，ＲｅｎＳ
，ｅｔａｌ
．Ｄｅｅｐｒｅｓｉｄｕａｌｌｅａｒｎｉｎｇｆｏｒｉｍａｇｅｒｅｃｏｇｎｉｔｉｏｎ
［Ｃ］
／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆ


ｔｈｅＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄＰａｔｅｒｎＲｅｃｏｇｎｉｔ
ｉｏｎ．２０１６
：７７０
－７７８
．


［４２
］ＣｈｒｅｎＷＡ
．Ｏｎｅ
－ｈｏｔｒｅｓｉｄｕｅｃｏｄｉｎｇｆｏｒｌｏｗｄｅｌａｙ
－ｐｏｗｅｒｐｒｏｄｕｃｔＣＭＯＳｄｅｓｉｇｎ
［Ｊ］
．ＩＥＥＥ


Ｔｉ＾ｎｓａｃｔ
ｉｏｎｓｏｎＣｉｒｃｕｉｔｓａｎｄＳｙｓｔｅｍｓＩＩ
：ＡｎａｌｏｇａｎｄＤｉｇ
ｉｔａｌＳｉｇｎａｌＰｒｏｃｅｓｓｉｎｇ，１９９８
，４５（３）
：


３０３
－３
１３
．


［４３
］ＭｉｋｏｌｏｖＴ
，ＣｈｅｎＫ
，ＣｏｒｒａｄｏＧ
，ｅｔａｌ
．Ｅｆｆｉｃｉｅｎｔｅｓｔｉｍａｔｉｏｎｏｆｗｏｒｄｒｅｐｒｅｓｅｎｔａｔｉｏｎｓｉｎｖｅｃｔｏｒ


ｓｐａｃｅ［Ｊ］
．ａｒＸｉｖ
ｐｒｅｐｒｉｎｔａｒＸｉｖ：１３０
１
．３７８
１
，２０１３
．


［４４
］ＭｉｋｏｌｏｖＴ
，ＳｕｔｓｋｅｖｅｒＩ
，ＣｈｅｎＫ
，ｅｔａｌ
．Ｄｉｓｔｒ
ｉｂｕｔｅｄｒｅｐｒｅｓｅｎｔａｔ
ｉｏｎｓｏｆｗｏｒｄｓａｎｄ
ｐｈｒａｓｅｓａｎｄ


ｔｈｅｉｒｃｏｍｐｏｓｉｔ
ｉｏｎａｉｉｔｙ［Ｊ］
．ＡｄｖａｎｃｅｓｉｎＮｅｕｒａｌＩｎｆｏｒｍａｔ
ｉｏｎＰｒｏｃｅｓｓｉｎｇＳｙｓｔｅｍｓ
，２０１３，２６（２）
：


３
１
１
１
－３１１９
．


［４５
］Ｈｕｆｆ
ｉｎａｎＤＡ
．Ａｍｅｔ
ｉｉｏｄｆｏｒｔｈｅｃｏｎｓｔｒｕｃｔｉｏｎｏｆｍｉｎｉｍｕｍ
－ｒｅｄｕｎｄａｎｃｙｃｏｄｅｓ［Ｊ］
．Ｐｒｏｃｅｅｄｉｎｇｓ


ｏｆｔｈｅＩＲＥ
，１９５２
，４０（９）
：１０９８
－１
１０１
．


［４６
］ＰｅｎｎｉｎｇｔｏｎＪ
，ＳｏｃｈｅｒＲ
，ＭａｎｎｉｎｇＣＤ
．Ｇｌｏｖｅ：Ｇｌｏｂａｌｖｅｃｔｏｒｓｆｏｒｗｏｒｄ


ｒｅｐｒｅｓｅｎｔａｔｉｏｎ［Ｃ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２０１４ＣｏｎｆｅｒｅｎｃｅｏｎＥｍｐ
ｉｒｉｃａｌＭｅｔｈｏｄｓｉｎＮａｔｕｒａｌ


ＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ
．２０１４
：１５３２
－１５４３
．


［４７
］ＶａｓｗａｎｉＡ，ＳｈａｚｅｅｒＮ
，ＰａｒｍａｒＮ
，ｅｔａｌ
．Ａｔｔｅｎｔｉｏｎｉｓａｌｌｙｏｕｎｅｅｄ
［Ｊ
］
．ＡｄｖａｎｃｅｓｉｎＮｅｕｒａｌ


ＩｎｆｏｒｍａｔｉｏｎＰｒｏｃｅｓｓｉｎｇＳｙｓｔｅｍｓ，２０１７，
３０
：５９９８
－６００８


［４８
］ＨｏｃｈｒｅｉｔｅｒＳ
，ＳｃｈｍｉｄｈｕｂｅｒＪ
．Ｌｏｎｇｓｈｏｒｔ
－
ｔｅｒｍｍｅｍｏｒｙ［Ｊ］
，ＮｅｕｒａｌＣｏｍｐｕｔａｔｉｏｎ
，
１９９７
，９
（８
）
；


１７３５
－１７８０
．


［４９
］ＤｅｖｌｉｎＪ
，ＣｈａｎｇＭＷ
，ＬｅｅＫ
，ｅｔａｌ
．Ｂｅｒｔ
：Ｐｒｅ
－ｔｉａｉｎｉｎｇｏｆｄｅｅｐｂｉｄｉｒｅｃｔ
ｉｏｎａｌｔｒａｎｓｆｏｒｍｅｒｓｆｏｒ


ｌａｎｇｕａｇｅｕｎｄｅｒｓｔａｎｄｉｎｇ［Ｊ］
．ａｒＸｉｖ
ｐｒｅｐｒｉｎｔａｒＸｉｖ
：１８
１０
．０４８０５
，２０
１８
．


［５０
］ＬｉｕＹ
，ＯｔｔＭ
，ＧｏｙａｌＮ
，ｅｔａｌ
．Ｒｏｂｅｒｔａ：Ａ
ｒｏｂｕｓｔｌｙｏｐｔｉｍｉｚｅｄｂｅｒｔ
ｐｒｅｔｒａｉｎｉｎｇａｐｐｒｏａｃｈ
［Ｊ］
，ａｒＸｉｖ


ｐｒｅｐｒｉｎｔａｒＸｉｖ
：１９０７
．１
１６９２
，２０１９
．


［５
１
］ＬａｎＺ
，ＣｈｅｎＭ
，ＧｏｏｄｍａｎＳ
，ｅｔａｌ．Ａｌｂｅｒｔ：Ａｌｉｔｅｂｅｒｔｆｏｒｓｅｌｆ
－ｓｕｐｅｒｖｉｓｅｄｌｅａｒｎｉｎｇｏｆ
ｌａｎｇｕａｇｅ


ｒｅｐｒｅｓｅｎｔａｔ
ｉｏｎｓ［Ｊ
］
．ａｒＸｉｖ
ｐｒｅｐｒｉｎｔａｒＸｉｖ
：１９０９
．
１
１９４２
，２０１９
．


［５２
］Ｒａｄｆｏｒｄ
Ａ
，ＮａｒａｓｉｍｈａｎＫ
，ＳａｌｉｍａｎｓＴ
，ｅｔａｌ．Ｉｍｐｒｏｖｉｎｇ
ｌａｎｇｕａｇｅｕｎｄｅｒｓｔａｎｄｉｎｇｂｙｇｅｎｅｒａｔ
ｉｖｅ


ｐｒｅ
－ｔｒａｉｎｉｎｇ［Ｊ
］
．ＯｐｅｎＡＩｂｌｏｇ，２０１８
，１（８）
：９
．


［５３
］ＲａｄｆｏｒｄＡ
，ＷｕＪ
５ＣｈｉｌｄＲ
，ｅｔａｌ
．ＬａｎｇｕａｇｅｍｏｄｅｌｓａｒｅｕｎｓｕｐｅｒｖｉｓｅｄｍｕｌｔｉｔａｓｋＩｅａｍｅｒｓ
［Ｊ］
．


ＯｐｅｎＡＩｂｌｏｇ，２０１９
，１（８
）
：９
．


［５４］ＢｒｏｗｎＴ
，ＭａｎｎＢ
，ＲｙｄｅｒＮ
，ｅｔａｌ
．Ｌａｎｇｕａｇｅｍｏｄｅｌｓａｒｅｆｅｗ
－ｓｈｏｔｌｅａｍｅｒｓ
［Ｊ
］
．Ａｄｖａｎｃｅｓｉｎ


ＮｅｕｒａｌＩｎｆｏｒｍａｔｉｏｎＰｒｏｃｅｓｓｉｎｇＳｙｓｔｅｍｓ
，２０２０
，３３
：１８７７
－
１９０１
．


［５５
］ＯｕｙａｎｇＬ
，ＷｕＪ
，ＪｉａｎｇＸ
，ｅｔａｌ
．Ｔｒａｉｎｉｎｇ
ｌａｎｇｕａｇｅｍｏｄｅｌｓｔｏｆｏｌｌｏｗｉｎｓｔｒｕｃｔｉｏｎｓｗｉｔｈｈｕｍａｎ


ｆｅｅｄｂａｃｋ
［Ｊ］
．ＡｄｖａｎｃｅｓｉｎＮｅｘｉｒａｌＩｎｆｏｒｍａｔｉｏｎＰｒｏｃｅｓｓｉｎｇＳｙｓｔｅｍｓ，２０２２
，
３５
：２７７３０
－２７７４４
．


［５６
］ＴｏｕｖｒｏｎＨ
，ＬａｖｒｉｌＴ
，ＩｚａｃａｒｄＧ
，ｅｔａｌ
．Ｌｌａｍａ
：Ｏｐｅｎａｎｄｅｆ
ｉｃｉｅｎｔｆｏｕｎｄａｔｉｏｎｌａｎｇｕａｇｅｍｏｄｅｌｓ［Ｊ］
．


６４


参考文献


ａｒＸｉｖ
ｐｒｅｐｒ
ｉｎｔａｒＸｉｖ
：２３０２
．
１３９７
１
，２０２３
．


［５７
］ＴｏｕｖｒｏｎＨ
，ＭａｒｔｉｎＬ
，ＳｔｏｎｅＫ
，ｅｔａｌ
．Ｌｌａｍａ２
：Ｏｐｅｎｆｏｘｍｄａｔｉｏｎａｎｄｆｉｎｅ
－ｔｕｎｅｄｃｈａｔｍｏｄｅｌｓ［Ｊ
］
．


ａｒＸｉｖ
ｐｒｅｐｒｉｎｔａｒＸｉｖ
：２３０７
．０９２８８
，２０２３
．


［５８
］ＤｕＺ
，Ｑ
ｉａｎＹ
，ＬｉｕＸ
，ｅｔａｌ
．Ｇｌｍ
：Ｇｅｎｅｒａｌｌａｎｇｕａｇｅｍｏｄｅｌ
ｐｒｅｔｒａｉｎｉｎｇｗｉｔｈａｕｔｏｒｅｇｒｅｓｓｉｖｅｂｌａｎｋ


ｉｎｆ
ｉｌｌｉｎｇ［Ｊ
］
．ａｒＸｉｖ
ｐｒｅｐｒ
ｉｎｔａｒＸｉｖ：２１０３
．
１０３６０
，２０２１
．


［５９
］ＬｏｗｅＤＧ
．Ｄｉｓｔｉｎｃｔｉｖｅｉｍａｇｅｆｅａｔｕｒｅｓｆ
ｒｏｍｓｃａｌｅ
－
ｉｎｖａｒ
ｉａｎｔｋｅｙｐｏｉｎｔｓ［Ｊ
］
．ＩｎｔｅｒｎａｔｉｏｎａｌＪｏｕｒｎａｌ


ｏｆＣｏｍｐｕｔｅｒＶｉｓｉｏｎ
，２００４
，６０
：９１
－
１
１０
．


［６０
］Ｈａｒｒ
ｉｓＣ
，ＳｔｅｐｈｅｎｓＭ
．Ａｃｏｍｂｉｎｅｄｃｏｍｅｒａｎｄｅｄｇｅｄｅｔｅｃｔｏｒ
［Ｃ］
／／ＡｌｖｅｙＶｉｓｉｏｎＣｏｎｆｅｒｅｎｃｅ
．


１９８８
，
１５（５０
）
：１０
－５２４４
．


［６１
］ＬｅＣｕｎＹ
，ＢｏｔｔｏｕＬ
，Ｂｅｎｇ
ｉｏＹ
，ｅｔａｌ
．Ｇｒａｄｉｅｎｔ
－ｂａｓｅｄｌｅａｒｎｉｎｇａｐｐ
ｌｉｅｄｔｏｄｏｃｕｍｅｎｔ


ｒｅｃｏｇｎｉｔｉｏｎ［Ｊ
］
．Ｐｒｏｃｅｅｄｉｎｇｓｏｆ
ｔｈｅＩＥＥＥ
，１９９８
，８６（１１
）
：２２７８
－２３２４
．


［６２
］ＳｉｍｏｎｙａｎＺｉｓｓｅｒｍａｎＡ．Ｖｅｒｙｄｅｅｐｃｏｎｖｏｌｕｔｉｏｎａｌｎｅｔｗｏｒｋｓｆｏｒｌａｒｇｅ
－ｓｃａｌｅｉｍａｇｅ


ｒｅｃｏｇｎｉｔｉｏｎ
［Ｊ
］
．ａｒＸｉｖ
ｐｒｅｐｒ
ｉｎｔａｒＸｉｖ
：
１４０９
．
１５５６
，２０１４
．


［６３
］ＳｚｅｇｅｄｙＣ
，ＬｉｕＷ
，
ＪｉａＹ
，
ｅｔａｌ
．Ｇｏｉｎｇｄｅｅｐｅｒｗｉｔｈｃｏｎｖｏｌｕｔｉｏｎｓ［Ｃ］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＩＥＥＥ


ＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄＰａｔｅｒｎＲｅｃｏｇｎｉｔｉｏｎ．２０１５
：
１
－９
．


［６４
］ＨｕａｎｇＧ
，
ＬｉｕＺ
，ＶａｎＤｅｒＭａａｔｅｎＬ
ｓｅｔａｌ
．Ｄｅｎｓｅｌｙｃｏｎｎｅｃｔｅｄｃｏｎｖｏｌｕｔｉｏｎａｌ


ｎｅｔｗｏｒｋｓ
［Ｃ］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄＰａｔｔｅｒｎ


Ｒｅｃｏｇｎｉｔｉｏｎ
．２０
１７
：４７００
－４７０８
．


［６５
］ＬｉｕＺ
，ＨｕＨ
，ＬｉｎＹ
，ｅｔａｌ
．Ｓｗｉｎｔｒａｎｓｆｏｒｍｅｒｖ２
：Ｓｃａｌｉｎｇｕｐｃａｐａｃｉｔｙａｎｄ


ｒｅｓｏｌｕｔｉｏｎ
［Ｃ］
／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄＰａｔｔｅｒｎ


Ｒｅｃｏｇｎｉｔｉｏｎ
．２０２２
：１２００９
－
１２０
１９
．


［６６
］ＨｅＫ
，
Ｇｋｉｏｘａｒ
ｉＧ
，Ｄｏｌｌ紅Ｐ
，
ｅｔａｌ
．Ｍａｓｋｒ
＿ｃｎｎ［Ｃ］
／／ＰｒｏｃｅｅｄｉａｇｓｏｆｔｈｅＩＥＥＥＩｎｔｅｒｎａｔｉｏｎａｌ


ＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＡ＾ｓｉｏｎ
．２０１７
：２９６
１
－２９６９
．


［６７
］ＢｏｃｈｋｏｖｓｋｉｙＡ
，ＷａｎｇＣＹ
，ＬｉａｏＨＹＭ
．Ｙｏｌｏｖ４
：Ｏｐｔ
ｉｍａｌｓｐｅｅｄａｎｄａｃｃｕｒａｃｙｏｆｏｂ
ｊｅｃｔ


ｄｅｔｅｃｔｉｏｎ［Ｊ］
．ａｒＸｉｖ
ｐｒｅｐｒｉｎｔａｒＸｉｖ
：２００４
．１０９３４
，２０２０
．


［６８
］ＫｕｈｎＨＷ．ＴｈｅＨｕｎｇａｒ
ｉａｎｍｅｔｈｏｄｆｏｒｔｈｅａｓｓｉｇｎｍｅｎｔ
ｐｒｏｂｌｅｍ
［Ｊ］
．ＮａｖａｌＲｅｓｅａｒｃｈＬｏｇ
ｉｓｔｉｃｓ


Ｑｕａｒｔｅｒｌｙ，１９５５
，２（１）
：８３
－９７
．


［６９
］ＭｅｎｇＤ
，ＣｈｅｎＸ
，ＦａｎＺ
，ｅｔａｌ
．Ｃｏｎｄｉｔ
ｉｏｎａｌｄｅｔｒｆｏｒｆａｓｔｔｒａｉｎｉｎｇｃｏｎｖｅｒｇｅｎｃｅ［Ｃ
］
／／Ｐｒｏｃｅｅｄｉｎｇｓ


ｏｆ
ｔｈｅＩＥＥＥＩｎｔｅｒｎａｔ
ｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎ
．２０２１
：３６５
１
－３６６０
．


［７０
］ＬｉｕＳ
，ＬｉＦ
，ＺｈａｎｇＨ
，ｅｔａｌ
．Ｄａｂ
－ｄｅｔｒ
：Ｄｙｎａｍｉｃａｎｃｈｏｒｂｏｘｅｓａｒｅｂｅｔｔｅｒｑｕｅｒ
ｉｅｓｆｏｒｄｅｔｒ［Ｊ］
．


ａｒＸｉｖ
ｐｒｅｐｒ
ｉｎｔａｒＸｉｖ
：２２０
１
．
１２３２９
，２０２２
．


［７１
］ＺｈｕＸ
，
ＳｕＷ
，ＬｕＬ
，
ｅｔａＬＤｅｆｏｒｍａｂｌｅｄｅｔｒ
：Ｄｅｆｏｒｍａｂｌｅｔｒａｎｓｆｏｒｍｅｒｓｆｏｒｅｎｄ
－ｔｏ
－ｅｎｄｏｂｊｅｃｔ


ｄｅｔｅｃｔｉｏｎ
［Ｊ
］
．ａｒＸｉｖ
ｐｒｅｐｒ
ｉｎｔａｒＸｉｖ
：２０
１０
．０４１５９
５２０２０
．


［７２
］ＭａｏＪ
，ＨｕａｎｇＪ
５ＴｏｓｈｅｖＡ
，ｅｔａｌ
．Ｇｅｎｅｒａｔｉｏｎａｎｄｃｏｍｐｒｅｈｅｎｓｉｏｎｏｆｕｎａｍｂｉｇｕｏｕｓｏｂ
ｊｅｃｔ


６５


北京邮电大学硕士学位论文


ｄｅｓｃｒ
ｉｐｔ
ｉｏｎｓ
［Ｃ
］
／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄＰａｔｔｅｒｎ


Ｒｅｃｏｇｎｉｔｉｏｎ．２０１６：１
１
－２０
．


［７３
］ＰｌｍｎｍｅｒＢＡ
，ＷａｎｇＬ
，ＣｅｒｖａｎｔｅｓＣＭ
５ｅｔａｌ
．Ｆｌｉｃｋｒ３０ｋｅｎｔｉｔｉｅｓ
：Ｃｏｌｌｅｃｔｉｎｇｒｅｇ
ｉｏｎ
－ｔｏ
－
ｐｈｒａｓｅ


ｃｏｒｒｅｓｐｏｎｄｅｎｃｅｓｆｏｒｒｉｃｈｅｒｉｍａｇｅ
－ｔｏ
－ｓｅｎｔｅｎｃｅｍｏｄｅｌｓ［Ｃ］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＩＥＥＥ


ＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎ．２０１５
：２６４１
－２６４９
．


［７４
］ＬｉｎＴ
Ｙ
，ＭａｉｒｅＭ
，Ｂｅｌｏｎｇ
ｉｅＳ
，ｅｔａｌ
．Ｍｉｃｒｏｓｏｆ
ｔｃｏｃｏ：Ｃｏｍｍｏｎｏｂｊｅｃｔｓｉｎｃｏｎｔｅｘｔ［Ｃ
］／／Ｅｕｒｏｐｅａｎ


ＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎ
．Ｓｐｒｉｎｇｅｒ
，２０１４
：７４０
－７５５
．


［７５
］ＥｓｃａｌａｎｔｅＨＪ
，ＨｅｒｎａｎｄｅｚＣＡ
，ＧｏｎｚａｌｅｚＪＡ
，ｅｔａｌ
．ＴｈｅｓｅｇｍｅｎｔｅｄａｎｄａｎｎｏｔａｔｅｄＬＡＰＲＴＣ
－


１２ｂｅｎｃｈｍａｒｋ［Ｊ
］
，Ｃｏｍｐｕｔｅｒ＼＾ｓｉｏｎａｎｄＩｍａｇｅＵｎｄｅｒｓｔａｎｄｉｎｇ，２０１０
，１
１４
（４
）
：４１９
－４２８
．


［７６
］ＹｏｕｎｇＰ
，ＬａｉＡ
，ＨｏｄｏｓｈＭ
，ｅｔａｌ．Ｆｒｏｍｉｍａｇｅｄｅｓｃｒ
ｉｐｔｉｏｎｓｔｏｖｉｓｕａｌｄｅｎｏｔａｔｉｏｎｓ
：Ｎｅｗ


ｓｉｍｉｌａｒｉｔｙｍｅｔｒｉｃｓｆｏｒｓｅｍａｎｔｉｃｉｎｆｅｒｅｎｃｅｏｖｅｒｅｖｅｎｔｄｅｓｃｒｉｐ
ｔｉｏｎｓ［Ｊ
］
．Ｔｒａｎｓａｃｔｉｏｎｓｏｆｔｈｅ


ＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ，２０１４
，２
：６７
－７８
．


［７７
］Ｒｅｚａｔｏｆ
ｉｇｈｉＨ
，ＴｓｏｉＮ
，ＧｗａｋＪＹ
，
ｅｔａｌ
．Ｇｅｎｅｒａｌｉｚｅｄｉｎｔｅｒｓｅｃｔ
ｉｏｎｏｖｅｒｕｎｉｏｎ
：Ａｍｅｔｒｉｃａｎｄａ


ｌｏｓｓｆｏｒｂｏｕｎｄｉｎｇｂｏｘｒｅｇｒｅｓｓｉｏｎ
［Ｃ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔ
ｉｉｅＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒ
Ｖｉｓｉｏｎ


ａｎｄＰａｔｅｒｎＲｅｃｏｇｎｉｔｉｏｎ．２０１９
：６５８
－６６６
．


［７８
］ＫｒｉｓｈｎａＲ
，ＺｈｕＹ
，ＧｒｏｔｈＯ
，ｅｔａｌ
．Ｖｉｓｕａｌｇｅｎｏｍｅ
：Ｃｏｎｎｅｃｔｉｎｇ
ｌ＾ｉｇｕａｇｅａｎｄｖｉｓｉｏｎｕｓｉｎｇ


ｃｒｏｗｄｓｏｕｒｃｅｄｄｅｎｓｅｉｍａｇｅａｎｎｏｔａｔ
ｉｏｎｓ
［Ｊ］
，ＩｎｔｅｒｎａｔｉｏｎａｌＪｏｕｒｎａｌｏｆＣｏｍｐｕｔｅｒＶｉｓｉｏｎ
，２０１７
，


１２３
：３２
－７３
．


［７９
］ＺｈａｎｇＨ
，ＮｉｕＹ
，ＣｈａｎｇＳＦ．Ｇｒｏｕｎｄｉｎｇｒｅｆｅｒｒｉｎｇｅｘｐｒｅｓｓｉｏｎｓｉｎｉｍａｇｅｓｂｙｖａｒ
ｉａｔｉｏｎａｌ


ｃｏｎｔｅｘｔ
ｔＣ］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒ

＂
Ｓ＾ｓｉｏｎａｎｄＰａｔｔｅｒｎＲｅｃｏｇｎｉｔｉｏｎ．


２０１８
：４１５８
－４１６６
．


［８０
］ＰｌｕｍｍｅｒＢＡ
，ＫｏｒｄａｓＰ
，ＫｉａｐｏｕｒＭＨ
，ｅｔａｌ
．Ｃｏｎｄｉｔｉｏｎａｌｉｍａｇｅ
－ｔｅｘｔｅｍｂｅｄｄｉｎｇｎｅｔｗｏｒｋｓ［Ｃ］／／


ＥｕｒｏｐｅａｎＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎ
．Ｓｐｒｉｎｇｅｒ
，２０
１８
：２４９
－２６４
．


［８１
］ＹｕＺ
，ＹｕＪ
，ＸｉａｎｇＣ
？ｅｔａｌ．Ｒｅｔｈｉｎｋｉｎｇｄｉｖｅｒｓｉｆｉｅｄａｎｄｄｉｓｃｒｉｍｉｎａｔ
ｉｖｅ
ｐｒｏｐｏｓａｌ
ｇｅｎｅｒａｔｉｏｎｆｏｒ


ｖｉｓｕａｌ
ｇｒｏｕｎｄｉｎｇ［Ｊ］
．ａｒＸｉｖ
ｐｒｅｐｒｉｎｔａｒＸｉｖ
ｉｌ８０５
．０３５０８
，２０１８
．


［８２
］ＷａｎｇＬ
，ＬｉＹ
，ＨｕａｎｇＪ
５ｅｔａｌ
．Ｌｅａｒｎｉｎｇｔｗｏ
－ｂｒａｎｃｈｎｅｕｒａｌｎｅｔｗｏｒｋｓｆｏｒｉｍａｇｅ
－ｔｅｘｔｍａｔｃｈｉｎｇ


ｔａｓｋｓ［Ｊ］
．ＩＥＥＥＴｒａｎｓａｃｔｉｏｎｓｏｎＰａｔｔｅｒ
ｎＡｎａｌｙｓｉｓａｎｄＭａｃｈｉｎｅＩｎｔｅｌｌｉｇｅｎｃｅ
，２０１８
，４１（２）
：３９４
－


４０７
．


［８３
］ＬｉｕＤ
，ＺｈａｎｇＨ
，ＷｕＦ
，ｅｔａｌ
．Ｌｅａｒｎｉｎｇｔｏａｓｓｅｍｂｌｅｎｅｕｒａｌｍｏｄｕｌｅｔｒｅｅｎｅｔｗｏｒｋｓｆｏｒｖｉｓｕａｌ


ｇｒｏｘｉｎｄｉｎｇ［Ｃ
］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＩＥＥＥＩｎｔｅｒ
ｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎ．２０１９
：


４６７３
－４６８２
．


［８４
］ＣｈｅｎＬ
，ＭａＷ
，ＸｉａｏＪ
，ｅｔａｌ
．Ｒｅｆ
－ｎｍｓ
：Ｂｒｅａｋｉｎｇｐｒｏｐｏｓａｌｂｏｔｔｌｅｎｅｃｋｓｉｎｔｗｏ
－ｓｔａｇｅｒｅｆｅｒｒ
ｉｎｇ


ｅｘｐｒｅｓｓｉｏｎｇｒｏｕｎｄｉｎｇ［Ｃ
］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＡＡＡＩＣｏｎｆｅｒｅｎｃｅｏｎＡｒｔｉｆｉｃｉａｌＩｎｔｅｌｌｉｇｅｎｃｅ．


２０２１
，３５（２
）
：１０３６
－１０４４
．


６６


参考文献


［８５
］ＭｕＺ
，ＴａｎｇＳ
，
ＴａｎＪ
，
ｅｔａｌ
．Ｄｉｓｅｎｔａｎｇ
ｌｅｄｍｏｔｉｆ
－ａｗａｒｅｇｒａｐｈｌｅａｒｎｉｎｇｆｏｒｐｈｒａｓｅ


ｇｒｏｕｎｄｉｎｇ［Ｃ］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＡＡＡＩＣｏｎｆｅｒｅｎｃｅｏｎＡｒｔｉｆ
ｉｃｉａｌＩｎｔｅｌｌｉｇｅｎｃｅ
．２０２１
，３５（
１５）
：


１３５８７
－１３５９４
．


［８６
］ＨｏｎｇＲ
，ＬｉｕＤ
，ＭｏＸ
，ｅｔａｌ
．Ｌｅａｒｎｉｎｇｔｏｃｏｍｐｏｓｅａｎｄｒｅａｓｏｎｗｉｔｈｌａｎｇｕａｇｅｔｒｅｅｓｔｒｕｃｔｕｒｅｓｆｏｒ


ｖｉｓｕａｌ
ｇｒｏｕｎｄｉｎｇ［Ｊ
］
．ＩＥＥＥＴｒａｎｓａｃｔｉｏｎｓｏｎＰａｔｔｅｒｎＡｎａｌｙｓｉｓａｎｄＭａｃｈｉｎｅＩｎｔｅｌｌｉｇｅｎｃｅ
，２０１９
，


４４
（２
）
：６８４
－６９６
．


［８７
］ＹｅＪ
，ＬｉｎＸ
，ＨｅＬ
，ｅｔａｌ
．Ｏｎｅ
－ｓｔａｇｅｖｉｓｕａｌｇｒｏｕｎｄｉｎｇｖｉａｓｅｍａｎｔｉｃ
－ａｗａｒｅｆｅａｔｕｒｅ


ｆｉｌｔｅｒ
［Ｃ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆ
ｔｈｅ２９ｔｈＡＣＭＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＭｕｌｔｉｍｅｄｉａ．２０２１
：１７０２
－


１７１
１
．


［８８
］ＨｕａｎｇＢ
５ＬｉａｎＤ
，ＬｕｏＷ
，ｅｔａｌ
．Ｌｏｏｋｂｅｆｏｒｅｙｏｕｌｅａｐ
：Ｌｅａｒｎｉｎｇ
ｌａｎｄｍａｒｋｆｅａｔｕｒｅｓｆｏｒｏｎｅ
－


ｓｔａｇｅｖｉｓｕａｌ
ｇｒｏｕｎｄｉｎｇ［Ｃ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆ
ｔｈｅ３ＥＥＥ／ＣＶＦＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄ


ＰａｔｔｅｒｎＲｅｃｏｇｎｉｔｉｏｎ
．２０２１
：１６８８８
－１６８９７
．


［８９］ＺｈａｏＨ
，ＺｈｏｕＪＴ
，ＯｎｇＹＳ
．Ｗｏｒｄ２Ｐｉｘ
：ＷｏｒｄｔｏＰｉｘｅｌＣｒｏｓｓ
－ＡｔｔｅｎｔｉｏｎＴｒａｎｓｆｏｒｍｅｒｉｎＶｉｓｕａｌ


Ｇｒｏｕｎｄｉｎｇ［Ｊ］
，ＩＥＥＥＴｒａｎｓａｃｔｉｏｎｓｏｎＮｅｕｒａｌＮｅｔｗｏｒｋｓａｎｄＬｅａｒｎｉｎｇＳｙｓｔｅｍｓ
，２０２４
，３５（２）
：


１５２３
－１５３３
．


［９０］ＨｏＣＨ
ｓＡｐｐａｌａｒａｊｕＳ
，ＪａｓａｎｉＢ
，ｅｔａｌ
．Ｙｏｒｏ
－
ｌｉｇｈｔｗｅｉｇｈｔｅｎｄｔｏｅｎｄｖｉｓｕａｌ


ｇｒｏｕｎｄｉｎｇ［Ｃ
］
／／ＥｕｒｏｐｅａｎＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎ．Ｓｐｒｉｎｇｅｒ
，２０２２
：３
－２３
．


［９
１
］ＺｈｕＣ
，ＺｈｏｕＹ
，ＳｈｅｎＹ
，ｅｔａｌ
．Ｓｅｑｔｒ：Ａｓｉｍｐ
ｌｅｙｅｔｕｎｉｖｅｒｓａｌｎｅｔｗｏｒｋｆｏｒｖｉｓｕａｌ


ｇｒｏｕｎｄｉｎｇ［Ｃ］／／ＥＴｉｒｏｐｅａｎＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎ．Ｓｐｒｉｎｇｅｒ
，２０２２
：５９８
－６１５
．


［９２
］ＳｈｉＦ
，ＧａｏＲ
，ＨｕａｎｇＷ
ｓｅｔａｌ
．ＤｙｎａｍｉｃＭＤＥＴＲ
：Ａｄｙｎａｍｉｃｍｕｌｔｉｍｏｄａｌｔｒａｎｓｆｏｒｍｅｒｄｅｃｏｄｅｒ


ｆｏｒｖｉｓｕａｌ
ｇｒｏｕｎｄｉｎｇ［Ｊ
］
．ＩＥＥＥＴｒａｎｓａｃｔｉｏｎｓｏｎＰａｔｅｒｎ
ＡｎａｌｙｓｉｓａｎｄＭａｃｈｉｎｅＩｎｔｅｌｌｉｇｅｎｃｅ
，２０２４
，


４６
（２
）
：１
１８
１
－
１
１９８
．


［９３
］ＬｕＪ
，
ＢａｔｒａＤ
，ＰａｒｉｋｈＤ
，ｅｔａｌ
．ＶｉＬＢＥＲＴ：ｐｒｅｔｒａｉｎｉｎｇｔａｓｋ
－ａｇｎｏｓｔｉｃｖｉｓｉｏｌｉｎｇｕｉｓｔｉｃ


ｒｅｐｒｅｓｅｎｔａｔ
ｉｏｎｓｆｏｒｖｉｓｉｏｎ
－ａｎｄ
－
ｌａｎｇｕａｇｅｔａｓｋｓ
［Ｃ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ３３ｒｄＩｎｔｅｒ
ｎａｔｉｏｎａｌ


Ｃｏｎｆ
ｏｅｎｃｅｏｎＮｅｕｒａｌＩｎｆｏｒｍａｔｉｏｎＰｒｏｃｅｓｓｉｎｇＳｙｓｔｅｍｓ
．２０１９
：１３
－２３
．


［９４
］ＣｈｅｎＹＣ
，ＬｉＬ
，ＹｕＬ
，ｅｔａｌ
．Ｕｎｉｔｅｒ：Ｕｎｉｖｅｒｓａｌｉｍａｇｅ
－ｔｅｘｔｒｅｐｒｅｓｅｎｔａｔｉｏｎｌｅａｍｉｎｇ［Ｃ
］
／／Ｅｕｒｏｐｅａｎ


ＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＭｓｉｏｎ．Ｓｐｒｉｎｇｅｒ
，２０２０
：１０４
－
１２０
．


［９５］ＣｈｅｎＫ
，ＺｈａｎｇＺ，ＺｅｎｇＷ
，ｅｔａｌ．Ｓｈｉｋｒａ
：ＵｎｌｅａｓｈｉｎｇＭｕｌｔｉｍｏｄａｌＬＬＭ
ｆ
ｓＲｅｆｅｒｅｎｔｉａｌＤｉａｌｏｇｕｅ


Ｍａｇ
ｉｃ
［Ｊ
］
．ａｒＸｉｖ
ｐｒｅｐｒｉｎｔａｒＸｉｖ
：２３０６
．
１５１９５
，２０２３
．


［９６
］ＬｉＣ
，ＸｕＨ
，ＴｉａｎＪ
，ｅｔａｌ
．ｍｐ
ｌｕｇ
：Ｅｆｆｅｃｔｉｖｅａｎｄｅｆｆ
ｉｃｉｅｎｔｖｉｓｉｏｎ
－
ｌａｎｇｕａｇｅｌｅａｒｎｉｎｇｂｙｃｒｏｓｓ
－


ｍｏｄａｌｓｋｉｐ
－ｃｏｎｎｅｃｔｉｏｎｓ［Ｊ］
＊ａｒＸｉｖ
ｐｒｅｐｒｉｎｔａｒＸｉｖ
：２２０５
．
１２００５
，２０２２
．


［９７
］ＷａｎｇＰ
，ＷａｎｇＳ
，ＬｉｎＪ
，ｅｔａｌ
．ＯＮＥ
－ＰＥＡＣＥ：Ｅｘｐ
ｌｏｒ
ｉｎｇＯｎｅＧｅｎｅｒａｌＲｅｐｒｅｓｅｎｔａｔｉｏｎＭｏｄｅｌ


ＴｏｗａｒｄＵｎｌｉｍｉｔｅｄＭｏｄａｌｉｔｉｅｓ［Ｊ
］
．ａｒＸｉｖ
ｐｒｅｐｒｉｎｔａｒＸｉｖ：２３０５
．
１
１
１７２
，２０２３
．


［９８
］ＺｈａｎｇＲ
，ＬｉＹ
？ＭａＹ
，ｅｔａｌ
．Ｌｌｍａａａ：Ｍａｋｉｎｇ
ｌａｒｇｅｌａｎｇｕａｇｅｍｏｄｅｌｓａｓａｃｔｉｖｅａｎｎｏｔａｔｏｒｓ
［Ｊ
］
．


６７


北京邮电大学硕士学位论文


ａｒＸｉｖ
ｐｒｅｐｒ
ｉｎｔａｒＸｉｖ：２３
１０
．１９５９６
，２０２３
．


［９９
］ＤｏｎｇＱ
，ＬｉＬ
，ＤａｉＤ
，
ｅｔａｌ
．Ａｓｕｒｖｅｙｆｏｒｉｎ
－ｃｏｎｔｅｘｔｌｅａｍｉｎｇ［Ｊ
］
．ａｒＸｉｖｐｒｅｐｒ
ｉｎｔ


ａｒＸｉｖ：２３０１
．００２３４
，２０２２
．


［
１００］ＷｅｉＪ
，ＷａｎｇＸ
，ＳｃｈｕｕｒｍａｎｓＤ
，ｅｔａｌ
．Ｃｈａｉｎ
－ｏｆ
－ｔｈｏｕｇｈｔ
ｐｒｏｍｐｔｉｎｇｅｌｉｃｉｔｓｒｅａｓｏｎｉｎｇ
ｉｎｌａｒｇｅ


ｌａｎｇｕａｇｅｍｏｄｅｌｓ［Ｊ］
．ＡｄｖａｎｃｅｓｉｎＮｅｕｒａｌＩｎｆｏｎｎａｔｉｏｎＰｒｏｃｅｓｓｉｎｇＳｙｓｔｅｍｓ，２０２２
，３５
：２４８２４
－


２４８３７
．


［１０１］Ｋｏｊ
ｉｍａＴ
，ＧｕＳＳ
，ＲｅｉｄＭ
，ｅｔａｌ
．Ｌａｒｇｅｌａｎｇｕａｇｅｍｏｄｅｌｓａｒｅｚｅｒｏ
－ｓｈｏｔｒｅａｓｏｎｅｒｓ［Ｊ
］
．Ａｄｖａｎｃｅｓ


ｉｎＮｅｕｒａｌＩｎｆｏｒｍａｔｉｏｎＰｒｏｃｅｓｓｉｎｇＳｙｓｔｅｍｓ
，２０２２
，３５
：２２
１９９
－２２２１３
．


［
１０２
］ＡｃｈｉａｍＪ，
ＡｄｌｅｒＳ
，
ＡｇａｒｗａｌＳ
，ｅｔａｌ
．Ｇｐｔ
－４ｔｅｃｈｎｉｃａｌｒｅｐｏｒｔ
［Ｊ］
．ａｒＸｉｖｐｒｅｐｒｉｎｔ


ａｒＸｉｖ：２３０３
．０８７７４
，２０２３
．


［
１０３
］ＬｉｕＸ
，ＷａｎｇＺ
，ＳｈａｏＪ
，ｅｔａｌ
．Ｉｍｐｒｏｖｉｎｇｒｅｆｅｒｒｉｎｇｅｘｐｒｅｓｓｉｏｎｇｒｏｕｎｄｉｎｇｗｉｔｈｃｒｏｓｓ
－ｍｏｄａｌ


ａｔｔｅｎｔｉｏｎ
－ｇｕｉｄｅｄｅｒａｓｉｎｇ［Ｃ］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄ


ＰａｔｔｅｒｎＲｅｃｏｇｎｉｔｉｏｎ
．２０１９
：１９５０
－１９５９
．


６８


