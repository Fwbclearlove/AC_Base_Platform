密级
：保密期限
：


＼＞


硕士学位论文


ｔｖ
％


题目
：
基于包结构的图神经网络


远程监督关系抽取研究及应用


学号
：２０１９１４０５２１


姓名
：铙梓钦


专业
：计算机技术


导师
：李睿凡


学院
：人工智能学院


２０２２年
６月
１
日


中国
？北京


密级
：保密期限
：


分ｉ却ｔ大綮


硕士学位论文


题目
：基于包结构的图神经网络


远程监督关系抽取研究及应用


学号
：
２０１９１４０５２１


姓名
：
烧梓钦


专业
：
计算机抛


导师
：
李睿凡


学院
：
人工智能学院


２０２２年
６月
１
日


ＳｅｃｒｅｔＬｅｖｅｌ
：Ｃｏｎｆ
ｉｄｅｎｔｉａｌｉｔｙＰｅｒｉｏｄ
：


Ｂｅｉｊ
ｉｎｇＵｎｉｖｅｒｓｉｔｙｏｆＰｏｓｔｓａｎｄ


Ｔｅｌｅｃｏｍｍｕｎｉｃａｔｉｏｎｓ


ＴｈｅｓｉｓｆｏｒＭａｓｔｅｒＤｅｇｒｅｅ


＠


ＴＩＴＬＥ
：ＲｅｓｅａｒｃｈａｎｄＡｐｐｌｉｃａｔｉｏｎｏｆＤｉｓｔａｎｔｌｙ


ＳｕｐｅｒｖｉｓｅｄＲｅｌａｔｉｏｎＥｘｔｒａｃｔｉｏｎＵｓｉｎｇ


Ｂａｇ
－ＢａｓｅｄＧｒａｐｈＮｅｕｒａｌＮｅｔｗｏｒｋ


ＳｔｕｄｅｎｔＩＤ
：２０１９１４０５２１


Ｃａｎｄｉｄａｔｅ
：ＲａｏＺｉｑｉｎ


Ｓｕｂ
ｊｅｃｔ：ＣｏｍｐｕｔｅｒＴｅｃｈｎｏｌｏｇｙ


Ｓｕｐｅｒｖｉｓｏｒ：ＬｉＲｕｉｆａｎ


Ｉｎｓｔｉｔｕｔｅ
：ＳｃｈｏｏｌｏｆＡｒｔｉｆｉｃｉａｌＩｎｔｅｌｌｉｇｅｎｃｅ


Ｊｕｎｅ１
，２０２２


基于包结构的图神经网络远程监督关系抽取研究及应用


摘
要


信息技术的蓬勃发展带来了互联网文本资源的爆炸式增长
。信息


抽取技术能够从冗余的信息中获取到关键信息
。而关系抽取旨在从自


然语言文本中获取实体对之间的关系
，是信息抽取领域的
一个重要课


题
，
同时也是知识图谱构建的重要步骤
。传统的有监督关系抽取任务


需要大量且高质量的人工标注数据
，而标注过程耗时耗力
。不同于传


统的关系抽取
，基于远程监督的关系抽取
，
能够通过启发式的对齐非


结构化文本和知识库获取大量的训练数据
。其优势不仅在于节省人力


资源和时间消耗
，还可以基于不同的知识图谱迁移到不同领域
，提高


关系抽取系统的泛化性能
。因此远程监督关系抽取任务具有
一定的应


用前景
。


然而远程监督的标注方式为远程监督关系抽取引出了错误标注


和长尾数据两大挑战
。针对以上问题本文提出了
一种基于包结构的层


级图神经网络框架
，
进
一步提升远程监督关系抽取的性能
。


本文具体研宄内容和贡献如下
：


１
）在远程监督的关系抽取领域
，
提出
一种从局部到全局进行学


习的层级图卷积神经网络框架
（ＬｏｃａｌｔｏＧｌｏｂａｌＧｒａｐｈＣｏｎｖｏｌｕｔｉｏｎａｌ


Ｎｅｔｗｏｒｋ
，
Ｌ２Ｇ
－ＧＣＮ）
。该框架在文本编码过程中先通过局部地学习单


个实例中的句法知识
，然后整体地聚合包结构内实例的语义关联信息
，


进而提升远程监督关系抽取的性能
。具体而言
，针对远程监督关系抽


取研究中处理长文本能力不足的问题
，本文结合了依存句法分析和图


神经网络
，构造了词级别的图神经网络结构
；针对远程监督关系抽取


中存在的噪声问题
，本文在包结构的基础上提出基于自注意力机制的


句子级图神经网络结构
；
针对远程监督关系抽取中存在的长尾问题
，


本文引入了互信息最大化正则器
，
对模型权重进行约束
。


２
）
提出了
一种基于预训练模型的异质图卷积神经网络模型


（ＨｅｔｅｒｏｇｅｎｅｏｕｓＧｒａｐｈＣｏｎｖｏｌｕｔｉｏｎａｌＮｅｔｗｏｒｋｂａｓｅｄｏｎＢＥＲＴ
，ＢＨ
－


ＧＣＮ）
。为了加强实体信息对关系抽取任务的指导作用
，本文在Ｌ２Ｇ
－


ＧＣＮ框架的基础上增加对实体信息的处理
，
设计了ＢＨ
－ＧＣＮ。
具体


而言
，我们构建了
一个实例和实体作为节点的异质图
，该图结构利用


自注意力机制更新节点信息
。并且该图根据不同的连接方式
，使得不


Ｉ


同类型的节点之间可以进行选择性的交互
。同时本文设计了
一种实体


和实例信息融合的门控机制来增强包的表示
。此外为了引入常识性知


识和语法语义信息
，本文使用了预训练模型作为编码器
，验证了预训


练模型在远程监督关系抽取任务的有效性
。


３）设计并实现了远程监督关系抽取演示系统
。
该系统涵盖了用


户管理模块、数据管理模块、关系抽取模块
、前端展示模块
。支持用


户自定义输入文本
，
进行包级别的关系抽取
，
并对相关结果可视化
。


关键词
：关系抽取
远程监督
图神经网络
注意力机制


Ｕ


ＲＥＳＥＡＲＣＨＡＮＤＡＰＰＬＩＣＡＴＩＯＮＯＦＤＩＳＴＡＮＴＬＹ


ＳＵＰＥＲＶＩＳＥＤＲＥＬＡＴＩＯＮＥＸＴＲＡＣＴＩＯＮＵＳＩＮＧＢＡＧ
－


ＢＡＳＥＤＧＲＡＰＨＮＥＵＲＡＬＮＥＴＷＯＲＫ


ＡＢＳＴＲＡＣＴ


Ｔｈｅｒａｐ
ｉｄｄｅｖｅｌｏｐｍｅｎｔｏｆｉｎｆｏｒｍａｔｉｏｎｔｅｃｈｎｏｌｏｇｙ
ｌｅａｄｓｔｏｔｈｅ


ｅｘｐ
ｌｏｓｉｖｅ
ｇｒｏｗｔｈｏｆ
Ｉｎｔｅｒ
ｎｅｔｄａｔａ
．Ｉｎｆｏｒｍａｔｉｏｎｅｘｔｒａｃｔｉｏｎｔｅｃｈｎｏｌｏｇｙｃｏｕｌｄ


ｏｂｔａｉｎｔｈｅｋｅｙｆｒｏｍｒｅｄｕｎｄａｎｔｉｎｆｏｒｍａｔｉｏｎ
．Ｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎ
，ｏｎｅｏｆｔｈｅ


ｆｕｎｄａｍｅｎｔａｌｔｏｐ
ｉｃｓｉｎｔｈｅｃｏｍｍｕｎｉｔｙｏｆｉｎｆｏｒｍａｔｉｏｎｅｘｔｒａｃｔｉｏｎ
，ａｉｍｓｔｏ


ｉｄｅｎｔｉｆ
ｙｔｈｅｒｅｌａｔｉｏｎｂｅｔｗｅｅｎｅｎｔｉｔｉｅｓ
．Ｉｔｉｓａｃｒｉｔｉｃａｌｓｔｅｐ
ｉｎｋｎｏｗｌｅｄｇｅ
ｇｒａｐｈ


ｃｏｎｓｔｒｕｃｔｉｏｎ
．Ｔｒａｄｉｔｉｏｎａｌｓｕｐｅｒｖｉｓｅｄｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎｒｅｑｕｉｒｅｓａｌａｒｇｅ
－


ｓｃａｌｅｈｉｇｈ
－
ｑｕａｌｉｔｙｈｕｍａｎ
－
ｌａｂｅｌｅｄｄａｔａ
．Ｄｉｆｅｒｅｎｔｆｒｏｍｉｔ
，ｄｉｓｔａｎｔｌｙ


ｓｕｐｅｒｖｉｓｅｄｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎｃｏｕｌｄｏｂｔａｉｎａｌａｒｇｅ
－ｓｃａｌｅｔｒａｉｎｉｎｇｄａｔａｂｙ


ｈｅｕｒｉｓｔｉｃａｌｌｙ
ｌａｂｅｌｉｎｇｐ
ｌａｉｎｔｅｘｔｗｉｔｈａｋｎｏｗｌｅｄｇｅｂａｓｅ
．Ｔｈｉｓｍｅｔｈｏｄｃｏｕｌｄ


ｓａｖｅｈｕｍａｎｒｅｓｏｕｒｃｅｓａｎｄｔｉｍｅｃｏｎｓｕｍｐ
ｔｉｏｎ
，ａｎｄｃａｎｂｅａｄａｐ
ｔｅｄｔｏ


ｄｉｆｆｅｒｅｎｔｆｉｅｌｄｓｂａｓｅｄｏｎｄｉｆｆｅｒｅｎｔｋｎｏｗｌｅｄｇｅｇｒａｐｈｓ
．Ｔｈｅｒｅｆｏｒｅ
，ｄｉｓｔａｎｔｌｙ


ｓｕｐｅｒｖｉｓｅｄｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎｃｏｕｌｄｂｅｗｉｄｅｌｙａｐｐ
ｌｉｃａｔｅｄ
．


Ｈｏｗｅｖｅｒ
，ｔｈｅｌａｂｅｌｉｎｇｗａｙｏｆｄｉｓｔａｎｔｓｕｐｅｒｖｉｓｉｏｎｃａｕｓｅｓｗｒｏｎｇ


ｌａｂｅｌｉｎｇａｎｄｌｏｎｇ
－ｔａｉｌ
ｐｒｏｂｌｅｍ
．Ｔｏａｌｌｅｖｉａｔｅａｂｏｖｅ
ｐｒｏｂｌｅｍｓ
，ｗｅｐｒｏｐｏｓｅａ


ｈｉｅｒａｒｃｈｉｃａｌｇｒａｐｈｃｏｎｖｏｌｕｔｉｏｎａｌｎｅｔｗｏｒｋｓｂａｓｅｄｏｎｂａｇｓｔｒｕｃｔｕｒｅｆｏｒ


ｄｉｓｔａｎｔｌｙｓｕｐｅｒｖｉｓｅｄｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎ
．


Ｔｈｅｍａｉｎｒｅｓｅａｒｃｈａｎｄｃｏｎｔｒｉｂｕｔｉｏｎｏｆｔｈｉｓ
ｐａｐｅｒａｒｅａｓｆｏｌｌｏｗｓ
：


１
）ＡＬｏｃａｌｔｏＧｌｏｂａｌＧｒａｐｈＣｏｎｖｏｌｕｔｉｏｎａｌＮｅｔｗｏｒｋｆｒａｍｅｗｏｒｋ
，
ｉ
．ｅ
．
，


Ｌ２Ｇ
－ＧＣＮ
，
ｉｓｐｒｏｐｏｓｅｄｆｏｒｄｉｓｔａｎｔｌｙｓｕｐｅｒｖｉｓｅｄｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎ
．Ｔｈｅ


ｆｒａｍｅｗｏｒｋｉｍｐｒｏｖｅｓｔｈｅｐｅｒｆｏｒｍａｎｃｅｏｆｄｉｓｔａｎｔｌｙｓｕｐｅｒｖｉｓｅｄｒｅｌａｔｉｏｎ


ｅｘｔｒａｃｔｉｏｎｂｙｆｉｒｓｔｌｏｃａｌｌｙ
ｌｅａｒｎｉｎｇｓｙｎｔａｃｔｉｃｋｎｏｗｌｅｄｇｅｉｎａｓｉｎｇ
ｌｅｉｎｓｔａｎｃｅ
，


ａｎｄｔｈｅｎｇ
ｌｏｂａｌｌｙａｇｇｒｅｇａｔｉｎｇｓｅｍａｎｔｉｃｃｏｒｒｅｌａｔｉｏｎｉｎｆｏｒｍａｔｉｏｎａｍｏｎｇ


ｉｎｓｔａｎｃｅｓｗｉｔｈｉｎｔｈｅｂａｇｓｔｒｕｃｔｕｒｅ
．Ｓｐｅｃｉｆｉｃａｌｌｙ，ｔｏｓｔｒｅｎｇ
ｔｈｅｎｔｈｅｍｏｄｅｌ
＇
ｓ


ａｂｉｌｉｔｙｏｆｈａｎｄｌｉｎｇ
ｌｏｎｇｔｅｘｔｓ
，ｔｈｅｄｅｐｅｎｄｅｎｃｙｐａｒｓｉｎｇ
ｉｓｉｎｃｏｒｐｏｒａｔｅｄｔｏ


ｃｏｎｓｔｒｕｃｔａｗｏｒｄ
－
ｌｅｖｅｌｇｒａｐｈｃｏｎｖｏｌｕｔｉｏｎａｌｎｅｔｗｏｒｋ（Ｗｏｒｄ
－ＧＣＮ）
．Ｔｏ


ｈｉ


ａｌｌｅｖｉａｔｅｔｈｅｎｏｉｓｅ
ｐｒｏｂｌｅｍ
，ａｓｅｎｔｅｎｃｅ
－
ｌｅｖｅｌｇｒａｐｈｃｏｎｖｏｌｕｔｉｏｎａｌｎｅｔｗｏｒｋ


（Ｓｅｎ
－ＧＣＮ）ｗｉｔｈｓｅｌｆ
－ａｔｔｅｎｔｉｏｎｍｅｃｈａｎｉｓｍｂａｓｅｄｏｎｂａｇｓｔｒｕｃｔｕｒｅｉｓ


ｐｒｏｐｏｓｅｄ．Ｔｏｓｏｌｖｅｔｈｅｌｏｎｇ
－ｔａｉｌｐｒｏｂｌｅｍ
，ａｒｅｇｕｌａｒ
ｉｚａｔｉｏｎｔｅｒｍｏｆｍｕｔｕａｌ


ｉｎｆｏｒｍａｔｉｏｎｍａｘｉｍｉｚａｔｉｏｎｉｓｉｎｃｏｒｐｏｒａｔｅｄｔｏｃｏｎｓｔｒａｉｎｔｈｅｍｏｄｅｌｗｅｉｇｈｔｓ
．


２）ＡＨｅｔｅｒｏｇｅｎｅｏｕｓＧｒａｐｈＣｏｎｖｏｌｕｔｉｏｎａｌＮｅｔｗｏｒｋｂａｓｅｄｏｎＢＥＲＴ
，


ｉ
．ｅ”ＢＨ
－ＧＣＮ
，
ｉｓ
ｐｒｏｐｏｓｅｄｆｏｒｄｉｓｔａｎｔｌｙｓｕｐｅｒｖｉｓｅｄｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎ．Ｉｎ


ｏｒｄｅｒｔｏｅｎｈａｎｃｅｔｈｅｇｕｉｄｉｎｇｒｏｌｅｏｆｅｎｔｉｔ
ｙ
ｉｎｆｏｒｍａｔｉｏｎｆｏｒｒｅｌａｔｉｏｎ


ｅｘｔｒａｃｔｉｏｎ
，ｗｅａｄｄｔｈｅｐｒｏｃｅｓｓｉｎｇｏｆｅｎｔｉｔｙｏｎｔｈｅｂａｓｉｓｏｆＬ２Ｇ
－ＧＣＮ


ｆ
ｒａｍｅｗｏｒｋａｎｄｐｒｏｐｏｓｅＢＨ
－ＧＣＮ．Ｓｐｅｃｉｆｉｃａｌｌｙ，ｗｅｃｏｎｓｔｒｕｃｔａ


ｈｅｔｅｒｏｇｅｎｅｏｕｓｇｒａｐｈｗｉｔｈｉｎｓｔａｎｃｅｓａｎｄｅｎｔｉｔｉｅｓａｓｎｏｄｅｓ
，ａｎｄｔｈｅｇｒａｐｈ


ｕｐｄａｔｅｓｔｈｅｎｏｄｅｓｂｙｓｅｌｆ
－ａｔｔｅｎｔｉｏｎｍｅｃｈａｎｉｓｍ
．Ｍｅａｎｗｈｉｌｅ
，ａｃｃｏｒｄｉｎｇ
ｔｏ


ｄｉｆｆｅｒｅｎｔｅｄｇｅｔｙｐｅｓ
，ｔｈｅｍｏｄｅｌｃｏｕｌｄｉｎｔｅｒｒｅｌａｔｅｔｈｅｎｏｄｅｓｓｅｌｅｃｔｉｖｅｌｙ
．


Ｂｅｓｉｄｅｓ
，ｗｅｄｅｓｉｇｎａ
ｇａｔｅｄｍｅｃｈａｎｉｓｍｆｏｒｔｈｅｆｕｓｉｏｎｏｆ
ｅｎｔｉｔｙａｎｄｉｎｓｔａｎｃｅ


ｉｎｆｏｒｍａｔｉｏｎ
，ｗｈｉｃｈｅｎｈａｎｃｅｓｔｈｅｂａｇｒｅｐｒｅｓｅｎｔａｔｉｏｎｓ．Ｉｎａｄｄｉｔｉｏｎ
，ｉｎｏｒｄｅｒ


ｔｏｉｎｔｒｏｄｕｃｅｃｏｍｍｏｎ
－ｓｅｎｓｅｋｎｏｗｌｅｄｇｅ
，ｓｙｎｔａｃｔｉｃａｎｄｓｅｍａｎｔｉｃ


ｉｎｆｏｒｍａｔｉｏｎ
，ｗｅａｄｏｐｔａｐｒｅ
－ｔｒａｉｎｅｄｍｏｄｅｌａｓａｎｅｎｃｏｄｅｒ．Ａｎｄｔｈｅ


ｅｆｅｃｔｉｖｅｎｅｓｓｏｆｔｈｅ
ｐｒｅ
－ｔｒａｉｎｅｄｍｏｄｅｌｉｓｖｅｒ
ｉｆ
ｉｅｄｆｏｒｄｉｓｔａｎｔｌｙｓｕｐｅｒｖｉｓｅｄ


ｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎ．


３）Ｄｅｓｉｇｎａｎｄｉｍｐ
ｌｅｍｅｎｔａｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎｓｙｓｔｅｍｂａｓｅｄｏｎ


ｄｉｓｔａｎｔｌｙｓｕｐｅｒｖｉｓｅｄ
．Ｔｈｅｓｙｓｔｅｍｃｏｎｔａｉｎｓｕｓｅｒｍａｎａｇｅｍｅｎｔｍｏｄｕｌｅ
，ｄａｔａ


ｍａｎａｇｅｍｅｎｔｍｏｄｕｌｅ
，ｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎｍｏｄｕｌｅａｎｄｗｅｂｄｅｓｉｇｎｍｏｄｕｌｅ．


Ａｎｄｔｈｅｓｙｓｔｅｍｓｕｐｐｏｒｔｓｕｓｅｒ
－ｄｅｆｉｎｅｄｎｕｍｂｅｒｓｏｆｉｎｐｕｔｔｅｘｔｓ
，ｒｅｌａｔｉｏｎ


ｅｘｔｒａｃｔｉｏｎｉｎｂａｇ
－
ｌｅｖｅｌ
，ａｎｄｖｉｓｕａｌｉｚａｔｉｏｎｏｆｔｈｅｒｅｌａｔｅｄｒｅｓｕｌｔｓ
．


ＫＥＹＷＯＲＤＳ：ｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎ
，ｄｉｓｔａｎｔｓｕｐｅｒｖｉｓｉｏｎ
，ｇｒａｐｈｎｅｕｒａｌ


ｎｅｔｗｏｒｋｓ
，ａｔｅｎｔｉｏｎｍｅｃｈａｎｉｓｍ


ｒｖ


目
录


第
一章绪论
１


１
．１研究背景及意义
１


１
．２
国内外研宄现状
２


１
．２
．
１
关系抽取
２


１
．２
．２远程监督关系抽取
３


１
．３主要研宄内容
５


１
．４本文结构安排
６


第二章背景知识与相关技术
９


２
．１
关系抽取简介
９


２
．２包结构简介
１０


２
．３远程监督关系抽取简介
１０


２
．４远程监督关系抽取任务定义
１２


２
．５相关技术
１２


２
．５
．
１循环祌经网络
１２


２
．５
．２预训练语言模型
１４


２
．５
．３
图神经网络
１７


２
．６本章小结
１９


第三章基于层级图神经网络的远程监督关系抽取模型２１


３
．１整体框架…

２１


３
．２句法依赖关系获取
２２


３
．２
．
１
相对位置特征
２２


３
．２
．２依存句法分析
２３


３
．３编码器

２４


３
．３
．
１输入层
２４


３
．３
．２基于依存句法分析的词级图神经网络模块２４


３
．３
．３基于自注意力机制的句级图神经网络模块２６


３
．４互信息最大化正则器
２８


３
．５
两阶段层级的训练方式
２９


３
．６本章小结
３
１


第四章基于预训练模型的异质图神经网络远程监督关系抽取模型
３３


４
．１
整体框架
，
３３


４
．２编码器
３４


４
．２
．
１输入层
３４


４
．２
．２异质图神经网络结构
３５


４
．２
．３异质信息融合模块
３６


４
．２
．４分类器
３７


４
．３训练算法
３８


４
．４本創
、结
３９


第五章实验设置与结果分析
４１


５
．１
实验环境
４１


５
．２实验数据集
４１


５
．３
评估指标介绍
４２


５
．３
．
１精确率
－召回率曲线
４３


５
．３
．２ＡＵＣ值
４４


５
．３
．３Ｐｒｅｃｉｓｉｏｎ＠Ｎ
４４


５
．３
．４Ｈｉｔｓ＠Ｋ
４４


５
．４实验细节…



４５


５
．５实验结果分析
４６


５
．５
．
１对比模型
４６


５
．５
．２结果分析
４７


５
．５
．３对比实验分析
５１


５
．５
．４
图卷积层数影响
５４


５
．５
．５案例分析
５５


５
．５
．６ＧＤＳ数据集实验结果分析
５７


５
．６本章小结
，
，
．
．…

５７


第六章远程监督关系抽取平台设计与实现
５９


６
．１
系统需求分析
５９


６
．２系统总体设计框架
６０


６
．３
系统详细功能设计与实现
６２


６
．３
．
１
用户管理模块
６２


６
．３
．２关系抽取模块
６３


６
．３
．３
数据管理模块
６６


６
．４系统测试
６７


６
．４
．
１
功能测试
６７


６
．５本章小结
６８


第七章
总结与展望
６９


７
．１研宄工作总结
６９


７
．２未来工作总结

７０


参考文献
７１


第
一章
绪论


第
一章绪论


１
．
１研究背景及意义


信息抽取技术
［１
－３］是
一种将非结构化的文本信息通过相应处理转化为结构化


信息的技术
。从文本信息中获取实体
、关系
、事件等信息是信息抽取的主要目的
。


关系抽取
（ＲｅｌａｔｉｏｎＥｘｔｒａｃｔｉｏｎ
，
ＲＥ
）是信息抽取领域的
一个重要方向
。其主要目


的是在自然语言文本中识别实体对之间的关系
，将获取的信息相关联以形成关系


三元组等结构化的数据存储到数据库中
，
从而便于其他下游任务应用
。


关系抽取在众多领域都有非常重要的意义和应用价值
，例如
：知识图谱构建
、


问答系统完善
、搜索引擎的搭建等
。知识图谱是存储实体关系的语义网络
，
随着


知识图谱的应用愈加广泛
，扩充知识图谱使其适应现实世界的庞大数据量和保持


相应的时效性是必不可少的
。因此如何利用自动化的关系抽取模型
，及时为知识


图谱进行知识扩充是当下研究者所需要探索的关键问题
。


然而传统的有监督关系抽取需要海量且高质量的人工标注数据才能保证抽


取的效果
。标注数据是
一项耗费大量时间和劳动力的工作
，使得传统的关系抽取


任务通常局限在限定领域
。
相比而言
，
半监督的关系抽取方法
，
如迭代式的


Ｂｏｏｔｓｔｒａｐｐ
ｉｎｇ方法
等
，
相对来说对数据要求不高
，
但精度较低
，
在学习的过程


中会产生语义漂移现象
。无监督关系抽取方法同样无需标注
，可以在大型数据集


抽取关系
，但需要
一定的领域知识才能将抽取的结果映射到所需的关系上
，效果


同样难以保证
。


基于以上背景
，
远程监督方法被引入关系抽取当中
。
２００９年
，
Ｍｉｎｔｚ等人
［５］


于论文中提出了
一个关键假设
：
“ 对于
一个存储实体关系三元组的知识库
，
如果


非结构化文本库中的语句中包含的实体对出现在这个知识库中
，则包含这个实体


对的所有语句也表达了该知识库中对应的关系
。
” 落实到具体操作
，
即将现有大


规模的知识库和非结构化文本进行启发式对齐
，挑选出含有相应知识库三元组的


语句进行数据处理作为训练数据
。例如
，假设知识库中存在关系三元组＜青蒿素
，


屠呦呦
，
发现者＞
，
文本库中存在语句
“ 屠呦呦在
２０１
１年
９月发现了青蒿素
”
，


那么远程监督方法将判定该语句表达了
“ 发现者
” 关系
。
以此类推
，
文本库中所


有涉及
“ 屠呦呦
”
，
“ 青蒿素
” 两个实体的语句都会被标记上
“ 发现者
” 关系标签
，


标注数据由此产生
。
由此可见
，基于远程监督的关系抽取优势在于可以通过远程


监督的方式快速生成训练所需数据
，而不需要消耗大量的人力物力
，减少了模型


对人工标注的依赖
，
同时也增强了模型跨领域的适应能力
。
因此
，远程监督兼具


１


＾北京邮电大学工程硕士学位论文


了有监督和无监督两种学习方式的优点
，既可以和有监督用有标签的数据的训练


分类模型
，
又可以像无监督方法快速适应任何领域的大型语料库
。


１
．２国内外研究现状


１
．２
．
１关系抽取


近来
，深度学习在人工智能的各个领域异军突起
，不断刷新各项任务的指标
。


相比传统机器学习方法
，深度学习方法避免了人工构造特征
，减少了由此带来的


误差累积
。
Ｌｉｕ等人
［６
］首次使用了卷积神经网络
（ＣｏｎｖｏｌｕｔｉｏｎａｌＮｅｕｒａｌＮｅｔｗｏｒｋ
，


ＣＮＮ）将关系抽取任务视为多分类任务
，打开了关系抽取任务在深度学习的大门
。


紧接着
，
Ｚｅｎｇ等人ｍ使用了ＣＮＮ自动提取了词汇和句子级别的特征
，
并设计了


沿用至今的位置特征
（ＰｏｓｉｔｉｏｎＦｅａｔｕｒｅｓ
，ＰＦ）
，
在该任务上取得了显著的成果
。


由于自然语言是以序列形式存在的数据
，前后文本无论是在语法还是语义上都有


很强的关联
，而ＣＮＮ难以捕捉这种全局的关联性
。因此循环神经网络
（Ｒｅｃｕｒｒｅｎｔ


Ｎｅｔｗｏｒｋ
，
ＲＮＮ
）被提出来更好地记忆上下文信息从而缓解长文本带来的困扰
。


ＺｈａｎｇａｎｄＷａｎｇ
［８
］使用了双向循环神经网络来抽取语义特征
，
在关系抽取任务上


也取得了不错的效果
。


关系抽取中有研究发现
，文本中不同成分对实体间关系的判别有不同程度的


作用
。如何在生成句子隐藏向量表示时
，分配不同成分不同的权重是
一个新的研


究问题
。
注意力机制的提出
，
为关系抽取任务提供了
一种解决该问题的思路
。


Ｂａｈｄａｎａｕ等人Ｍ提出了注意力机制
，利用注意力机制在生成句子向量表示时
，通


过计算赋予
一句话中的每个词不同的权重
，
这与关系抽取任务相契合
。
Ｚｈｏｕ等


人
［
１
（）
］使用了对
ＲＮＮ
网络做出了
一定改进的长短时记忆网络
（ＬｏｎｇＳｈｏｒｔ
－Ｔｅｒｍ


Ｍｅｍｏｒｙ
，ＬＳＴＭ
）
［ｎ
］
，
并结合了注意力机制模型
，对语句中不同单词赋予不同权


值
，
以捕捉更加对关系抽取更有利的信息
。


关系抽取任务中的
一大趋势是利用图卷积神经网络
（ＧｒａｐｈＣｏｎｖｏｌｕｔｉｏｎａｌ


Ｎｅｔｗｏｒｋ
，
ＧＣＮ
）
。
２０
１６年Ｋｉｐｆ和Ｗｅｌｌｉｎｇ
［
１２
］首次提出了图卷积神经网络
。
图神经


网络在图的学习上应用了深度学习中提取特征的思想
，并取得了良好的实验效果
。


研究者们尝试将其应用在关系抽取领域上
，
如
Ｚｈａｎｇ等人
将注意力机制思想


与ＧＣＮ结合
，
一定程度上提升了关系抽取的效果
。


近年来
，联合实体关系抽取成为了研宄的热门方向
，
Ｗｅｉ等
提出实体关系


联合抽取模型
，
即先抽取头实体
，然后在指定关系下抽取尾实体
，
该串联式方法


达到了该方向上的最佳效果
。Ｚｈｅｎｇ等人
１
１５
］将联合关系抽取任务划分为三个子任


２


第
一章绪论


务
：关系判断
、实体提取和主客实体对齐
，进
一步提升了联合实体关系抽取模型


的性能
。


１
．２
．２远程监督关系抽取


标注数据匮乏
一直是关系抽取领域中困扰研究者的问题之
一
，人工标注耗时


耗力
。为了解决数据匮乏的问题
，
Ｍｉｎｔｚ等人呵是出了基于远程监督的关系抽取


方法
，其属于弱监督学习的
一种
。该方法通过启发式对齐知识库和非结构化文本


库
，快速生成训练数据
，有效缓解了数据匮乏的问题
。并且其可以基于不同的知


识库扩展到不同领域
，
进行快速的领域迁移
，
有益于提高关系抽取的泛化性能
。


在生成数据的基础上
，
Ｍｉｎｔｚ等人使用规则方法提取文本的词法特征和句法特征


作为分类器的输入
，
最终在该任务上取得不错的效果
。


据
１
．
１节所述
，
Ｍｉｎｔｚ等人提出了关于远程监督的关键假设
。
显而易见的是


该假设过于严苛
，导致通过远程监督的方法获取的训练数据中存在错误标注的问


题
，
即语句中的实体之间可能存在多种关系
，
亦或是没有任何关系
。所以Ｒｉｅｄｅｌ


等人
［
１６
］根据这
一现象
，
对该假设做出了改进
，
提出了
“至少存在
一次
” 假设
。
该


假设认为如果两个实体之间存在某
一种关系
，
那么包含这两个实体的所有实例
，


至少有
一实例表达了这种关系
。此外
，为了验证远程监督方法在不同领域的适应


性
，
Ｒｉｅｄｅｌ等人对齐纽约时报语料和
ＦｒｅｅＢａｓｅ知识库
，
产出了ＮＹＴ
－
１０数据集
。


同时Ｒｉｅｄｅｌ等人引入了多７Ｋ例学习
（Ｍｕｌｔｉ
－ＩｎｓｔａｎｃｅＬｅａｒｎｉｎｇ
，ＭＩＬ
）
［
１７］到远程监


督关系抽取研宄当中
，
并取得了不错的实验效果
。其中
，
多示例学习可以被描述


为
：
数据集以具有概念标记的包
（Ｂａｇ）为基本单位
，
每个包含有若千个实例
。


若包中至少存在
一个正标记的实例
，则该包标记为正
，若
一个包中所有的实例均


为负例
，
则该包被标记为负
。
Ｒｉｅｄｅｌ等人采用该方法
，将含有相同实体对的实例


划分为包
，
在包级别上展开关系抽取任务
。第二章将具体介绍ＭＩＬ应用在远程


监督关系抽取中的情况
。


然而传统的特征工程方法需要人为的设计任务相关的抽取特征
，机器分类的


效果受到设计者的主观影响
。并且人工的特征工程常常需要借助外部的语言处理


工具
，这种方法会将工具本身具有的不稳定性和误差传递给下
一阶段的分类任务
，


导致分类效果下降
。


随着深度学习的崛起
，神经网络中端到端的训练模式更适用于关系抽取任务
，


并且有关
ＣＮＮ和
ＲＮＮ的神经网络模型层出不穷
，
关于深度学习在远程监督关


系抽取任务的研宄日益增多
。
Ｚｅｎｇ等人
［
１８］构建了分段式的卷积神经网络


（ＰｉｅｃｅｗｉｓｅＣｏｎｖｏｌｕｔｉｏｎａｌＮｅｕｒａｌＮｅｔｗｏｒｋ
，
ＰＣＮＮ）
。考虑到远程监督带来的噪音


问题
，他们沿用了Ｒｉｅｄｅｌ等人
提出的假设
，在训练中
，选取出
一个包中置信度


最高的实例作为训练模型的包表示
，最终的抽取效果较之前基于特征工程的方法


３


北京邮电大学工程硕士学位论文


有了较大提升
。但是也正因为该假设的引入
，使得模型丢失了数据中的大部分信


息
。
Ｌｉｎ等人
［
１９］为了解决信息丢失问题
，
在包内使用了语句级的注意力机制
，
为


一个包中不同实例赋予不同的权重
，
从而利用加权得到的包表示进行关系分类
。


这
一做法使得模型在训练的过程中
，可以动态的减少噪音数据的权重
，增加有利


信息的权重
，有效提升了基于远程监督关系抽取的效果
。
因此该选择性注意力机


制的使用成为
一种主流的范式
。
Ｖａｓｈｉｓｈｔｈ等人
［２Ｇ］将图神经网络结构引入远程监


督关系抽取任务
，
使用图神经网络编码了语法信息
，
使用Ｂｉ
－ＧＲＵ
［２
１
］结构作为基


本的编码器
。同时在包内结合上述选择性注意力机制
，完成包级别关系抽取
。ＹＥ


等人
［２２］提出了ＰＣＮＮ＋ＡＴＴ
＿ＲＡ＋ＢＡＧ
＿ＡＴＴ模型
。
其首次使用了包级别的注意力


机制
，利用了表达相同实体关系的包的特征向量相似的原理
，动态分配高的权重


给相似的包
，低的权重给噪音包数据
，最终在该任务上取得了不错的效果
。
Ｘｉｎｇ


等人
［２３］在
ＰＣＮＮ＋ＡＴＴ的基础上
，
提出了ＳＨＴＣＮＮ结构
，
不采用
ＰＣＮＮ的三段


式最大池化
，
而采用了突出头实体和尾实体信息的新池化方式
。
Ｓｈａｎｇ等人
［２４
］设


计的
ＰＳＡＮ
－ＲＥ模型采用
一种模版感知的自注意力网络
，
来识别不同种类的关系


模版结构
。然后利用这些模版信息来辅助预训练模型捕捉文本内局部的依赖关系


和短语结构
。


同时
，
也有部分研宄工作不在上述注意力机制的范式上进行研宄
。
Ｊｉａｎｇ等


人
［２５
］通过跨句子的最大池化方法
，
来融合
一个包中多个句子实例的信息
。
另外


Ｊｉａｎｇ等人也注意到相同的实体对不应该只有
一个关系标签
。根据数据分析
，
在


ＮＹＴ
－
１０数据集中大约有
１８
．３％的实体对拥有重叠的关系
（ＲｅｌａｔｉｏｎＯｖｅｒｌａｐ）
，
因


此他们提出了多示例多标签的卷积神经网络结构用于关系抽取
。该方法分别对每


个关系类别使用二元分类器来解决多标签分类问题
。
Ｌｉｕ等人
［２６
］为了更好的处理


错误标注的问题
，着眼于标签的改动
，提出了生成软标签的方法
。
即根据原标注


标签和真实标签两者与实体对特征向量的相似程度来动态地修改标签
，并利用新


标签进行训练
。
同年
，
Ｈｕａｎｇ等人
［２７
］
，将深度残差网络引入远程监督的关系抽取


任务中
，证明深度的卷积网络在自然语言处理任务上也能取得较好的效果
。
Ｆｅｎｇ


等人
［２８］引入了记忆网络结构
，
用于学习在分类过程中词语所占实例表示的权重


和捕捉关系类别之间的联系
，实验结果证明了其在远程监督关系抽取任务上的有


效性
。


另
一方面
，在训练关系抽取模型之前识别并排除噪音也是
一种处理噪音的方


式
。强化学习
（ＲｅｉｎｆｏｒｃｅｍｅｎｔＬｅａｒ
ｎｉｎｇ
，ＲＬ
）
的引入无疑是给该任务注入了新力


量
。
Ｆｅｎｇ等人％提出了
一个包含关系抽取器和实例选择器的模型框架
，
将实例


选择建模为强化学习问题
。在数据输入关系抽取器前
，通过实例选择器去除噪声
，


再将去除噪声的数据应用到关系抽取器上
，用关系抽取的效果作为强化学习的奖


４


第
一章绪论


励
。Ｑ
ｉｎ等人＿和Ｚｅｎｇ等人
［３
１
］将生成对抗网络（ＧｅｎｅｒａｔｉｖｅＡｄｖｅｒｓａｒ
ｉａｌＮｅｔｗｏｒｋｓ
，


ＧＡＮｓ
）引入远程监督关系抽取任务
，
利用对抗学习去除噪声
，
解决错误标注


问题
。
同年
，
Ｑ
ｉｎ等人
［３３
］再提出了ＤＳＧＡＮ模型
，
以类似的思想
，
挑选高质量的


实例参与模型训练
，
以此解决噪声问题
。


由于文本的侧重性往往不同
，数据的均衡性难以保证
，
因此数据长尾问题是


远程监督关系抽取中的另
一大挑战
。引入外部知识信息是
一种缓解长尾问题的有


效途径
。
Ｊｉ等人
［３４
］引入了ＦｒｅｅＢａｓｅ中的实体描述信息
，
从而更准确的表示了实


体
。同时利用
ＴｒａｒｉＳＥ
［３５
］的关系表示机制
，将关系向量用实体向量之差表示
，紧密


了实体和关系之间的联系
。Ｈａｎ等人
［３６
］则从另外
一个角度来考虑提升远程监督关


系抽取的效果
，他们不再独立地考虑每个关系标签
，而是利用关系标签先验的层


级信息
。并且基于这种关系标签之间的联系
，设计了
一种新颖的层级注意力模式


来识别包中的有效实例
。另外
，
Ｚｈａｎｇ等人
【３７］设计了ＰＣＮＮ＋ＫＡＴＴ模型
，借助了


知识图谱的图嵌入表示
，
将关系标签中蕴含的知识融入模型
，
并且用ＧＣＮ显示


地建模关系标签
。而Ａｌｔ等人
［３８
］观察到此前的模型倾向于预测包为某些拥有大量


训练数据的特定关系
。针对这个问题
，他们提出了ＤＩＳＴＲＥ模型
，通过使用预训


练语言模型来引入语义
，语法特征和
一些常识知识
。这些特征被认为是识别更多


种类实体关系的关键信息
。
Ｇｏｕ等人
［３９］利用关系标签和实体类型的联系设计了


一种动态生成参数的网络模型ＤＰＥＮ
。
Ｃａｏ等人
［４（）
］构建了
一个词汇共现图来学习


文本的表示
，
同时也从无标签的数据中学习关系原型
，通过借助包含充分训练数


据的关系类型的知识
，
提升模型解决长尾问题的能力
。


在目前基于远程监督关系抽取的研究中
，大多数工作都采用多示例学习框架
，


将含有相同实体对的实例打包
，使用注意力机制来为包内的实例分配权重
，
降低


噪音实例的权重
，从而构成包的表示
。然而
，现有的研究工作没有显示的考虑包


内实例之间的联系
，忽略了包内全局的实例间结构信息的对降低噪声影响的作用
。


同时
，为解决长尾问题
，先前研究工作常常引入额外的先验知识
，缺少
一种更直


接通用的解决方式
。
以上问题导致模型不充分的泛化能力和不尽人意的效果
，这


也为目前远程监督关系抽取研究带来了更大的挑战
。


１
．３主要研究内容


本文针对基于远程监督的关系抽取任务进行研宄
，其目标是有效的处理远程


监督带来的噪声问题和长尾问题
，使关系抽取在基于远程监督方法构建的伪标注


数据集上达到较为可观的效果
。同时实现基于远程监督的包级关系抽取演示系统
。


本文具体研究内容如下
：


５


北京邮电大学工程硕士学位论文


１
）研宄层级的图神经网络结构
，
缓解噪音问题和长尾问题


本文提出了
一种从局部到整体学习的图神经网络模型框架Ｌ２Ｇ
－ＧＣＮ
。
Ｌ２Ｇ
－


ＧＣＮ基于多示例学习框架
，
是
一种基于包结构的远程监督关系抽取算法
。
该模


型包括
一个词级别的图卷积神经网络结构
，
一个句级别的图卷积神经网络结构以


及
一个互信息最大化（Ｍｕｔｕａｌ
ＩｎｆｏｒｍａｔｉｏｎＭａｘｉｍｉｚａｔｉｏｎ
，ＭＩＭ
）正则器
。其中
，
词级


别图结构借助句法依存分析局部地编码单个语句内的信息
，生成同时包含语义和


句法信息的语句特征向量
。句级别图结构整体地编码包内的全局结构信息
，其基


于自注意力机制挖掘包内实例之间的联系
，然后将信息聚合到
一个包表示中
。针


对长尾问题
，
本文引入了最大化互信息的正则化方法
，
通过对模型权重的约束
，


学习到更有区分性的特征
。此外
，整个自底向上的模型结构采用分阶段的方式进


行训练
，
进而更稳定地学习具有多粒度信息的包表示
。


２
）研宄基于预训练模型的异质图网络结构
，
增强实体信息的指导作用


对于实体关系抽取任务
，实体信息的重要性不言而喻
。因此本文在Ｌ２Ｇ
－ＧＣＮ


框架的基础上设计了引入实体信息的异质图神经网络结构
ＢＨ
－ＧＣＮ
。
该模型通


过异质图结构节点的更新来增强实体和实例信息之间的交互
。通过
一个融合实体


信息和实例信息的门控机制
，对实体信息和实例信息进行不同程度的融合
，最终


生成与实体关联性更强的包表示
。此外
，
引入预训练模型
，验证其在远程监督关


系抽取任务上的有效性
。


３
）远程监督关系抽取演示系统设计与实现


根据远程监督关系抽取的多示例学习框架
，本工作搭建了
一种基于包级别的


关系抽取演示系统
。其中后端采用
Ｆｌａｓｋ框架
，前端使用Ｂｏｏｔｓｔｒａｐ框架
，整个系


统分为如下功能模块
：数据管理模块
，
用户管理模块
，关系抽取模块以及前端展


示模块
。通过输入
１
？ｎ条含有相同实体对的文本
，对多条文本进行以包为单位的


关系抽取
，
并且可以对相关抽取结果可视化
。


１
．４本文结构安排


论文分为七个章节
，
文章的组织结构如图
１
－
１及主要内容如下
：


第
一章
：首先介绍了本文研究主题的背景和意义
，接着详细介绍了远程监督


关系抽取的国内外研宄现状
，
最后简要总结了本文的主要研宄内容
。


第二章
：
详细介绍了关系抽取以及远程监督领域中的相关术语和背景知识
，


包括了关系抽取的任务定义
，远程监督的原理和流程
，远程监督关系抽取的任务


定义和形式
。
最后阐述远程监督关系抽取的优势
。


６


第
一章
绪论


？
＊Ｎ


第
一章绪论


Ｖ



＞
ｒ


第１３背景知识辅关


技术





Ｉ










■


Ｉ
ｒ
＾
厂齒細
…
丨
丨


｜
关
１ｆ
ｅｓｓｍ？
）
Ｉ


■

＜／Ｖ
？


；Ａ
“
；


！
Ｉ
■
■
１
■
■
Ｉ
■
！


ｉ
｜问题
１
：＿存
｜问题
｜问题３
：＾体作
；


：
丨在噪音
丨
尾
丨用欠缺
：



＞
ｊ
ｒ


第五章
与


结果分析



＞
Ｊ
ｒ


第清远鑑督先系抽取


平甜Ｓｉ十键现


，
ｒ


ｆ
Ｎ


第七章雜与展望


图Ｍ
论支组织结构图


第四章
：详细的介绍引入实体信息的异质图神经网络结构
，包括了当下最流


行的预训练语言模型的基本介绍
，如何引入预训练语言模型
，如何在层级图神经


网络结构上做改进
，
以及针对远程监督关系抽取任务
，如何引入实体信息到图神


经网络中来构建异质图祌经网络
。


第五章
：主要介绍了本文的实验内容
，包括数据集的相关统计和特点
，实验


的评测指标
，
实验环境和实现细节
，实验结果与其余研究的对比
，
以及对实验结


果的具体分析
，
证明本文设计方法的有效性
。


第六章
：主要阐述了远程监督关系抽取系统设计与实现
。依次介绍了系统框


架设计
，
模块设计以及系统实现结果展示
。
本文基于
ＦＬＡＳＫ框架实现了
一个


ＷＥＢ端的关系抽取系统
，
用户可以指定实体对
，
通过输入句子
，
系统返回该实


体对即包表达的关系
。


第七章
：
总结本文的主要研究内容和创新点
，并指出工作不完善之处
，做出


下
一步的研宄方案
。


７


．


第二章背景知识与相关技术


第二章背景知识与相关技术


关系抽取是自然语言处理中的关键技术
，也是远程监督关系抽取的基础
。基


于此
，
本文首先介绍关系抽取任务
，
再引出远程监督关系抽取涉及的基础知识
，


远程监督关系抽取任务定义以及相关技术
。


２
．
１关系抽取简介


关系抽取是自然语言处理领域的基本任务之
一
。关系抽取任务有许多实际应


用
，
包括构建知识库
［４
１域知识图谱
［４２］
、
构建智能问答系统
［４３
］以及搭建在线搜索


系统
［４４
］
。关系抽取的主要目标是发现文本中两个给定实体之间的语义关系
。为了


说明关系抽取任务
，
本文选择图
２
－
１
中的实例
Ｓ
１和
Ｓ２作为例子
。
对于实例
Ｓ
１


和
Ｓ２
，这两个句子包含相同的实体对
，但表达不同的关系
。因此
，关系抽取任务


是抽取句子中实体对
（
“ Ｒｏｎａｌｄｏ
”
，
“
Ｂｒａｚｉｌ
”
）之间的特定关系
。对于句子
Ｓ
１
，有


效的关系抽取算法可以抽取出三元组＜Ｒｏｎａｌｄｏ
，
Ｂｒａｚｉｌ
，
Ｎａｔｉｏｎａｌｉｔｙ〉
。


Ｄｉｓ
ｔａｎｔＳｕｐｅｒｖｉｓｉｏｎＬａｂｅｌ
ｉｎｇＰｒｏｃｅｓｓ



ＩＮＬ＼


ＫｎｏｗｌｅｄｇｅＲｅｐ？ｓ
ｉｉｏｒｊ
｜Ｋ
＼＼


Ｒｅ
ｌａｔｉｏｎＥｎｔｉｔｙｌＥｎｔ
ｉ
ｌｙ２Ｃｏｒｐｕｓ
｜
Ｎ
＼
＾


Ｎａｔｉｏｎａｌ
ｉ
ｔｙＲｏｎａｌｄｏＢｒａｚｉ
ｌＢｒａｚ
ｉ
ｌ
＇
ｓ
Ｒｏｎａｌｄｏ
ｓｕｆｅｒｅｄ
ａ



ａ
ｓｅ
ｉｚｕｒｅｂｅｆｏｒｅ
ｔｈｅ１９９８ｍａｔｃｈ
ａｎｄ


ＣＥＯ
＿ＯｆＴｉｍＣｏｏｋＡｐｐ
ｌｅ
Ｉｐ
！ａ
＞ｃｄ

ｌ
ｉｓｔｌｃｓｓｈ
ｉｎｔｈｅｆｉｎａｌ
．




ＶＲｏｎａｌｄｏ
ｗａｓ
ｂｏｍ

ｉｎ１９７６
．
Ｈｅ

．
．


Ｅｍｐ
ｌｏｙｅｄ
＿ＢｙＴｉｍＣｏｏｋＡｐｐ
ｌｅ
Ｄａｔａｓｅｔ
ｊ
—


Ｒｅ
ｌａ
ｔ
ｉｏｎＩｎｓ
ｔａｎｃｅｓＤＳＬａｂｅｌＧｏ
ｌｄｅｎＬａｂｅ
ｌＢａｇ
ＩＤ（Ｍ
ＩＬ
）


Ｓ
Ｉ
：
Ｆｏｒ

ｌｉ
ｉｓ
ｐａｒ
ｔｎｅｒｕｐ
ｆｒｏｎｔ
，
Ｐｏｄｏ
ｌｓｋｉ
ｃ
ｌ
ｉｏｓｅ
Ｒｏｏａ
ｌｄｏ
ｏｆ
Ｂｒａｚｉ
ｌ
ｒ
ａｔｈｅｒ
ｔｈａｎ
ｐ
ｉｃｋｉｎｇ
Ｋｉｏｓｅ
Ｎａｔｉｏｎａｌ
ｉ
ｔｙＮａｔ
ｉｏｎａｌ
ｉ
ｔｙ

１


Ｓ２
：Ｒｏｎ，Ｗ〇
ＭＸ
ｔｈｅ
ｇａｍｅ
ｉｎ
ｔｈｅ
６９ｄ，
ｍｉｎｕ
ｊ
ｅ
，
＾
ｔｈｅＮ—
？ＮＡ
１


ｎｅｗｓｐａｐ
ｅｒ
ｒｅｘｉｅｕｓ
ｗｅｒｅ
ｍｅｒｃ
ｉｌｅｓｓ

ｉｎＢｒａｚｉ
ｌ
－


Ｓ３
：Ｂ
ｉａｙｉ
ｌ
＇
ｓ
Ｒｏｎａ
ｉｄｏ
ｓｕｆｅｒｅｄ
ａ
ｓｅｉｚｕｒ
ｅ
ｂｅｆｏｒｅ

ｔｈｅ１９９８


ｍａ
ｔｃｈ
ａｉ＾
ｐ
ｌ＾ｅｄ

Ｉｉｓｄｅｓｓｈ
ｉｎ

ｔｈｅ
ｆ
ｉ
ｉａｌ
Ｎａｔｉｏｎａｌ
ｉｔｙＮａｔｉｏｎａｌ
．ｔ＞
－

１＼
ｊ



Ｓ４
：
Ｐｅｒｈａｐｓ
ｒｅｃａｌｌｉｎｇ
ｂｏｗ
Ｒｏｎａｌｄ
＊
＊
ｏｆＢｒａｚｉｌ
ｃｅ
ｌｄｊｒ
ａｔｅｄ
ＶＴ
．
．
，


ｗｉｎｎｉｎｇ

ｔｈｅ
２００２
ｃｕｐ
ｂ＞
－
ａｎｎｏｕｎｃｉｎｇ
Ｕｉａｔ
．—
ｔｙＮａｔ．ｏｎａｌ
．ｔｙ

１


Ｓ５
：
Ｔ
ｉｍＣ
＇
？？ｏｋ

ｉｓ

ｔｂｅ
ｃｕｎｃｎ
ｌ
ＣＥＯ
〇ｒＡｐｐ
ｌｅＣＥＯ
＿ＯｆＣＥＯＯｆ１


Ｓ６
：
ＴｉｍＣｏｏｋ
ｉｓ
ｔ
ｌ
？ｅ
ｃｕｉＴｅｎｔ
ＣＥＯ
ｏｆＡｐｐ
ｌｅＥｍｐ
ｌｏｙｅｄ
＿ＢｙＣＥ０
＿０ｆ３


Ｓ７
：
ＴｉｍＣ
ｉＨ
＞ｋ
ｊｏ
ｉｎｅｄ
Ａｐｐ
ｌｅ
ａｉ
ｔｈｅ

ｉｍ
ｉ
ｔａ
ｔｉｏｎ
ｏｆ
Ｊｏｂｓ

ｉｎ
１９９８ＣＥＯ
一ＯｆＥｍｐ
ｌ〇ｙｅｄ
Ｂｙ
２


Ｓ８
：
ＴｉｂｉＣ〇？ｋ
ｊｏ
ｉｎｅｄ
Ａｐｐ
ｌｔ
ａｔ

ｔｈｅ

ｉｍ
ｉｔａｔｉｏｎ
ｏｆ
Ｊｏｂｓ

ｉｎ
１９９８Ｈｍｐ
ｌｏｙｅｄ
＿ＢｙＥｍｐ
ｌｏｙｅｄ
＿Ｂｙ３


图
２
－
１远程监督标记数据的过程。
在多示例学习框架下分为三个包
。


传统上
，关系抽取的任务是使用有监督的方法来完成的
，例如使用支持向量


机
（Ｓｕｐｐｏｒ
ｔＶｅｃｔｏｒＭａｃｈｉｎｅ
，
ＳＶＭ）
［４３］
、基于统计的方法
［４６
］和基于核函数的方法


［４７
］
。
然而
，
这些监督方法的性能严重依赖于大规模标记数据
。


９


北京邮电大学工程硕士学位论文


２
．２包结构简介


１９９７年Ｄｉｅｔｅｒｉｃｈ等人
１
１７
］在研宄药物活性预测问题中首次提出了多示例学习


ＭＩＬ的概念
。在多示例学习中
，样本不再以单个实例为训练样本
，而是以包（Ｂａｇ）


为基本单位
。
训练数据由若干个包组成
，每个包只有
一个标签
。换言之
，
引入多


示例学习后
，
使用多个实例的表示来构成包的表示
，
以包为单位对模型更新
。与


全监督学习方式相比
，
多示例学习的训练数据中单个实例的标签是未知的
，
已知


的只有构成的包的标签
；与无监督学习比较
，无监督学习的训练集完全没有标签
，


而多示例学习的训练集中包的标签是己知的
。因此
，远程监督兼具了全监督和无


监督两种学习方式的优点
。


据统计
，
ＮＹＴ
－
１０数据集约有
３
１％的数据不符合远程监督的假设
［
１６］
。
因此包


中正确样本占大多数
，即使
一个包中含有实际不同于包标签的数据
（噪音数据
）
，


其对模型训练不会造成过大的影响
，
从而也就减缓了噪音的问题
。


２
．３远程监督关系抽取简介


为了缓解标注数据匮乏的问题
，
Ｍｉｎｔｚ等人
［５］提出了基于远程监督的关系抽


取方法
，
引入远程监督标注数据
，这种方法被认作是弱监督学习的
一种
。他们基


于上文描述的假设
，将现有大规模的知识库和非结构化文本进行启发式对齐获取


训练数据
，
标注过程如图
２
－
１所示
。而这个过程无需人工标注数据
，
减少了人工


的工作量
，
使得算法可以快速迁移至其他领域
。


然而
，远程监督关系抽取假设过于严格
，在不同的语境中
，提到同
一个实体


对的实例可能表达截然不同的关系
。这
一现象导致远程监督数据集中存在错误的


标注数据
，许多包含在训练数据中的实例并不能真实表达标签所表示的实体关系
，


这些样本被称为噪音
。此外
，
由于在现实应用中
，
知识库通常是不完整的
，
很难


找到
一个足够巨大的知识库囊括了世界上所有的关系三元组
，这种不完整性则会


在远程监督数据的启发式匹配中产生假负例
（ＦａｌｓｅＮｅｇａｔｉｖｅ
）
。
即文本实际表达


了实体间的某种关系
，而在知识库对齐后被认定为不表达任何实体关系
。
由于噪


声隐式的存在于数据中
，
因此如何在关系抽取的过程中识别出噪声成为
一个研


宄难题
。本文称这种错误标注问题为噪音问题
。
以图
２
－
１所示为例
，
本文假设实


体对
（
“ Ｒｏｎａｌｄｏ
”
，
“Ｂｒａｚｉｌ
”
）在语料库中有四个对应的语句
，
它们都将使用知识


库中的关系事实
“ 〈Ｒｏｎａｌｄｏ
，
Ｂｒａｚｉｌ
，
Ｎａｔｉｏｎａｌｉｔｙ〉
” 来标注上Ｎａｔｉｏｎａｌｉｔｙ关系
。


事实上
，
实例
Ｓ２不表达任何关系
，
本文记为ＮＡ类别即不表达任何实体关系
。


１０


第二章背景知识与相关技术


而ＮＡ标签与实际的标签不
一致
，
其他三个句子则符合实际的标签
。
因此
，错误


的标签问题可能会导致生成的数据包含大量噪声
。从统计数据来看
，将ＦｒｅｅＢａｓｅ


与
《纽约时报》新闻语料库进行启发式对齐时
，约有
３
１％的数据不符合远程监督


的假设
这种噪声问题严重阻碍了远程监督关系抽取的应用
。


为了缓解大规模噪声问题
，
Ｒｉｅｄｅｌ等人
［
１６
】引入了多示例学习框架
。多示例学


习不再以单个实例
（单个句子
）为训练样本
，而是在训练过程中将含有相同实体


关系三元组的实例打包
，
以包的标签作为该包内所有实例的标签
。换言之
，该方


法将包作为单个训练样本
，
以包为单位对模型更新
。因为包中大部分样本为正确


数据
，
即使
一个包中有噪音数据
，对模型训练的影响也因被放入包中而降低
，
从


而也就减缓了噪音的问题
。


因此
，
在ＭＩＬ框架下
，
关系抽取任务中具有相同实体对的实例都被作为建


模的
一个样本
，
即被视为
一个包
。
图
２
－１用
“
Ｂａｇ
ｌＤ
”表示包的编号
，
其展示了三


个关系三元组和相应的包
。实体对
（
“
ＴｉｍＣｏｏｋ
”
，
“ Ａｐｐ
ｌｅ
”
）在知识库中对应多个


关系
，
这种现象导致相同的实例会以不同的关系类别标签标记
，
例如实例
Ｓ５和


实例
Ｓ６
。
同时
，有些实例
，例如实例
Ｓ２
、
Ｓ６和
Ｓ７
，
并不表示实际的实体关系
，


也会被放进Ｂａｇ里
。因此
，这血不同种类的噪声实例在
一定程度上都损害了包的


表示
，
从而破坏了模型的有效性
。


＜Ｒｅ
ｌａｔｉｏｎＬａｂｅｌｓＤ
ｉｓｔｒ
ｉｂｕｔｉｏｎ


１０
ｆ＼

ｊ

ｉ

：
—
１


＼Ｉ
Ｉ
；
ｌ


１０
．
：
ｉ
—
：
，
３＼
！
！
；
！


Ｏ１０
：


．Ｌ
：
！Ｎ


ｉ
ｉ卜、
ｖ
—Ｌ


ｒ
—
—
ｔｒ
一


１０
°
－
ｉ
ｉ
＇￣
ｊ


０
１０２０３０４０５０


ＳｏｒｔｅｄＲｅ
ｌａｔ
ｉｏｎＩＤ


图
２
－２远程监督数据集的长尾问题


长尾问题是远程监督关系抽取中的另
一大挑战
。由于对齐的文本数据往往存


在不同方面的侧重点
，
产生的数据类别是不平衡的
，
部分关系类型的数据稀缺
。


以ＮＹＴ
－
１０数据集为例
，
其具体的数据分布如图
２
－２所示
。
可见位于蓝线的右侧


大约
４０个关系类型对应实例数少于
１
，０００
，
而位于红线右侧的部分即长尾关系
。


此外
，
据统计
，
ＮＹＴ
－
１０数据集中ＮＡ类型数据
（不含其余关系类型）甚至占据


了整个数据集的
７２％
。这导致模型的预测容易偏向于某
一个类别
，在
一定程度上


损害了模型的鲁棒性
。


１
１


北京邮电大学工程硕士学位论文


２
．４远程监督关系抽取任务定义


本文定义知识库为
１Ｋ＝
｛Ｅ
，股，Ｆ｝，其中Ｅ代表所有的实体集合
，
脱代表所有


的关系标签集合
，
１Ｆ代表所有的关系事实集合
。
ｅｔ，ｒ）ｅＦ表不
一个包含在知


识库中的关系事实
，
其中头实体ｑ
，
尾实体ｅｔ和关系ｒｅＲ。
然后本文将实例分


组
，
划分为包的集合Ｂ（ｅ
／ｌ，ｅｔ，
ｒ〇ｅＦ｝
。其中


表不
一个含有ｎｓ个实例的包
。对于其中
一个实例
．
．
．
，
ａｃｎｗ｝
，
其中包含


ｎｗ个的词
。如果本文指定
一个实体和其对应的包
本文的模型将会为这


个包预测
一个标签
。


远程监督关系抽取相关概念名称如表２
－１所示
：


表２
－
１
远程监督相关概念


名称



知识库存储世界客观知识
’
如ＦｒｅｅＢａｓｅ


语料库存储非结构化或半结构化的文本数据
，
如纽约时报


实体表示客观存在的概念
，
如人名
，
国家等


关系表示客观存在概念之间的联系


实例表示数据集中的
一条句子


表示含有相同实体对的实例集合


关系事实
｜即关系三元组
，
通常存储为＜头实体
，
尾实体
，
关系＞


２
．５相关技术


２
．５
．
１循环神经网络


１
）循环神经网络
（ＲｅｃｕｒｒｅｎｔＮｅｕｒａｌＮｅｔｗｏｒｋ
，
ＲＮＮ）


自然语言处理中大部分任务都需要处理序列信息
，序列信息的上下文通常具


有
一定关系
，如果仅仅单独理解某个片段而不结合上下文
，往往会对文本表达的


语义造成误解。
因此本文需要对文本有
一个更为整体的理解。


传统的神经网络模型中
，在输入层
、隐含层
、输出层之间以全连接的形式连


接
，
而层内节点缺少交互
，
难以解决序列问题
。
因此
，＿被提出用以建模序


列数据
。当ＲＮＮ处理序列问题时
，序列中某片段的当前输出与之前输出相关联。


具体的表现形式为
，
网络结构会运用前文信息
，
计算当前序列片段输出的信息
，


换言之
，模型设计了连接结构连接隐藏层之间的节点
。除此之外
，隐藏层同时包


括了输入层的输出和上
一时刻隐藏层的输出
。
ＲＮＮ的网络结构如图
２
－３所示
。


１２


第二章背景知识与相关技术


理论上
，
ＲＮＮ
网络结构能够处理任意长度的序列数据
。
但是在实践中
，
为了降


低复杂性往往假设当前的状态只与前面相邻的几个状态相关
。
从理论上来说
，


ＲＮＮ将输入序列ｘ编码为
一个固定维度的隐藏状态／ｘ
。本文假设ｘ＝


是输入序列
，
则
／ｉ
ｔ
＝
／〇／ｘ
ｔ＋＋是与其对应的时刻的隐藏状态
，其中ｔ


代表时刻
，
代表权重矩阵
，
６代表偏置项
，
ｔ时刻的隐藏状态￣由当前时刻的


输入；＾和前
一时刻的隐藏状态
得到
，
越早输入的序列
，
在后续更新状态中所


占的比例越小
。


（＞
＂
）ｆ）
（Ａ
）（？
）


｜
ｆＩ—ｒｊＬ
—
ｑ
＇
＾ＪＬ


“Ａ
—＝Ａ
—？Ａ
—？Ａ
ＨＡ


ｔｌＸ１


？（５）ｔ
ｅ）？
－？


图
２
－３循环神经网络结构


２）长短期记忆网络
（ＬｏｎｇＳｈｏｒ
ｔ
－ＴｅｒｍＭｅｍｏｒｙ
，
ＬＳＴＭ
）


如上文所述
，
ＲＮＮ在编码序列时
，
随着长度增加
，
前文的信息传递到后方


会有不定程度的损失
。
因此ＲＮＮ网络难以处理有长期依赖的序列
，
容易造成梯


度消失或者梯度爆炸的现象
。
为了解决该问题
，
提出了门限循环神经网络结构
，


而
ＬＳＴＭ就是其中最著名的
一种
。
门限循环神经网络允许在不同时刻改变单元


模块连接中的权重系数
，且允许网络丢弃当前已经累积的信息
，从而利于累积长


距离节点之间的信息
。


相比ＲＮＮ只有
一个传递状态ｈ
ｔ
，
ＬＳＴＭ有两个传输状态
，
一个称为细胞状


态
（ＣｅｌｌＳｔａｔｅ
）ｃ
ｔ
，
和
一个隐藏状态
（ＨｉｄｄｅｎＳｔａｔｅ），
以此记录额外的信息
。


ＬＳＴＭ
中还设计了３个类型的门控机制
，
即输入门
（
ＩｎｐｕｔＧａｔｅ）
、遗忘门
（Ｆｏｒｇｅｔ


Ｇａｔｅ
）和输出门
（ＯｕｔｐｕｔＧａｔｅ）
。
整体结构如图
２
－４所示
：


兮
．
罕


＾￣
ｆｆＷ
１
＂


ＡＡ


九ＸＸ


ｉＸｔ）Ｕｖｙ


图
２
－４长短期记忆网络结构


ＬＳＴＭ
的核心思想就是通过门控机制来决定保留或者忘记细胞状态中的信


息
。
三个门控机制的介绍如下
：


遗忘门的作用是选择性遗忘细胞状态中保留的信息
，
如公式２
－
１所示
：


１３


北京邮电大学工程硕士学位论文


ｆｔ
＝ａ（Ｗ
ｆ［ｈ
ｔ
＿１＞Ｘ
ｔ］＋ｂ
ｆ）（２
－１）


其中
代表激活函数
Ｓｉｇｍｏｉｄ，
输出
一个在
０到
１之间的数值来决定保留或忘记


细胞状态Ｃ
ｔｑ中的信息
。
１表示
“ 完全保留
”
，
０表示
“ 完全丢弃
”
。


输入门
，
将新的信息选择性的记录到细胞状态中
，
具体如下
。


ｈ
＝＜ｒｍ
［ｈｔ
－ｉ，Ｘｔ＼＋ｂｄ（２
－２）


Ｃｔ
＝Ｔａｎｈ（Ｍ＾［
／ｉ
ｔ
＿ｉ＾ｔ］＋ｂｃ）（２
－３）


Ｃｔ
＝＊Ｃ
ｔ
一
ｉ＋ｉ
ｔ
＊Ｃ
ｔ
（２
－４）


其中输入门可分为两个部分
，＾为
一个在０到
１之间的数值来决定更新什么数值
，


计算如公式
２
－２
，￥为候选细胞状态
，
根据公式
２
－３计算
，
用来加入到新的细胞


状态中
，
最后根据公式２
－４更新细胞状态
为新的细胞状态Ｃｔ
。


输出层过Ｓｉｇｍｏｉｄ函数来确定细胞状态的哪个部分将输出
，如公式２
－５所示
。


最后根据公式２
－６来确定最后隐藏状态输出的部分。


〇ｔ＝（ｒ（Ｗ０［ｈ
ｔ
＿ｌ
ｔｘｔ］＋ｂ〇）（２
－５）


ｈｔ
＝ｏ
ｔ
＊Ｔａｎｈ（Ｑ）
（２
－６）


通过观察可以发现
，细胞状态＾在模块的上方贯穿运行
，
只有
一些简单的线性交


互
，
易于保存前文传递的信息
。


２
．５
．２预训练语言模型


近年来
，预训练语言模型
（ＰｒｅｔｒａｉｎｅｄＬａｎｇｕａｇｅＭｏｄｅｌｓ
，
ＰＬＭｓ）在多项ＮＬＰ


任务中频频刷新多项纪录
［４２
，
４８
＿５（）］
。
因此也成为ＮＬＰ领域中深度学习范畴内的中


流砥柱。语言模型
（ＬａｎｇｕａｇｅＭｏｄｅｌ
，
ＬＭ）根据上下文的文本信息来预测目标词


的向量表示
，生成相应文本的概率分布
。这个过程为无监督的学习过程
，无需人


工标注数据
，
因此便于模型从大规模的数据中学习语言特征。


预训练语言模型即依据特定的语言任务对语言模型进行训练
，然后获得该语


料上的语言特征
，学习到的参数可以作为模型的初始化
。最后使用预训练好的语


言模型
，
当作最基本的编码模型来对接不同的模型
，
以处理不同的
ＮＬＰ任务
。


通过对大规模语料库进行预训练
，ＢＥＲＴ
［５１］获得了捕捉大量
“ 常识
” 知识的能力
，


并在微调方案后的许多任务中获得了显著的改进
。同时
，０？丁
［５２］和紅１＾
［５３］也是


著名的预训练模型代表
，
具有出色的迁移学习能力
。此外
，
包括ＧＰＴ在内的
一


些研宄发现
，显著增加ＬＭ的大小可以更好地泛化到下游任务
。因此预训练语言


模型的使用不需要额外的具体的语言特征和信息
，例如例如语言成分标注和实体


类型信息
，
仍旧能实现较好的任务效果
。


１４


第二章背景知识与相关技术


１
）Ｔｒａｎｓｆｏｒｍｅｒ


由于ＲＮＮ是自回归的模型
，
难以进行并行化处理
，
并且在顺序计算过程中


信息容易发生丢失
，尤其对于特别长期的依赖关系
，即使采用ＲＮＮ的变种ＬＳＴＭ


和ＧＲＵ也无效
。
如果采用ＣＮＮ模型代替ＲＮＮ
，
可以进行并行计算
，
不需要按


顺序等待输入
。
但ＣＮＮ需要采用更高层的计算才可以考虑到较长的序列
，
这取


决于卷积核的大小
。
因此用仅仅基于注意力机制的方式代替ＲＮＮ结构
，
实现并


行化计算
，
并且可以同时考虑句子中的所有信息
。


因此
，
Ｔｒａｎｓｆｏｒｍｅｒ网络架构由Ｖａｓｗａｎｉ等人
１５４
］提出
，该网络结构中
，编码器


和解码器没有采用
ＲＮＮ或ＣＮＮ等网络架构
，
而是采用完全依赖于注意力机制


结构
，
模型整体框架如下图
２
－５所示
：


Ｏｕｔｐｕｔ


Ｐｒｏｂａｂｉ
ｌ
ｉｔ
ｉｅｓ


ｔ


ＩＳｏｆｔｍａｘ１


ｔ


Ｉｌｉｎｅａｒｊ


ｒ
￣￣
Ｉ
￣￣


Ａｄｄ＆Ｎｏｒｍ


Ｆｅｅｄ


ＰｔＫｖｖａｒｄ


｛
—
ｐ
，ｔ—＾


ｆ
ＩａＡｄｄＳＮｏｒｍＨ
－？ｖ



ＦｅｅｄＡ＾ｔｅｎｔｋ＞ｎ


Ｆｏ
ｒｗａｒｄ

Ｊ

ｙ
＊Ｎｘ



１


Ｍ
ＩＡｄｄ＆Ｎｏｒｍ


ＭｕｌＳ
－ＨｅａｄＭｕｌｔ
ｉ
－Ｈｅａｄ


Ａｔｔｅｎｔ
ｉｏｎＡｔｔｅｎｔ
ｉｏｎ


ｌ＾
＝ｒ＜＿ｐｊ


ｖ
Ｉ／Ｖｙ


Ｐｏｓｉｔ
ｉｏｎａｌ
＾Ｔ＼１丄Ｐｏｓｉｔ
ｉｏｎａ
ｌ


ＥｎｃｏｄｉｎｇＥｎｃｏｄ
ｉｎｇ


ｉｎｐｕｔＯｕｔｐｕｔ


ＥｍｂｅｄｄｉｎｏＥｍｂｅｄｄｉｎｇ


！


ＩｎｐｕｔｓＯｕｔｐｕｔｓ


（ｓｈ
ｉｆｔｅｄｒｉｇｈｔ）


图２
－５Ｔｒａｎｓｆｏｒｍｅｒ模型框架
［５４
］


Ｔｒａｎｓｆｏｒｍｅｒ是
一个序列到序列模型
（Ｓｅｑ２ｓｅｑ）
，
由编码器Ｅｎｃｏｄｅｒ和解码器


Ｄｅｃｏｄｅｒ两个部分组成
。其中Ｅｎｃｏｄｅｒ由Ｎ
＝６个相同的层组成
。每层由两个核心


部分组成
，
即多头自注意力机制模块和全连接前向网络模块
。并在每个模块增加


了残差网络
（ＲｅｓｉｄｕａｌＣｏｎｎｅｃｔｉｏｎ）和层次归
一化
（ＬａｙｅｒＮｏｒｍａｌｉｚａｔｉｏｎ）
。


１５


北京邮电大学工程硕士学位论文


Ｓｃａ
ｌｅｄＤｏｔ
－ＰｒｏｄｕｃｔＡｕｅｎｉｉｏｎＭｕｌｔ
ｉ
－ＨｅａｄＡｔｔｅｎｔｉｏｎ


會


ｆ
ｆＬ
ｉｎｅａｒ
］


｜ＭａｔＭｕｉ｜
｜


ｔ１
［Ｃｏｎｃａｔ１


ＩＳｏｆｔＭａｘＩ


｜Ｍａｓｋ
（ｏｐ
ｔ
．
）｜ＳｃａｌｅｄＯｏｔ
－ＰｒｏｃｋｉＣｔｈ


ｉｕＡｔｔｅｎｔｋｘｉ＊０


［
Ｓｃａｔ
ｅＩ
ｔｌ一
？＇鲁
，、
寺


ＩＭａＬｔｕＩＩ
ＩＵｒｗａｒ
ｌｌ
ｆ
ｌｉｎｅａｒ
｜ｌｆｕｎＭｒ
｜］
－


ｔｔ


ＱＫＶ


ＶＫＱ


图
２
－６
注意力点积计算和多头注意力机制
｜５４
］


其中注意力机制可以表示为如下公式
：


Ａｔｔｅｎｔｉｏｎ（Ｑ
，Ｋ
，Ｖ）
＝Ｓｏｆｔｍａｘ
（＾
＿
．
）
Ｖ（２
－７）


通过注意力机制
，
模型计算每个词和其他词的相似度计算
，
学习句子内部


词之间的依赖
。
多头注意力机制则是通过按头数个不同的线性变换对＜３
，
欠
，
１／


进行投影
，
如图
２
－６所示
，
最后将不同的注意力结果拼接起来得到多头注意力


得到的表示
：


ＭｕｌｔｉＨｅａｄ（Ｑ
，Ｋ
，Ｖ）
＝Ｃｏｎｃａｔ（／ｉｅａｄ１
（
．
．
．
．ｈｅａｄ＾Ｗ
０（２
－８）


其中


ｈｅａｄ
ｉ
＝ＡＸＸｅｎＸｘｏｎｉｊＱＷ＾

．ＫＷ＾
．ＶＷ＾（２
－９）


对于自注意力机制来说
＜？
＝Ｋ＝Ｋ
。
多头注意力的目的是让不同的头学习到不同


的子空间的信息
。


此外
，
前向神经网络可表示为
：


ＦＦＮ（ｘ）
＝ｍａｘＣＯ
．ｘＶＫ
ｉ＋Ｗ２＋ｂ
２（２
－
１０）


其中
，
为线性变换的权重
，
６１和６２为偏置项
，
ｍａｘ（０
，
ｘ）函数表示ＲｅＬＵ


激活函数
。此外
，在多头注意力模块和前馈神经网络层之后都设有
一个残差网络


模块
，
并且对每层进行归
一化
，
达到提升模型鲁棒性的效果
。


Ｄｅｃｏｄｅｒ和Ｅｎｃｏｄｅｒ的结构类似
，
相比
Ｅｎｃｏｄｅｒ而言增加了
一个多头注意力


模块
。


２
）预训练语言模型


对于自然语言而言
，
一个好的模型表达应该捕捉到文本数据中蕴含的语言规


１６


第二章背景知识与相关技术


贝
Ｉ
Ｊ
，语法和常识
，
如词义
、
句法结构
、依赖关系等
。而用分布式表示的核心思想


是用低维实值向量来代表文本的含义
。预训练语言模型的出现就是使得词语的分


布式表示可以根据上下文的不同而动态变化
。本文通过介绍ＢＥＲＴ模型
，来简单


介绍预训练语言模型
。


ＢＥＲＴ通过构建了两个预训练任务来完成对模型的预训练
。第
一个任务为掩


码语言模型
（ＭａｓｋｅｄＬＭ
）
，
具体做法为在训练文本中随机选取
１５％的单词进行


掩盖处理
，然后通过模型重构这些单词
，
即在输出层进行预测单词
。通过这种方


法
，模型可以学习到双向的语言特征
，
而不再单单是
一个方向
。其中
，
被掩盖的


１５％的单词中
８０％被替换成特殊符号
［ＭＡＳＫ］
，
有
１０％的被替换成另外
一个随机


的单词
，
另有
１０％维持原样
。这样增加了模型编码的困难性
，无法了解具体哪个


单词是需要被预测
，不知道哪些单词被随机替换
，从而获得
一个更加鲁棒的向量


表示
。


第二个任务是句子对预测
（ＮｅｘｔＳｅｎｔｅｎｃｅＰｒｅｄｉｃｔｉｏｎ
，ＮＳＰ
）
，
即判断数据中


的两个句子是否相邻
。有人认为ＬＭ缺失的是对句子之间关系的理解
，所以构造


了
一个二元的分类任务
，
预测句子Ｂ是否在句子Ａ之后
，
其中有
５０％的数据为


正样本
，
５０％的数据是在语料中选择任意
一句句子构成负样本
。


预训练得到模型后
，可以在具体的任务数据上
，进行微调
（Ｆｉｎｅ
－ｔｕｎｉｎｇ）
，
从


而使得模型更加适应下游任务
。
通过ＢＥＲＴ模型
，
展现了Ｐｒｅｔｒａｉｎ
－Ｆｉｎｅｔｕｎｉｎｇ模


式可以在各种下游任务上取得最佳效果
。


２
．５
．３图神经网络


深度学习在图像视频处理
、
语音识别
、
自然语言理解等任务已经大展拳脚
。


通常这些任务中所处理的数据在欧几里得空间中可以表示
。
ＣＮＮ等神经网络结


构可以有效的处理这种规则化的矩阵结构数据
，即每个节点的周围节点个数是确


定的
。然而
，对于不规则的或非欧几里德空间上的数据
，如知识图谱
、基因数据
、


论文引用
、
社交网络等
，
传统的神经网络结构如ＣＮＮ
、
ＲＮＮ等都难以处理
。
以


ＣＮＮ为例
，
其中的卷积核无法在不同节点上保持平移不变性
。
这类数据常常可


以使用图
（Ｇｒａｐｈ）
的形式来表示
，
从而更好地描述其中富含对象之间的复杂关


系
。


图在计算机科学中是
一种数据结构
，
其由顶点和边两部分组成
，
可以用Ｇ
＝


｛＆
！／
｝表示
。其中Ｋ表示图中节点的集合
，
Ｅ表示图中边的集合
，
表示了图中从


节点
ｉ到节点的边
。
图中节点的连接关系可以用
一个矩阵进行量化表示
，
称为邻


接矩阵儿
可以按照如下公式表示
：


卜
节点
１和
】雛接（２⑴


Ｕ）
，节点
ｉ和
ｊ无连接


１７


北京邮电大学工程硕士学位论文


对于此类型数据
，如果使用图表示学习
，可以同时编码图中的节点特征以及


图中所含有的结构信息
。
图神经网络
（ＧｒａｐｈＮｅｕｒａｌＮｅｔｗｏｒｋ
，
ＧＮＮ
）是
一种可


以直接作用在图数据结构上的神经网络模型结构
。
图卷积神经网络
（Ｇｒａｐｈ


ＣｏｎｖｏｌｕｔｉｏｎａｌＮｅｔｗｏｒｋ
，
ＧＣＮ
）是其中的
一
■种
。


图神经网络结构的目标是学习构建出的图的特征的映射
。模型不光需要输入


图的连接关系
，
即邻接矩阵
，
还需要输入表示每个节点的特征向量
。
ＧＣＮ是将


卷积操作引申
，使用在图结构的数据上
，通过卷积的方式获取每个节点的邻居节


点的信息
，
即每个节点的空间特征
，
最终来更新图中每个节点或图的向量表示
，


一般输出的也是每个节点的特征向量
，
如图
２
－７所示
。
对于多层ＧＣＮ而言
，
每


一层的映射可以表示如下
：


Ｈ
ｌ＋１＝
ｆｉＨ
ｌ
，Ａ）
（２
－
１２）


，ｄ（
― ― 
＾
丨）


Ｃ＠
Ｆ
亨
－
…
－Ｋｇ）


＠
Ｊ
－
＞＠


ｉｎｐｕｔｌａｙｅｒｏｕｔｐｕｔｌａｙｅｒ


图
２
－７
图卷积神经网络
［
１２
］


如上文所述
，
ＧＣＮ需要获取节点的邻接节点信息
，
而最直接的方式就是将


邻接矩阵和节点特征相乘
。
如公式
２
－
１３所示
：


ｆｉＨ
ｌ
，Ａ）＝ａ（ＡＨ
ｌＷ
ｌ
）（２
－
１３）


其中
１４＾是第层神经网络的权重矩阵
，
〇
？是非线性的激活函数
。但是上述模型仍有


缺陷
，
例如邻接矩阵乂通常没有进行归
一化操作
，
乘法操作很容易改变输入节点


特征向量的量级
。通过加入节点度的对角矩阵
，对邻接矩阵进行对称归
一化
，
如


下所示
：


ｆＣＨ
ｌ
，Ａ）＝ａ０
￣
ｉＡＤ
￣
ｉＨ
ｌＷ
ｌ
）（２
－１４）


其中
称为图的拉普拉斯矩阵
。


本质上
，
ＧＣＮ是谱图卷积的局部
一阶近似
，
是
一种可以直接在图上运行的


神经网络模型
，
利用逐层传播规则进行网络的更新
。
ＧＣＮ
的模型规模会随所应


１８


第二章背景知识与相关技术


用的图中边的数量的增长而线性增长
。
总的来说
，
ＧＣＮ可以用于对局部图结构


与节点特征进行编码
。


２
．６本章小结


本章首先阐述了关系抽取的任务定义和相关背景
，然后介绍了多示例学习的


基础知识
，
引出远程监督关系抽取
。紧接着介绍了远程监督关系抽取的背景和优


势
，领域中的相关术语
，
以及远程监督的原理。最后阐述了远程监督关系抽取的


任务定义和形式
。


１９


第三章
基于层级图神经网络的远程监督关系抽取模型


第三章基于层级图神经网络的远程监督关系抽取模型


在多示例学习框架下
，
一系列基于深度学习的远程监督关系抽取工作被提出


［
１８
，
１９
，５５
－５７
］
。为了更好的处理包中的噪音数据
，
其中
一部分的工作
［
１９
，２Ｇ
，２２
，２６３８
，
５８
，
５９］


采用选择注意力机制为包中的噪音实例分配了低权重来学习包表示
。然而
，他们


通过注意力机制
，将每个包的关系标签作为查询项
，实例作为键值项来计算实例


构成包所占的权重
，
是
一种隐式地利用实例之间联系的方法
。换言之
，现有的大


多数方法都忽略了包内实例之间全局的语义联系
。


远程监督的关系抽取的另
一个挑战是长尾问题
。使用远程监督获得的数据集


通常属于长尾分布
，每个类别数据出现的频数不尽相同
。通常少部分关系类别会


含有大量冗余的数据
，而获得的大多数类型的关系都缺乏标注数据
。针对长尾问


题
，
需要
一种直接有效并且泛化能力更强的方法从不平衡的数据中提取信息
。因


此
，本文不光提出了分层模型来学习训练实例中的局部和全局信息
，
以获得更鲁


棒的包表示
，还使用了最大化互信息的正则器来平衡模型的预测分布
，从而缓解


长尾问题
。


３
．
１整体框架


本文提出
一种从局部到全局进行学习的层级图卷积神经网络模型框架
，
即


（ＬｏｃａｌｔｏＧｌｏｂａｌＧｒａｐｈＣｏｎｖｏｌｕｔｉｏｎａｌＮｅｔｗｏｒｋ
，Ｌ２Ｇ
－ＧＣＮ
）
。
该框架利用分层


ＧＣＮ结构和最大互信息正则化器
，
从局部到全局地学习来增强包表示
，
以缓解


噪声问题和长尾问题
。
总体而言
，本文首先对实例进行依存句法分析
，根据依存


分析结果构建词级的
ＧＣＮ并更新
，
从而对单个句子的局部句法信息进行编码
。


然后
，
与现有的基于选择注意力机制生成包表示的方法不同
，
本文使用句子级


ＧＣＮ来捕捉实例之间的语义联系
，全局的学习包内的结构信息来调整句子表示
，


最后通过特征压缩聚合成增强的包表示
。具体地
，本文将包中的句子作为节点构


建
一个完全连通的加权图
。边的权重通过自注意力机制（Ｓｅｌｆ
－ＡｔｅｎｔｉｏｎＭｅｃｈａｎｉｓｍ
）


学习的
，
随着模型训练动态变化
。
此外
，
本文提出的Ｌ２Ｇ
－ＧＣＮ框架是以两阶段


的分层方式训练
，
先训练词级ＧＣＮ至拟合后
，
再加上句子级ＧＣＮ以完整的模


型框架进行训练直至拟合
。最后
，
本文加入互信息正则化器来增强Ｌ２Ｇ
－ＧＣＮ模


型的鲁棒性
，
以缓解长尾问题
。
长尾问题会导致模型的预测偏向于特定的类别
。


因此
，本文鼓励模型通过最大化输入和模型预测之间的互信息来预测更合理且更


２
１


北京邮电大学工程硕士学位论文


均衡的分布
。据本文所知
，这是首次将互信息引入远程监督关系抽取的研宄
。模


型的整体框架如图
３
－
１所示
：


Ｂｉ
－ＬＳＴＭ


ＰＥＷＥ


＾〇６
—

ｗａｓ

：
〇
〇
Ｓｏｉ
？ｅｉ？ｃｅ－
Ｉ＾ｖｄ
ＧＣＮ＾


〇〇—

＂
ｇ
：
－
．


１９Ｍ〇〇
．
＇
— ＊
－
ｆｒｉ
：
丨
；


—

＇ｎ
，
：
？
？


－ＯＯ— ｌＤ
；
＿
＇

＊


Ｘ５


图
３
－
１
Ｌ２Ｇ
－ＧＣＮ模型框架
，
其中包含输入层
，
Ｗｏｒｄ
－ＧＣＮ层
，
Ｓｅｎ
－ＧＣＮ层


总结本章的主要贡献如下
：


１
．提出了
一个简洁的Ｌ２Ｇ
－ＧＣＮ模型框架来学习具有鲁棒性的包表示
，以缓


解基于远程监督的关系抽取中的噪声问题
。
在这个模型中
，
词级别ＧＣＮ被用来


编码句子的局部句法信息
。
句子级ＧＣＮ被用来更好地利用句子间的相关性
，
将


句子间有关实体关系的全局结构信息聚合到
一个包表示中
。整个自底向上的模型


结构采用两级分层的方式进行训练
，
进而生成具有多粒度信息的包表示
。


２
．本文将互信息最大化
ＭＩＭ
引入至模型当中
，
以此来缓解长尾问题
。
以


ＭＩＭ作为正则化器
，约束模型的训练过程
，鼓励模型预测保持相对均衡的分布
，


提高模型的鲁棒性
，
提高模型在长尾关系上的分类性能
。


３
．本文在Ｒｉｅｄｅｌ提出的ＮＹＴ
－
１０基准数据集上进行了大量实验
。
实验结果


表明
，
本文提出的分层
Ｌ２Ｇ
－ＧＣＮ模型和层级训练策略是有效的
。
本文的
Ｌ２Ｇ
－


ＧＣＮ实现了卓越的性能
。


３
．２句法依赖关系获取


该部分主要介绍本文如何获得模型输入数据的相关特征以及相应的技术
。


３
．２
．
１相对位置特征


在基于远程监督的关系抽取任务中
，每个实例包含两个实体
，为了更好的抽


取两个实体之间的关系
，获得指向更明确的句子表示
，需要更加强调两个实体单


词在语句中所处于的位置
，
从而使模型更好地捕捉实体的信息
。
本文采用
Ｚｅｎｇ


等人
［７】提出的相对位置特征ＰＦｓ来突出句子中两个实体的位置
。相对位置具体指


的是语句当中
“
ＴＯＫＥＮ
” 与分别与头尾实体词＆和＆之间的距离
。例如
“ Ａｌｂｅｒｔ
ｏ


ｗａｓｂｏｍ
ｉｎＭｉｌａｎ
ｉｎ１９
１４
＿
”
，
“ Ａｌｂｅｒ
ｔｏ
” 和
“ Ｍｉｌａｎ
” 是语句中的两个实体
，以
“
ｂｏｍ
”


２２


第三章基于层级图神经网络的远程监督关系抽取模型


为例
，
其与
“ Ａｌｂｅｒ
ｔｏ
” 和
“ Ｍｉｌａｎ
” 之间的相对距离就是
２和
－２
，
其余词的相对位


置如表
３
－
１所示
：


表
３
－
１
相对位置示例


ＡｌｂｅｒｔｏｗａｓＢｏｍｉｎＭｉｌａｎｉｎ１９
１４
．


相对Ａ的位置０

１

２３

４

５

６

７


相对￡２的位置
丨
－４
｜
－３
丨
－２
｜
－
１
｜０１２３


位置特征矩阵通过随机初始化产生
。通过查询位置特征矩阵
，本文将相对距离转


化为实值的向量
，
即将￥中的第７
＿
个单词相对于实体词＆的距离转化为
一维度


的向量Ｐｅｉ？
ｄｐ
。


３
．２
．２依存句法分析


依存句法分析是自然语言处理中的
一项关键技术
，认为单词与单词之间存在


主从关系
，如果
一个词修饰另
一个词
，则修饰词称为从属词
，被修饰的词语称为


支配词
，两者之间则存在依存关系
，且这种依存关系有相对应的类型
。通过依存


句法分析
，则可以得到句子中词汇之间的有向依赖关系
。其通常将句子中的核心


谓语作为句子结构的中心词
，
即根节点
，该节点不依存于其他节点
，
而其他词语


均直接或者间接地依赖于中心词
，且如果单词Ｘ依存于单词Ｙ
，那么Ｘ
，
Ｙ之间


的单词Ｚ只能依存于
Ｘ
、
Ｙ或ＸＹ之间的单词
。
具体依存句法分析举例如图
３
－


２所示
：


，ｎｓｕｂ
ｊｖｙｐｕｎｃｔ＾


圆ｄ
：ｎｎ
Ｎ
ＮＭＴ
ｎｍｏｄ
：ｐ「ｅｐ＾ｒ
ａ＾＾
＇
＇４５＼Ｅｉｍ


ｆ八
ｖ
ｆ
— ￣入￣￣
＼
ｒ？
、
＾＇
ｖ


屠在
２０１１年９月发现了青蒿素
。


图
３
－２依存句法分析结果


在以往的研究工作中
，大部分仅仅依赖相对位置特征来建模句中文本的依赖


关系
。
因此本文尝试加入依存句法分析的结果
，
当两个实体之间的距离较远时
，


利用依存句法路径可以使模型忽略句子中与实体没有依赖的词
，更直接的建立两


个词语之间的联系
，使模型能更加关注有助于判断实体关系的信息
，
降低实体间


的长文本对关系分类的影响
。


为了捕捉长距离的依赖
，
本论文结合图神经网络相关结构和句法依存关系
，


用图结构来表示单词之间的依赖关系
，
用ＧＣＮ来编码每个单词的隐藏表示
。
最


终通过平均池化
（ＭｅａｎＰｏｏｌｉｎｇ）
的方式得到句子的完整表示
。


２３


北京邮电大学工程硕士学位论文


３
．３编码器


在这
一部分
，本文对模型的主要框架进行分模块描述
，其主要包含了输入层
，


词级图神经网络层
，句子级图神经网络层以及最后的分类层。紧接着介绍互信息


最大化
。最后给出本文的模型的损失函数以及训练策略。


３
．３
．１输入层


本论文设计的网络结构在编码层加入位置信息编码（Ｐｏｓｉｔ
ｉｏｎＥｍｂｅｄｄｉｎｇ，ＰＥ）
，


结合词表示（ＷｏｒｄＥｍｂｅｄｄｉｎｇ，ＷＥ）
，
通过双向长短时记忆循环神经网络（Ｂｉ
？


ｄｉｒｅｃｔ
ｉｏｎａｌＬＳＴＭ
，Ｂｉ
－ＬＳＴＭ）编码得到实例的向量表示。具体而言是将每个文本中


每个单词映射到
一个
维度的分布式词向量
位置特征向量
ＰＦｓ被用来编码


实体位置信息
，对于每个单词
，位置特征会被映射为
一个ｄ
ｐ维度的特征向量ｐｆ和


ｐｆ
，
其代表单词到相应的两个实体词的相对距离
，
具体介绍如
３
．３
．１节
。


如图
３
－１所示
，
在本文的模型当中
，
本文首先利用Ｂｉ
－ＬＳＴＭ来编码每个单


词的局部上下文信息
。本文输入单词＆进入Ｂｉ
－ＬＳＴＭ来生产前向向量／ｉｆ和后向


向量峙
。拼接两者［
／ｉｆ
；峙］来代表符号相应的隐藏状态
，
以此获取文本的上下文


表示
。


ｈ
ｔ
＝Ｂｉ
－ＬＳＴＭ（Ｘ
ｆ），ｈ
ｔ６Ｒ
２ｘｄ＾（３
－
１）


其中＆是
一个２ｘｒ
ｆｈ维度的隐藏状态表示
，
ｄＡ＝ｄｗ＋２ｘｄ
ｐ
。
实例集合


＝会被映射到
一个矩阵孖
＝认１
，九２，
…
，Ｕ，其中ｗ代表
一个批


次数据中实例的最长长度
。


３
．３
．２基于依存句法分析的词级图神经网络模块


相比于传统的特征工程方法，深度学习拥有更强大的线性和非线性网络结构
，


从而能够深度挖掘文本中含有的语义信息
，快速地拟合高维特征
，但是在处理前


后文字可能拥有强关联或者含有复杂逻辑关系的长文本时
，其表现仍有不足
。在


以往的研究工作中
，大部分相关工作依赖相对位置特征ＰＦｓ来解决此问题
。然而


仅仅依靠相对位置特征仍不能很好的建模实体之间的关系
。


本文尝试结合图卷积神经网络（ＧｒａｐｈＣｏｎｖｏｌｕｔ
ｉｏｎａｌＮｅｔｗｏｒｋ
ｊＧＣＮ）相关结构


和依存句法分析
。将句法依赖信息融入到语句的编码信息中
，生成同时包含语义


和句法信息的特征向量。当两个实体之间的距离较远时
，利用依存路径可以使模


型更直接地建立两个词语之间的联系
，使模型能更加关注于有助于判断实体关系


的信息
，
降低实体间的冗余长文本对关系分类的影响
。


２４


第三章基于层级图神经网络的远程监督关系抽取模型


本文设计的基于依存句法分析的词级图神经网络模块
（Ｗｏｒｄ
－ＬｅｖｅｌＧＣＮ
，


Ｗｏｒｄ
－ＧＣＮ）
，
如图
３
－３所示
。本文借助依存句法分析工具对文本进行了分析
，将


分析的依存结果转化为邻接矩阵
。基于该邻接矩阵
，
以词作为节点建立词的依存


关系图
。


Ｗｏｒｄ
Ｌｅｖｅｌ
ＧＣＮ




——
ｈ


ｗａｓＴｉｗａｓ


ｊ
Ｍｉｌａｎ
｜
Ｍｉｌａｎ


□请令十
：》
１
‘ 目


１９
１４
＿＿＿Ｊ
ｙ
ｊ＾
１９｝４
＿—
－
Ｉ
Ｊ｜
ｆ

＂
＂
＂
＂


图３
－３Ｗｏｒｄ
－ＧＣＮ模块


具体邻接矩阵的计算和构图的过程做法如图
３
－４所示
，
说明如下
：


１
．借助句法依存分析工具对句子进行依存句法分析
；


２
．将分析结果转化为无向图
，并用邻接矩阵表示
，具体步骤如算法
１所示
；


３
．将单词作为节点
，
利用图神经网络结合邻接矩阵对节点表示进行更新
。


Ｗｏｒｄ
－ＧＣＮＷｏｒｄＡｄｊａｃｅｎｃｙＭａｔｒｉｘ


＾００１０００００


《
＇、此
丫
Ｍ
ｉ
ｌ
ｙ
００１０００００


Ａ
ｌｂｅｒｔｆ
１
１
０１
０１
０１


＾３
００１０１０００


丨ｎ
＇＼
、
、０
０
０
１
０
０
０
０


００１０００１０


０
０
０
０
０
１
０
０


００１０００００




金


Ｒｏｏ
ｔ


：
Ａ
ｌｂｅｒｔｏｗａｓｂｏｍｂＭ
ｉ
ｌａｎ
ｉｎ１９
１４


，

．憂
— 、


ＤｅｐｅｎｄｅｎｃｙＰａｒｓｅｒ


—
—
全

、


Ｓｅｎ，ｅ？ｃｅ
ｉ〇Ｍ〇
ｉ
；〇ｎ〇ＭＣＭ〇Ｕ〇
：Ｋ＾Ｔ


（Ｑ
ｉ没
１
：２Ｋ〇Ｈ２．Ｈ°
Ｊ（Ａｊ


Ａ
ｌｂｅｒｔｏｗａｓｂｏｒｎｉｎＭ
ｉ
ｌａｎ
ｉｎ１９
１４


图
３
－４Ｗｏｒｄ
－ＧＣＮ模块中获得邻接矩阵并构图的过程


通过依存句法分析工具得到单词的依存树后
，本文通过算法
１将其转化为邻


接矩阵１Ｗｏｒｄ
－ＧＣＮ基于邻接矩阵７１对单词
ｉ（节点Ｚ
）
的隐藏状态表示￣进行更


新
。
本文定义４为
ＧＣＮ更新的输入向量
，
／Ｉ
丨为输出向量
，
则该总共Ｌ层的
ＧＣ


Ｎ更新Ｚ层的节点的公式如下
：


２５


北京邮电大学工程硕士学位论文


ｈ
ｌ＝ＲｅＬＵ
＾
Ａ［ｊＷ
ｌ
ｇ）＋ｂ＾
ｊ
（３
－２）


其中ｉ＝４＋１
，
Ｉ是
一个ｍｘｍ的单位矩阵
，
通过增加自循环部分来保留节点的


原始信息
。
此外本文设计了
一种类残差连接结构
，
其定义ＧＣＮ每层的输入特征


为沒ｊ
＝即将低于当前层的隐藏状态向量拼接＾代表Ｗｏｒｄ
－


ＧＣＮ第Ｚ层新的输入特征。除了／ｉ
；
，网络每层会将输入映射至ｄＡｌｄｄｅｊｌ
＝冬／Ｚ维度
。


＾是／ｉ
；
？相应的维度
。此外
，本文定义妒
ｅ表示第ｚ层的可学习参数
，


Ｗ４＋ｄＷｄｄｅｎｘ（Ｚ
－
１）
，ＲｅＬＵ（〇表示非线性激活函数。具体的表现形式可参


考图
３
－３
〇


算法ｈ
依存句法关系的路径抽取算法


输入：
Ｂａｇ中的每个实例Ｓ
ｆ


输出
：
以每个实例＆对应的依存句法关系路径构成的邻接矩阵Ｘ


算法


１
：
使用依存句法分析工具
ＬＡＬＰａｒｓｅｒ＾获取Ｓａ总中的每个实例￥对应的依存句法关


系路径
如图
３
－４的例子
，
获得形如［０
，
１
，
１
，３
，４
，５
，５
，
１］的路径
，
其中数字代表每个


词所依赖的词在句子中的位次索引
；


２
：
根据依存句法关系路径７＼构建依存关系邻接矩阵儿


ｉ）将实例＆中每个单词作为无向图的节点
，
如果两个单词之间存在依存关


系
，
则认为它们在无向图Ｇ中存在边
，
从而得到无向图Ｇ
。


ｉｉ）若总共有ｍ个单词
，则邻接矩阵４由ｍｘｍ个元素构成
，如果两个单词ｉｊ
’


节点之间存在边，
则置次，和＾元素为
１
，
否则置为０。


图卷积神经网络通过不断聚合其他相邻节点的信息至当前节点
，来更新当前


节点的向量。该图神经网络基于依存句法分析构造的邻接矩阵来更新节点
，使得


当前节点凭借存在的依赖关系更好的捕捉句子中真正与之有联系的节点信息
，从


而更准确的表示节点特征。换言之，该网络结构编码了该句子内的局部结构信息
，


与后文的包内的整体结构信息相对应
。


３
．３
．３基于自注意力机制的句级图神经网络模块


本文在多示例学习的基础上缓解噪声问题
。对于如何生成包表示
，先前的研


宄工作常采用平均池化包内实例的方式
，即为每个实例分配相同的权重。同时也


有相关工作使用注意力机制来为包内的实例分配权重
，降低包内的噪音实例的权


重
，来构成包的表示。然而这些方法没有显示的考虑包内实例之间的关系
，他们


忽略了包内实例间的语义联系
。与实例的局部结构信息相对应
，本文称这种联系


为包内实体关系的全局结构信息
。


２６


第三章基于层级图神经网络的远程监督关系抽取模型


针对以上所述问题
，
本文提出了基于自注意力机制的句子级别图神经网络
，


捕捉包内实例之间的关系
，学习全局的结构信息
。本文认为如果两个实例表达的


实体对之间的关系相同
，则在某种程度上或者某个部分上这两个实例存在结构性


的相似或联系
。相反
，噪声实例则会与其余数据有所不同
。如图
２
－
１所示的例子
，


句子Ｓ１和
Ｓ４都表不
“
／ｐｅｏｐ
ｌｅ／ｐｅｒｓｏｎ／ｎａｔｉｏｎａｌｉｔｙ
” 关系
，
他们含有相似的语言结


构
“ ＲｏｎａｌｄｏｏｆＢｒａｚｉｌ
”
，反观句子
Ｓ２不包含相似的语言结构
，
同时所表达的语义


也完全不同
，
因此这个句子有更大的概率是噪音
。为了捕捉这种全局的结构相似


性以及语义联系
，获得
一个鲁棒性更强的包表示
，本文提出了基于自注意力机制


的句级的图神经网络
（Ｓｅｎｔｅｎｃｅ
－ＬｅｖｅｌＧＣＮ
，
Ｓｅｎ
－ＧＣＮ）
，
如图
３
－５所示
。


ＢａｇＳｅｎｔｅｎｃｅＡｄｊａｃｅｎｃｙＭａｔｒｉｘＳｅｎ
－ＧＣＮ


’、Ｓ１Ｓ２Ｓ３Ｓ４Ｓ５



Ｓ１
；Ｓ１
１Ｂａｇ


—
Ｓ２
１Ｓ２
ＳＩ


：Ｓｅ
ｌｆ
Ｓ５


［
Ｓ３
｜Ａｔｔｅｎ
ｔｉｏｎＳ３十於以
…


：Ｍｏｄｕ
ｌｅ


ｌ
５４）
５４
Ｓ２Ｓ４


「
ＳＳ
—Ｓ５
〇
幻



－

－—
…
－
—

— －
｛


＇
、／


图３
－５Ｓｅｎ
－ＧＣＮ模块


算法２
：
句子级别的无向图构建


输入
：
５叫中的每个实例的表示￥


输出
：
每个实例Ｓ
ｆ更新以后的表示


算法流程
：


１
：
以包内每个实例作为节点Ｓ
ｉ
；


２
：
假设每两个实例之间都有相关性
，
构造
一个无向连通图Ｇ
；


３
：
使用自注意力机制计算每两个实例
ｉ，Ｊ之间的注意力分数
，
以该分数
作为邻


接矩阵的值
，
视为两个实例之间的相关程度
。


使用基于自注意力机制的语句级图神经网络分为以下几个步骤
：


１
．构建句子级别的无向图
，
如算法２所示
；


２
．根据邻接矩阵权重对节点进行更新
，
获取实例的新表示
。


具体而言
，
在得到
３
．３
．２节所述的词级图神经网络更新的单词的隐藏状态表


示后
，
本模块以最大池化方式获取实例的隐藏状态表示
。
公式如下
：


［ｓ］ｊ
＝
ｐ〇〇ｌｍａｘ
（Ｗ］
；）
（３
－３）


其中
代表向量）位置上的值
。


２７


北京邮电大学工程硕士学位论文


假设包
＝
｛＆，ｓ２，
．
．
．ｊｆ｝
，获得包内实例隐藏状态向量表示后
，本文按


照算法２建立以实例为节点的无向全连通带权图
，
邻接矩阵为Ｃ
ＧＲＷ
，
数值


代表相应实例之间的相关性权重
。与词级别的图神经网络不同
，实例图的邻接矩


阵不仅仅由
０或
１构成
，每条边都赋予了之间的权重
，其权值代表了两个句子之


间的相关程度
，
并且此权重可由模型端到端的学习得到
。
具体公式如下
：


广
＾ｘ（欠Ｗ、… 、


Ｃ＝Ｓｏｆｔｍａｘ
（
－
＾
＝＝
Ｊ
（３
－４）


其中
，
办
，
表示可学习的参数
，
代表输入句子特征的


维度
，
Ｑ
ｅＲｆｘ＃和Ａ：ｅＲ
Ｔ
Ｃ
Ｓｘｄｈ均是句子的向量表示
，
（３表示查询项
，
尺表示键值


项
，此处两者相同
。注意力计算
一般分为三步
，首先是利用相似度得分函数对查


询项与键值项进行相似度计算得到相似度得分
；
然后利用Ｓｏｆ
ｔｍａｘ函数对相似度


得分进行归
一化
，归
一化后的相似度得分被称为注意力分布
；最后将注意力分布


与值项进行矩阵内积得到最终的注意力加权后的结果
。本文只采用了前两步
，通


过自注意力机制计算包内实例间的相关性程度
，将归
一化后的分数作为边的权重
。


得到邻接矩阵后
，
即可根据公式
３
－２对节点进行更新
，
得到加强的实例表示。


３
．４互信息最大化正则器


远程监督数据集的数据存在不平衡现象
，这
一现象被称为长尾问题
，具体如


图
２
－２所示
。数据集中部分类别的数据稀缺
，ＮＡ数据甚至占了整个数据集的
７２％
，


对于位于数据集分布尾部的关系类别
，几乎没有可用的训练数据
。数据的不平衡


会影响模型的训练
，导致模型的预测很容易偏向于某
一个类别
，这在
一定程度上


损害了模型的鲁棒性
。针对这
一问题
，本文引入了最大化互信息的方法
，将最大


化互信息视为
一种正则化方式
，通过对模型权重的调整
，学习到更有区分性的特


征
，
互信息的计算公式如下
：


Ｉ（Ｘ
；Ｙ）
＝Ｈ（Ｙ）
－Ｈ（Ｙ
ＩＸ）（３
－５）


其中Ｆ为模型预测的结果
，
Ａ
■为模型的输入特征
。


相应的正则化函数定义如下
：


＝Ｅｙ［ｌｏｇｐ９（ｙ）］
－Ｅｘ＾
ｐ０（ｙ
｜ｘ）
ｌｏｇｐｅＣｙ
｜ｘ）（３
－６）


？ｙ


其中
，
ｐ（ｆ）表示预测的分布
，
ｐ０（ｙ
｜ｘ）表示预测输出的概率
。最大化互信息
，


根据公式
３
－５可以看作两部分
，即最大化／／〇〇和最小化／／（ｙｐ〇
。通过该方案
，最


２８


第三章
基于层级图神经网络的远程监督关系抽取模型


大化／／〇〇
，
提升Ｋ的信息熵
，
即不确定度
，
预测的不确定性越高
，
预测的分布越


均衡
，
以此来防止模型倾向于某些类别
。最小化／／（ｙｐ〇
，最小化条件熵
，
维持模


型预测的平衡性
，
同时也提升了模型在预测时的置信度
。
受Ｌｉ等人
ｔ６
１
］启发
，
本


文计算了
Ｐｅ（ｙ）作为ｐｅ（ｙ
｜ｘ）平均值的近似值
。


３
．５两阶段层级的训练方式


ＴｒａｎｓｆｏｒｍｔｏＢａｇｓａｎｄＳａｍｐ
ｌｅＴｗｏＳｔａｇｅＨ
ｉｅｒａｒｃｈ
ｉｃａ
ｌＴｒａ
ｉｎ


ｃ
—
＾
Ｉ（ｉｎ
－＾
，


ＩｎｓｔａｎｃｅＢａｇ１．
．．＼
’ＬｏｓｓＦｕｎｃｔ
ｉｏｎ


Ｓｅｔ




＾—＾Ｑ…
ｓｔｇｇｅ２“



＾
ＳｅｎＧＣＮ
＾
）



图
３
－６层级训练流程图


受到人类由局部到整体的认知方式的启发
，本文将模型训练分为两个阶段的


层级方式
。第
一阶段
，本文先训练词级别的图神经网络直至收敛
，从而先获得
一


个良好的局部结构信息
。依据公式
３
－３获得实例的隐藏状态表示后
，将个实例向


量平均
，
映射到输出向量即包表不
。


ｍ


ｂ＾
＝
ｂｌ
Ｓ
ｉ
（３
＿７）


ｉ＝ｌ


将包表示分类到
ｋ
－ｔｈ关系中的分数计算如下
：


° ｋ
－Ｍｂｈ
ｉ
ｔｅ
ｉ＋＾
（３
＿８）


其中
ｅ是偏置项
，化是可能的关系数量
，Ｍｅ是关系的嵌入矩阵


表示
。


而后本文通过函数
，
定义了包正确分类到第／Ｃ个关系的条件概率
：


Ｐ（ｒ
＼Ｂ
；０）
＝：
Ｐ（〇ｒ）
（３
－９）


＾ｆ
ｃ＝ｉ
ｅｘＰ（° ｆ
ｃ）


第二阶段
，本文训练完整的模型结构
（包含词级别和语句级别的图祌经网络


结构
）直至收敛
。具体而言
，通过语句级图神经网络结构得到增强的句子表示后
，


仍用公式
３
－７得到第二阶段的包表示
。而后的分类过程同公式
３
－８
，３
－９
。该阶段


中
，模型以端到端的训练方式利用已经学习过的局部结构信息来更好地聚合包内


全局的结构信息
。
整体训练流程图如图
３
－６所示
，
损失函数如下所示
：


２９


北京邮电大学工程硕士学位论文


ｎｂ


￡ｃ
＝－＾
Ｚｏ沒ｐＯ＾
ｌｈ
；０）（３
－１０）


ｉ
—
１


＾ｍｉ
＝Ｅｙ［
ｌ〇ｇｐ〇（ｙ）］
￣
Ｅｘ＼＾
ｐｅ（ｙ
＼ｘ）ｌｏｇｐ０（ｙ
＼ｘ）］（３
－１
１）


ｙ


＝Ｘｃ＋尤ｍ，
（３
－１２）


其中表示训练数据中包的个数
，
代表包的标签
，
代表模型所有的参数
。


最终
，本文的模型通过随机选取ｍｉｎｉ
－ｂａｔｃｈ来在训练数据集上训练直至拟合
，


采用随机梯度下降
（ＳｔｏｃｈａｓｔｉｃＧｒａｄｉｅｎｔＤｅｓｃｅｎｔ，
ＳＧＤ）优化算法来最小化损失函


数
。


模型整体的训练流程如算法
３所示
：


算法３
：Ｌ２Ｇ
－ＧＣＮ模型的训练流程


数据：
远程监督数据集Ｄ
，
关系集合Ｒ，
邻接矩阵Ａ


输入：
包＆


输出
：
关系ｒｅＲ


雛化：


为Ｌ２Ｇ
－ＧＣＮ模型Ｍ初始化参数０


Ｇ
＝
｛０］，Ｇ
ｒ
＝
｛０｝


ＩｇｉＩｇ＞ｌ
—ｂｎａｘｉＩ
ｍａｘｉ？


算法雜：


１
：
分割数据集为包的形式
，
即Ｚ）＝
｛的，＾
Ｂ＝
｛ＳｐＳｚ
，
．
．
．
，ｓｎ
ｓ
｝
，


Ｓ
＝


２
：
随机选取
一个小批次数据
，
输入包数据进入模型


３
：
按照公式３
－
１编码单词为＆


４：：ｗｈｉｌｅｌ＜ｌＧｄｏ


根据公式３
－２
，
结合邻接矩阵Ａ更新符号表示


Ｇ＊
－Ｇ
＼Ｊ｛ｘ｝


ｌ
＾
－
ｌ＋１


ｅｎｄｗｈｉｌｅ


５
：
拼接集合Ｇ中的元素
，
依据公式
３
－３得到实例ｓ的表示


６
：
依据公式
３
－１２得到包表示６


７
：
根据ＳＧＤ算法，
基于ｆ
ｃ的梯度更新参数０


８
：１＾
０
，Ｃ＝
｛０｝


９
：
重复步骤２
－６直到Ｗｏｒｄ
－ＧＣＮ拟合
，
或者超过限制的轮次数


１０
：
根据公式３
￣４计算实例的邻接矩阵Ｃ


３０


第三章基于层级图神经网络的远程监督关系抽取模型


续上表


算法３：Ｌ２Ｇ
－ＧＣＮ模型的训练流程


１１
：ｗｈｉｌｅＩ＜ＶＧｄｏ


根据公式
３
－２
，
结合邻接矩阵Ｃ更新符号表示


Ｇ
＇
ｉ
－Ｇ
＇
Ｕ
｛ｓ｝


ｌ
＊
－
ｌ＋１


ｅｎｄｗｈｉｌｅ


１２
：
拼接Ｇ
’ 中的元素
，
得到实例ｓ表示
，
并依据公式３
－１２得到包表示


１３
．
？
根据ＳＧＤ算法
，
基于６
？的梯度更新参数０


１４：Ｚ
＜
－０
，Ｇ
＇
＝
｛０｝


１５
：
重复步骤２
－１４至Ｌ２Ｇ
－ＧＣＮ模型拟合
，
或者超过限制的
ｅｐｏｃｈ数


１６
：
通过全连接层得到关系ｒ


１７
：
返回ｒ


３
．６本章小结


在本章节中
，
本文介绍了
一个新的远程监督关系抽取框架
，
即
Ｌ２Ｇ
－ＧＣＮ
，


该框架利用分层
ＧＣＮ和最大互信息正则化器从局部到全局学习增强的包表示
，


以缓解噪声问题和长尾问题。


具体来说
，本文首先利用自然语言处理中的句法依赖解析工具
，对文本进行


依赖解析
，
结合依赖项解析结果进行单词级ＧＣＮ操作
，
对单个实例的局部语法


信息进行编码。


然后
，
与现有的采用注意力机制表示包的方法不同
，
本文使用语句级
ＧＣＮ


来捕获包内实体关系的全局结构信息
，调整实例表示来增强包表示。此处
，本文


以包中的实例为节点
，将其转换为完全连通的带权图
。其中边的权重通过自注意


力机制学习
。值得注意的是
，本文提出的Ｌ２Ｇ
－ＧＣＮ是以两阶段分层方式训练的
。


先拟合词级ＧＣＮ网络
，
再拟合完整的模型框架
。


最后
，
本文加入最大化互信息的正则化器来增强Ｌ２Ｇ
－ＧＣＮ模型的鲁棒性
，


以缓解长尾问题
。


３
１


第四章基于预训练模型的异质图神经网络远程监督关系抽取模型


第四章基于预训练模型的异质图神经网络远程监督关系抽


取模型


根据第三章所述
，本文设计了
一种层级图神经网络结构来处理远程监督关系


抽取问题
。然而
，预训练语言模型目前凭借其庞大的参数量和编码能力
，
已经成


为当下主流的基线模型
。因此本文尝试引入预训练模型
，验证其在远程监督关系


抽取任务上的效果
。
同时
，本文考虑到实体信息对于关系抽取任务的重要性
，设


计了引入实体信息的异质图祌经网络结构
，提出了基于预训练模型的异质图神经


网络远程监督关系抽取模型即
（ＨｅｔｅｒｏｇｅｎｅｏｕｓＧｒａｐｈＣｏｎｖｏｌｕｔｉｏｎａｌＮｅｔｗｏｒｋｂａｓｅｄ


ｏｎＢＥＲＴ
，
ＢＨ
－ＧＣＮ）
。
本章主要介绍模型ＢＨ
－ＧＣＮ的具体设计
，
相关实验见第


五章
５
．５
．３节
。


４
．
１整体框架


对于基于远程监督的关系抽取任务
，本文的方法不仅考虑了包内单句文本中


的语义信息
，也考虑了包内多个实例之间的联系
，但是对于实体关系抽取任务而


言
，实体有关信息的重要性没有被体现
。
因此
，本章提出引入实体的异质图祌经


网络结构
。除了将所有的实例表示为节点
，还将文本中的头尾实体单独提出
，作


为两个额外的节点
，将所有节点的关联表示为边
，从而得到
“ 实体
－实体
”
，
“ 实体


－实例
”
，
“ 实例
－实例
” 三种类型的边
。
具体的构图过程如图
４
－
１所示
。


ＧｒａｐｈＣｏａｓｔｎｉｃｔｉｏｎ


Ｔｅｘｔ
，Ｔ
￣
：
￣ｉ


？
＾
ＳＩ：
Ｆｏｒ
ｈｉｓ
ｐａｒｍｅｒ
ｕｐ

ｆｒｏｍ
，
Ｐｏｄ＜４ｓｋ
ｉ
ｄｘ？ｅ
ＲＡＢｕｋ
ｌｅｏｆ

：


＾
Ｅｎｔｆｔｉｅｓ
、
Ｂｒｕｉ
ｔｍｈｅｒ
ｔｈａｒ．
ｐ
ｉｃｋａ
ｉｇ
Ｋｌｏｓｅ
—


ＨｏｎａＳＯＡ
ｉ


，沿
？
？
ＨｉｔｈｂｐａｒＤＫＴ
ｕｐ

ｌｉｏｎＬ
ＰｕＪｕｂｉｉ
ｃｂｉＭ
ＲｕｎａＵａｏＴ
＇
Ｎ．



Ｂｒｏｓ
！
ｍｆ
ａｅｒ
ｔｈａｎｐ
ｉｃｋｉｎｇ
Ｋｌｏｉｅ
一
－，
－一Ｂｒａ＾Ｓ


ＳＪ：Ｆｏｒｈ
ｉｓｐａｒ
ｔｎｅｒ
ｕｐ
（ｒｏｏｔ
，
Ｉ
＊
？ｄｏｌｓｋｉｃｈｏｓｅ
Ｋｏｎ？
Ｊｄ
＜
？ｏｆ
＇
ｖ
：Ｋｔｆｎｏｉｄｕ／
，
＇二





—

Ｎ．ＳＪ：
Ｆｏｒｂｉｓ
ｐａｒ
ｔＤＣ
Ｔ
ｕｐ
＆？＊
，
Ｐｂｄｏｋｋｉ

？＊〇？ｏｆ

ｊ


Ｓ２：
Ｒｏｎａｌｄｏｋｆ
ｌ
ｔｈｅ
ｇｓｍｅ
ｕｉｄ
ｉｅ
６９ｔｈｍｉｎｕｔｅ
，
ｔｏｄＡｃ
Ｓｒｓｚｉ
）
ｎｓｄｘｒ
ｔｈａｎ
ｐ
ｉｃｋ
ｉｎｇＫｌｏｓｅ
ｎｃｗ＾
Ｍ
＾
ｃｒｒ
ｅｖｉｅｗ＊ｗｅｒ
ｅｍｅｒ
ｅｉｋｓ

ｉｎ
Ｂｒｕａｔ
．
Ｎ．


—
Ｂｒａｇｇ
＇
ｓＲｎＢａＭｃ
ｓ
ａ＾
￣
ｅ
ｉ＾ａ
ｌｅｒ
ｎｕｅｂｅｆ
ｏｒ
ｅｔ
ｉｉｅ

１９９８
＇
Ｖ
；




￣


Ｓ４：
Ｐｅｒ
ｔｉ＾Ｍｒ
ｅｃａｌ
ｌ
ｉｎｇｂｉｎｖＲｅｕｉｄｅ
？＞ｆ
ｌｉｒａｊｆ
ｉ
ｃｅｉｄｎｔｃｄ
？
？
ｃｕｐａｎｎｏｕｎｃｉｎｇ＊ａ
ｔ？
？
？Ｆ？
ｈａ
ｐ
ｓｍｕａ
ｕｐ
ｆ
ｅｏｎ＾
Ｐｏｄｏｆ
ｅｆ
ｃｉ
ｃｈｏ？ｃ
？？？＊
丨
ｏＨ


— 
—
Ｂｒｓｚ
ｉｉ
ｒａｄｓｃｒ
ｔｈａｎｐ
ｉｄｄｎｇＫ
ｌｏｓｃ
．


图
４
－
１
异质图构建过程


构图之后
，利用图卷积网络和自注意力机制学习图中节点表示
，最终利用异


质信息融合模块结合异质信息
，并使用池化操作进行特征压缩得到包表示
。模型


３３


北京邮电大学工程硕士学位论文


ＢＨ
－ＧＣＮ整体框架如图
４
－２所示
，
该网络框架主要包括四个部分
：
输入编码层
、


异质图神经网络模块、
异质信息融合模块和分类器
。


ｅｓｓ
觸


，
＂ｒ


ｆｈＲｉ降￥


：
＂１；


ＨｉＸＪ
＇ｔｉ


Ｉ＾ｒｍｍｍ＾
门娜ａｋ、


翁
：
ｉ０
Ｉ


Ｖｔ
ｉ
＿＿＿ｙｖ＿＿
，
＞Ｊ


，
丨
Ｉ


＾」异质圉网络结构


广
＇
（


０
｜７＆５１？１
｜
（１
？
－
？Ｔ
＼
ｈｍｉ？ｅｉＣ
）＾Ｅｎｔ
ｉ
ｆｊａ
＾
｜ｗｕｔ？ｅ
（Ｄ
；
？？
．
｜＾ＳＥＰ
）

ｊ


ＢＥＲＴ


＾
ｆＣＬＳ
ｉ
？
？
？＾
Ｊ
ｉｍｕｅｄＣｉ

－＾ＥｎＵｔｐ
ｉ？ｎｎｕ＊ｅ
？ｉｌ
＇
－
？
？＾
ｕｎ？＊？Ｃ
］＾Ｂｎｔ
ｉｔ
ｊｐ＾
［
ｘｍｕｔｅｄｉ
？
？
■


令
心
令
？＜
、
—〇令
心心


｜
ＩＣＬＳ
】
｜
－
（ｕｎｕｓｅｄＯ
］
｜ｊ
Ｅｎ
ｔｉｔｙ
ｌ

｜
｜
【ｕｎｕｓｅｄ”

｜
．
．
．

［ｕｎｕｓｅｄ２
Ｊ

｜

Ｅｎｔ
ｉｔｙ２

｜
｜

［ｕｎｕｓｅｄ３
Ｊ

］
．
．
．
｜
［ＳＥＰ
］
｜


包
－
■
…
１－
一
－
－
－

— ：
－
－
一
―


１ｆＣＬＳ
］
１
．
．
．


图‘ ２
异质图模型框架
（根据实体
、
实例不同的组合方式进行不同的更新）


４
．２编码器


４
．２
．
１输入层


本文引用了预训练语言模型作为该模型的基础编码层
，来验证其在远程监督


关系抽取任务上的有效性
。本文选择最基础的ＢＥＲＴ
－ｂａｓｅ英文版本
。有关的符号


表示同第三章
。


３４


第四着基于预训练模型的异质图神经网络远程监督关系抽取模型


实例表示头实体表示
尾实体表示


身
＿＿会
＿＿金
—
—
－—
．— －－


Ｃ＾
［ｗｍｓｅｄＯ
］＾Ｅｎｉｉｔｙｉ
｜
－
－
－＾
｜
ｉｍｕｓｅｒ
ｆ２
ｊ

；１
，
Ｅｎｔｉｔｙ２＾ＳＥＰ
］


ＢＥＲＴ


＾
［ＣＬＳ
＼
－
＊
－
忍
［ｍｔｔｓｅｄＯ
ｊ＾Ｅｎｉｉｔｙ
ｌＪ＾
［ｕｎｕ？ａｆ２
］＾Ｅｎｔｉｌ
ｊｆ
ｉ？
ｊ
ｔｍｔｉ＊ｅｄ３
■
■
■＾＊
＼ＳＥＰ
］


／＼八Ｚ
＇
／＼／＼／＼


ｒ
＇＾
ｉ
ｒ
＊￣
！
ｒ
＊
ｒ
￣


［ＣＬＳ
］
…
［ｕｎｕｓｅｄＯ
］
；Ｅｎ
ｔｉｔｙｌ［ｕｎｕｓｅｄｌ
ｊ
．
．
．［ｕｎｕｓｅｄ２
］Ｅｎｔ
ｉｔｙ２Ｉｕｎｕｓｅｄ３
］
…
［ＳＥＰ
］


ＢＡＧ
— －
■
－


［ＣＬＳ
］
．
．
．


图
４
－３
模型输入层


如图
４
－３
所示
，
对于所有的实例
，
本文将符号序列依照
｛
［ＣＬＳ］
，


［ｕｎｕｓｅｄＯ］
，ｅ
ｆ
ｃ
，［ｕｎｕｓｅｄｌ］
，
…
，
［ｕｎｕｓｅｄ２］，ｅ
￡
，［ｕｎｕｓｅｄ３］
，ｘｎｗ
，［ＳＥＰ］｝的格式作


为ＢＥＲＴ模型的输入
。即在头实体和尾实体的前后插入ＢＥＲＴ预留的
［ｕｎｕｓｅｄ］符


号表示
，利用ＢＥＲＴ等预训练模型的上下文嵌入特性
，利用不同上下文学习每个


符号
。
然后
，
本文将
［ＣＬＳ
］看作实例的表示
，
以
［ｕｎｕｓｅｄＯ
］作为头实体的表示
，


［ｕｎｕｓｅｄ２
］作为尾实体的表示
，提取出相对应的隐藏状态札
，
Ｇ１＾构建


异质图
。


４
．２
．２异质图神经网络结构


这部分主要描述异质图神经网络结构
。该图结构将实体
、实例表示为图中的


节点
，根据不同的节点构建不同的边
，然后利用改进的图卷积网络进行节点间的


信息传递
。


实例表示５
实体表示五
实例表示Ｓ
’ 实体表示五
’


／＾（
：
／
！
丨
｜
’
丨
Ｉ


Ｃｌ
Ｉ


ｖ＾异质图网结结构


实例表示瓦錢体表示办駿体表示私


香｜香


图
４
－４
异质图神经网络结构


３５


北京邮电大学工程硕士学位论文


如图
４
－４所示
，
图中节点可以归为两类
，
即实体节点￡和实例节点５
■
，而边则


可以归为三类
，
即实体
－实体￡
■
－￡
■
，
实例
－实例Ｓ
－Ｓ
，
实体
－实例￡ＸＳ
。
本文设计了
一


种新的图节点更新方式
，根据边的类型不同
，
依次对图进行更新
。我们以实体表


示为例介绍节点的更新
，
假设模型按照五
－￡
■的边进行更新
，
此时模型只考虑图中


的实体节点
，
实体节点之间两两有边
，
模型按照上文公式
３
－４
，
计算连接节点的


边所含权重
，得到
一个全连接带权图
。然后
，模型按照图卷积网络的更新公式４
－


１进行计算
，
从而得到新的实体节点方表示
。
以此类推
，
按照上文公式
３
－４
，
实现


对Ｓ
－Ｓ边权重的更新
，
通过公式
４
－
１
，
得到新的实例表示
。通过对边
的更新
，


结合公式
４
－
１得到新的实体Ｅ和实例Ｓ表示
。
通过以上更新方式
，
实现了实体
－实


体
，
实体
－实例
，
实例
－实例之间带有选择性的信息交互
，
让最终节点表示可以获


得更加丰富的信息
，
以此来增加分类的准确性
。
具体公式如下
：


ｒＥ
ｌ＋１＝
ｐ｛ＡｅＷ＾Ｅ
１＋ｂ
ｌ
Ｅ
）


？５
Ｚ＋１＝ｐ
（ＡｓＷｊＳ
ｌ＋ｂ
ｌ
ｓ
）（４
－１）


、五
，Ｓ＝
Ｐ（
．＾ｅｓ＾ｅｓ１＾
１
＞＾
１
］＋


其中ｐ表示ＲｅＬＵ激活函数
。依照异质图网络结构多层更新后
，最终得到四个矩阵


表示
，
即实体表示￡
■
，
实例表示５
？
，
实体表示￡
＂
，
实例表示Ｖ
。


４
．２
．３异质信息融合模块


在得到实体和实例￡
■
，
５
，Ｐ
，
Ｖ四个矩阵表示后
，本文设计了
一种基于异质


信息的融合门控机制
，
目的在于建模以仅在同类型信息范围内交互得到的矩阵表


示￡
？
，
５
，
和在不同类型信息间互相交互得到的矩阵表示￡
＂
，
Ｓ
＇这两种信息对同
一


实体或实例表示的贡献
，使得两种信息相互指导
，得到具有不同信息贡献的实体


和实例特征向量表示
。


＾Ｅｆｉｎａｌ＾


！
［９［＾
―Ｓｏｆｔｍａｘ
—＞
；


Ｉ
ｔ
＜个
“
；


ｉ＿
：


■ｔ
：


！ｒ
－ＪＣｏｎｃａｔ
ｒ＼
■


：
雙ｔ
—
；ｔ，
：


ｔ＿ｔ
：


：— ｒ
ｉ


；

Ｃｏｎｃａｔ
ｉ


．令
门控组件
：


＇
ｆ
－
—＾

＇


ＥＥ
ｒ


图
４
－５
异质信息融合模块


３６


第四章基于预训练模型的异质图神经网络远程监督关系抽取模型


如图
４
－５所示
，
以生成实体特征向量表示为例
，
具体做法如下
。本文将两种


实体表示￡
■
，
扩矩阵拼接
，乘上
权重矩阵后
，
再拼接上实体表示Ｆ
，
而后再通


过乘上
权重和
Ｓｏｆ
ｔｍａｘ操作后获得门控向量ｇ
，
ｌ
－
＿ｇ
。接着实体表示￡
■
，
￡
■＇通


过两个不同的全连接网络后
，通过相应的门控向量过滤信息
，并相加获得融合后


的实体表不
。


Ｑ＝
［Ｅ
；Ｅ
＇
］
（４
－２）


ｇ
＝Ｓｏｆｔｍａｘ（ＷＥ２
［ＷＥｉＱ
：Ｅ
］）（４
－３）


Ｅｇａｔｅ
— ＷＥ３Ｅ＾ｇ
（４
－４）


Ｅ
＇
ａａｔｅ
＝ＷＥ＾Ｅ
＇？
（ｌ
－
ｇ）（４
－５）


Ｅ
ｆｉｎａｌ
＝Ｅｇａｔｅ？５＋Ｅ
＇
ｇａｔｅ
（ｇ
）（１
－（４
－６）


同理得到５
＞
ｉ７ｉａＺ
。
之后本文对
Ｓ
／ｉ７ｉａｉ做平均池化得到两个向量ｅ
，
ｓ
：


ｅ
— Ｐ〇〇ｌｉｎｇ＾ｊｅａｎ（￡
ｙ
－
ｊｎａｊ）
（４
－７）


Ｓ— Ｐｏｏｌｉｎｇｊ＾ｅａｎ
（４
－８）


４
．２
．４分类器


获得模型更新后的图表示后
，将其映射至包表示
，最终用包表示进行关系分


类
，
通过分类器来输出最终的关系类别的概率
，
具体如下
。


酿示


＾
分类器



Ｉ＾

全雜


头例表７Ｆ实体表不


Ｆｈ■Ａ


ｙｊｕｊ４．ｎ
．


图
４
－６分类器结构图


如图４
－６模型将得到的两个向量表示
，输入两个全连接层
，并将输出相加得


到最后的包表


ｂｈ
．ｅ
．＝ＭＬＰ＾ｅ）＋ＭＬＰ
２ｄｓ）（４
－９）


用包表示做最后的分类
：


ｒ＝Ｗｂｈｂｅ
．＋ｂ
（４
－
１０）


３７


北京邮电大学工程硕士学位论文


损失函数和互信息最大化正则器的使用同第三章的Ｌ２Ｇ
－ＧＣＮ框架
。


４
．３训练算法


模型整体的训练流程如算法４所示
：


算法４
：ＢＨ
－ＧＣＮ模型的训练流程


数据：
远程监督数据集Ｄ
，
关系集合Ｒ，节点类型集合
幻


输入：
包＆


输出
：关系ｒｅＲ


嫌化：


为ＢＨ
－ＧＣＮ模型Ｍ初始化参数０


Ｇ＝
｛０｝


【Ｇ
＇Ｉ
—［ｍａｘ，〇


算法臟


１
：
分割数据集为包的形式
，
即Ｚ）＝
ｄＡ
，
Ｂ＝
｛ｓｐＳｈ
．
．
．
，ｓｎ
ｓ
｝
，


Ｓ
＝
１ｘ＾Ｘ２，
．
：
，ｘｎｗ｝


２
：
随机选取
一个小批次数据
，
数据以｛［ＣｉＳ］
，
ｊｃｎｗ
，
［Ｓ￡Ｐ］｝格式输入


３
：
使用
ＢＥＲＴ编码数据得到Ｈｃ，／／
ｅ
／＾／／ｅ
ｔ
６
ｌＲ
ｄ


４
：ｗｈｉｌｅｌ＜ｌＧｄｏ


ｆｏｒｆｉｎ０


根据公式
３４
，
更新邻接矩阵Ａ
ｊ


根据公式４
－１对节点类型ｉ进行更新


Ｇ＜
－ＧＵ
｛
￡｝


Ｚ＜
－
Ｚ＋１


ｅｎｄｗｈｉｌｅ


５
：
拼接集合Ｇ中的元素
，
依据公式４
－３
，４
＞４得到ｆ／ｉｎａｌ
，
Ｓ
＞ｉｎａＩ的表示


６
：
依据公式４
－５
，‘６得到ｅ
，
ｓ


７
：
根据公式４
￣７得到包表示６


８
：
根据Ａｄａｍｗ算法
，
基于６的梯度更新参数０
，
直到模型拟合
，
或者超过限制的轮


次数
，


９
：
根据公式４
－８得到关系ｒ
＊


１０
：
返回ｒ


３８


第四章基于预训练模型的异质图神经网络远程监督关系抽取模型


４．４本章小结


本章主要介绍了
一种基于预训练模型的异质图神经网络远程监督关系抽取


模型
。该模型主要是在第三章的层级图祌经网络结构上进行改进。
一是验证了预


训练模型在远程监督关系抽取任务上的有效性。二是考虑引入实体信息
，使其对


关系分类起到更有效的作用
，其考虑图中不同节点的特征
，针对不同特征提出不


同的更新方式
，
以更好的进行信息交互
。三是设计了信息融合门控机制
，更好地


融合不同来源得到的信息
，利用自注意力机制更加合理的分配信息权重
，得到增


强的包表示。


３９


第五章实验设置与结果分析


第五章实验设置与结果分析


这
一章主要介绍本文实验的实验环境、
实验数据集、
评估指标，
以及
Ｌ２Ｇ
－


ＧＣＮ模型和ＢＨ
－ＧＣＮ模型的实验结果和分析。本文设计了多组不同的对比实验


来验证设计模型的有效性。此外
，选择了数据集中几个具有代表性的案例
，具体


地分析模型效果
。


５
．
１实验环境


本论文实验使用的硬件环境为：
图形处理器ＮＶＩＤＩＡＧｅＦｏｒｃｅＲＴＸ２０８０Ｈ、


ＣＰＵ为Ｉｎｔｅｌ（Ｒ）Ｘｅｏｎ（Ｒ）ＣＰＵＥ５
－２６７８ｖ３
＠２
．５０ＧＨｚ、
内存大小４７Ｇ、服务器操


作系统为
Ｕｂｕｎｔｕ１６
．０４．６ＬＴＳ、使用Ｐｙｔｈｏｎ３
．７作为编程语言
、深度学习开源框


架为
ＰｙＴｏｒ
ｃｈ１
．４
．０。系统的硬件配置
，操作系统以及主要的编程语言如表
５
－１所


示。


表５
－１
硬件环境


环境服务器墙


显卡配置ＮＶＩＤＩＡＧｅＦｏｒｃｅＲＴＸ２〇８〇Ｔｉ


ＧＰＵ

Ｉｎｔｅｌ（Ｒ
）Ｘｅｏｎ（Ｒ）ＣＰＵＥ５
－２６７８ｖ３
＠２．５０ＧＨｚ


内存
４７Ｇ


操作系统Ｕｂｕｎｔｕ１６
．０４
．６ＬＴＳ


编程语言Ｐｙｔｈｏｎ３
．７


深度学习框架
ＰｙＴｏｒｃｈ１
．４．０


其他开源工具包Ｓｋｌｅａｍ
，
Ｔｒａｎｓｆｏｒｍｅｒｓ
，
Ｎｕｍｐｙ


５
．２实验数据集


本文使用了两个公开数据集来评估所设计的模型
。
一是Ｒｉｅｄｅｌ等人
制作


的ＮＹＴ
－１０基准数据集上评估了所设计的模型
，数据集的统计结果如表５
－２所示。


表
５
－２ＮＹＴ
－
１０数据集统计


＾
絲錄实例》ｍｍ￥ｍｔ


训练集４５５
，７
１
１２３３
，０６４
￣


ＮＹＴ
－
１０５３集１
１４
，
３
１７５８，
６３５


测试集１７２
，
４４８９６
，
６７８


４１


北京邮电大学工程硕士学位论文


该数据集通过将数据库
ＦｒｅｅＢａｓｅ中的关系三元组与
《纽约时报》新闻语料


库对齐生成。ＮＹＴ
－１０数据集总共收集了３９
，５２８个实体
，支持
５３种不同的关系


类别
（包括ＮＡ类型
，
即两个实体之间没有关系）
。数据集数据格式如表
５
－３所


表
５
－３ＮＹＴ
－
１０数据集格式展示


项目内容


ＦｒｅｅＢａｓｅｉｄ
＿
ｌｍ．Ｏｃｃｖｘ


ＦｒｅｅＢａｓｅｉｄ
＿２ｍ
．０５ｇｆＤ８


Ｅｎｔｉｔｙ」ｑｕｅｅｎｓ


Ｅｎｔｉｔｙ—２ｂｅｌｌｅ
—ｈａｒｂｏｒ


Ｒｅｌａｔｉｏｎ
／ｌｏｃａｔｉｏｎ／ｌｏｃａｔｉｏｎ／ｃｏｎｔａｉｎｓ


Ｓｅｎｔｅｎｃｅ
．
．．ｉｎｔｏｔｈｅｆａｔａｌｃｒａｓｈｏｆ
ａ
ｐａｓｓｅｎｇｅｒ
ｊｅｔｉｎ


ｂｅｌｌｅ
＿ｈａｒｂｏｒ
，ｑｕｅｅｎｓ
．
．
．


二是
Ｊａｔ等人
制作的ＧＤＳ数据集
，数据集的统计结果如表
５
￣４所示
。该数


据集是近年通过使用Ｇｏｏｇ
ｌｅ关系抽取语料构建。为了满足多示例学习中
“至少存


在
一次
” 的假设
，该数据集中的包内至少有
一句是正确标注数据
。这让实验在数


据集上的自动评估变得更加可靠
。


表
５
￣４ＧＤＳ数据集统计


￣￣
絲
实例賴
￣


训练集１
１
，
２９７


ＧＤＳ５验证集１
，
８６４


测试集５
，６６３


本文沿用Ｃａｏ等人
［４（）１的评估方法
，使用ｈｅｌｄ
－ｏｕｔ模式评估模型
。该方案将测


试数据中发现的关系三元组与
Ｆｒ
ｅｅＢａｓｅ提供的关系三元组进行比对
，
提供了
一


种近似的精度测量方法
，无需耗时耗力的人工检查
。数据集关系类型如表
５
－５所


不
。


５
．３评估指标介绍


在本文的评估中
，
采用了四个计算指标
。这些指标包括精确率
－召回率曲线


（ＰＲ
－Ｃｕｒｖｅ）
、
曲线下面积
（ＡｒｅａＵｎｄｅｒＣｕｒｖｅ
，
ＡＵＣ）
，
Ｐｒｅｃｉｓｉｏｎ＠Ｎ（Ｐ＠Ｎ）
，
和


Ｈｉｔｓ＠Ｋ。
以下简要介绍这四个指标。


４２


第五章实验设置与结果分析


表
５
－５ＮＹＴ
－１０数据集关系类型


／ｌｏｃａｔｉｏｎ／ｎｅｉｇｈｂｏｒｆ
ａｏｏｄ／／ｌｏｃａｔｉｏｎ＾ｅｇ
ｉｏｎ／ｃａｐｈａｌ／ｌｏｃａｄｏｎ／ｃｎ
＿ｐｒｏｖｉｏｃｅ／ｃａｐｈａｌ／ｂｕｓｉｎｅｓｓ／ｏｍｉｐａｎｙ
／ｆｏｕｎｄｅｒｓ


ｎｅｉ＾ｉｂｏｒｈｏｏｄ
＿ｏｆ


／ｌｏｃａｔｉｏｎ／ｉｎ
＿ｓｔａｔｅ／／ｐｅｏｐ
ｌｅ／ｐｅｒｓｏｎ／ｐ
ｌａｃｅ
＿ｏｆ
＿
ｌｎｉｔｈ／ｌｏｃａｔｉｏｎ／ｃｏｕｎｔｉｙ
／／ｌｏｃａｔｉｏｉｉ＾ｔ
＿ｒｅｇ
ｉｏｉｉ／ｃａｐ
ｉｔａｌ


ａｄｍｉｎｉｓｔｒａｔｉｖｅ
一ｃａｐ
ｉｔａｌ
ｌ９ｉ＾ｕａｇｅｓ
＿ｓｐｏｋｅｎ


／ｂａｓｅ／ｌｏｃａｔｉｏｎｓ／ｃｏｉｉｎｔｒｉｅｓ／／ｐｅｏｐ
ｌｅ／ｆａｍｉｌｙ／ｍｅｍｂｅｒｓ／ｌｏｃａｔｉｏｎ／ｕｓ
＿ｃｏｕｎｔｙ／／ｌｏｃａｄ（Ｍｉ／ｂｒ
＿ｓｔａｔｅ／
ｃａｐ
ｉｔａｌ


ｓｔａｔｅｓ
＿ｊ
＞ｒｏｖｉｎｃｅｓ
＿ｗｉｔｈｉｎ
ｃｏｕｎｔｙ＿ｓｅａｔ


／ｐｅｏｐ
！ｅ／ｄｅｃｅａｓｅｄ
＿ｐｅｒｓｏｎ／／ｌｏｃａｔｉｏｎ／ｕ￥
＿ｓｔａｔｅ／ｃａｐ
ｉｔａｌ／Ｉｏｃａｔｉｏｎ／ｍ
＿ｓｔａｔｅ／／ｓｐｏｒｔｓ／ｓｐｏｒｔｓｊ
ｔ＾ｍ／ｌｏｃａｔｉｏｎ


ｐ
ｌａｃｅ
＿ｏｆ
＿ｄｅａｔｈ
ｌｅｇ
ｉｓｌａｔｉｖｅ
．ｃａｐ
ｉｔａｌ


／ｐｅｏｐ
ｌｅ／ｐｒｏｆｅｓｓｉｏｎ／／ｐｅｏｐ
ｌｅ／ｐｅｒｓｏｎ／ｒｅｌｉｇ
ｉｏｎ７１ｏｃａｔｉｏｎ／ｍ
＿ｓｔａｔｅ／
／ｂｉｍｉ＾ｓｓ／ｃｏｍｐａｎｙ
／


ｐｅｏｐ
Ｉｅ
＿ｗｈｈ
＿
ｔｈｉｓ
＿ｐｒｏｆｅｓｓｉｏｎ
ｊｕｄｉｃｉａｌ
＿ｃａｐ
ｉｔａｌｐ
ｌａｃｅ
—ｆｏｕｎｄｅｄ


／ｂｕｓｉｎｅｓｓ／ｃｏｍｐａｎｙ＿ａｄｖｉｓｏｒ／／ｐｅｏｐ
ｌｅ／ｆａｍｉｌｙ／ｃｏｉｍｔｉｙ／ｔｉｍｅ／ｅｖｅｎｔ／ｌｏｃａｔｉｏｎｓ／ｐｅｏｐ
ｌｅ／ｅｔｈｎｉｃｉｔ
ｙ／


ｃｏｍｐａｎｉｅｓ
＿ａｄｖｉｓｅｄ
ｉｎｃｌｕｄｅｄｉｎｇｒｏｕｐ


／ｌｏｃａｔ
ｉｏｎ／ａｄｍｉｎｉｓｔｒａｔｉｖｅ
＿ｄｉｖｉｓ
－／ｉｏｃａｔ
ｉｏｎ／ｍｘｊｓｔａｔｅ／ｃａｐ
ｉｔａｌ／ｌｏｃａｔｉｏｎ／ｐｒｏｖｉｎｃｅ／ｃａｐ
ｉｔａｌ／ｐｅｏｐ
ｌｅ／ｐｏｓｏｎ／ｎａｔｉｏｎａｌｉｔｙ


ｉｏｎ／ｃｏｕｎｔｉｙ


／ｂｕｓｉｎｅｓｓ／ｓｈｏｐｐ
ｉｎｇ＿ｃｅｎｔｅｒ
＿ｏｗ／ｂｕｓｉｎｅｓｓ／ｐｅｒｓｏｎ／ｃｏｍｐａｎｙ／ｂｕｓｉｎｅｓｓ／ｃｏｍｐａｎｙ／ａｄｖｉｓｏｒｓ／ｂｕｓｉｎｅｓｓ／ｓｈｏｐｐ
ｉｎｇｃｅｎｔ
ｅｒ／


ｎｅｒ／ｓｈｏｐｐ
ｉｎｇ＿ｃｅｎｔｅｒｓ
＿ｏｗｎｅｄ
ｏｗｎｅｒ


／ｐｅｏｐ
ｌｅ／ｄｅｃｅａｓｅｄｊ
＞ｅｒｓｏｎ／／ｐｅｏｐ
ｌｅ／ｐｅｒｓｏｎ／ｅｔｈｎｉｃｉｔｙ／ｐｅｏｐ
ｌｅ／ｐｅｒｓｏｎ／ｐ
Ｉａｃｅ
＿＿
ｌｉｖｅｄ／ｂｕｓｉｎｅｓｓ／ｃｏｍｐａｎｙ／


ｐ
Ｉａｃｅ
＿ｏｆ
＾ｂｉｍａｌ
ｍａｊｏｒｊｓｈａｒｄｉｏｌｃｔ
ｅｒｓ


／ｐｅｏｐ
ｌｅ／ｅｔｈｎｉｃｉｔｙ／
／ｂｒｏａｄｃａｓｔ／ｐｒｏｄｕｃｅｒ／ｌｏｃａｔｉｏｎ／ｂｒｏａｄｃａｓｔ／ｃｏｎｔｅＤｔ／ｌｏｃａｔｉｏｎ／ｂｕｓｉｉ＾ｓｓ／ｌＨｉｓｉｎｅｓｓ
＿
ｌｏｃａｔｉｏｎ／


ｇｅｏｇ
ｒａｐｈｉｃ
一ｄｉｓｔｒｉｂｕｔｉｏｎ
ｐａｒｅｎｔ
—ｃｏｍｐａｎｙ


／ｌｏｃａｔ
ｉｏｎ＾ｐｊｐＦｅｆｅｃｔｕｒｅ／ｃａｐ
ｉｔａｌ／ｆｉｌｍ／ｆ
ｉｌｍ／
／ｐｅｏｐ
ｌｅ／ｐ
Ｉ＾ｅ
＿ｏｆ
＿
ｉａｔａｎ＾ｉｉｔ／／ｌｏｃａｔｋ＞ｎ／ｄｅ
＿ｓｌａｔｅ／ｃａｐ
ｉｔａｌ


ｆｅａｔｕｒｅｄ
一ｆｉｌｍｊｏｃａｔｉｏｎｓ
ｉｎｔｅｒｒｅｄ
一ｈｅｒｅ


／ｐｅｏｐ
ｉｅ／ｐｅｒｓｏｏ／ｐｒｏｆｅｓｓ
ｉｏｎ／ｂｕｓｉｎｅｓｓ／ｃｏｍｐａｉ＾
／ｌｏｃａｔｉｏｎｓ／ｌｏｃａｌｉｏｉｉ／ｃｏｕｉｉｔｒｙ／ｃａ
｛？ｔａｌ／ｌｏｃａｔ
ｉｏｎ／ｌｏｃａｔｉｏｎ／ｃｏｎｔａｉｎｓ


／ｌｏｃａｔｉｏｎ／ｃｏｕｎｔｉｙ／／ｐｅｏｐ
ｌｅ／ｐｅｒｓｏｎ／ｃｈｉｌｄｒｅｎ／ｎｉｍ／ｆ
ｉｌｍ
＿
Ｉｏｃａｔｉｏｎ／／ｆＵｍ／ｆｉｌｉｉｉ
＿ｆ
ｅｓｄｖａｌ／ｌｏｃａｔｉｏｎ


ａｄｍｉｎｉｓｔｒａｔｉｖｅ
一ｄｉｖｉｓｉｏｎｓｆ
ｅａｔｕｒｅｄ
一
ｉｎ
一ｆ
ｉｌｍｓ


ＮＡ


５
．３
．１精确率
－召回率曲线


对于ＰＲ曲线
，
其水平横轴为召回率
，
竖轴为精确率
。该曲线是通过将预测


关系三元组依照预测分数的值从高到低排序绘制的
。通过遍历排序后的列表
，计


算相应的精确率和召回率：


ＴＰ
，＼


Ｐ＝
（５
－１）


ＴＰ＋ＦＰ
ｋ｝


４３


北京邮电大学工程硕士学位论文


ＴＰ
，
，


＇
■
＝
（５
－２）


ＴＰ＋ＦＮ
ｙＪ


从ＰＲ曲线可以观察模型在不同召回水平下的分类能力
。
一般来说
，
曲线越


靠近右上方
，
模型的分类能力越强。


５
．３
．２ＡＵＣ值


ＡＵＣ值代表ＰＲ曲线下的面积数值
，
数值在
０到
１的范围内
。
ＡＵＣ可以使


用梯形规则计算
，
也可以用平均精度
（Ａｖｅｒ
ａｇｅＰｒｅｃｉｓｉｏｎ
，ＡＰ）代替
。
ＡＰ计算每


个阈值下精度的加权平均值来总结ＰＲ曲线
，并将召回率相对于前
一个阈值的召


回率的增加作为计算的权重
，
即
：


ＡＰ
＝＾
（Ｒｎ
－Ｒｎ
＿１）Ｐｎ（５
－３）


７１


其中
分别为在第ｎ个阈值点的精确率和召回率。类似于
ＰＲ
曲线
，
当


ＡＵＣ的值靠近
１的时候
，
模型的预测能力更强
。


５
．３
．３Ｐｒｅｃｉｓｉｏｎ＠Ｎ


从高到低遍历模型预测分数
，并计算精确率
。这里
，表示按预测分数排序后


所取的个数
，
这里取前
１００、
２００、
３００、
５００、
１０００和２０００个预测的关系三元组


即包
。
Ｐ＠Ｎ计算如下
：


Ｘｎ＾Ｐｒｅｃｉｓｉｏｒｉｎ


Ｐ＠Ｎ＝
（５
－４）


Ｎ


该模型在高召回率水平下的表现更能体现出该模型对长尾数据分类的能力
。


５
．３
．４Ｈｉｔｓ＠Ｋ


Ｈｉｔｓ＠Ｋ指标用于评估模型在长尾类别的数据中的性能
。本论文提取测试集


中关系类别所对应的数据少于
１００或
２００个的部分作为测试数据
，
来计算该指


标
。因为模型难以处理长尾问题
，所以比对正确的关系标签和模型预测的前Ｋ个


候选关系更合适。沿用前人工作？〇＾＋１＾１
＇
７
［３６］设置
，参数
１＾从集合｛
１０
，１５
，２０｝


中选择
。然后本文报告扭１５＠
１＾的宏观平均结果
（ＭａｃｒｏＡｖｅｒａｇｅ）
。Ｈｉｔｓ＠Ｋ计算


如下
：


Ｈｉｔｓ＠Ｋ＝
Ｌ（５
－５）


Ｒ


其中是
一个指示函数
，
当＆ｅ成立时函数返回
１
，
否则返回
〇
，
欠代表模型预


测的前欠个候选值
，
＃代表正确的标签
，
／？代表小于
１００或者２００相应的关系的总


数
，
ｉＶ代表预测的目标关系的数量
。


４４


、
第五章实验设置与结果分析


５
．４实验细节


本文模型的实验参数设置如下表５
－６所示
。本论文处理依存句法分析时使用


了ＬＡＬ解析器，对文本进行单句的依赖解析，并将结果添加到相应的数据集中
。


ＬＡＬ解析器展示了ＰｅｎｎＴｒｅｅｂａｎｋ上依赖解析的最新结果
，
并为基准数据集实现


了９７．４２ＵＡＳ和９６
．２６ＬＡＳ
，
成为了当时的ＳＯＴＡ。
然后本文将数据集以包为单


位划分
，
并以ｐｋｌ格式保存在文件中
。本文设计的Ｌ２Ｇ
－ＧＣＮ模型采用Ｌｉｎ等人


间中的
５０维单词嵌入初始化单词
ｅｍｂｅｄｄｉｎｇｓ。位置向量的维数为
５
。
紧接着
，


单词和位置的向量表示拼接后
，
使用Ｂｉ
－ＬＳＴＭ模块进行编码
，
该模块的隐藏状


态表示维数为
１００。
词级ＧＣＮ层数为２
，
句子级的ＧＣＮ层数为
１
，
这两个模块


的隐藏状态表示维数为
１００。


表
５
－６实验参数设置


模型成分参数说明数值


ｄｗ词向量维度５０


ｄ
ｐ位置向量维度５


输入层


ｄｆ
ｔ隐藏层向量维度１００


Ｐｗ词嵌入的Ｄｒｏｐｏｕｔ概率０．１


ｌＧ词级图中ＧＣＮ层数２


词级图


ｐａ词级图的Ｄｒｏｐｏｕｔ概率０．３


Ｌ２Ｇ
－ＧＣＮ


ｌ
＇
Ｇ句级图中ＧＣＮ层数１


句级图


ｐ
＇
ｇ句级图的Ｄｒｏｐｏｕｔ概率０．３


ｍＢ批量大小１２８


优化ｌｒ学习率０．３


Ａ互信息损失系数０．０１


通用ｐｉ线性层的Ｄｒｏｐｏｕｔ概率０．５


ｄｈ隐藏层向量维度７６８


ＢＥＲＴ


输入最大长度１２０


１〇异质图的ＧＣＮ层数２


ＢＨ
－ＧＣＮ异质图


ｐｇ异质图的Ｄｒｏｐｏｕｔ概率０．３


ｌｒ学习率２ｘｌ〇
－５


通用


批量大小３２


４５


北京邮电大学工程硕士学位论文


为了防止过度拟合
，
本文对单词嵌入应用
ｄｒｏｐｏｕｔ＝０．１
，
对Ｗｏｒｄ
－ＧＣＮ和


Ｓｅｎ
－ＧＣＮ模块应用
ｄｒｏｐｏｕｔ＝０．３
，
对所有线性层应用
ｄｒｏｐｏｕｔ＝０
．５
。本文使用


Ｐｙｔ
ｏｒｃｈ框架＾构建Ｌ２Ｇ
－ＧＣＮ模型
。所有模型参数均采用统
一的分布ｉ；
？
［０
，１］


进行初始化
。
Ｌ２Ｇ
－ＧＣＮ模型在批大小为
１２８的
１００个ｅｐｏｃｈｓ内分两个阶段进


行训练。本文采用经典的
ＳＧＤ优化器对Ｌ２Ｇ
－ＧＣＮ模型进行优化
，
学习率为


０
．３
。本文将互信息最大化正则化添加到总损失中
，
系数为０．０１
。
此外
，
对于异


质图模型框架ＢＨ
－ＧＣＮ
，
其采用ＡｄａｍＷ优化器进行优化训练
，
其ＢＥＲＴ模型


和异质图网络等参数设置如表
５
－６所示。


５
．５实验结果分析


５
．５
．１对比模型


本文将所设计的模型与之前研究工作提出的基线方法进行比较
，包括前三个


非神经网络模型方法和其他神经网络方法。
以下简要说明了这些方法：


１
）Ｍｉｎｔｚ等人Ｗ首次将远程监督应用于关系抽取
。这是
一种新的关系抽取范


式
，不需要标注语料
，并且适用于任何大小的语料库
。他们使用了多类别的逻辑


回归模型处理远程监督关系抽取
，
完成了
一项开创性的工作
。


２）Ｍｕｌｔ
ｉＲ
ｆＭ
：Ｈｏｆ
ｉｎａｎｎ等人在ＭＩＬ框架下采用了概率图模型来处理远程


监督关系抽取中的关系重叠问题
。ＭｕｌｔｉＲ模型不仅可以用于包级关系抽取
，还可


以很好地处理句子级抽取和语料库级抽取
。


３
）ＭＩＭＬＲＥ
［６５］
：Ｓｕｒｄｅａｎｕ等人使用了
一个结合ＭＥＬ和多标签学习的模型
。


这是第
一种使用隐藏变量联合建模
一个实体对和多个标签的方法。


此处介绍的前三种方法为非神经网络方法。


４）ＰＣＮＮ
［１８］
：Ｚｅｎｇ等人采用了基于ＣＮＮ的模型
，设计了分段式的最大池化


来向量化实例
，并在
一个包里选择最有可能符合该关系标签的实例
。
ＰＣＮＮ方法


首次将ＭＩＬ框架融入了针对远程监督关系抽取的深度学习研究中
，
并且没有使


用复杂的人工设计特征
。这对远程监督关系抽取来说是
一项开创性的工作
。


５）
ＰＣＮＮ＋ＡＴＴ
ｔ１９ｌ
：Ｌｉｎ等人设计了ＰＣＮＮ模型的
一个变体
，
该模型在包内


多个实例上使用选择性注意力机制来加权实例表示获取包的表示。通过采用句子


级别的注意力模型
，该研宄期望减少噪声实例的在训练中的权重
，从而减少噪音


数据对模型分类的影响
。


４６


第五章
实验设置与结果分析


６
）ＰＣＮＮ＋ＨＡＴＴ
［３６
］
：Ｈａｎ等人没有孤立地处理某个关系类型
，而是更好地利


用了关系的层级信息
。在关系间关联信息的基础上
，提出了
一种新的层级的注意


力机制以识别包内的有效实例
。


７
）ＲＥＳＩＤＥ＾
：Ｖａｓｈｉｓｈｔｈ等人使用额外的知识信息
，包括实体类型和知识库


中关系标签的别名
，
用于对关系预测施加约束
。此外
，
该方法在远程监督关系抽


取任务中第
一次使用了ＧＣＮ对文本进行编码
。


８
）
ＰＣＮＮ＋ＫＡＴＴ
［３７
】
：Ｚｈａｎｇ等人融入了从图嵌入表示中获取的关系信息
。此


夕卜
，
利用ＧＣＮ对显性的知识进行建模
。
同时
，
它采用了从粗粒度到细粒度的知


识感知的注意力机制来表示包
。


９
）ＤＩＳＴＲＥＮ］
：Ａｌｔ等人观察到之前的模型倾向于预测数据量多的关系类别
。


为了解决这个问题
，他们通过使用生成式预训练语言模型来引入语义
、句法
、语


法特征以及
一些
“ 常识
”知识
。这些特征被认为是识别
一些不同关系类别的关键
。


１０）
ＤＰＥＮ
［３９
］
：Ｇｏｕ等人利用实体类型与关系类型之间的的联系
，
构建了
一


个具有动态生成参数能力的神经网络
。此外
，还提出了
一种关系感知的注意力机


制来聚合实体类型信息
。


１
１
）ＤＣＲＥ
［５５
］
：Ｓｈａｎｇ等人应用了
一个无监督的深度聚类方法来检测包内的


噪声实例
，
并且为这些噪声实例生成
一个置信度较高的标签
。


１２）ＰＡ
－ＴＲＰ＿
：Ｃａｏ等人构造了
一个共现图来学习文本嵌入
，
并且从未标


记的数据中学习关系原型
。该模型通过迁移富含有效训练实例的关系类别知识到


长尾数据上
，
以此提高长尾关系提取的性能
。


１３
）ＰＳＡＮ
－ＲＥ
［２４
］
：Ｓｈａｎｇ等人设计了
一种模式感知的自注意力网络结构来识


别不同种类的短关系模版
。然后将这些模版信息输入预训练模型
，来辅助预训练


模型捕捉局部的依赖关系和短语结构
。


５
．５
．２结果分析


在这节中本文依照上述指标依次报告主要的实验结果
，并且同时分析所提出


模型的性能
。此外
，通过将实验结果与基线方法进行比较
，更深入地分析本文的


工作和实验结果
。


１
）ＰＲ曲线


如图
５
－１中
，
本文将提出的Ｌ２Ｇ
－ＧＣＮ模型框架的
ＰＲ曲线与上述基线方法


的
ＰＲ曲线进行比较
，
并得到以下观察结果
：
（
１
）所有非神经网络基线模型
，
包


括Ｍｉｎｔｚ、
ＭｕｌｔｉＲ和ＭＩＭＬＲＥ都逊于基于神经网络的基线模型
。
主要原因是非


神经网络方法不能准确地捕捉句子的语义信息
，并且实验结果很大程度上取决于


人工设计的机器学习特征
，这些特征会导致错误传播的问题
。
（２）从ＰＲ曲线看
，


本论文设计的Ｌ２Ｇ
－ＧＣＮ模型相比其他基线方法有着更好的性能
。当召回率超过


４７


北京邮电大学工程硕士学位论文


０
．
１后
，
模型的
ＰＲ曲线呈现稳定的趋势
。
特别是当召回率超过
０
．
１５时
，
本文设


计的方法都在
一定程度上领先于其他方法
，这说明本文设计的模型在不同种类的


Ｉ
Ｄ
Ｐｒｅｃｉｓ
ｉｏｎ
－Ｒｅｃａ
ｌ
ｌＣｕｒｖｅｓ


—＊一Ｍ
ｉｎｔｚ


Ｉ
ｊＭｕＩｔｉＲ


ｎ
ｑｌｉ
ＩＭＩＭＬＲＥ


？
ＢＨ
ＰＣＮＮ


｜
亂，４ＰＣＮＮ＋ＡＴＴ


ｆ
－４
－ＲＥＳＩＤＥ


〇
８
ＩＵ
ｋＮｉ－ＰＣＮＮ＋ＨＡＴＴ


＝ＮＷ


Ａ


」
＼４＾
！


０
．００
Ｊ０
．２０
．３０
．４０
．５


Ｒｅｃａ
ｌ
ｌ


图
５
－
１ＰＲ曲线实验结果对比


关系类别的预测上有
一定的优势
。
相比于基于注意力的模型
ＰＣＮＮ＋ＡＴＴ
、


ＰＣＮＮ＋ＨＡＴＴ
，
本文的
Ｌ２Ｇ
－ＧＣＮ模型利用依存句法知识建模依赖关系
。
因此
，


该模型能够更好地理解句子中的局部信息
。此外
，无论是基于注意力的方法还是


基于先验知识的方法
，它们都没有利用实例之间的相关性
。实验结果表明
，本文


提出的方法有效地利用了模型自底向上的结构
，层级地学习句子的局部信息和句


子之间的全局实体关系信息
。
（３
）此外
，
与基于ＧＣＮ的方法相比
，
本文的模型


不仅利用
ＧＣＮ建模句子中词与词之间的依赖信息
，
还利用
ＧＣＮ建模了句子之


间的相关性来获取句子之间全局的的结构信息
，因此可以获得
一个鲁棒性更强的


包表示
，
以避免噪声的影响
。此外
，本文注意到ＲＥＳＩＤＥ模型利用了额外的依赖


关系类型信息
，
但本文模型没有使用
。
可以观察到
，
本文设计的模型在
ＰＲ评估


曲线中
，
在所有召回值的阈值下
，精确率都是大幅领先的
，这表明本文模型中的


Ｓｅｎ
－ＧＣＮ模块有助于提高关系抽取的性能
，
更表明本文设计模型的优势
。


总体而言
，本文设计的模型无论是在低召回还是高召回范围内都具有竞争力
。


２）Ｐ＠Ｎ评价


本文使用包中所有的实例进行模型评估
，
Ｐ＠Ｎ的实验结果如表
５
－７所示
。观


察发现
，
本文设计的Ｌ２Ｇ
－ＧＣＮ模型在大部分指标上显著优于所有基线方法
。总


体而言
，
本文此处列举了Ｌ２Ｇ
－ＧＣＮ模型在
Ｔｏｐ
１００、
２００
、
３００
、
５００
、
１０００和


２０００的精度表现以及以上六个值平均的精度值
，
其在
Ｐ＠
１００、
３００、
５００
、
１０００


４８


第五章
实验设置与结果分析


上分别提高了２
．４％
、
１
．３％
、
４
．７％和
３
．８％的模型效果
。结果表明本文的模型在整


个分布上有着惊人的表现
。此外
，本论文模型与所有的基线模型相比
，在六个指


标项的平均值Ｐ＠ＭＥＡＮ上最高
，对比所有基于选择性注意的方法和基于先验知


识的方法
，
其提升了３
．７％
。
结果表明
，
该模型结合句子中的局部信息和全局结


构信息来处理含噪实例时具有较强的鲁棒性
。


表
５
－７Ｐ＠Ｎ实验结果对比


Ｍｅｔｈｏｄ
１００２００３００５００１０００２０００ＭＥＡＮ


Ｍ
ｉｎｔｚ
５２
．３５０
．２４５
．０３９
．７３３
．６２３
．４４０
．７


ＰＣＮＮ＋ＡＴＴ７３
．０６８
．０６７
．３６３
．６５３
．３４０
．０６０
．９


ＲＥＳＩＤＥ
８
１
．８７５
．４７４
．３６９
．７５９
．３４５
．０６７
．６


ＰＣＮＮ＋ＨＡＴＴ８２
．０７９．５７５
．３６７
．０５７
．７４
１
．９６７
．２


ＤＩＳＴＲＥ６８
．０６７
．０６５
．３６５
．０６０
．２４７
．９６２
．２


ＤＰＥＮ
８０
．０７８
．０
－
－


ＤＣＲＥ
８
１
．２７４
．６７２
．８６５
．
１５５
．
１４
１
．７６５
．
１


ＰＡ
－ＴＲＰ
８
１
．０７８
．５７５
．７６８
．６５９
．０４４
．６６７
．９


ＰＳＡＮ
－ＲＥ７９
．２７
１
．
１６６
．８６５
．９６０
．４４８．１６５
．３


Ｌ２Ｇ
－ＧＣＮｍ
，／ｏＳｅｎ
－ＧＣＮ８
１
．０７７
．０７４
．０７０
．０５９
．３４５
．２６７
．８


Ｌ２Ｇ
－ＧＣＮｗ／ｏＷｏｒｄ
－ＧＣＮ８３
．０７９．５７６．７６８
．８５９
．０４５
．８６８
．８


Ｌ２Ｇ
－ＧＣＮｗ／ｏＨ
ｉｅｒａｒｃｈｉｃａｌ
－
ｔｒａｉｎ
ｉｎｇ８３
．０７６
．５７４
．３６９
．６５７
．６４４
．３６７
．６


Ｌ２Ｇ
－ＧＣＮｗ／ｏＭＩＭ８３
．０７８
．０７４
．７７０
．４５８
．４４５
．６６８
．４


ＯｕｒＬ２Ｇ
－ＧＣＮ８４．０７８
．０７６．７７３
．０６２
．７４７
．９７０．４


具体来说
，
与基于选择性注意的神经基线方法ＰＣＮＮ＋ＡＴＴ相比
，
本文的模


型的在
Ｐ＠ＭＥＡＮ表现比它高出
１５
．６％
。
即使
ＰＣＮＮ＋ＨＡＴＴ
引入了关系中的层


级知识
，
并利用
一种新的层级注意力范式来识别有效实例
，
以缓解噪声问题
，本


文的模型也在
Ｐ＠ＭＥＡＮ上有
４
．８％的显著改善
。
这表明相对于传统的选择性注


意力机制
，
本文层次ＧＣＮ方法从局部和全局句子之间的相关性中获取的信息有


一定的作用
。利用这些信息可以得到
一个鲁棒的包表示
，
从而缓解噪音问题
。此


夕卜
，
基于先验知识的方法效果通常优于具有相似模型结构的其他方法
，
例如


ＲＥＳＩＤＥ
、
ＤＩＳＴＲＥ和
ＰＡ
－ＴＲＰ
，
它们通常旨在实５见更平衡的整体性能
，
但对夕卜部


知识进行建模是
一个较为繁琐的过程
，且不易与迁移到其他领域数据上
。与这些


方法相比
，
本文的模型仍有很大的优势
。
最后
，
通过比较基于ＧＣＮ的方法与本


文的模型
，本文可以观察到
，在没有在先验知识的支持下
，本文的Ｌ２Ｇ
－ＧＣＮｗ／ｏ


ＳｅｎＧＣＮ模型具有基于ＧＣＮ方法的近似能力
，
同时基于ＧＣＮ方法还额外利用


了先验知识
，
证明了本论文中的
Ｗｏｒｄ
－ＧＣＮ模块的设计更好地利用句法依赖知


识
，
即更好地利用句子中的局部信息
。而且本文模型的整体结构在性能上始终优


于上述方法
，
这证明了本文的模型在面对噪音句子时的鲁棒性
。


４９


北京邮电大学工程硕士学位论文


３
）ＡＵＣ值


表
５
－８ＡＵＣ值实验结果对比



ＭｅｔｈｏｄＡＵＣ


Ｍｉｎｔｚ０．
１０７


ＰＣＮＮ＋ＡＴＴ０．３４１


ＲＥＳＩＤＥ０．４１５


ＰＣＮＮ＋ＨＡＴＴ０
．４２０


ＤＩＳＴＲＥ０
．４２２


ＤＰＥＮ
０．３８０


ＤＣＲＥ０．３６９


ＰＡ
－ＴＲＰ０．４１５


ＰＳＡＮ
－ＲＥ
０．４３８


Ｌ２Ｇ
－ＧＣＮｗ／ｏＳｅｎ
－ＧＣＮ０
．４２６


Ｌ２Ｇ
－ＧＣＮｗ／ｏＷｏｒｄ
－ＧＣＮ０
．４１９


Ｌ２Ｇ
－ＧＣＮｗ／ｏＨｉｅｒａｒｃｈｉｃａｌ
－ｔｒａｉｎｉｎｇ０．４０６


Ｌ２Ｇ
－ＧＣＮｗ／ｏＭＩＭ
０
．４２８


ＯｕｒＬ２Ｇ
－ＧＣＮ
０．４４８


本文将Ｌ２Ｇ
－ＧＣＮ模型与其余基线方法在ＡＵＣ值上进行比较
，
如表
５
－８所


示
。本文注意到ＡＵＣ值与上述两个指标
ＰＲ曲线和
Ｐ＠Ｎ值保持趋势
一致
，
即


Ｌ２Ｇ
－ＧＣＮ模型实现了最佳性能
，
ＡＵＣ值达到了最高
，
其提高了２．３％的性能。


通过观察ＰＲ曲线和Ｐ＠Ｎ表格
，可以发现虽然基于选择性的注意力的方法
，


例如
ＰＣＮＮ＋ＨＡＴＴ
，
在低召回水平下产生了更高的结果
，
基于先验知识的方法
，


如ＤＩＳＴＲＥ在高召回水平下实现了高置信度
，
但是它们在ＡＵＣ度量下都具有相


似的结果
。结果表明
，这些方法在数据集的总体分布上具有相似的性能
。与上述


方法相比
，本文的模型通过整合局部结构信息和全局的结构信息以及互信息最大


化正则器突破了瓶颈。


４）Ｈｉｔｓ＠Ｋ


为了进
一步证明本文模型的优势
，并验证该模型在长尾关系类别的性能
，本


文报告了Ｈｉｔｓ＠Ｋ宏观平均值
（ＭａｃｒｏＡｖｅｒａｇｅ）
，
见表
５
－９。本文从ＮＹＴ
－１０中提


取
一个测试数据集的子集
，均为训练实例数据数量少于
１００／２００的关系类别数据
。


每个实体对
，本文选择模型给出的前Ｋ个候选关系并和正确的关系类别比较。Ｋ


从｛
１０
，１５
，２０｝中选择
。从表５
－８中
，本文有以下观察结果
。
（
１
）从Ｈｉｔｓ＠Ｋ指标而


言
，之前使用最广泛的方法ＰＣＮＮ＋ＡＴＴ性能相对较差
。尤其是在Ｈｉｔｓ＠
１０指标
，


只有不到
５％的长尾类别实例会进入前
１０名推荐候选关系
。这表明长尾问题仍


然是远程监督关系抽取面临的
一个严重问题
。
（２）先验知识的引入可以提高长尾


５０


第五章实验设置与结果分析


关系抽取的性能
。基于先验知识的方法
，
如受益于知识图谱中实体描述信息
、实


体类型信息和关系原型的模型方法如
ＰＣＮＮ＋ＫＡＴＴ、
ＤＰＥＮ、
ＰＡ
－ＴＲＰ通常比基


于注意力的模式更好
。这
一改进表明
，
引入外部知识是
一个重要而有效的方法来


缓解长尾问题
。
（３
）从总体上看
，
本文的模型表现出了惊人的性能
，
在很大程度


上改善了模型在长尾数据上的性能
。具体来说
，对于少于
１００个训练实例的长尾


类别而言
，
本文的模型比之前最好的模型的性能在Ｈｉｔｓ＠
１５高出了２
．７％
，
而在


Ｈｉｔｓ＠２０高出了２３
．
１％
。而且本文的模型性能在少于
２００个训练实例的长尾类别


保持了
一致的有效性
，
在Ｈｉｔｓ＠
１０
，Ｈｉｔｓ＠
１５和Ｈｉｔｓ＠２０上都有大幅度增长
，
即


４
．５％
、
６
．９％和
２３
．２％
。从效果来看
，无论是基于注意力的模型还是基于先验知识


的方法
，本文的模型都优于这些以往的模型
，这证明了互信息最大化正则器的有


效性
。增加了信息最大化后
，该模型倾向于预测关系的多样性分布
，
不会偏向于


某些特定的关系标签
。换句话说
，本文的模型将以更高的概率预测尾部关系类别
。


此外
，本文的模型探索了层次结构中句子的局部信息和句子之间全局关系的有效


性
，
其增强了ＤＳＲＥ的包表示
。
实验结果验证了模型的鲁棒性
。


表
５
－９Ｈｉｔｓ＠Ｋ实验结果对比


Ｉｎｓｔａｎｃｅｓｄｕｒｉｎｇｔｒａｉｎｉｎｇ＜
１００＜２００


Ｈｉｔｓ＠Ｋ
（Ｍａｃｒｏ）
１０１５２０１０１５２０


ＰＣＮＮ＋ＡＴＴ＜５
．０７
．４４０
．７１７
．２２４
．２５
１
．５


ＰＣＮＮ＋ＨＡＴＴ２９
．６５
１
．９６
１
．
１４
１
．４６０
．６６８
．２


ＰＣＮＮ＋ＫＡＴＴ３５
．３６２
．４６５
．
１４３
．２６
１
．３６９
．２


ＤＰＥＮ５７
．６６２
．
１６６
．７６４
．
１６８
．０７
１
．８


ＰＡ
－ＴＲＰ６３
．９７０
．３７２
．２６６
．７７２
．３７３
．８


Ｌ２Ｇ
－ＧＣＮｗ／ｏＭＩＭ５７
．０７４．１８
１
．５６５
．
１７８．８８４
．９


ＯｕｒＬ２Ｇ
－ＧＣＮ６３
．０７２
．２８８．９６９
．７７７
．３９０．９


５
．５
．３对比实验分析


为了展示Ｌ２Ｇ
－ＧＣＮ模型中有关模块的有效性
，
以及验证第四章提出的异质


图网络结构
，
即
ＢＨ
－ＧＣＮ的有效性
，
本文进行了如下对比实验
。
具体而言
，
本


文移除了Ｌ２Ｇ
－ＧＣＮ中的
一些模块
，从而产生了四种变体３ＰＬ２Ｇ
－ＧＣＮｗ／ｏＷｏｒｄ
－


ＧＣＮ
、
Ｌ２Ｇ
－ＧＣＮｗ／ｏＳｅｎ
－ＧＣＮ
、
Ｌ２Ｇ
－ＧＣＮｗ／ｂＨｉｅｒａｒｃｈｉｃａｌ
－ｔｒａｉｎｉｎｇ和Ｌ２Ｇ
－ＧＣＮ


ｗ／ｏＭＩＭ
。同时
，在预训练语言模型ＢＥＲＴ的基础上
，增加了ＢＥＲＴ
，
＋ＳｅｎＧＣＮ
，


＋Ｈｅｔｅ
－ＧＣＮ和ＢＨ
－ＧＣＮ四种模型的对比实验
。
具体地
，
ＰＲ曲线如图
５
－２所示
，


Ｐ＠Ｎ值的对比情况如表
５
－９所示
，
ＡＵＣ值的对比情况如表
５
－
１０
。


Ｌ２Ｇ
－ＧＣＮｗ／ｏＳｅｎ
－ＧＣＮ表示移除了Ｓｅｎ
－ＧＣＮ模块
。该模型Ｗｏｒｄ
－ＧＣＮ对实


例中的局部语法信息进行编码
。
本文观察到
，
在这种情况下
，
模型的性能在


５
１


北京邮电大学工程硕士学位论文


Ｐ＠ＭＥＡＮ和ＡＵＣ下降了３
．８％和
５
．２％
。
这表明
Ｓｅｎ
－ＧＣＮ模块能更加关注包中


实例之间的全局语义关联信息
，全局信息确实增强了模型的能力
。此外
，与另
一


种基于ＧＣＮ的方法ＲＥＳＩＤＥ相比
，可以观察到本文的Ｌ２Ｇ
－ＧＣＮ模型在没有Ｓｅｎ
－


ＧＣＮ模块时
，
仍具有可比较的性能
。换§ 之
，
本文的模型在没有先验知识的情


况下性能略好于ＲＥＳＩＤＥ模型
。


Ｐｒｅｃｉｓ
ｉｏｎ
－Ｒｅｃａｌ
ｌＣｕｒｖｅｓ


Ｉ
０
１二一＿一



－Ｖ
－Ｌ２Ｇ
－ＧＣＮ
ｕ
／ｏ
Ｗｏｒｄ
－ＧＣＮ


Ｂ
．
— Ａ
— Ｌ２Ｇ
－ＧＣＮ
ｗ／ｂ
Ｈ
ｉｃｒａｒｃｈ
ｉｃａＭｒａ
ｉｎｉｎｇ


｜
Ｌ２Ｇ
－ＧＣＮ
ｗ／ｏ
Ｍ
ＩＭ


０
．９
ｗ
ｆ

■Ｌ２Ｇ
－ＧＣＮ（ｏｕｒｓ
）


ｎ／
－？
＊＊ＢＨ
－ＧＣＮ（ｏｕｒｓ
）


Ｉｋ，


ｒ
—
ｒ
－
—
－
ｒ
－
－
１
。，


０
６


０
－５

■






°
＇４
０
．００
．
１０
．２０
．３０
．４０
．５


Ｒｅｃａｌｌ


图
５
－２模型组件有效性分析
（ＰＲ曲线
）


表
５
－
１０模型组件有效性分析
（Ｐ＠Ｎ）


Ｍｅｔｈｏｄ
１００２００３００５００１０００２０００ＭＥＡＮ


Ｌ２Ｇ
－ＧＣＮｗ／ｏＷｏｒｄ
－ＧＣＮ８
１
．０７６
．０７６
．３６８
．８５８
．７４５
．
１６７
．６


Ｌ２Ｇ
－ＧＣＮｗ／ｏＳｅｎ
－ＧＣＮ８
１
．０７７
．０７４
．０７０
．０５９
．３４５
．２６７
．８


Ｌ２Ｇ
－ＧＣＮｗ／ｏＨ
ｉｅｒａｒｃｈｉｃａｌ
－
ｔｒａｉｎｉｎｇ８３
．０７６
．５７４
．３６９
．６５７
．６４４
．３６７
．６


Ｌ２Ｇ
－ＧＣＮｗ／ｏＭＩＭ８３
．０７８
．０７４
．７７０
．４５８
．４４５
．６６８
．４


Ｌ２Ｇ
－ＧＣＮ８４
．０７８
．０７６．７７３
．０６２
．７４７
．９７０
．４


ＢＥＲＴ＋Ａｖｇ７７
．０７８
．０７５
．３６９
．２６０
．２５０
．６６８
．４


＋ＡＴＴ
８０
．０７５
．０７４
．０６８
．２５８
．０５０
．４６７
．６


＋Ｓｅｎ
－ＧＣＮ
８
１
．０７９．５７５
．０６６
．６５６
．２４９
．６６８
．０


＋Ｈｅｔｅ
－ＧＣＮ８２
．０７５
．５７５
．７７２
．８６３
．６５０
．６７０
．０


ＢＨ
－ＧＣＮ８４．０７７
．０７５
．０７３
．２６５
．５５２
．４７１
．２


Ｌ２Ｇ
－ＧＣＮｗ／ｂＷｏｒｄ
－ＧＣＮ表示移除Ｗｏｒｄ
－ＧＣＮ模块
。
其中Ｓｅｎ
－ＧＣＮ模块从


一开始就伴随模型训练
。
本文观察到在这种情况下
，
Ｐ＠ＭＥＡＮ和ＡＵＣ性能下


降了４
．
１％和
６
．９％
。这表明应该首先使用Ｗｏｒｄ
－ＧＣＮ模块编码单实例信息
，
即在


本文设计的框架中
，
从局部到全局的编码是
一个必要的过程
。此外
，
还可以观察


到Ｌ２Ｇ
－ＧＣＮｗ／ｂＷｏｒｄ
－ＧＣＮ的性能略好于Ｌ２Ｇ
－ＧＣＮｗ／ｏＳｅｎ
－ＧＣＮ
。
它表明
，


５２


第五章实验设置与结果分析


Ｓｅｎ
－ＧＣＮ模块通过捕获与噪声问题更直接相关的包结构中的全局信息
，
为本文


的框架提供了更多帮助
。


Ｌ２Ｇ
－ＧＣＮｗ／ｏＨｉｅｒａｒｃｈｉｃａｌ
－ｔｒ
ａｉｎｉｎｇ表示从Ｌ２Ｇ
－ＧＣＮ模型中移除分层训练策


略
。换言之
，直接以端到端的方式训练整个模型
。与Ｌ２Ｇ
－ＧＣＮｗ／ｏＷｏｒｄ
－ＧＣＮ和


Ｌ２Ｇ
－ＧＣＮｗ／ｏ
Ｓｅｎ
－ＧＣＮ相ｐ
，
性能有所下降
。这表明
，
两个模块的简单组合不


能很好地训练模型
，
得到较好的效果
。
Ｓｅｎ
－ＧＣＮ模块可能会干扰Ｗｏｒｄ
－ＧＣＮ模


块的学习
，导致Ｗｂｒｄ
－ＧＣＮ模块不能很好地编码语法信息
。也就证明了从局部到


全局的逐步编码过程有利于模型更好地表示包
，
验证了本文学习策略的有效性。


表
５
－１１
模型组件有效性分析
（ＡＵＣ）


ＭｅｔｈｏｄＡＵＣ


Ｌ２Ｇ
－ＧＣＮｗ／ｏＷｏｒｄ
－ＧＣＮ０
．４１９


Ｌ２Ｇ
－ＧＣＮｗ／ｏＳｅｎ
－ＧＣＮ０
．４２６


Ｌ２Ｇ
－ＧＣＮｗ／ｏＨｉｅｒａｒｃｈｉｃａｌ
－ｔｒａｉｎｉｎｇ０
．４０６


Ｌ２Ｇ
－ＧＣＮｗ／ｏＭＩＭ
０
．４２８


Ｌ２Ｇ
－ＧＣＮ
０
．４４８


ＢＥＲＴ＋
ＡＶＥ０
．４６７


＋ＡＴＴ０
．４５９


＋Ｓｅｎ
－ＧＣＮ０．４７６


＋Ｈｅｔｅ
－ＧＣＮ
０
．４８１


ＢＨ
－ＧＣＮ
０．４９１


Ｌ２Ｇ
－ＧＣＮ
ｗ／ｏＭＩＭ表示移除了ＭＩＭ
正则化器
。
其结果显示
，
模型在


Ｐ＠ＭＥＡＮ还有ＡＵＣ指标分别下降了２．９％和４．７％
。这验证了模型与ＭＩＭ正则


化器相结合可以获得比较好的性能。
因此本文采用ＭＩＭ正则化器来缓解长尾问


题
。如表
５
－９所示
，
在这个指标Ｈｉｔｓ＠Ｋ中本文的模型性能大幅提升
，
尤其是在


Ｈｉｔｓ＠２０指标下
。这表明ＭＩＭ正则化器通过鼓励模型预测长尾关系类别
，
对于


缓解长尾问题是有效的
。
此外
，
ＡＵＣ值的度量结果表明
，
完整的层次结构的性


能略优于其他变体
，
充分证明了本文的分层图祌经网络结构的有效性。


ＢＥＲＴ模型表示使用预训练模型编码文本后
，
直接使用分类器分类。实验结


果可以看出引入预训练语言模型
，对远程监督关系抽取任务确实有改善作用
。相


比词级图神经网络结构
，
ＢＥＲＴ所包含的语义语法知识更丰富
，有更强的编码效


果
。


模型＋Ｓｅｎ
－ＧＣＮ表示组合
ＢＥＲＴ和第三章所述的句级图神经网络模块
。


＋Ｈｅｔｅ
－ＧＣＮ表示增加了异质图网络结构
，
但仅仅使用加和平均的方法融合实体


和实例的信息表示。从以上实验结果可以看出增加了Ｓｅｎ
－ＧＣＮ模块后
，
模型因


为学习到实例之间的相关性
，
效果有了
一定的提升
。在使用
Ｈｅｔｅ
－ＧＣＮ模块后
，


因为増强了实体信息对关系抽取的指导作用
，
实验效果得到了进
一步提升
。


５３


北京邮电大学工程硕士学位论文


ＢＨ
－ＧＣＮ表示第四章所述的基于预训练模型的异质图神经网络远程监督关


系抽取模型
。
从以上实验结果可以看出
，
ＢＨ
－ＧＣＮ模型在
Ｐ＠Ｎ
，
ＡＵＣ值
，
以及


ＰＲ曲线三个指标上
，对比Ｌ２Ｇ
－ＧＣＮ模型
、＋Ｓｅｎ
－ＧＣＮ
、＋Ｈｅｔｅ
－ＧＣＮ均有所提升
。


相比于＋ＳｅｎＧＣＮ模型
，
ＢＨ
－ＧＣＮ在
Ｐ＠ＭＥＡＮ上提升了４
．７％
，
ＡＵＣ值提升了


３
．２％
。实验结果表明
，异质图结构增强的实体指导作用
，有效的改善了模型的效


果
。同时相比于＋Ｈｅｔｅ
－ＧＣＮ模型
，完整的ＢＨ
－ＧＣＮ模型进
一步改善了实验效果
，


这表明相比于简单直接的融合实例和实体的信息表示
，通过异质信息融合模块建


模后的实例
、
实体表不更加完备
。


５
．５
．４图卷积层数影响


４５
１
—￣
Ｔ

十一
丨


１２
３
４
５
６
７
８
９１０


Ｗｏｒｄ
－ＧＣＮｌａｙｅｒｓ


图
５
－３Ｗｏｒｄ
－ＧＣＮ层数对模型ＡＵＣ值的影响


４５
ＴＴ

一一剔


４
１


？
Ｉ
Ｉ
Ｉ
Ｉ
Ｉ
Ｉ
Ｉ
Ｉ
Ｉ


１２
３
４
５
６
７
８
９１０


Ｓｅｎ
－ＧＣＮｌａｙｅｒｓ


图
５
－４Ｓｅｎ
－ＧＣＮ层数对模型ＡＵＣ值的影响


为了进
一步研宄
Ｌ２Ｇ
－ＧＣＮ中ＧＣＮ层数的影响
，
本文在ＮＹＴ
－
１０的数据集


上评估了本文的
Ｌ２Ｇ
－ＧＣＮ模型
，
并且将ＧＣＮ层数设置在
｛
１
，２
，４
，５
，
１０
｝层的范围


内
。
为了研究不同模块对实验结果的影响
，
本文分别评估了Ｗｏｒｄ
－ＧＣＮ和
Ｓｅｎ
－


ＧＣＮ模块
，实验结果如图５
－３
，图５
－４所示
。根据实验结果
，本文得出以下结论
，


即两层Ｗｏｒｄ
－ＧＣＮ和
一层
Ｓｅｎ
－ＧＣＮ是最佳的参数组合
。
当本文对Ｗｏｒｄ
－ＧＣＮ进


行实验时
，
本文将
Ｓｅｎ
－ＧＣＮ设置为
一层
。
同时
，
当本文评估
Ｓｅｎ
－ＧＣＮ时
，
本文


使用两层的Ｗｏｒｄ
－ＧＣＮ
。此外
，
从实验结果可以观察出
，
当模型层数过多或过少


时
，模型实验结果均不佳
。
一方面
，
当模块结构太浅时
，
信息无法在节点间传播


很远
，
节点只能捕获邻域信息
。
另
一方面
，
当模块结构变得过深
、
层数过多时
，


５４


第五章实验设置与结果分析


模型将面临过度平滑的问题
，
导致节点的表示变得过于相似
，
丢失独有的信息
。


同时
，
由于梯度消失问题
，
模型也将变为不稳定状态
。
此外
，
本文认为当
Ｓｅｎ
－


ＧＣＮ模块层数为
１时效果最佳可以归因于Ｓｅｎ
－ＧＣＮ中全连通的图结构和本文整


体模型的多层结构
。
具体而言
，
由于
Ｓｅｎ
－ＧＣＮ的全连通图结构
，
所有节点的信


息都可以通过
一跳捕捉
。而自下而上的结构使得模型在
Ｓｅｎ
－ＧＣＮ模块下面已经


有
一个两层的Ｗｏｒｄ
－ＧＣＮ模块作为基础
，
多层
Ｓｅｎ
－ＧＣＮ会使得整体模型包含过


多的层数
。


５
．５
．５案例分析


表
５
－
１２从ＮＹＴ
－１０中随机挑选的
一个包数据。
该包的三元组为


（／ｂｕｓｉｎｅｓｓ／ｐｅｒｓｏｎ／ｃｏｍｐａｎｙ
，ＬａｕｒｅｎｃｅＤＪＦｉｎｋ，Ｂｌａｃｋｒｏｃｋ）
．


正确Ｗｏｒｄ
－Ｓｅｎ
－


序号实例关系三元组


标注ＧＣＮＧＣＮ


ＭｅｒｒｉｌｌＬｙｎｃｈ
＇
ｓｃｈｉｅｆ＾ｃｅｃｕｔｉｖｅ，Ｅ
．ｓｔａｎｌ＾０
＊
ｎｅａｌ，


０ｍｅｔｆｏｒ
ａ
ｑｕｉｅｔｂｒｅａｋｆａｓｔｗｉｔｈＬａｕｒｅｎｃｅＤ．Ｆｉｎｋ，ｈｉｓＦ


ｃｏｕｎｔｅｒｐａｒｔａｔＢｌａｃｋｒｏｃｋ，
．
．
．


Ｍｒ．Ｍａｃｋｈａｓａｌｓｏｈａｄｎｕｍｅｒｏｕｓｄｉｓｃｕｓｓｉｏｎｓｗｉｔｈ


１ＬａａｒｅｎｃｅＤ．Ｆｉｎｋ
，ｔｈｅｃｈｉｅｆｅｘｅｃｕｔｉｖｅｏｆＴ


Ｂｌａｃｋｒｏｃｋ＾
．
．


ｗｈｏ
ｊｕｓｔｗｅｅｋｓａｇｏｈａｄ
ｐｕｒｓｕｅｄａｓｉｍｉｌａｒ


２ａｇｒｅｅｍｅｎｔ
ｗｉｔｈＢｌａｃｋｒｏｃｋａｎｄｉｔｓｃｈｉｅｆｅｘｅｃｕｔｉｖｅ，Ｔ


／ｂｕｓｉｎｅｓｓ／ｐｅｒｓｏｎ／ｃｏｍｐａｎｙ


ＬａｕｒｅｎｃｅＤ．Ｆｉｎｋ
．



ＬａｕｒｅｎｃｅＤ
－Ｆｉｎｋ＾０
．４６２５０
．６６２６


Ｗｈｉｌｅｍｒ．ｋｎｉ＾
ｉｔｈａｓｈａｄｃｏｎｖｅｒｓａｔｉｏｎｓｗｉｔｈ


Ｂｌａｃｋｒｏｃｋ


３ＬａｕｒｅｎｃｅＤ．Ｆｉｎｋ，ｔｈｅｃｈｉｅｆ
ｅｘｅｃｕｔｉｖｅｏｆＴ


ＢＵｃｋｒｏｃｋ＾…


ａｎｄＬａｕｒｅｎｃｅＤ．Ｆｉｎｋ
，ｔｈｅｃｈｉｅｆ＾ｘｅｃｕｔｉｖｅ
ｏｆ


４ｔｈｅｍｏｎｅｙｍａｎａｇｅｍｅｎｔｆｉｒｍＢｌａｃｋｒｏｃｋ；ａｍｏｎｇＴ


ｏｔｈｅｒｓ
．


ｓａｉｄＬａｕｒｅｎｃｅＤ．ＦＩｎｋ，ｔｈｅｃｈａｉｎｎａｎｏｆ


５Ｂｌａｃｋｒｏｃｋ，ｗｈｏｓａｉｄｔｈａｔｒｏｂｓｐｅｙｅｒｏｖｅｒｓａｗｔｈｅＴ


ｄｅａｌｆｏｒ
ｔｉｓｈｍａａｓｐｅｙｅｒ．


为了进
一步评估
Ｓｅｎ
－ＧＣＮ模块的有效性
，
本文可视化了训练过程中实例的


分布
。本文在ＮＹＴ
－１０数据集中随机选择
一个包
，
数据展示在表
５
－１２，并使用
ｔ
－


ＳＮＥ来可视化该包中的实例
。
图
５
－５是经由Ｗｏｒｄ
－ＧＣＮ编码后的实例向量表示


的可视化
。
图
５
－６则是经
Ｓｅｎ
－ＧＣＮ获得的高级的实例向量表示分布。其中
，
红


色标记对应表５
－
１２中的实例数据。观察表
５
－１
１
，
可以发现实例＃０是该包中的
一


个噪音实例
，
实际上该实例的实体之间不包含任何的关系
。


５５


北京邮电大学工程硕士学位论文


１０００


＃
ｂ


８００


６〇〇＃
ｄ


４００


ＭＯ▲
〇ｍ


▲
２Ａ
３＊


０＾＊
ＶＡ
５


－２００＊
Ｃ


，
ＵＡ
４


－
６００
４００
－２０００２００４００６００８００


图
５
－５
实例表示的
／
－ＳＮＥ可视化
。
红色三角形标记为表
５
－
１２中数据
。
绿色圆标记和蓝色星


形标记是ＮＹＴ
－
１０数据集中选择的另外两条包数据
，
他们分别代表关系


“
／ｌｏｃａｔｉｏｎ／ｌｏｃａｔｉｏｎ／ｃｏｎｔａｉｎｓ
”
和
“
／ｆ
ｉｌｍ／ｆｉｌｍ／ｆｅａｔｕｒｅｄ
＿ｆ
ｉ
ｌｍ
—
ｌｏｃａｔｉｏｎｓ
”
。


▲
〇
．＊
ｂ
，
ｅＸ


２
（ｋ
？ｘｍ


。
：


－


、
、
、
一／


－
２００


４００


ＶＤ
＇


－６００★


Ｖ


★


４００
２０００２００４００６００


图
５
－６
经过Ｓｅｎ
－ＧＣＮ模块编码后的实例向量
／
－ＳＮＥ分布表示


经过
Ｓｅｎ
－ＧＣＮ模块的编码
，
本文在图
５
－６可以观察到除了实例＃０之外
，
所


有实例的分布都变得更加紧凑
。实例＃０与其他实例之间的距离被拉大
。换言之
，


噪音和正确数据在空间分布上越来越分离
。
从定量分析的角度来说
，
仅使用


Ｗｏｒｄ
－ＧＣＮ模块预测真实标签的概率为
０
．４６２５
，
而对于整个Ｌ２Ｇ
－ＧＣＮ模型其概


率为
０
．６６２６
，
大幅提高了４３
．３％
。


此外
，
本文随机选择数据集中的另外两个包
，
在图
５
－５
，
图
５
－６中这两个包


分别用绿色和蓝色标记
。
可以看出
，
经过
Ｓｅｎ
－ＧＣＮ模块的编码
，
这两个包的分


布也呈现收缩的趋势
。
同时可以发现
，本文的模型将两个蓝色实例编码后
，
距离


５６


第五章实验设置与结果分析


其他数据相差很远
，
因此模型将右图底部的实例Ｖ和ＶＩＩ视为噪音
。在调整实例


表示之后
，
本文可以获得更准确的包表示用于关系抽取
。


５
．５
．６ＧＤＳ数据集实验结果分析


；
Ｉ」
ｌ＿｜Ｉ
．
Ｉ


ＰＣＮＮ＋ＡＴＴＲＥＳＩＤＥＰＣＮＮ＋ＨＡＴＴＰＡ
－ＴＲＰＯｕｒＬ２Ｇ
－ＧＣＮＯｕｒＢＨ
－ＧＣＮ


■Ｐ＠１００＊Ｐ＠２００＊ＡＵＣ


图
５
－７ＧＤＳ数据集实验结果对比


图
５
－７报告了模型Ｌ２Ｇ
－ＧＣＮ和ＢＨ
－ＧＣＮ在ＧＤＳ数据集上的实验结果
。
可


以看出我们的模型在ＧＤＳ数据集上仍旧表现出了优秀的性能
。尤其是我们的ＢＨ
－


ＧＣＮ模型
，在Ｐ＠
１００
，Ｐ＠２００和ＡＵＣ三项上达到了１００
．０
，９８
．０和
８９
．０的指标
。


与之前的基线模型相比
，
ＢＨ
－ＧＣＮ模型取得了更好的性能
。实验结果说明验证了


本文设计的Ｌ２Ｇ
－ＧＣＮ和ＢＨ
－ＧＣＮ在相对较小且可靠的数据集ＧＤＳ上仍具有优


势和可靠性
。


５
．６本章小结


在本章中
，本文主要介绍了实验的主要环境
、实验所使用的数据集、评估指


标
，并总结了本论文相关的基线模型方法
，与本论文设计的模型实验结果进行比


较
，对本文模型实验结果进行了展示和分析
。同时
，本论文做了相关的对比实验
，


对模型的几种变体进行相关分析
，对两个图卷积模块的层数对模型的影响进行剖


析
，最后选取了数据集中的案例进行可视化分析
。总体而言
，
实验结果体现了本


文提出的两个模型
Ｌ２Ｇ
－ＧＣＮ和
ＢＨ
－ＧＣＮ均能有效提升远程监督关系抽取的性


能
。


５７


第六章远程监督关系抽取平台设计与实现


第六章远程监督关系抽取平台设计与实现


本论文基于上述远程监督关系抽取的研宄
，结合上文第三章提出的引入互信


息的最大化层级图神经网络结构和第四章提出的异质图网络结构
，搭建了基于包


级别的关系抽取演示平台
。本章将详细介绍搭建的演示系统的实现原理和相关技


术
，并对系统的组成模块进行介绍
。针对本论文的主要研究内容
，系统需要完成


的模块主要有
：
数据管理模块
，
用户管理模块
，
关系抽取模块
，
前端展示模块
。


６
．１系统需求分析


根据研究内容
，远程监督关系抽取系统主要需要以下四个功能模块
，功能需


求分析如下
，
用户用例图如图
６
－
１
：


１
）用户管理模块
：
该系统面向众多用户群体
，
为了区分用户数据
，
需要提


供用户管理服务
，保证拥有访问权限的用户才能进入系统
。该模块具体包括用户


注册、
登录、
登出功能
。
系统根据某
一唯
一
ＩＤ确定用户
，
用数据库记录用户的


相关信息
。


２）数据管理模块
：
用户与系统的交互时刻伴随着数据
，
对数据进行记录并


管理是
一个系统必不可少的服务。对于用户来说
，系统需要对用户的相关信息记


录到数据库中
，此外本系统支持用户上传文件
，进行批量分析
，并下载分析结果
。


３）关系抽取模块
：
本系统基于本论文提出的两个算法模型
，
进行包级别的


关系抽取系统
。用户可根据需求指定模型
，根据需求输入自定数量的语句
，对文


本进行关系抽取
。


４）前端展示模块
：
本系统根据用户的需求
，
用户可自定义输入实例数量
，


根据抽取结果
，
实时展现抽取三元组
，
并对抽取结果进行知识图谱可视化。


此外
，
本系统还同时考虑了系统非功能性需求
。


１
）
易用性需求
：
本系统基于浏览器和服务器结构
（Ｂｒｏｗｓｅｒ／Ｓｅｒｖｅｒ，
Ｂ／Ｓ）


结构搭建
，
采用
Ｆｌａｓｋ作为后端开发框架
，
前端采用ＨＴＭＬ５语言
。
网页的设计


功能列表清晰
，界面简洁
，
易于用户交互
，方便用户快速上手
，从而提高用户效


率
。


２）兼容性需求
：
本系统的前端采用
Ｂｏｏｔｓｔｒａｐ框架
，
支持所有主流浏览器
，


包括
ＩＥ、
Ｃｈｒｏｍｅ、
Ｆｉｒｅｆｏｘ等
。其响应式ＣＳＳ能够自适应于各种移动设备
，包括


台式电脑、平板电脑、手机等各种屏幕尺寸和分辨率的设备
。
，
并且前端元素保


持
一致
，
不会出现错位、
丢失等现象。


５９


北京邮电大学工程硕士学位论文


３
）鲁棒性需求
：
系统在执行过程中遇到输入错误
、
系统处理错误
，
运算错


误等异常时需要拥有继续稳定运行的能力
。本系统为用户错误的操作行为
，在前


端提供了指导说明
，在后端也设置了相对应的处理方法
。此外
，
系统包含日志模


块
，



＜
ｉｎｄｕｄｅ＞


：
－
；
－
；
＊
－＜
ｉｎｃｉｕｄｅ＞
－


／
＜
ｉｎｃＪｕｄｅ＞


／＾


〇／
＾
＾


用户＼＜
ｉｎｄｕｄｅ＞


－＜
ｉｎｄｕｄｅ＞
－
—
＜
ｉｎｄｕｄｅ＞
－


＜
ｉｎｄｕｄｅ＞


、、
－
、



＼志


＜
ｉｎｄｕｄｅ＞＾


图
６
－
１
包级关系抽取系统用户用例图


６
．２系统总体设计框架


针对以上所述功能需求
，本工作构建的演示平台整体框架如图
６
－２所示
。为


了方便用户交互
，维修方便
，本工作基于浏览器和服务器结构进行系统搭建
，前


端负责与用户交互
，
以可视化的方式展示关系抽取结果
；
后端进行数据处理
，模


型管理
，关系预测
。在该关系抽取系统中
，用户通过浏览器输入或上传文本数据
，


向服务器发送请求
，服务器接受用户数据和请求
，并完成相应的操作
，将抽取出


的关系三元组以及相关信息返回浏览器
，
并做出相应的可视化展示
。


本系统前端主要提供用户交互功能和结果展示功能
。其主要包括用户管理模


块中的用户注册
、
登录
、
登出功能
，
任务的相关介绍
，
数据模块中的文本输入
、


文本数量控制
、实体指定
，模型管理模块中的模型调用和测试
，
以及关系抽取模


块中最后的结果展示
，
并用知识图谱形式可视化
。
本文采用的技术框架如图
６
－３


所示
。


６０


第六章
远程监督关系抽取平台设计与实现


舰先系抽縣统



＞
ｊ
ｒ
＞
ｊｒ
＞
ｊ
ｒ
＞
ｊ
ｒ


麵難
￣
ｉ
｜
用户管理模块ｎ
丨
輕麵。
｜
关系抽取模块
ＹｙｖｙｙＹｙＴ
＞ｒＹ▼


用模
关
三


数
数
数户
用
用模
模
型Ｉ芫


据
据
据信
户
户型
型
数二
组


上
处
下息
登
登测
切
据ｇ可


传
理
载注
录
出试
换
管＾视


册理＾化


图
６
－２
包级关系抽取演示系统结构图


前端
Ｉ后端籠库


ＨＴＭＬ５


Ｐｙｔｈｏｎ


Ｂｏｏｔｓｔｒａｐ


！
！ＳＱＬｉｔｅ



：Ｆｌａｓｋ
：
丨


ｃｓｓ
｜

！


１
丨Ｐｙ
ｔｏｒｃｈ
ｉ
ＳＱＬＡｌｃｈｅｍｙ


Ｊａｖａｓｃｒ
ｉｐ
ｔ

：


Ｔ
．．＿Ｔｒａｎｓｆｏｒｍｅｒｓ


Ｊ
ｉｎｊａｚ
：

；


Ｅｃｈａｒｔｓ
丨
ｉ


图
６
－３
系统技术架构图


具体而言
，
本文采用
Ｂｏｏｔｓｔｒａｐ框架作为前端开发框架
，
Ｂｏｏｔｓｔｒａｐ框架适用


于快速开发Ｗｅｂ程序
，其基于Ｈｔｍｌ
、
ＣＳＳ、
Ｊａｖａｓｃｒｉｐ
ｔ开发
，提供了Ｗｅｂ定制和


响应式设计的功能
。
此外还应用了Ｅｃｈａｒｔｓ组件进行关系三元组的可视化
，
为用


户提供了更加直观和美观的系统页面
。
系统的后端部分主要包括数据管理模块
、


关系抽取模块
、
模型管理模块
、
用户管理模块
。
主要是用
Ｆｌａｓｋ框架进行后端搭


建
，
Ｈａｓｋ是
一个使用
Ｐｙｔｈｏｎ编写的轻量级
Ｗｅｂ应用框架
。
其模板引擎使用


Ｊｉｎｊａ２
，
ＷＳＧＩ工具箱采用Ｗｅｒｋｚｅｕｇ
。
同时
，
使用了ｂｃｒｙｐ
ｔ散列功能保护敏感数


据
。
当后端收到前端传入的数据
，
后台进行数据处理
，并调用相应模型进行关系


抽取
，
此处使用ＰｙＴｏｒ
ｃｈ作为深度学习模型搭建框架
。此外
，
本系统使用关系型


数据
ＳＱＬｉｔｅ存储用户信息
，
使用
ＳＱＬＡｌｃｈｅｍｙ从
ｓｑ
ｌ语句中脱离
，
按照
Ｐｙｔｈｏｎ


语法进行数据操作
。


６
１


北京邮电大学工程硕士学位论文


６
．３系统详细功能设计与实现


６
．３
．
１用户管理模块


当新用户使用本系统时
，
需要先填写图
６
－４中的相关信息来注册账号
。本系


统以用户
ｉｄ作为数据库主键
，
即唯
一
ＩＤ
。


登录注掰


用
：户名


齡箱墙ｉｌｋ


■


■
电话号码


ｐＭｍｉｍｍ■■


屬ｍ


图
６
－４
系统注册界面


当系统保存了用户信息后
，再次使用系统即可在登录页面
，如图
６
－５输入
“ 用


户名
” 和
“ 密码
” 直接登录
。


登录注琎


Ｍ户名


ｍｍ


錄１
：
：
：
：
：
：


— 咖
＿


图
６
－５
系统登录界面


进入系统以后
，
可以看到系统首页图
６
－６
，
即远程监督关系抽取的任务简介


即相关定义
。


６２


第六章远程监督关系抽取平台设计与实现


雖黯关系麵任务简介


■酱关系视简介


＇
关系抽取是自然语言处理领域的基本任务之
一
。
关系抽取任务有许多实际应用
，
包字


搭建在线搜索系统。
关系抽取的主要目标是发现文本中两个给定实体之间的语义关Ｉ


｜
Ｓ
１和Ｓ２作为示例
。
对于Ｓ１和Ｓ２
，
这两个句子包含相同的实体对
，
但表达不同的关Ｉ


：十
巴西
）
之间的特定关系
。
对于句子Ｓ１
，
有效的关系抽取算法可以识别关系并抽取关
；


ＤｉｓｔａｏｆＳｕｐｅｒｖｉｉｉｏＤＬａｂｅｌｉｎｇ
Ｐｒｏｃｅｓｓ

ｆ


Ｋｄｏｖ
ｔＭ＾ｒ
Ｒｒｐ？？
ｉｔ？ｎ
＞
｛
．Ｋ
｜


Ｒｆｆ
ａ
ｒｔ
ｉｏｎ

Ｅｎ
？ｉｒ
＞
１
ＩＥａｄｌ＞２Ｏｒｐａｓ


Ｎｓｉｋｓａｉｉ
ｔｙＲｏｎａ
ｌｄｏＢｏｚｉ！Ｂ
ｒａｚｉ
！
＊
＊
ＲｉｍａＪｄｏ
ｓｕｆ
ｌｅｒ
ｅｄ
ａ




＊Ｋ
ｓｅｕＭｒｔ
ｂｅｆｏｒ
ｅ

ｔｈｅ

１Ｖ９Ｊ
ｊ

ｔｎａ
ｉｃｈ
ａｎｄ
ｊ


０？
＿０（Ｔｉｍ
ＣｏｏｋＡｐｊＡｒ＼ｐｔ
ｅｙａＪ
Ｉｃａｋｓ＊
！ｖ

ｉｎ

ｔｈｅ
ｆｒ
ｎａ
ｉ




；ｖＲｏｔｕｋｂ
ｖａ？
ｂｅｗｎ
ｍＨｅ
ｊ
｜


Ｅｍｐ
ｌｏｙ＊ｄ
＿ＢｙＴｕｎＣｏｏｋＡｐｆ＾ｃ

ｊ


图
６
－６
系统主页


６
．３
．２关系抽取模块


关系抽取模块是本系统该的核心模块
。主要实现了模型切换
、实体指定
、数


据输入
、模型加载
，
抽取结果
，
结果可视化功能
。用户通过指定实体
，
输入文本


信息
，通过配置关系抽取模型
，便可对数据打包
，
进行包级关系抽取
。具体关系


抽取流程如图
６
－７所示
：


开始



＞ｒ
＿


指定实体
，输入文本数据


＞



麟麵


Ｉ先系抽取
Ｉ


ｉ训练
￣— ￣￣
ｉ


：Ｎ
｜
Ｙ｜
：


！ｉ
＞
ｊ
ｒ
；


ｉ觀预舰
，贿句殺析
麵预麵
三元雖果可视化
｜


！
＞
＜
Ｉ


；
＞

［
＞
ｒ

！


；加载词向量
？
加载模型
＞关系抽取
｜


图
６
－７
包级关系抽取流程图


６３


北京邮电大学工程硕士学位论文


模型具体实现界面展示如下
。首先
，本文点击系统左侧菜单栏的
“ 包级别关


系抽取
” 按钮
，
进入关系抽取核心页面
。如图
６
－８所示
，
本文可以根据需求
，
进


行模型选择
。


包级别关系抽取


赴远程監督关系抽敢包级别关系抽敢


模型选择


Ｌ２Ｇ
－ＧＣＮ


独远程监督关系抽取包级别关系抽取


模型选择


Ｌ２Ｇ
－ＧＣＮ


ＢＧＭ


ＨＧＣＮ


图
６
－８模型选择界面


选定模型之后
，本文可以通过输入实体
，来指定本文在文本中所要抽取的对


应实体关系
。然后本文可以在句子相应位置的输入框输入文本
，本文可以根据所


需的输入数量
，
点击
“ 添加句子
” 来增加文本输入框
，
同时实现了相应的删除功


能
。
具体如图
６
－９所示
：


输入文本


指足买体：


：


滗实体


肝
：ｈｅ８〇


图
６
－９
指定实体
、
输入文本界面


输入文本之后
，
点击
“ 抽取关系
” 按钮
，
会对输入文本进行相应处理
，
并认


为输入文本为包
，在包结构的基础上进行关系抽取
。关系抽取的结果以下图
６
－
１０


形式进行返回
。


６４


第六章远程监督关系抽取平台设计与实现


头实体
：
！ａｕｒｅｎｃｅ
＿ｄＪｎｋ


尾实体
：
ｂ
ｌａｃｋｒｏｃｋ


实体关系
：
／ｂｕｓ
ｉｎｅｓｓ／ｐｅｒｓｏｎ／ｃｏｍｐａｎｙ


图
６
－
１０
关系抽取结果展示界面


抽取结果之后
，本文可以点击
“ 可视化
” 按钮
，
对相应的抽取结果进行可视


化的展示
，
可视化结果如图
６
－
１
１所示
。


可视化
清空


麵头买体＿ＭＳ体


图
６
－
１
１
关系三元组可视化界面


此外
，
用户可以通过上传按照要求格式处理好的文件
，
进行批量的关系抽


取
，
系统进行相应处理后
，
同样会以文件格式返回
，
并提供相应的下载接口
。


系统的界面如图
６
－
１２所示
，
数据输入格式如图
６
－１３所示
，
数据输出格式如图


６
－
１４所示
。


包级别关系抽取


模型选择


ＢＧＭ分
｜


文件上传


这
未这待任圬文锋


〇


图
６
－
１２批量关系抽取的界面


６５


北京邮电大学工程硕士学位论文


籲＃
ｒｅｆ
ｉ
ｌｅ
．ｔｘｔ


｜
（ｆｌｏｒｉｄａ
，ｂｏｃａ
一ｒａｔｏｎ
）


ｆｒｉｅｓ
—
ｊｅｒｏｍｅｅ
．
－Ｉｒｂ
－
ｊａｃｋ
－
ｒｒｂ
－
，１０３ｄｉｅｄｔｕｅｓｄａ
ｊ／
ｆｆｅｂｒｕａｒ
＾６ｔｈｉｎｂｏｃａ
＿ｒａｔｏｎ，ｆｌｏｒｉｄａ．


ｄｒ
．ｉｈ
Ｊ
Ｌｅｗａｓａｒｅｔｉｒｅｄｃａｐｔａｉｎｉｎｔｈｅｕｓｎａｖａｌｒｅｓｅｒｖｅａｎｄａｆｏｒｍｅｒｍｅｍｂｅｒｏｆｔｈｅｎｅｗｙｏｒｋａｃａｄｅｍｙｏｆ


ｄｅｎｔｉｓｔｒｙａｎｄｔｈｅｎａｖｙｌｅａｇｕｅｏｆｂｏｃａ＿ｒａｔｏｎ？ｆｌｏｒｉｄａ．


ｈｅｌｌｉｎａｎ
－一ｂｅｒｎａｒｄａ
．，ａｇｅ８８ｏｆｂｏｃａ＿ｒａｔｏｎ，ｆｌｏｒｉｄａ．


ｇｕｔｔｅｒｍａｎｗａ
ｒｈｅｉｔｍｅｍｏｒｉａｌｃｈａｐｅｌ，ｂｏｃａｒａｔｏｎ，ｆｌｏｒｉｄａ．


ｂｅｌｏｖｅｄｍｏｔｈｅｒｏｆｄｏｎｈａ
ｒｖｏｐｄ，ｂｏｃａ一ｒａｔｏｎ，ｆｌｏｒｉｄａ．


ｐａｐａａｂｅｉｓａｌｓｏｓｕｒｖｉｖｅｄｂｙｈｉｓｅｘｔｅｎｄｅｄｆａｍｉｌｙａｄａｍ，ｓｕｓａｎ，ａｎｄｒｅｗ，ｍａｘａｎｄｉｏｒｄａｎｓａｈｎｏｆｎｅｗ


ｃｉｔｙ，知
＃Ｊｏｎａｔｈａｎ，ｋａｔｈ＾
，ｓａ
ｒｎａｎｔｈａ，ａｎｄｅｒｉｎｓａｈｎｏｆｂ６ｃａ＿ｒａｔｏｎ
＜
，ｆｌｏｒｉｄａ，ａｎｄ
＇
ｅｒｉｃ，ａｍｙ，


ｚａｃｈａｎｄｌｕｃａｓｏｆｒｅｄｗｏｏｄｃｉｔｙ，
ｃａ．


图
６
－
１３
文件输入格式


鬱參＃義
ｒｅｆ
ｉ
ｌ一ａｎｓ
．ｔｘｔ
— Ｅ缠辑


（ｆｌｏｒｉｄａ
，ｂｏｃａ
＿ｒａｔｏ
，／ｌｏｃａｔｉｏｎ／ｕｓ＿ｃｏｕｎｔｙ／ｃｏｕｎｔｙ＿ｓｅａｔ
）
｜


图
６
－
１４
文件输出格式


６
．３
．３数据管理模块


数据管理模块主要负责数据的存储
、读取、预处理等功能
。本系统主要使用


ＳＱＬｉｔｅ进行用户信息的存储
，
数据表设计如下表６
－
１
：


表
６
－
１
数据库用户表设计


名称数据类型约束类型是否创建索引
是否非空
字段注释


ｕｉｄＩｎｔｅｇｅｒ主键是ｎｏｔｎｕｌｌ用户的唯
一
ＩＤ


ｕｓｅｒｎａｍｅＳｔｒｉｎｇ（２０）
－是ｎｏｔｎｕｌｌ用户名


ｐａｓｓｗｏｒｄＳｔｒｉｎｇ（２０）
－否ｎｏｔｎｕｌｌ用户系统密码


ｅｍａ
ｉ
ｌＳｔｒｉｎｇ（３０）
－是ｎｕｌｌ用户电子邮箱


ｔｅｌｅｐｈｏｎｅＳｔｒｉｎｇ（２０
）
－是ｎｕｌｌ用户联系方式


ｒｅｇ
ｉｓｔｅｒｅｄＤａｔｅｔｉｍｅ
－否ｎｕｌｌ时间戳
，
账户



｜创建的时间


对于后端服务
，获取到前端数据后
，根据模型不同对数据进行不同的预处理
，


根据用户指定实体词
，
对实体位置进行定位
，
便于模型使用
。数据库的ＥＲ图如


图
６
－
１５所示
：


６６


第六章远程监督关系抽取平台设计与实现


＾
ＴＹ＾
）＾


（？
）＼％＼（＾
）


图
６
－
１５ＳＱＬｉｔｅ数据库Ｅ
－Ｒ图


６
．４系统测试


６
．４
．
１功能测试


表
６
－２
功能性测试结果


测试模块测试功能测试内容
测试结果结论


输入非法格式的
提示相应信息格


校验功能正常


测试数据式错误


注册



注册功能有有效


胃ａ＾
正常注册注册功能正常


输入错误密码无法登录校验功能正常


登录



登录功能有效性
正常进入系统登录功能正常


＾注销账户
正常退出系统登出功能正常


多种格式上传成


数据上传文件上传文件上传正常






数据管理模块数据下载
预测结果下载下载成功
数据下载正常


增加或删除输入输入数量增删正


输入数量框增删增删成功


ｍ＃


模型管理
模型切换
切换模型
模型切换成功模型切换正常


三元组抽取模型预测
返回抽取结果模型预测正常


关系抽取模块实体
、
关系可视


三元组可视化三元组展示可视化功能正常



｜
丨化正常



６７


北京邮电大学工程硕士学位论文


完成系统开发后
，对系统含有功能逐
一测试
，保证系统的正常运行
。系统的


测试结果如表６
－２所示
。


６
．５本章小结


本章主要介绍了基于远程监督的关系抽取演示系统的设计与实现
。首先介绍


了系统的功能需求
，
明确系统功能。接着介绍了系统的整体框架
，本文使用Ｂ／Ｓ


结构搭建系统
，
使用Ｂｏｏｔｓｔｒａｐ和
Ｆｌａｓｋ的框架组合
，
结合Ｐｙｔｈｏｎ编程语言进行


系统开发。然后依次介绍了系统各个模块的实现方法，并演示了相应功能模块的


使用
。


６８


第七章总结与展望


第七章总结与展望


７
．
１研究工作总结


信息抽取技术旨在对复杂的异构数据源进行分类
、精简
，获取细化的知识单


元
。关系抽取作为信息抽取的关键技术之
一
，在知识图谱的自动化构建中起到至


关重要的作用
。然而
，
高昂的标注成本是研究者所需要面临的问题
。远程监督关


系抽取的提出
，缓解了数据标注带来的困扰
，降低了数据标注的成本
。与之同时
，


也伴随着噪音标注和长尾问题两大挑战
。首先针对噪声问题
，前人工作常用注意


力机制为包内噪音分配低权重
，但忽略了包内实例之间全局语义联系
。其次针对


长尾问题
，
引入大量的外部知识是
一种有效的解决策略
，但需要
一种更直接通用


的方式使得模型可以快速迁移
。本文针对以上两大问题
，提出了基于包结构的远


程监督关系抽取框架
。
具体研究工作如下
：


１
）提出基于互信息最大化的层级图神经网络
Ｌ２Ｇ
－ＧＣＮ模型来学习鲁棒性


的包表示
，
以减缓远程监督带来的噪声问题和长尾问题
。在该模型中
，词级别的


图神经网络结构用于对实例的局部句法信息进行建模
。句子级图神经网络聚合包


中实例之间实体关系的全局信息
，从而更好地利用实例之间的相关性
。
以提出更


直接和通用方法为目的
，
引入最大化互信息作为正则化器
，通过对模型的训练进


行约束
，来鼓励模型保持预测相对平衡的分布
。通过以上方法增加模型的鲁棒性


并提高模型处理长尾关系的性能
。此外
，完整的自下而上的模型结构以两阶段分


层的方式进行训练
，
来学习多粒度的信息
。


２）提出异质图神经网络结构ＢＨ
－ＧＣＮ来加强实体信息对关系抽取的指导作


用
。在上述Ｌ２Ｇ
－ＧＣＮ框架中句子级图神经网络模块的基础上
，
加入实体作为独


立节点
，构建实体和实例作为节点的异质图网络结构
。该结构依据不同节点类型


通过自注意力机制更新图节点状态
，来学习实体和实例之间的关系
，从而获得实


体和实例的增强表示
。此外
，本文设计了
一种实体和实例信息融合的门控机制来


增强包表示
。同时
，本文使用了预训练模型替代了词级别图神经网络作为编码器
，


来引入预训练模型所含的语义语法信息
。


３
）搭建了
一种基于包级别的关系抽取演示系统
。
整个系统包含如下功能模


块
：
数据管理模块
，
用户管理模块
，关系抽取模块以及前端展示模块
。通过输入


自定义数量的文本
，对文本进行以包为单位的关系抽取
，并且可对抽取结果进行


可视化展示
。


６９


北京邮电大学工程硕士学位论文


７
．２未来工作总结


本文围绕远程监督关系抽取中存在的两大挑战
，噪音问题和长尾问题开展研


宄
，取得了
一定程度的进展
，但仍然存在提升的空间
。未来
，我们可能探索的方


向如下
：


１
）更有效的处理只包含单实例的包数据
。
由于远程监督关系抽取数据集是


采用启发式对齐方法产生
，有些关系三元组通过对齐只产生了单条数据
。在ＭＩＬ


框架下
，该包只由单句实例构成
。本文提出的模型在包结构的基础上
，通过句级


图神经网络模块学习包内实例的语义联系
，取得了良好的实验效果。但如何更有


效地处理单实例的包数据
，
进
一步提升模型性能
，
是未来工作需要探索的问题
。


２）探索更多的降噪方法。
在远程监督关系抽取领域中
，
噪声信息始终无法


完全地去除
，其对模型的损害仍然是无可避免。因此探索更加直接有效的降噪方


法对于远程监督关系抽取来说至关重要。当前的降噪方法大多仍处于利用注意力


机制降低噪声权重的模式。
引入重标方法
，利用模型对文本知识的学习
，对数据


进行自动化的重新标注
，
是未来远程监督关系抽取可以研究的方向之
一
。


７０


辦嫌，


参考文献


［１
］ＶｅｙｓｅｈＡ
ＰＢ
，ＶａｎＮｇｕｙｅｎＭ
，ＴｒｕｎｇＮＮ
？ｅｔａｌ
．ＭｏｄｅｌｉｎｇＤｏｃｕｍｅｎｔ
－Ｌｅｖｅｌ


ＣｏｎｔｅｘｔｆｏｒＥｖｅｎｔＤｅｔｅｃｔｉｏｎｖｉａＩｍｐｏｒｔａｎｔＣｏｎｔｅｘｔＳｅｌｅｃｔｉｏｎ
［Ａ］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆ


ｔｈｅ２０２１ＣｏｎｆｅｒｅｎｃｅｏｎＥｍｐ
ｉｒｉｃａｌＭｅｔｈｏｄｓｉｎＮａｔｕｒａｌＬａｎｇｕａｇｅ


Ｐｒｏｃｅｓｓｉｎｇ［Ｃ
］９２０２１
：５４０３
－５４１３
．


［２
］ＤｉｎｇＨ
，ＬｕｏＸ．ＡｔｔｅｎｔｉｏｎＲａｎｋ：ＵｎｓｕｐｅｒｖｉｓｅｄＫｅｙｐｈｒａｓｅＥｘｔｒａｃｔ
ｉｏｎｕｓｉｎｇＳｅｌｆａｎｄ


ＣｒｏｓｓＡｔｔｅｎｔｉｏｎｓ［Ａ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆ
ｔｈｅ２０２１ＣｏｎｆｅｒｅｎｃｅｏｎＥｍｐ
ｉｒｉｃａｌＭｅｔｈｏｄｓ


ｉｎＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ［Ｃ］，２０２１
：１９１９
－１９２８
．


［３
］
孙茂松，李莉，刘知远
．面向中英平行专利的双语术语自动抽取
［Ｊ］
．清华大


学学报（自然科学版），２０１４
，５４（１０）
：１３３９
－１３４３
．


［４
］Ａｇ
ｉｃｈｔｅｉｎＥ
，ＧｒａｖａｎｏＬ
．Ｓｎｏｗｂａｌｌ
：Ｅｘｔｒａｃｔｉｎｇｒｅｌａｔｉｏｎｓｆｒｏｍｌａｒｇｅｐ
ｌａｉｎ
－ｔｅｘｔ


ｃｏｌｌｅｃｔｉｏｎｓ［Ａ
］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅｆｉｆｔｈＡＣＭｃｏｎｆｅｒｅｎｃｅｏｎＤｉｇ
ｉｔａｌ


ｌｉｂｒａｒ
ｉｅｓ
［Ｃ
］，２０００：８５
－９４．


［５
］ＭｉｎｔｚＭ
，ＢｉｌｌｓＳ
，ＳｎｏｗＲ，ｅｔａｌ
．Ｄｉｓｔａｎｔｓｕｐｅｒｖｉｓｉｏｎｆｏｒｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎｗｉｔｈｏｕｔ


ｌａｂｅｌｅｄｄａｔａ
［Ａ
］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＪｏｉｎｔＣｏｎｆｅｒｅｎｃｅｏｆ
ｔｈｅ４７ｔｈＡｎｎｕａｌＭｅｅｔｉｎｇ


ｏｆｔｈｅＡＣＬａｎｄｔｈｅ４ｔｈＩｎｔｅｒ
ｎａｔｉｏｎａｌＪｏｉｎｔＣｏｎｆｅｒｅｎｃｅｏｎＮａｔｕｒａｌＬａｎｇｕａｇｅ


Ｐｒｏｃｅｓｓｉｎｇｏｆ
ｔｈｅＡＦＮＬＰ
ｆＣ
］
，２００９：１００３
－１０１１
．


［６
］ＬｉｕＣ
，ＳｕｎＷ
５ＣｈａｏＷ
，ｅｔａｌ
．Ｃｏｎｖｏｌｕｔｉｏｎｎｅｕｒａｌｎｅｔｗｏｒｋｆｏｒｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎ［Ａ
］


／／Ｉｎｔｅｒ
ｎａｔｉｏｎａｌｃｏｎｆｅｒｅｎｃｅｏｎａｄｖａｎｃｅｄｄａｔａｍｉｎｉｎｇａｎｄａｐｐ
ｉｉｃａｔｉｏｎｓ［Ｃ
］
：


Ｓｐｒｉｎｇｅｒ
，２０１３
：２３
１
－２４２，


［７
］ＺｅｎｇＤ
，ＬｉｕＫ
，ＬａｉＳ
，
ｅｔａｌ
－Ｒｅｌａｔ
ｉｏｎｃｌａｓｓｉｆ
ｉｃａｔ
ｉｏｎｖｉａｃｏｎｖｏｌｕｔ
ｉｏｎａｌｄｅｅｐｎｅｕｒａｌ


ｎｅｔｗｏｒｋ
［Ａ
］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆＣＯＬＩＮＧ２０１４
，ｔｈｅ２５ｔｈｉｎｔｅｒ
ｎａｔｉｏｎａｌｃｏｎｆｅｒｅｎｃｅｏｎ


ｃｏｍｐｕｔａｔｉｏｎａｌｌｉｎｇｕｉｓｔｉｃｓ：ｔｅｃｈｎｉｃａｌ
ｐａｐｅｒｓ［Ｃ］，２０１４：２３３５
－２３４４
，


［８］ＺｈａｎｇＤ
，ＷａｎｇＤ
．Ｒｅｌａｔｉｏｎｃｌａｓｓｉｆｉｃａｔｉｏｎｖｉａｒｅｃｕｒｒｅｎｔｎｅｕｒａｌｎｅｔｗｏｒｋ［Ｊ］
，ａｒＸｉｖ


ｐｒｅｐｒ
ｉｎｔａｒＸｉｖ：１５０８０１００６
，２０１５
，


［９］ＢａｈｄａｎａｕＤ
，ＣｈｏＫ，Ｂｅｎｇ
ｉｏＹ．ＮｅｕｒａｌＭａｃｈｉｎｅＴｒａｎｓｌａｔｉｏｎｂｙＪｏｉｎｔｌｙＬｅａｒｎｉｎｇｔｏ


ＡｌｉｇｎａｎｄＴｒａｎｓｌａｔｅ［Ｊ］
．ＣｏｍｐｕｔｅｒＳｃｉｅｎｃｅ
，２０１４．


［
１０
］ＺｈｏｕＰ
５ＳｈｉＷ
，ＴｉａｎＪ
，ｅｔａｌ．Ａｔｔｅｎｔｉｏｎ
－ＢａｓｅｄＢｉｄｉｒｅｃｔｉｏｎａｌＬｏｎｇＳｈｏｒｔ
－Ｔｅｒｍ


ＭｅｍｏｒｙＮｅｔｗｏｒｋｓｆｏｒＲｅｌａｔｉｏｎＣｌａｓｓｉｆｉｃａｔｉｏｎ［Ａ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ５４ｔｈ


ＡｎｎｕａｌＭｅｅｔｉｎｇｏｆｔｈｅＡｓｓｏｃｉａｔ
ｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ（Ｖｏｌｕｍｅ２：


ＳｈｏｒｔＰａｐｅｒｓ）［Ｃ
］５２０１６


［１
１
］ＨｏｃｈｒｅｉｔｅｒＳ
？ＳｃｈｍｉｄｈｕｂｅｒＪ．Ｌｏｎｇｓｈｏｒｔ
－ｔｅｒｍｍｅｍｏｒｙ［Ｊ］
．Ｎｅｕｒａｌｃｏｍｐｕｔａｔｉｏｎ，


７１


北京邮电大学工程硕士学位论文


１９９７
，９（８）
：１７３５
－１７８０．


［１２
］ＫｉｐｆＴＮ
，ＷｅｌｌｉｎｇＭ
．Ｓｅｍｉ
－ｓｕｐｅｒｖｉｓｅｄｃｌａｓｓｉｆ
ｉｃａｔ
ｉｏｎｗｉｔｈｇｒａｐｈｃｏｎｖｏｌｕｔｉｏｎａｌ


ｎｅｔｗｏｒｋｓ［Ｊ
Ｊ
．ａｒＸｉｖ
ｐｒｅｐｒｉｎｔａｒＸｉｖ：１６０９０２９０７
，２０１６．


［
１３
］ＧｕｏＺ
？ＺｈａｎｇＹ
３ＬｕＷ．Ａｔｔｅｎｔｉｏｎ
ｇｕｉｄｅｄ
ｇｒａｐｈｃｏｎｖｏｌｕｔｉｏｎａｌｎｅｔｗｏｒｋｓｆｏｒｒｅｌａｔｉｏｎ


ｅｘｔｒａｃｔｉｏｎ［Ｊ］
，ａｒＸｉｖ
ｐｒｅｐｒｉｎｔａｒＸｉｖ
：１９０６０７５１０，２０１９．


［
１４
］ＷｅｉＺ
５ＳｕＪ，ＷａｎｇＹ
？ｅｔａｌ
．Ａｎｏｖｅｌｈｉｅｒａｒｃｈｉｃａｌｂｉｎａｒｙｔａｇｇ
ｉｎｇｆｒａｍｅｗｏｒｋｆｏｒ
ｊｏｉｎｔ


ｅｘｔｒａｃｔｉｏｎｏｆｅｎｔｉｔｉｅｓａｎｄｒｅｌａｔｉｏｎｓ［Ｊ］
．ａｒＸｉｖ
ｐｒｅｐｒｉｎｔａｒＸｉｖ
：１９０９０３２２７
，２０１９
．


［
１５］ＺｈｅｎｇＨ
，ＷｅｎＲ
，ＣｈｅｎＸ
，ｅｔａｌ．ＰＲＧＣ：ＰｏｔｅｎｔｉａｌＲｅｌａｔｉｏｎａｎｄＧｌｏｂａｌ


ＣｏｒｒｅｓｐｏｎｄｅｎｃｅＢａｓｅｄＪｏｉｎｔＲｅｌａｔ
ｉｏｎｓｄＴｒｉｐｌｅＥｘｔｒａｃｔｉｏｎ［Ａ
］
／／Ｏｎｌｉｎｅ：Ａｓｓｏｃｉａｔｉｏｎ


ｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔ
ｉｃｓ？２０２１
：６２２５
－６２３５
．


［１６］ＲｉｅｄｅｌＳ
，ＹａｏＬ
，ＭｃＣａｌｌｕｍＡ．Ｍｏｄｅｌｉｎｇｒｅｌａｔｉｏｎｓａｎｄｔｈｅｉｒｍｅｎｔｉｏｎｓｗｉｔｈｏｕｔ


ｌａｂｅｌｅｄｔｅｘｔ
［Ａ］／／ＪｏｉｎｔＥｕｒｏｐｅａｎＣｏｎｆｅｒｅｎｃｅｏｎＭａｃｈｉｎｅＬｅａｒｎｉｎｇａｎｄＫｎｏｗｌｅｄｇｅ


Ｄｉｓｃｏｖｅｒｙ
ｉｎＤａｔａｂａｓｅｓ［Ｃ
］
：Ｓｐｒ
ｉｎｇｅｒ，２０１０
：１４８
，１６３
．


［１７］ＤｉｅｔｔｅｒｉｃｈＴＧ，ＬａｔｈｒｏｐＲＨ
，Ｌｏｚａｎｏ
－ＰｅｒｅｚＴ．Ｓｏｌｖｉｎｇ
ｔｈｅｍｕｌｔｉｐ
ｌｅｉｎｓｔａｎｃｅ


ｐｒｏｂｌｅｍｗｉｔｈａｘｉｓ
－
ｐａｒａｌｌｅｌｒｅｃｔａｎｇ
ｌｅｓ［Ｊ］
．Ａｒｔ
ｉｆ
ｉｃｉａｌｉｎｔｅｌｌｉｇｅｎｃｅ
，１９９７
，８９（
１
－２）
：３１
－


７Ｌ


［
１８
］ＺｅｎｇＤ
，ＬｉｕＫ
９ＣｈｅｎＹ
？ｅｔａｌ
．Ｄｉｓｔａｎｔｓｕｐｅｒｖｉｓｉｏｎｆｏｒｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎｖｉａ


ｐ
ｉｅｃｅｗｉｓｅｃｏｎｖｏｌｕｔｉｏｎａｌｎｅｕｒａｌｎｅｔｗｏｒｋｓ
［Ａ］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆ
ｔｈｅ２０１５ｃｏｎｆｅｒｅｎｃｅ


ｏｎｅｍｐ
ｉｒｉｃａｌｍｅｔｈｏｄｓｉｎｎａｔｕｒａｌｌａｎｇｕａｇｅ
ｐｒｏｃｅｓｓｉｎｇ［Ｃ］
？２０１５
：
１７５３
－
１７６２
．


［１９］ＬｉｎＹ
，ＳｈｅｎＳ
，ＬｉｕＺ
，ｅｔａｌ
．Ｎｅｕｒａｌｒｅｌａｔ
ｉｏｎｅｘｔｒａｃｔ
ｉｏｎｗｉｔｈｓｅｌｅｃｔｉｖｅａｔｅｎｔ
ｉｏｎｏｖｅｒ


ｉｎｓｔａｎｃｅｓ［Ａ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ５４ｔｈＡｎｎｕａｌＭｅｅｔｉｎｇｏｆｔｈｅＡｓｓｏｃｉａｔ
ｉｏｎｆｏｒ


ＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ
（Ｖｏｌｕｍｅ１
：ＬｏｎｇＰａｐｅｒｓ）［Ｃ
］
，２０１６
：２１２４
－２１３３
．


［２０
］ＶａｓｈｉｓｈｔｈＳ，ＪｏｓｈｉＲ
，Ｐｒａｙ
ｅ＾ａＳＳ
？ｅｔａｌ
．Ｒｅｓｉｄｅ：Ｉｍｐｒｏｖｉｎｇｄｉｓｔａｎｔｌｙ
－ｓｕｐｅｒｖｉｓｅｄ


ｎｅｕｒａｌｒｅｌａｔｉｏｎｅｘｔｒａｃｔ
ｉｏｎｕｓｉｎｇｓｉｄｅｉｎｆｏｒｍａｔｉｏｎ［Ｊ
］
，ａｒＸｉｖｐｒｅｐｒ
ｉｎｔ


ａｒＸｉｖ：
１８１２０４３６１
，２０１８
．


［２１］ＣｈｏＫ
，ＶａｎＭｅｒｒｉｅｎｂｏｅｒＢ
５ＧｕｌｃｅｈｒｅＣ
，ｅｔａｌ．Ｌｅａｒｎｉｎｇｐｈｒａｓｅｒｅｐｒｅｓｅｎｔａｔｉｏｎｓ


ｕｓｉｎｇＲＮＮｅｎｃｏｄｅｒ
－ｄｅｃｏｄｅｒｆｏｒｓｔａｔ
ｉｓｔｉｃａｌｍａｃｈｉｎｅｔｒａｎｓｌａｔｉｏｎ［Ｊ
］
．ａｒＸｉｖ
ｐｒｅｐｒｉｎｔ


ａｒＸｉｖ
：１４０６１０７８
，２０１４．


［２２
］ＹｅＺ
－Ｘ
，ＬｉｎｇＺ
－Ｈ
．Ｄｉｓｔａｎｔｓｕｐｅｒｖｉｓｉｏｎｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎｗｉｔｈｉｎｔｒａ
－ｂａｇａｎｄｉｎｔｅｒ
？


ｂａｇａｔｅｎｔ
ｉｏｎｓ［Ｊ］
，ａｒＸｉｖ
ｐｒｅｐｒｉｎｔａｒＸｉｖ：１９０４００１４３
，２０１９
．


［２３］ＸｉｎｇＲ
，ＬｕｏＪ．Ｄｉｓｔａｎｔｓｕｐｅｒｖｉｓｅｄｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎｗｉｔｈｓｅｐａｒａｔｅｈｅａｄ
－ｔａｉｌ


ＣＮＮ
［Ａ］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ５ｔｈＷｏｒｋｓｈｏｐｏｎＮｏｉｓｙＵｓｅｒ
－ｇｅｎｅｒａｔｅｄＴｅｘｔ（Ｗ
－


ＮＵＴ２０１９）［Ｃ
］，２０１９
：２４９
－２５８．
，


［２４
］ＳｈａｎｇＹ
－Ｍ
？ＨｕａｎｇＨ，ＳｕｎＸ
，ｅｔａｌ
．Ａ
ｐａｔｔｅｒｎ
－ａｗａｒｅｓｅｌｆ
－ａｔｔｅｎｔｉｏｎｎｅｔｗｏｒｋｆｏｒ


７２






ｄｉｓｔａｎｔｓｕｐｅｒｖｉｓｅｄｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎ［Ｊ］
．ＩｎｆｏｒｍａｔｉｏｎＳｃｉｅｎｃｅｓ，２０２２
，５８４
：２６９
－


２７９．


［２５
］ＪｉａｎｇＸ
，ＷａｎｇＱ３ＬｉＰ
５ｅｔａｌ
．Ｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎｗｉｔｈｍｕｌｔｉ
－ｉｎｓｔａｎｃｅｍｕｌｔｉ
－ｌａｂｅｌ


ｃｏｎｖｏｌｉｒｔ
ｉｏｎａｌｎｅｕｒａｌｎｅｔｗｏｒｋｓ［Ａ］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆＣＯＬＩＮＧ２０１６
，ｔｈｅ２６ｔｈ


ＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ：Ｔｅｃｈｎｉｃａｌ


Ｐａｐｅｒｓ［Ｃ
］？２０１６
：
１４７１
－１４８０
．


［２６
］ＬｉｕＴ
，ＷａｎｇＫ
，ＣｈａｎｇＢ
，ｅｔａｌ
．Ａｓｏｆ
ｔ
－ｌａｂｅｌｍｅｔｈｏｄｆｏｒｎｏｉｓｅ
－ｔｏｌｅｒａｎｔｄｉｓｔａｎｔｌｙ


ｓｕｐｅｒｖｉｓｅｄｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎ
［Ａ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２０１７Ｃｏｎｆｅｒｅｎｃｅｏｎ


Ｅｍｐ
ｉｒ
ｉｃａｌＭｅｔｈｏｄｓｉｎＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ［Ｃ］？２０１７：
１７９０
－１７９５
．


［２７
］ＨｕａｎｇＹＹ
，ＷａｎｇＷＹ．Ｄｅｅｐｒｅｓｉｄｕａｌｌｅａｒｎｉｎｇｆｏｒｗｅａｋｌｙ
－ｓｕｐｅｒｖｉｓｅｄｒｅｌａｔｉｏｎ


ｅｘｔｒａｃｔｉｏｎ
［Ｊ］
，ａｒＸｉｖ
ｐｒｅｐｒ
ｉｎｔａｒＸｉｖ：１７０７０８８６６
，２０１７
．


［２８
］ＦｅｎｇＸ
，ＧｕｏＪ
，Ｑ
ｉｎＢ
，ｅｔａｌ
．ＥｆｅｃｔｉｖｅＤｅｅｐＭｅｍｏｒｙＮｅｔｗｏｒｋｓｆｏｒＤｉｓｔａｎｔ


ＳｕｐｅｒｖｉｓｅｄＲｅｌａｔｉｏｎＥｘｔｒａｃｔｉｏｎ
［Ａ
］／／ＩＪＣＡＩ
［Ｃ
］
？２０１７


［２９］ＦｅｎｇＪ
，ＨｕａｎｇＭ
，ＺｈａｏＬ
，ｅｔａｌ
．Ｒｅｉｎｆｏｒｃｅｍｅｎｔｌｅａｒｎｉｎｇｆｏｒｒｅｌａｔｉｏｎｃｌａｓｓｉｆｉｃａｔｉｏｎ


ｆｒｏｍｎｏｉｓｙｄａｔａ［Ａ］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅａａａｉｃｏｎｆｅｒｅｎｃｅｏｎａｒｔｉｆｉｃｉａｌ


ｉｎｔｅｌｌｉｇｅｎｃｅ［Ｃ
］，２０１８


［３０］Ｑ
ｉｎＰ
？ＸｕＷ５ＷａｎｇＷＹ．Ｒｏｂｕｓｔｄｉｓｔａｎｔｓｕｐｅｒｖｉｓｉｏｎｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎｖｉａｄｅｅｐ


ｒｅｉｎｆｏｒｃｅｍｅｎｔｌｅａｒｎｉｎｇ［Ｊ］
．ａｒＸｉｖ
ｐｒｅｐｒｉｎｔａｒＸｉｖ：１８０５０９９２７
，２０１８
．


［３
１
］ＺｅｎｇＤ
？ＤａｉＹ
？ＬｉＦ
，ｅｔａｌ
．Ａｄｖｅｒｓａｒ
ｉａｌｌｅａｒｎｉｎｇｆｏｒｄｉｓｔａｎｔｓｕｐｅｒｖｉｓｅｄｒｅｌａｔｉｏｎ


ｅｘｔｒａｃｔ
ｉｏｎ
［Ｊ］
．Ｃｏｍｐｕｔｅｒｓ
，Ｍａｔｅｒｉａｌｓ＆Ｃｏｎｔｉｎｕａ
，２０１８
，５５（
１）
：１２１
－
１３６．


［３２
］ＧｏｏｄｆｅｌｌｏｗＩ．Ｎｉｐｓ２０１６ｔｕｔｏｒ
ｉａｌ
：Ｇｅｎｅｒａｔｉｖｅａｄｖｅｒｓａｒ
ｉａｌｎｅｔｗｏｒｋｓ［Ｊ］
．ａｒＸｉｖ


ｐｒｅｐｒｉｎｔａｒＸｉｖ：１７０１００１６０
，２０１６
．


［３３
］Ｑ
ｉｎＰ
，ＸｕＷ
５ＷａｎｇＷＹ．ＤＳＧＡＮ：Ｇｅｎｅｒａｔｉｖｅａｄｖｅｒｓａｒ
ｉａｌｔｒａｉｎｉｎｇｆｏｒｄｉｓｔａｎｔ


ｓｕｐｅｒｖｉｓｉｏｎｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎ
［Ｊ
］
．ａｒＸｉｖ
ｐｒｅｐｒｉｎｔａｒＸｉｖ：１８０５０９９２９，２０１８
．


［３４
］ＪｉＧ，ＬｉｕＫ
，ＨｅＳ
？ｅｔａｌ．Ｄｉｓｔａｎｔｓｕｐｅｒｖｉｓｉｏｎｆｏｒｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎｗｉｔｈｓｅｎｔｅｎｃｅ
－


ｌｅｖｅｌａｔｔｅｎｔｉｏｎａｎｄｅｎｔｉｔｙｄｅｓｃｒ
ｉｐｔｉｏｎｓ［Ａ
］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＡＡＡＩＣｏｎｆｅｒｅｎｃｅ


ｏｎＡｒｔ
ｉｆ
ｉｃｉａｌＩｎｔｅｌｌｉｇｅｎｃｅ
［Ｃ
］，２０１７


［３５］ＢｏｒｄｅｓＡ
，ＵｓｕｎｉｅｒＮ
，Ｇａｒｃｉａ
－ＤｕｒａｎＡ
？ｅｔａｌ
．Ｔｒａｎｓｌａｔｉｎｇｅｍｂｅｄｄｉｎｇｓｆｏｒｍｏｄｅｌｉｎｇ


ｍｕｌｔｉ
－ｒｅｌａｔ
ｉｏｎａｌｄａｔａ
［Ｊ］
．Ａｄｖａｎｃｅｓｉｎｎｅｕｒａｌｉｎｆｏｒｍａｔｉｏｎ
ｐｒｏｃｅｓｓｉｎｇｓｙｓｔｅｍｓ
，２０１３
，


２６．


［３６
］ＨａｎＸ
，ＹｕＰ
，ＬｉｕＺ
５ｅｔａｌ
．Ｈｉｅｒａｒｃｈｉｃａｌｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎｗｉｔｈｃｏａｒｓｅ
－ｔｏ
－ｆ
ｉｎｅ


ｇｒａｉｎｅｄａｔｔｅｎｔｉｏｎ
［Ａ］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆ
ｔｈｅ２０１８ＣｏｎｆｅｒｅｎｃｅｏｎＥｍｐ
ｉｒｉｃａｌＭｅｔｈｏｄｓ


ｉｎＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ［Ｃ］？２０１８：２２３６
－２２４５
．


［３７
］ＺｈａｎｇＮ
，ＤｅｎｇＳ，ＳｕｎＺ
，
ｅｔａｌ
．Ｌｏｎｇ
－ｔａｉｌｒｅｌａｔｉｏｎｅｘｔｒａｃｔ
ｉｏｎｖｉａｋｎｏｗｌｅｄｇｅ
ｇｒａｐｈ


７３


北京邮电大学工程硕士学位论文


ｅｍｂｅｄｄｉｎｇｓａｎｄ
ｇｒａｐｈｃｏｎｖｏｌｕｔ
ｉｏｎｎｅｔｗｏｒｋｓ
［Ｊ］
．ａｒＸｉｖ
ｐｒｅｐｒ
ｉｎｔａｒＸｉｖ：１９０３０１３０６
，


２０１９．


［３８］ＡｌｔＣ，ＨｉｉｂｎｅｒＭ
，ＨｅｎｎｉｇＬ
．Ｆｉｎｅ
－ｔｕｎｉｎｇｐｒｅ
－ｔｒａｉｎｅｄｔｒａｎｓｆｏｒｍｅｒｌａｎｇｕａｇｅｍｏｄｅｌｓ


ｔｏｄｉｓｔａｎｔｌｙｓｕｐｅｒｖｉｓｅｄｒｅｌａｔｉｏｎｅｘｔｒａｃｔ
ｉｏｎ［Ｊ］
．ａｒＸｉｖｐｒｅｐｒ
ｉｎｔａｒＸｉｖ：１９０６０８６４６
，


２０１９．


［３９
］ＧｏｕＹ
？ＬｅｉＹ
，ＬｉｕＬ，ｅｔａｌ
．Ａｄｙｎａｍｉｃｐａｒａｍｅｔｅｒｅｎｈａｎｃｅｄｎｅｔｗｏｒｋｆｏｒｄｉｓｔａｎｔ


ｓｕｐｅｒｖｉｓｅｄｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎ［Ｊ］
．Ｋｎｏｗｌｅｄｇｅ
－ＢａｓｅｄＳｙｓｔｅｍｓ
，２０２０
，１９７
：１０５９１２
．


［４０
］ＣａｏＹ
？ＫｕａｎｇＪ
？ＧａｏＭ
？ｅｔａｌ
．Ｌｅａｒｎｉｎｇｒｅｌａｔｉｏｎ
ｐｒｏｔｏｔｙｐｅｆ
ｒｏｍｕｎｌａｂｅｌｅｄｔｅｘｔｓｆｏｒ


ｌｏｎｇ
－ｔａｉｌｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎ［Ｊ］
，ＩＥＥＥＴｒａｎｓａｃｔ
ｉｏｎｓｏｎＫｎｏｗｌｅｄｇｅａｎｄＤａｔａ


Ｅｎｇ
ｉｎｅｅｒｉｎｇ，２０２１
．


［４１
］ＤｉｓｔｉａｗａｎＢ
，ＷｅｉｋｕｍＧ
，ＱｉＪ，ｅｔａｌ
．Ｎｅｕｒａｌｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎｆｏｒｋｎｏｗｌｅｄｇｅｂａｓｅ


ｅｎｒｉｃｈｍｅｎｔ［Ａ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ５７ｔｈＡｎｎｕａｌＭｅｅｔｉｎｇｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒ


ＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ［Ｃ
］？２０１９
：２２９
－２４０．


［４２
］ＦａｎＴ，ＷａｎｇＨ
．Ｒｅｓｅａｒｃｈｏｆ
Ｃｈｉｎｅｓｅｉｎｔａｎｇ
ｉｂｌｅｃｕｌｔｕｒａｌｈｅｒｉｔａｇｅｋｎｏｗｌｅｄｇｅ
ｇｒａｐｈ


ｃｏｎｓｔｒｕｃｔ
ｉｏｎａｎｄａｔｔｒ
ｉｂｕｔｅｖａｌｕｅｅｘｔｒａｃｔｉｏｎｗｉｔｈｇｒａｐｈａｔｔｅｎｔｉｏｎｎｅｔｗｏｒｋ［Ｊ］
．


Ｉｎｆｏｒｍａｔ
ｉｏｎＰｒｏｃｅｓｓｉｎｇ＆Ｍａｎａｇｅｍｅｎｔ
，２０２２
，５９（１）
：１０２７５３
．


［４３
］ＳｈｉｎＳ
，ＪｉｎＸ
，ＪｕｎｇＪ
，ｅｔａｌ
．Ｐｒｅｄｉｃａｔｅｃｏｎｓｔｒａｉｎｔｓｂａｓｅｄｑｕｅｓｔｉｏｎａｎｓｗｅｒｉｎｇｏｖｅｒ


ｋｎｏｗｌｅｄｇｅｇｒａｐｈ［Ｊ］
，ＩｎｆｏｒｍａｔｉｏｎＰｒｏｃｅｓｓｉｎｇ＆Ｍａｎａｇｅｍｅｎｔ
，２０１９，５６（３）
：４４５
－


４６２
．


［４４
］ＫｒａｌｌｉｎｇｅｒＭ
？Ｒｏｄｒ
ｉｇｕｅｚ
－ＰｅｎａｇｏｓＣ
，ＴｅｎｄｕｌｋａｒＡ
？ｅｔａｌ
．ＰＬＡＮ２Ｌ
：ａｗｅｂｔｏｏｌｆｏｒ


ｉｎｔｅｇｒａｔｅｄｔｅｘｔｍｉｎｉｎｇａｎｄｌｉｔｅｒａｔｕｒｅ
－ｂａｓｅｄｂｉｏｅｎｔｉｔｙｒｅｌａｔ
ｉｏｎｅｘｔｒａｃｔｉｏｎ
［Ｊ］
．Ｎｕｃｌｅｉｃ


ａｃｉｄｓｒｅｓｅａｒｃｈ
，２００９，３７（ｓｕｐｐ
ｌ
＿２）
：Ｗ１６０
－Ｗ１６５
．


［４５
］ＺｅｌｅｎｋｏＤ
，ＡｏｎｅＣ，ＲｉｃｈａｒｄｅｌｌａＡ．Ｋｅｒ
ｎｅｌｍｅｔｈｏｄｓｆｏｒｒｅｌａｔｉｏｎｅｘｔｒａｃｔ
ｉｏｎ［Ｊ］
．


Ｊｏｕｒ
ｎａｌｏｆ
ｍａｃｈｉｎｅｌｅａｒｎｉｎｇｒｅｓｅａｒｃｈ
，２００３
，
３（Ｆｅｂ）
：１０８３
－
１
１０６
．


［４６
］ＫａｍｂｈａｔｌａＮ．Ｃｏｍｂｉｎｉｎｇ
ｌｅｘｉｃａｌ
，ｓｙｎｔａｃｔｉｃ，ａｎｄｓｅｍａｎｔｉｃｆｅａｔｕｒｅｓｗｉｔｈｍａｘｉｍｕｍ


ｅｎｔｒｏｐｙｍｏｄｅｌｓｆｏｒｉｎｆｏｒｍａｔｉｏｎｅｘｔｒａｃｔｉｏｎ［Ａ］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＡＣＬ


ＩｎｔｅｒａｃｔｉｖｅＰｏｓｔｅｒａｎｄＤｅｍｏｎｓｔｒａｔ
ｉｏｎＳｅｓｓｉｏｎｓ［Ｃ］，２００４
：１７８
－
１８１
．


［４７
］ＢｕｎｅｓｃｕＲＣ
，ＭｏｏｎｅｙＲＪ．Ａｓｈｏｒｔｅｓｔｐａｔｈｄｅｐｅｎｄｅｎｃｙｋｅｒ
ｎｅｌｆｏｒｒｅｌａｔｉｏｎ


ｅｘｔｒａｃｔｉｏｎｆＡ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆ
ｔｈｅｃｏｎｆｅｒｅｎｃｅｏｎｈｕｍａｎｌａｎｇｕａｇｅｔｅｃｈｎｏｌｏｇｙａｎｄ


ｅｍｐ
ｉｒｉｃａｌｍｅｔｈｏｄｓｉｎｎａｔｕｒａｌｌａｎｇｕａｇｅ
ｐｒｏｃｅｓｓｉｎｇ［Ｃ］５２００５
：７２４
－７３１
．


［４８
］ＴａｎｇＨ
，ＳｕｎＸ
，ＪｉｎＢ
，ｅｔａｌ
．ＩｍｐｒｏｖｉｎｇＤｏｃｕｍｅｎｔＲｅｐｒｅｓｅｎｔａｔｉｏｎｓｂｙＧｅｎｅｒａｔｉｎｇ


ＰｓｅｕｄｏＱｕｅｒｙＥｍｂｅｄｄｉｎｇｓｆｏｒＤｅｎｓｅＲｅｔｒ
ｉｅｖａｌ［Ｊ］
，ａｒＸｉｖｐｒｅｐｒ
ｉｎｔ


ａｒＸｉｖ：２１０５０３５９９
，２０２１
．


［４９
］ＷａｎｇＹ
，ＳｕｎＣ
，ＷｕＹ
，ｅｔａｌ
．ＵｎｉＲＥ：Ａｕｎｉｆ
ｉｅｄｌａｂｅｌｓｐａｃｅｆｏｒｅｎｔｉｔｙｒｅｌａｔｉｏｎ


７４


＃＃嫌


ｅｘｔｒａｃｔ
ｉｏｎ
［Ｊ］
．ａｒＸｉｖ
ｐｒｅｐｒｉｎｔａｒＸｉｖ：２１０７０４２９２
，２０２１
．


［５０］ＬｉＺ
，ＺｏｕＹ，ＺｈａｎｇＣ
９ｅｔａｌ
．ＬｅａｒｎｉｎｇＩｍｐ
ｌｉｃｉｔＳｅｎｔｉｍｅｎｔｉｎＡｓｐｅｃｔ
－ｂａｓｅｄ


ＳｅｎｔｉｍｅｎｔＡｎａｌｙｓｉｓｗｉｔｈＳｕｐｅｒｖｉｓｅｄＣｏｎｔｒａｓｔ
ｉｖｅＰｒｅ
－Ｔｒａｉｎｉｎｇ［Ｊ］
．ａｒＸｉｖｐｒｅｐｒｉｎｔ


ａｒＸｉｖ：２１
１
１０２１９４
，２０２１
．


［５１］ＤｅｖｌｉｎＪ
，ＣｈａｎｇＭ
－Ｗ
，ＬｅｅＫ
？ｅｔａｌ．Ｂｅｒｔ：Ｐｒｅ
－ｔｒａｉｎｉｎｇｏｆｄｅｅｐｂｉｄｉｒｅｃｔｉｏｎａｌ


ｔｒａｎｓｆｏｒｍｅｒｓｆｏｒｌａｎｇｕａｇｅｕｎｄｅｒｓｔａｎｄｉｎｇ［Ｊ］
．ａｒＸｉｖｐｒｅｐｒｉｎｔａｒＸｉｖ：１８１００４８０５
，


２０１８
．


［５２］Ｒａｄｆｏｒｄ
人ＮａｒａｓｉｍｈａｎＫ，
ＳａｌｉｍａｎｓＴ
，ｅｔａｌ．Ｉｍｐｒｏｖｉｎｇ
ｌａｎｇｕａｇｅｕｎｄｅｒｓｔａｎｄｉｎｇ


ｂｙｇｅｎｅｒａｔｉｖｅ
ｐｒｅ
－ｔｒａｉｎｉｎｇ［Ｊ］
．２０１８
．


［５３］ＹａｎｇＺ
？ＤａｉＺ
，ＹａｎｇＹ
，ｅｔａｌ
．Ｘｌｎｅｔ：Ｇｅｎｅｒａｌｉｚｅｄａｕｔｏｒｅｇｒｅｓｓｉｖｅｐｒｅｔｒａｉｎｉｎｇｆｏｒ


ｌａｎｇｕａｇｅｕｎｄｅｒｓｔａｎｄｉｎｇ［Ｊ］
．Ａｄｖａｎｃｅｓｉｎｎｅｕｒａｌｉｎｆｏｒｍａｔｉｏｎｐｒｏｃｅｓｓｉｎｇｓｙｓｔｅｍｓ
，


２０１９
，３２
．


［５４
］ＶａｓｗａｎｉＡ
，ＳｈａｚｅｅｒＮ
？ＰａｒｍａｒＮ
？ｅｔａｌ
．Ａｔｔｅｎｔｉｏｎｉｓａｌｌ
ｙｏｕｎｅｅｄ［Ｊ］
．Ａｄｖａｎｃｅｓｉｎ


ｎｅｕｒａｌｉｎｆｏｒｍａｔｉｏｎ
ｐｒｏｃｅｓｓｉｎｇｓｙｓｔｅｍｓ，２０１７
，
３０
，


［５５］ＳｈａｎｇＹ
，ＨｕａｎｇＨ
－Ｙ
？ＭａｏＸ
－Ｌ
，ｅｔａｌ
．Ａｒｅｎｏｉｓｙｓｅｎｔｅｎｃｅｓｕｓｅｌｅｓｓｆｏｒｄｉｓｔａｎｔ


ｓｕｐｅｒｖｉｓｅｄｒｅｌａｔｉｏｎｅｘｆ
ｒａｃｔｉｏｎ？
［Ａ
］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＡＡＡＩＣｏｎｆｅｒｅｎｃｅｏｎ


Ａｒ
ｔｉｆｉｃｉａｌＩｎｔｅｌｌｉｇｅｎｃｅ［Ｃ
］？２０２０：８７９９
－８８０６．


［５６
］ＬｉＹ
，ＬｏｎｇＧ
？ＳｈｅｎＴ
，ｅｔａｌ．Ｓｅｌｆ
－ａｔｔｅｎｔｉｏｎｅｎｈａｎｃｅｄｓｅｌｅｃｔｉｖｅｇａｔｅｗｉｔｈｅｎｔｉｔ
ｙ
－


ａｗａｒｅｅｍｂｅｄｄｉｎｇｆｏｒｄｉｓｔａｎｔｌｙｓｕｐｅｒｖｉｓｅｄｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎ
［Ａ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆ


ｔｈｅＡＡＡＩｃｏｎｆｅｒｅｎｃｅｏｎａｒｔｉｆ
ｉｃｉａｌｉｎｔｅｌｌｉｇｅｎｃｅ［Ｃ
］５２０２０
：８２６９
－８２７６．


［５７
］ＤｅｎｇＸ
？ＳｕｎＨ．Ｌｅｖｅｒａｇ
ｉｎｇ２
－ｈｏｐｄｉｓｔａｎｔｓｕｐｅｒｖｉｓｉｏｎｆ
ｒｏｍｔａｂｌｅｅｎｔｉｔｙｐａｉｒｓｆｏｒ


ｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎ
［Ｊ
］
．ａｒＸｉｖ
ｐｒｅｐ
ｒｉｎｔａｒＸｉｖ：１９０９０６００７
，２０１９
－


［５８
］ＹｕａｎＹ
，ＬｉｕＬ
，ＴａｎｇＳ，ｅｔａｌ
．Ｃｒｏｓｓ
－ｒｅｌａｔ
ｉｏｎｃｒｏｓｓ
－ｂａｇａｔｔｅｎｔｉｏｎｆｏｒｄｉｓｔａｎｔｌｙ
－


ｓｕｐｅｒｖｉｓｅｄｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎ［Ａ
］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＡＡＡＩｃｏｎｆｅｒｅｎｃｅｏｎ


ａｒｔｉｆｉｃｉａｌｉｎｔｅｌｌｉｇｅｎｃｅ
［Ｃ
］，２０１９：４１９
－４２６
．


［５９］黄兆玮，常亮，宾辰忠，
ｅｔａｌ．基于
ＧＲＵ和注意力机制的远程监督关系抽取


［Ｊ］
．计算机应用研究，
２０１９，３６（１０）
：２９３０
－２９３３
．


［６０］ＭｒｉｎｉＫ
，ＤｅｍｏｎｃｏｕｒｔＦ
，ＢｕｉＴ
？ｅｔａｌ
．Ｒｅｔｈｉｎｋｉｎｇｓｅｌｆ
－ａｔｔｅｎｔｉｏｎ：Ａｎｉｎｔｅｒｐｒｅｔａｂｌｅ


ｓｅｌｆ
－ａｔｔｅｎｔｉｖｅｅｎｃｏｄｅｒ
－ｄｅｃｏｄｅｒ
ｐａｒｓｅｒ［Ｊ］
．２０１９．


［６１
］ＬｉＢ
？ＷａｎｇＹ
，ＣｈｅＴ
５ｅｔａｉ
，Ｒｅｔ
ｉｉｉｎｋｉｎｇｄｉｓｔｒ
ｉｂｕｔｉｏｎａｌｍａｔｃｈｉｎｇｂａｓｅｄｄｏｍａｉｎ


ａｄａｐ
ｔａｔｉｏｎ
［Ｊ］
．ａｒＸｉｖ
ｐｒｅｐｒ
ｉｎｔａｒＸｉｖ：２００６１３３５２
，２０２０
．


［６２
］ＪａｔＳ，ＫｈａｎｄｅｌｗａｌＳ
？ＴａｌｕｋｄａｒＰ．Ｉｍｐｒｏｖｉｎｇｄｉｓｔａｎｔｌｙｓｕｐｅｒｖｉｓｅｄｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎ


ｕｓｉｎｇｗｏｒｄａｎｄｅｎｔｉｔｙｂａｓｅｄａｔｔｅｎｔｉｏｎ［Ｊ］
．ａｒＸｉｖ
ｐｒｅｐｒｉｎｔａｒＸｉｖ：１８０４０６９８７
，２０１８
．


［６３］ＰａｓｚｋｅＡ
，ＧｒｏｓｓＳ
，ＭａｓｓａＦ
？ｅｔａＬＰｙｔｏｒｃｈ：Ａｎｉｍｐｅｒａｔｉｖｅｓｔｙ
ｌｅ，ｈｉｇｈ
－
ｐｅｒｆｏｒｍａｎｃｅ


７５


北京邮电大学工程硕士学位论文


ｄｅｅｐ
ｌｅａｒ
ｎｉｎｇ
ｌｉｂｒａｒｙ［Ｊ］
．Ａｄｖａｎｃｅｓｉｎｎｅｕｒａｌｉｎｆｏｎｎａｔｉｏｎ
ｐｒｏｃｅｓｓｉｎｇｓｙｓｔｅｍｓ
，２０１９
，


３２．


［６４
］Ｈｏｆ
ｉ＆ｎａｎｎＲ
，ＺｈａｎｇＣ
？ＬｉｎｇＸ
，ｅｔａｌ
．Ｋｎｏｗｌｅｄｇｅ
－ｂａｓｅｄｗｅａｋｓｕｐｅｒｖｉｓｉｏｎｆｏｒ


ｉｎｆｏｒｍａｔｉｏｎｅｘｔｒａｃｔｉｏｎｏｆｏｖｅｒｌａｐｐｉｎｇｒｅｌａｔｉｏｎｓ
［Ａ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ４９ｔｈ


ａｎｎｕａｌｍｅｅｔｉｎｇｏｆｔｈｅａｓｓｏｃｉａｔｉｏｎｆｏｒｃｏｍｐｕｔａｔｉｏｎａｌｌｉｎｇｕｉｓｔｉｃｓ
：ｈｕｍａｎｌａｎｇｕａｇｅ


ｔｅｃｈｎｏｌｏｇ
ｉｅｓ［Ｃ
］，２０１
１
：５４１
－５５０
．


［６５］ＳｕｒｄｅａｎｕＭ
？ＴｉｂｓｈｉｒａｎｉＪ
，ＮａｌｌａｐａｔｉＲ
，ｅｔａｌ
．Ｍｕｌｔｉ
－
ｉｎｓｔａｎｃｅｍｕｌｔｉ
－
ｌａｂｅｌｌｅａｒｎｉｎｇ


ｆｏｒｒｅｌａｔ
ｉｏｎｅｘｔｒａｃｔｉｏｎ［Ａ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆ
ｄｉｅ２０１２
ｊｏｉｎｔｃｏｎｆｅｒｅｎｃｅｏｎｅｍｐ
ｉｒ
ｉｃａｌ


ｍｅｔｈｏｄｓｉｎｎａｔｕｒａｌｌａｎｇｕａｇｅｐｒｏｃｅｓｓｉｎｇａｎｄｃｏｍｐｕｔａｔｉｏｎａｌｎａｔｕｒａｌｌａｎｇｕａｇｅ


ｌｅａｍｉｎｇ［Ｃ
］
？２０１２
：４５５
－４６５
．


７６


