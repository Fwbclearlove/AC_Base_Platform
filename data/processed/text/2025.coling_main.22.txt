Proceedings of the 31st International Conference on Computational Linguistics, pages 313–323
January 19–24, 2025. ©2025 Association for Computational Linguistics
313
Multimodal Aspect-Based Sentiment Analysis under Conditional Relation
Xinjing Liu1, Ruifan Li1,3,4*, Shuqin Ye1, Guangwei Zhang2,3, Xiaojie Wang1,3,4
1School of Artificial Intelligence, Beijing University of Posts and Telecommunications
2School of Computer Science, Beijing University of Posts and Telecommunications
3Engineering Research Center of Information Networks, Ministry of Education, China
4Key Laboratory of Interactive Technology and Experience System,
Ministry of Culture and Tourism, China
Correspondence: Ruifan Li {liuxj_ai, rfli, shuqinye, gwzhang, xjwang}@bupt.edu.cn
Abstract
Multimodal Aspect-Based Sentiment Analysis
(MABSA) aims to extract aspect terms from
text-image pairs and identify their sentiments.
Previous methods are based on the premise
that the image contains the objects referred
by the aspects within the text. However, this
condition cannot always be met, resulting in
a suboptimal performance. In this paper, we
propose COnditional Relation based Sentiment
Analysis framework (CORSA). Specifically, we
design a conditional relation detector (CRD) to
mitigate the impact of the unmet conditional
image. Moreover, we design a visual object
localizer (VOL) to locate the exact condition-
related visual regions associated with the as-
pects.
With CRD and VOL, our CORSA
framework takes a multi-task form. In addi-
tion, to effectively learn CORSA we conduct
two types of annotations.
One is the con-
ditional relation using a pretrained referring
expression comprehension model; the other
is the bounding boxes of visual objects by
a pretrained object detection model. Experi-
ments on our built C-MABSA dataset show
that CORSA consistently outperforms existing
methods. The code and data are available at
https://github.com/Liuxj-Anya/CORSA.
1
Introduction
In recent years, fine-grained Multimodal Aspect-
Based Sentiment Analysis (MABSA) (Zhao et al.,
2024b) has received great attention, due to its sig-
nificant applications in analyzing social media sen-
timents. MABSA includes three key subtasks: Mul-
timodal Aspect Term Extraction (MATE), Multi-
modal Aspect-oriented Sentiment Classification
(MASC), and Joint Multimodal Aspects of Sen-
timent Analysis (JMASA). Given a text-image pair,
MATE (Wu et al., 2020a; Li et al., 2023; Guo et al.,
2023) aims to extract all the aspect terms men-
tioned in the text; MASC (Yu and Jiang, 2019;
Feng et al., 2024) aims to determine the sentiment
Output
(President Obama, POS)
(LeBron James, NEU)
Image
Text
Conditional
Relation
So are the actions President
Obama is taking to tackle it
LeBron James to Produce
NBA Documentary
Relevance
Irrelevance
a)
b)
Figure 1: JMASA aims to extract the aspects and iden-
tify their corresponding sentiments from a text-image
pair. a) The conditional relation for the aspect (i.e., Pres-
ident Obama) and the image is relevant, and the visual
information in the box (in red) could benefit for identify-
ing the sentiment. b) The irrelevant visual information
would distract the sentiment prediction.
towards each given aspect term. JMASA (Ju et al.,
2021; Yu et al., 2022; Ling et al., 2022; Liu et al.,
2024b) aims to jointly predict the aspect terms and
the corresponding sentiments. In Figure 1, JMASA
produces two aspect-sentiment pairs, i.e., (Presi-
dent Obama, POS) and (LeBron James, NEU).
For fine-grained MABSA task, it mainly in-
volves two challenges. One is the semantic com-
plexity. The given sentences often contain multiple
aspects, each referring to different objects in the
image. The other is the sentimental complexity.
These aspects and image regions could carry dif-
ferent sentiments. To this end, recently proposed
methods focus on aligning cross-modal text-image
precisely. For example, Ling et al. (2022) propose
a task-specific vision-language pretraining frame-
work to solve the cross-modal alignment. Zhou
et al. (2023) propose an aspect-oriented method to
detect aspect-relevant semantic and sentiment infor-
mation. Very recently, Xiao et al. (2024) propose to
utilize the aesthetic information of the images for
textual visual alignment and the sentiment-aware
image aesthetic assessment.
314
However, all of previous methods are designed
based on the premise that the image always con-
tains the objects referred by the aspects in the text.
Unfortunately, this condition sometimes cannot be
met, leading to aligning cross-modal text-image in-
accurately. The text and image could be irrelevant,
especially in social media domain. As shown in
Figure 1b), the image does not contain any infor-
mation about the aspect, i.e., LeBron James which
negatively impacts the model’s performance. In
contrast, the image-text in Figure 1a) satisfies the
condition. The visual information contributes to
the sentiment analysis.
To mitigate the negative impact of unqualified
image-text pairs, in this paper, we propose COn-
ditional Relation based Sentiment Analysis frame-
work (i.e., CORSA) for MABSA. Firstly, we per-
form two types of annotations. Specifically, we
leverage a pretrained Referring Expression Com-
prehension (REC) (Yan et al., 2023) model to anno-
tate the conditional relation between an image and
aspects. Moreover, a pretrained object detection
model is employed to annotate visual objects (i.e.,
bounding boxes and categories) on two popular
datasets.
Secondly, we propose two key modules in our
CORSA framework. Conditional Relation Detector
(CRD) is designed to filter out visual information
irrelevant to the aspects considering their compli-
ance with the condition. Furthermore, to precisely
locate condition-related regions with the aspects,
we propose Visual Object Localizer (VOL). VOL
locates visual objects and uses an attention mecha-
nism to align visual objects with aspects.
Thirdly, we design Multimodal Sentiment An-
alyzer (MSA) based on an encoder-decoder mul-
timodal Bidirectional and Auto-Regressive Trans-
formers (multimodal BART) (Ling et al., 2022) to
obtain the aspect-sentiment pairs.
Our contributions are summarized as follows:
• We propose a multi-task framework, CORSA
for MABSA task, involving a detector and a
localizer. CRD mitigates the impact of the un-
met conditional image-text; VOL locates the
exact condition-related visual regions referred
by the aspects.
• We perform two types of annotations, con-
ditional relation and bounding boxes on two
benchmark datasets. Annotations are automat-
ically performed using two pretrained models,
respectively.
• We conduct extensive experiments on two
benchmark datasets.
The experimental re-
sults show the effectiveness of our proposed
CORSA model.
2
Problem Formulation
We formulate MABSA as a multi-task framework
composed of a tuple extraction, a binary classifi-
cation, and a coordinate regression. Given a tweet
containing an image V and a sentence S, we aim to
obtain a set for all aspects and the sentimental po-
larities. These are denoted as ˆY = {(ak, sk)}K
k=1,
where ak is the k-th aspect and sk is its sentiment.
In addition, for each sample, we determine a con-
ditional relation ˆr and further detect the location
of visual objects, generating the bounding boxes
{ˆbm}B
m=1 and the categories {ˆcj}C
j=1.
3
Methodology
3.1
Data Generation for C-MABSA
We construct datasets, i.e., C-MABSA with con-
ditional relations for MABSA task. Specifically,
we perform two types of annotations automati-
cally on two popular datasets, i.e., TWITTER-15
and TWITTER-17. Firstly, we use a pretrained
multi-task universal instance perception model,
UNINEXT (Yan et al., 2023) to annotate the con-
ditional relation whether the image contains vi-
sual objects referred by the aspects. In UNINEXT,
the REC function is adopted.
In our settings,
UNINEXT takes an aspect and the corresponding
image as input and then generates the probability
that the image contains the aspect. For samples
with more than one aspect, we average the multiple
output probabilities. For TWITTER-15, when this
probability exceeds a threshold τ1 of 0.7, we anno-
tate the conditional relation as relevant; otherwise
as irrelevant. For TWITTER-17, due to its image-
aspect pairs being more relevant, the threshold τ1
is set as 0.5 to keep more visual information.
Secondly, we use an object detection model
YOLOv8 (Jocher et al., 2023) pretrained on
MSCOCO (Lin et al., 2014) to annotate the visual
objects in the image, including the bounding boxes
and categories. For the object category, we define
three types, including person, object, and back-
ground. Considering the two benchmark datasets
often contain people, food, and other objects, we
define the person and other categories. For images
without any objects, we define the background cat-
egory. Thus, YOLOv8 takes an image as input
315
So are the actions
President Obama is
taking to tackle it
Text Encoder
Image Encoder
CRD
VOL
Cross-Attention
Q  K  V
Self-Attention
Max-Pooling
HVi
HT
H
A
T
Ĥ
Embedding
Word Embedding
Condition-Related Visual Features
Condition-Aligned Visual Features
Conv
Conv
Conv
<BOS>
President
Obama
POS
President
POS
<EOS>
Multi-modal BART Encoder
Multi-modal BART Decoder
Obama
Cross-Attention
K  V  Q
σ
1-ɑ
ɑ
H
'''
Vi
H
'''
Vi
Visual Features
Textual Features
Relevance Score
Relation
Detection Loss
Object
Localization Loss
MSA
CRD
VOL
Figure 2: The framework of our proposed CORSA. Unimodal features are extracted using image and text encoder,
respectively. Note that three-scale visual features are used. Then, the conditional relation detector (CRD) uses
image and text features to capture condition-related visual features. The visual object localizer (VOL) utilizes
condition-related visual features and candidate aspects feature to capture condition-aligned visual features. Finally,
the multi-modal BART in multimodal sentiment analysis (MSA) is adopted to extract aspect-sentiment pairs.
and generates the detected bounding boxes with
its category probability. If the probability exceeds
the threshold τ2 of 0.8, we annotate the image as
person or object; otherwise, we annotate it as a
background category. Here, the bounding box is
taken as the image size.
3.2
Our Proposed Model
Figure 2 provides an overview of CORSA frame-
work. Conditional relation detector (CRD) is to mit-
igate the impact of the irrelevant condition. Visual
object localizer (VOL) is to locate exact condition-
related regions to specific aspects. Finally, multi-
modal sentiment analyzer (MSA) built on the multi-
modal BART encodes multimodal information to
extract aspects and the corresponding sentiments.
Visual and Textual Encoders. Our multimodal
feature encoder comprises two encoders of vision
and language. The textual encoder uses a pretrained
BART (Lewis et al., 2020). We first obtain the
initial word embeddings E. Then, BART gener-
ates the contextualized representation HT ∈Rs×d.
Here, s is the length of the sequence S. The visual
encoder uses the backbone of YOLOv8 (Jocher
et al., 2023) to obtain multi-scale features in an
image. Thus, we obtain the visual features in three
scales, i.e., HV1 ∈R49×2048, HV2 ∈R196×1536,
and HV3 ∈R784×1024. The three features could
help the model detect multi-scale objects.
Conditional Relation Detector (CRD). The
goal of our CRD is to detect the relevance of image-
aspects, thus filtering out irrelevant visual informa-
tion to the aspects in a given image. Specifically,
we first use the self-attention to capture the inter-
actions between different image patches. In other
word, we apply three self-attention layers for each
of the three scales of visual features as follows,
H′
Vi = AttSLF
i
(HViWVi) , i ∈{1, 2, 3}
(1)
316
where WVi is the learnable weight and the index i
is used for the three scales.
Then, we apply cross-modal attention to model
the interaction between the text and the image. We
design three cross-modal attention layers. Here, we
regard image features H′
Vi as queries, and the text
features HT as keys and values. The formulation
is given as follows,
H′′
Vi = AttCM
i
 H′
Vi, HT , HT

, i ∈{1, 2, 3} (2)
where H′′
Vi is the generated text-based image fea-
tures. Next, we apply max-pooling on the fea-
ture H′′
Vi, obtaining the most salient feature, Hmax
Vi
.
Then, based on the most salient feature, we use a
softmax function to detect the conditional relation,
ˆri = Softmax
 WRiHmax
Vi
+ bRi

, i ∈{1, 2, 3}
(3)
where WRi is the learnable weight.
To learn CRD, we use the cross-entropy loss to
optimize the binary classification task, i.e.,
LCRD = −1
N
N
X
n=1
3
X
i=1
log ˆri,
(4)
where N is the total number of training samples.
Finally, we filter the visual features most relevant
to aspects, i.e., condition-related visual features,
used in VOL. Specifically, the probability ˆri in Eq.
(3) indicates the relevant degree of an image-text
pair. We use it to construct a visual filter matrix Gi,
where each entry equals to the probability ˆri. Thus,
we obtain the filtered image feature H′′′
Vi, i.e.,
H′′′
Vi = Gi ⊙H′′
Vi, i ∈{1, 2, 3}
(5)
where ⊙denotes the element-wise multiplication.
Visual Object Localizer (VOL). VOL aims to
further enhance CRD and then localize the exact
condition-related regions to the aspects. Specifi-
cally, we incorporate an object detector to obtain
the bounding box of the visual object and its cat-
egory. Since multi-scale features are adopted, we
apply three object detection headers. Following
YOLOv8 (Jocher et al., 2023), we detect visual
objects with the previously filtered feature H′′′
Vi as
input. The formulation is given as follows,
Oi = DET(H′′′
Vi), i ∈{1, 2, 3}
(6)
where O1 ∈R49×24, O2 ∈R196×24, and O3 ∈
R784×24. We predict three bounding boxes at each
scale. The column size of Oi equals [3×(4+1+3)].
Here, we have four bounding box offsets ˆb, one
object’s prediction ˆo, and three class’s prediction ˆc.
We use two losses to optimize the coordinate re-
gression task. The regression prediction loss is the
distance between the predictive boxes and the real
boxes. It includes the MSE of boxes’ coordinates,
LLOC = 1
N
N
X
n=1
3
X
i=1

bi
n −ˆbi
n
2
.
(7)
The classification loss is used to predict the class
of objects in the boxes. The loss takes the form of
cross-entropy loss,
LCLS = −1
N
N
X
n=1
3
X
i=1
 ci
n log ˆci
n + log ˆoi
n

, (8)
where cn is annotated visual object’s categories and
ˆcn is the corresponding prediction.
Then, to obtain the visual object features associ-
ated with aspects, we utilize cross-model attention
to align visual objects and aspects. Specifically, we
use Spacy 1 to extract noun phrases as candidate
aspects and obtain their features HA
T . The textual
features HA
T = {hA
1 , hA
2 , ..., hA
l } is obtained from
the hidden state HT of the BART encoder, where
l is the number of noun phrases. We use features
of all candidate aspects HA
T as key-value pairs and
visual features H′′′
Vi as queries. The process for the
aligned visual features is formulated as follows,
HA
Vi = AttCM
i
(H′′′
Vi, HA
T , HA
T ), i ∈{1, 2, 3} (9)
Finally, we use gating mechanism to concatenate
two visual features, i.e., H′′′
Vi and HA
Vi as follows,



αj = σ

Wα[W ′′′h′′′
vj ⊕W AhA
vj] + bα

ˆhj = αjh′′′
vj + (1 −αj)hA
vj
(10)
where h′′′
vj and hA
vj are j-th column of H′′′
Vi
and HA
Vi, respectively.
In addition, Wα, W ′′′
and W A are learnable weights.
Thus, we ob-
tain the condition-aligned visual features ˆHVi =
{ ˆh1, ..., ˆhj, ..., ˆ
hm}, i ∈{1, 2, 3}. The visual fea-
ture ˆHVi is relevant to aspects and contain the ac-
curate aspect’s visual information.
Multi-modal Sentiment Analyzer (MSA). The
goal of MSA is to encode multimodal inputs while
decoding aspects and their sentiment. Specifically,
1https://spacy.io/
317
TWITTER-15
TWITTER-17
Train
Dev
Test
Train
Dev
Test
Positive
928
303
317
1508
515
493
Neutral
1883
670
607
1638
517
573
Negative
368
149
113
416
144
168
Sentence
2101
727
674
1746
577
587
Single aspect
1302
441
416
586
202
188
Multiple aspects
799
286
258
1160
375
399
Table 1: Statistics on two benchmark datasets.
we first concatenate the three scales of visual fea-
tures, i.e., ˆH′ = ˆHV1 ⊕WV2 ˆHV2 ⊕WV3 ˆHV3, where
WV2 ∈R49×196 and WV3 ∈R49×784 are learnable
weights for obtaining an identical dimension. Then,
we use a linear layer to map the concatenated fea-
ture to 49-dimension, obtaining a visual feature
ˆH′′. Second, we use the multimodal BART (Ling
et al., 2022) encoder-decoder to predict the token
probability distribution yt as follows,





ˆH′′′ = Encoder( ˆH′′ ⊕E)
ht = Decoder( ˆH′′′; Y<t)
yt = Softmax(Wtht + bt)
(11)
in which, E is the word embedding and Y<t is the
previous time-step decoder outputs. yt is predicted
aspects and sentiment.
To learn MSA, we use the cross-entropy loss to
optimize the tuple extraction task, i.e.,
LMSA = −1
N
N
X
n=1
T
X
t=1
log yt,
(12)
where T is the length of Y .
Finally, to train our CORSA, we use a joint
framework by optimize the following loss,
L = λDLCRD +λL(LLOC +LCLS)+LMSA, (13)
where λD and λL are two hyper-parameters.
4
Experiment and Analysis
4.1
Experimental settings
Datasets. We use two benchmark datasets, Twitter-
15 and Twitter-17 (Yu and Jiang, 2019) for all our
experimental evaluations. The statistics of these
two datasets are summarized in Table 1. Specifi-
cally, Twitter15 has fewer aspects for one sample,
and one aspect accounts for 61.6%. In contrast,
Twitter17 has more aspects, and multiple aspect
accounts for 66.7%. Thus, we could evaluate the
TWITTER-15
TWITTER-17
Method
P
R
F1
P
R
F1
UMT-collapse (Yu et al., 2020)
61.0
60.4
61.6
60.8
60.0
61.7
OSCGA-collapse (Wu et al., 2020b)
63.1
63.7
63.2
63.5
63.5
63.5
RpBERT-collapse (Sun et al., 2021)
49.3
46.9
48.0
57.0
55.4
56.2
JML (Ju et al., 2021)
65.0
63.2
64.1
66.5
65.5
66.0
VLP (Ling et al., 2022)
65.1
68.3
66.6
66.9
69.2
68.0
CMMT (Yang et al., 2022b)
64.6
68.7
66.5
67.6
69.4
68.5
MOCOLNet (Mu et al., 2023)
66.3
67.9
67.1
67.3
68.7
68.0
AoM (Zhou et al., 2023)
67.9
69.3
68.6
68.4
71.0
69.7
M2DF (Zhao et al., 2023)
67.0
67.3
67.6
67.9
68.8
68.3
Atlantis (Xiao et al., 2024)
65.6
69.2
67.3
68.6
70.3
69.4
MCPL-VLP (Zhang et al., 2024)
67.2
69.2
68.2
69.0
69.4
69.2
RNG (Liu et al., 2024b)
67.8
69.5
68.6
69.5
71.0
70.2
CORSA (Ours)
69.0
70.8
69.9
70.1
71.0
70.6
Table 2: Performance comparison on JMASA task.
model’s performance when dealing with various
settings.
Evaluation Metrics. For JMASA and MATE
tasks, we evaluate the performance of these models
by Micro-F1 score (F1), Precision (P), and Recall
(R). In addition, following previous works such as
(Yu and Jiang, 2019; Zhou et al., 2023), we use
Accuracy (Acc) and F1 on MASC task.
Implementation Details. We use AdamW op-
timizer (Loshchilov and Hutter, 2017) during the
training of our CORSA. Specifically, we set the
batch size to 32 and the training epoch to 50.
The learning rate is set to 7e-5. The two hyper-
parameters λD and λL are set to 1.0 and 0.5.
Baselines. We compare three groups of base-
lines. They correspond to the main task JMASA
and two auxiliary ones, i.e., MATE and MASC.
The detailed comments are given in Section 5.
4.2
Main Results
We show the performance of CORSA with state-
of-the-art baselines on benchmark datasets. The
results of the main task, JMASA and the other two
tasks, MATE and MASC are reported as follows.
On JMASA. The results for JMASA task are
reported in Table 2. Our CORSA model outper-
forms all multimodal methods on all metrics on
Twitter-15 and Twitter-17 datasets. Specifically,
our model achieves the improvement of 1.3% and
0.4% with respect to F1 in contrast with the second
best model, i.e., RNG, on these two datasets. The
results demonstrate the effectiveness of detecting
unmet conditional information and localizing exact
condition-related regions from the image.
On MATE. As shown in Table 3, our model
performs the best in Twitter-15 by 0.1%, which is
higher than the second best AoM on F1. The per-
formance of CMMT in Twitter-17 is 0.3% higher
than ours. This may due to Twitter-17 containing
318
TWITTER-15
TWITTER-17
Method
P
R
F1
P
R
F1
RAN (Wu et al., 2020a)
80.5
81.5
81.0
90.7
90.7
90.0
UMT (Yu et al., 2020)
77.8
81.7
79.7
86.7
86.8
86.7
OSCGA (Wu et al., 2020b)
81.7
82.1
81.9
90.2
90.7
90.4
JML (Ju et al., 2021)
83.6
81.2
82.4
92.0
90.7
91.4
VLP (Ling et al., 2022)
83.6
87.9
85.7
90.8
92.6
91.7
CMMT (Yang et al., 2022b)
83.9
88.1
85.9
92.2
93.9
93.1
MNER-QG (Jia et al., 2023)
77.4
72.1
74.7
88.2
85.6
86.9
PGIM (Li et al., 2023)
79.2
79.4
79.3
90.8
92.0
91.4
MGICL (Guo et al., 2023)
80.3
80.0
80.1
91.0
90.6
90.9
Prompt-Me-Up (Hu et al., 2023)
80.0
80.9
80.5
91.7
91.3
91.6
M2DF (Zhao et al., 2023)
85.0
87.2
86.1
91.2
93.0
92.2
AoM (Zhou et al., 2023)
84.6
87.9
86.2
91.8
92.8
92.3
Atlantis (Xiao et al., 2024)
84.2
87.7
86.1
91.8
93.2
92.7
MCPL-VLP (Zhang et al., 2024)
84.8
87.4
86.1
91.9
92.4
92.2
CORSA (Ours)
85.1
87.6
86.3
92.6
93.0
92.8
Table 3: Performance comparison on MATE task.
TWITTER-15
TWITTER-17
Method
ACC
F1
ACC
F1
TomBERT (Yu and Jiang, 2019)
77.2
71.8
70.5
68.0
CapTrBERT (Khan and Fu, 2021)
78.0
73.2
72.3
70.2
FITL (Yang et al., 2022a)
78.7
74.7
73.8
73.0
VEMP (Yang and Li, 2023)
78.88
75.09
73.01
72.42
JML (Ju et al., 2021)
78.7
-
72.7
-
VLP (Ling et al., 2022)
78.6
73.8
73.8
71.8
CMMT (Yang et al., 2022b)
77.9
-
73.8
-
SeqCSG (Wang et al., 2023)
79.3
75.0
74.6
73.2
ARFN (Xiao et al., 2023)
78.50
73.70
70.58
68.43
CoolNet (Huang et al., 2023)
79.92
75.28
71.64
69.58
M2DF (Zhao et al., 2023)
78.9
74.8
74.3
73.0
AoM (Zhou et al., 2023)
80.2
75.9
76.4
75.0
A2II (Feng et al., 2024)
79.5
75.1
74.3
72.3
Atlantis (Xiao et al., 2024)
79.3
-
74.2
-
AMIFN (Yang et al., 2024)
78.69
75.50
72.29
70.21
MCPL-VLP (Zhang et al., 2024)
79.3
74.9
75.1
74.0
CORSA (Our)
81.1
77.7
76.6
74.5
Table 4: Performance comparison on MASC task.
a larger sample of multiple aspects, as shown in
Table 1. Our approach to annotate the conditional
relation by averaging the probabilities undermines
the conditional relation detection and the sentiment
prediction, when dealing with multiple aspects.
On MASC. Table 4 shows the performance of
MASC. Our model achieves the best results with
the improvement of 0.9% on Accuracy, 1.8% on
F1 score on Twitter-15 and 0.2% on Accuracy on
Twitter-17. AoM’s F1 on Twitter-17 is 0.5% higher
than that of our CORSA. In fact, the reason behind
is the same as on MATE task. In other words, the
inaccuracies in our annotation method is imperfect.
4.3
Ablation Study
In this section, we conduct ablation studies on
Twitter-15 and Twitter-17 of the JMASA task.
To verify the effectiveness of CRD and VOL
in CORSA, we perform the ablation studies. The
results are reported in Table 5. First, we remove
CRD. The obtained F1 scores decline by 0.9% on
TWITTER-15
TWITTER-17
Method
P
R
F1
P
R
F1
Full CORSA
69.0
70.8
69.9
70.1
71.0
70.6
w/o CRD
68.4
70.0
69.0
69.7
69.6
69.6
w/o VOL
68.1
70.5
69.3
69.4
70.7
70.0
w/o CRD+VOL
67.1
69.1
68.0
68.5
69.7
69.1
Table 5: The performance comparison of our full model
and its variants.
TWITTER-15
TWITTER-17
Scale
P
R
F1
P
R
F1
Full CORSA
69.0
70.8
69.9
70.1
71.0
70.6
L
66.0
69.7
67.8
68.4
70.0
69.2
M
66.7
70.4
68.5
69.7
69.3
69.5
S
67.6
70.7
69.1
70.0
69.8
69.9
L+M
68.0
69.7
68.9
69.2
70.3
69.7
L+S
69.2
69.3
69.2
68.7
69.1
70.1
M+S
69.6
69.5
69.6
69.4
71.2
70.3
Table 6: Ablation results on scales of the visual encoder.
Twitter-15 and 1.0% on Twitter-17. It shows that
the damage inflicted by unmet conditional image
information and the necessary elimination of such
information. Second, we remove VOL. We observe
that the model’s performance on Twitter-15 and
Twitter-17 has also declined notably. The results
demonstrate the importance of locating condition-
related regions. Meanwhile, we notice that the
performance decline of removing CRD is more re-
markable than that of removing VOL. This demon-
strates the significance of the processing sequence,
i.e., first eliminating unmet conditional image in-
formation and then localizing the exact condition-
related regions. Third, we remove both CRD and
VOL. The decrease in the model’s performance
shows their contributions to learning the most valu-
able information.
To show the advantage of using multi-scale fea-
tures, we perform ablations. Specifically, we com-
pare the performances of using only small scale
(i.e., HV1), middle scale (i.e, HV2), and large scale
(i.e., HV3), and a combination of any two scales,
respectively. The experimental results are reported
in Table 6. This result shows the use of multi-scale
features facilitates the model’s prediction. In ad-
dition, small-scale features in our stetting is more
beneficial to the model.
To show the effect of two hyper-parameters λD
and λL in our loss, we perform experiments as
follows. To be brief, we fix the parameter λD =
1.0 to evaluate the parameter λL. On the other
319
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
value
68.8
69.0
69.2
69.4
69.6
69.8
F1-score. (%)
Twitter 15
D
L
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
value
69.25
69.50
69.75
70.00
70.25
70.50
F1-score. (%)
Twitter 17
D
L
Figure 3: F1-score against two hyper-parameters λD and λL on two benchmark datasets for JMASA task.
Threshold
TWITTER-15
TWITTER-17
τ1
P
R
F1
P
R
F1
0.4
67.5
69.7
68.6
68.8
70.8
69.8
0.5
67.6
70.2
68.9
70.1
71.0
70.6
0.6
69.6
69.3
69.4
69.6
70.6
70.1
0.7
69.0
70.8
69.9
69.0
69.8
69.4
0.8
68.8
70.5
69.6
68.9
69.5
69.2
Table 7: The performance comparison of different anno-
tating threshold τ1 for CRD.
Threshold
TWITTER-15
TWITTER-17
τ2
P
R
F1
P
R
F1
0.6
68.7
69.5
69.1
69.6
70.2
69.9
0.7
69.8
69.0
69.5
69.2
70.9
70.1
0.8
69.0
70.8
69.9
70.1
71.0
70.6
Table 8: The performance comparison of different anno-
tating thresholds τ2 for VOL.
hand, we fix the parameter λL = 0.5 to evaluate
the parameter λD. The experimental results are
shown in Figure 3. Therefore, we suppose that our
CORSA model is optimal when the two parameters
λD equals 1.0 and λL equals 0.5, respectively.
To show the impact of various thresholds in our
annotation data generation, we perform the follow-
ing experiments. Specifically, we first evaluate the
annotating conditional relation with thresholds τ1
of 0.4, 0.5, 0.6, 0.7, and 0.8. The results are shown
in Table 7. The thresholds τ1 of 0.7 and 0.5 are
chosen for Twitter-15 and Twitter-17, respectively.
Compared to those in Twitter-15, the aspects in
Twitter-17 have better relevance to the correspond-
ing images. Secondly, we evaluate the annotating
visual objects with various thresholds τ2 of 0.6, 0.7
and 0.8. The results are shown in Table 8. There-
fore, the threshold τ2 of 0.8 is chosen.
TWITTER-15
TWITTER-17
Method
P
R
F1
P
R
F1
ChatGPT 3.5
54.3
53.6
55.0
58.2
57.6
58.8
Llama 2
51.4
50.9
51.9
55.8
55.6
56.1
CORSA (Ours)
69.0
70.8
69.9
70.1
71.0
70.6
Table 9: The performance comparison with LLMs on
JMASA task.
4.4
Comparison with LLMs
Recently,
large-scale language models have
evolved extremely rapidly and have advanced lan-
guage understanding and generation skills in a va-
riety of NLP tasks. Therefore, we conduct exper-
iments on LLMs and MLLMs to compare with
CORSA. Firstly, we compare our model with Chat-
GPT 3.5 (OpenAI, 2023) and Llama 2 (Touvron
et al., 2023) on the JMASA task. Here, we use only
text as input, since they cannot support multimodal
input. Table 9 shows the result that our model
obtains better performance than these two LLMs.
Secondly, to demonstrate the superiority of the us-
age of multimodal input, we compare our model
with MLLM, including VisualGLM-6B (Du et al.,
2022), Llava 1.5 (Liu et al., 2024a), MMICL (Zhao
et al., 2024a), mPLUG-Owl2 (Ye et al., 2024) and
GPT4V (OpenAI, 2024) on the MASC task. Ta-
ble 10 shows the experimental results. The results
demonstrate that our model achieves higher perfor-
mance, even though with fewer parameters.
4.5
Case Study
Figure 4 shows four examples with their predic-
tions from VLP-MABSA (Ling et al., 2022), AoM
(Zhou et al., 2023), and our CORSA model.
Consider the left two columns. For the first ex-
ample, VLP-MABSA and AoM incorrectly predict
320
VLP        (Kingston, POS) ×
(Lionel Messi, NEU) ×
Image
Text
Conditional
Relation
School holiday program kicks off
in Kingston and Glen Eira.
RT @ UberFootbalI: On this day,
8-years ago, a 19 - year old Lionel
Messi did this . Wow .
Irrelevance
Irrelevance
@ CristianoRonaldo set for a @ M-
anUtd return next season # # SSFo-
Otball.
David Cameron unveils h-
is most convincing argum-
ent yet to stay in the EU.
Relevance
Relevance
(Glen Eira, POS) ×
(Kingston, POS) ×
(Glen Eira, POS) ×
AoM
(Kingston, NEU)√
(Glen Eira, NEU)√
CORSA
(Lionel Messi, NEU) ×
(Lionel Messi, POS)√
(CristianoRonaldo, NEU) ×
(ManUtd, NEU)√
(CristianoRonaldo, NEU) ×
(ManUtd, NEU)√
(CristianoRonaldo, POS)√
(ManUtd, NEU)√
(David Cameron, NEU) ×
(EU, NEU)√
(David Cameron, NEG)√
(EU, NEG) ×
(David Cameron, NEG)√
(EU, NEU)√
(Kingston, NEU)
(Glen Eira, NEU)
GT
(Lionel Messi, POS)
(CristianoRonaldo, POS)
(ManUtd, NEU)
(David Cameron, NEG)
(EU, NEU)
Figure 4: The results of the comparison among different methods on four testing samples. The left two columns are
two unmet conditional samples; the other two columns are met conditional samples. The ground-truth and predicted
bounding boxes for VOL are visualized as red and blue boxes, respectively.
TWITTER-15
TWITTER-17
Method
ACC
F1
ACC
F1
VisualGLM-6B
66.1
68.2
69.0
68.5
Llava 1.5
77.9
74.3
74.6
74.3
MMICL
76.0
72.7
74.1
74.0
mPLUG-Owl2
76.8
72.3
74.2
73.0
GPT4V
75.3
74.2
76.0
75.5
CORSA (Our)
81.1
77.7
76.6
74.5
Table 10: The performance comparison with Multi-
modal LLMs (MLLMs) on MASC task.
the sentiment of aspects Kingston and Glen Eira as
positive. It is due to the condition that the image
contains the objects referred by the aspect Kingston
and Glen Eira cannot be met. And the girl’s smiling
face misleads incorrect predictions. Our CORSA
model filters information about the girl in the im-
age and could predict correctly. The image in the
second example does not contain information about
Lionel Messi. Therefore, for the same reason, VLP-
MABSA and AoM wrongly utilize the information,
and give incorrect predictions. Our CORSA model
obtains the correct predictions due to filtering the
unmet conditional image information. The results
demonstrate the importance of filtering unmet con-
ditional information from the image.
Consider the right two columns. For the first
example, VLP-MABSA and AoM do not correctly
predict the positive sentiment in CristianoRonaldo.
These two methods cannot detect the aspect Cris-
tianoRonaldo related visual Information, especially
the smiling face of the man in the image. Our
model explicitly detect the exact condition-related
regions with CristianoRonaldo, correctly predict-
ing its positive sentiment. Similarly, in the second
example, due to the lack of explicitly locating exact
condition-related regions, VLP-MABSA and AoM
incorrectly predict the sentiments of two aspects.
Our model gives correct predictions.
5
Related Work
MABSA consists of three related tasks: MATE,
MASC, and JMASA. On MATE. Earlier methods
(Moon et al., 2018; Arshad et al., 2019; Wu et al.,
2020a) adopt cross-modal attention mechanisms.
These methods are too simple to effectively learn
multimodal information. Recent methods (Yu et al.,
2020; Liu et al., 2022; Zheng et al., 2023) use pre-
trained language models and modality translation-
based approaches. With the wide applications of
large-scale generative models, Yu et al. (2023) ex-
pand Multimodal NER methods to MATE with a
generative framework.
On MASC. Existing MASC methods are usually
based on attention mechanisms and graph convo-
lutional networks (GCNs). For example, Zhang
et al. (2021) introduce an attention network with
a discriminative mechanism. Xiao et al. (2023)
propose a crossmodal fine-grained alignment and
fusion network. Zhao and Yang (2023) propose a
fusion model with GCN and SE-ResNeXt network.
To solve the problem of irrelevant aspects-images,
Wang et al. (2023) design an aspect-oriented fil-
tration module which utilizes the given aspects to
321
compute attention scores with sentence as well as
image. However, this method of calculating the
relevance scores between the given aspects to the
images cannot transfer well to JMASA, in which
the aspects are not given. Recently, some meth-
ods adopt LLMs. Feng et al. (2024) propose to
use LLVM during the fusion of textual and visual
modalities.
On JMASA. Some methods are based on the
pipeline framework. Ju et al. (2021) jointly learn
MATE and MASC tasks. Yang et al. (2022b) intro-
duce a text-guided cross-modal interaction module
to dynamically control the contributions of the vi-
sual information. This pipeline approach ignores
the potential semantic associations between these
two tasks. Other methods are based on generative
models. Ling et al. (2022) propose a task specific
Vision-Language Pretraining framework.
Zhou
et al. (2023) propose to detect aspect-relevant se-
mantic and sentiment information. This generative
approach could flexibly produce complex text, but
their training are time-consuming. Recently, Liu
et al. (2024b) propose a framework which implic-
itly calculates the similarity between the sentences
and the images to simultaneously reduce multi-
level modality noise and multi-grained semantic
gap. However, this method lacks explicit monitor-
ing of aspect-image relevance, and therefore cannot
learn fine-grained relations.
Unfortunately, most of previous works ignore
the multimodal conditional relations between the
images and texts when performing MABSA task.
The premise that the image contains the objects
referred by the aspects within the text sometimes
cannot be met. In this paper, we propose CORSA
framework by explicitly considering this issue.
6
Conclusion and Future Work
In this paper, we propose CORSA framwork for
MABSA. Our CORSA involves two key modules,
CRD and VOL. CRD is designed to mitigate the
impact of the unmet conditional image. VOL aims
to locate the exact condition-related visual regions
with the aspects. We perform two types of anno-
tations on benchmark datasets for training. Exten-
sive experimental results show the effectiveness
of our proposed CORSA model. Although our
model achieves significant performance, there are
still room for improvement. In the future, we con-
sider exploring annotation methods, such as using
MLLMs to improve the accuracy of the annotation.
Limitation
Our method still has some limitations. We automat-
ically annotate the data using a pretrained model
(UNINEXT), which would cause the problem of
inaccuracies. In other words, we have no ground-
truth for this conditional relation. Therefore, on
one side, we cannot perform accurate statistics on
the conditional relation on these two benchmark
datasets. On the other side, the inaccuracies affect
the CORSA model’s performance. These limita-
tions present challenges for further investigations.
Acknowledgment
This work was supported by the National Na-
ture Science Foundation of China under Grant
62076032 and the CCF-Zhipu Large Model Inno-
vation Fund (NO. CCF-Zhipu202407). In addition,
the authors thank the anonymous reviewers for their
constructive feedback.
References
Omer Arshad, Ignazio Gallo, Shah Nawaz, and Alessan-
dro Calefati. 2019. Aiding intra-text representations
with visual context for multimodal named entity
recognition. In 2019 International conference on
document analysis and recognition (ICDAR), pages
337–342. IEEE.
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,
Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM:
General language model pretraining with autoregres-
sive blank infilling. In Proceedings of the 60th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 320–335,
Dublin, Ireland. Association for Computational Lin-
guistics.
Junjia Feng, Mingqian Lin, Lin Shang, and Xiaoy-
ing Gao. 2024. Autonomous aspect-image instruc-
tion a2ii: Q-former guided multimodal sentiment
classification. In Proceedings of the 2024 Joint In-
ternational Conference on Computational Linguis-
tics, Language Resources and Evaluation (LREC-
COLING 2024), pages 1996–2005.
Aibo Guo, Xiang Zhao, Zhen Tan, and Weidong Xiao.
2023. Mgicl: multi-grained interaction contrastive
learning for multimodal named entity recognition. In
Proceedings of the 32nd ACM International Confer-
ence on Information and Knowledge Management,
pages 639–648.
Xuming Hu, Junzhe Chen, Aiwei Liu, Shiao Meng,
Lijie Wen, and Philip S Yu. 2023. Prompt me up:
Unleashing the power of alignments for multimodal
entity and relation extraction. In Proceedings of the
31st ACM International Conference on Multimedia,
pages 5185–5194.
322
Yufeng Huang, Zhuo Chen, Jiaoyan Chen, Jeff Z Pan,
Zhen Yao, and Wen Zhang. 2023. Target-oriented
sentiment classification with sequential cross-modal
semantic graph. In International Conference on Arti-
ficial Neural Networks, pages 587–599. Springer.
Meihuizi Jia, Lei Shen, Xin Shen, Lejian Liao, Meng
Chen, Xiaodong He, Zhendong Chen, and Jiaqi Li.
2023.
Mner-qg: An end-to-end mrc framework
for multimodal named entity recognition with query
grounding. In Proceedings of the AAAI conference on
artificial intelligence, volume 37, pages 8032–8040.
Glenn Jocher, Ayush Chaurasia, and Jing Qiu. 2023.
Ultralytics YOLO.
Xincheng Ju, Dong Zhang, Rong Xiao, Junhui Li,
Shoushan Li, Min Zhang, and Guodong Zhou. 2021.
Joint multi-modal aspect-sentiment analysis with aux-
iliary cross-modal relation detection. In Proceedings
of the 2021 conference on empirical methods in natu-
ral language processing, pages 4395–4405.
Zaid Khan and Yun Fu. 2021. Exploiting bert for mul-
timodal target sentiment classification through input
space translation. In Proceedings of the 29th ACM
international conference on multimedia, pages 3034–
3042.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Ghazvininejad, Abdelrahman Mohamed, Omer Levy,
Veselin Stoyanov, and Luke Zettlemoyer. 2020.
BART: Denoising sequence-to-sequence pre-training
for natural language generation, translation, and com-
prehension. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics,
pages 7871–7880, Online. Association for Computa-
tional Linguistics.
Jinyuan Li, Han Li, Zhuo Pan, Di Sun, Jiahao Wang,
Wenkun Zhang, and Gang Pan. 2023. Prompting
chatgpt in mner: enhanced multimodal named entity
recognition with auxiliary refined knowledge. arXiv
preprint arXiv:2305.12212.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,
and C Lawrence Zitnick. 2014.
Microsoft coco:
Common objects in context. In Computer Vision–
ECCV 2014: 13th European Conference, Zurich,
Switzerland, September 6-12, 2014, Proceedings,
Part V 13, pages 740–755. Springer.
Yan Ling, Jianfei Yu, and Rui Xia. 2022.
Vision-
language pre-training for multimodal aspect-based
sentiment analysis.
In Proceedings of the 60th
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1:Long Papers), page
2149–2159.
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae
Lee. 2024a. Improved baselines with visual instruc-
tion tuning. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition,
pages 26296–26306.
Luping Liu, Meiling Wang, Mozhi Zhang, Linbo Qing,
and Xiaohai He. 2022. Uamner: uncertainty-aware
multimodal named entity recognition in social media
posts. Applied Intelligence, 52(4):4109–4125.
Yaxin Liu, Yan Zhou, Ziming Li, Jinchuan Zhang,
Yu Shang, Chenyang Zhang, and Songlin Hu. 2024b.
Rng: Reducing multi-level noise and multi-grained
semantic gap for joint multimodal aspect-sentiment
analysis. 2024 IEEE International Conference on
Multimedia and Expo.
Ilya Loshchilov and Frank Hutter. 2017.
Fixing
weight decay regularization in adam.
CoRR,
abs/1711.05101.
Seungwhan Moon, Leonardo Neves, and Vitor Carvalho.
2018. Multimodal named entity recognition for short
social media posts. In Proceedings of the 2018 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long Papers), pages
852–860, New Orleans, Louisiana. Association for
Computational Linguistics.
Jie Mu, Feiping Nie, Wei Wang, Jian Xu, Jing Zhang,
and Han Liu. 2023. Mocolnet: A momentum con-
trastive learning network for multimodal aspect-level
sentiment analysis. IEEE Transactions on Knowl-
edge and Data Engineering.
OpenAI. 2023. Chatgpt: A large language model.
OpenAI. 2024.
Gpt-4 technical report.
Preprint,
arXiv:2303.08774.
Lin Sun, Jiquan Wang, Kai Zhang, Yindu Su, and Fang-
sheng Weng. 2021. Rpbert: a text-image relation
propagation-based bert model for multimodal ner.
In Proceedings of the AAAI conference on artificial
intelligence, volume 35, pages 13860–13868.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023.
Llama 2:
Open founda-
tion and fine-tuned chat models.
arXiv preprint
arXiv:2307.09288.
Qianlong Wang, Hongling Xu, Zhiyuan Wen, Bin Liang,
Min Yang, Bing Qin, and Ruifeng Xu. 2023. Image-
to-text conversion and aspect-oriented filtration for
multimodal aspect-based sentiment analysis. IEEE
Transactions on Affective Computing.
Hanqian Wu, Siliang Cheng, Jingjing Wang, Shoushan
Li, and Lian Chi. 2020a. Multimodal aspect extrac-
tion with region-aware alignment network. In Nat-
ural Language Processing and Chinese Computing:
9th CCF International Conference, NLPCC 2020,
Zhengzhou, China, October 14–18, 2020, Proceed-
ings, Part I 9, pages 145–156. Springer.
Zhiwei Wu, Changmeng Zheng, Yi Cai, Junying Chen,
Ho-fung Leung, and Qing Li. 2020b. Multimodal
representation with embedded visual guiding objects
323
for named entity recognition in social media posts.
In Proceedings of the 28th ACM International Con-
ference on Multimedia, pages 1038–1046.
Luwei Xiao, Xingjiao Wu, Junjie Xu, Weijie Li, Cheng
Jin, and Liang He. 2024. Atlantis: Aesthetic-oriented
multiple granularities fusion network for joint multi-
modal aspect-based sentiment analysis. Information
Fusion, 106:102304.
Luwei Xiao, Xingjiao Wu, Shuwen Yang, Junjie Xu, Jie
Zhou, and Liang He. 2023. Cross-modal fine-grained
alignment and fusion network for multimodal aspect-
based sentiment analysis. Information Processing &
Management, 60(6):103508.
Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Ping Luo,
Zehuan Yuan, and Huchuan Lu. 2023. Universal in-
stance perception as object discovery and retrieval.
In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, pages 15325–
15336.
Bin Yang and Jinlong Li. 2023. Visual elements mining
as prompts for instruction learning for target-oriented
multimodal sentiment classification.
In Findings
of the Association for Computational Linguistics:
EMNLP 2023, pages 6062–6075.
Hao Yang, Yanyan Zhao, and Bing Qin. 2022a. Face-
sensitive image-to-emotional-text cross-modal trans-
lation for multimodal aspect-based sentiment analy-
sis. In Proceedings of the 2022 conference on empir-
ical methods in natural language processing, pages
3324–3335.
Juan Yang, Mengya Xu, Yali Xiao, and Xu Du. 2024.
Amifn: Aspect-guided multi-view interactions and
fusion network for multimodal aspect-based senti-
ment analysis. Neurocomputing, 573:127222.
Li Yang, Jin-Cheon Na, and Jianfei Yu. 2022b. Cross-
modal multitask transformer for end-to-end multi-
modal aspect-based sentiment analysis. Information
Processing & Management, 59(5):103038.
Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, An-
wen Hu, Haowei Liu, Qi Qian, Ji Zhang, and Fei
Huang. 2024. mplug-owl2: Revolutionizing multi-
modal large language model with modality collabo-
ration. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages
13040–13051.
Jianfei Yu and Jing Jiang. 2019.
Adapting bert for
target-oriented multimodal sentiment classification.
In Proceedings of the Twenty-Eighth International
Joint Conference on Artificial Intelligence. IJCAI.
Jianfei Yu, Jing Jiang, Li Yang, and Rui Xia. 2020.
Improving multimodal named entity recognition via
entity span detection with unified multimodal trans-
former. In Proceedings of the 58th Annual Meeting
of the Association for Computational Linguistics. As-
sociation for Computational Linguistics.
Jianfei Yu, Ziyan Li, Jieming Wang, and Rui Xia. 2023.
Grounded multimodal named entity recognition on
social media. In Proceedings of the 61st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 9141–9154.
Zhewen Yu, Jin Wang, Liang-Chih Yu, and Xuejie
Zhang. 2022. Dual-encoder transformers with cross-
modal alignment for multimodal aspect-based senti-
ment analysis. In Proceedings of the 2nd Conference
of the Asia-Pacific Chapter of the Association for
Computational Linguistics and the 12th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers), pages 414–423, Online
only. Association for Computational Linguistics.
Jing Zhang, Jiaqi Qu, Jiangpei Liu, and Zhe Wang.
2024.
Mcpl: Multi-model co-guided progressive
learning for multimodal aspect-based sentiment anal-
ysis. Knowledge-Based Systems, 301:112331.
Zhe Zhang, Zhu Wang, Xiaona Li, Nannan Liu, Bin
Guo, and Zhiwen Yu. 2021. Modalnet: an aspect-
level sentiment classification model by exploring mul-
timodal data with fusion discriminant attentional net-
work. World Wide Web, 24:1957–1974.
Fei Zhao, Chunhui Li, Zhen Wu, Yawen Ouyang, Jian-
bing Zhang, and Xinyu Dai. 2023. M2df: Multi-
grained multi-curriculum denoising framework for
multimodal aspect-based sentiment analysis. Pro-
ceedings of the 2023 conference on empirical meth-
ods in natural language processing.
Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian
Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng
Wang, Wenjuan Han, and Baobao Chang. 2024a.
Mmicl: Empowering vision-language model with
multi-modal in-context learning. the 12th Interna-
tional Conference on Learning Representations.
Hua Zhao, Manyu Yang, Xueyang Bai, and Han Liu.
2024b. A survey on multimodal aspect-based senti-
ment analysis. IEEE Access.
Jun Zhao and Fuping Yang. 2023. Fusion with gcn
and se-resnext network for aspect based multimodal
sentiment analysis. In 2023 IEEE 6th Information
Technology, Networking, Electronic and Automation
Control Conference (ITNEC), volume 6, pages 336–
340. IEEE.
Changmeng Zheng, Junhao Feng, Yi Cai, Xiaoyong Wei,
and Qing Li. 2023. Rethinking multimodal entity and
relation extraction from a translation point of view.
In Proceedings of the 61st Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 6810–6824.
Ru Zhou, Wenya Guo, Xumeng Liu, Shenglong Yu,
Ying Zhang, and Xiaojie Yuan. 2023. Aom: De-
tecting aspect-oriented information for multimodal
aspect-based sentiment analysis. In Proceedings of
the 61th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1:Long Papers).
