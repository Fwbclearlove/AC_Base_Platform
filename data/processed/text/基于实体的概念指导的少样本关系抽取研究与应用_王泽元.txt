密级
：
保密期限
：


遽耆卿ｆＡ＃


硕士学位论文


Ｗ


题目
：
基于实体的概念指导的


少样本关系抽取研究与应用


学号
：２０１９１８０１１２


姓名
：
王泽元


专业
：计算机技术


导
师
：
李睿凡


学院
：人工智能学院


２０２２年
６
月
１
日


中国
？北京


密级
：保密期限
：


分ｆ却耄大聲


硕士学位论文


？


题目
：
基于实体的概念指导的


少样本关系抽取研究与应用


学号
：２０１９１８０１１２


姓名
：王泽元


专业
：计算机技术


导师
：李睿凡


学院
：人工智能学院


２０２２年
６月
１
日


ＳｅｃｒｅｔＬｅｖｅｌ
：ＣｏｎｆｉｄｅｎｔｉａｌｉｔｙＰｅｒｉｏｄ
：


？
ＢＥＩＪＩＮＧＵＮＩＶＥＲＳＩＴＹＯＦ


ＰＯＳＴＳＡＮＤ


Ｔｅｌｅｃｏｍｍｕｎｉｃａｔｉｏｎｓ


ＴｈｅｓｉｓｆｏｒＭａｓｔｅｒＤｅｇｒｅｅ


ＴＩＴＬＥ
：ＲＥＳＥＡＲＣＨＡＮＤＡＰＰＬＩＣＡＴＩＯＮ


ＯＦＦＥＷ
－ＳＨＯＴＲＥＬＡＴＩＯＮＥＸＴＲＡＣＴＩＯＮ


ＷＩＴＨＥＮＴＩＴＹＣＱＣＥＰＴＧＵＩＤＡＴＩＯＮ


ＳｔｕｄｅｎｔＮｏ
．
：２０１９１８０１１２


Ｃａｎｄｉｄａｔｅ
：ＺｅｖｕａｎＷａｎｇ


Ｓｕｂ
ｊｅｃｔ
：Ｃｏｍｐｕｔｅｒｔｅｃｈｎｏｌｏｇｙ


Ｓｕｐｅｒｖｉｓｏｒ
：ＲｕｉｆａｎＬｉ


Ｉｎｓｔｉｔｕｔｅ
：ＳｃｈｏｏｌｏｆＡｒｔｉｆ
ｉｃｉａｌＩｎｔｅｌｌｉｇｅｎｃｅ


Ｊｕｎｅ１
，２０２２


基于实体的概念指导的少样本关系抽取研究与应用


摘要


关系抽取任务是指根据语义信息判断文本中两个实体的关系
，从


而将非结构化文本转化成结构化知识
。传统的关系抽取模型需要较多


的标注数据
，并且很难实现新型关系的建模
；为了降低标注成本且满


足新型关系的建模需求
，
少样本关系抽取任务逐渐成为研宄热点
。


少样本关系抽取任务通过少量标注数据建模
，保证模型的泛化能


力是该任务的
一大挑战
。近期的研宄通过将知识库融入少样本关系抽


取模型
，在该任务取得了较大的进展。但在现实应用中
，特别是存在


领域迁移的情况下
，知识库的来源和类型可能存在较大的差异
，知识


库和融入模块的泛化能力很难得到保证
。


在上述任务背景下
，本文针对少样本关系抽取任务下的知识融入


和领域迁移问题上进行研究
，
具体研宄工作如下
：


１
．利用实体概念作为融入知识
，
寻找不同类型和来源的知识库


的有效融入方式
。针对图类型的知识库
，设计了结合语义门控机制和


距离打分器的知识融合模块
，该知识融合方式能在不同领域间的知识


库上实现有效迀移
。针对文本类型的知识库
，将文本知识以模版的形


式输入语言学模型
，提高了模型对于知识的感知
。此外
，针对已知目


标领域的情况
，设计了领域导向的元训练方法
，从训练数据中获取与


领域更为相关的知识
，并在训练过程中利用样本对维度的正则项来约


束样本的特征表示
，
提高训练的稳定性
。


２
．基于上述模型
，设计并实现了少样本关系抽取系统
。该系统支


持用户定义关系
，
包括用户管理
、数据管理、
图谱管理等模块
；
通过


上述模块完成训练数据和测试数据的上传
，经过离线训练实现少样本


关系抽取的自动化流程
。系统测试结果表明设计的系统能够高效完成


新型关系的快速抽取工作
。


关键词
：
关系抽取少样本学习
领域迁移知识库


ＲＥＳＥＡＲＣＨＡＮＤＡＰＰＬＩＣＡＴＩＯＮＯＦＦＥＷ
－ＳＨＯＴ


ＲＥＬＡＴＩＯＮＥＸＴＲＡＣＴＩＯＮＷＩＴＨＥＮＴＩＴＹＣＯＣＥＰＴ


ＧＵＩＤＡＴＩＯＮ


ＡＢＳＴＲＡＣＴ


Ｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎｒｅｆｅｒｓｔｏｃｌａｓｓｉｆｙｔｈｅｒｅｌａｔｉｏｎｏｆｅｎｔｉｔｉｅｓｉｎ


ｕｎｓｔｒｕｃｔｕｒｅｄｓｅｎｔｅｎｃｅａｃｃｏｒｄｉｎｇ
ｔｏｔｈｅｓｅｍａｎｔｉｃｉｎｆｏｒｍａｔｉｏｎ
，ｓｏａｓｔｏ


ｔｒａｎｓｆｏｒｍｕｎｓｔｒｕｃｔｕｒｅｄｔｅｘｔｉｎｔｏｓｔｒｕｃｔｕｒｅｄｋｎｏｗｌｅｄｇｅ．Ｔｈｅｔｒａｄｉｔｉｏｎａｌ


ｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎｍｏｄｅｌｎｅｅｄｓｍａｎｙａｎｎｏｔａｔｉｏｎｄａｔａ
，ａｎｄｉｔｉｓｄｉｆｆｉｃｕｌｔｔｏ


ｍｏｄｅｌｔｈｅｎｅｗｒｅｌａｔｉｏｎ．Ｉｎｏｒｄｅｒｔｏｒｅｄｕｃｅｔｈｅａｎｎｏｔａｔｉｏｎｃｏｓｔａｎｄｍｅｅｔｔｈｅ


ｍｏｄｅｌｉｎｇｒｅｑｕｉｒｅｍｅｎｔｓｏｆｎｅｗｒｅｌａｔｉｏｎ
，ｔｈｅｆｅｗ
－ｓｈｏｔｒｅｌａｔ
ｉｏｎｅｘｔｒａｃｔｉｏｎ


ｔａｓｋｈａｓ
ｇｒａｄｕａｌｌｙｂｅｃｏｍｅａｒｅｓｅａｒｃｈｈｏｔｓｐｏｔ．


Ｉｎｆｅｗ
－ｓｈｏｔｒｅｌａｔｉｏｎｅｘｔｒａｃｔ
ｉｏｎｔａｓｋ
，ｔｈｅｍｏｄｅｌｅｄｉｓｔｒａｉｎｅｄｗｉｔｈｆｅｗ


ｌａｂｅｌｅｄｓａｍｐ
ｌｅｓ
，ｗｈｉｃｈｌｅａｄｓｔｏ
ｐｏｏｒｍｏｄｅｌ
ｇｅｎｅｒａｌｉｚａｔｉｏｎａｂｉｌｉｔｙ
．Ｒｅｃｅｎｔ


ｒｅｓｅａｒｃｈｅｓｈａｖｅｍａｄｅ
ｇｒｅａｔ
ｐｒｏｇｒｅｓｓｂｙ
ｉｎｃｏｒｐｏｒａｔｉｎｇｋｎｏｗｌｅｄｇｅｂａｓｅｉｎｔｏ


ｆｅｗ
－ｓｈｏｔｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎｍｏｄｅｌ
．Ｈｏｗｅｖｅｒ
，ｉｎｐｒａｃｔｉｃａｌａｐｐ
ｌｉｃａｔｉｏｎｓ
，


ｅｓｐｅｃｉａｌｌｙ
ｉｎｔｈｅｃａｓｅｏｆｄｏｍａｉｎｍｉｇｒａｔｉｏｎ
，ｔｈｅｓｏｕｒｃｅａｎｄｔｙｐｅｏｆ


ｋｎｏｗｌｅｄｇｅｂａｓｅｍａｙｄｉｆｆｅｒｇｒｅａｔｌｙ，ａｎｄｉｔｉｓｄｉｆｆ
ｉｃｕｌｔｔｏｇｕａｒａｎｔｅｅｔｈｅ


ｇｅｎｅｒａｌｉｚａｔｉｏｎａｂｉｌｉｔｙｏｆｋｎｏｗｌｅｄｇｅｂａｓｅａｎｄｋｎｏｗｌｅｄｇｅｉｎｔｅｇｒａｔｉｏｎ


ｍｏｄｕｌｅ
．


Ｕｎｄｅｒｔｈｅａｂｏｖｅｂａｃｋｇｒｏｕｎｄ
，ｔｈｉｓｐａｐｅｒｓｔｕｄｉｅｓｋｎｏｗｌｅｄｇｅ


ｉｎｔｅｇｒａｔｉｏｎａｎｄｄｏｍａｉｎｔｒａｎｓｆｅｒｐｒｏｂｌｅｍｉｎｆｅｗ
－ｓｈｏｔｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎ


ｔａｓｋ．Ｔｈｅｓｐｅｃｉｆ
ｉｃｒｅｓｅａｒｃｈｗｏｒｋｓａｒｅａｓｆｏｌｌｏｗｓ
：


１
．Ｔｈｉｓｐａｐｅｒｅｍｐ
ｌｏｙｅｄｅｎｔｉｔｙｃｏｎｃｅｐｔａｓｉｎｔｅｇｒａｔｅｄｋｎｏｗｌｅｄｇｅａｎｄ


ｆｉｎｄｔｈｅｅｆｆｅｃｔｉｖｅｗａｙｓｏｆｉｎｔｅｇｒａｔｉｎｇｋｎｏｗｌｅｄｇｅｉｎｄｉｆｆｅｒｅｎｔｆｏｒｍｓｏｆ


ｋｎｏｗｌｅｄｇｅ
．Ｉｎｔｈｅｆｏｒｍｏｆｃｏｎｃｅｐｔ
ｇｒａｐｈ
，ｗｅｄｅｓｉｇｎａｋｎｏｗｌｅｄｇｅｆｕｓｉｏｎ


ｍｏｄｕｌｅｃｏｍｂｉｎｉｎｇｓｅｍａｎｔｉｃｇａｔｉｎｇｍｅｃｈａｎｉｓｍａｎｄｄｉｓｔａｎｃｅｃｌａｓｓｉｆｉｅｒ
，


ｗｈｉｃｈｃａｎｅｆｆｅｃｔｉｖｅｌｙ
ｔｒａｎｓｆｅｒｔｈｅｋｎｏｗｌｅｄｇｅｉｎｔｅｇｒａｔｉｏｎｍｅｔｈｏｄｉｎ


ｄｉｆｆｅｒｅｎｔｄｏｍａｉｎｓ．Ｉｎｔｈｅｆｏｒｍｏｆｔｅｘｔｕａｌｌａｂｅｌ
，ｗｅｆｏｕｎｄｔｈａｔｉｎｓｅｒｔｉｎｇ
ｔｈｅ


ｌａｎｇｕａｇｅｔｏｔｈｅｔｅｍｐ
ｌａｔｅｏｆｌａｎｇｕａｇｅｍｏｄｅｌｉｓａｎｅｆｆｅｃｔｉｖｅｔｅｘｔｆｕｓｉｏｎ
．Ｉｎ


ａｄｄｉｔｉｏｎ
，ａｉｍｉｎｇａｔｔｈｅｓｉｔｕａｔｉｏｎｏｆ
ｋｎｏｗｎｔａｒｇｅｔｄｏｍａｉｎ
，ｔｈｉｓ
ｐａｐｅｒｄｅｓｉｇｎｓ


ａｄｏｍａｉｎ
－ｏｒ
ｉｅｎｔｅｄｍｅｔａ
－ｔｒａｉｎｉｎｇｍｅｔｈｏｄｔｏｏｂｔａｉｎｍｏｒｅｄｏｍａｉｎ
－ｒｅｌａｔｅｄ


ｋｎｏｗｌｅｄｇｅｆ
ｒｏｍｔｒａｉｎｉｎｇｄａｔａ
，ａｎｄｕｓｅｓｔｈｅｒｅｇｕｌａｒｔｅｒｍｏｆｓａｍｐ
ｌｅ
ｐａｉｒ


ｄｉｍｅｎｓｉｏｎｓｔｏｃｏｎｓｔｒａｉｎｔｈｅｆｅａｔｕｒｅｒｅｐｒｅｓｅｎｔａｔｉｏｎｏｆｓａｍｐ
ｌｅｓｄｕｒ
ｉｎｇ


ｔｒａｉｎｉｎｇ
ｔｏｉｍｐｒｏｖｅｔｈｅｓｔａｂｉｌｉｔｙｏｆｔｒａｉｎｉｎｇ
．


２
．Ｔｈｅｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎｓｙｓｔｅｍｂａｓｅｄｏｎｆｅｗ
－ｓｈｏｔｍｏｄｅｌｉｓｄｅｓｉｇｎｅｄ


ａｎｄｉｍｐ
ｌｅｍｅｎｔｅｄ
．Ｔｈｅｓｙｓｔｅｍｓｕｐｐｏｒｔｓｔｈｅｄｅｆ
ｉｎｉｔｉｏｎｏｆｒｅｌａｔｉｏｎｓａｎｄ


ｄｅｓｉｇｎｓｕｓｅｒｍａｎａｇｅｍｅｎｔ
，ｄａｔａｍａｎａｇｅｍｅｎｔａｎｄｋｎｏｗｌｅｄｇｅｇｒａｐｈ


ｍａｎａｇｅｍｅｎｔｍｏｄｕｌｅｓ
．Ｉｔｓｕｐｐｏｒｔｓｔｈｅａｕｔｏｍａｔｅｄｐｒｏｃｅｓｓｏｆｕｓｅｒｄｅｆ
ｉｎｅｄ


ｒｅｌａｔｉｏｎｓａｎｄｄａｔａｕｐ
ｌｏａｄａｎｄ
ｇｅｎｅｒａｔｅｓｋｎｏｗｌｅｄｇｅｇｒａｐｈｔｈｒｏｕｇｈｏｆｆｌｉｎｅ


ｔｒａｉｎｉｎｇ
．Ｔｈｅｔｅｓｔｒｅｓｕｌｔｓｓｈｏｗｔｈａｔｔｈｅｓｙｓｔｅｍｄｅｓｉｇｎｅｄｉｎｔｈｉｓ
ｐａｐｅｒｃａｎ


ｅｆｆｉｃｉｅｎｔｌｙｃｏｍｐ
ｌｅｔｅｔｈｅｔａｓｋｏｆｎｅｗｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎ
．


ＫＥＹＷＯＲＤＳ
：ｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎ
，ｆｅｗ
－ｓｈｏｔｌｅａｒｎｉｎｇ，ｄｏｍａｉｎａｄａｐ
ｔａｔｉｏｎ
，


ｋｎｏｗｌｅｄｇｅｂａｓｅ．


目
录


第
一章绪论
１


１
．
１研宄背景及意义
１


１
．２
国内外研究现状
２


１
．２
．
１
知识图谱发展现状
２


１
．２
．２关系抽取研究现状
３


１
．２
．３少样本关系抽取研宄现状
４


Ｕ
主要研究内容
．…
，
」
６


１
．４章节组织形式
７


第二章基础知识
９


２
．
１语言模型
９


２
．１
．
１ＣＢＯＷ
９


２
．
１
．２ＥＬＭＯ
１０


２
．
１
．３ＢＥＲＴ
１１


２
．２少样本学习
１２


２
．３元学ｇ
１３


２
．４度量学习
１４


２
．４
．
１孪生网络
１４


２
．４
．２原型网络
１５


２
．５Ｐｒｏｍｐ
ｔ方法
１５


２
．５
．
１ＰＥＴ
１６


２
．５
．２ＥＦＬ
１７


２
．６本章小结

１８


第三章
实体概念指导的少样本关系抽取模型
１９


３
．
１
弓唁
１９


３
．２整体框架
２０


３
．３基础模型
２１


３
．４
．
１ＢＥＲＴ
－Ｐａｉｒ模型
２１


３
＿４
．２ＢＥＲＴ
－Ｐａｉｒ
（Ｐｒｏｍｐ
ｔ
）模型


２２


３
．４概念知识融合模块
２３


３
．４
．
１
实体概念知识
２３


３
．４
．２概念映射与表示
２４


３
．４
．３文本形式的概念融入
２５


３
．４
．４
图谱形式的概念融入
２５


３
．５元学习增强模块
２７


３
．５
．
１对比学习正则项
２７


３
．５
．２领域导向性训练
２９


３
．６实验结果分析
３０


３
．６
．
１
实验环境
３０


３
．６
．２实验数据与参数设计
３０


３
．６
．３文本形式概念知识融入实验
３１


３
．６
．４
图谱形式的概念融入增强
３３


３
．６
．５元学习增强模块
３６


３
．７本章小结
３７


第四章少样本关系抽取系统设计与实现
３９


４
．
１
系统需求分析
３９


４
．
１
．
１
系统模块需求分析
３９


４
．
１
．２系统非功能模块需求分析
４１


４
．２系统概要设计
４３


４
．２
．
１
系统架构设计
４３


４
．２
．２数据库设计
４４


４
．３功能模块设计
４６


４
．３
．
１前端展示模块
４６


４
．３
．２后端模块
４６


４
．３
．３数据上传模块
４７


４
．３
．４关系抽取模型构建模块
４８


４
．３
．５
用户管理模块
４９


４
．４系统测试
４９


４
．４
．
１
测试环境
４９


４
．４
．２功能性测试
４９


４
．４
．３非功能性测试
５３


４
．５本章小结
５４


第五章总结与展望
５５


５
．
１总结
５５


５
．２展望
５５


参考文献
５７


第
一章绪论


第
一章绪论


１
．
１研究背景及意义


知识图谱通过结构化的形式描述客观世界中的实体及实体间的关系
，将互联


网的非结构化信息以更加简单的结构化形式进行表示
。近年来
，随着知识图谱的


研宄不断推进以及图谱类型知识的不断扩充
，
ＷｏｒｄＮｅｔ、
Ｆｒｅｅｂａｓｅ和Ｗｉｋｉｄａｔａ等


通用知识图谱在信息检索
、
问答系统
、推荐系统等多个领域有广泛的应用
。知识


图谱是由知识三元组构成
，
以实体和实体间的关系存储了大量的世界知识
。
目前


的公开知识图谱已经具有较大的规模
，
但仍然与现实世界的知识量级差距巨大
，


知识图谱还有很大的完善空间
。现实世界中的新知识也不断产生
，为了保证知识


图谱下游相关应用的持续有效性
，知识图谱也要不断进行扩充
。通过人工标注得


到知识图谱的方式费用昂贵
，为了准确并及时为现有知识图谱增添新的知识内容
，


研宄者探索高效自动获取世界知识的办法
，即通过标注部分数据并训练实体抽取


与关系抽取模型
，
随后利用模型进行自动化的知识抽取
。


关系抽取是三元组知识抽取的子环节
，传统的方法是通过大量的标注数据训


练得到关系抽取模型
，该类模型在大多情况下取得了不错的效果
，但无法解决新


型关系的抽取问题
。新型关系的标注数据通常较少
，传统的关系抽取模型由标注


数据驱动的
，在该情况下通常会面临过拟合的问题
，从而无法保证模型的知识抽


取性能
。此外
，
现实中的新型关系出现的速度是极快的
，通用知识图谱Ｗｉｋｉｄａｔａ


在
２０１４年包含
９２０种关系
，但目前关系类别已超过
３０００种
。面对新型关系抽取


的挑战
，通常有两种建模方法
，包括远程监督模型和少样本模型
。远程监督模型


通过相关假设来构造伪标注数据
，训练成本较高
；少样本模型则直接利用少量标


注数据来训练模型
。少样本关系抽取方法相较于远程监督方法
，在新关系下拥有


更快的适应能力
，
能快速构建新型关系抽取模型
、
高效扩充知识图谱
。


本文在上述背景下开展研究
，
目标是将现有知识库融入少样本关系抽取模型
，


利用知识来指导模型进行更加有效的关系抽取
。此外
，针对领域迁移背景
，本文


探索知识库与少样本模型的高效融合方式以及领域迁移下的训练策略
。通过将少


样本关系抽取模型与知识库的有效融入
，为新型关系抽取挑战提供了有效的解决


方案
，
为知识图谱扩充与领域知识图谱构建等现实应用问题提供便利
。


１


北京邮电大学工程硕士学位论文


１
．２
国内外研究现状


１
．２
．
１
知识图谱发展现状


知识图谱Ｗ是由谷歌在
２０
１２年提出
，通过将知识图谱融入搜索引擎来提升搜


索的答案质量
，
从而提高用户的查询效率
。通过知识图谱
，搜索引擎能够返回更


加准确的结构化信息
，
极大提升了用户的搜索体验
。


知识图谱是符号主义思想下的
一种产物
，
旨在希望以符号的形式存储知识和


理解知识
。知识图谱是在众多技术积累后的产物
，包括语义网
、万维网
、本体论
、


链接数据等
。
１９６０年语义网络
被提出来表示知识
，
利用节点表示对象
，边表示


关系
。语义网络能更简单地表示物理世界事物的关联
，但语义网络尚未有明确的


定义标准
。
１９６５年第
一个专家系统
Ｄｅｎｄｒａｌｄ
［３
］通过使用计算机符号表达知识库
，


并且利用推理机对知识进行处理与决策
，但该专家系统的知识库主要依赖专家手


工获取
、
构建规则
。
随着哲学理论应用的研宂深入
，
哲学的本体论
［４
］被用于专家


系统中
，
对知识进行概括性
、抽象性的描述
，
强调概念之间的关系
，
为知识的和


关系定义提供了理论支撑
。
１９８９年万维网
［５
］的技术被提出
，通过ＨＴＴＰ协议将网


页链接起来
，
以较灵活的形式将众多网页知识展现给用户
。
１９９８年语义网Ｗ被提


出
，
旨在希望以语义的形式来表达信息的含义
，机器是通过语义描述来读懂信息


数据
。但语义网缺乏严格的标准
，
并且随着发展导致技术栈过于复杂
，应用门槛


较高
。
２００６年链接数据＾被提出
，
并制定五星标准来保证其高效可持续的发展
，


链接数据通过设置
一定的标准将公开数据链接到
一起
，实现不同的知识间的转化
，


形成规模庞大的数据生态
。


知识图谱的构建包括两部分
，
实体抽取和关系抽取
。根据抽取顺序包括基于


Ｐ
ｉｐｅ
ｌ
ｉｎｅ？的方法和联合抽取
［９
］的方法
。
基于Ｐｉｐｅｌｉｎｅ的方法通过实体和关系的依


次抽取完成知识图谱的构建
，基于联合抽取的方法则是端到端直接抽取知识三元


组
。基于Ｐｉｐｅｌｉｎｅ方法首先通过实体识别任务得到句子中的实体
，
然后利用文本


和上阶段抽取到的实体
，判断文本中实体间的关系
，最终得到知识图谱中的三元


组
。早期的实体抽取方法依赖人工抽取或是构建规则
［
１１应用场景较为局限且费


时费力
。近年来
，基于统计学习的方法逐渐应用于实体抽取
，包括隐马尔可夫
ｔ
Ｉ
１
］
、


条件随机场
［
１２
］等模型
，取得了显著的性能提升
。随着深度学习的发展
，基于深度


学习的实体识别方法实现了更好的性能
。
Ｚｅｎｇ
［
１３＾ｊ用双向长短记忆网络和条件


随机场模型
，
相较于ＣＲＦ取得了显著的性能提升
。
Ｌ
ｉｕ
［
１４＾ｊ用卷积网络ＣＮＮ与


Ａｔｅｎｔｉｏｎ机制能够从相邻的字符获取更多的模式化信息
，并利用注意力机制实现


特征的有效加权
。
Ｇｒｅｇｏｒ
［
１５
］利用
Ｓｅｌｆ
－Ａｔｅｎｔｉｏｎ实现文本间的信息交互
，
Ｇｕ
［
１
＜ｓ
］将


预训练模型运用于实体识别任务
，通过大量的外部知识融入来提升实体识别性能
。


２


第
一章绪论


Ｓｏｕｚａ
［ｍ将预训练模型与条件随机场ＣＲＦ模块进行有效结合
，通过语义信息和模


式规则信息进行有效结合
。基于
ｐ
ｉｐｅｌｉｎｅ的方法需要利用关系抽取判断文本中实


体的关系
，从而实现非结构化文本到三元组形式知识的转变
，关系抽取也是本文


的研宂重点
。


１
．２
．２关系抽取研究现状


关系抽取是指根据文本描述
，
判断文本中两个实体之间的关系
。传统的关系


抽取任务利用高质量的标注数据来训练关系抽取模型从而完成关系抽取任务
。根


据模型的特点
，
传统的关系抽取模型分为基于特征
、基于核
、深度学习和联合抽


取等方法
。


基于特征的关系抽取模型通过特征编码器和分类器的结构进行建模
，
即需要


先提取对关系抽取的文本特征
，并利用编码器进行特征表示
，后续利用分类器对


特征进行关系分类
。众多工作针对关系抽取任务
，设计了有效的句法特征表征
［
１
８
］
＿


包括文本序列
、句法分析树
、依赖解析树等
，
并对这些特征的单独使用和组合使


用的有效性进行分析
。语义特征在关系表示上也有不错的表现
，包括文本的短语


特征
［
１９
］
、关系的关键词
［２Ｇ
］等
。此外
，利用模式匹配
根据不同关系类型设定规则


进行关系特征提取也是
一种有效的方法
。基于特征的关系抽取模型利用编码器将


特征进行符号化表示
，
然后利用支持向量机
、
线性回归等方法进行关系分类
。


基于核函数的关系抽取模型利用核函数来计算文本间的实体关系相似度
，
能


避关系表示特征通过规则显式抽取的问题
。
当关系以文本序列形式输入模型时
，


该类方法利用序列核
［２２
］来计算文本间关系的相似度
，序列的内容包括了词语
、词


性
、实体等类型
。
当使用语法树的来表示关系时
，该类方法利用统计解析模型
［２３
］


来计算抽取实体的关系
，或利用卷积解析核
［２４
］来计算两个语法树的相似度
，从而


表示文件间的关系相似度
。当使用依赖树表示实体关系时
，
依赖树中单词表示成


节点
，单词间的关系表示为边
，该类型方法利用核方法计算两个树整体的相似度


［２５
］
，也可以计算实体之间的最小树的相似度
［２６
］
，此外
，两个实体之间的最短路径


也可以用来计算文本间实体关系的相似度
。


基于深度学习的关系抽取模型通过将词向量技术与深度学习模型进行结合
，


通过大量的数据训练实现关系抽取任务性能的显著提升
。
该类方法通过词向量


［２７
，２８康示语义信息
，
利用ＣＮＮ
［２９
１
、
ＰＣＮＮ
［３
（）
］
、
ＬＳＴＭ＿等方法进行文本编码
，
并


利用分类器进行关系分类
。在文本编码过程中
，结合语义信息和位置信息
［３２
］能够


有效让模型关注到实体
，
利用注意力机制并结合序列模型
ＬＳＴＭ能够让模型关


注与关系更相关的信息编码
。
随着词向量技术的发展
，
利用预训练语言模型


ＢＥＲＴ
［３３
］对文本进行编码能够在语义信息中融入丰富的外部知识
。
此外
，
关系抽


取任务也可以嵌入联合学习
［３４
］中
，
该类方法将关系抽取和实体识别转化为成序


３


北京邮电大学工程硕士学位论文


列标注任务
，通过组合关系和实体标签
，通过序列标注实现实体和关系的同时抽


取
。


１
．２
．３少样本关系抽取研究现状


本小节主要介绍少样本关系抽取方法的发展现状
，
按照方法类型分为三类
：


基于元学习的少样本关系抽取方法
、基于预训练模型的少样本关系抽取方法
、基


于知识增强的少样本关系抽取方法
。


１
．
基于元学习的少样本关系抽取方法


基于元学习的少样本关系抽取方法会利用较多的关系类别数据进行元训练
，


然后在少样本任务上进行微调
。
元学习会从原始数据集构建
Ｅｐ
ｉｓｏｄｅ
，
每个


Ｅｐ
ｉｓｏｄｅ包括支撑集
Ｓｕｐｐｏｒ
ｔＳｅｔ和查询集ＱｕｅｒｙＳｅｔ。
支撑集
Ｓｕｐｐｏｒ
ｔＳｅｔ首先从


原始数据抽取Ｎ个类别
，然后每个类别抽取Ｋ条样本
；查询集ＱｕｅｒｙＳｅｔ在抽取


的的Ｎ个类别中每类别抽取
Ｌ条样本
。
每个
Ｅｐ
ｉｓｏｄｅ可以视为少样本情况下的


Ｎ分类任务
，元学习通过不断抽取Ｅｐ
ｉｓｏｄｅ进行训练
，模型获取到了多个Ｅｐ
ｉｓｏｄｅ


间的公有知识
。元学习得到的模型通常有较强的泛化能力
，能在不同任务上取得


较好的性能
，从而在新关系分类任务上进行快速迁移
。根据元学习的构建方式不


同
，
该类方法分为基于数据
、
度量
、
优化
、
语言模型等四种
。


基于数据的方法是通过数据增强方法来提升模型在少样本任务上的性能
。


Ｆａｂｂｒ
ｉ等人
利用与少样本任务相似的任务数据来进行训练
，然后在少样本数据


上进行训练
，
实现模型性能的提升
。
Ｒｅｎ等人
利用无标注数据进行半监督的训


练
，即先利用少样本模型和远程监督技术从无标注样本中选取置信度较高的样本
，


然后利用置信度高的样本作为新增的数据来训练模型
。
Ｘｕ等人
［３７
］利用语言模型


生成部分数据
，
实现少样本任务上的数据扩充
。


基于度量的方法是利用度量指标作为分类器
，减缓模型在少量标注数据上的


过拟合问题
。孪生网络
［３８
］利用双路神经网络进行样本编码
，通过构造不同标签的


样本对进行训练
，利用度量公式计算编码向量的距离来表示样本对的相似度
，推


理时也根据样本特征距离判断两样本是否属于同
一类别
。
原型网络
［３９
］通过计算


每个类别中所有样本编码后的平均向量
，将其作原型向量表示该类别
，通过计算


样本与多个类别原型向量的距离
，再经过
Ｓｏｆｔｍａｘ激活函数得到样本的类别概率
。


基于原型网络的想法
，
Ｇｅｎｇ等人
利用动态路由计算样本权重
，
通过加权求和


的方式得到类别的原型向量
，从而避免离群样本对于类别表示的影响
。
Ｇａｏ等人


对样本和特征等两个维度设计权重
，
减小了少样本任务对标注数据中的异常


值的敏感度
。
Ｄｈｉｌｌｏｎ等人
［４２
１利用类别的原型向量进行分类器的初始化
，
利用少


样本数据对其进行再次训练
。
针对于少样本的过拟合问题
，
Ｂｅｒ
ｔｉｎｅｔｏ等人
ｔ４３球Ｊ


４


第
一章绪论


用矩阵变化求取分类器的近似解
，并将该种近似求解的方法运用到元学习中
，获


得了更稳定且性能优异的分类器
。度量学习的计算方式有很多种
，包括欧式距离
、


余弦相似度等
，
不同的度量方法在不同任务上的性能可能会有较大的差异
。


基于优化的方法是通过学习少样本情况下的训练策略
，
包括优化路径或是模


型初始点
。ＭＡＭＵ
４４
］在元学习中利用二阶导的方向进行优化
，学习到不同Ｅｐ
ｉｓｏｄｅ


之间公有优化方向
，
其优化目标是得到更有潜力的模型参数
，
即在不同
Ｅｐ
ｉｓｏｄｅ


间实现快速调整
，
从而在真实少样本任务上快速拟合
。
１＾？
１如
［４５
］将Ｅｐ
ｉｓｏｄｅ的二


阶导近似为
一阶导
，并且简化了ＳｕｐｐｏｒｔＳｅｔ和
（＾
１＾１７３６１的划分
，来解决
１＼１八
］＾１


的二阶导优化导致训练速度过慢的问题
，在保证模型性能情况下极大加快了元学


习的训练速度
。Ｌｉ等人
［４６
］直接利用基模型输出参数用于少样本模型
，利用
ＬＳＴＭ


建模少样本的优化过程
，
从而实现少样本任务的快速训练
。


２
．
基于预训练模型的少样本关系抽取方法


基于语言模型的方法是通过语言模型学习到的知识来解决样本不足的问题
。


语言模型通过预训练任务在规模数据上进行训练
，大量的知识融入模型中
，合理


利用语言模型中的知识是该类方法的主要思路
。
Ｓｃｈｉｃｋ等人
［４７
］基于预训练语言


模型
ＢＥＲＴ提出了ＰＥＴ方法
，
通过设置模板将下游任务转换为预训练任务中的


ＭＡＳＫ词预测子任务
，减少了下游任务与预训练任务的差距
。Ｗａｎｇ等人
［４８
］针对


下游任务的每个类别设置文本模版表示该类别
，将下游的分类任务转换为预训练


任务中的文本对关系判断任务
，并且利用多语言模型能够实现相同任务的不同语


种间的迁移
。
该类型任务均大多需要设计文本模版
，
针对模版敏感问题
，
Ｌｅｓｔｅｒ


等人
［４９
］提出了模版的自动构建方法Ｐ
－Ｔｕｎｉｎｇ
，减少了任务上的人工损耗
。在大规


模语言模型上
，少样本甚至零样本任务可以完全依赖语言模型
，
ＧＰＴ３％］利用超


大规模数据以自回归的方式进行训练
，能够在没有数据或者少量数据下
，直接利


用自回归的结果进行类别判断或文本生成
。


３
．
基于知识增强的少样本关系抽取方法


将现有的知识库融入模型是提升少样本任务性能的有效方式
。
Ｑｕ等人
［５
１
］利


用现有的知识图谱作为知识库
，通过图编码技术得到关系的先验表示
，并利用少


量数据对关系的先验表示进行修正
，
在少样本关系抽取任务上性能提升明显
。


Ｓａｉｎｚ
［５２
］将实体和关系的描述作为知识库
，综合考虑文本间的语义信息和文本和


标签的语义关系来计算实体关系的相似度
。
Ｙａｎｇ等人Ｐ味」用实体的概念作为知


识库和开源的概念嵌入表示
，针对概念与实体的多对
一问题
，利用注意力机制得


到与实体更匹配的概念
。Ｚｈａｎｇ等人
将实体概念知识和实体描述融入原型网络


中
，通过元更新技术对关系表示进行少量修正
，并且验证了不同的知识对于模型


的影响情况
。
Ｂａｏ等人
将相关领域文本中的统计信息作为知识融入模型
，包括


５


北京邮电大学工程硕士学位论文


词频特征
、逆文档频率等特征
，利用ＬＳＴＭ编码器进行编码和岭回归近似求解
，


从而增强模型在少样本任务上的性能
。


１
．３主要研究内容


本文通过融入知识库信息来提升模型在少量标注数据下的关系抽取能力
。在


知识库类型上
，本文综合考虑知识的获取难度和覆盖率等因素
，选择实体概念作


为知识库
，并制定了知识库的获取方式和知识表示形式
。本文旨在针对不同类型


的知识
，设计泛化能力较强的知识融合模块
，能在不同领域的知识库上均实现有


效的知识融入
。此外
，
基于少样本关系抽取模型构建关系抽取系统
，
在领域图谱


构建和新型关系抽取任务上进行实际应用也是本文的研究目标
。


本文根据研宄目标
，
着重研宄以下两个方面内容
：


１
．
基于知识增强的少样本关系抽取与领域适配研究


少样本关系抽取任务通过将知识库融入模型
，
能够大幅度提升模型性能
。
本


文选择实体概念作为融入知识
，根据不同的知识形式需要设计泛化能力较强的知


识融入模块
，实现不同领域知识库的有效融入
，从而提升模型在领域迁移场景下


的少样本关系抽取能力
。此外
，
在目标领域已知的背景下
，
本文探索在元学习过


程中进行领域导向性训练
，获取与目标领域更相关的元知识
；并且利用样本对维


度的对比学习目标作为正则项
，
来提升模型的泛化能力
。


２
．基于实体概念增强的少样本关系抽取系统设计


新型关系的出现和领域关系建模是知识图谱抽取的重大挑战
，
为高效应对该


种情况
，本文基于知识增强的少样本关系抽取技术设计并实现了应用系统
。该系


统通过户管理
、数据管理
、
图谱管理等模块
，满足用户上传训练数据
、关系模型


构建和测试数据推理等任务
，
能够有效满足应用需求
。


本文的创新点为
：


１
．
设计了结合语义门控机制和距离打分器的知识融入模块
。


当实体概念知识以图形式表示时
，
本文首先利用图嵌入技术完成概念知识表


示
，
然后设计合适的知识融入模块
。本文的基础模型以文本对形式进行建模
，
在


融合模块中
，利用文本对的语义物理含义作为门控信号输入知识融入单元
；随后


基于图知识的特点
，
利用融入知识的语义向量的距离特征来对关系相似度打分
。


语义门控融合机制提高了知识特征和语义特征融合效果
，距离打分器则确保融合


模块在不同领域的知识库的泛化能力
。


２
．设计了元学习增强模块
，
实现稳定的领域导向性元训练
。


本文针对于目标领域已知的情况
，
设计了元学习增强模块
。元学习增强模块


包括领域导向性元训练和样本对维度的对比学习正则项
。领域导向性元训练通过


６


第
一章绪论


两阶段的元训练从训练数据中获取与领域更为相关的知识
，首次训练得到领域相


关性的打分器
，第二次训练时
，利用打分器对样本进行领域相关性打分
，并将打


分结果作为权重加入损失函数中
。对比学习正则项旨在约束元学习过程中的样本


差异过大问题
，
通过约束样本的特征空间
，
提高元学习过程中的稳定性
。


１
．４章节组织形式


论文分为
５章
，
各章节内容如下
：


第
一章首先介绍了开展本文研宄的背景和意义
，接下来回顾了知识图谱、关


系抽取和少样本关系抽取的研究现状
，在少样本关系抽取研宄现状中展示了基于


元学习
、预训练和知识增强等三类主流方法
，最后介绍了本文的研宄目标、主要


研宂内容和创新点
。


第二章介绍了本文中需要的基础知识及相关算法
，
包括少样本学习的定义
，


语言模型
，
少样本学习中的元学习
、
度量学习和ｐｒｏｍｐ
ｔ方法。


第三章详细描述了基于实体概念指导的少样本关系抽取模型
，包括概念知识


融合模块、元学习增强模块和相应的实验结果分析
。在概念知识融合模块介绍了


实体概念知识
、知识的映射和表示、文本形式和图谱形式概念知识的融入
。在元


学习增强模块
，
介绍了对比学习正则项和领域导向性训练
。


第四章介绍了少样本关系抽取系统的设计与实现
，
包括系统需求分析、
系统


概要设计、
功能模块设计和系统测试
。


第五章对本文工作和研究作了总结
，分析了目前己完成的工作内容和有待改


进的地方
。


７


北京邮电大学工程硕士学位论文


８


第二章基础知识


第二章基础知识


深度学习技术在少样本学习上扮演着重要的角色
，也是实现少样本关系抽取


系统上的技术基础
。在本节
，我们对深度学习中的预训练模型
、少样本学习
、知


识增强等技术的基础知识进行介绍
。


２
．
１语言模型


２
．
１
．
１ＣＢＯＷ


语言模型是判断文本序列概率的
一种方法
，现有的自然语言处理任务通常利


用语言模型获取文本的表征
。ＣＢＯＷ
［５６
］模型
，又称词袋模型是语言模型的
一种
，


通过相应任务学习文本的语义表征
。该模型通过设定中心词和窗口大小ｈ
，
利用


中心词的上下文２ｈ长度的语言环境来预测中心词
。


针对
一个中心词
，
首先生成大小为
２ｈ的上下文窗口
，
根据度热编码生成
２ｈ


个向量
，
中心词的左侧和右侧各
ｈ个
，
向量的长度为
｜ＫｏｃａＺ？
｜
。然后利用每个向


量与输入词矩阵Ｖ进行矩阵乘法计算
，
共获得２ｈ个词向量表示
。


Ｖｃ
－ｍ
＝Ｘｃ
＿ｍＶ
（２
－
１
）


然后计算２Ｌ个词向量的平均值
：


＿
ｙｃ
－
／ｔ＋
… ＋ｙｃ
－ｌ＋”ｃ＋ｌ＋… ＋”ｃ＋ｆ
ｔ（２２
）


将上述的窗口内词向量的平均值与输出矩阵Ｕ进行矩阵相乘
：


ｚ＝Ｕｖ
＇
ｃ
（２
－３
）


得到的
Ｚ表示了向量Ｗ与其他词的相似分数
，
分数越高表示与其他词的相似


度更高
，
随后通过
ｓｏｆｔｍａｘ激活函数将分数转换成概率分布
，
该分布表示中心词


的预测概率
。


ｐｃ＝ｓｏｆｔｍａｘ（ｚ）
（２
－４）


模型的优化目标是让中心词的概率分布与中心词的真实分布尽可能
一致
，利


用交叉熵损失作为优化的损失函数。


Ｗ（ｐｃ
，ｙｃ）
＝－
ｌｏｇ（ｐｃ）
）


９


北京邮电大学工程硕士学位论文


ＣＢＯＷ使用梯度下降算法优化模型
，通过最小化上述损失
，实现对模型参数


的优化更新
。
在优化过程中
，
输入词矩阵Ｖ和输出词矩阵Ｕ会不断被更新
，
最


后这两个矩阵会作为词的表征应用于下游任务
。


此外
，
与ＣＢＯＷ语言模型相似的
Ｓｋｉｐ
－Ｇｒａｍ
［５７
］模型则是通过中心词预测上


下文环境中的词
，也是通过输入词矩阵和输出词矩阵
，构建上下文词与中间词的


关系实现
，
最终得到矩阵Ｕ和矩阵Ｖ
。


２
．
１
．２ＥＬＭＯ


２０
１８年
，
Ｍａｔｔｈｅｗ
ｆ５８墙出了ＥＬＭＯ模型
，
ＥＬＭＯ对于词的表示是基于整个句


子的信息
，其中包含了词的语义和语法结构等内容
。
ＥＬＭＯ通过整句的编码
，
相


同的词在不同语境中的表达的含义也不
一致
。该模型可以作为语义模块
，快速嵌


入到不同自然语言模型中
，
包括情感分析
、
问答系统等多个自然语言任务
。


ＥＬＭＯ采用双向
ＬＳＴＭ结构
，对词的表示能够抓住上下文的语义
，其结构如


图
２
－
１所示
。
给定
一个序列（＇
，
７＾
，
．
．
．
７＾４
，７；）
，
ＥＬＭＯ从前后两个方向对文本进


行编码
，
左侧为词的上文表示
，
右侧为词的下文表示
。



—
＾



丨
＇
（
１
．ＳＴＭ
１ＳＴＭ

ＬＳＴ１！
Ｊ
；
丨
１
（
｜
．，）
＜
＿＾
丨、
｜
＇
｜
）
＜
ｌ＞
ＩＭ
Ｊ
）


！
＾
ｉＳ
Ｉ
ｌ
！
Ｊ

｜


Ｅ
，Ｅ，
…Ｅｎ


图
２
－
１ＥＬＭＯ模型结构
［５８
］


在前向语言模型中
，
句子的序列概率是通过前向文本序列的条件概率乘积计


算
，
具体的计算方式如下
：


ｐ｛ｔ
ｌ
ｔ
ｔ
２
，
．
．
．
，
ｔｎ）＝ＰＩｐ（Ｐｋ
｜
ｔ
ｉ
，
ｔ
２
－－
，
ｔｋ
－
ｉ）
（２
－６
）


丄
上ｆ
ｃ＝
ｌ


在后向语言模型中
，
句子的序列概率是通过前向文本序列的条件概率乘积计


算
，
具体的计算方式如下
：


ｐｆ
ｅ
．ｕｊ
＝ｐ（ｐｆ
ｃ
｜ｔｆ
ｃ＋１
，
ｔｆ
ｃ＋２
，
…
，
ｔｊ
（２
－７）


丄
丄ｆ
ｃ＝
ｌ


在
ＥＬＭＯ
中
，
句子中
ｋ位置的词在每个
ＬＳＴＭ均有表示
，
在最顶层处的


ＬＳＴＭ输出用于预测当前位置的词的表示
。
通过词的表示
，
结合
ｓｏｆ
ｔｍａｘ函数和


１０


第二章基础知识


输出词矩阵来预测下
一个词
。通过交叉熵损失最小化概率分布和真实分布间的来


优化语言模型
。


Ｌ／ＯＳＳ
Ｉ＾
７１）
一


Ｚ
ｋ＝ｎ
（２８
）


ｆ
ｅ＝ｉ
ｄ〇ｇ
（ｐ（ｐｆ
ｅ
｜
ｔｆ
ｃ＋１
，
…
，
＋１
，
，
…
，


在下游任务中
，
ＥＬＭＯ可以作为语义模块嵌入到相应的模型中
，对于每个词


ｔｆ
ｃ
，当ＥＬＭＯ中有Ｌ层双向ＬＳＴＭ
，结合最原始的输入词矩阵嵌入
，该词有
２Ｌ＋
１


个语义特征
。


￡
＂
ｋ
＝
｛＾〇
＜＾－
１
；ｈ－
ｉ
，
．
．
．
，ｈｉ
，
（２
－９
）


２
．
１
．３ＢＥＲＴ


２０
１８年谷歌ＡＩ团队发布了ＢＥＲＴ
［３３
】模型
，
在ＮＬＰ领域中开启了新的章程
。


ＢＥＲＴ在ＮＬＰ的诸多任务取得了惊人的成绩
，包括机器阅读理解
、ＧＬＵＥ基准
、


自然语言推理任务
。
ＢＥＲＴ模型采用
Ｔｒ
ａｎｓｆ
ｏｒｍｅｒ的编码器结构
，
采用两阶段的


训练方式来获得语言模型
。第
一阶段
，
在大规模无标注语料库上进行预训练
，来


获得无标注文本上通用的语义表示
。第二阶段
，针对与特定任务的数据集进行监


督训练
。
ＢＥＲＴ的编码器结构如图
２
－２所示
。


Ｔ１Ｔ２
－Ｔｎ


（ｆＴｒｎＴ
＾）（＾
ｒｎ＼）（
ＴｒｍＪ


图
２
－２ＢＥＲＴ模型结构
［３３
］


ＢＥＲＴ的预训练阶段任务包括两个任务
：
掩盖词预测任务和上下句关系预测


任务
。掩盖词预测的任务是指在无标注语料上随机掩盖部分词
，让模型去预测掩


盖位置的词
，通过最大化真实词的概率进行训练
。上下句关系预测是指随机抽取


文本对
，其中部分文本对在原文中是具有顺序关系的
，部分文本对是随机组合的
，


通过判断两个文本间的关系进行语言模型的训练
。


ＢＥＲＴ对于文本的表示
，
采用三种表征的加和
，
包括字表征
（Ｔｏｋｅｎ


Ｅｍｂｅｄｄｉｎｇ）
、段表征
（ＳｅｇｍｅｎｔＥｍｂｅｄｄｉｎｇ
）和位置表征
（Ｐｏｓ
ｉｔｉｏｎＥｍｂｅｄｄｉｎｇ
）
，


如图
２
－３所示
。上述的表征都是ＢＥＲＴ在预训练阶段得到
，在下游任务可以对这


些表征进行固定或者微小调整
。


１
１


北京邮电大学工程硕士学位论文


／Ｓ／

Ｓ／＼／
ｖ／＼／Ｓ／Ｓ／＼
／＼／＼／＼


＊ｎＰｕｔ
ｆＯｓ
ｊｍｙｄｏｇ
ｉｓ
！ｃｕｔｅ
［ｓｅｐｊｈｅ
Ｊ
ｉｋｅｓ
ｊｐ
ｌａｙ
ｊ＃＃
ｉｎｇ
｜
ｆｓｅｐ］
｜


Ｅｍｂｅｄｄ
ｉｎｇｓ
＾
！ＣＬＳ
）
Ｅ
ｍｙ
Ｅ
０〇〇
Ｉ
＾
？ｓ
Ｉ＾
ｃｕｔｅ
｜
巳
［ＳＥＰ
］Ｅ
ｈｅ
＾
ｌ
ｉｋｅｓ
Ｅ
ｐ
ｌａｖ己
＊
ｉｎｇ
＾
［５￡Ｐ
］
［


＋＋＋＋＋＋＋＋＋＋＋


Ｅ＾ｉｇｓ［ｘｉ
ｒ＾ｒ＾ｒ＾ｉｒ＾ｒ＾ｒ＾ｒ＾ｉｒ＾ｒ＾ｒ＾


＋＋＋＋＋＋＋＋＋＋＋


ＥｍＳ
ｉｎｇｓ＾ｆｊ＾］


图
２
－３ＢＥＲＴ文本表征
［３３
］


微调是利用下游的任务数据对模型进行调整
，
不同的下游任务的调整方式不


同
。
如图
２
－４所示
，
当下游任务输入为单句文本时
，
增加特殊字
［ＣＬＳ
］
、
［ＳＥＰ
］
，


直接输入ＢＥＲＴ模型
；
当下游任务输入为双句时
，
利用
［ＳＥＰ
］拼接两个句子
，
然


后再增加其他特殊字
。


Ｃｌａｓｓ
Ｃ
ｌａｓｓ


Ｌａｂｅ
丨
Ｌａｂｅ
ｌ


ｑｑｑＣＺＤＯＺＯＤ
■ＯＤ


ＢＥＲＴＢＥＲＴ


ｈＪＲｉ
．Ｒｎｒ＾ｊｒ＾ｉ
回
ｉ＾
ｉ
ｅ
，｜
ｅ
，
｜
ｌ匕
Ｉ


ＨＤ
＂■ＤＧ
Ｑ￣ＱＯ
－１
１


ｒ＾
ｉｆＹｉｒ＾ｒ＾
ｉｆ￥ｉ如
１

ｉ
ｉ放２
１
ｉ
：
ｉ
＂＾
￣￣


！
Ｔ
—
］


Ｓｅｎｔｅｎｃｅ１Ｓｅｎｔｅｎｃｅ２Ｓｍｑ
ｌｅＳｅｎｔｅｎｃｅ


图
２４ＢＥＲＴ下游任务微调
［３３
］


ＢＥＲＴ的单句和双句表示均在［ＣＬＳ
］字位置处输出
，然后在后续部分根据任务


特点增加分类器模块
，
利用下游任务的优化目标进行对分类器模块和
ＢＥＲＴ进


行整体调整
。


２
．２少样本学习


机器学习是计算机程序经过与任务Ｔ相关的经验Ｅ的学习
，实现了性能的提


升
，任务的性能体现在性能指标Ｐ中
。任务Ｔ在文本任务中包括文本分类
、实体


识别等任务
，
性能指标体现在准确率
，
召回率等指标
。
例如
，
文本分类任务
Ｔ
，


通过在大量的文本分类标注数据上学习经验
Ｅ
，
机器学习程序获得性能
Ｐ
的提


升
。


少样本学习是机器学习其中的
一种类型
，
主要针对数据量不足的问题
。少样


本学习定义与机器学习定义相似
，
均是利用标注数据学习到与任务
Ｔ相关的经


验Ｅ
，实现性能Ｐ的提升
。传统的机器学习的经验Ｅ是通过大量的标注数据进行


监督训练得到
，而少样本学习则是通过极为有限的监督数据进行训练
，并且需要


保证
一定的性能Ｐ
。


１２


第二章
基础知识


是
．
写的
一部小说
，
内容为唐僧取经的故
：４ｆ
．


西汉著名史学家
〔
：
撰写了
一部纪传体史书
；



出演了最新的动作片电影々
＿．


ｗ在电影
Ａ
，ｕ
中的演技很精彩
．


查询集４
：
、主演的电影票房人奕
．


图
２
－５少样本学习任务示例


少样本学习在分类任务中通常会有Ｎ
－ｗａｙＫ
－
ｓｈｏｔ的设定
，这是指少样本任务


Ｔ的训练集中包含Ｎ个类别
，
每个类别上有Ｋ条标注样本
。
一般情况下
，
Ｎ和


Ｋ的都较小
；
当Ｋ为
０时
，
就是Ｚｅｒｏ
－ｓｈｏｔ问题
；
当Ｋ为
１时
，
就是Ｏｎｃ
－ｓｈｏｔ问


题
。
此外
，
少样本任务也有对应的测试集
，
称为查询集
，
当训练集为
Ｎ
－ｗａｙＫ
－


ｓｈｏｔ情况时
，测试集的标签是包含在这Ｎ个类别的标志前集合中的
。如图
２
－５所


示
，
这是
２
－ｗａｙ２
－ｓｈｏｔ的任务示例
，
即支撑集中有两个类别
，
每个类别包括两条


样本
，
查询集的数据类别包含于支撑集数据的类别集合中
。


２
．３
元学习


元学习是少样本学习的
一种训练策略
。在元学习中
，少样本学习的数据集Ｄ＝


｛Ｄ
ｔｒ
ｅ
ｉｎ
，Ｄ
ｓ
，Ｄ
ｔｅｓｔ｝包括训练集
、支撑集和测试集
。测试集Ｄ
ｔｅｓｊＰ支撑集０
５是在少样


本情况下用于测试和训练的数据
。训练集Ｄ
ｔＩ
＿
ａ
ｉｎ是与少样本训练集Ｄ
ｓ类似的数据
，


在标签集合层面
，
两者集合没有交集
；
在数据量层面
，
训练集Ｄ
ｔＩ
＿
ａｉｎ比少样本训


练集Ｄ
ｓ庞大很多
。


元学习的思想是通过训练集Ｄ
ｔｒ
ａ
ｉｎ进行元训练
，
通过将训练过程中的经验以


先验知识的形式存储进模型
，在少样本任务上能够实现快速的迁移
。元学习从训


练集Ｄ
ｔｒ
ａ
ｉｎ构建
ｅｐ
ｉｓｏｄｅ进行训练
，
Ｅｐ
ｉｓｏｄｅ是由支撑集和查询集构成
。
当真实少


样本的任务训练集〇
５是Ｎ
－ｗａｙＫ
－ｓｈｏｔ设定时
，
其中支撑集会从数据Ｄ
ｔｍ
ｉｎ中随机


抽取Ｎ个类别
，
每个类别抽取Ｋ条数据得到支撑集
，
每个类别Ｌ条数据作为查


训集
。通过上述构建方式
，
每个
ｅｐ
ｉｓｏｄｅ是少样本Ｎ分类任务
，
其中支撑集用来


构建与任务极其相关的模块
，
如分类器
；查询集用来测试当前少样本任务的模型


效果
，
并且以损失的形式优化任务的公有部分
，
如编码器
。元学习的通过不断构


建
ｅｐ
ｉｓｏｄｅ
，学习到Ｎ分类任务间的公有知识在新的少样本任务上实现快速迁移
。


在真实少样本任务中
，直接利用支撑集Ｄ
ｓ进行分类器的训练
，而编码器部分则是


复用元学习得到的编码器
。
与机器学习的设置类似
，
少样本学习构建
ｅｐ
ｉｓｏｄｅ的


时候应该考虑少样本任务０
５的Ｎ和Ｋ设置
，
例如当少样本任务是
５
－ｗａｙ５
－ｓｈｏｔ


时
，
那么在元学习时构建
ｅｐ
ｉｓｏｄｅ的支撑集最好也保持
５
－ｗａｙ５
－ｓｈｏｔ
。


１３


北京邮电大学工程硕士学位论文


２
．４度量学习


度量学习利用度量公式直接计算样本相似度
，
避免了参数优化的过程
，
是目


前元学习中最为通用的方法
。度量学习在元学习过程中得到样本与特征空间的映


射函数
，利用度量公式计算特征点的距离表示关系相似性
，同类的样本距离更近
，


不同类的样本距离更远
。


２
．４
．
１
孪生网络


２０
１
５年Ｋｏｃｈ
［３８
］提出孪生网络来解决少样本图像分类问题
。孪生网络有两个


输入；＾和巧
，两个输入经过多层神经网络来进行特征提取与编码
，得到两个编码


心和６２
，
其网络结构如图
２
－６所示
。


Ｉｎｐｕｔ
！
ｊＯ
ｔ
ｉｔ
ｊｍｔ


ｌａｖｅｒ
ｌａｙｅｒ
ｌａｖｔ
ｉ
ｒ
ｌａｙ
ｔ
＾ｒ


：
：
、
＇
、，＇


八，
火５
丨


｛
ｖ
３Ｔ
－
，

＊ｖ
Ｉｊ
— ￣＊
｛
Ｕ
－
１
．Ｓ
－
ｊ


、
、一


图
２
－６
孪生网络


利用计算６
１和６２的绝对值距离
，
再经过
Ｓｉｇｍｏｉｄ函数得到两个向量的距离
，


当距离
ｄ越小
，
则表明输入的两个样本越相似
。


ｄ（ｅ１
，ｅ２）
— Ｓｉｇｍｏｉｄ（
＼ｅ
１
—
ｅ２
｜）
（２
－
１０
）


孪生网络在训练时
，
会将训练集的类别数据抽取成正负样本对的形式
，
正样


本表示两者的类别相同
，负样本对表示不
一致
，然后送入孪生网络得到样本对的


距离
，
随后利用交叉熵损失进行优化
。


Ｌ（ｘ
［
ｌ
＼ｘ
＾
ｌ）
）
＝
ｙ＾
．ｘ＾ｙ
ｏｇｐｆｘ＾
．ｘ＾）
＋


（１
－
ｙ（
ｘ
［
ｌ
＼
＾
°
））
ｌｏｇ（１
－
ｐ（
ｘ
｛
°
，ｘ
＾
°
））


孪生网络在训练阶段通过构建样本对学习样本到特征的映射函数
，在少样本


任务的测试阶段
，计算预测样本和标注样本特征空间的距离
，选择与距离最小的


样本的类别作为预测类别
。通过直接利用绝对值计算相似度的方式
，避免了在新


类别上重新训练分类器导致的过拟合问题
。


１４


第二章
基础知识


２
．４
．２原型网络


２０
１７年
Ｓｎｄ
ｌ
ｆ３９
］等人提出原型网络来解决少样本问题
，
原型网络也是度量学


习的
一种方式
，利用欧式距离来衡量样本和类别原型的距离
。原型网络在少样本


中的结构如图
２
－７所示
。


图
２
－７
原型网络
［３９
］


给定训练集Ｄ＝（ｘ２
，ｙ２））
，
首先通过编码器／＾得到每个样本的特


征向量
，
然后计算每个类别的
ｐｒｏｔｏｔｙｐｅ向量来表示该类别
。类别的
ｐｒｏｔｏｔｙｐｅ向


量计算方式如下
：


Ｃｋ
＾
Ｙ
ｋＸ々（々）
（２
－
１２
）


Ｋ
（Ｘ
ｉ
，ｙｉ）ｅＳｋ


通过上述方式计算所有类别的
ｐｒｏｔｏｔｙｐｅ
向量
，
然后计算测试样本与所有类


另
ｌ
ｊｐｒ
ｏｔｏｔｙｐｅ向量的欧式距离来得到所有的类别得分
，
最后通过
ｓｏｆ
ｔｍａｘ激活函


数来得到类别概率
。


＾
－
１３
）


Ｉ；ｋ
．
ｅｘｐ（
－ｄ（／＾
，（ｘ）
，ｃｆ
ｃ〇）


在元学习中
，
利用支撑集来计算每个类别的原型向量
，
查询集通过上述方式


得到对应的类别概率
。元训练时
，利用交叉熵损失优化模型
，如下所示
。测试时
，


直接选择上述类别概率最大类别作为相应的预测类别
。


Ｋ０）
＝
－
ｌｏｇ
（ｐ０〇
＝ｆ
ｃ
’
｜ｘ））（２
－
１４
）


２
．５Ｐｒｏｍｐ
ｔ方法


自然语言处理任务随着预训练语言模型的研究深入
，
从原有的特征工程
、模


型结构方式转变成基于预训练与微调范式
。在这
一范式下
，语言模型在预训练过


程中
，获取了大量的文本知识
，在下游任务中通过引入额外的参数来建模不同任


１５


北京邮电大学工程硕士学位论文


务
，
并使用对应任务的损失函数对模型进行微调
。在预训练微调的模式下
，
需要


根据任务的特性对语言模型増加部分额外参数
，该方式需要较多的标注数据进行


学习才能保证模型的性能
。


Ｐｒｏｍｐ
ｔ方法是通过重新修改下游任务形式
，使下游任务与预训练任务更为相


似
，希望利用预训练期间的模型结构就能完成下游任务
。该方式通过选取合适的


ｐｒｏｍｐ
ｔ来进行任务形式修正
，经过少量数据训练就可适应下游任务
。该类型方法


的性能与ｐｒｏｍｐ
ｔ制定有较大的关系
，
需要选择合适的ｐｒｏｍｐｔ才能有效利用语言


模型解决下游任务。在本节会介绍两种经典的
ｐｒｏｍｐ
ｔ方法
，
ＰＥＴ和ＥＦＬ
，
分别


将下游任务转化为掩盖词预测和文本对预测的两种预训练任务
。


２
．５
．
１ＰＥＴ


ＰＥＴ（Ｐａｔｅｒｎ
－Ｅｘｐ
ｌｏｉｔｉｎｇＴｒａｉｎｉｎｇ）是Ｓｃｈｉｃｋ
［４７】等人在２０２１年提出的方法
，


ＰＥＴ是
一种基于ｐｒｏｍｐｔ的方法
，通过设置模版
，将下游任务转化为ＭＡＳＫ词预


测预训练任务。
ＰＥＴ可以看作为
一个完形填空任务
，
函数ｐ为定义模版
，
ｘ作为


模版的输入
，
输出ｐ〇ｃ）为模版掩码标记位置的输出
（
［ＭＡＳＫ
］
）
。
该任务需要设


计词和标签的映射器ｖｅｒｂａｌｉｚｅｒ
，
即映射函数ｖ：Ｌ
－＞Ｖ
，
其中Ｌ表示标签集合
，
Ｖ


表示词表
。给定输入；ｃ
，
本文将标签
的分数表示为
：


ｓｐ（ｌ
＼ｘ）
＝Ｍ（ｉ７
（
／）
｜ｐ（ｊｃ））（２
－
１５）


在标签ｆ和输入＞：
，
利用语言模型Ｍ得到对应标签的分数
，
再利用激活函数


ｓｏｆｔｍａｘ函数得到标签的概率分布
。


ｅ
ｓ
Ｐ｛
ｌ
＼ｘ
）


＝／．
（
｜
＼（２
－
１６）


ＰＥＴ在少样本任务中的训练
，利用标签的概率和真实标签的计算交叉熵损失


心￡
，
计算语言模型的随机掩码语言模型损失
，
并且最后利用两者的加权损


失优化模型
，
损失函数如下所示
。


Ｌ＝（ｘＬｃｅ＋〇
■
—
〇ｃ）Ｌｍｌｍ（２
－
１７）


如图
２
－８所示
，
模版函数为
“这是［ＭＡＳＫ
］的情感
”
，
映射函数ｖ为
“积极— 正


向
，
消极— 负向
”
。


１６


第二章基础知识


＜
预测
：
电影太好看了
！
￣
（正向
：
〇
＿８
、


模板
：
这是＿的情感
。负向
：
０
．２


＼／Ｖ
，Ｊ


ＩＴ
＇


＾Ｈｉｔ
－〇
：
＇
＇


Ｖ
／ｖ
／


图
２
－８ＰＥＴ模型


ＰＥＴ方法构建多个
（ｐ
，ｖ
）映射来解决模版敏感问题
，然后通过多个模板的集


成提升模型性能
，
并选取置信度高的标签作为伪标签
。具体而言
，
ＰＥＴ有三个步


骤
：
（
１
）对于每个模版（ｐ
，ｖ
）使用小的训练集进行预训练模型的训练
；
（２
）对于


未标注的数据集
，通过集成上述多个预训练模型得到未标注数据的伪标签
；
（３
）


利用数据集的伪标签训练
一般的模型
。此外
，集成的时候可以根据不同的模版映


射ｐ
，
设置相应的权重
，
对集成的分数进行加权
。模型的加权分数是采用语言模


型计算得到
。


２
．５
．２ＥＦＬ


２０２
１年Ｆａｃｅｂｏｏｋ提出ＥＦｌＪ
４８
］（ｅｎｔａｉ
ｌｍｅｎｔａｒｅｆｅｗ
－ｓｈｏｔ丨ｅａｒｎｅｒ）
，将少样本文


本分类任务转换为文本对关系预测的预训练任务
。针对每个类别设定相应的模板
，


通过计算所有类别模版与句子的逻辑关系来计算句子的类别概率
。
输入文本
Ｘ
，


每个类别标签／通过模版函数ｐ构成标签模板ｐ
（Ｘ
）
。
ＥＦＬ利用语言模型Ｍ来计算


输入文本
ｘ
和每个模版的逻辑关系
。
句子对的逻辑关系集合
Ｓ
为


｛ｅｎｔａｉＺｍｅｒＵ
，ｎｏｔｅｎｔａＺＺｍｅｎｔ
｝
〇


Ｓ
ｐＧＷ
＝Ｍ（ｐ（
Ｚ
）
，ｘ）
（２
－
１８
）


通过计算输入句子
ｘ
与每个标签的ｐ（
〖）的逻辑关系
，
选择逻辑关系为


ｅｎｔａｉＺｍｅｎｔ概率最大的标签作为预测值
。


■
ｐｒｅｄｉｘ）
＝ａｒｇｍａｘ
ｌｅＬｓ
ｐ
｛ｌ
＼ｘ）（２
－
１９
）


在少样本任务的训练中
，
ＥＦＬ将所有的标签模版和文本进行拼接
，
然后根据


句子的真实标签标签
，得到句子对的逻辑关系标签
。当句子与模版的标签
一致时
，


两者的逻辑关系是ｅｎｔｃ＾ｍｅｎｔ
；
当句子与模版的标签不
一致时
，两者的逻辑关系


是ｎｏｔｅｎｔａｉｈｎｅｎｔ
。通过句子对的逻辑标签和模型的预测值使用交叉熵损失来训


练逻辑关系判断模型
。如图２
－９所示
，在新闻分类任务中对每个标签构建了模版
，


通过计算输入文本与各个模版的关系来进行训练和预测
。


１７


北京邮电大学工程硕士学位论文


，
—＾


Ｓｏｆｔｎｉａｘ＊
｜Ｅｎｔａｉ
ｌ
ＩＦＬＹＴＥＫＴａｓｋ
ｉ


－▲
，ｌ
ｉ
］


／
、


ＭａｃＢｅｒｔ


！


Ｌ▲
．


１ｆ
ｉ
Ｐｒｅｄｉｃｉ


，
、＾
＾

｛
—

—
打车


ｒ
■
＞（＾
、
（Ｎ
（＼
＂
Ｎ
！
Ｊ＾
Ｊ
Ｊ


ｉｒＩＳ１堠足车拼车出行利Ｓ
．
１这教应用是关于这敖应用是关于这农疫抝是关于
丨


１Ｊ紆趿上下班
丨ＳＥＰ
〗
；打车
ｌＥＯＳ
ｊ小说
ｌ
ｌｉＯＳ
Ｊ电商
｜ＥＯＳ
丨
Ｉ
…｜


Ｏｒ
ｉａ
ｉｎＳａｉｔｃｏｃｅＬａｂｅ
ｌＤｅｓｃｒｉｐ
ｔ
ｉｏｎｓ


ｖ
＇— 一
＾）


Ｔｅｘｔｕａ
ｌＥｉ
ｉｔａｉｈｎｅｎｔ


图
２
－９ＥＦＬ模型


ＥＦＬ方法通过将文本分类任务转化为句子对的逻辑关系预测任务
，
由于句子


对的逻辑关系集合是较为固定的
，因此可以利用公开的句子关系逻辑判断数据集


进行中间训练
。
在
ＥＦＬ训练模式中
，
首先通过逻辑关系判断数据集对模型进行


中间训练
，然后利用少样本数据和标签模板构建句子对的逻辑关系判断任务
，最


后利用模型和标签模版对文本类别进行预测
。


２
．６本章小结


本章主要介绍了深度学习和少样本学习的基础知识
。首先我们介绍了语言模


型的相关研究进展
，
包括了Ｃ
－ＢＯＷ、
ＥＬＭＯ和
ＢＥＲＴ
；
然后介绍了少样本学习


的定义和训练方法元学习
，
以及少样本学习中的经典方法
，度量学习
；最后还介


绍了基于预训练模型的ＰＥＴ方法和ＥＦＬ方法
。我们对上述方法的原理进行阐述
，


并对部分方法的特点进行了简要分析
。


１８


第三章
实体概念指导的少样本关系抽取模型


第三章实体概念指导的少样本关系抽取模型


３
．
１
引言


少样本关系抽取任务需要利用少量标注样本训练模型的关系分类能力
，极大


减少了数据的标注成本
。该任务由于缺少充足的标注数据
，会导致模型有较大的


过拟合风险
。现有的研究工作大多基于元学习来解决该问题
，元学习能够从大量


关系类别的标注数据中获取较为通用的元知识
，通过元知识来指导模型在少样本


场景下的关系分类能力
。但在领域迁移场景下
，某领域的关系抽取任务和元学习


使用的关系类别数据差异较大
，元学习得到的元知识对目标领域上关系抽取任务


的帮助极为有限
。因此
，在领域迁移情况下或测试集与训练集差异较大的情况下
，


传统的元学习很难获得性能良好的少样本关系抽取模型
。


为了增强模型在领域迁移下的少样本关系抽取能力
，将目标领域的知识库融


入模型是
一种有效的方案
。
目前知识库融入模型的相关研究大多针对通用领域的


知识
，并且只关注到知识的有效性
。在实际应用中
，知识的覆盖率以及获取成本


是需要考虑的问题
；知识融入模块的通用性也是不同领域的知识库能够有效融入


模型的保证
。此外
，在目标领域已知的情况下
，在元学习过程中获取与目标领域


更相关的元知识也是提高领域迁移下的少样本关系抽取模型性能的方法
，这也是


本文研究的问题
。


本文针对上述问题进行探索
，
选择ＢＥＲＴ
－ＰＡＩＲ作为基础模型
，
并将实体的


概念作为知识库融入基础模型
。概念的更新速度较慢
，
一定程度上保证了知识的


覆盖率
。实体概念知识有多种表示形式
，本文针对文本标签和概念图谱两种常见


形式进行研宄
，针对不同形式的知识设计了相应知识融入模块
。在已知目标领域


的元学习建模问题中
，本文设计领域导向性的元学习
，通过置信度对元学习的优


化目标进行加权
，提高领域相关数据的权重
。此外
，本文利用了句子对维度的对


比学习优化目标作为正则项
，
通过限制模型的特征空间来增强模型的泛化能力
。


为了验证模型的有效性
，
本文在少样本关系抽取公开数据集
ＦｅｗＲｅｌ上进行


实验。实验结果表明
，融入实体概念知识对基础模型Ｂｅｒ
ｔ
－Ｐａｉｒ的性能有着明显的


提升
；本文还对模型中各组件进行了有效性分析
，证明了模型设计的有效性以及


组件参数选择的合理性。


１９


北京邮电大学工程硕士学位论文


３
．２整体框架


本文旨在将实体概念作为知识库融入模型中
，模型的输入包括待抽取实体关


系的的非结构化文本
，
以及包含概念信息的知识库
。
首先利用基础模型
ＢＥＲＴ
－


Ｐａｉｒ将非结构化文本信息进行编码得到语义表征
；
然后利用概念知识融合模块
，


将外部概念知识库融入基础模型
；在训练模块
，根据训练情况选择元学习增强模


型方法来进行领域导向性训练
。


念信息
￣，
｜
概念表示
｜


＾
ｉ
｜
两段式训练


Ｉ
概念链接


Ｃ文本信息
—？
基础模型
—？－Ｖ
－— —？


＾—＊—
｜
对比学习正则


—概念融入


概念知识融入模快元学习增强模型


图
３
－
１
模型整体框架


将概念知识融入模型时
，
包括概念表示
、
概念链接和概念融入三个部分
。
概


念表示根据知识形式选择不同的编码方式
，本文使用的知识形式包括概念图谱和


文本标签两种
。概念链接是指将实体和概念进行匹配
，通过非结构化文本中的实


体链接到知识库对应的概念知识
，知识库中实体通常对应多个概念
。概念融入是


指将概念知识融入基础模型中
，将文本表征和知识库进行编码融合
。本文根据不


同的知识表征设计了不同的知识融入模块
，知识融入模块需要考虑知识库和文本


信息的模态差异和不同领域知识库的表征差异
。概念知识融入模块能将概念知识


有效融入文本表征
，
并且在不同领域知识库间实现较好的迁移
。


元学习增强模型是指在元学习训练过程中
，通过两段式训练和对比学习正则


项来提升少样本关系抽取模型的性能
。在己知目标领域的情况
，通过两段式训练


模式实现对于目标领域的导向性训练
。在元学习过程中
，利用样本对维度的对比


学习优化目标作为正则项实现对文本表征空间进行约束
，
增强模型的泛化能力
，


并增加元学习过程的稳定性
。元学习增强模型中的领域导向性训练可以根据应用


的具体情况进行选择
。


将概念知识融合模型和利用元学习增强模块能够将外部概念知识信息融入


模型中
，
以及领域导向性的训练方法
，能够在领域迁移的少样本关系抽取任务上


实现较好的性能
。本文的模型的优化思路主要是针对于现实应用的关系抽取任务


中可能出现的问题
，在后续的少样本关系抽取系统上
，该部分的模型设计也提高


了模型的落地能力
。


２０


第三章
实体概念指导的少样本关系抽取模型


３
．３基础模型


３
．４
．
１ＢＥＲＴ
－Ｐａｉｒ模型


ＢＥＲＴ
－Ｐａｉｒ模型是预训练语言模型中的
一种微调形式
，
通过将两条文本拼接


构成文本对
，
利用
一些特出字段［ＣＬＳ
］和
［ＳＥＰ
］对文本进行格式转换
；
随后将转换


格式后的文本对输入到ＢＥＲＴ编码器中进行编码
，
在［ＣＬＳ
］位置会得到文本的语


义表征
；文本对的语义表征通常是两条文本的语义关系
，最后利用语义表征进行


下游任务的建模
。
在本文中
，
ＢＥＲＴ
－Ｐａｉｒ模型如图
３
－２所示
，
输入是描述两者实


体关系的文本
，在
［ＣＬＳ
］位置处输出两个句子中实体的关系是否
一致的语义特征
，


利用语义特征进行两句中实体关系是否
一致的二分类任务
。此外
，
ＢＥＲＴ
－Ｐａｉｒ的


输入进行格式转换时
，
本文利用
［ｕｎｕｓｅｄ
］字来标记实体的位置
。


□


ＢＥＲＴ


□□
□
□
□
□
□


［ＣＬＳ
］
．
．
．Ｅ
ｌ
．
．
．Ｅ２
．
．
．［ＳＥＰ
］
．
．
．Ｅ３
．
．
．Ｅ４
．
．
．［ＳＥＰ
］


图３
－２ＢＥＲＴ
－ＰＡＩＲ关系抽取模型


少样本关系抽取模型在元学习训练时
，
会构造大量Ｅｐ
ｉｓｏｄｅ
，每个Ｅｐ
ｉｓｏｄｅ包


括支撑集
Ｓｕｐｐｏｒ
ｔＳｅｔ和查询集ＱｕｅｒｙＳｅｔ
。
针对查询集的某条数据＆
，
本文将其


与支撑集
Ｓｕｐｐｏｒ
ｔＳｅｔ的所有数据进行拼接得到文本对
，
并且利用
ＢＥＲＴ
－Ｐａｉｒ对


文本对实体关系的相似度Ｓｉｍ进行打分
。数据＆类别打分是将数据＆与支撑集Ｓ中


某个类别的所有数据＆相似度打分的加和
。


Ｓｃｏｒｅ＾ｑｉ）
＝
）Ｓｉｍ＾ｑｉ
．Ｓ
ｉ）（３
－１
）


数据＆通过上述方法计算各类别的相似度得分
，
然后利用
Ｓｏｆ
ｔｍａｘ激活函数


得到数据Ａ的类别概率分布
，然后使用查询集上的标签和类别的概率分布通过交


叉熵损失优化模型
。


ｎ二
ｅｘｐ（ＳｃｏｒｅＣｃＪｑｊ））〇２
）


ＰｌＳｍ￡ｃｅｘｐ（Ｓｃｏｒｅ〇上））


ＢＥＲＴ
－Ｐａｉｒ模型的输入为文本对
，输出为两句话的实体关系
一致的打分
。Ｂｅｒ
ｔ


模型通过多层
Ｓｅｌｆ
－ａｔｅｎｔｉｏｎ机制对文本对间的语义关系进行信息交互
，相较于原


２
１


北京邮电大学工程硕士学位论文


型网络等模型
，语义表征更为准确
。此外
，本文在ＢＥＲＴ
－Ｐａｉ
ｉ
？中利用特殊字来标


注实体的位置
，
也能让模型更加关注到文本中的实体的信息
。


３
．４
．２ＢＥＲＴ
＿Ｐａｉｒ（Ｐｒｏｍｐ
ｔ
）模型


Ｐｒｏｍｐ
ｔ方法通过设定模版将下游任务转化为语言模型的预训练任务
，该种方


式通过减少下游任务和预训练任务的差距
，从而提升少样本关系抽取模型的性能
。


在ＢＥＲＴ
－Ｐａｉｒ模型中
，下游任务是判断输入文本对中实体关系
一致性的二分类任


务
。
如图
３
－３所示
，
本文利用模版将下游任务转换为预训练的ＭＡＳＫ词预测任


务
，
利用ＭＡＳＫ字的预测情况得到文本对的实体关系表示情况
。
模版的位置处


于两个句子之间
，通过将模版和输入文本对拼接输入语言模型ＢＥＲＴ中
，并使用


ＭＡＳＫ词的语义表征进行关系
一致性的二分类判断
。


ＢＥＲＴ


□□□ｆｐｃｉｍ
）□□□


［ＣＬＳ
］
．
．
．Ｅ
ｌ
．
．
．Ｅ２
．
．
．Ｅ
ｌＥ２
［ＭＡＳＫ
］
Ｅ３ＥＡ
．
．
．Ｅ３
．
．
．Ｅ４
．
．
．［ＳＥＰ
］


图３
－３ＢＥＲＴ
－ＰＡＩＲ
（Ｐｒｏｍｐ
ｔ）关系抽取模型


在元学习训练过程中
，
ＢＥＲＴ
－Ｐａｉｒ（Ｐｒｏｍｐ
ｔ）的训练方式与ＢＥＲＴ
－Ｐａｉｒ模型基本


一致
；将查询集的每条数据与支撑集的所有数据进行拼接
，将模版和文本对按照


上述格式输入模型中
，
在ＭＡＳＫ字位置输出实体关系的语义表征
，
本文直接利


用ＭＡＳＫ词的语义表征进行二分类
，并未选择映射器
ｖｅｒｂａｌ
ｉｚｅｉ
？将文本和类别进


行映射
。ＢＥＲＴ
－Ｐａｉｒ（Ｐｒｏｍｐ
ｔ
）模型针对单条文本仅有两个类别
，即
“关系
一致
”与
“关


系不
一致
”
，
在元学习过程中能够学习到较鲁棒的类别映射
，
因此本文没有选择


映射器
ｖｅｒｂａｌｉｚｅｒ将文本和类别进行映射
。


表３
－
１ＢＥＲＴ
－Ｐａｉｒ（Ｐｒｏｍｐ
ｔ
）糢版集合






ｒｅｌａｔ
ｉｏｎｂｅｔｗｅｅｎｅ２
ｉｓ［ｍａｓｋ
］ａｓｒｅ
ｌａｔｉｏｎｂｅｔｗｅｅｎｅ３ａｎｄｅ４


ｒｅｌａｔ
ｉｏｎｅ１ａｎｄｅ２［ｍａｓｋ
］ｅ３ａｎｄｅ４


ｅｉｅ２［ｍａｓｋ
］ｅ３ｅ４


ｅ１ｅ２ｅ
３ｅ４［ｍａｓｋ
］


本文验证了多种模版对于知识融入模型效果的影响
，
如表
３
－
１所示
，
模版集


合中包括描述较详细的文本或较为简洁的文本
，其中＾和６２为模版前句子的头实


体和尾实体
，
６３和６４是模版后句子的头实体和尾实体
。


２２


第三章
实体概念指导的少样本关系抽取模型


３
．４概念知识融合模块


３
．４
．
１
实体概念知识


概念是人类认知过程中
，
将感知的事物进行抽象和概括
，
从而形成概念式思


维惯性
。在少样本关系抽取任务中
，文本需要根据语义判断实体的关系
，
少量标


注样本中的实体信息是极为稀疏的
，模型很可能学习到有偏的实体关系表示
，导


致模型的泛化能力极大降低
；此时
，实体概念可以作为
一种有效的知识融入来引


导模型做出合适的关系判断
。
如图
３
－４所示
，
ＷｉｋｉＤａｔａ包括本体
（Ｏｎｔｏ
ｌｏｇｙ
）
图


谱和实体
（Ｉｎｓｔａｎｃｅ
）
图谱两个维度的信息
，
利用实体对应的本体
（Ｏｎｔｏｌｏｇｙ
）作


为概念融入模型时
，以
“ ＢａｒａｃｋＯｂａｍａｗａｓｂｏｍ
ｉｎＨｏｎｏｌｕｌｕ
”为例
，
“ ＢａｒａｃｋＯｂａｍａ
”


的概念为
“
Ｐｏｌｉｔｉｃｉａｎ
” 和
“
Ｐｅｒｓｏｎ
”
，
“Ｈｏｎｏｌｕｌｕ
” 的概念为
“
ｃｉｔｙ
”
，
“
ｗａｓｂｏｍｉｎ
”关系从


两个
Ｉｎｓｔａｎｃｅｌｅｖｅｌ的实体抽象到Ｏｎｔｏｌｏｇｙ的关系
，
显然Ｏｎｔｏｌｏｇｙ维度的表不包


含更多的信息
，
该种方式得到的建模结果更具泛化能力
。


Ｏｎｔｏ
ｌｏｇｙ
－ｖ
ｉｅｗＫｎｏｗ
ｌｅｄｇｅＧｒａｐｈ


Ｃ＾Ｐｅｒｓｏｎ＾
）Ｒａｃｅ


” ，—
Ｍ＼＾＾Ｊ〇ｃａｕｆ＼＾


Ｃ＾
ＳｉｎｇＣ
Ｔ＾
）（＾Ｐｏｌ
ｉｔｉｃｉ＾）
—
；：
，了

ＳｔａｔｅＣ＾
ｉｉｖｅｒｓｉｔｖ＾）


ｆ／＇
’
、
？
？
？
？
、／


：
：
＇
＇
．
．
．
？
？
？
、
，／


：
：
？
＇
、似— Ｃ＾ｏｎｏｌｕｈＴ＾
）＾
ＮｅｗＹｏ＾ｒ＾／


Ｚ
°
＿ｈ；Ｊｏｃａ：ｅｄＪ
／＼


＾
ｙ＾ＯＴ＾＾Ｑ＾
＾ａｌｂｕｍ
＼＼
：


ｒ
？
▼
ｖｓ＾Ｕｎｌｖｅｒａｔ＾ｘ
／／


Ｃｏｎｃｅｐｔ
／＾＾ｏｂｅＴＳａｃｅ
＾
Ｘｇ
ｒａｄｕａｉｅｄ＾ｆ
ｒｏｍ


Ｅ＾
ｉｔｙ＼



ｔ＞ＴｙｐｅＬ
ｉｎｋｓＲ
ｉｃｈａｒｄ


？Ｒｅ
ｌａｔ
ｉｏｎｗａｓ
＿ｂ〇ｍ
＿
／＾Ｓｓ＾Ｈ〇ｆｓｔａｄｔｅｒ＾
，／／


？
？Ｍｅｔａ
－Ｒｅ
ｉａｔ
ｉｏｎ


Ｉｎｓｔａｎｃｅ
－ｖ
ｉｅｗＫｎｏｗ
ｌｅｄｇｅＧｒａｐｈ


图３
－３Ｗｉｋ
ｉｄａｔａ实体与本体示意图
［５９
］


在实际应用层面
，
上述利用实体的本体作为概念是
一种高效的概念获取方式
，


本文也利用该方式获取实体概念信息
，
该方式简洁高效
，
获取成本较低
。
此外
，


仍有很多其他渠道可以获取概念知识
，
包括Ｍ
ｉｃｒｏｓｏｆｔＣｏｎｃｅｐ
ｔＧｒａｐｈ
，
ＣＮ
－Ｐｈｒａｓｅ


等多个大规模实体概念知识库
。实体概念知识有以下特点
：概念和实体通常是
“ 多


对多
” 的映射关系
，
概念的表示形式不同和概念粒度不固定
，
这都是概念知识作


为知识库时需要关注的问题
。


２３


北京邮电大学工程硕士学位论文


３
．４
．２概念映射与表示


将概念知识融入基础模型时
，
需要利用文本中的实体在知识库中查询对应的


．
概念
，这种查询过程本文将其称为概念映射
；此外
，
知识库的类型不同
，
查询到


的概念表示也不同
。例如在图知识库中
，查询到的概念是图谱节点
，可以利用图


嵌入编码算法表示得到节点表示
，也可以直接利用图谱节点的文本标签表示概念。


在概念映射部分
，
已经有很多工作完成了该部分的数据收集
，
但是由于实体


的不断出现
，维护实体和概念的映射是较为困难的工作
。以微软构建的Ｍｉｃｒ
ｏｓｏｆ
ｔ


Ｃｏｎｃｅｐ
ｔ概念图谱中
，
在
ＦｅｗＲｅｌ数据集的实体映射中仅有约
５０％的覆盖率
。
本


文参考了Ｚｈａｎｇ
ｔ５５
］的工作
，利用实体和
“
ｉｓａ
”关系构建查询语句
，将查询得到的知


识节点作为实体的伪映射概念。具体而言
，
本文利用公有知识图谱ＷｉｋｉＤａｔａ和


医学知识图谱ＵＭＬＳ作为知识库
，利用ＷｉｋｉＤａｔａ的
ＳＰＡＲＱＬ查询服务来完成实


体和概念的映射。通过上述方式
，
本文能对ＦｅｗＲｅｌ中的绝大多数实体查询到对


应的概念
。


表３
－２Ｗｉｋｉｄａｔａ和ＵＭＬＳ图谱统计情况


图谱实体数关系数三元组数


Ｗｉｋｉｄａｔａ６４０９３９８０５７


ＵＭＬＳ
１２７
５４
５８９０


在概念编码部分
，
本文使用的知识库ＷｉｋｉＤａｔａ和ＵＭＬＳ均为开源的知识图


谱
。在知识融入时
，本文选择两种知识表示形式
，包括概念的文本标签和概念节


点的图编码向量
。在对概念图谱进行图嵌入编码时
，
由于公开知识图谱Ｗｉｋｉｄａｔａ


的规模巨大
，
本文根据Ｈａｏ
［５９ｌ等人的工作将公有知识图谱Ｗｉｋｉｄａｔａ划分为概念


层级和实体层级
，在知识图谱Ｗｉｋｉｄａｔａ上仅用概念层级的图谱进行图嵌入编码
。


在ＵＭＬＳ知识库中
，
由于图谱的数量较少
，
本文直接使用整个图谱进行编码
。


本文使用到的知识图谱ＵＭＬＳ和Ｗｉｋｉｄａｔａ的规模
，
如表
３
－２所示
，
该规模的图


谱能够利用图嵌入算法快速得到对应的图编码结果
。
编码方式本文选择了


ＤｉｓｔＭｕｌｔ
［６０】技术
。


通过概念映射和概念表示模块
，能够将文本中的头实体６＆￡１￡１和尾实体ｅ
ｔａｉｉ映


射成概念ｃｈｅａｄ和ｃｔａｉｆ
。提取概念节点的文本标签和图嵌入表征
得到


知识表示
，
概念知识的链接和表示为知识融入模块的提供了知识输入
。


ｃ
ｂｅｚｄ
＞ｃｔａｉｉ
＝Ｃｏｎｃｅｖｔ
＿Ｌｉｎｋｉｎｇ｛ｅｈｅａｄ
，ｅ
ｔａｌ｛）（３
－３
）


２４


第三章
实体概念指导的少样本关系抽取模型


３
．４
．３文本形式的概念融入


当概念知识以文本标签形式融入基础模型时
，
由于基础模型的输入也是文本
，


本文直接将实体概念的文本标签拼接到实体后面
，通过文本序列位置上的关联让


模型关注到实体和概念的联系
。
此外
，
实体与概念的映射关系通常是
一对多的
，


当某个实体存在多个概念时
，本文直接将所有概念的文本标签拼接起来
，将整体


的拼接结果放在文本序列中实体后方
，
并用
ＢＥＲＴ
的特殊字来标记概念知识的


开始和结束位置
。


ＢＥＲＴ


ｆｆ令
个
个
一
个
＂—
Ｔ
￣
Ｔ
？


□□□□□□□□□□□


［ＣＬＳ
］
．
．
．Ｅ
ｌＣ
ｌ
．
．
．Ｅ２Ｃ２
．
．
．［ＳＥＰ
］
．
．
．Ｅ３Ｃ３
．
．
．Ｅ４Ｃ４
．
．
．［ＳＥＰ
］


图３４ＢＥＲＴ
－ＰＡＩＲ概念融合模型


当基础模型为ＢＥＲＴ
－Ｐａｉｒ时
，本文将概念文本标签直接拼接到文本对中的实


体后方
，
并利用语言模型ＢＥＲＴ来编码概念文本
，
如图
３
－４所示
，
其中概念标签


Ｃｉ
、
〇２
、
Ｃ３和Ｃ
ｊ是头体＾
１
、
Ｑ和Ｑ对应的概念
。


＊


ＢＥＲＴ


□□□
［□□□Ｕｂ□□□□
［□□□


［ＣＬＳ
］
．
．
．Ｅ
ｌ
．
．
．Ｅ２Ｅ
ｌＣ
ｌＥ２Ｃ２［ＭＡＳＫ
］Ｅ３Ｃ４Ｅ４Ｃ４…
Ｅ３
．
．
．Ｅ４
．
．
．［ＳＥＰ
］


图３
－５ＢＥＲＴ
－ＰＡＩＲ
（Ｐｒｏｍｐ
ｔ
）概念融合模型


当基础模型为
ＢＥＲＴ
－Ｐａｉｒ（ｐｒｏｍｐ
ｔ）
时
，
本文将概念文本标签拼接到模版中


的实体后方
。
由于
ＢＥＲＴ
－Ｐａｉｒ（ｐ
ｒｏｍｐ
ｔ）是通过［ｍａｓｋ
］词的预测结果来判断文本


对的语义关系
，
在模版中的概念知识融入更容易被模型获取
，
如图
３
－５所示
。在


训练方式上
，融入概念后的基础模型与基础模型的训练方式
一致
，均是通过元学


习训练方法得到少样本关系抽取模型
。


３
．４
．４
图谱形式的概念融入


当概念以图谱编码的形式融入基模型时
，
由于文本与概念的表示存在较大的


模态差异
，
概念不能直接以输入的形式输入基础模型
。针对模态差异
，
需要设计


２５


北京邮电大学工程硕士学位论文


知识融入模块来实现文本和概念知识的融合
。知识融入模块需要考虑文本和知识


的模态差异
，
以及不同来源知识库在的领域差异
。模态差异是由于文本语义和概


念知识的数据来源和编码方式不
一致产生的
，文本语义是输入的非结构化文本经


过
ＢＥＲＴ编码器得到的语义向量
，
概念知识则是概念图谱中的节点通过图编码


技术得到的编码向量
，数据来源不同和编码方式不同导致两者在模态维度上差异


巨大。领域差异是指训练领域上用的知识库和测试用的知识库是不
一致的
，这会


导致图嵌入编码的向量差异巨大
，因此如何保证知识融入方式在不同领域间具有


一定的泛化能力也是知识融合模块的挑战
。


概念融合模块的输入包括句子对的语义向量
，两文本中头实体和尾实体对应


的概念向量
。本文将每个句子中的头概念向量ＧＥ
ｈｅａｄ和尾概念向量
拼接起来
，


构成对应句子的概念向量Ｃｎｐｔ
ｓ
。经过实验
，
本文发现ＤｉｓｔＭｕｌｔ在不同初始化条


件下的概念向量ＣｎＰｔ
ｓ的编码结果差异巨大
，
但利用编码向量的距离特征计算相


似度具有较强的泛化能力
，因此本文将基模型ＢＥＲＴ
－Ｐａｉｒ中得到文本对的语义向


量分别融入两个文本的概念向量中
，然后利用融合后的概念向量的距离特征来表


示文本对关系的相似度
。


在将文本对向量融入单文本的概念向量时
，基于文本对的物理含义在不同领


域间具有
一定的泛化能力
，利用文本对的语义物理含义构建基于语义门控的知识


融合方法。为了增强融合向量间的距离特征
，当物理信号表示文本对中实体关系


一致时
，需要尽可能减少融合后向量的距离差异
；物理信号表示文本对中实体关


系表示不
一致时
，
需要尽可能增大融合后向量的距离差异
。基于该思路
，本文设


计了语义门控融合机制
。
给定语义特征向量
，
使用全连接层和激活函数


Ｓｂｗｏｉｄ计算门控信号５＾ａｔｅ
。


Ｓｇａｔｅ
＝ＳｉｇｍｏｉｄｉＦＣｉＥｓ））
（３
－４）


当门控信号接近
１时
，
表示语义特征表示的实体关系
一致
；
当门控信号接近


０时
，表示语义特征表示的实体关系不
一致
。根据门控信号４＆对语义向量进行


加权
，然后拼接到已有的概念量
。针对概念向量ＣｎｐｔＳ
ｉ
，融合方式如下公式所示
。


ＣｎｐｔＳｌ
－Ｃｏｎｃａｔ（Ｑ
．ＳＳｇａｔｅ
＊Ｅｓ，ＣｎｐｔＳｌ）（３
－５
）


针对概念向量ＣｎｐｔＳ２
，
融合方式如下公式所示
：


ＣｎｐｔＳ２
＝＝Ｃｏｎｃａｔ（（ｌ
—
０
．５Ｓ
ｇａｔｅ＾
＊Ｅｓ，ＣｎｐｔＳ２）（３
－６）


２６


第三章
实体概念指导的少样本关系抽取模型


为了进
一步保留融合概念向量的距离特征
，
本文使用了残差和标准化技术
。


具体融合结构如图
３
－７所示
，
通过语义门控信号将语义向量加权
，
然后将概念向


量与语义向量进行交互
，在交互过程中利用残差和标准化机制对融合的向量进行


距离特征保留和特征空间约束
。


，／
二
－二
－
为


概念向量
ｎ
「
１ｎ口
丨
概念向量
丨


语义向量
１

！
Ｉ＾｜
°
！


－
ｊＨＪ３Ｑ；



概念向量
１＾
１ｇ
１
ｉ
—
°
ｆ
ｉ＿＿
＿＿＿


、
、
、
、
、ｉｚＬｉｚｄ—


概念融合


图３
－７概念融入模块细节


通过上述方法能够实现了语义向量和概念向量的融合
，
获得两个文本的融合


向量
，随后利用距离打分器对两个融合向量进行相似度打分
，具体计算方式如下


式所示
。


Ｓｓｉｍ
＝

＼ＣｎｐｔＳｘ
－
ＣｎｐｔＳ２
＼
（３
－７
）


在元学习训练时
，
融合向量的距离叉＾被用来表示文本对关系相似度
，
根据


基础模型的损失计算方法得到交叉熵损失ＬｏｓＳ〇＾ｍ）
。
此外
，
为了让门控信号获


取到明确的物理含义
，本文也利用门控信号作为句子对的关系相似度
，并采用类


似方法得到交叉熵损失
最终利用两个损失的加和来优化模型
。


Ｌｏｓｓ
— Ｌｏｓｓ（Ｓｓｉｍ）＋Ｌｏｓｓ
［Ｓ
ｇａｔｅ）
（３
－８
）


通过上述优化目标能够得到有效的文本对编码器和泛化能力较强的知识融


合模块
。
在测试时
，
将测试文本与各关系类别的标注文本进行拼接得到文本对
，


然后利用文本对中的实体获取相应的概念表示
，利用融合机制将概念向量和文本


对语义向量进行融合
，并计算融合向量的距离来表示关系相似度
，从而得到每个


类别的概率
，
最终选择概率最高的类别作为预测关系类别
。


３
．５
元学习增强模块


３
．５
．
１
对比学习正则项


在Ｂｅｒ
ｔ
－Ｐａｉｒ基础模型中
，将关系分类任务转化为文本对关系
一致性的判别任


务
，并且通过元学习获取相似关系分类任务上的通用知识
。元学习构建的Ｅｐ
ｉｓｏｄｅ


２７


北京邮电大学工程硕士学位论文


间差异较大
，导致训练过程波动较大
；此外
，基础模型ＢＥＡＲ
－Ｐａｉｒ参数量较大
，


过大的参数波动可能会导致模型难以训练
。本文针对该问题
，构建句子对级别的


对比学习优化目标作为正则项
，
来减小过拟合的风险和训练时的过大波动问题
。


具体而言
，
如图
３
－８所示
，
在对比学习正则项中
，
将包含某相同关系的句子对作


为矛点
ａｎｃｈｏｒ
，将包含其他相同关系的句子对作为正样本
，将包含不同关系的句


子对作为负样本
。
例如
，
给定类别Ａ、
Ｂ和Ｃ
，
当矛点为（Ａ
，Ａ
）时
，
正样本集合


为｛（Ｂ
，Ｂ
）
，（Ｃ
，Ｃ）｝
，
负样本结合为｛（Ａ
，Ｂ
）
，（Ａ
，Ｃ
）
，（Ｂ
，Ｃ）｝
。


ＡｎｃｈｏｒＰｏｓ
ｉｔ
ｉｖｅＳａｍｐ
ｌｅＮｅｇａｔ
ｉｖｅＳａｍｐ
ｌｅ


；
ｔ
＇＾ｉＦ
＇
：
；：ｃｈｊｒ
，
：
■
：


ＣＺＺＨｓ＾Ｔ
ｌＥＳＫＺＺ）


ｖ
ｌ／


Ｌ—
一；ｌ＿＿ｊＬ：＿＿？１
－
－－ｊ


图
３
－８
对比学习的样本对构建方式


对比学习的优化目标是通过拉近矛点
ａｎｃｈｏｒ和正样本间的距离
，让实体关系


相似的句子对的表示更加接近
，让实体关系不
一致的句子对的表示更加远离
，从


而让模型更加关注句子对中的实体关系差异
。由于每个矛点会对应多个正样本和


多个负样本
，
本文通过计算矛点与所有样本间的点积
，
随后通过经过
Ｓ０ｆ
ｔｍａｘ激


活函数
，
输出矛点与所有样本表征的相似度的分布概率
。
给定样本
ｉ和样本
ｊ
，


本文通过ＢＥＲＴ
－Ｐａｉｒ模型得到文本的语义表征氏和￡
｝
，
然后利用以下公式计算样


本间的相似度
。


ｅｘｐ（Ｅ
ｊ
？
Ｅ
ｊ）


Ｐ
ｌＪＳｍ＝ｉｅｘｐ（Ｆ
ｉ
■
Ｅｍ）


本文采用如下方式得到相似度的真实分布标签
，
针对某个矛点
ａｎｃｈｏｒ
，
正样


本的分布概率为ｌ／
／Ｖ
ｐＭ
，其中
表示为正样本的数量
，
负样本的概率分布为
０
。


通过真实分布和预测分布的ＫＬ散度值作为优化目标
，
具体计算如下所示
。


Ｕ０
＝工二啊奈
（３
－
１０
）


其中Ｐ表示样本的预测概率
，
Ｑ表示样本的真实概率分布
。通过上述优化目标


能够将文本对表示关系
一致的模型在空间上更为接近
，对文本的语义表征进行了


一定的限制
，
减小了优化过程中的参数波动过大问题
。


２８


第三章
实体概念指导的少样本关系抽取模型


３
．５
．２领域导向性训练


在领域迁移任务中
，通常情况下
，通用领域的数据更容易收集并作为训练集
，


而测试集和验证集是在特殊领域的任务
。在现实应用中
，特殊领域存在已知和未


知两种情况
，本小节主要针对己知特殊领域的情况进行模块设计
。在元学习框架


下
，
训练时会随机构建
Ｅｐ
ｉｓｏｄｅ
，
显然不同的
Ｅｐ
ｉｓｏｄｅ对于目标领域的贡献度是


不
一致的
，让模型更加关注与目标领域相关的数据能够获取与目标领域相关的知


识
。
本文以此为出发点
，
构建了两阶段的元学习训练模式
，
具体如图
３
－９所示
，


首先通过训练集和验证集进行元学习训练
，
选取验证集上最好的关系抽取模型


Ｍｏｄｅ／？ａＺ作为打分模型
，然后利用打分模型
在第二次训练中计算每个样


本的真实标签的置信度作为权重
，
以损失加权的方式进行领域导向性训练
。



Ｓ
ｊ
＼


Ｔｒａ
ｉｎＤｅｖ


ｍｉ
＾


ｒ

ＡｄｏｄＢｌｖａｉ


Ｋ－一
…
：
…
一广
一
：
，
一
、
ｊ
…
ｙ


Ｖ


？〇＃


＃＠〇


＼Ｊ


图
３
－９领域导向性训练


上述领域导向性训练中的首段训练主要用于获取打分模型
然后利


用该模型的输出概率作为样本权重
，
权重的计算方式具体如下所示
。


Ｗｅｉｇｈｔ
ｙ
＝Ｐ（Ｙ＝ｌａｂｅｌ
ｉ
＼Ｍｏｄｅｌｖａ｛）（３
－
１
１
）


在第二段元学习训练中
，
将样本的权重加权到元学习的交叉熵损失中
，
具体


如下如下所示
。


１ｓｒ
，Ｎ
￣１


Ｌｏｓｓ（Ｙ
，Ｐ
，Ｗｅｉｇｈｔ）
＝

〉Ｗｅｉｇｈｔ
ｉ
＊
ｙｉ
ｌｏｇｐｉ
（３
－
１２
）


ＮＺ＿ｊ
￡＝〇


通过两段式训练
，模型能够在元学习训练中更加关注与目标领域更相关的数


据
，从而获取与领域更相关的知识
，在相应领域下的少样本关系抽取任务中获得


更好的性能
。


２９


北京邮电大学工程硕士学位论文


３
．６实验结果分析


３
．６．１实验环境


本次实验使用的硬件环境为
：
ＣＰＵ为
Ｉｎｔｅｌ（Ｒ
）Ｃｏｒｅ（ＴＭ
）
ｉ９
－１０９００Ｘ
，
内存为


６４Ｇ
，
图形处理器ＧＰＵ为两块ＮＶＩＤＩＡＧｅｆｏｒｃｅＧＴＸ２０８０Ｔｉ
，通过ＳＦＴＰ远程服


务进行训练及测试
，
服务器操作系统为
Ｕｂｉｍｔｕ１６
．０４
，
使用Ｐｙ
ｔｈｏｎ３
．８作为编程


工具
，
深度学习开源框架为Ｐｙ
ｔｏｒｃｈ１
．７
．０。


服务端的硬件配置
，
操作系统以及主要的编程语言如表
３
－３所示
。


表
３
－３
系统測试环境


环境
｜配置


ＣＰＵ及显卡配置Ｉｎｔｄ（Ｒ
）Ｃｏｒｅ（ＴＭ
）
ｉ９
－
１０％０Ｘ


ＮＶＩＤＩＡＧｅｆｏｒｃｅＧＴＸ２０８０Ｔｉ＊２


内存
６４Ｇ


操作系统Ｕｂｕｎｔｕ１６
．０４


编程语言
Ｐｙｔｈｏｎ３
．８


模型框架
Ｐｙ
ｔｏｒｃｈ
１
．７
．０


扩展程序库Ｔｒａｎｓｆｏｒｍｅｒｓ４
．５
．０


３
．６
．２实验数据与参数设计


本文釆用公开数据集ＦｅＷＲｄ
［６
１
，６２
：！作为实验数据集
，该数据集是少样本关系抽


取领域较为权威的数据集
，包括训练集
、验证集和测试集
，其中训练集是较大规


模的关系类别标注数据
，
验证集和测试集是其他类别的关系抽取数据
，
训练集、


验证集和测试集间的关系类别是没有交集的
。本文利用训练集进行元学习训练
，


在验证集和测试集上
，
通过构建Ｅｐ
ｉｓｏｄｅ来实现少样本任务的测试
，
通过构建大


量Ｅｐ
ｉｓｏｄｅ并使用其平均准确率作为评判指标
，
如下公式所示
。


１


Ａｃｃｕｒａｃｙ（ｊｉａｔａ
，ｍｏｄｅｌ）
＝—
／Ａｃｃｕｒａｃｙ
（ｅｐ
ｉｓｏｄｅ
＾，ｍｏｄｅｌ）
（３
－
１３
）


Ｎ
■ｔ— ＇
￡＝０


少样本关系抽取数据集主要包括两个版本
ＦｅｗＲｄ
ｌ
．Ｏ
ｌＭ和
ＦｅＷＲｅｌ２
．〇
ｔ６２ｌ
。其


中ＦｅｗＲｄｌ
．Ｏ是常规的少样本关系抽取任务
，
训练集
、验证集和测试集数据是从


公开知识图谱
ｗｉｋｉｄａｔａ利用远程监督抽取
，
然后通过人工修正得到的
。
在


ＦｅｗＲｅｌ２
．０包括两个任务类型
，
包括领域迁移任务和不包含类别任务
。领域迁移


任务的训练集和
ＦｅｗＲｅｌｌ
．Ｏ
—致
，
但验证集和测试集是来自于医学领域的数据
；


不包含类别任务
，
在验证集和测试集中
，
在构建Ｅｐ
ｉｓｏｄｅ时
，在查询集中包含
一


部分数据不属于设定的关系类别
。ＦｅｗＲｅｌ２
．０相较于ＦｅｗＲｅｌ
ｌ
．Ｏ
，任务难度更大
，


３０


第三章
实体概念指导的少样本关系抽取模型


也更接近于现实应用中的少样本关系抽取任务
。
本文主要的实验主要采用


ＦｅｗＲｅｌ２
．０的领域迁移任务数据
。


在知识库构建中
，
本文参考Ｚｈａｎｇ
ｔＷ的工作
，
利用知识图谱ｗｉｋｉｄａｔａ和医学


领域图谱ＵＭＬＳ分别构建训练集和验证集的知识库
。
Ｗｉｋｉｄａｔａ上的实体概念映


射是利用头实体和
“
ＩｓＡ
”关系作为
ＳｐａｒＳＱＬ查询语句查询得到
。
Ｗｉｋｉｄａｔａ是通用


领域中最受认可的知识图谱之
一
，包含了大量的图谱三元组
，每个实体有其定义


和描述
，实体关系也有定义和描述
。ＵＭＬＳ则是医疗领域中较为权威的知识图谱
，


它同样存在实体和关系的定义和描述
。当知识库以图结构数据进行编码时
，本文


利用图嵌入技术Ｄｉｓｔｍｕ
ｌｔ对
ｗｉｋｉｄａｔａ的概念维度图谱和整个ＵＭＬＳ图谱进行编


码
。通过上述知识库和知识库和文本映射构建
，在
ＦｅｗＲｅｌ２
．０的数据集上的实体


和知识覆盖率统计情况如表
３
－４所示
，
概念知识在训练集
、验证集和测试集的覆


盖率均较高
，
这也为知识融入的有效性提供了保证
。


表
３
－４ＦｅｗＲｅｌ２
．０知识覆盖统计情况


１据关系数样本数实体数概念覆盖率


Ｔｒａｉｎ５９４４８００８９６００９９
．５４％


Ｖａｌ１０１０００２０００９８
．７０％


Ｔｅｓｔ１５
１５００

３０００９９
．０
１％


本文的实验参数设定如下
。实验选择ＢＥＲＴ
－ＢＡＳＥ模型对文本进行编码
，包


括
１２层Ｔｒａｎｓｆｏｒｍｅｒ的编码器结构
，隐藏层维度为
７６８
。训练时
，本文利用Ａｄａｍ


优化器和
ＬｉｎｅａｒＷａｒｍｕｐ学习率调器
，
学习率初始设定为
２ｅ
－５
。
在兀学习构建


ｅｐ
ｉｓｏｄｅ时
，
支撑集的类别Ｎ和类别样本数Ｋ与测试设定的Ｎ和Ｋ
一致
，
查询


集的样本数Ｑ为
１
，
即每个类别会选取
一个样本构成查询集
，上述模型和训练方


式使用深度学习框架
ＰｙＴｏｒｃｈ进行实现
。在少样本参数设定方面
，使用
ＦｅｗＲｅｌ２
．０


官方参数
，
包括５
－ｗａｙ
１
－ｓｈｏｔ、
５
－ｗａｙ５
－ｓｈｏｔ
、
１０
－ｗａｙ
１
－ｓｈｏｔ和１０
－ｗａｙ５
－
ｓｈｏｔ等设


置
。
由于参数规模原因
，
１０
－ｗａｙ５
－ｓｈｏｔ在ＮＶＩＤＩＡＧｅＦｏｒｃｅＡ
１００进行模型训练
，


其他参数设置均在ＮＶＩＤＩＡＧｅＦｏｒｃｅＲＴＸ２０８０Ｔｉ上进行模型训练
。


３
．６
．３文本形式概念知识融入实验


根据概念知识的不同形式
，
本文选择了不同的概念编码方式
，
并设计了不同


的知识融入模块
。
当概念知识表示为文本标签时
，
将知识融入到文本的输入侧
。


该小节阐述概念知识表示成文本标签的相关实验与分析
。


首先对概念知识融入的有效性进行测试
，将ＢＥＲＴ
－Ｐａｉｒ和ＢＥＲＴ
－Ｐａｉｒ（ｐｒｏｍｐ
ｔ
）


作为基础模型
，分别在输入文本中的实体位置后增加实体概念标签文本来进行概


念知识的融入
。实验结果如表３
－５所示
，基础模型ＢＥＲＴ
－Ｐａｉｒ和ＢＥＲＴ
－Ｐａｉｒ（ｐｒｏｍｐ
ｔ
）


经过元学习的性能指标基本
一致
，这表明通过元学习训练己经极大降低下游任务


３
１


北京邮电大学工程硕士学位论文


和预训练的任务的差异
。当融入概念知识时
，
ＢＥＲＴ
－Ｐａｉｒ（ｐｒ〇ｍｐ
ｔ
）的性能显然优于


ＢＥＲＴ
－Ｐａｉｒ的性能
，
多个少样本设置下的准确率平均值大约有
２
．５
：ｐｐ的提升
，
这


是由于
ＢＥＲＴ
－Ｐａｉｒ
（ｐｒｏｍｐ
ｔ
）在融入概念时
，
将概念知识融入模版中
，
概念知识的


位置与
［ｍａｓｋ
］词的位置更近且较为固定
，
在文本表征向量中更容易融入概念知识
，


从而获得更好的少样本关系抽取结果
。
此外
，
概念的融入大多能够实现
ｌ〇ＰＰ以


上的准确率性能提升
，
这也说明实体概念在关系抽取任务上有很强的指导特性
。


表
３
－５概念文本标签融入模型


５
－ｗａｖ５
－ｗａｙ
１０
－ｗａｙ
１０
－ｗａｙ


ＭｅｔｈｏｄＡＶＧ
３


１
－ｓｈｏｔ５
－ｓｈｏｔ１
－ｓｈｏｔ５
－ｓｈｏｔ


ＢＥＲＴ
－Ｐａ
ｉｒ７０
．２８７０
．７０８０
．５９５９
．５２７０
．３０


ＢＥＲＴ
－Ｐａ
ｉｒ
－Ｃｏｎｃｅｐ
ｔ８０
．９３８０
．８８８７
．４２７５
．４０８０
．０３


ＢＥＲＴ
－Ｐａ
ｉｒ（ｐｒｏｍｐ
ｔ）７０
．０５７０
．９８８０
．２
１５９
．
１４６９
．８５


ＢＥＲＴ
－Ｐａｉｒ（ｐｒｏｍｐ
ｔ）
－


ＦＦ’８３．４１８３
．５４８８
．８６７８．２４８２
．９８


Ｃｏｎｃｅｐ
ｔ


ＢＥＲＴ
－Ｐａｉｒ
（ｐｒｏｍｐ
ｔ
）模型需要人工设定对应的模板
，在传统的ｐｒｏｍｐ
ｔ方法中
，


模版的不同对于模型的影响是巨大的
，
在本节也验证模版对于知识融入的影响
。


在
５
－ｗａｙ
１
－ｓｈｏｔ的少样本设置下
，
进行多个模板的知识融入效果对比
，
并将验证


集上的少样本性能指标进行展示
。实验结果证明模版的不同对知识的融入效果有


明显的影响
；
这说明概念作为
一种很强的指导信息
，
概念与
［ｍａｓｋ
］词的位置实体


概念知识对文本表征向量的影响很大
，训练集上和测试集上的实体关系映射数量


分布具有
一定差异也可能是导致该问题的原因
。具体而言
，在训练集上实体和概


念通常是
一对多的情况
，
即
一个实体对应多个概念
，而测试集和验证集则大多是


实体和概念是
一对
一的关系
。当出现多个概念时
，概念编码是拼接所有概念文本


标签并融入非结构化文本中
，
该方式导致模版间存在
一定的差异
。此外
，
相较于


一些相关词的引入
，
概念词和
［ｍａｓｋ
］词的关系对模型的性能影响更大
，
当
［ｍａｓｋ
］


词出现在两句概念的中间时
，
模型的性能更佳
。


表
３
－６
模版对于概念融入的影响情况


ｉｉ准确率
￣


ｒｅ
ｌａｔｉｏｎｂｅｔｗｅｅｎｅ
ｌａｎｄｅ２ｉｓ［ｍａｓｋ
］ａｓｒｅｌａｔｉｏｎｂｅｔｗｅｅｎｅ３ａｎｄｅ４８３
．７６


ｒｅｌａｔｉｏｎｅ
ｌａｎｄｅ２［ｍａｓｋ
］ｅ３ａｎｄｅ４８２
．７９


ｅ１ｅ２［ｍａｓｋ
］ｅ３ｅ４
８３
．５４


ｅ
ｌｅ２ｅ３ｅ４［ｍａｓｋ
］８
１
．５２


文本形式的概念知识融入实验结果表明
，
实体概念知识以文本标签形式输入


融入模型中
，能够大幅度提升少样本关系抽取的性能
。
Ｐｒｏｍｐ
ｔ形式的ＢＥＡＲ
－Ｐａｉｒ


模型在融入概念知识后能够取得更好的效果
。此外
，模版的设计对与知识的融入


效果有
一定的影响
，
特别是
［ｍａｓｋ
］词和概念的相对位置影响尤为明显
。


３２


第三章
实体概念指导的少样本关系抽取模型


３
．６
．４
图谱形式的概念融入增强


该小节阐述概念知识以概念图谱形式融入模型的相关实验与分析
。首先利用


图嵌入技术Ｄ
ｉｓｔｍｕ
ｌｔ对图谱进行编码
，然后设计了概念融入模块将概念信息与语


义信息进行融入
，整体模型结构如图
３
－
１０所示
。文本对通过语义编码模块
（Ｔｅｘｔ


Ｅｎｃｏｄｅｒ）
，实体信息通过实体概念链接（Ｃｏｎｃｅｐ
ｔＬｉｎｋｉｎｇ）和概念编码（Ｃｏｎｃｅｐ
ｔＥｍｂ
）


得到概念表示
，利用融合模块
（Ｃｏｎｃｅｐ
ｔＦｕｓ
ｉｎｇ
）进行语义和概念的有效融合
，
随


后利用距离打分器（ＤｉｓｔａｎｃｅＳｃｏｒｅｒ
）进行相似度打分
，整个模型在元学习框架下进


行端到端的训练
。


概念图知识，
— 一一


（
＾ｍ）
４
图编码器Ｈ
ａ
ｉ
ｔ
ｉ
；


鬱＃０／￡

－Ｊ
：ｕ＾
；


、
’ 、
Ｖｊ
ｉ
】＜
Ｉ


、— ／—
：


Ｉｖ
ｖ
？ｙ


／＾
、
、
＼／


丨
头实体
Ｉ
尾雖
！实体＿接／区ｒ
ａ
’
初響淫
、
｜


－
概念编码
，
知１、喊口器
；
｜
ｊ


头实体尾实体
＊
ｊ
头
｜
尾
｜
－
ｉ槪念向里
｜
｜


＂
Ｖ—Ｊ
ｊ


距离


，
打分器


、
？
＾

＊
＼



Ｖ
Ｖ
ｖ
４

１义
人〇
？、
、
？


ＣＬＳ
｜Ｑｕｅｒｙ文本

ＳＥＰ
｜
Ｓｕｐｐｏｒ
ｔ文本
文本综码器
｜
语义向Ｓ分类Ｓ
＾


’／／〇
．ｓ
．ｙ
ｙ０
｜
ｒｔ


图
３
－
１０
图谱概念知识融入模型


本文对比的基线方法有
：
ｗｏｒｄ２ｖｅｃＰｒｏｔｏ
（ＣＮＮ
）是利用词嵌入
、卷积网络
、
原


型网络构建少样本学习模型
，
Ｐｒｏｔｏ（ＢＥＲＴ
）是利用语言模型
ＢＥＲＴ
、
原型网络构


建少样本学习模型
，
ｐｒ〇ｔ〇
－ＡＤＶ
（ＢＥＲＴ
）是利用语言模型ＢＥＲＴ
、
原型网络
、
对


抗训练构建少样本学习模型
，ＢＥＲＴ
－Ｐａｉｒ是利用ＢＥＲＴ
－Ｐａｉｒ模型进行元学习建模
，


ＫＥＦＤＡ是在Ｐｒｏｔｏ（ＢＥＲＴ
）中融入实体描述
、
概念描述
、
元更新等技术
。
此外
，


还有部分未公开的方法
，
包括
ＰＡＭＮ
、
ＤｕａｌＧｒａｐｈ和ＧＴＰ等方法
。
基线方法和


本文方法在ＦｅｗＲｅｌ２
．０的领域迁移挑战的实验结果如表
３
－７所示
。
从表中可以发


现
，
本文设计的模型相较于其他模型有着更佳的性能
，
相较于基线模型
ＢＥＲＴ
－


Ｐａｉｒ在多种少样本设置的平均值上有着
２０ｐｐ以上的明显提升
，
这证明了概念知


识以图形是融入的有效性
。
此外
，
ＫＥＦＤＡ是该数据集上的
Ｓｏｔａ模型
，
在
５
－ｗａｙ


１
－
ｓｈｏｔ和
１０
－ｗａｙ
１
－
ｓｈｏｔ本文模型的准确率更高
，
并且本文利用的知识库信息更


少
，
这证明了模型设计的有效性
。


３３


北京邮电大学工程硕士学位论文


表
３
－７ＦｅｗＲｅｌ２
．０领域迁移实验模型性能比较结果


５
－ｗａｙ５
－ｗａｙ
１０
－ｗａｙ
１０
－ｗａｙ


ＭｅｔｈｏｄＡＶＧ


１
－ｓｈｏｔ５
－ｓｈｏｔ１
－ｓｈｏｔ５
－ｓｈｏｔ


ｗｏｒｄ２ｖｅｃＰｒｏｔｏ３５
．６７３５
．０９４９
．３７２２
．９８３５
．２２


Ｐｒｏｔｏ
（ＢＥＲＴ
）３８
．７５４０
．
１２５１
．５０２６
．４５３６
．９３


Ｐｒｏｔｏ
－ＡＤＶ
（ＢＥＲＴ
）４０
．３５４１
．９０５４．７４２７
．３６３７
．４０


Ｐｒｏｔｏ
－ＡＤＶ
（ＣＮＮ）４３
．５４４２
．２１５８
．７１２８
．９１４４
．３５


ＢＥＲＴ
－ＰＡＩＲ６６
．９３６７
．４１７８
．５７５４
．８９６６
．８５


ＫＥＦＤＡ
８７
．６９８６
．
１８９４
．３８７９
．４６９０．７７


ＰＡＭＮ７８
．９８７７
．５４９０
．４０６５
．９８８２
．０３


ＤｕａｌＧｒａｐｈ８
１
．８３８０．
１
１９１
．０１７３
．８９８２
．３４


ＧＴＰ８２
．
１８８０
．０４９２．５８６９
．２５８６
．８８



Ｏｕｒ
８７
．７６８６
．９６９２
．７８８０
．９８９０
．３２


本文在ＦｅｗＲｅｌｌ
．Ｏ的数据集也进行了实验
，
用于说明知识融入对于常规少样


本关系抽取任务的影响
。验证集的实验结果如表
３
－８所示
，在对比方法中
，
相较


于Ｐｒｏｔ
ｏ类型的方法
，
ＢＥＡＲ
－Ｐａｉｒ模型取得了较好的结果
，
这说明将文本对输入


模型进行语义提取能够获取更好的关系表征
，在通用领域上有着极具竞争力的性


能
。本文方法相较于ＢＥＲＴ
－Ｐａｉｒ模型取得了更好的性能
，这说明引入概念知识能


够在通用领域上也有很明显的性能提升
，同时证明了实体概念该类知识对于关系


抽取任务的有效性
。


表
３
－８ＦｅｗＲｅｌ１
．０模型性能比较结果


５
－ｗａｙ５
－ｗａｙ
１０
－ｗａｙ１０
－ｗａｙ


ＭｅｔｈｏｄＡＶＧ


１
－ｓｈｏｔ５
－ｓｈｏｔ１
－ｓｈｏｔ５
－ｓｈｏｔ


Ｐｒｏｔｏ
（ＢＥＲＴ）３８．７５４８
．
１５６８
．２１３５
．０５４５
．９３


Ｐｒｏｔｏ
－ＡＤＶ
（ＢＥＲＴ）５０
．４５４９
．９２６７
．３２３８
．４６５２．４２


Ｐｒｏｔｏ
－ＡＤＶ
（ＣＮＮ）４９
．５８４８
．７３６４
．３８３４
．８２５０
．３９


ＢＥＲＴ
－ＰＡＩＲ

８３
．４３８５
．６６８９
．４８７６
．８４８１
．７６


Ｏｕｒ
８８
．５５８７
．３０９４
，２２８２．６１９０
．０５


在领域迁移的少样本关系抽取任务中
，本文将语义信息和概念信息进行融合
，


然后利用距离打分器进行关系相似性打分
。该方法包括知识融合和相似度打分靓


噶部分
；
传统的知识融合方法
，更常见的是将所有输入融入成
一个向量
，但本文


融合得到了两个向量
，并利用距离打分器计算相似度
，此处设计实验对其有效性


进行分析
。
该实验的对比方法包含两种类型
：
多层感知机ＭＬＰ和距离Ｄｉｓｔａｎｃｅ


打分器
。
多层感知机
ＭＬＰ类的方法需要知识融合模块得到
一个融合向量
，


Ｄｉｓｔａｎｃｅ打分器类则需要知识融合模块得到两个概念向量
，用于验证距离打分器


的有效性。知识融入模块的输入包含句子对的语义向量和两个句子对应的概念向


量
；在对比方法中包括
：
Ｃｏｎｃａｔ是指通过拼接方式得到融合向量
；
Ｄｉｆ
－Ｃｏｎｃａｔ是


优先计算概念向量的差值
，然后再拼接
；
Ｓｅｌｆ
－Ａｔｅｎｔｉｏｎ是将文本和概念认为输入


３４


第三章
实体概念指导的少样本关系抽取模型


序歹
！Ｊ
，然后利用自注意力加权进行融合
；
Ｓｅｍａｎｔｉｃ
－Ｇａｔｅ是本文提出的利用语义门


控信号进行加权进行文本的拼接
；
具体对比方法描述如表
３
－９所示
。


表
３
－９
融合方法详情


打分器融合方式描述


Ｃｏｎｃａｔ拼接语义向量和概念向量


ＭＬＰＤ
ｉｆｆ
－Ｃｏｎｃａｔ计算概念向量差值
，
并拼接语义向量



Ｓｅｌｆ
－Ａｔｅｎｔｉｏｎ利用自注意力机制融合模型得到融合语义向量


Ｃｏｎｃａｔ分别拼接语义向量至不同的概念向量


Ｄ
ｉｓｔａｎｃｅＳｅｌｆ
－Ａｔｔｅｎｔｉｏｎ利用自注意力机制融合模型得到融合的概念向量



Ｓｅｍａｎｔｉｃ
－Ｇａｔｅ利用语义门控拼融合语义向量至不同的概念向量


上述的对比方法在ＦｅｗＲｅｌ２
．０的验证集中的实验结果如表
３
－
１０所示
，可以发


现本文提出的语义门控机制结合距离打分器的融合效果最好
，这说明在领域迁移


任务上
，
不同领域间的文本对信息通过
ＢＥＡＲ
－Ｐａｉｒ建模的关系表征具有
一定的


泛化能力
，
利用语义的物理含义来设计融合机制在不同知识库间泛化能力较强
。


Ｓｅｌｆ
－Ａｔｅｎｔｉｏｎ融合方法在不同的打分器下的表现均不理想
，这说明文本和概念图


谱表征的差异是较为明显的
，
通过序列建模的方式很难保证有良好的融合结果
。


此外
，
当融合方式为拼接Ｃｏｎｃａｔ时
，
距离打分器Ｄｉｓｔａｎｃｅ的效果明显优于多层


感知机打分器
ＭＬＰ
，
这说明当概念知识库以图谱形式进行表示
，
在不同领域的


知识图谱类型的知识库
，距离的表征更具有泛化力
，通过元学习得到的知识融合


模块在不同领域的知识库均有不错的效果
，
利用打分器ＭＬＰ训练得到的知识融


合模块泛化能力较差
。此外
，
Ｄｉｆｆ
－Ｃｏｎｃａｔ利用图谱概念向量的差值来拼接语义向


量
，
随后利用ＭＬＰ打分器进行训练
，
该方法效果较差也印证了上述结论
。


表
３
－
１０ＦｅｗＲｅｉ２
．０模型性能比较


Ｉｎｔｅｇｒａｔｉｏｎ５
－ｗａｙ５
－ｗａｙ
１０
－ｗａｙ
１０
－ｗａｙ


ＳｃｏｒｅｒＡＶＧ


Ｍｅｔｈｏｄ
１
－ｓｈｏｔ５
－ｓｈｏｔ１
－
ｓｈｏｔ５
－
ｓｈｏｔ


Ｃｏｎｃａｔ６４
．２
１６５
．３７７６
．２９４８
．８３６６
．３４


ＭＬＰＤ
ｉｆｆ
－Ｃｏｎｃａｔ６
１
．５２６２
．８９７５
．８２４２
．３８６４
．９８


Ｓｅｌｆ
－Ａｔｅｎｔｉｏｎ６０
．
１９６
１
．６２６４
．３２５４
．９９５９
．８４


＿Ｃｏｎｃａｔ８４
．３５８４
．３６９０
．０４７７
．８５８５
．
１４


＾Ｄ
ｉｓｔ＾ｒｉｃｃ


Ｓｅｌｆ
－Ａｔｅｎｔ
ｉｏｎ６４
．５９６５
．２６７２
．２２５４
．５０６６
．３８


Ｓｃｏｒｅｒ


Ｓｅｍａｎｔｉｃ
－Ｇａｔｅ８６
．５４８５
．８８９
１
．８８８０
．０８８８
．３２


为了进
一步展示多层感知机ＭＬＰ打分器和距离
Ｄｉｓｔａｎｃｅ打分器对知识融合


结模块的影响
，本文在Ｃｏｎｃａｔ融合方式下对两类打分器的融合向量进行可视化
。


当打分器为多层感知机
ＭＬＰ时
，
直接可视化融合向量的结果
；
当打分器为


Ｄ
ｉｓｔａｎｃｅ打分器不
，
可视化融合向量的差值
。
在
５
－ｗａｙ５
－ｓｈｏｔ的少样本设置进行


可视化
，
随机抽取
５０个
ｅｐ
ｉｓｏｄｅ作为可视化数据并利用
Ｔ
－ＳＮＥ降维技术进行可


视化
。
可视化结果如图
３
－
１
１所示
，
图中的黄点表示句子对的实体关系
一致
，
紫


３５


北京邮电大学工程硕士学位论文


点表示句子对的实体关系不
一致
。使用Ｄ
ｉｓｔａｎｃｅ打分器训练得到的融合向量在不


同领域间的融合结果表征是类似的
，
使用ＭＬＰ打分器训练得到的融合特征向量


的表征在不同领域中的差异较大
，通过该方式得到的知识融合模块在不同领域上


的泛化能力不足
。


ｆ
＾Ｋ／］
丨


４０
－２００２５４Ｃｅｃ
６０
－４Ｃ
－２００２０４０６０


（ａ
）ＭＬＰ
－Ｓｃｏｒｃｒ
ｔｇｃｎｃｒａ
ｌｄｏｍａ
ｉｎ
）
（ｂ）ＭＬＰ
－Ｓｃｏｒｃｒ（ｓｐｃｃ
ｉａ
ｌｄｏｍａ
ｉｎ
）


７０
２〇
．＾Ｖ


－？〇
？〇ｏ？ｎ＊〇
－？ａ
— ２Ｃ
ｊｃｏｉｏ２０３０


（ｃ
）Ｄｉｓ
ｉａｎｃｃ
－Ｓｃｏｒｃｒｉ
ｊｉｃｎｃｒｄ
ｌｄｏｍ
－
ｄｉｎ）
（ｄ）Ｄ
ｉｓｔａｎｃｅ
－Ｓｃｏｒｃｒ（ｓｐｅｃ
ｉａ
ｌｄｏｍａ
ｉｎ
）


图
３
－
１
１
不同打分器下的融合特征可视化


３
．６
．５
元学习增强模块


元学习増强模块在元学习过程中利用对比学习正则项来对元学习过程中的


语义表征进行限制
，并且设计两段的元学习训练实现领域导向性的元学习知识获


取
。元对比学习正则项主要解决ＢＥＲＴ
－Ｐａｉｒ模型参数过多的问题以及元学习训练


过程的不稳定性问题
，领域导向性元学习是在目标领域的部分数据已知的情况下
，


在元学习过程中让模型更加关注与领域更相关的数据
，从而提高模型在领域迁移


任务上的性能
。


概念以图谱形式的融入实验己经利用了元学习增强模块
，此处通过消融实验


来验证模型元学习增强模块的有效性
。本文逐渐从基于实体概念增强的少样本关


系抽取模型去除对比学习正则项
Ｒｃｏｓ
，
两段式训练Ｗｓａｍｐ
ｌｅ和概念融合机制


Ｃｎｐ
ｔｆｌｘｓｅ来验证各个模块的效果
。
实验结果如表
３
－
１
１所示
，
随着逐步剥离多个


模块
，
可以发现模型性能逐渐下降
，
从而验证了各模块的有效性
。知识的融合模


块提升的性能明显优于其他模块
，这说明了概念知识的有效性
。在元学习增强模


块中
，
相较于两段式训练
，
对比学习正则项对模型带来的性能提升更小
。


３６


第三章实体概念指导的少样本关系抽取模型


表
３
－１
１
消融实验结果


５
－ｗａｙ５
－ｗａｙ１０
－ｗａｙ１０
－ｗａｙ


ＭｅｔｈｏｄＡＶＧ
ＪＪＪＪ


１
－ｓｈｏｔ５
－ｓｈｏｔ１
－ｓｈｏｔ５
－ｓｈｏｔ


Ｏｕｒ
（
－Ｒｃｏｓ
－Ｗｓａｍｐ
ｌｅ
－Ｃｎｐ
ｔｆｕｓｅ
）６６
．９３６７．４
１７８
．５７５４
．８９６６
．８５


Ｏｕｒ
（
－Ｒｃｏｓ
－Ｗｓａｍｐ
ｌ
）８４
．９０８６
．
１０８９
．３２７７
．９６８６
．２３


Ｏｕｒ
（
－Ｒｃｏｓ）８６
．８９５８６
．６４９
１
．７６８０
．２４８８
．９４


Ｏｕｒ
８７
．７６８６
，９６９２
．７８８０
．９８９０
．３２


对比学习正则项融入元学习训练时
，
是将对比学习正则＆。５加权的加到最终


的损失中
，本文对正则项的权重进行研宄
，并且在
５
－ｗａｙ
ｌ
－ｓｈｏ和
５
－ｗａｙ５
－ｓｈｏｔ两


种少样本设置的结果进行对比
。
实验结果如表
３
－１２所示
，
权重对模型的性能指


标具有
一定的影响
，但准确率的波动在
ｌｐｐ以内
，模型效果差异不大
。不同的少


样本参数设置随着正则项权重的增大
，模型性能指标大致都是先增加后减少
，这


也证明了参数设置区间的相对正确
。


表
３
－
１２概念损失权重指标影响


Ｓｅｔｔｉｎｇ０
．
１０
．３０
．５０
．７０
．９


５
－ｗａｙ
１
－ｓｈｏｔ８６
．７４８６
．９６８６
．８４８６
．４８８６
．
１０


５
－ｗａｙ５
－ｓｈｏｔ９１
．３７９２
．７８９２
．８９９２
．４６９１
．８５


通过上述的消融实验和对于对比学习正则项的权重分析
，可以发现元学习增


强模块对模型有
一定的提升
，参数选择也较为正确
。通过合适的参数设计
，元学


习増强模块能够有效实现模型性能的提升
，辅助少样本关系抽取模型在领域迁移


任务上完成有效的关系建模
。


３
．７本章小结


本章完成了基于实体概念増强的少样本关系抽取模型的设计与实验
。首先详


细介绍了少样本关系抽取模型的各模块
，包括概念知识增强模块和元学习增强模


块
。在概念知识增强模块中
，针对不同形式的概念知识
，本文设计了相应的概念


表示和概念融合机制
。在元学习增强模块
，通过设计对比学习正则项来对模型的


语义表征进行限制
，针对目标领域已知的情况设计了领域导向性元训练来获取更


好的领域迁移性能
。本文在公开数据集ＦｅｗＲｅｌ对上述模型进行实验
，
验证了实


体概念知识在少样本关系抽取上的有效性
，
并对比了不同知识融入方法的性能
，


并对融合结果进行可视化等多维度的分析。此外
，本章还验证了元学习增强模块


的有效性
，
实验表明对比学习正则项和领域导向性元学习均能取得不错的结果
。


３７


北京邮电大学工程硕士学位论文


３８


第四章少样本关系抽取系统设计与实现


第四章少样本关系抽取系统设计与实现


传统的关系抽取系统需要人工预先定义关系类别
，通过收集数据并标注大量


数据来训练关系抽取模型
，然后利用关系抽取模型在无标注数据上进行关系抽取


推理
，结合实体识别结果完成知识图谱三元组的构建
。传统的关系抽取系统在处


理新型关系上仍然需要收集和标注新关系类别的数据
，并且重新训练关系抽取模


型
，构建成本较高
。此外
，在构建专业领域的知识图谱时
，数据获取相对较困难
，


且对标注人员的要求更高
，大量的数据标注会产生更高的成本
。本文基于上述问


题设计了少样本关系抽取系统
。该系统通过应用上文的少样本关系抽取模型在标


注数据较少的情况实现髙效的关系抽取建模
，能够应对真实场景下的新型关系的


和领域关系的抽取任务
。本章内容包括系统需求分析
，
系统概要设计
、功能模块


设计和功能测试
。


４
．
１
系统需求分析


本系统的目标是构建少样本关系抽取系统
，
采用Ｂ／Ｓ架构进行实现
。系统在


Ｗｅｂ端为图谱构建人员提供新型关系和领域关系的快速建模能力和图谱管理能


力
，
为超级管理人员额外提供了用户权限管理能力
。


４
．１
．
１系统模块需求分析


（
１
）用户权限模块


本系统主要设定了两种角色
，
包括超级管理员和普通用户
。用户权限管理主


要是为系统不同用户权限的用户提供相应的用户信息管理
，
图４
－
１中展现了该模


块的用例图
，
具体的功能需求包括以下几部分
：


１
）用户管理
．
？超级管理员通过用户管理模块
，可以实现对普通账户查询
、账


户删除
、
账户编辑等功能
，
通过新增用户模块来实现管理员用户的新增
。


２）个人信息管理
：个人信息管理功能主要是方便所有用户对个人信息进行管


理
，
包括信息查询
、
登陆密码修改和昵称修改等
。


３
）登陆
：登陆功能是所有用户进入系统唯
一途径
，登陆时会验证用户的权限
，


从而跳转至对应的功能目录。


３９


北京邮电大学工程硕士学位论文


４）注册
：普通用户可以通过注册功能完成用户账号的创建
，超级管理员只能


通过用户管理中的账户新增模块完成管理员账号的创建
。通过该种注册限制方式
，


维护整体系统的安全性能
。


用户


超级符理员＼Ｖ
—



ｅ油用户１Ａ
询个人信


、
－
辑个人信


图
４
－
１
用户权限模块用例图


（２）数据操作模块


数据操作模块为不同权限用户提供了训练数据和推理数据的管理
，
如图
４
－２


所示
，
该模块主要包括数据管理
、数据上传
、个人数据管理等功能
，
具体的功能


需求包括以下几部分
：


１
）数据管理
：超级管理员可以通过数据管理功能实现对全局数据的管理
，包


括所有用户的数据统计信息
，
数据查看
、
删除
、
分享和下载功能
。


２
）数据上传
：所有用户可以通过数据上传功能完成训练和推理数据的上传
，


上传数据完成后
，
可以展示在数据管理界面
。


３
）个人数据管理
：个人数据管理是用户对自己的数据进行管理
，包括数据查


看
、
删除
、
分享和下载功能
。


＆ｍ＼％＾
＾


〇Ｊ＼


图
４
－２数据操作模块用例图


４０


第四章
少样本关系抽取系统设计与实现


（３
）关系抽取模块


关系抽取模块为系统用户提供了少样本关系抽取功能
，并且支持对抽取的三


元组进行管理或可视化
。如图
４
－３所示
，
该模块主要包括了关系抽取
、
图谱管理


和个人图谱管理等功能
。
具体的功能需求包括以下几部分
：


１
）关系抽取
：所有用户通过关系抽取功能快速构建少样本关系抽取模型
，通


过新关系定义
、训练和推理的数据选择
，数据图谱生成功能能够直接完成少样本


关系抽取模型的构建和测试数据的推理
。


２
）图谱管理
：超级管理员用户能够通过图谱管理对所有用户通过关系抽取模


型得到的知识图谱进行管理
，包括图谱查看
、
图谱删除
、
图谱可视化和图谱下载


功能
。


３
）个人图谱管理
：所有用户能够通过个人图谱管理模块对自己通过少样本关


系抽取模型得到的知识图谱进行管理
，包括图谱查看
、
图谱删除
、
图谱可视化和


图谱下载功能
。


；


义



（］
＼／图谱数据生成ｊ


超级符理贷Ｆ


Ｉ数据杏


之谱数据＿
１＾）


ｒ
＾
ｉ
人图


務通，管理一
￣—


＼＼可视化


＼＼


＾＼


Ｖ＼

二＾
谱下载）


图
４
－３
关系抽取模块用例图


４丄２系统非功能模块需求分析


（
１
）性能需求


系统在运行时需要保证系统稳定
，
能够及时响应用户请求
。
在用户登陆
、普


通操作页面下
，等待时间不宜过长
，登陆界面打开不宜超过
３ｓ
，界面切换不超过


ｌｓ
，
系统接口的响应时间小于
０
．５ｓ
。
当用户进行关系抽取模块时
，
由于本系统配


置为单模型推理
，利用先来先服务调度机制
，并保证模型每条数据的推理耗时少


于５０ｍｓ〇


４
１


北京邮电大学工程硕士学位论文


（２）可靠性需求


系统开发过程中要遵循开发设计原则
，在开发的各个周期进行周密的计划和


考核
，在测试阶段进行多层次的测试
，减少系统出现异常的几率
。系统能够应对


绝大多异常
，
包括网络问题、服务器问题和数据问题等
，拥有较强的健壮性
，保


证系统的稳定运行
。


（３）可维护性需求


系统开发过程中要选择合适的技术选型
，需要根据系统的性能需求进行有效


选择
，
并考虑到对应的开发成本与风险
。
减少开发过程中的不确定因素
。
表
４
－１


是本文的技术栈选型
。在开发过程中进行必要的文档书写和代码注释
，规范进行


开发
。


表４
－
１
少样本关系抽取演示系统技术选型


系统组件工具名称


Ｒｅａｃｔ
，
单页面应用框架


Ｒｅａｃｔ
－ｃｒ
ｅａｔｅ
－ａｐｐ
，
交互式项目脚手架


前端框架Ａｘｉｏｓ
，
一个—于ｐｒｏｍｉｓｅ
的ＨＴＴＰ库


ａｎｔ，
前端组件库


Ｅｃｈａｒｔ
，
前端组件库


Ｆｌａｓｋ
，
基于ｐｙｔｈｏｎ的轻量级Ｗｅｂ应用框架


Ｆｌａｓｋ
－ｓｑ
ｌａｌｃｈｅｍｙ


后端框架


Ｆｌａｓｋ
－ｍａｉｌ


Ｆｌａｓｋ
－
ｊｗｔ
－ｅｘｔｅｎｄｅｄ


Ｆｌａｓｋｃａｃｈｉｎｇ


数据犀管理系统环境
：
Ｍｙｓｑ
ｌ


玄絲邱罢
Ｐｙ
ｔｈｏｎｗｓｇｉ应用服务器
：
Ｇｕｎｉｃｏｒ
ａ


承攻邡君Ｗｅｂ服务器
：
Ｎｇ
ｉｎｘ


版本控制
：
ｇ
ｉｔ／ｇ
ｉｔｌａｂ


其他工具系统文档
：
ｍｉｎｄｏｃ



深度学习框架
：
Ｐｙｔｏｒｃｈ


（４）
易用性需求


系统的操作流程应该简洁明了
，符合日常生活的逻辑
。在操作过程中
，避免


无效操作逻辑
。在交互过程中
，用户的操作能够直观反映在操作界面中
，适时给


予用户适当的操作提示
。


（５
）安全性需求


系统应该能为系统管理者提供系统的信息管理界面
，对于潜在风险给予管理


者提示
，
对用户的不当操作进行限制
。


４２


第四章少样本关系抽取系统设计与实现


４
．２系统概要设计


４
．２
．
１
系统架构设计


本系统采用浏览器／服务器
（Ｂ／Ｓ
）设计架构
，
通过前后端分离的交互形式实


现
，
前端采用
Ｒｅａｃｔ的单页面应用框架
，
后端采用基于
Ｆｌａｓｋ的Ｗｅｂ框架来实


现
，
使用ＭｙＳＱＬ完成存储系统数据
，
利用Ｎｇ
ｉｎｘ来实现服务器间的复杂平衡
，


使用
Ｓｃｈｅｄｕｌｅｒ模块进行后台应用调度来应实现离线任务
，
使用
Ｔｏｋｅｎ技术来验


证用户状态和控制用户权限
。
前后端的交互方式通过ＨＴＴＰ协议的ＲｅｓｔｆｕａｌＡＰＩ


进行实现
。
系统的详细架构模块使用情况如图
４
－４所示
，
本文的开发工作均基于


该架构
。


／
，

Ｊ


ＨＩ
Ｒｅａｃｔ１腦７
一
］


＿模版渲染

＾— 
！
－ｍ
（＿ｍ
）
— — ￣
［

—
－—
前端Ｉ
Ｒｅｄｕｘ
Ａｎｔ
圆
ＮＰＭ


ＮＯＤＥＪＳ＿Ｗｅｂｐａｃｋ＿


邏
難靈觀＾


通信层Ａ
ｊａｘ交互ＧＥＴＰＯＳＴＤＥＬＥＴＥＰＵＴ



’
一
’

Ｊ


，
＇


Ｆ
ｌａｓｋ


ｉ
务层Ｆ
ｌａｓｋ
－ＳＱＬＡＬＣＨＥＭＹＦ
ｌａｓｋ
－ＷＴＦＦ
ｌａｓｋ
－Ｌｏｇ
ｉｎ


Ｆ
ｌａｓｋ
－ＲＥＳＴｆｕ
ｌＳｃｈｅｄｕ
ｌｅ
ｒＦ
ｌａｓｋ
－ＣＯＲＳ


ｖ
ｓ一

－
＿
■
■
■
ｙ


ｆ
、


■— 
— 一
—— 


数据库ＭｙＳＱＬＭｙＳＱＬ





＼


ｆ
＇
、
＇


运行环境腾讯云Ｎｇ
ｉ
ｒ？负载器应用服务器


ｋ
＾


图
４４
少样本关系抽取系统技术架构图


该系统中主要服务于上文提出的少样本关系抽取系统
，
在系统中
，
针对该系


统的模块包括
：数据操作模块
、关系抽取模块
。数据操作模块需要完成数据上传
、


数据管理等功能
。关系抽取模块需要实现文本预处理
、新关系定义
、少样本关系


抽取
、图谱管理等功能
。用户按照数据规则要求上传需要抽取关系的无标注数据


和对应的标注数据
，上传后的数据通过数据操作模块进行处理
，并且通过前端界


面展示给用户
。用户在己完成上传的数据中选取需要进行抽取关系数据
，并利用


４３


北京邮电大学工程硕士学位论文


关系抽取模块通过利用定义关系的少量标注数据实现关系的定义和建模
，通过训


练得到的少样本关系抽取模型在大量的无标注数据进行推理得到知识三元组
，在


关系抽取模块对抽取的知识图谱的三元组进行管理
。


「数

＂
■
￣
Ｊ


１
据数据上传
Ｉ
Ｉ
Ｉ


；
｜１
；
！前端可视化界面１
！


模麵理
—Ｌ＾
－端
；


ｉ
ＩｍＬ
Ｉ
！ｒ４
￣
￣
￣
！


匡
少
—
ｒ
—！
■
Ｉ


！１
｜
絲定Ａ
！
—
＾
Ｆ
ｌａｓｋ应用管理｜
｜


■
关
１
１
ｉ
Ｉ


ｉ系
ｉ；
ｉ


１
抽关系建模
一
Ｊ
—
，
１



ｉ＿＊ｉ
ｉ
ｉ


？
块关系推理
１
｜锶忠
■


；Ｉ
；
Ｌ一ｈ
关系抽取模型ＩＩ
：


；
｜
图谱管理
￣￣
Ｉ
￣
！
＊
１
！



ＪＬ
Ｊ


图
４
－５
少样本关系抽取系统数据流


４
．２
．２
数据库设计


少样本关系抽取系统的存储内容主要包括上传的数据和抽取的图谱
。存储的


数据和图谱主要是以
Ｊｓｏｎ文件格式进行存储
。
上传文件和图谱的具体格式如表


４
－２所示
。


表
４
－２
系统上传数据和抽取图谱格式


数据类型Ｋｅｙ
ｖａ
ｌｕｅ


上传数据ａｎｎｏｔａｔｉｏｎ
ｄａｔａｃａｔｅｇｏｒｙ数组


ｃａｔｅｇｏｒｙ
［［ｓｌ＾ｅ＾ＵｓＩ＾ｅ＾
］
，
．
．
．
］



ｔｅｓｔｄａｔａ


图谱
ｋｇ
［
［ｅ
ｌ
，ｅ２
，ｒ
ｌ
，ｓｃｏｒｅ
］
，
．
．
．
］


在文件描述中
，上传文件包括少样本标注数据
ａｎｎｏｔａｔｉｏｎｄａｔａ和推理数据
ｔｅｓｔ


ｄａｔａ
，
每条数据包括句子ｓｉ和句子中对应的头实体４和尾实体ｅ
ｓ
２
ｉ
；
在图谱设置的


格式中
，
ｅ
ｌ
、
ｅ２表７Ｋ头实体和尾实体
，
ｒ
ｌ表不关系
，
ｓｃｏｒｅ表不实体关系的模型


置信度
。


系统利用ＭｙＳＱＬ来存储用户的相关信息
，
角色信息
，
图谱数据的路径等信


息
。本系统共设定了５张表
，
具体包括
：
用户基本信息表
（ｕｓｅｒ
＿
ｉｎｆｏ表
）
，
角色


４４


第四章
少样本关系抽取系统设计与实现


权限映射表
（ｒｏｌｅ
＿ｐｅｒｍｉｓｓｉｏｎ表
）
，
用户权限表
（ｐｅｒｍｉｓｓｉｏｎ表
）
，
数据管理表


（ｕｐ
ｌｏａｄ
＿ｄａｔａ表
）
，
图谱管理表
（ｋｇ＿ｄａｔａ表
）
，
具体的功能如表４
－３所不
。


表
４
－３数据表描述信息


数据表＾


用户基本信息表存储用户的基本信息和注册信息


角色权限映射表角色对应的权限等级


用户权限表用户和权限的映射


数据管理表数据的描述信息和存储地址


图谱管理表图谱的描述信息和存储地址


通过上述表
，
能够实现系统功能的实现
，
并对信息进行管理
，
具体的数据表


字段如图
４
－６所示
。


＇、
、、
＇、
、


ｕｓｅｒ
ｉｎｆｏ
、ｒｏｌｅ
＿ｐｅｒｍｉｓｓｉｏｎ＇ｐｅｒｍｉｓｓｉｏｎ
、


ｉｄ
：ｂ
ｉｇ
ｉｎｔｕｓｅｒ
＿
ｉｄ
：ｂｉｇ
ｉｎｔ
ｉｄ
：ｂ
ｉｇ
ｉｎｔ


ｕｓｅｒ
＿ｎａｍｅ
：ｖａｒｃｈａｒｕｓｅｒ
＿ｎａｍｅ
：ｖａｒｃｈａｒｄｅｓｃｒ
ｉｐ
ｔｉｏｎ
ｒｖａｒｃｈａｒ


ｕｓｅｒ
＿ｐａｓｓｗａｒｄ
：ｖａｒｃｈａｒＯｐｅｒａｔｏｒｂ
ｉｇ
ｉｎｔｃｒｅａｔｅ
ｔｉｍｅ
：ｔｉｍｅｓｔａｍｐ


ｕｓｃｒ
＿ｃｍａｉｌ
：ｖａｒｃｈａｒ
，ｃｒｃａｔｃ
＿ｔｉｎｉｃ
：ｔｉｍｃｓｔａｍｐ
＾ｕｐｄａｔｅ
ｔｉｍｅ
：ｔｉｍｅｓｔａｍｐ


ｕｓｅｒｐｈｏｎｅ
ｎｕｍ
：ｃｈａｒｕｐｄａｔｅ
＿
ｔｉｍｅ
：ｔ
ｉｍｅｓｔａｍｐ


ｒｏｌｅ
：ｖａｒｃｈａｒ


ｃｒｅａｔｅ
＿
ｔｉｍｅ
：ｔｉｍｅｓｔａｍｐ


ｕｐｄａｔｅ
ｔｉｍｅ
：ｔｉｍｅｓｔａｍｐ


ｋｇ＿ｄａｔａｕｐ
ｌｏａｄ
ｄａｔａ


ｉｄｒｂｉｇ
ｉｎｔ
ｉｄ
ｒｂｉｇ
ｉｎｔ


ｄｅｓｃｒ
ｉｐ
ｔｉｏｎ
ｒｖａｒｃｈａｒｄｅｓｃｒ
ｉｐｔ
ｉｏｎ
ｒｖａｒｃｈａｒ


ｓｏｕｒｃｅ
＿ｄａｔａ
：ｂｉｇ
ｉｎｔ
ｓａｖｅ
＿ｐａｔｈ
：ｖａｒｃｈａｒ


：
＾ｓａｖｅ
＿ｐａｔｈ
：ｖａｒｃｈａｒｏｗＴｉｅｒ
＿
ｉｄ
：ｂｉｇ
ｉｎｔ


ｏｗｎｅｒ
＿
ｉｄ
：ｂ
ｉｇ
ｉｎｔ
ｃｒｅａｔｅ
ｔｉｍｅ
：ｔｉｍｅｓｔａｍｐ


ｃｒｅａｔｅ
＿
ｔｉｍｅ
：ｔｉｍｅｓｔａｍｐ
ｉｑｘｉａｔｅ
＿
ｔ
ｉｍｅ
：ｔｉｍｅｓｔａｍｐ


ｕｐｄａｔｅ
＿
ｔｉｍｅ
：ｔｉｍｅｓｔａｍｐ


專


图
４
－６数据库Ｅ
－Ｒ图


通过上述数据库和文件格式的设置
，可以实现少样本关系抽取系统数据的有


效存储
。根据该文件格式
，
可以有效提取标注数据和测试数据
，为后续功能模块


的实现提供便利
。


４５


北京邮电大学工程硕士学位论文


４
．３功能模块设计


４
．３
．
１前端展示模块


前端展示模块是用户与系统的交互模块
，
设计系统时
，
需要考虑到用户操作


的便利性
。前端展示模块包括两个功能栏
，
包括菜单栏和功能栏
，菜单栏和功能


栏会
一一对应
，
功能栏会随
ｘ着菜单栏的点击随着改变
。
菜单栏主要包括三部


分
：
用户管理
、数据列表和图谱列表
。如上述描述
，
具体的前端模式页面布局如


图
４
－７所示
。


菜中
．栏功能栏


图
４
－７前端展示模块设计图


前端使用Ｒｅａｃｔ底层框架
，
利用Ｒｅａｃｔ
－ｒｏｕｔｅｒ作为前端路由
，
搭配ＡｎｔＤｅｓｉｇｎ


ＵＩ库搭建整体前端框架
，
并利用
Ｅ
－ｃｈａｒ
ｔ组件完成知识图谱三元组的可视化
。


４
．３
．２后端模块


后端模块是前端展示模块通过界面和系统的数据进行交互
，
前端模块通过


ｈｔｔｐ请求完成对应的数据传输
。在数据层面
，通过将
ｈｔｐ请求将对应的功能转换


成
ＳＱＬ语句
，
将查询结果转换成
Ｊｓｏｎ文件格式
，
然后展示给用户
。
后端模块的


部分接口如表４
－４和表
４
－５所示
。


４６


第四章少样本关系抽取系统设计与实现


表４
＊４登陆请求接口


请求路径
／ａｐ
ｉ／ｕｓｅｒ／Ｉｏｇ
ｉｎ


请求方法ＰＯＳＴ


接口功能说明用户登陆



请求参数示例ｋｅｙｖａｌｕｅ


ｕｓｅｒ
ｎａｍｅｘｘ


ｐａｓｓｗｏｒｄｘｘ


返回参数示例
ＸＸ


ｔｏｋｅｎｘｘ


ｕｓｅｒｉｄｘｘ


ｕｓｅｒ
ｎａｍｅｘｘ


表４
－５查询用户信息接口


请求路径／ａｐ
ｉ／ｕｓｅｒ／
丨ｕｓｅｒ
ｉｄ
｝７ｄｅｔａｉｌ


请求方法ＧＥＴ


接口功能说明
查询用户数据


请求参数示例
无


返回参数示例




ｕｓｅｒｎａｍｅｘｘ


ｔｅｌｅｐｈｏｎｅｘｘ


ｅｍａｉｌｘｘ


ｒｏｌｅｘｘ


４
．３
．３数据上传模块


数据上传模块是指用户通过该模块将本地文件上传至服务器
，
用户可以对服


务器中上传的文件进行操作
，包括删除
，使用关系抽取模块构建抽取模型等功能
。


数据上传模块的部分接口如下表所示
。


表４
￣６数据刪除接口


请求路径
／ａｐ
ｉ／
丨ｕｓｅｒ
ｉｄ
丨
／ｄｅｌｅｔｅ／｛ｄａｔａｉｄ
｝


请求方法ＤＥＬＥＴＥ


接口功能说明删除数据


请求参数示例无


返回参数示例



ｓｔａｔｅｘｘ


４７


北京邮电大学工程硕士学位论文


表４
＾７数据上传接口


请求路径
／ａｐ
ｉ／
｛ｕｓｅｒ
ｉｄ
｝
／ｕｐ
ｌｏａｄ


请求方法ＧＥＴ


接口功能说明上传数据


请求参数示例＾


ｆｉｌｅｘｘ


ｄｅｓｃｒ
ｉｐ
ｔｉｏｎｘｘ


ｏｔｈｅｒｓｘｘ


ｕｓｅｒ
ｉｄｘｘ


返回参数示例
ｓｔａｔｅ＾


４
．３
．４关系抽取模型构建模块


关系抽取模块会利用上文少样本关系抽取模型实现少样本关系抽取分类
，为


了避免模型在多个少样本任务生成过多模型
，导致系统的存储风险
。该系统要求


用户在上传训练数据的同时上传测试数据
，
方便测试后即可删除训练后的模型
。


在上传数据模块需要上传满足如上述表４
＞４的数据
，然后选择对应的数据建模并


对图谱进行管理。
该模块的接口如下表４
－８所示
。


表４
－８关系抽取建模接口


请求路径
／ａｐ
ｉ／｛ｕｓｅｒ
ｉｄ
丨

／ｃｏｎｓｔｒｕｃｔ


请求方法ＰＯＳＴ


接口功能说明构建图谱


请求参数示例—
｜
ｖａｌｕｅ


ｄａｔａｉｄｘｘ


ｄｅｓｃｒｉｐ
ｔｉｏｎｘｘ


ｏｔｈｅｒｓｘｘ


返回参数示例
ｓｔａｔｅ＾


通过少样本关系抽取模型
，
用户得到对应的知识图谱列表
，
并且可以在知识


图谱列表进行图谱管理和可视化
，
该模块的部分接口如下表４
－９所示
。


４８


第四章少样本关系抽取系统设计与实现


表４
－９
图谱列表获取接口


请求路径
／ａｐ
ｉ／
｛ｕｓｅｒ
ｉｄ｝／ｋｇ
ｌｉｓｔ


请求方法ＰＯＳＴ


接口功能说明构建图谱


请求参数示例无



返回参数示例



ｓｔａｔｅｘｘ


ｄａｔａｋｇ
ｉｎｆｏ数组


ｋｇ＿
ｉｎｆｏ
［ｋｇ
ｉｄ
，ｄｅｓｃｒ
ｉｐｔｉｏｎ
］


４
．３
．５用户管理模块


用户管理模块可以对系统用户的信息进行管理
，通过对用户的权限进行判断
，


来限制用户的部分操作
。


４
．４系统测试


４
．４
．
１测试环境


系统开发结束后
，
会对系统进行功能性测试和非功能性测试
，
功能性测试是


指对系统的各个模块是否能够正常运行
，
非功能性测试是指对系统的响应时间
，


运行流畅度进行测试。
在测试过程中
，
系统的运行环境如表
４
－１０所示
，
并记录


测试结果
。


表４
－１０
系统測试环境说明


名称信息


客户端硬件１
．４ＧＨｚ
四核
ＩｎｔｅｌＣｏｒｅｉ５＋８Ｇ
内存


客户端操作系统ｍａｃＯＳＭｏｊａｖｅ１０
．
１４
．５


测试浏览器Ｃｈｒｏｍｅ８３
．０
．４
１０３
．
１２２／Ｓａｆａｒ
ｉ１４
．０
．２


服务器硬件Ｕｂｕｎｔｕ１６
．０４ＬＴＳ


服务器操作系统
Ｉ７
－８７００＋１６Ｇ
内存



４
．４
．２功能性测试


本系统的目的是构建少样本关系抽取模型的落地应用
。用户通过上传某类型


关系的少量标注数据和测试数据
，系统利用少样本关系抽取系统和少量标注数据


完成模型的快速训练。在测试数据上进行快速推理
，得到对应的三元组知识图谱
。


此外
，
需要满足系统的登陆功能
、
用户管理功能等
。如图
４
－８和图４
－９所示是对


应的用户登陆界面和用户管理界面
。
用户管理界面可以对用户的权限进行限制
、


密码进行重置、
以及用户删除等操作
。


４９


北京邮电大学工程硕士学位论文


Ｃ▲
不安重ｉｃｎ
．？３

ｉ？
ｉｆ＾
ｔｒ
．
｛ｉ＊ｉ
：


图
４
－８
用户登陆界面


＜
５
－Ｃ？Ａ
５＊ｉ
：


：
＾Ｋ＂


？＝？权ＳＳ＾Ｓ＾
ＥＢ？Ｓ
ｉ抑
ＩｔＳＳ私碰《＿


疆


議


你
《？３？Ｓ
丨：％＊ｎ＊
１ｒｉ
ｌ＊


■


＿


Ｆｅｗ
－ｓｈｏｔＫＧＩ


＊
‘
Ｍｉｎｉ圍虐ａ
ｉｌ罕ａＩ


图
４
－９
用户管理界面


５０


第四章
少样本关系抽取系统设计与实现


在该系统中
，
用户需要上传数据
，
并对数据进行管理
。
上传数据界面和数据


管理界面如图
４
－
１０和图
４
－
１
１所示
。
上传数据时
，
可以选择本地文件
，
并对文件


进行备注
，
系统将对文件的格式进行判断
，
并对数据进行统计
。数据管理揭秘那


颗对数据进行管理
。


〇
Ｖ＜Ｃ
－
１５
？
￡３
１９
－
：
Ｓ＇〇＋Ｍ


－ｆ
；
；
＂
；＾
＿圈＿＿


图
４
－
１０数据上传界面


ｆｒＣ▲
不安金１０１
４３Ｗｉ
８？０￥０／＊？
；
ｓ５
＆☆＝ｉｉ


１


ａｓｅａｓｆｋ？ｅ


＆
－Ｗ？ｙ
５
－ｓＭｏｔ
ｆａｓＫ
ＳＳ
Ｔｅｓｔ
ｓａｒ＾ｐｔ
ｅｉＳＯｗｋｉ
ａａ
ｔａ
ｔｒａ
ｒＡ
ｉ？ｓ；ｘ＾Ｉｉ
ｌ４！


— —
？
：一—



ｉ


＿
：
Ｉ


Ｆｓｗ
－ｓｈｃｔＫＣ


？
：＃
？
■
－
＇
ａｇｇｉｉｉｉｇｇ


图
４
－
１
１
数据管理界面


５
１


北京邮电大学工程硕士学位论文


用户可以在数据管理界面中
，选择对应的数据生成图谱
，生成的图谱界面如


图
４
－
１２所示
，
在图谱管理界面中可以查看图谱的统计信息
、
图谱详情和对图谱


进行删除
。


；
－


｜
￥谓列表
ａ
ｉ
－ｒ
ｔＥａ格述Ｋｆ
ｌＳ


Ｅｎｔ
ｉｔｙ

ｉｖｊ
ｔｒｉ
３６７
Ｔｎｏ
ｌｃ

ｒｕ
ｉｍ
ＳＳＣ
ｒ
－ｃｎｔｎｏｎｅ£
ｆ數芦孩


Ｉ


Ｅｎｔ
？ｔｙ
Ｍｍ
７？６
Ｔ＾ｕｅ
ｒｈ＊ｎ
－６００
ｒｔ〇ｎｂｎｓｙ？ｆ
ｉ‘ Ｓ２！；
２Ｐ


Ｆ？？Ｙ

ｆｗｆ
ｉＣＴｎｐ
ｔｒ
ｎｕｎｖＳＣ
ｆＷｎｏｎｅ￡
＞？５
ｆｔＥ＾？


ｔｎｂｔｙ
ｎｕｍ３Ｈ１Ｔｒｃ＾ｅ
ｍ＾ｎ
２；３Ｇ
ｎｏｎｅｎｏｎｅ资卷Ｗ
ｉ§？？
ｊ￡Ｂ


Ｉ


一―Ｉ



ｊｊｊ＿ｉ
ｉ＿ｌ體跑１


图
４
－
１２
图谱管理界面


在图谱管理界面点击
“ 查看图谱
”弹出新界面
，
里面包括图谱的可视化
，
三元


组的详情
，并且可以根据模型的置信度分数对三元组的知识数据进行排序
，如图


４
－
１３和图
４
－
１４所示
，
并且在该界面可以对抽取的知识进行人工修改
，
并提供下


载功能
。


？
－ＣＡ
￣
＇Ｓｔ
１Ｃ１
．４３Ｗ
．８
Ｓ０ＳＧ／
）８
：
－
＇
？５
＾３＊Ｘ
：


Ｉ知识ｆ
ｉ３


５＊ｓ？夹系分ｓ


Ｅｎ
ｔ
ｉｒ／ｎｕｍ＾６Ｔ
ｒｉｐ
ｌｅｎｕｍ
：５０


ｃａｐｅ
ｇ

ｒａｒｄｓｄｗ
ｂｒｉｄｇｅ
ｔｒｓｓｓ
ｉｓｓｉｑｏ
ｉ
ｆ
ｉｖｅｃｆ〇ｕｓｍ
＞０
７６
＞
￡
ｒｔｒ
ｉＪＷ＾％
．


ｗ？Ｒ〇ｒ
ｂｆ？９〇
ｒ
ｉｖｅ
ｆ
？
．
＞？ｃｒｏｓｓｅ＊。力
＊
■
－
？＇ＫＳ画


ｔｏｌ
ｂ？ｎ？
５
ｉｅｗａＤａｓｈ
ｒｖｗ
ｃ
ｆｏｓｓｅｓ０
０２？Ｓｉ
－￡
．
＇
＊


ａｏｅｒｔ
ｂｏｏａｃ
ｉｈｊ＾ｒ
ｉｔｓｃ
：ｏｃｓｅｓ０
６５＜
Ｊ５Ｓ
＊


ｋ〇ｒｕｓ？ｖｍ？
＊＊
！ｋｙ＆
ａｏａｇｅｓｏｔｏ
ｕｖａｒ
＞ａ

？
ｉＭ
ｃｌａｓｓｅｓ０５３ｔ！＇ｆ

？ＢＨ
｜｜


＾５ｇ
ｎｊ６
ｂｎａ？ｅ
ｔｏｒｃｓｗｒ
ｔ＾
ｆｒａｗｓＣ３５ｆＨ


＾＾
Ｖｒ＾
ｌｃｒ
。Ｃｈａ
ｉｒ－
ｂｒｋＳｇｅＣａｒｎｊｈｅ
ｃｒｅｍｅｓ
（
５
３７缺ＨＲ


ｓｕ？３ｎ
ａｂｕ
ｌ
＞ａｋｉｒ

Ｉｘ
ｉｉＪｐｅｐａ
丨《ｒ？ａ

ｆ
ｉｖｅ？ｃｒｏｓｓｅｓ０
８８ｆ？＾
：Ｔ
＾Ｓ


＂
，
？ｒｓ
ｔｃ
ｏｏｏｎｅｃｊｓｉｇ
ｒ？
ｉ
－ｏ＆ｄ
ｂｒ
ｉｄｇｏＯｈ
ｉｏ
ｒ
ｔｖｅｒｍｏｓｓｅｓ０
２５￥
ｉ
？Ｓ


■
－备
一《Ｃ
．７６？
■


？
？
Ｍｔ＾ｖｙｖｗｇＴ
ｔｃ〇
．
－
＾ｐｅｔ
ｉｔ
ｔ〇ｎ
ｃ
ｉａｓｓ０
？２ｆｆｉ
：Ｓ！ｆｒ｜＾Ｈ


ｙｕｒ
ｉ
ａｒＳｉａｃｈａｋｓｘｆ
ｌｙｗｅ
ｉｇｈｔｃｃｔｉ
ｉｐｅ＾
－Ｊｎ
Ｃ？ｉｓｓ０＾６纪结


■？？＞？〇ｃｏａｘｅｓ


＞＾１
：
ｎｍａｈａｎｈ？３ｖ．ｉＭ？ｇＭｃ？
（ｒｐｅｔ
ｉ
５
ｉ〇ｎ
ｃＪａｓｓＧ
．７２綠
５ｊ§
：Ｓ？ＩＨ
ｊｔ


ＣＯＷＯｆｔ
：？
！？＾
Ｃ＾３ＳＫ
；
ｒ
ｊｊ
ｉｃ
＜
－
ｖ
ｒｏｏｅｎ
Ｓｖ
－
ｓｇａｎ＾？ａｖ＾ｖ＜ｗｇｍｃｃｎ＾ｅｔｆ
ｔ
ｔｏｎ
ｃｉａｓｓ０８４修这
駿龙ＢＭ


＝ｏｎ５ｉ？６
ｉｔｔ
ｉｅｖ
．
ｆＭＭＵ


■
？？？＞ｐｏｓＳ
ｉｏｒ
ｐ＾
＊
ｙ９Ｃ
ｌ
９＾
ｔ？ａ
＊
ｒ
．
ＢＢＨ
｜


ｉ國
⑵画ｕ


图
４
－
１３
图谱详情界面


５２


第四章
少样本关系抽取系统设计与实现


—ＣＴ▲
孑安金１Ｄ１
４３
．
１９１
．８
；８〇ａ＾／
Ｓｓｇ
Ｓｒ亡
亡Ｚ


— —
？＊■


＇＾＾＾＾
：
一”：
；
二：：＿


—
卜：
：
：
：
：
：■


ｗｍｂ
：脚狄撕部
站
狱
＿ｇ


＾＾ＰＢＭｒ
｜
ｌｌ
ｌ？Ｍ

＇
…
州ＷＷＭｈｗ＿
？ｆＷ０？热
繫珐｜


—：：
：
：：＝■


一—一
… … ■


一啦
＿＿＿，ｎ


图
４
－
１４
图谱三元组修改界面


本系统进行通过上述界面完成了系统中的用户管理模块
、数据操作模块
、少


样本关系抽取模块的搭建
，
能够完成少样本关系抽取系统的基本功能
。


４
．４
．３非功能性测试


本节主要对系统的兼容性和流畅性进行测试
，
以及少样本学习算法的时间消


耗进行测试
。在流畅性测试中
，
本文进行了登陆界面速度
、页面切换速度
、接口


响应速度的测试
。在兼容性中
，本文进行了浏览器兼容性的测试
，
以验证该系统


的运行情况
。
具体测试结果如表
４
－
１
１所示
，
可以看到测试结果满足系统需求分


析中的要求
。


表
４
－
１
１
系统非功能性测试结果


须赋内容测试用例测试结果


登陆页面速度通过登陆界面提交登陆信息到系统后台
胃


以内
。


并返回结果
。


点击不同的菜单栏切换右侧的不同功能
页面加载时间
Ｉ
ｓ以


区
。内
。


接口响应速度页面提交请求到后端响应
。服务器响５２时
Ｉ司在


２ｓ
－５ｓ之间
。


浏览器兼容性在Ｃｈｒｏｍｅ和
Ｓａｆａｒ
ｉ浏览器登陆系统界页面展示正常
，
系


ｆ
ｉ
〇统运行正常
。


系统在保证稳定流畅运行的基础上
，
需要利用少样本数据进行关系抽取任务
。


由于少样本关系抽取系统是基于文本对的形式
，即将测试文本与标注数据进行拼


接
，所以标注数据量的增多
，会增加模型推理时间的损耗
。在不同少样本设置下
，


５３


北京邮电大学工程硕士学位论文


单条样本推理时耗情况如表
４
－
１２所示
，
可见该少样本关系抽取系统在大多数情


况下的推理时间在接受范围内
。


表４
－
１２模型推理时耗情况


少样本设置平均时耗时耗方差


５
－ｗａｙ
１
－ｓｈｏｔ４０ｍｓ＾ｍｓ


５
－ｗａｙ５
－
ｓｈｏｔ
１８２ｍｓ３
１ｍｓ


１０
－ｗａｙ
１
－ｓｈｏｔ１７４ｍｓ
４２ｍｓ


１０
－ｗａｙ５
－ｓｈｏｔ３９８ｍｓ１３４ｍｓ


４
．５本章小结


本章介绍了少样本关系抽取系统的设计与实现的具体细节
，
包括需求分析
、


系统概要设计
、系统的各个功能模块的详细实现方案
。整个系统的功能模块包括


数据管理模块
，用户管理模块
，关系抽取模块三个部分
，通过这三部分的有效组


合
，能够完成少样本关系抽取系统的基本功能
。在测试阶段对系统的各个功能模


块的功能性和非功能性进行了测试
，
以验证系统的运行流畅度和兼容性
，并且对


少样本关系抽取模型的性能时耗进行了测试
。通过该系统
，少样本关系抽取模型


能够应对新关系或是特殊领域上的关系建模问题
，
提升了关系抽取任务的效率
。


５４


第五章总结与展望


第五章总结与展望


５
．
１
总结


本文针对少样本关系抽取任务进行研宄
，设计并实现了基于实体概念增强的


少样本关系抽取模型
，能够在新关系下利用较少标注数据实现关系抽取模型的快


速搭建。此外
，基于上述少样本关系抽取模型
，本文设计并实现了少样本关系抽


取系统
。本文主要取得了以下成果
：


１
．
设计了基于实体概念指导的少样本关系抽取模型
。
该模型通过设计鲁棒


性较强的概念知识融入模块
，将实体概念知识与语义信息进行结合
，有效


提升了少样本关系抽取任务的模型性能
，
在领域迁移下的少样本关系抽


取任务效果提升尤为显著
。此外
，针对目标领域己知的情况
，本文设计了


元学习增强模块
，
通过对比学习正则项和领域导向性训练进
一步提升该


情况下的少样本关系抽取模型性能
。


２
．
设计并实现了少样本关系抽取系统
。本文基于上述少样本关系抽取模型
，


通过数据管理模块
、用户管理模块和关系抽取模块
，实现少样本关系抽取


系统的搭建
。用户通过该系统可以实现训练数据和测试数据上传
，新型关


系建模和推理
，
并对抽取到结果进行管理和可视化等操作
。


总体来说
，本文利用实体概念知识作为知识库
，设计了知识融合模块
，在不


同领域和类型的知识库上均提升少样本关系抽取任务的性能
，并且针对目标领域


已知的情况设计了对应的适配模块
。本文还将研究模型进行落地应用
，设计了少


样本关系抽取系统
，
方便用户完成新型关系或领域关系的快速建模工作
。


５
．２展望


本文通过对少样本关系抽取任务的深入研究
，设计并实现了基于实体概念增


强的少样本关系抽取模型和基于该模型的少样本关系抽取系统
。由于时间和人力


等问题的制约
，
仍然有
一些需要改进的地方
：


１
．
本文设计的少样本关系抽取模型使用的测试数据具有
一定限制
，
即类别


需要存在于用户定义的关系类别集合中
。
但在实际应用中
，
该要求很难


被满足
，测试数据中的大部分是不会存在于用户定义的关系类别集合中
。


５５


北京邮电大学工程硕士学位论文


该种情况可以控制模型预测结果的置信度快速实现
，
但由于缺乏领域迁


移背景下的该类型数据
，
本文并未对此进行实验验证
。


２
．
本文设计的少样本关系抽取模型需要利用实体的概念作为辅助知识融入


模型
，
实验中数据集的知识覆盖率较高
。
在实际应用中
，
知识库的不充


足问题可能会导致少样本关系抽取模型性能上具有相应的隐患
。


５６


参考文献


参考文献


［
１
］ＳｉｎｇｈａｌＡ
．Ｉｎｔｒｏｄｕｃｉｎｇｔｈｅｋｎｏｗｌｅｄｇｅ
ｇｒａｐｈ：ｔｈｉｎｇｓ
，ｎｏｔｓｔｒｉｎｇｓ［Ｊ
］
．Ｏｆｆ
ｉｃｉａｌ
ｇｏｏｇ
ｌｅ


ｂｌｏｇ，２０
１２
．


［２
］Ｑｕｉｌｌｉａｎ
，Ｍ
．Ｒ
．
，Ｓｅｍａｎｔｉｃｎｅｔｗｏｒｋｓ［Ｊ］
，ＡｐｐｒｏａｃｈｅｓｔｏＫｎｏｗｌｅｄｇｅＲｅｐｒｅｓｅｎｔａｔｉｏｎ


ＲｅｓｅａｒｃｈＳｔｕｄｉｅｓ
，１９６８
，２３（９２
）
：１
－５０
．


［３
］ＬｅｄｅｒｂｅｒｇＪ．ＤＥＮＤＲＡＬ
－６４
：ＡＳｙｓｔｅｍｆｏｒＣｏｍｐｕｔｅｒＣｏｎｓｔｒｕｃｔｉｏｎ
，Ｅｎｕｍｅｒａｔｉｏｎ


ａｎｄＮｏｔａｔｉｏｎｏｆＯｒｇａｎｉｃＭｏｌｅｃｕｌｅｓａｓＴｒｅｅＳｔｒｕｃｔｕｒｅｓａｎｄＣｙｃｌｉｃＧｒａｐｈｓ
．Ｐａｒ
ｔＩＩ
．


ＴｏｐｏｌｏｇｙｏｆＣｙｃｌｉｃＧｒａｐｈｓ［Ｊ］
．ＹｕｎｎａｎＥｎｖｉｒｏｎｍｅｎｔａｌｅｎｃｅ
，
１９６５
．


［４
］ＦｅｉｇｅｎｂａｕｍＥＡ．Ｅｘｐｅｒｔｓｙｓｔｅｍｓｉｎｔｈｅ１９８０ｓ
［Ｊ
］
．Ｓｔａｔｅｏｆ
ｔｈｅａｒｔｒｅｐｏｒｔｏｎｍａｃｈｉｎｅ


ｉｎｔｅｌｌｉｇｅｎｃｅ．Ｍａｉｄｅｎｈｅａｄ
：Ｐｅｒｇａｍｏｎ
－Ｉｎｆｏｔｅｃｈ
，１９８
１
．


［５
］Ｂｅｒｎｅｒｓ
－ＬｅｅＴ
．Ｉｎｆｏｒｍａｔｉｏｎｍａｎａｇｅｍｅｎｔ
：Ａ
ｐｒｏｐｏｓａｌ
［Ｊ
］
．１９８９
．［６
］


［６
＼Ｂｅｍｅｒｓ
－ＬｅｅＴ
．ＷｈａｔｔｈｅＳｅｍａｎｔｉｃＷｅｂｃａｎｒｅｐｒｅｓｅｎｔ
［Ｊ］
，１９９８
．


［７
］Ｂｅｍｅｒｓ
－ＬｅｅＴ
．Ｌｉｎｋｅｄｄａｔａ
－ｄｅｓｉｇｎｉｓｓｕｅｓ［Ｊ］
．ｈｔ〇
）
：／／ｗｗｗ
．ｗ３
．ｏｒｇ
／ＤｅｓｉｇｎＩｓｓｕｅｓ／


ＬｉｎｋｅｄＤａｔａ
．ｈｔｍｌ
，２００６
．


［８
］Ｌｉｕ
Ｑ，ＬｉＹ
，ＤｕａａＨ，ｅｔａｌ
．Ｋｎｏｗｌｅｄｇｅ
ｇｒａｐｈｃｏｎｓｔｒｕｃｔｉｏｎｔｅｃｈｎｉｑｕｅｓ
［Ｊ
］
．Ｊｏｕｒ
ｎａｌ


ｏｆＣｏｍｐｕｔｅｒＲｅｓｅａｒｃｈａｎｄＤｅｖｅｌｏｐｍｅｎｔ
，２０１６
．


［９
］Ｓｕｎｃｏｎｇ，Ｚｈｅｎｇ，Ｙｕｅｘｉｎｇ，ｅｔａｌ
．Ｊｏｉｎｔｅｎｔｉｔｙａｎｄｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎｂａｓｅｄｏｎａ


ｈｙｂｒ
ｉｄｎｅｕｒａｌｎｅｔｗｏｒｋ
［Ｊ
］
．Ｎｅｕｒｏｃｏｍｐｕｔｉｎｇ，２０１７
．


［
１０
］ＨｗａｎＫｉｍ
，ＷｏｏｄｌａｎｄＰＣ
．Ａｒｕｌｅ
－ｂａｓｅｄｎａｍｅｄｅｎｔｉｔｙｒｅｃｏｇｎｉｔｉｏｎｓｙｓｔｅｍｆｏｒ


ｓｐｅｅｃｈｉｎｐｕｔ
［Ａ
］
．／／Ｉｎｔｅｒ
ｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＳｐｏｋｅｎＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ［Ｃ
］
．


ＤＢＬＰ
Ｓ２０００
．


［
１
１
］ＭｏｒｗａｌＳ
，ＪａｈａｎＮ
，ＣｈｏｐｒａＤ
．ＮａｍｅｄｅｎｔｉｔｙｒｅｃｏｇｎｉｔｉｏｎｕｓｉｎｇｈｉｄｄｅｎＭａｒｋｏｖ


ｍｏｄｅｌ
（ＨＭＭ）［Ｊ
］
．ＩｎｔｅｒｎａｔｉｏｎａｌＪｏｕｒ
ｎａｌｏｎＮａｔｕｒａｌＬａｎｇｕａｇｅＣｏｍｐｕｔｉｎｇ（ＩＪＮＬＣ）


Ｖｏｌ
，２０１２
，Ｌ


［
１２
］ＬｉｎＹ
，ＳｕｎＣ
，ＬｉＳ
，ｅｔａｌ
．ＣＲＦ
－ｂａｓｅｄＡｃｔｉｖｅＬｅａｒｎｉｎｇｆｏｒＣｈｉｎｅｓｅＮａｍｅｄＥｎｔｉｔｙ


Ｒｅｃｏｇｎｉｔｉｏｎ
［Ａ
］
．／／ＩＥＥＥＩｎｔｅｒ
ｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＳｙｓｔｅｍｓ
，Ｍａｎ＆Ｃｙｂｅｒ
ｎｅｔｉｃｓ


［Ｃ
］
，ＩＥＥＥ
，２００９
．


［
１３
］ＺｅｎｇＤ
？ＳｕｎＣ
５ＬｉｎＬ
，ｅｔａｌ
．ＬＳＴＭ
－ＣＲＦｆｏｒｄｒｕｇ
－ｎａｍｅｄｅｎｔｉｔｙｒｅｃｏｇｎｉｔｉｏｎ［Ｊ
］
，


Ｅｎｔｒｏｐｙ，２０１７
，１９
（６）
：２８３
．


［
１４
］ＬｉｕＺ
？ＷａｎｇＸ
５ＣｈｅｎＱ？ｅｔａｌ
．ＣｈｉｎｅｓｅＣｌｉｎｉｃａｌＥｎｔｉｔｙＲｅｃｏｇｎｉｔｉｏｎｖｉａＡｔｅｎｔｉｏｎ
－


ＢａｓｅｄＣＮＮ
－ＬＳＴＭ
－ＣＲＦ［Ａ
］
．／／２０１８ＩＥＥＥＩｎｔｅｒ
ｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎ


ＨｅａｌｔｈｃａｒｅＩｎｆｏｎｎａｔｉｃｓＷｏｒｋｓｈｏｐ（ＩＣＨＩ
－Ｗ）［Ｃ
］
，ＩＥＥＥＣｏｍｐｕｔｅｒＳｏｃｉｅｔｙ，


２０１８
：６８
－６９
．


５７


北京邮电大学工程硕士学位论文


［
１５
］Ｚｕｋｏｖ
－ＧｒｅｇｏｒｉｃＡ
，ＢａｃｈｒａｃｈＹ
，ＭｉｎｋｏｖｓｋｙＰ
，ｅｔａｌ
．ＮｅｕｒａｌＮａｍｅｄＥｎｔｉｔｙ


ＲｅｃｏｇｎｉｔｉｏｎＵｓｉｎｇａＳｅｌｆ
－ＡｔｔｅｎｔｉｏｎＭｅｃｈａｎｉｓｍ［Ａ
］
．／／２０１７ＩＥＥＥ２９ｔｈ


Ｉｎｔｅｒ
ｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＴｏｏｌｓｗｉｔｈＡｒｔｉｆｉｃｉａｌＩｎｔｅｌｌｉｇｅｎｃｅ
（
ＩＣＴＡＩ
）［Ｃ
］
．ＩＥＥＥ
，


２０１７
．


［
１６
］ＧｕＬ
，ＺｈａｎｇＷ
，ＷａｎｇＹ
，ｅｔａｌ
．ＮａｍｅｄＥｎｔｉｔｙＲｅｃｏｇｎｉｔｉｏｎｉｎＪｕｄｉｃｉａｌＦｉｅｌｄＢａｓｅｄ


ｏｎＢＥＲＴ
－ＢｉＬＳＴＭ
－ＣＲＦＭｏｄｅｌ［Ａ
］
．／／２０２０Ｉｎｔｅｒ
ｎａｔｉｏｎａｌＷｏｒｋｓｈｏｐｏｎＥｌｅｃｔｒｏｎｉｃ


ＣｏｍｍｕｎｉｃａｔｉｏｎａｎｄＡｒｔｉｆ
ｉｃｉａｌＩｎｔｅｌｌｉｇｅｎｃｅ
（ＩＷＥＣＡＩ
）［Ｃ
］
．２０２０
．


［
１７］ＳｏｕｚａＦ
，ＮｏｇｕｅｉｒａＲ
，ＬｏｔｕｆｏＲ．ＰｏｒｔｕｇｕｅｓｅＮａｍｅｄＥｎｔｉｔｙＲｅｃｏｇｎｉｔｉｏｎｕｓｉｎｇ


ＢＥＲＴ
－ＣＲＦ［Ｓ
］
．２０１９
．


［
１８
］ＺｅｎｇＤ
，ＬｉｕＫ
，ＬａｉＳ
，ｅｔａｌ
．Ｒｅｌａｔｉｏｎｃｌａｓｓｉｆ
ｉｃａｔｉｏｎｖｉａｃｏｎｖｏｌｕｔｉｏｎａｌｄｅｅｐｎｅｕｒａｌ


ｎｅｔｗｏｒｋ［Ａ
］
．／／ＰｒｏｃｅｅｄｉｎｇｓｏｆＣＯＬＩＮＧ２０
１４
，ｔｈｅ２５ｔｈｉｎｔｅｒ
ｎａｔｉｏｎａｌｃｏｎｆｅｒｅｎｃｅ


ｏｎｃｏｍｐｕｔａｔｉｏｎａｌｌｉｎｇｕｉｓｔｉｃｓ
：ｔｅｃｈｎｉｃａｌ
ｐａｐｅｒｓ［Ｃ
］
，２０１４
：２３３５
－２３４４．


［
１９］ＪｉａｎｇＪ
，ＺｈａｉＣＸ
．Ａｓｙｓｔｅｍａｔｉｃｅｘｐ
ｌｏｒａｔｉｏｎｏｆｔｈｅｆｅａｔｕｒｅｓｐａｃｅｆｏｒｒｅｌａｔｉｏｎ


ｅｘｔｒａｃｔｉｏｎ［Ａ
］
．／／ＨｕｍａｎＬａｎｇｕａｇｅＴｅｃｈｎｏｌｏｇ
ｉｅｓ２００７
：ＴｈｅＣｏｎｆｅｒｅｎｃｅｏｆｔｈｅ


ＮｏｒｔｈＡｍｅｒ
ｉｃａｎＣｈａｐ
ｔｅｒｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ
；


ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＭａｉｎＣｏｎｆｅｒＣ
Ｔｃｅ［Ｃ
］
．２００７
：１
１３
－
１２０
．


［２０
］ＮｇｕｙｅｎＴＨ
，Ｇｒ
ｉｓｈｍａｎＲ．Ｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎ
：Ｐｅｒｓｐｅｃｔｉｖｅｆ
ｒｏｍｃｏｎｖｏｌｕｔｉｏｎａｌ


ｎｅｕｒａｌｎｅｔｗｏｒｋｓ［Ａ
］
．／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ１ｓｔｗｏｒｋｓｈｏｐｏｎｖｅｃｔｏｒｓｐａｃｅｍｏｄｅｌｉｎｇ


ｆｏｒｎａｔｕｒａｌｌａｎｇｕａｇｅ
ｐｒｏｃｅｓｓｉｎｇ［Ｃ
］
．２０１５
：３９
－４８
．


ｐ
ｉ
］ＫａｍｂｈａｔｌａＮ．Ｃｏｍｂｉｎｉｎｇ
ｌｅｘｉｃａｌ
，
ｓｙｎｔａｃｔ
ｉｃ
，
ａｎｄｓｅｍａｎｔｉｃｆｅａｔｕｒｅｓｗｉｔｈｍａｘｉｍｕｍ


ｅｎｔｒｏｐｙｍｏｄｅｌｓｆｏｒｉｎｆｏｒｍａｔｉｏｎｅｘｔｒａｃｔｉｏｎ［Ａ
］
．／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＡＣＬ


Ｉｎｔｅｒ
ａｃｔｉｖｅＰｏｓｔｅｒａｎｄＤｅｍｏｎｓｔｒａｔｉｏｎＳｅｓｓｉｏｎｓ［Ｃ
］
．２００４
：１７８
－
１８１
．


［２２
］ＺｈａｎｇＭ
，ＺｈａｎｇＪ
，
ＳｕＪ
，
ｅｔａｌ
．Ａｃｏｍｐｏｓｉｔｅｋｅｒ
ｎｅｌｔｏｅｘｔｒａｃｔｒｅｌａｔｉｏｎｓｂｅｔｗｅｅｎ


ｅｎｔ
ｉｔ
ｉｅｓｗｉｔｈｂｏｔｈｆ
ｌａｔａｎｄｓｔｒｕｃｔｕｒｅｄｆｅａｔｕｒｅｓ［Ａ
］
．／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２
１ｓｔ


Ｉｎｔｅｒ
ｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓａｎｄ４４ｔｈＡｎｎｕａｌＭｅｅｔｉｎｇ


ｏｆ
ｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ［Ｃ
］
．２００６
：８２５
－８３２
．


［２３
］ＭｏｏｎｅｙＲ
，ＢｕｎｅｓｃｕＲ．Ｓｕｂｓｅｑｕｅｎｃｅｋｅｒ
ｎｅｌｓｆｏｒｒｅｌａｔ
ｉｏｎｅｘｔｒａｃｔｉｏｎ［Ｊ
］
．Ａｄｖａｎｃｅｓ


ｉｎｎｅｕｒａｌｉｎｆｏｒｍａｔｉｏｎ
ｐｒｏｃｅｓｓｉｎｇｓｙｓｔｅｍｓ
，２００５
，１８
．


［２４
］ＺｈａｏＳ
，Ｇｒ
ｉｓｈｍａｎＲ．Ｅｘｔｒａｃｔｉｎｇｒｅｌａｔｉｏｎｓｗｉｔｈｉｎｔｅｇｒａｔｅｄｉｎｆｏｒｍａｔｉｏｎｕｓｉｎｇｋｅｒ
ｎｅｌ


ｍｅｔｈｏｄｓ［Ａ］
．／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ４３ｒｄａｎｎｕａｌｍｅｅｔｉｎｇｏｆｔｈｅａｓｓｏｃｉａｔｉｏｎｆｏｒ


ｃｏｍｐｕｔａｔｉｏｎａｌｌｉｎｇｕｉｓｔｉｃｓ
［Ｃ
］
．２００５
：４１９
－４２６
．


［２５
］ＷａｎｇＭ
．Ａｒｅ
－ｅｘａｍｉｎａｔｉｏｎｏｆ
ｄｅｐｅｎｄｅｎｃｙｐａｔｈｋｅｒｎｅｌｓｆｏｒｒｅｌａｔ
ｉｏｎｅｘｔｒａｃｔｉｏｎ
［Ａ
］
．


／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＴｈｉｒｄＩｎｔｅｒｎａｔｉｏｎａｌＪｏｉｎｔＣｏｎｆｅｒｅｎｃｅｏｎＮａｔｕｒａｌＬａｎｇｕａｇｅ


Ｐｒｏｃｅｓｓｉｎｇ［Ｃ］
．２００８
：ＶｏＩｕｍｅ
－ＩＬ


５８


参考文献


［２６
］ＢｕｎｅｓｃｕＲＣ
５ＭｏｏｎｅｙＲＪ．Ａｓｈｏｒｔｅｓｔｐａｔｈｄｅｐｅｎｄｅｎｃｙｋｅｒ
ｎｅｌｆｏｒｒｅｌａｔｉｏｎ


ｅｘｔｒａｃｔｉｏｎ
［Ａ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆ
ｔｈｅｃｏｎｆｅｒｅｎｃｅｏｎｈｕｍａｎｌａｎｇｕａｇｅｔｅｃｈｎｏｌｏｇｙａｎｄ


ｅｍｐ
ｉｒｉｃａｌｍｅｔｈｏｄｓｉｎｎａｔｕｒａｌｌａｎｇｕａｇｅ
ｐｒｏｃｅｓｓｉｎｇ［Ｃ
］
．２００５
：７２４
－７３
１
．


［２７
］ＪａｓｏｎＷｅｓｔｏｎ
，ＡｎｔｏｉｎｅＢｏｒｄｅｓ
，ＯｋｓａｎａＹａｋｈｎｅｎｋｏ
，ａｎｄＮｉｃｏｌａｓＵｓｕｎｉｅｒ．


Ｃｏｎｎｅｃｔｉｎｇ
ｌａｎｇｕａｇｅａｎｄｋｎｏｗｌｅｄｇｅｂａｓｅｓｗｉｔｈｅｍｂｅｄｄｉｎｇｍｏｄｅｌｓｆｏｒｒｅｌａｔｉｏｎ


ｅｘｔｒａｃｔｉｏｎ［Ａ
］
．ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆＥＭＮＬＰ
［Ｃ
］
．２０１３
：１３６６
－
１３７１
．


［２８
］ＭａｔｈｅｗＲＧｏｒｍｌｅｙ，ＭｏＹｕ
，ａｎｄＭａｒｋＤｒｅｄｚｅ．Ｉｍｐｒｏｖｅｄｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎｗｉｔｈ


ｆｅａｔｕｒｅ
－ｒ
ｉｃｈｃｏｍｐｏｓｉｔ
ｉｏｎａｌｅｍｂｅｄｄｉｎｇｍｏｄｅｌｓ［Ａ
］

＿
／／ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆＥＭＮＬＰ


［Ｃ
］
．２０１５：１７７４
＾１７８４
．


＼２９
］ＣｈｕｎｙａｎｇＬｉｕ
，ＷｅｎｂｏＳｕｎ
，ＷｅｎｈａｎＣｈａｏ
，ａｎｄＷａｎｘｉａｎｇＣｈｅ．Ｃｏｎｖｏｌｕｔｉｏｎ


ｎｅｕｒａｌｎｅｔｗｏｒｋｆｏｒｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎ［Ａ
］
．／／ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆＩＣＤＭ［Ｃ
］
，２０１３
：


２３
１
—２４２．


［３０
］ＤＺｅｎｇ，ＫａｎｇＬ
，ＣｈｅｎＹ
，ｅｔａｌ
．ＤｉｓｔａｎｔＳｕｐｅｒｖｉｓｉｏｎｆｏｒＲｅｌａｔｉｏｎＥｘｔｒａｃｔｉｏｎｖｉａ


ＰｉｅｃｅｗｉｓｅＣｏｎｖｏｌｕｔｉｏｎａｌＮｅｕｒａｌＮｅｔｗｏｒｋｓ［Ａ
］
．／／ＣｏｎｆｅｒｅｎｃｅｏｎＥｍｐ
ｉｒｉｃａｌ


ＭｅｔｈｏｄｓｉｎＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ［Ｃ
］
．２０１５
．


ｐ
ｉ
］ＳｈｕＺｈａｎｇ，ＤｅｑｕａｎＺｈｅｎｇ，ＸｉｎｃｈｅｎＨｕ
，ａｎｄＭｉｎｇＹａｎｇ
．Ｂ
ｉｄｉｒｅｃｔｉｏｎａｌｌｏｎｇｓｈｏｒ
ｔ
－


ｔｅｒｍｍｅｍｏｒｙｎｅｔｗｏｒｋｓｆｏｒｒｅｌａｔｉｏｎｃｌａｓｓｉｆ
ｉｃａｔｉｏｎ
［Ａ
］
．／／ＩｎＰｒｏｃｅｅｄｉｎｇｓｏｆ
ＰＡＣＬＩＣ


［Ｃ
］
．２０１５
：７３
－７８
．


［３２
］ＳｈｉＷ
５ＳｈｅｎｇＧ
．ＲｅｌａｔｉｏｎＥｘｔｒａｃｔｉｏｎｖｉａＰｏｓｉｔｉｏｎ
－ＥｎｈａｎｃｅｄＣｏｎｖｏｌｕｔｉｏｎａｌＮｅｕｒａｌ


Ｎｅｔｗｏｒｋ［Ａ
］
．／／ＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＩｎｔｅｌｌｉｇｅｎｔＥｎｖｉｒｏｎｍｅｎｔｓ［Ｃ
］
．ＩＥＥＥ
，


２０１７
．


［３３
］ＤｅｖｌｉｎＪ
，ＣｈａｎｇＭＷ
，ＬｅｅＫ
，ｅｔａｌ
．ＢＥＲＴ；Ｐｒｅ
－ｔｒａｉｎｉｎｇｏｆＤｅｅｐＢｉｄｉｒｅｃｔｉｏｎａｌ


ＴｒａｎｓｆｏｒｍｅｒｓｆｏｒＬａｎｇｕａｇｅＵｎｄｅｒｓｔａｎｄｉｎｇ［Ｊ
］
．２０１８
．


［３４
］ＭｉｗａＭ，ＳａｓａｋｉＹ．ＭｏｄｅｌｉｎｇＪｏｉｎｔＥｎｔｉｔｙａｎｄＲｅｌａｔｉｏｎＥｘｔｒａｃｔｉｏｎｗｉｔｈＴａｂｌｅ


Ｒｅｐｒｅｓｅｎｔａｔｉｏｎ［Ａ
］
，／／ＣｏｎｆｅｒｅｎｃｅｏｎＥｍｐ
ｉｒｉｃａｌＭｅｔｈｏｄｓｉｎＮａｔｕｒａｌＬａｎｇｕａｇｅ


Ｐｒｏｃｅｓｓｉｎｇ［Ｃ］
．２０１４．


［３５
］ＦａｂｂｒｉＡＲ
，ＨａｎＳ
５ＬｉＨ
，ｅｔａｌ
．ＩｍｐｒｏｖｉｎｇＺｅｒｏａｎｄＦｅｗ
－ＳｈｏｔＡｂｓｔｒａｃｔｉｖｅ


ＳｕｍｍａｒｉｚａｔｉｏｎｗｉｔｈＩｎｔｅｒｍｅｄｉａｔｅＦｉｎｅ
－ｔｕｎｉｎｇａｎｄＤａｔａＡｕｇｍｅｎｔａｔｉｏｎ［Ｊ］
，２０２０
．


［３６
］ＭＲｅｎ
，ＥＴｒｉａｎｔａｆ
ｉｌｌｏｕ
，ＳＲａｖｉ
，ｅｔａｌ
．Ｍｅｔａ
－ＬｅａｍｉｎｇｆｏｒＳｅｍｉ
－ＳｕｐｅｒｖｉｓｅｄＦｅｗ
－


ＳｈｏｔＣｌａｓｓｉｆ
ｉｃａｔｉｏｎ［Ｊ
］
，２０１８
．


［３７
］ＸｕＸ
，ＷａｎｇＧ
，ＫｉｍＹＢ
，ｅｔａｌ
．ＡｕｇＮＬＧ
：Ｆｅｗ
－ｓｈｏｔＮａｔｕｒａｌＬａｎｇｕａｇｅＧｅｎｅｒａｔｉｏｎ


ｕｓｉｎｇＳｅｌｆ
－ｆ
ｒａｉｎｅｄＤａｔａＡｕｇｍｅｎｔａｔｉｏｎ
［Ｊ］
．２０２１
．


［３８
］ＫｏｃｈＧ
，ＺｅｍｅｌＲ
，
ＳａｌａｋｈｕｔｄｉｎｏｖＲ．
Ｓｉａｍｅｓｅｎｅｕｒａｌｎｅｔｗｏｒｋｓｆｏｒｏｎｅ
－ｓｈｏｔｉｍａｇｅ


ｒｅｃｏｇｎｉｔｉｏｎ［Ａ
］
，／／ＩＣＭＬｄｅｅｐ
ｌｅａｒ
ｎｉｎｇｗｏｒｋｓｈｏｐ［Ｃ
］
．２０１５
．


５９


北京邮电大学工程硕士学位论文


［３９
］ＳｎｅｌｌＪ
，
ＳｗｅｒｓｋｙＫ
，
ＺｅｍｅｌＲ．Ｐｒｏｔｏｔｙｐ
ｉｃａｌｎｅｔｗｏｒｋｓｆｏｒｆｅｗ
－ｓｈｏｔｌｅａｍｉｎｇ［Ｊ］
．


Ａｄｖａｎｃｅｓｉｎｎｅｕｒａｌｉｎｆｏｒｍａｔｉｏｎ
ｐｒｏｃｅｓｓｉｎｇｓｙｓｔｅｍｓ
，２０１７
，３０
．


［４０
］ＧｅｎｇＲ
，ＬｉＢ
，ＬｉＹ
，ｅｔａｌ
．Ｉｎｄｕｃｔｉｏｎｎｅｔｗｏｒｋｓｆｏｒｆｅｗ
－ｓｈｏｔｔｅｘｔｄａｓｓｉｆｉｃａｔｉｏｎ［Ｊ
］
．


ａｒＸｉｖ
ｐｒｅｐｒ
ｉｎｔａｒＸｉｖ：１９０２
．
１０４８２
，２０１９
．


［４１
］ＧａｏＴ
，ＨａｎＸ
，ＬｉｕＺ
，ｅｔａｌ
．Ｈｙｂｒ
ｉｄａｔｔｅｎｔｉｏｎ
－ｂａｓｅｄ
ｐｒｏｔｏｔｙｐ
ｉｃａｌｎｅｔｗｏｒｋｓｆｏｒｎｏｉｓｙ


ｆｅｗ
－ｓｈｏｔｒｅｌａｔｉｏｎｃｌａｓｓｉｆｉｃａｔｉｏｎ［Ａ
］
，／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＡＡＡＩＣｏｎｆｅｒｅｎｃｅｏｎ


ＡｒｔｉｆｉｃｉａｌＩｎｔｅｌｌｉｇｅｎｃｅ［Ｃ
］
．２０
１９
：６４０７
－６４１４
．


［４２
］ＤｈｉｌｌｏｎＧＳ
，Ｃｈａｕｄｈａｒ
ｉＰ
，ＲａｖｉｃｈａｎｄｒａｎＡ
，ｅｔａｌ
．Ａｂａｓｅｌｉｎｅｆｏｒｆｅｗ
－ｓｈｏｔｉｍａｇｅ


ｃｌａｓｓｉｆ
ｉｃａｔｉｏｎ
！＾］

？ａｒＸｉｖ
ｐｒｅｐｒ
ｉｎｔａｒＸｉｖ：１９０９
．０２７２９
，２０
１９
．


［４３］ＢｅｒｔｉｎｅｔｏＬ
？Ｈｅｎｒ
ｉｑｕｅｓＪＦ
，ＴｏｒｒＰＨＳ
５ｅｔａｌ．Ｍｅｔａ
－
ｌｅａｒｎｉｎｇｗｉｔｈｄｉｆｆｅｒｅｎｔｉａｂｌｅ


ｃｌｏｓｅｄ
－ｆｏｒｍｓｏｌｖｅｒｓ［Ｊ］
．ａｒＸｉｖ
ｐｒｅｐｒ
ｉｎｔａｒＸｉｖ
：１８０５
．０８
１３６
，２０１８
．


［４４
］ＦｉｎｎＣ
，ＡｂｂｅｅｌＰ
，ＬｅｖｉｎｅＳ
．Ｍｏｄｅｌ
－ａｇｎｏｓｔｉｃｍｅｔａ
－
ｌｅａｍｉｎｇｆｏｒｆａｓｔａｄａｐ
ｔａｔｉｏｎｏｆ


ｄｅｅｐｎｅｔｗｏｒｋｓ［Ａ
］
，／／Ｉｎｔｅｒ
ｎａｔｉｏｎａｌｃｏｎｆｅｒｅｎｃｅｏｎｍａｃｈｉｎｅｌｅａｒｎｉｎｇ［Ｃ
］
．ＰＭＬＲ
，


２０１７
：１
１２６
－
１
１３５
．


［４５
］ＮｉｃｈｏｌＡ
，ＳｃｈｕｌｍａｎＪ．Ｒｅｐ
ｔｉｌｅ
：ａｓｃａｌａｂｌｅｍｅｔａｌｅａｍｉｎｇａｌｇｏｒｉｔｈｍ
［Ｊ
］
．ａｒＸｉｖ


ｐｒｅｐｒ
ｉｎｔａｒＸｉｖ
：１８０３
．０２９９９
，２０
１８
，２
（３）
：４．


［４６
］ＬｉＺ
？ＺｈｏｕＦ
，ＣｈｅｎＦ
，ｅｔａｌ
．Ｍｅｔａ
－ｓｇｄ：Ｌｅａｒｎｉｎｇ
ｔｏｌｅａｒｎｑｕｉｃｋｌｙｆｏｒｆｅｗ
－ｓｈｏｔ


ｌｅａｍｉｎｇ［Ｊ］
．ａｒＸｉｖ
ｐｒｅｐｒｉｎｔａｒＸｉｖ：１７０７
．０９８３５
，２０１７
．


［４７
］ＳｃｈｉｃｋＴ
，ＳｃｈｉｉｔｚｅＨ
．Ｅｘｐ
ｌｏｉｔｉｎｇｃｌｏｚｅ
ｑｕｅｓｔｉｏｎｓｆｏｒｆｅｗｓｈｏｔｔｅｘｔｃｌａｓｓｉｆ
ｉｃａｔｉｏｎａｎｄ


ｎａｔｕｒａｌｌａｎｇｕａｇｅｉｎｆｅｒｅｎｃｅ［Ｊ
］
．ａｒＸｉｖ
ｐｒｅｐｒｉｎｔａｒＸｉｖ
：２００
１
．０７６７６
，２０２０
．


［４８
］ＷａｎｇＳ
，ＦａｎｇＨ
，ＫｈａｂｓａＭ
，ｅｔａｌ
．Ｅｎｔａｉｌｍｅｎｔａｓｆｅｗ
－ｓｈｏｔｌｅａｍｅｒ
［Ｊ］
．ａｒＸｉｖ
ｐｒｅｐｒ
ｉｎｔ


ａｒＸｉｖ
：２１０４
．１４６９０
，２０２１
．


［４９
］ＬｅｓｔｅｒＢ
，Ａｌ
－ＲｆｏｕＲ
，ＣｏｎｓｔａｎｔＮ
．Ｔｈｅｐｏｗｅｒｏｆｓｃａｌｅｆｏｒｐａｒａｍｅｔｅｒ
－ｅｆｆｉｃｉｅｎｔ


ｐｒｏｍｐ
ｔｔｕｎｉｎｇ［Ｊ
］
．ａｒＸｉｖ
ｐｒｅｐｒ
ｉｎｔａｒＸｉｖ：２１０４
．０８６９
１
，２０２１
．


［５０
］ＢｒｏｗｎＴ
，ＭａｎｎＢ
，ＲｙｄｅｒＮ
，ｅｔａｌ
．Ｌａｎｇｕａｇｅｍｏｄｅｌｓａｒｅｆｅｗ
－ｓｈｏｔｌｅａｍｅｒｓ［Ｊ
］
．


Ａｄｖａｎｃｅｓｉｎｎｅｕｒａｌｉｎｆｏｒｍａｔｉｏｎ
ｐｒｏｃｅｓｓｉｎｇｓｙｓｔｅｍｓ
，２０２０
，３３
：１８７７
－
１９０１
．


［５１
］ＱｕＭ
，ＧａｏＴ
，ＸｈｏｎｎｅｕｘＬＰ
，ｅｔａｌ
．Ｆｅｗ
－ｓｈｏｔｒｅｌａｔｉｏｎｅｘｔｒａｃｔｉｏｎｖｉａｂａｙｅｓｉａｎｍｅｔａ
？


ｌｅａｍｉｎｇｏｎｒｅｌａｔｉｏｎ
ｇｒａｐｈｓ［Ａ］
．／／Ｉｎｔｅｒ
ｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＭａｃｈｉｎｅＬｅａｒｎｉｎｇ


［Ｃ
］
．ＰＭＬＲ
，
２０２０
：７８６７
－７８７６
．


［５２
］ＳａｉｎｚＯ
，ｄｅＬａｃａｌｌｅＯＬ
，ＬａｂａｋａＧ
，ｅｔａｌ
．ＬａｂｅｌＶｅｒｂａｌｉｚａｔｉｏｎａｎｄＥｎｔａｉｌｍｅｎｔｆｏｒ


ＥｆｆｅｃｔｉｖｅＺｅｒｏ
－ａｎｄＦｅｗ
－ＳｈｏｔＲｅｌａｔｉｏｎＥｘｔｒａｃｔｉｏｎ［Ｊ］
．ａｒＸｉｖｐｒｅｐｒ
ｉｎｔ


ａｒＸｉｖ：２１０９
．０３６５９
，２０２１
．


［５３］ＹａｎｇＳ
，ＺｈａｎｇＹ
，ＮｉｕＧ
，ｅｔａｌ
．ＥｎｔｉｔｙＣｏｎｃｅｐ
ｔ
－ｅｎｈａｎｃｅｄＦｅｗ
－ｓｈｏｔＲｅｌａｔｉｏｎ


Ｅｘｔｘａｃｔｉｏｎ
［Ｊ］
，ａｒＸｉｖ
ｐｒｅｐｒｉｎｔａｒＸｉｖ：２１０６．０２４０１
，２０２１
．


６０


参考文献


［５４
］ＢａｏＹ
，ＷｕＭ
，ＣｈａｎｇＳ
５ｅｔａｌ
．Ｆｅｗ
－ｓｈｏｔｔｅｘｔｃｌａｓｓｉｆ
ｉｃａｔｉｏｎｗｉｔｈｄｉｓｔｒ
ｉｂｕｔｉｏｎａｌ


ｓｉｇｎａｔｕｒｅｓ
［Ｊ
］
．ａｒＸｉｖ
ｐｒｅｐｒ
ｉｎｔａｒＸｉｖ
：１９０８
．０６０３９
，２０１９
．


［５５
］ＺｈａｎｇＪ
，ＺｈｕＪ
，ＹａｎｇＹ
，ｅｔａｌ
．Ｋｎｏｗｌｅｄｇｅ
－ＥｎｈａｎｃｅｄＤｏｍａｉｎＡｄａｐ
ｔａｔｉｏｎｉｎＦｅｗ
－


ＳｈｏｔＲｅｌａｔｉｏｎＣｌａｓｓｉｆｉｃａｔｉｏｎ［Ａ
］
．／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２７ｔｈＡＣＭＳＩＧＫＤＤ


ＣｏｎｆｅｒｅｎｃｅｏｎＫｎｏｗｌｅｄｇｅＤｉｓｃｏｖｅｒｙ＆ＤａｔａＭｉｎｉｎｇ［Ｃ
］
．２０２１
：２１８３
－２１９１
．


［闲ＭｉｋｏｌｏｖＴ
，ＣｈｅｎＫ
，Ｃｏｒｒａｄｏ（３
，ｅｔａｌ
．Ｅｆｆ
ｉｃｉｅｎｔｅｓｔｉｍａｔ
ｉｏｎｏｆ
ｗｏｒｄｒｅｐｒｅｓｅｎｔａｔｉｏｎｓ


ｉｎｖｅｃｔｏｒｓｐａｃｅ
［Ｊ］
．ａｒＸｉｖ
ｐｒｅｐｒ
ｉｎｔａｒＸｉｖ：１３０
１
．３７８１
，２０１３
．


［５７
］ＭｉｋｏｌｏｖＴ
，
ＳｕｔｓｋｅｖｅｒＩ
，ＣｈｅｎＫ
，
ｅｔａｌ
．Ｄｉｓｔｒ
ｉｂｕｔｅｄｒｅｐｒｅｓｅｎｔａｔｉｏｎｓｏｆｗｏｒｄｓａｎｄ


ｐｈｒａｓｅｓａｎｄｔｈｅｉｒｃｏｍｐｏｓｉｔｉｏｎａｌｉｔｙ［Ｊ
］
，Ａｄｖａｎｃｅｓｉｎｎｅｕｒａｌｉｎｆｏｒｍａｔｉｏｎ
ｐｒｏｃｅｓｓｉｎｇ


ｓｙｓｔｅｍｓ
，
２０
１３
，２６
．


［５８
］Ｐｅｔｅｒｓ
，ＭａｔｔｈｅｗＥ
．
５ＭａｒｋＮｅｕｍａｎｎ
，ＭｏｈｉｔＩｙｙｅｒ
，ＭａｔｔＧａｒｄｎｅｒ
，Ｃｈｒ
ｉｓｔｏｐｈｅｒＣｌａｒｋ
，


ＫｅｎｔｏｎＬｅｅａｎｄＬｕｋｅＺｅｔｔｌｅｍｏｙｅｒ［Ａ
］
，／／ＤｅｅｐＣｏｎｔｅｘｔｕａｌｉｚｅｄＷｏｒｄ


Ｒｅｐｒｅｓｅｎｔａｔｉｏｎｓ３ＮＡＡＣＬ
［Ｃ
］
．２０１８
．


［５９
］ＨａｏＪ
？ＣｈｅｎＭ
，ＹｕＷ
，ｅｔａｌ
．Ｕｎｉｖｅｒｓａｌｒｅｐｒｅｓｅｎｔａｔｉｏｎｌｅａｒｎｉｎｇｏｆ
ｋｎｏｗｌｅｄｇｅｂａｓｅｓ


ｂｙ
ｊｏｉｎｔｌｙｅｍｂｅｄｄｉｎｇ
ｉｎｓｔａｎｃｅｓａｎｄｏｎｔｏｌｏｇ
ｉｃａｌｃｏｎｃｅｐｔｓ［Ａ
］
．／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆ
ｔｈｅ


２５ｔｈＡＣＭＳＩＧＫＤＤＩｎｔｅｒ
ｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＫｎｏｗｌｅｄｇｅＤｉｓｃｏｖｅｒｙ＆Ｄａｔａ


Ｍｉｎｉｎｇ［Ｃ
］
．２０１９
：１７０９
－１７１９
．


［６０
］ＹａｎｇＢ
，ＹｉｈＷ
，ＨｅＸ
，ｅｔａｌ
．Ｅｍｂｅｄｄｉｎｇｅｎｔｉｔ
ｉｅｓａｎｄｒｅｌａｔｉｏｎｓｆｏｒｌｅａｒｎｉｎｇａｎｄ


ｉｎｆｅｒｅｎｃｅｉｎｋｎｏｗｌｅｄｇｅｂａｓｅｓ
［Ｊ
］
．ａｒＸｉｖ
ｐｒｅｐｒ
ｉｎｔａｒＸｉｖ
：１４１２
．６５７５
，２０
１４
．


［６１
］ＨａｎＸ
？ＺｈｕＨ
，ＹｕＰ
，ｅｔａｌ
．Ｆｅｗｒｅｌ
：Ａｌａｒｇｅ
－ｓｃａｌｅｓｕｐｅｒｖｉｓｅｄｆｅｗ
－ｓｈｏｔｒｅｌａｔｉｏｎ


ｃｌａｓｓｉｆ
ｉｃａｔｉｏｎｄａｔａｓｅｔｗｉｔｈｓｔａｔｅ
－ｏｆ
－ｔｈｅ
－ａｒｔｅｖａｌｕａｔｉｏｎ［Ｊ
］
．ａｒＸｉｖｐｒｅｐｒ
ｉｎｔ


ａｒＸｉｖ
：
１８１０
．
１０
１４７
，２０１８
．


［６２
］ＧａｏＴ
，ＨａｎＸ
，ＺｈｕＨ
，ｅｔａＬＦｅｗＲｅｌ２
．０
：Ｔｏｗａｒｄｓｍｏｒｅｃｈａｌｌｅｎｇ
ｉｎｇｆｅｗ
－ｓｈｏｔ


ｒｅｌａｔｉｏｎｃｌａｓｓｉｆｉｃａｔｉｏｎ
［Ｊ
］
．ａｒＸｉｖ
ｐｒｅｐｒ
ｉｎｔａｒＸｉｖ
：１９
１０
．０７
１２４
，２０
１９
．


６
１


