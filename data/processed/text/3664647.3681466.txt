DiffHarmony++: Enhancing Image Harmonization with
Harmony-VAE and Inverse Harmonization Model
Pengfei Zhou
zhoupengfei@bupt.edu.cn
Beijing University of Posts &
Telecommunications
China, Beijing
Fangxiang Fengâˆ—
f.fangxiang@gmail.com
Beijing University of Posts &
Telecommunications
China, Beijing
Guang Liu
marscrazy_90@163.com
Beijing Academy of Artificial
Intelligence
China, Beijing
Ruifan Li
rfli@bupt.edu.cn
Beijing University of Posts &
Telecommunications
China, Beijing
Xiaojie Wang
xjwang@bupt.edu.cn
Beijing University of Posts &
Telecommunications
China, Beijing
Abstract
Latent diffusion model has demonstrated impressive efficacy in im-
age generation and editing tasks. Recently, it has also promoted the
advancement of image harmonization. However, methods involv-
ing latent diffusion model all face a common challenge: the severe
image distortion introduced by the VAE component, while image
harmonization is a low-level image processing task that relies on
pixel-level evaluation metrics. In this paper, we propose Harmony-
VAE, leveraging the input of the harmonization task itself to en-
hance the quality of decoded images. The input involving composite
image contains the precise pixel level information, which can com-
plement the correct foreground appearance and color information
contained in denoised latents. Meanwhile, the inherent generative
nature of diffusion models makes it naturally adapt to inverse image
harmonization, i.e. generating synthetic composite images based on
real images and foreground masks. We train an inverse harmoniza-
tion diffusion model to perform data augmentation on two subsets
of iHarmony4 and construct a new human harmonization dataset
with prominent foreground objects. Extensive experiments demon-
strate the effectiveness of our proposed Harmony-VAE and inverse
harmonization model. Code and pretrained models are available at
https://github.com/nicecv/DiffHarmony .
CCS Concepts
â€¢ Computing methodologies â†’Computer vision tasks.
Keywords
image harmonization, latent diffusion model, VAE, data augmenta-
tion, inverse harmonization, stable diffusion
âˆ—Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
MM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia
Â© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0686-8/24/10
https://doi.org/10.1145/3664647.3681466
ACM Reference Format:
Pengfei Zhou, Fangxiang Feng, Guang Liu, Ruifan Li, and Xiaojie Wang.
2024. DiffHarmony++: Enhancing Image Harmonization with Harmony-
VAE and Inverse Harmonization Model. In Proceedings of the 32nd ACM
International Conference on Multimedia (MM â€™24), October 28-November
1, 2024, Melbourne, VIC, Australia. ACM, New York, NY, USA, 10 pages.
https://doi.org/10.1145/3664647.3681466
1
Introduction
Image composition is a crucial technique in digital editing, where a
new image is created by combining the foreground of one image
with the background of another. This process often encounters a
challenge: the merged image may appear unrealistic due to mis-
matches in lighting and color between the foreground and back-
ground. To solve this problem, image harmonization is proposed.
It involves modifying the foreground to ensure visual consistency
with the background, focusing on aligning color, illumination and
texture without changing the original content. Recent advance-
ments in deep learning [43, 11, 17, 40] have significantly facilitated
image harmonization.
The latent diffusion model (LDM) [36] is gaining increasing atten-
tion and has established new SoTA in the field of image generation.
It can swiftly transfer to downstream image-to-image translation
tasks such as image editing [1, 23] and restoration [32, 46] . As image
harmonization can be categorized as an image-to-image translation
task, itâ€™s also suitable for applying latent diffusion model. For exam-
ple, Li et al. [26] follows the network architecture in ControlNet [50]
and adapt pretrained latent diffusion model, i.e. Stable Diffusion, to
perform image harmonization, but they ignore the image distortion
problem inherent in VAE decoding process, resulting in unsatisfac-
tory performance. Zhou et al. [51] adopts a two-stage approach.
The first stage finetunes the denoising UNet, while the second stage
involves designing a refinement module to alleviate the image dis-
tortion issue. It finally achieves SoTA performance. However, this
approach exhibits several limitations: 1) The second-stage training
relies on the results of the first stage, necessitating retraining of
the second stage upon updates in the first stage. 2) Introducing an
independent UNet model complicates the harmonization process.
3) Itâ€™s limited to usage at fixed resolutions.
10592
MM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia
Pengfei Zhou, Fangxiang Feng, Guang Liu, Ruifan Li, Xiaojie Wang
To solve aforementioned limitations, we propose Harmony-VAE,
which incorporates composite images as additional information
into the decoding process of the VAE, enabling the VAE decoder to
reconstruct images with realistic details. Our motivation stems from
the highly precise object shape information present in composite
images, which can complement the correct foreground appearance
and color information contained in denoised latents. The training
of Harmony-VAE is independent of fine-tuning denoising UNet;
it reconstructs real images with composite images as condition,
significantly reducing training costs, and it can improve the per-
formance of all LDM-based harmonization models. Furthermore,
although trained only on 256px images, Harmony-VAE significantly
enhances the decoding quality of higher resolution images, demon-
strating strong generalization capabilities. We call DiffHarmony
equipped with Harmony-VAE as DiffHarmony++.
The latent diffusion model enhanced by our proposed Harmony-
VAE, with simple modifications, can be adapted to construct a model
that performs inverse image harmonization, serving the purpose
of data augmentation. Inverse image harmonization refers to the
generation of synthetic composite images from real images and
foreground masks. The outcomes should not be unique, as the con-
tent of the images may be influenced by various factors such as
weather and lighting conditions. This process can be modeled as
a one-to-many image-to-image translation task, and employing
a diffusion model to handle it is a very natural choice. Based on
DiffHarmony++, we train an inverse harmonization diffusion model
on iHarmony4[10]. This model enables virtually unlimited expan-
sion of harmonization data. Leveraging the inverse harmonization
model, we perform data augmentation on two smaller subsets of
iHarmony4, namely Hday2night and HFlickr. Compared to training
solely on the original data, training with augmented data greatly
enhances performance on these two datasets.
Another significant advantage of our proposed inverse harmo-
nization model is to automatically construct new image harmo-
nization datasets, which has long been a challenge in the field of
image harmonization. The creation of datasets like RealHM[20]
often needs extensive manual effort and expertise in digital im-
age processing, proving to be time-consuming and labor-intensive.
iHarmony4 employs automatic color transfer algorithms to gener-
ate synthetic composite images, but still need manual screening to
filter out unreasonable data. Leveraging the inverse harmonization
model, we generate a substantial number of synthetic composite
images based on the imaterialist[12] dataset and train a harmony
classifier to identify most unharmonized images, culminating in
the creation of the Human Harmony dataset. Training harmoniza-
tion models on this newly constructed dataset, both qualitative
and quantitative evaluations affirm that the proposed inverse har-
monization diffusion model is a highly promising approach for
building new datasets.
Our contributions can be summarized as follows:
â€¢ We propose Harmony-VAE to tackle the image distortion
problem inherent in VAE decoding process, which is the most
challenging aspect of applying LDM to image harmonization
tasks.
â€¢ We design a simple yet effective inverse harmonization dif-
fusion model, and validate its efficacy on two image harmo-
nization datasets.
â€¢ We contribute a new Human Harmony dataset with inverse
harmonization model and use harmony classifier to further
filter out high quality data.
2
Related Works
2.1
Image Harmonization
Image harmonization is a critical task in image composition [33],
aimed at achieving visual consistency between the foreground and
background of composite images. Early efforts were primarily cen-
tered around traditional color matching algorithms [48, 41, 42, 49,
7]. These methods focused on aligning the low-level color statistics
between the foreground and background, utilizing techniques such
as shifting, scaling, and histogram matching. With the advent of
deep learning, a surge of supervised deep harmonization methods
came out [43, 11, 17, 40, 35, 44, 47, 53, 20, 2, 3, 4]. Tsai et al. [43]
propose an end-to-end deep convolutional neural network which
captures both the context and semantic information of the compos-
ite images during harmonization. Cun et al. [11] learn the feature
map in the masked region and the others individually with a novel
attention module. The application of domain translation or style
transfer techniques to image harmonization [8, 16, 29, 10] offers
a creative means of reconciling discrepancies between the fore-
ground and background by conceptualizing different illumination
conditions as distinct domains or styles. Cong et al. [10] translate
foreground to the same domain as background with a domain veri-
fication discriminator. Cong et al. [8] use a domain code extractor
to capture the background domain information to guide the fore-
ground harmonization, which is regulated by well-tailored triplet
losses. In the mean time, the integration of Retinex theory [15, 14,
13] into image harmonization has opened new pathways by decom-
posing images into reflectance and illumination maps, and also the
development of deep networks predicting color transformation [9,
22, 28, 48] signifies a balance between efficiency and effectiveness,
streamlining the harmonization process without sacrificing quality.
2.2
Image Harmonization Dataset
Jiang et al. [20] construct RealHM dataset with 216 image pairs by
manually adjusting the foreground according to the background.
Another line of work is collecting a set of foreground images cap-
tured in different illumination conditions, followed by replacing
one foreground with another counterpart, for example Transient
Attributes Database [25]. Cao et al. [3] and Guo et al. [15] try to
construct harmonization dataset by varying the lighting condition
of the same scene using 3D rendering techniques, however the ren-
dered images have large domain gap with real images. Some other
works [43, 11, 10] adopted an inverse approach, i.e., adjusting the
foreground of real image to create synthetic composite image. The
representative, iHarmony4 dataset utilize automatic color transfer
for foreground adjustment followed by aesthetic predictor and bi-
nary CNN classifier for filtering (also at last need manual screening).
Niu et al. [34] learns a VAE conditioned on ground-truth real image
and foreground mask to predict color transformation LUT to adjust
10593
DiffHarmony++: Enhancing Image Harmonization with Harmony-VAE and Inverse Harmonization Model
MM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia
foreground part in real image, producing new synthetic composite
images automatically.
2.3
Diffusion Model
Diffusion models are adept at generating realistic images from
random noise. Ho et al. [19] introduced Denoising Diffusion Proba-
bilistic Models (DDPMs) employing a Markovian diffusion process,
generating images from pure noise. Afterwards, diffusion models
have been explored in various image-to-image translation tasks by
researchers, such as Palette [39], RePaint [31] and SR3+ [38]. Lately
latent diffusion models have garnered unprecedented attention.
LDM [36] is proposed to serve as the theoretical foundation for
Stable Diffusion. Numerous works based on Stable Diffusion tar-
geted on solving image-to-image translation tasks, and some have
applied latent diffusion model framework to image harmonization
task (which can be categorized as image-to-image translation). For
instance, Appearance Consistency Discriminator [26] is proposed to
guide LDM and convert the generated image from RGB to HSV/HSL
space to adjust the lightness channel. Chen et al. [6] achieved Zero-
Shot Image Harmonization by incorporating Attention-Constraint
Text and Content Retention modules into Stable Diffusion. Lu et
al. [30] focused on using pretrained LDM to blend photographic
objects into paintings, achieving artistically coherent composite
images. Zhou et al. [51] proposed DiffHarmony, which adapted
the Stable Diffusion inpainting variation for image harmonization
and propose a refinement stage to address image distortion issues.
There are also other works attempting at solving image distortion
issues in Stable Diffusion [45, 52]. But itâ€™s noteworthy that they
focus on image inpainting task, while our method is designed for
image harmonization including repairing content of foreground
area and at mean time maintain the harmonized appearance, which
is more challenging.
3
Harmony-VAE
3.1
Preliminary
The Stable Diffusion (SD) model stands as a quintessential repre-
sentation of latent diffusion model and serves as the cornerstone
for our approach. It undergoes pre-training in two distinct stages
employing a VAE (Variational AutoEncoder) and a denoising U-Net.
In the first stage, it trains the VAE, where the encoder E first en-
codes the image I into a latent space, resulting the latent variable
ğ‘§0 = E(I); subsequently the decoder D endeavors to reconstruct it
into the original image, yielding the reconstructed image Ë†I. In the
second stage, the VAE is frozen and the goal is training a denoising
U-Net ğœ–ğœƒ, which involves adding noise over ğ‘¡steps to the latent
variable ğ‘§0 to get noisy latent ğ‘§ğ‘¡(ğ‘¡âˆˆ[1,ğ‘‡]), and updating the
denoising U-Net with latent denoising loss, as formulated below:
L = Eğ‘§0,ğ‘,ğœ–âˆ¼N(0,1),ğ‘¡

âˆ¥ğœ–âˆ’ğœ–ğœƒ(ğ‘§ğ‘¡,ğ‘¡,ğ‘)âˆ¥2
2

(1)
Here, ğœ–denotes the noise added to the latent variable ğ‘§0 at each
noise step, ğœ–ğœƒrepresents the denoising U-Net, receiving timestep ğ‘¡
and additional conditions ğ‘(e.g., text, conditional images, masks,
etc.) as input.
During the inference process, pure noise ğ‘§ğ‘‡is sampled from
a normal distribution. The model will iteratively employ ğœ–ğœƒto
estimate the noise for each denoising step ğ‘¡, progressively refining
the latent variable ğ‘§ğ‘‡to ultimately attain denoised latent variable
ğ‘§â€²
0. Finally, the denoised latent variable ğ‘§â€²
0 is fed into the decoder
D to generate the image. For intricate details on the training and
inference of Stable Diffusion, please refer to [36].
In image harmonization task, we define variables as follows: the
input to image harmonization task comprises the composite image
ğ¼and the foreground mask image ğ‘€(where the foreground is rep-
resented as 1 in white, and the background as 0 in black), while
the output is the harmonized image Ëœğ¼. Ground-truth real image
utilized in model training and evaluation is denoted as Ë†ğ¼. The com-
posite image ğ¼can be divided into the foreground component ğ¼ğ‘“
and the background component ğ¼ğ‘. The objective of image harmo-
nization task is to adjust the composite foreground ğ¼ğ‘“to produce
the harmonized image Ëœğ¼, which should closely resemble Ë†ğ¼.
When employing the latent diffusion model for image harmoniza-
tion, a viable approach could be: during training, setting ğ‘§0 = E(Ë†ğ¼),
ğ‘= (E(ğ¼), down(ğ‘€)) (where down denotes the downsampling op-
eration), in detailed words utilizing the encoded composite image
and downsampled mask image as conditions (typically concate-
nated along the channel dimension) for denoising the noisy latent
ğ‘§ğ‘¡. During inference, initialize from random noise ğ‘§ğ‘‡, progressively
denoise it to obtain ğ‘§â€²
0 and decode it through D to yield Ëœğ¼. This ap-
proach has already been proven effective in DiffHarmony[51], and
in our subsequent descriptions, we assume that the latent diffusion
model part adopts the same architecture as it.
3.2
Our Method
When employing latent diffusion models for image harmoniza-
tion task, a common obstacle to model performance is the image
distortion problem caused by VAE[24] decoding process. Image har-
monization falls within the domain of low-level image processing,
demanding pixel-level accuracy of output images. However, VAE in
Stable Diffusion, which decodes a single latent variable to an image,
often results in distortion in subtle high-frequency textures or lead
to the fabrication of content not present in the original image (as
VAE fundamentally operates as a generative model).
To address the aforementioned issue, we propose the Harmony-
VAE, aiming at enhancing the quality of the output harmonized
image Ëœğ¼during the decoding stage of VAE by utilizing the input of
the harmonization task, namely the composite image ğ¼and fore-
ground mask image ğ‘€.
Specifically, we incorporate an additional encoder, the condi-
tional encoder Eğ‘, to encode ğ¼and ğ‘€. During the encoding process,
we obtain the final latent variables and intermediate features of the
encoder. These features are fused into VAE decoder through skip
connections (similar to UNet[37]) and element-wise addition. The
training objective of the Harmony-VAE is to reconstruct the real im-
age Ë†ğ¼with ğ¼and ğ‘€as condition. This process can be mathematically
formalized as:
L =
Ë†ğ¼âˆ’D

E(Ë†ğ¼), Eğ‘(ğ¼, ğ‘€)

2
2
(2)
Strictly speaking, during the training of the Harmony-VAE, E(Ë†ğ¼)
part should actually be the latent variable ğ‘§â€²
0. However, when the
denoising module performs sufficiently well, the distributions of
10594
MM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia
Pengfei Zhou, Fangxiang Feng, Guang Liu, Ruifan Li, Xiaojie Wang
Figure 1: Demonstration of Harmony-VAE training and inference of DiffHarmony++. (a) The training goal of Harmony-VAE is
to reconstructing Ë†ğ¼with ğ¼and ğ‘€as condition. The extracted features will be fed into VAE decoder through skip connection in
way of element-wise addition. We add zero-initialized convolution layer before each skip connection to stablize training. (b) At
denoising stage, the process starts with pure Gaussian noise ğ‘§ğ‘‡, iteratively refines it through Denoising U-Net, and finally get
denoised latent variable ğ‘§â€²
0, during which the encoded composite image E(ğ¼) and downsampled foreground mask image are
concatenated in channel dimension as condition. Then the latent variable ğ‘§â€²
0 will be decoded into harmonized image Ëœğ¼.
the latent variables corresponding to the real image Ë†ğ¼and the har-
monized image Ëœğ¼are close enough, thereby using the encoded real
image E(Ë†ğ¼) as the model input for training does not incur signifi-
cant performance loss. Moreover, this choice brings another benefit:
the training of the Harmony-VAE model does not depend on the
denoising part, greatly reducing the training cost, as there is no
need to generate data samples from the diffusion model offline or
online.
During training of the Harmony-VAE, the weights of the condi-
tional encoder Eğ‘are initialized from the weights of the original
VAE encoder. To maintain training stability, we add zero-initialized
convolution layers before each skip connection, and utilize the
zero-initialization strategy for the portion of convolution weights
processing the mask image ğ‘€at the input convolution conv_in.
During training, we set the weights of the conditional encoder Eğ‘,
decoder D, and all zero-initialized convolution layers trainable.
After training, it can be integrated seamlessly into DiffHarmony
inference process. We call DiffHarmony equipped with Harmony-
VAE as DiffHarmony++. For a comprehensive schematic illustration
of Harmony-VAE training and DiffHarmony++ inference, please
refer to Figure 1. We highlight the newly added model weights in
red dashed lines.
4
Inverse Image Harmonization
4.1
Inverse Harmonization Model
The acquisition of training data for image harmonization task has
always been a costly endeavor. Traditional procedures for con-
structing harmonization datasets often involve three main steps:
selecting challenging compositing pairs, creating precise masks for
foreground objects with distinct boundaries, and adjusting fore-
ground appearance to match the background using tools like Pho-
toshop [20]. These steps require a considerable amount of manual
labor and expertise in image processing, making them challeng-
ing and time-consuming. The construction of the iHarmony4 [10]
dataset employs a highly automated process, such as color trans-
fer modules and classifiers to identify unreasonable data, but still
requires manual screening.
Diffusion models are initially applied to image generation and
conditional image generation. Considering harmonization data gen-
eration (as in iHarmony4), given the foreground mask image ğ‘€
and the real image Ë†ğ¼, the output is the composite image ğ¼. The re-
sulting composite image ğ¼is not unique because many variations
can make the foreground appear incongruous with the background.
This process can be modeled as a typical one-to-many conditional
image-to-image translation task, hence employing diffusion models
to generate harmonization data is a natural choice.
Our approach is based on DiffHarmony++ to train an inverse
harmonization model. The model takes the foreground mask image
ğ‘€and the real image Ë†ğ¼as input and outputs the composite image ğ¼.
We train it using the existing harmonization dataset iHarmony4.
We can use the inverse harmonization model to generate addi-
tional training data for existing harmonization datasets. Specifically,
we generate ğ¾candidate composite images for each (Ë†ğ¼, ğ‘€) data pair
in the dataset and specify the blending ratio as ğ›¾. Assuming the
number of (Ë†ğ¼, ğ‘€) data pairs in the original dataset is ğ‘1, and the to-
tal number of original composite images is ğ‘2, we randomly select
lğ›¾ğ‘2
ğ‘1
m
newly generated composite images for each (Ë†ğ¼, ğ‘€) data pair
and add them to the training data. After blending, we train new
harmonization models separately using the original dataset and the
augmented dataset, comparing the results of the two experiments
on oiriginal test set to observe the benefits of data augmentation
(Experimental results show that our data augmentation strategy
significantly improves model performance compared to training
solely on the original data. Details can be found in the 5.3 section).
4.2
Construct Human Harmony Dataset
The field of image harmonization has long lacked datasets specifi-
cally tailored to human portraits domains. These datasets have the
10595
DiffHarmony++: Enhancing Image Harmonization with Harmony-VAE and Inverse Harmonization Model
MM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia
Dataset
Metric
Composite
DoveNet[10]
BargainNet[8]
RainNet[29]
iS2AM[40]
D-HT[14]
SCS-Co[16]
HDNet[5]
Li ğ‘’ğ‘¡ğ‘ğ‘™. [27]
GIFT[34]
DiffHarmony[51]
Ours
HCOCO
PSNRâ†‘
35.47
35.83
37.03
37.08
39.16
38.76
39.88
41.04
34.33
39.91
41.71
42.42
MSEâ†“
41.07
36.72
24.84
29.52
16.48
16.89
13.58
11.60
59.55
12.70
9.18
8.43
fMSEâ†“
542.06
551.01
397.85
501.17
266.19
299.30
245.54
-
-
229.68
170.44
155.73
HAdobe5k
PSNRâ†‘
33.77
34.34
35.34
36.22
38.08
36.88
38.29
41.17
33.18
38.76
41.08
41.78
MSEâ†“
63.40
52.32
39.94
43.35
21.88
38.53
21.01
13.58
161.36
18.35
19.51
18.81
fMSEâ†“
404.62
380.39
279.66
317.55
173.96
265.11
165.48
-
-
143.96
120.78
113.18
HFlickr
PSNRâ†‘
30.03
30.21
31.34
31.64
33.56
33.13
34.22
35.81
29.21
34.44
37.10
37.74
MSEâ†“
143.45
133.14
97.32
110.59
69.97
74.51
55.83
47.39
224.05
54.33
30.89
28.77
fMSEâ†“
785.65
827.03
698.40
688.40
443.65
515.45
393.72
-
-
360.08
216.27
201.52
Hday2night
PSNRâ†‘
34.50
35.27
35.67
34.83
37.72
37.10
37.83
38.85
34.08
38.28
39.45
39.49
MSEâ†“
76.61
51.95
50.98
57.40
40.59
53.01
41.75
31.97
122.41
37.81
22.42
22.48
fMSEâ†“
989.07
1075.71
835.63
916.48
590.97
704.42
606.80
-
-
566.47
470.846
464.35
Average
PSNRâ†‘
34.35
34.76
35.88
36.12
38.19
37.55
38.75
40.46
32.70
38.92
40.97
41.66
MSEâ†“
59.67
52.33
37.82
40.29
24.44
30.30
21.33
16.55
141.84
19.46
14.86
13.98
fMSEâ†“
594.67
532.62
405.23
469.60
264.96
320.78
248.86
-
-
225.30
166.48
153.98
Table 1: Quantitative comparison across four sub-datasets of iHarmony4 and in general. Top two performance are shown in red
and blue. â†‘means the higher the better, and â†“means the lower the better.
potential to optimize product showcases and enhance user experi-
ences, offering significant value and innovation opportunities for
e-commerce, retail, and fashion design. However, for the reasons
mentioned earlier, constructing such datasets has been challenging.
Now, after verifying the effectiveness of our inverse harmonization
model, we have the opportunity to construct a Human Harmony
dataset in a cheap and fast way.
Furthermore, the analysis in DiffHarmony [51] suggests that
latent diffusion models may perform better on samples with large
foregrounds. Portrait photography data typically features large
foregrounds objects. Building a Human Harmony dataset will help
us further validate this hypothesis.
We select imaterialist-fashion-2020-fgvc7 [12] as our initializa-
tion. It comprises a vast number of high-resolution portrait pho-
tographs, and the majority features large foregrounds objects (hu-
man body parts). Each image is paired with highly-detailed segmen-
tation map, which can be used to construct accurate foreground
mask.
To ensure the quality of the generated harmonization data, we
train a harmony classifier using the iHarmony4 dataset. Given an
input image, the classifier outputs the probability of it belonging to
composite images.
Following Section 4.1, we generateğ¾candidate composite images
for each (Ë†ğ¼, ğ‘€) data pair. To ensure the quality of the generated
images, we separately train a Harmony-VAE suitable for inverse
harmonization model using the iHarmony4 dataset. Subsequently,
we use the harmony classifier to classify the candidate set, selecting
the image with the highest classification probability as the final
composite sample.
5
Experiment
5.1
Datasets and Metrics
5.1.1
iHarmony4 Dataset. We conduct experiments on the bench-
mark iHarmony4 [10] dataset, which consists of four sub-datasets:
HCOCO, HFlickr, HAdobe5K, and Hday2night, with a total of 65,742
(7,404 for testing) pairs of composite and real images in the train-
ing (testing) sets. Following previous work [10, 40], we merge the
training set of the four sub-datasets into one entire training set and
evaluate each sub-dataset individually.
5.1.2
Human Harmony Dataset. For the Human Harmony dataset,
we filter the imaterial-fashion-2020-fgvc7 [12] dataset to remove
images containing only products. After cleaning the dataset size is
29,106. We set all valid segmentation parts (except background) to
1 to construct foreground mask images. When building the Human
Harmony dataset, we set ğ¾= 10 and use the harmony classifier
to select the image with the highest classification probability. We
divide the Human Harmony dataset according to the same train-
ing/testing set ratio as the iHarmony4 dataset, resulting in a training
set of size 26,157 and a testing set of size 2,946.
Figure 2: Cumulative distribution curves about foreground
area ratios. We analyze and plot both iHarmony4 and Human
Harmony dataset.
We analyze the distribution of foreground ratios and plot cumu-
lative distribution curves. We also simultaneously plot the curve of
iHarmony4. From Figure 2, it can be observed that approximately
70% of the images in the Human Harmony dataset have foreground
ratios above 0.2, whereas in iHarmony4 this is less than 15%.
5.1.3
Evaluation Metrics. In line with previous works all methods
are evaluated using PSNR, MSE and fMSE.
5.2
Implementation Details
During the training of Harmony-VAE, we use all iHarmony4 train-
ing data. We set lr = 1ğ‘’âˆ’4, with warmup = 0.02, and then keep
it constant. The training lasts 10 epochs. We use the AdamW op-
timizer, with weight_decay=0, ğ›½1 = 0.9, and ğ›½2 = 0.999. We save
model weights using exponential moving average (EMA), with
10596
MM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia
Pengfei Zhou, Fangxiang Feng, Guang Liu, Ruifan Li, Xiaojie Wang
Figure 3: Qualitative results of Harmony-VAE.
max_decay = 0.999. Images are resized to 256px during train-
ing. As for the diffusion model, we adopt the pre-trained DiffHar-
mony [51] directly. Following [51], we use Euler ancestral discrete
scheduler [21] to generate the samples in only 5 steps during infer-
ence.
The implementation details of inverse harmonization model are
mostly consistent with DiffHarmony++, except that we use (Ë†ğ¼, ğ‘€)
as conditions and composite image ğ¼as the denoising target during
training. The training and inference of corresponding Harmony-
VAE is also modified accordingly.
For the harmony classifier, we utilize pre-trained ResNet50 [18]
model, taking the final 2048-dimensional features after pooling
for linear probing. We train a binary classification head using all
iHarmony4 training data.
5.3
Performance Comparison
5.3.1
Results on iHarmony4. On the iHarmony4 dataset, we com-
pare our approach with the following image harmonization meth-
ods: DoveNet[10] , BargainNet[8] , RainNet[29] , iS2AM[40] , D-
HT[14] , SCS-Co[16] , HDNet[5] , Li ğ‘’ğ‘¡ğ‘ğ‘™.[27] , GIFT[34], DiffHar-
mony[51], etc. In Table 1, we report the average results for the
four sub-test sets and the entire test set, which are either replicated
from the original papers or reproduced using publicly available
models. For the overall results on the entire test set, our method
significantly outperforms previous SOTA methods. Our method
achieves the best results on almost all sub-sets , except for the MSE
on the HAdobe5k and Hday2night subsets.
Our proposed model outperforms DiffHarmony because we uti-
lize Harmony-VAE instead of the VAE from Stable Diffusion. In the
process of image decoding, using only the regular VAE from Stable
Diffusion can result in severe distortion. In contrast, our proposed
Harmony-VAE can preserve more details. Our qualitative results
are provided in Figure 3. It can be observed that Harmony-VAE
successfully repairs severely damaged facial features, architectural
patterns, small text, and other contents during the decoding process.
base
augmented
Hday2night
PSNR: 39.06
PSNR: 42.26
MSE: 25.70
MSE: 17.49
fMSE: 439.11
fMSE: 223.67
HFlickr
PSNR: 34.52
PSNR: 36.57
MSE: 61.15
MSE: 41.18
fMSE: 413.08
fMSE: 252.86
Table 2: Qualitative results of data augmentation experi-
ments. Data generated by inverse harmonization model can
greatly boost model performace on small datasets.
5.3.2
Effectiveness of Data Augmentation. We conduct data aug-
mentation experiments with inverse harmonization model on two
subsets of iHarmony4, Hday2night and HFlickr, as they have rel-
atively fewer training data, making them more likely to benefit
from data augmentation. HFlickr has 4,836 real images and 8,280
composite images, while Hday2night has only 109 real images and
447 composite images.
Our experimental results are shown in Table 2. "base" repre-
sents the results obtained by training only on the original dataset,
while "augmented" represents the results obtained by training on
the augmented dataset. For both datasets, we set the blending ra-
tio ğ›¾= 1. All experiments use the same hyperparameter setting
10597
DiffHarmony++: Enhancing Image Harmonization with Harmony-VAE and Inverse Harmonization Model
MM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia
Dataset
Model
0% âˆ¼5%
5% âˆ¼15%
15% âˆ¼100%
iHarmony4
ğ‘ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘ 
3835
1951
1618
HDNet512
PSNR: 45.64
PSNR: 39.97
PSNR: 34.59
MSE: 3.16
MSE: 11.33
MSE: 47.19
fMSE: 143.93
fMSE: 129.87
fMSE: 152.01
DiffHarmony++
PSNR: 45.20
PSNR: 40.17
PSNR: 34.97
MSE: 3.67
MSE: 11.31
MSE: 41.94
fMSE: 173.40
fMSE: 128.27
fMSE: 136.67
Human
Harmony
ğ‘ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘ 
26
323
2597
HDNet512
PSNR: 41.05
PSNR: 37.16
PSNR: 32.60
MSE: 7.66
MSE: 18.48
MSE: 65.71
fMSE: 210.96
fMSE: 160.73
fMSE: 182.04
DiffHarmony++
PSNR: 42.48
PSNR: 39.52
PSNR: 34.31
MSE: 5.60
MSE: 11.45
MSE: 60.17
fMSE: 176.26
fMSE: 101.86
fMSE: 154.02
Table 3: Comparison between HDNet trained with high-resolution images and DiffHarmony++ on both iHarmony4 and Human
Harmony dataset. Number of samples of every subset with different foreground proportions are denoted as ğ‘ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘ .
and train with the same total amount of data. We train enough
steps to ensure convergence and then evaluate on the original test
set. We perform inference only on 512px images and then scale
the results to 256px for evaluation, as achieving optimal inference
performance is not necessary for observing the effectiveness of
data augmentation. The results show that data augmentation with
inverse harmonization model significantly improves the model per-
formance on both datasets, thus validating its ability to generate
high-quality composite images.
5.3.3
Advanced Analysis. In Table 3, we present a detailed compar-
ison of the performance of DiffHarmony++ and HDNet on both
iHarmony4 and Human Harmony dataset. To ensure fairness, we
stipulate that models directly utilize composite images as input
during both training and inference. (Note: HDNet in its original im-
plementation use real image background as input, resulting signifi-
cantly improvement but unfair comparisons with other approaches.)
Following HDNet[5], we divide data into three ranges based on the
ratio of the foreground region area and the entire image: 0% âˆ¼5%,
5% âˆ¼15%, and 15% âˆ¼100%. We calculate metrics for each range
respectively. We train HDNet with 512px images, denoting it as
HDNet512. During testing, we utilize 1024px images as input and
subsequently resize harmonized images to 256px for evaluation,
maintaining consistent experimental settings with our approach.
Our results on the iHarmony4 dataset indicate that when test
samples have small foreground proportions (0% âˆ¼5%), HDNet512
outperforms DiffHarmony++. However, as the foreground propor-
tion increases, DiffHarmony++ demonstrates increasingly superior
performance. On the Human Harmony dataset, particularly on
larger foreground proportions (15% âˆ¼100%), DiffHarmony++ con-
sistently outperforms HDNet512 . Regarding its performance on
samples with smaller foreground proportions, we speculate that
the limited number of data samples leads to a significant variance
in statistical metrics. Due to constraints in time and computational
resources, further investigation into the underlying reasons for this
phenomenon is deferred for future research.
Qualitative results are shown in Figure 4. Our approach often
generates visually appealing outcomes that closely resemble the
authentic real images.
5.4
Ablation Study
We conduct ablation study to validate the effectiveness of the com-
ponents in our proposed Harmony-VAE.
ğ‘…ğ‘–ğ‘›ğ‘“
Harmony-VAE
PSNR
MSE
fMSE
512px
âœ˜
38.12
25.09
292.16
512px
âœ”
40.11
19.81
215.01
1024px
âœ˜
40.98
14.86
166.48
1024px
âœ”
41.66
13.98
153.98
1024px
âœ˜
41.72
13.35
151.65
1024px
âœ”
41.75
13.23
150.92
Table 4: Ablation study of using Harmony-VAE at multiple
inference resolutions. For easier comparison we add addi-
tional results of DiffHarmony plus refinement module at
bottom and mark the corresponding lines in gray color.
We do inference with and without Harmony-VAE at multiple
resolutions. As shown in Table 4, adding the Harmony-VAE results
in an improvement in the overall performance. The benefit of intro-
ducing Harmony-VAE is more prominent when DiffHarmony uses
lower image resolutions, as the Harmony-VAE and using higher
resolution input both aim to address the issue of image distortion,
and they complement each other. We also add additional results
of DiffHarmony plus refinement module [51] at bottom and mark
the corresponding lines in gray color. Comparing to using only
Harmony-VAE, using only the refinement module and the cascaded
use of Harmony-VAE and refinement module only bring negligible
improvement. This suggests that employing Harmony-VAE alone
to enhance DiffHarmony could achieves optimal performance, and
furthermore Harmony-VAE can be trained in a more elegant and
cost-effective manner. It is noteworthy that even training only
10598
MM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia
Pengfei Zhou, Fangxiang Feng, Guang Liu, Ruifan Li, Xiaojie Wang
Figure 4: Qualitative results on Human Harmony Dataset.
on 256px images, the Harmony-VAE still brings significant im-
provement when inferring with 1024px images, demonstrating the
generalization of our approach.
zero init
random init
Eğ‘
PSNR: 39.86
NaN
MSE: 20.45
fMSE: 221.05
Eğ‘+ D
PSNR: 40.11
PSNR: 31.49
MSE: 19.81
MSE: 114.81
fMSE: 215.01
fMSE: 1231.56
Table 5: Ablation study of our proposed zero init strategy and
finetuning different parameters.
From Table 5, it can be observed that not using zero-initialized
convolution severely damages the normal training of the model.
The random initialization of convolution layers introduces nonneg-
ligible perturbations to the original feature distribution when fused
with the VAE decoder. From the fine-tuning parameter ablation
experiments, it can be seen that fine-tuning only the conditional
encoder already provides a considerable improvement in recon-
struction performance, while unfreezing the decoder can further
increase the benefits.
6
Conclusion
In this paper, we have proposed the Harmony-VAE, aimed at lever-
aging the conditional information in image harmonization tasks to
enhance the quality of image decoding of the VAE component in
the latent diffusion model. Our proposed Harmony-VAE preserves
finer details, effectively restoring severely damaged facial features,
architectural patterns, small text during the decoding process. Fur-
thermore, we have trained an inverse harmonization model which
can synthesize new composite images based on real images and
foreground masks. The substantial improvements observed on the
Hday2night and HFlickr datasets attest to the efficacy of our model.
Building upon this, we have subsequently constructed the Human
Harmony Dataset, comprising samples with prominent foreground
areas. Experimental results demonstrate the effectiveness of our
inverse harmonization model of superiority of the LDM-based har-
monization approach on samples featuring prominent foreground
objects.
10599
DiffHarmony++: Enhancing Image Harmonization with Harmony-VAE and Inverse Harmonization Model
MM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia
Acknowledgments
This work was supported by the NSFC (No.61906018, No.62076032),
the industry-university cooperation collaborative education project
of the Ministry of Education of China (No. 230800001284513), the
Science and Technology Project of State Grid Corporation of China
(No. 5700-202058480A-0-0-00), and the Super Computing Platform
of Beijing University of Posts and Telecommunications. We are
particularly grateful for the comments of the area chair.
References
[1]
Omri Avrahami, Ohad Fried, and Dani Lischinski. 2023. Blended latent diffusion.
ACM Transactions on Graphics (TOG), 42, 4, 1â€“11.
[2]
Zhongyun Bao, Chengjiang Long, Gang Fu, Daquan Liu, Yuanzhen Li, Jiaming
Wu, and Chunxia Xiao. 2022. Deep image-based illumination harmonization.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 18542â€“18551.
[3]
Junyan Cao, Wenyan Cong, Li Niu, Jianfu Zhang, and Liqing Zhang. 2021. Deep
image harmonization by bridging the reality gap. arXiv preprint arXiv:2103.17104.
[4]
Junyan Cao, Yan Hong, and Li Niu. 2023. Painterly image harmonization in
dual domains. In Proceedings of the AAAI Conference on Artificial Intelligence
number 1. Vol. 37, 268â€“276.
[5]
Haoxing Chen, Zhangxuan Gu, Yaohui Li, Jun Lan, Changhua Meng, Weiqiang
Wang, and Huaxiong Li. 2023. Hierarchical dynamic image harmonization. In
Proceedings of the 31st ACM International Conference on Multimedia (MM â€™23).
Association for Computing Machinery, New York, NY, USA, 1422â€“1430. isbn:
9798400701085.
[6]
Jianqi Chen, Zhengxia Zou, Yilan Zhang, Keyan Chen, and Zhenwei Shi. 2023.
Zero-shot image harmonization with generative model prior. arXiv preprint
arXiv:2307.08182.
[7]
Daniel Cohen-Or, Olga Sorkine, Ran Gal, Tommer Leyvand, and Ying-Qing Xu.
2006. Color harmonization. In ACM SIGGRAPH 2006 Papers, 624â€“630.
[8]
Wenyan Cong, Li Niu, Jianfu Zhang, Jing Liang, and Liqing Zhang. 2021. Bar-
gainnet: background-guided domain translation for image harmonization. In
2021 IEEE International Conference on Multimedia and Expo (ICME). IEEE, 1â€“6.
[9]
Wenyan Cong, Xinhao Tao, Li Niu, Jing Liang, Xuesong Gao, Qihao Sun, and
Liqing Zhang. 2022. High-resolution image harmonization via collaborative
dual transformations. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, 18470â€“18479.
[10]
Wenyan Cong, Jianfu Zhang, Li Niu, Liu Liu, Zhixin Ling, Weiyuan Li, and
Liqing Zhang. 2020. Dovenet: deep image harmonization via domain verifica-
tion. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition, 8394â€“8403.
[11]
Xiaodong Cun and Chi-Man Pun. 2020. Improving the harmony of the compos-
ite image by spatial-separated attention module. IEEE Transactions on Image
Processing, 29, 4759â€“4771.
[12]
Sheng Guo, Weilin Huang, Xiao Zhang, Prasanna Srikhanta, Yin Cui, Yuan Li,
Hartwig Adam, Matthew R Scott, and Serge Belongie. 2019. The imaterialist
fashion attribute dataset. In Proceedings of the IEEE/CVF International Conference
on Computer Vision Workshops.
[13]
Zonghui Guo, Zhaorui Gu, Bing Zheng, Junyu Dong, and Haiyong Zheng. 2022.
Transformer for image harmonization and beyond. IEEE Transactions on Pattern
Analysis and Machine Intelligence.
[14]
Zonghui Guo, Dongsheng Guo, Haiyong Zheng, Zhaorui Gu, Bing Zheng, and
Junyu Dong. 2021. Image harmonization with transformer. In Proceedings of
the IEEE/CVF international conference on computer vision, 14870â€“14879.
[15]
Zonghui Guo, Haiyong Zheng, Yufeng Jiang, Zhaorui Gu, and Bing Zheng.
2021. Intrinsic image harmonization. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, 16367â€“16376.
[16]
Yucheng Hang, Bin Xia, Wenming Yang, and Qingmin Liao. 2022. Scs-co: self-
consistent style contrastive learning for image harmonization. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 19710â€“
19719.
[17]
Guoqing Hao, Satoshi Iizuka, and Kazuhiro Fukui. 2020. Image harmonization
with attention-based deep feature modulation. In BMVC. Vol. 1, 2.
[18]
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition, 770â€“778.
[19]
Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion proba-
bilistic models. Advances in Neural Information Processing Systems, 33, 6840â€“
6851.
[20]
Yifan Jiang et al. 2021. Ssh: a self-supervised framework for image harmo-
nization. In Proceedings of the IEEE/CVF International Conference on Computer
Vision, 4832â€“4841.
[21]
Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. 2022. Elucidating
the design space of diffusion-based generative models. Advances in Neural
Information Processing Systems, 35, 26565â€“26577.
[22]
Zhanghan Ke, Chunyi Sun, Lei Zhu, Ke Xu, and Rynson WH Lau. 2022. Har-
monizer: learning to perform white-box image and video harmonization. In
European Conference on Computer Vision. Springer, 690â€“706.
[23]
Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. 2022. Diffusionclip: text-
guided diffusion models for robust image manipulation. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2426â€“2435.
[24]
Diederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes.
arXiv preprint arXiv:1312.6114.
[25]
Pierre-Yves Laffont, Zhile Ren, Xiaofeng Tao, Chao Qian, and James Hays. 2014.
Transient attributes for high-level understanding and editing of outdoor scenes.
ACM Transactions on graphics (TOG), 33, 4, 1â€“11.
[26]
Jiajie Li, Jian Wang, Chen Wang, and Jinjun Xiong. 2023. Image harmonization
with diffusion model. arXiv preprint arXiv:2306.10441.
[27]
Jiajie Li, Jian Wang, Chen Wang, and Jinjun Xiong. 2023. Image harmonization
with diffusion model. (2023). arXiv: 2306.10441 [cs.CV].
[28]
Jingtang Liang, Xiaodong Cun, Chi-Man Pun, and Jue Wang. 2022. Spatial-
separated curve rendering network for efficient and high-resolution image
harmonization. In European Conference on Computer Vision. Springer, 334â€“349.
[29]
Jun Ling, Han Xue, Li Song, Rong Xie, and Xiao Gu. 2021. Region-aware
adaptive instance normalization for image harmonization. In Proceedings of
the IEEE/CVF conference on computer vision and pattern recognition, 9361â€“9370.
[30]
Lingxiao Lu, Jiangtong Li, Junyan Cao, Li Niu, and Liqing Zhang. 2023. Painterly
image harmonization using diffusion model. In Proceedings of the 31st ACM
International Conference on Multimedia, 233â€“241.
[31]
Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte,
and Luc Van Gool. 2022. Repaint: inpainting using denoising diffusion proba-
bilistic models. In Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition, 11461â€“11471.
[32]
Ziwei Luo, Fredrik K Gustafsson, Zheng Zhao, Jens SjÃ¶lund, and Thomas B
SchÃ¶n. 2023. Refusion: enabling large-size realistic image restoration with
latent-space diffusion models. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition, 1680â€“1691.
[33]
Li Niu, Wenyan Cong, Liu Liu, Yan Hong, Bo Zhang, Jing Liang, and Liqing
Zhang. 2021. Making images real again: a comprehensive survey on deep image
composition. ArXiv, abs/2106.14490. https://api.semanticscholar.org/CorpusID:
235658778.
[34]
Li Niu, Linfeng Tan, Xinhao Tao, Junyan Cao, Fengjun Guo, Teng Long, and
Liqing Zhang. 2023. Deep image harmonization with globally guided feature
transformation and relation distillation. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision, 7723â€“7732.
[35]
Jinlong Peng, Zekun Luo, Liang Liu, and Boshen Zhang. 2024. Frih: fine-grained
region-aware image harmonization. In Proceedings of the AAAI Conference on
Artificial Intelligence number 5. Vol. 38, 4478â€“4486.
[36]
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn
Ommer. 2022. High-resolution image synthesis with latent diffusion models.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 10684â€“10695.
[37]
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: convolutional
networks for biomedical image segmentation. In Medical Image Computing and
Computer-Assisted Interventionâ€“MICCAI 2015: 18th International Conference,
Munich, Germany, October 5-9, 2015, Proceedings, Part III 18. Springer, 234â€“241.
[38]
Hshmat Sahak, Daniel Watson, Chitwan Saharia, and David Fleet. 2023. De-
noising diffusion probabilistic models for robust image super-resolution in the
wild. (2023). arXiv: 2302.07864 [cs.CV].
[39]
Chitwan Saharia, William Chan, Huiwen Chang, Chris A. Lee, Jonathan Ho,
Tim Salimans, David J. Fleet, and Mohammad Norouzi. 2021. Palette: image-to-
image diffusion models. ACM SIGGRAPH 2022 Conference Proceedings.
[40]
Konstantin Sofiiuk, Polina Popenova, and Anton Konushin. 2021. Foreground-
aware semantic representations for image harmonization. In Proceedings of the
IEEE/CVF Winter Conference on Applications of Computer Vision, 1620â€“1629.
[41]
Shuangbing Song, Fan Zhong, Xueying Qin, and Changhe Tu. 2020. Illumination
harmonization with gray mean scale. In Advances in Computer Graphics: 37th
Computer Graphics International Conference, CGI 2020, Geneva, Switzerland,
October 20â€“23, 2020, Proceedings 37. Springer, 193â€“205.
[42]
Kalyan Sunkavalli, Micah K Johnson, Wojciech Matusik, and Hanspeter Pfister.
2010. Multi-scale image harmonization. ACM Transactions on Graphics (TOG),
29, 4, 1â€“10.
[43]
Yi-Hsuan Tsai, Xiaohui Shen, Zhe Lin, Kalyan Sunkavalli, Xin Lu, and Ming-
Hsuan Yang. 2017. Deep image harmonization. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, 3789â€“3797.
[44]
Jeya Maria Jose Valanarasu et al. 2022. Interactive portrait harmonization. arXiv
preprint arXiv:2203.08216.
[45]
Yikai Wang, Chenjie Cao, and Yanwei Fu. 2023. Towards stable and faithful
inpainting. arXiv preprint arXiv:2312.04831.
10600
MM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia
Pengfei Zhou, Fangxiang Feng, Guang Liu, Ruifan Li, Xiaojie Wang
[46]
Bin Xia, Yulun Zhang, Shiyin Wang, Yitong Wang, Xinglong Wu, Yapeng Tian,
Wenming Yang, and Luc Van Gool. 2023. Diffir: efficient diffusion model for
image restoration. In Proceedings of the IEEE/CVF International Conference on
Computer Vision, 13095â€“13105.
[47]
Yazhou Xing, Yu Li, Xintao Wang, Ye Zhu, and Qifeng Chen. 2022. Composite
photograph harmonization with complete background cues. In Proceedings of
the 30th ACM International Conference on Multimedia (MM â€™22). Association
for Computing Machinery, Lisboa, Portugal, 2296â€“2304. isbn: 9781450392037.
doi: 10.1145/3503161.3548031.
[48]
Ben Xue, Shenghui Ran, Quan Chen, Rongfei Jia, Binqiang Zhao, and Xing
Tang. 2022. Dccf: deep comprehensible color filter learning framework for
high-resolution image harmonization. In European Conference on Computer
Vision. Springer, 300â€“316.
[49]
Su Xue, Aseem Agarwala, Julie Dorsey, and Holly Rushmeier. 2012. Under-
standing and improving the realism of image composites. ACM Transactions
on graphics (TOG), 31, 4, 1â€“10.
[50]
Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023. Adding conditional
control to text-to-image diffusion models. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision, 3836â€“3847.
[51]
Pengfei Zhou, Fangxiang Feng, and Xiaojie Wang. 2024. Diffharmony: latent
diffusion model meets image harmonization. (2024). arXiv: 2404.06139 [cs.CV].
[52]
Zixin Zhu, Xuelu Feng, Dongdong Chen, Jianmin Bao, Le Wang, Yinpeng
Chen, Lu Yuan, and Gang Hua. 2023. Designing a better asymmetric vqgan for
stablediffusion. arXiv preprint arXiv:2306.04632.
[53]
Ziyue Zhu, Zhao Zhang, Zheng Lin, Ruiqi Wu, Zhi Chai, and Chun-Le Guo.
2022. Image harmonization by matching regional references. arXiv preprint
arXiv:2204.04715.
10601
