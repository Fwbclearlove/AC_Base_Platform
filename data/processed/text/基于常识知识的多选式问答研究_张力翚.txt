密级
：
公开


踏耆部ｆＡｆ


硕士学位论文


＿


题目
：基于常识知识的多选式问答研宄


学号
：２０２０１
１０６４３


姓名
：张力翬


学科专业
：智能科学与技术


培养方式
：全日制


导师
：李睿凡


学院
：人工智能学院


２０２３年
６月
１
日


中国
？北京


密级
：
公开


分ｉ却ｔ大聲


硕士学位论文
（学术学位）


题目
：基于常识知识的多选式间答研究


学号
：２０２０１１０６４３


姓名
：张力翬


学科专业
：智能科学与技术


培养方式
：全日制


导师
：李睿凡


学院
：人工智能学院


２０２３年
６月
１
日


ＳｅｃｒｅｔＬｅｖｅｌ
：Ｐｕｂｌｉｃ


＃Ｂｅｕ
ｉｎｇＵｎｉｖｅｒｓｉｔｙｏｆ


Ｐｏｓｔｓａｎｄ


Ｔｅｌｅｃｏｍｍｕｎｉｃａｔｉｏｎｓ


ＴｈｅｓｉｓｆｏｒＭａｓｔｅｒＤｅｇｒｅｅ


Ｔｉｔｌｅ
：ＭＵＬＴＩＰＬＥ
－ＣＨＯＩＣＥＱＵＥＳＴＩＯＮ


ＡＮＳＷＥＲＩＮＧＢＡＳＥＤＯＮＣＯＭＭＯＮＳＥＮＳＥ


ＫＮＯＷＬＥＤＧＥ


ＳｔｕｄｅｎｔＩＤ
：２０２０１１０６４３


Ｃａｎｄｉｄａｔｅ
：ＬｉｈｕｉＺｈａｎｇ


Ｍａ
ｊｏｒ
：ＩｎｔｅｌｌｉｇｅｎｔＳｃｉｅｎｃｅａｎｄＴｅｃｈｎｏｌｏｇｙ


Ｔｒａｉｎｉｎｇｍｅｔｈｏｄｓ
：Ｆｕｌｌ
－ｔｉｍｅ


Ｓｕｐｅｒｖｉｓｏｒ
：ＲｕｉｆａｎＬｉ


Ｉｎｓｔｉｔｕｔｅ
：ＳｃｈｏｏｌｏｆＡｒｔｉｆｉｃｉａｌＩｎｔｅｌｌｉｇｅｎｃｅ


Ｊｕｎｅ１
，２０２３


答辩委员会名单


职务姓
名职
称工
作
单
位


主席鲁鹏副教授北京邮电大学


委员李蕾教授北京邮电大学


委员周延泉副教授北京邮电大学


委员李睿凡副教授北京邮电大学


委员杜建凤高级工程师中国移动北京公司


秘书刘咏彬讲师北京邮电大学


答辩日期２０２３年
５月
２８
日


基于常识知识的多选式问答研究


摘
要


随着移动互联网的飞速发展与普及
，问答系统己在多个工业领域


成功落地并取得了良好的经济收益与社会价值
，而常识知识作为海量


认知信息中的研究重点
，在问答系统中的作用也在不断凸显
，具有极


大的研究前景
。
常识问答任务
（ＣｏｍｍｏｎｓｅｎｓｅＱｕｅｓｔｉｏｎＡｎｓｗｅｒｉｎｇ，


ＣＱＡ）正是研宂如何获取相关常识知识
，并通过问题解析与知识推理
，


进而获取精准答案
。
本文重点聚焦于ＣＱＡ任务在有监督和无监督场


景下的研究
。


目前ＣＱＡ任务存在以下问题亟待解决
：
在有监督场景下
，
目前


的研究工作集中于优化和改进模型的知识推理策略
，而忽视了知识覆


盖面不足及知识噪声等问题
，这会严重折损模型在知识推理阶段的性


能而致使预测偏差
。在无监督场景下
，
当前的多数研宄方法着重于设


计特定任务的手工规则以提升知识的生成质量
，因而导致知识类型受


限并且模型框架的自适应迁移能力较弱
。


故本文针对以上挑战进行了探索与研究
，并将具体工作总结如下
：


一
、针对有监督ＣＱＡ任务中的知识覆盖不足以及噪声问题
，
本


文提出了
一种基于知识增强的图对比学习模型
（ＫｎｏｗｌｅｄｇｅＥｎｈａｎｃｅｄ


ＧｒａｐｈＣｏｎｔｒａｓｔｉｖｅＬｅａｒ
ｎｉｎｇ，ＫＥ
－ＧＣＬ
）
。首先
，该模型将问答对的实体


上下文描述集成到当前知识子图以实现多源知识融合
；随后
，模型提


出了
一个自适应的带权采样策略以生成当前子图的增强视图
，并同时


完成正负图例的构建
；最后
，该模型通过关系边的散射与实体节点的


信息聚集以完成知识图谱的更新与推理
。


二
、
针对无监督ＣＱＡ任务中的迁移及自适应能力弱的问题
，
本


文设计了
一种基于通用提示模版的知识生成模型
（Ｐｒｏｍｐ
ｔ
－ｂａｓｅｄ


ＫｎｏｗｌｅｄｇｅＧｅｎｅｒａｔｉｏｎＮｅｔｗｏｒｋ
，ＰＫＧＮ）〇首先
，
该模型通过Ｄｒｏｐｏｕｔ


增强策略进行无监督的对比学习
，以捕获问题之间的细微差异并学到


更好的问题表征
；接着
，
ＰＫＧＮ模型通过带指令的模版提示以生成问


题相关的知识描述
；最后
，该模型利用文本匹配模型完成知识推理与


答案预测
。


本文在ＣｏｍｍｏｎｓｅｎｓｅＱＡ
、
ＯｐｅｎｂｏｏｋＱＡ
、
ＳｏｃｉａｌｌＱＡ等三个常识


问答数据集上开展了大量的实验
。通过定量分析和定性对比
，验证了


Ｉ


ＫＥ
－ＧＣＬ和
ＰＫＧＮ模型在有监督和无监督方向的可行性与有效性
。


实验结果表明
，本文所提出的常识问答模型在有监督和无监督两个方


向均优于当前的基线方法
，
且具备较好的鲁棒性与泛化能力
。


关键词
：常识问答知识图谱对比学习
图神经网络注意力机制


ＩＩ


ＭＵＬＴＩＰＬＥ
－ＣＨＯＩＣＥＱＵＥＳＴＩＯＮ


ＡＮＳＷＥＲＩＮＧＢＡＳＥＤＯＮ


ＣＯＭＭＯＮＳＥＮＳＥＫＮＯＷＬＥＤＧＥ


ＡＢＳＴＲＡＣＴ


Ｗｉｔｈｔｈｅｒａｐ
ｉｄｄｅｖｅｌｏｐｍｅｎｔａｎｄ
ｐｏｐｕｌａｒｉｚａｔｉｏｎｏｆ
ｔｈｅｍｏｂｉｌｅＩｎｔｅｒ
ｎｅｔ
，


ｑｕｅｓｔｉｏｎａｎｓｗｅｒｉｎｇｓｙｓｔｅｍｓｈａｖｅｂｅｅｎｓｕｃｃｅｓｓｆｕｌｌｙ
ｉｍｐ
ｌｅｍｅｎｔｅｄｉｎｍａｎｙ


ｉｎｄｕｓｔｒｉａｌｆｉｅｌｄｓａｎｄｈａｖｅａｃｈｉｅｖｅｄｇｏｏｄｅｃｏｎｏｍｉｃｂｅｎｅｆｉｔｓａｎｄｓｏｃｉａｌ


ｖａｌｕｅ
．Ｃｏｍｍｏｎｓｅｎｓｅｋｎｏｗｌｅｄｇｅ
，ａｓｔｈｅｒｅｓｅａｒｃｈｆｏｃｕｓｏｆｍａｓｓｉｖｅ


ｃｏｇｎｉｔｉｖｅｉｎｆｏｒｍａｔｉｏｎ
，
ｐ
ｌａｙｓａｎｉｍｐｏｒ
ｔａｎｔｒｏｌｅｉｎｑｕｅｓｔｉｏｎａｎｓｗｅｒｉｎｇ


ｓｙｓｔｅｍｓ
．Ｉｔｉｓａｌｓｏｃｏｎｓｔａｎｔｌｙｈｉｇｈｌｉｇｈｔｅｄａｎｄｈａｓ
ｇｒｅａｔｒｅｓｅａｒｃｈ
ｐｒｏｓｐｅｃｔｓ
．


ＴｈｅＣｏｍｍｏｎｓｅｎｓｅＱｕｅｓｔｉｏｎＡｎｓｗｅｒ
ｉｎｇ（ＣＱＡ）ｔａｓｋｉｓｔｏｓｔｕｄｙｈｏｗｔｏ


ｏｂｔａｉｎｒｅｌｅｖａｎｔｃｏｍｍｏｎｓｅｎｓｅｋｎｏｗｌｅｄｇｅ
，ａｎｄｐｒｅｄｉｃｔｃｏｒｒｅｃｔａｎｓｗｅｒｓ


ｔｈｒｏｕｇｈｑｕｅｓｔｉｏｎｕｎｄｅｒｓｔａｎｄｉｎｇａｎｄｋｎｏｗｌｅｄｇｅｒｅａｓｏｎｉｎｇ
．Ｔｈｉｓｔｈｅｓｉｓ


ｆｏｃｕｓｅｓｏｎｔｈｅｒｅｓｅａｒｃｈｏｆＣＱＡｔａｓｋｓｉｎｔｈｅｓｕｐｅｒｖｉｓｅｄａｎｄｕｎｓｕｐｅｒｖｉｓｅｄ


ｓｃｅｎａｒｉｏｓ
．


Ａｔ
ｐｒｅｓｅｎｔ
，ｔｈｅｆｏｌｌｏｗｉｎｇｐｒｏｂｌｅｍｓｉｎＣＱＡｔａｓｋｓｎｅｅｄｔｏｂｅｓｏｌｖｅｄ


ｕｒｇｅｎｔｌｙ
：ｉｎｓｕｐｅｒｖｉｓｅｄｓｃｅｎａｒｉｏｓ
，ｔｈｅｃｕｒｒｅｎｔｒｅｓｅａｒｃｈｗｏｒｋｆｏｃｕｓｅｓｏｎ


ｏｐ
ｔｉｍｉｚｉｎｇａｎｄｉｍｐｒｏｖｉｎｇ
ｔｈｅｋｎｏｗｌｅｄｇｅｒｅａｓｏｎｉｎｇｓｔｒａｔｅｇｙｏｆｔｈｅｍｏｄｅｌ
，


ｗｈｉｌｅｉｇｎｏｒ
ｉｎｇｐｒｏｂｌｅｍｓｓｕｃｈａｓｉｎｓｕｆ
ｉｃｉｅｎｔｋｎｏｗｌｅｄｇｅｃｏｖｅｒａｇｅａｎｄ


ｋｎｏｗｌｅｄｇｅｎｏｉｓｅ
，ｗｈｉｃｈｗｉｌｌｓｅｒｉｏｕｓｌｙｄａｍａｇｅｔｈｅｍｏｄｅＰｓｄｅｇｒａｄａｔｉｏｎｉｎ


ｔｈｅｋｎｏｗｌｅｄｇｅｒｅａｓｏｎｉｎｇｓｔａｇｅｒｅｓｕｌｔｉｎｇ
ｉｎ
ｐｒｅｄｉｃｔｉｏｎｂｉａｓ
．Ｉｎｕｎｓｕｐｅｒｖｉｓｅｄ


ｓｃｅｎａｒｉｏｓ
，ｍｏｓｔｃｕｒｒｅｎｔｒｅｓｅａｒｃｈｍｅｔｈｏｄｓｆｏｃｕｓｏｎｄｅｓｉｇｎｉｎｇｍａｎｕａｌｒｕｌｅｓ


ｆｏｒｓｐｅｃｉｆ
ｉｃｔａｓｋｓｔｏｉｍｐｒｏｖｅｔｈｅｑｕａｌｉｔｙｏｆ
ｇｅｎｅｒａｔｅｄｋｎｏｗｌｅｄｇｅ
，ｗｈｉｃｈ


ｌｅａｄｓｔｏｒｅｓｔｒｉｃｔｅｄｋｎｏｗｌｅｄｇｅｔｙｐｅｓａｎｄｗｅａｋｔｒａｎｓｆｅｒｃａｐａｂｉｌｉｔｉｅｓｂｅｔｗｅｅｎ


ｄｉｆｆｅｒｅｎｔｔａｓｋｓ
．


Ｔｈｅｒｅｆｏｒｅ
，ｔｈｅｔｈｅｓｉｓｍａｋｅｓｄｅｅｐｅｘｐ
ｌｏｒａｔｉｏｎｓａｎｄｓｔｕｄｉｅｓｒｅｇａｒｄｉｎｇ


ｔｈｅｓｅｃｈａｌｌｅｎｇｅｓ
，ａｎｄｓｕｍｍａｒ
ｉｚｅｓｔｈｅｗｏｒｋａｓｆｏｌｌｏｗｓ
：


１
．Ａｉｍｉｎｇａｔｉｎｓｕｆｆ
ｉｃｉｅｎｔｋｎｏｗｌｅｄｇｅｃｏｖｅｒａｇｅａｎｄｎｏｉｓｅ
ｐｒｏｂｌｅｍｓｉｎ


ｈｉ


ｓｕｐｅｒｖｉｓｅｄＣＱＡｔａｓｋｓ
，ｔｈｅｔｈｅｓｉｓｐｒｏｐｏｓｅｓａｋｎｏｗｌｅｄｇｅ
－ｅｎｈａｎｃｅｄｇｒａｐｈ


ｃｏｎｔｒａｓｔｉｖｅｌｅａｒｎｉｎｇｍｏｄｅｌ（ＫＥ
－ＧＣＬ）
．Ｆｉｒｓｔ
，ｔｈｅｍｏｄｅｌｉｎｔｅｇｒａｔｅｓｔｈｅ


ｃｏｎｔｅｘｔｕａｌｄｅｓｃｒｉｐ
ｔｉｏｎｓｆｏｒｔｈｅｅｎｔｉｔｉｅｓｉｎｔｈｅＱＡｐａｉｒｉｎｔｏｔｈｅ


ｃｏｒｒｅｓｐｏｎｄｉｎｇｋｎｏｗｌｅｄｇｅｓｕｂｇｒａｐｈ
，ｔｏａｃｈｉｅｖｅｍｕｌｔｉ
－ｓｏｕｒｃｅｋｎｏｗｌｅｄｇｅ


ｆｕｓｉｏｎ
；ｔｈｅｎ
，ｔｈｅｍｏｄｅｌ
ｐｒｏｐｏｓｅｓａｎａｄａｐ
ｔｉｖｅｓａｍｐ
ｌｉｎｇｓｔｒａｔｅｇｙｔｏ
ｇｅｎｅｒａｔｅ


ｔｈｅｋｎｏｗｌｅｄｇｅ
－ｅｎｈａｎｃｅｄｖｉｅｗｏｆ
ｔｈｅｃｕｒｒｅｎｔｓｕｂｇｒａｐｈ
，ａｎｄｓｉｍｕｌｔａｎｅｏｕｓｌｙ


ｃｏｎｓｔｒｕｃｔｓｔｈｅｐｏｓｉｔｉｖｅａｎｄｎｅｇａｔｉｖｅｇｒａｐｈｐａｉｒｓ
；ｆｉｎａｌｌｙ，ｔｈｅｍｏｄｅｌ


ｐｅｒｆｏｒｍｓｅｄｇｅｓ
５
ｓｃａｔｅｒｉｎｇａｎｄｎｏｄｅｓ
５
ａｇｇｒｅｇａｔｉｏｎｔｏｕｐｄａｔｅａｎｄｒｅａｓｏｎ


ｏｖｅｒｔｈｅｋｎｏｗｌｅｄｇｅ
ｇｒａｐｈ
．


２
．Ｔｏｓｏｌｖｅｔｈｅｐｒｏｂｌｅｍｏｆｒｅｓｔｒｉｃｔｅｄｋｎｏｗｌｅｄｇｅｔｙｐｅｓａｎｄｗｅａｋ


ｔｒａｎｓｆｅｒｃａｐａｂｉｌｉｔｉｅｓｉｎｔｈｅｕｎｓｕｐｅｒｖｉｓｅｄＣＱＡｔａｓｋ
，ｔｈｅｔｈｅｓｉｓｄｅｓｉｇｎｓ


Ｐｒｏｍｐｔ
－ｂａｓｅｄＫｎｏｗｌｅｄｇｅＧｅｎｅｒａｔｉｏｎＮｅｔｗｏｒｋ（ＰＫＧＮ）
．Ｆｉｒｓｔ
，ｔｈｅｍｏｄｅｌ


ｐｅｒｆｏｒｍｓｕｎｓｕｐｅｒｖｉｓｅｄｃｏｎｔｒａｓｔｉｖｅｌｅａｒｎｉｎｇ
ｔｈｒｏｕｇｈｔｈｅＤｒｏｐｏｕｔ


ｅｎｈａｎｃｅｍｅｎｔｓｔｒａｔｅｇｙｔｏｃａｐ
ｔｕｒｅｔｈｅｓｕｂｔｌｅｄｉｆｆ
ｅｒｅｎｃｅｓｂｅｔｗｅｅｎ
ｑｕｅｓｔｉｏｎｓ


ａｎｄｌｅａｒｎｂｅｔｅｒｒｅｐｒｅｓｅｎｔａｔｉｏｎｓ
；ｔｈｅｎ
，ｔｈｅｍｏｄｅｌｕｓｅｓｉｎｓｔｒｕｃｔｉｏｎ
ｐｒｏｍｐｔｓ


ｔｏｇｅｎｅｒａｔｅｑｕｅｓｔｉｏｎ
－ｒｅｌａｔｅｄｋｎｏｗｌｅｄｇｅｓｔａｔｅｍｅｎｔｓ
；ｆｉｎａｌｌｙ，ｔｈｅｍｏｄｅｌ


ｌｅｖｅｒａｇｅｓｔｈｅｔｅｘｔｍａｔｃｈｉｎｇｍｏｄｅｌｔｏｐｅｒｆｏｒｍｋｎｏｗｌｅｄｇｅｒｅａｓｏｎｉｎｇａｎｄ


ａｎｓｗｅｒ
ｐｒｅｄｉｃｔｉｏｎ
．


Ｉｎｔｈｉｓｔｈｅｓｉｓ
，ａｌａｒｇｅｎｕｍｂｅｒｏｆｅｘｐｅｒｉｍｅｎｔｓｈａｖｅｂｅｅｎｃａｒｒ
ｉｅｄｏｕｔｏｎ


ｔｈｒｅｅｃｏｍｍｏｎｓｅｎｓｅ
ｑｕｅｓｔｉｏｎａｎｓｗｅｒｄａｔａｓｅｔｓ
，ｉｎｃｌｕｄｉｎｇＣｏｍｍｏｎｓｅｎｓｅＱＡ
，


ＯｐｅｎｂｏｏｋＱＡ
，ａｎｄＳｏｃｉａｌｌＱＡ
．Ｔｈｒｏｕｇｈｑｕａｎｔｉｔａｔｉｖｅａｎａｌｙｓｉｓａｎｄ


ｑｕａｌｉｔａｔｉｖｅｃｏｍｐａｒｉｓｏｎｓ
，ｔｈｅｆｅａｓｉｂｉｌｉｔｙａｎｄｅｆｆ
ｅｃｔ
ｉｖｅｎｅｓｓｏｆＫＥ
－ＧＣＬａｎｄ


ＰＫＧＮｍｏｄｅｌｓｉｎｓｕｐｅｒｖｉｓｅｄａｎｄｕｎｓｕｐｅｒｖｉｓｅｄｄｉｒｅｃｔｉｏｎｓａｒｅｆｕｒｔｈｅｒ


ｖｅｒｉｆｉｅｄ
．Ｔｈｅｅｘｐｅｒｉｍｅｎｔａｌｒｅｓｕｌｔｓｓｈｏｗｔｈａｔｔｈｅｐｒｏｐｏｓｅｄｍｏｄｅｌｓ
，ｗｉｔｈ


ｇｏｏｄｒｏｂｕｓｔｎｅｓｓａｎｄ
ｇｅｎｅｒａｌｉｚａｔｉｏｎｃａｐａｂｉｌｉｔｉｅｓ
，ｉｎｔｈｅｔｈｅｓｉｓａｒｅｃｏｎｓｔａｎｔｌｙ


ｂｅｔｔｅｒｔｈａｎｔｈｅｃｕｒｒｅｎｔｂａｓｅｌｉｎｅｓｉｎｂｏｔｈｓｕｐｅｒｖｉｓｅｄａｎｄｕｎｓｕｐｅｒｖｉｓｅｄ


ｓｃｅｎａｒｉｏｓ
．


ＫＥＹＷＯＲＤＳ
：ｃｏｍｍｏｎｓｅｎｓｅｑｕｅｓｔｉｏｎａｎｓｗｅｒｉｎｇ，ｋｎｏｗｌｅｄｇｅｇｒａｐｈ
，


ｃｏｎｔｒａｓｔｉｖｅｌｅａｒｎｉｎｇ，ｇｒａｐｈｎｅｕｒａｌｎｅｔｗｏｒｋ
，ａｔｅｎｔｉｏｎｍｅｃｈａｎｉｓｍ


ＩＶ


目
录


第
一章绪论
１


１
．
１研究背景及其意义
１


１
．２研究现状及分析
３


１
．２
．
１有监督常识问答
３


１
．２
．２无监督常识问答
，
５


１
．３研宄内容和工作贡献
７


１
．３
．１现存问题
７


１
．３
．２研究内容
８


１
．３
．３贡献及创新点
９


１
．４文章组织结构
１０


１
．５本章小结



１
１


第二章
相关理论与技术
１２


２
．１
文本向量化
１２


２
．
１
．
１独热编码
１２


２
．１
．２神经概率语言模型

１３


２
．
１
．３词嵌入表示
１４


２
．２知识表示
１６


２
．２
．
１
神经网络模型
１７


２
．２
．２翻译模型
１８


２
．３深度学习技术
１９


２
．３
．
１注意力机制
２０


２
．３
．２预训练语言模型
２１


２
．３
．３
图神经网络
２７


２
．３
．４提示学习
３０


２
．４本章小结
３２


第三章基于知识增强型图对比学习的常识问答算法研究
３３


３
．
１弓
Ｉｇ
３３


３
．２基于知识增强的图对比学习模型


３３


３
．２
．
１模型总览
３３


３
．２
．２多源知识表示与融合模块
３４


３
．２
．３
自适应图增强模块
３６


３
．２
．４知识图推理模块
３６


３
．２
．５答案预测模块
３７


３
．２
．６正负图例构造与目标函数
３７


３
．３实验分析
３９


３
．３
．
１数据集与评估指标
３９


３
．３
，２对比基线设置
４０


３
．３
．３实验参数设置
４２


３
．３
．４实验结果分析
４２


３
．３
．５消融实验
４３


３
．３
．６案例分析
４５


３
．３
．７可视化分析
４６


３
．３
．８关键参数分析
４７


３
．４本章小结
４８


第四章基于提示型知识生成的常识问答算法研究４９


４
．
１
弓
丨言
４９


４
．２基于提示学习的知识生成模型
４９


４
．２
．１模型总览
４９


４
，２
．２基子问题语义的对比学习模块
５０


４
．２
．３提不型知识生成模块

５２


４
．２
．４知识推理与答案预测模块
５６


４
．３实验对比与分析
５６


４
．３
．１数据集与评估指标
５６


４
．３
．２对比基线设置


５７


４
．３
．３实验参数设置
５８


４
．３
．４实验结果分析
５９


４
．３
．５消融实验
６０


４
．３
．６模版敏感度分析
６１


４
．３
．７案例分析

６２


４
．４本章小结
６３


第五胃
总结Ｓ展望
６５


５
．
１
工作总结
６５


５
．２未来工作展望
６６


参考文献
６７


第
一章绪论


第
一章绪论


１
．
１研究背景及其意义


近年来深度学习相关的人工智能技术得到了飞速发展
，
自然语言处理
、语音


信号处理
、计算机视觉等领域实现了革新式的突破
，并为工业界带来了良好的经


济收益和社会价值
。伴随着各种智能化产业服务的应用落地
，如何从大规模的海


量数据中准确地提取价值信息己成为现代科技发展的核心重点
，而获取知识及信


息的途径也在不断朝着自然交互化的趋势演进
：
一方面期望通过更自然的提问方


式与机器系统产生交互
，
另
一方面期望能够直接获取精准的答案
。这种信息获取


的发展态势也逐渐形成了下
一代搜索引擎的主体形态
，相比于传统的网页搜索能


极大地提尚检索效率
。


问句
０國０
语义表示０


Ｐｅｒｓｏｎ３４
（
１ｒ
ｔ）


＾知识库


”
中
―
国


图
１
－
１
基于知识的
自动问答任务


自动问答
（ＱｕｅｓｔｉｏｎＡｎｓｗｅｒｉｎｇ
）
旨在回答用户提出的各类自然语言问题
，在


具备海量数据的开源互联系统中
，知识作为行为与认知的重点
，在问答理解与推


理上作用不断凸显
。
谷歌于
２０
１２年首次提出了知识图谱的概念
，
知识图谱通过


模拟人类理解客观世界的方式来动态构建知识
，同时赋予了机器建模知识与理解


语义的可能
，高质量的数据由此开始以大规模的开源知识库方式出现
，其中蕴涵


了大量实体属性关系与拓扑结构信息
。
由此
，
基于知识的自动问答
（Ｋｎｏｗｌｅｄｇｅ


Ｂａｓｅｄ
ＱｕｅｓｔｉｏｎＡｎｓｗｅｒｉｎｇ
，ＫＢＱＡ
）应运而生
，并迅速成为了问答任务的
一个重要


的分支
［
１
］
。
图
１
－
１展示了ＫＢＱＡ任务的具体定义
，
即给定
一个自然语言问题
，机


器需要对问题进行语法解析和语义匹配
，并结合相关的外部知识库进行查询和推


理
，
进而获得精确的预测答案
。
根据知识来源的不同
，
ＫＢＱＡ任务可主要划分


１


北京邮电大学工学硕士学位论文


为开放领域
（ＯｐｅｎＤｏｍａｉｎ）知识问答
，
如百科知识
、常识的问答
；
以及特定


领域
（ＣｅｒｔａｉｎＤｏｍａｉｎ
）
知识问答
［３４
］
，
如金融
、
医学领域的问题问答
。


从工业落地的角度来看
，基于知识的问答系统在现阶段也得到了广泛的应用
，


典型的落地服务场景包括
：智能语音助手
、智能客服
、搜索引擎
、情感类聊天等
。


在语音助手领域
，
广为人知的微软小冰
、苹果
Ｓｉｒ
ｉ
、
小米小爱等智能助手产品正


是依托了高性能ＫＢＱＡ系统的支撑
，才能为用户提供日常化的精准化定制服务
。


在搜索引擎场景下
，
百度推出的
“框计算
”
，
搜狗推出的
“ 立知
”等引擎服务
［４３
］
，
正


是将用户的自然语言问题作为搜索查询词进行知识查询与检索排序
，从而直接为


用户提供准确简洁的答案
，而不是返回
一个和问题最相关的网页供用户自行查找
，


这大大降低了用户获取信息的成本
。在智能客服领域
，
阿里小蜜通过知识图谱技


术
，
自动解答用户关于支付宝使用时遇到的问题
，这不仅降低了使用人工客服带


来的人力成本
，
同时也降低了用户获得服务的响应时间
，并提升用户体验
。
除此


之外
，
基于知识库的自动问答在电信运营商
、金融保险
、税务管理等行业领域也


有着广泛的应用
。


ｎＨ
ｃｏｍｍｕｎｉｃａｔ
ｉｎｇｗｉｔｈｍｙｂｏｓｓ
，ｗｈａｔｓｈｏｕｌｄＩｄｏ？


［在和老板交流的时候，
我应该怎么做］


Ａ
．ｍｉｓｕｎｄｅｒｓｔａｎｄｉｎｇｓ（产生）误会


欠案屯
ｔｒａｎｓｆｅｒｏｆ
ｉｎｆｏｒｍａｔｉｏｎｓＢ．
（进行｝值患传递


选项Ｃ
．ｌｅａｒｎｉｎｇＣ
．学习


Ｄ
．ｃｏｎｆｕｓｉｏｎＤ
．疑惑


Ｅ
．ｓｉｌｅｎｃｅ（保持）沉默


图
１
－２
常识问答任务的真实样例


在
ＫＢＱＡ任务中
，
常识作为人类社会对同
一事物普遍存在的日常共识
，
是


知识的
一种重要表现形式
。
常识问答任务
（ＣｏｍｍｏｎｓｅｎｓｅＱｕｅｓｔｉｏｎＡｎｓｗｅｒｉｎｇ，


ＣＱＡ
）考验的正是模型的常识推理能力
，这类任务
一般聚焦于多项选择式问答形


式
，
需要模型针对给定的问题进行推理
，
从而选择最为贴切的答案
，
目前常见的


相关数据集有ＣｏｍｍｏｎｓｅｎｓｅＱＡ
［２
］
、
ＯｐｅｎｂｏｏｋＱＡ
［３
］
、
Ｓｏｃ
ｉａｌＩＱＡ
［４
］
、
ＣｏｓｍｏｓＱＡ
［
：！
］
、


以及ＮｕｉｎｅｒＳｅｎｓｅＷ等
。区别于机器阅读理解和事实性抽取等传统问答任务
，常识


问答任务通常只给出题干而没有给出相关的背景知识
。
在ＣＱＡ任务中
，
模型不


仅要理解问题和选项的语义
，还需要具备相关的常识背景知识才能进行合理的推


断及预测
。
在图
１
－２所示的
ＣＱＡ样例中
，
模型只有在知道
“老板需要员工向他


汇报消息
” 的知识前提下
，才能推断出正确选项
Ｂ（
“ 进行信息传递
”
）
。因此
，


２


第
一章绪论


如何有效获取高质量的常识知识
，并执行有效地知识推理
，这也正是常识问答任


务的难点所在
。
目前常见的大型知识库
，
例如
ＹＡＧ〇Ｐ］
，
ＯｐｅｎＩＥ
［８
］
，ＮＥＬＬＭ
，


ＤＢｐｅｄｉａ
［１Ｑ］
，Ｆｒｅｅｂａｓｅ
［
１
１
］
，ＷｅｂＣｈｉｌｄ
［１２］以及Ｃｏｎｃｅｐ
ｔＮｅｔ
［１３＾
，
这些结构型数据库以


机器可读的方式存储
“ 实体
－关系
－实体
” 的三元组知识
，并可根据资源描述框架规


范对知识进行描述
。这些知识库组成了常识信息的大型语义网络
，具有极好的知


识表达能力和灵活的建模空间
，
因而能够辅助模型进行结构化的常识知识推理
，


并输出具备高置信度的答案决策
。


本文的工作聚焦于基于知识的自动问答领域中的常识问答任务
，并在有监督


和无监督的两个方向展开详细研宄
。通过建立端到端的深度学习网络
，并运用知


识建模及推理的相关技术解决ＣＱＡ任务中现存的
一些挑战和困难
。


１
．２研究现状及分析


本节将针对基于常识知识库的多选式问答任务中的有监督和无监督两类研


究方向
，
简述对应的相关工作及研宄现状
。


１
．２
．
１有监督常识问答


有监督常识问答任务
（ＳｕｐｅｒｖｉｓｅｄＣｏｍｍｏｎｓｅｎｓｅ
ＱｕｅｓｔｉｏｎＡｎｓｗｅｒｉｎｇ）
己近经


历了
一段相当长的发展时间
，研宄要点也从最开始的针对于垂直领域的小型知识


库扩展到更为通用领域的超大型知识库
，技术路线也有了极大的丰富
，从答案获


取方式的区别来看
，
目前的研宄大致可以分为以下两个方向
［
１４第
一种是基于语


义解析（ＳｅｍａｎｔｉｃＰａｒｓｉｎｇ）的方法
；第二种是基于信息检索（ＩｎｆｏｒｍａｔｉｏｎＲｅｔｒ
ｉｅｖａｌ）


的方法
，包括了现阶段广泛应用的基于深度学习进行向量建模
（Ｖｅｃｔ
ｏｒＭｏｄｅｌｉｎｇ）


的思路
，
下面将依次进行说明
。


基于语义解析的方法通常将自然语言转化成中间逻辑表示
（Ｌｏｇ
ｉｃａｌＦｏｒｍｓ）
，


然后再将其转化为可以在知识图谱中直接执行的描述性语言
，
例如
ＳＰＡＲＱＬ语


言
［
１５］
。此类方法是
一种比语法分析更高层次的分析
，其核心环节是将非结构化的


自然语言问题自底向上转化为
一系列逻辑形式
，结构化查询知识库得到能够表达


问题语义的无二义性的表达式
。


传统语义解析的方法主要是在语法的角度
，依靠人工规则进行构造
：如基于


经典组合类别语法
［
１６
］（ＣＣＧ）
、
词汇树伴随语法
［２４
］（ＬＴＡＧ）等方法
，
从而将自


然语言问题转化为逻辑表达式或知识库的查询语言
。
由此可见
，传统的方法依赖


于高质量的字典
、
手工构建的模版和语言特性
，
当面对如
ＦｒｅｅｂａｓｅＭ或


ＤＢｐｅｄｉａ
［
１
（）
］等的大规模知识图谱的时候
，
往往效果欠佳
，
并且大多没有利用知识


３


北京邮电大学工学硕士学位论文


图谱的信息来进行语义解析
。因此
，Ｙｉｈ等人
［２５
］首次提出了查询图
（ＱｕｅｒｙＧｒａｐｈ）


方法
，该框架首先定义了
一个可以直接转化为
Ｌａｍｂｄａ演算的查询图
，并定义了


如何将语义解析的过程演变为查询图的生成过程
，
最后通过
ＬａｍｂｄａＲａｎｋ算法


［２６
］对生成的查询图进行排序
，
筛选出最佳答案
。
Ｈｕ等人
则通过状态迁移可学


习
（ＳｔａｔｅＴｒａｎｓｉｔｉｏｎ
－ｂａｓｅｄ）方法
，
定义了四种原子操作和
一种可学习的状态迁移


模型
，
将复杂的文本图转化为查询图
。
该方法可以利用
Ｂ
ｉＬＳＴＭ模型识别出多


个节点
，并采用卷积操作以抽取节点间的多种关系
，弥补了查询图生成方法中假


设的只能具备
一个主要关系并使用特定规则识别的缺陷
，
具备更好的泛化特性
。


Ｄｏｎｇ等人
［２８
］则提出了
一种基于注意力增强的编
－解码器
（Ｅｎｃｏｄｅｒ
－Ｄｅｃｏｄｅｒ
）方法
，


将语义解析问题建模为源序列到目标序列的生成问题
。这个带注意力的编解码模


型用于输入自然语言以获得逻辑形式
。
Ｘｕ等人
［２９Ｈ人为编码器的句法特征是非常


重要的
，并利用
一个句法图来表示词序
、依赖关系和区域特征
，并利用ＧｒａＰｈ２Ｓｅｑ


图编码器对句法图进行编码
，然后利用带注意力的循环神经网络进行解码以得到


其逻辑形式
。
Ｃｕｉ
等人＿则首次应用模版来表示自然语言问题并解决
ＢＦＱ


（
ＢｉｎａｒｙＦａｃｔｏｉｄＱｕｅｓｔｉｏｎｓ）问题
，
并从
一个大规模的问答语料库中学习自动学习


模版
，
而不是手动对模版进行标注
。
此外
，
近年来还出现了利用神经符号思想


（ＮｅｕｒａｌＳｙｍｂｏｌｉｃＭａｃｈｉｎｅｓ
）来处理语义解析问题
，
Ｌｉａｎｇ等人
［３
１
］提出神经符号


机器思想
，它包含
一个神经程序设计算法
，
即
一个端到端的模型来将语言映射到


程序
，
同时包括
一个符号计算器
，
即
一个可执行程序的
Ｌｉｓｐ解释器


基于信息检索的方法首先会确定自然语言问题中的实体提及词
（Ｅｎｔｉｔｙ


Ｍｅｎｔｉｏｎ
）
，然后链接到知识图谱中的主题实体
（Ｔｏｐ
ｉｃＥｎｔｉｔｙ
）
，将与主题实体相


关的子图
（Ｓｕｂｇｒａｐｈ）提取出来
，子图中的每
一个节点或边都可以作为候选答案
，


通过观察问题依据某些规则或模版进行信息抽取
，得到表征问题和候选答案特征


的特征向量
，建立分类器通过输入特征向量对候选答案进行筛选和排序
，从而得


出最终答案
。与语义解析的方法相比
，该方法无需设计并定义特征或者逻辑规则
，


可利用深度学习网络抽取问题及知识的语义特征表示并进行端到端的训练学习
，


能迀移与应用到很多领域
，
后文对应的有监督研究工作也是基于此分支展开
。


依据特征表示技术不同
，信息检索的方法可细分为基于特征工程和基于表示


学习的方法
。
Ｙａｏ等人
［３５
］首次基于特征工程提出
一种基础模型
，首先对问句进行


句法分析
，并对其依存句法分析结果提取问题词
（Ｑｗｏｒｄ）
、问题焦点词
（Ｑｆ
ｏｃｕｓ
）
、


主题词
ＣＱｔｏｐ
ｉｃ
）和中心动词
（Ｑｖｅｒｂ
）特征
，
并逐次转化为问题特征图
；
随后利


用主题词在知识库内抽取相关子图
，从而生成候选项对应的特征图
；根据人工规


则和模版将问句与候选特征图中的特征向量进行组合
，建立分类器对输入的特征


向量进行过滤
，最终得到正确答案
。但这种特征工程思想需要人为定义并抽取相


４


第
一章绪论


关特征
，难以处理多领域的复杂问题情况
。随着词嵌入技术和深度学习的飞速发


展
，近年来的研宄工作逐渐趋向应用表示学习的方法
，将问句和候选答案转换为


同
一语义空间的向量
，知识库问答的处理过程则看成是问句与候选答案的表示向


量进行匹配计算过程
。
目前多种基于嵌入的方法都展现了颇具前景的结果
，这种


类型的方法采用多种形式来编码问题和知识子图
，并且直接在映射空间上对它们


进行匹配
，利用注意力机制融合问题查询与知识子图的信息
，从而可实现端到端


的训练
。
Ｂｏｒｄｅｓ等人
［３６
］率先将问句以及知识子图中的候选答案实体映射到同
一


语义空间
，
候选答案实体利用三种向量进行表示
：
１
）答案实体本身
；
２）答案实


体与主实体关系路径
；
３
）与答案实体相关子图
。Ｄｏｎｇ等人
则考虑了词序信息
、


问句与答案的关系对任务效果的提升是有效的
，
利用ＣＮＮ分别对问句和答案类


型
、答案实体关系路径和答案实体单跳
（Ｏｎｅ
－Ｈｏｐ）范围内的子图进行编码
，
以


获取不同的语义表示
。Ｂｏｒｄｅｓ等人
［３８］在之后的工作中又结合了记忆网络（Ｍｅｍｏｒｙ


Ｎｅｔｗｏｒｋ）
用于训练过程的弱监督学习
［３９
］
，
并利用知识图谱中的三元组知识信息


来解决图谱推理中简单的单跳问题
。
Ｃｈｅｎ等人
［４Ｑ］提出的ＢＡＭｎｅｔ模型
，
则考虑


通过
Ａｔｔ
ｅｎｔｉｏｎ机制来捕捉问题与知识库三元组信息间的两两交互性
，并以此利


用相关性增强问题的特征表示
，
同时也能结合到更多的知识库信息
，从而提升处


理复杂问题的能力
。


为了应对知识图谱的不完整性
，
Ｓａｘｅｎａ等人Ｍ采用图谱嵌入
（Ｇｒａｐｈ


Ｅｍｂｅｄｄｉｎｇ）
的方式进行链接预测
，
为知识图谱中的实体和关系学习高维向量表


示
，
并通过学习得分算法可将原来不存在关系的实体之间建立联系
。
Ｌｖ等人
［４２］


则结合了维基百科的外部知识源
，并根据问题和答案选项
，从不同的两种知识源


中抽取出证据
，
再根据证据构建异构图
，
借助图卷积网络对节点进行表示学习
。


针对问答上下文的非结构数据和与知识图谱结构化数据的语义对齐与融合
，


Ｙａｓｕｎａｇａ等人
［２２］采用了联合推理的思路
，
将问答上下文与筛选出的知识图谱的


子图构建
一张联合图
，
使用图卷积网络的消息传递机制来更新彼此的表示
。


综上所述
，有监督常识问答任务的研究工作可分为语义解析和信息检索两个


主流方向
，而任务的重难点在于从外部知识源中获取与问题语义强相关的常识知


识
，
以及如何有效地整合知识来完成答案的推理
。


１
．２
．２无监督常识问答


有监督常识问答模型通过人工标注数据集来对问答模型进行训练
，然而这样


的数据在真实落地场景中往往获取成本非常高昂
。因此
，为减少对人工标注数据


的依赖
，
研究者们提出了无监督常识问答任务
（ＵｎｓｕｐｅｒｖｉｓｅｄＣｏｍｍｏｎｓｅｎｓｅ


ＱｕｅｓｔｉｏｎＡｎｓｗｅｒｉｎｇ）
。该任务是指在没有标注数据的情况下
，通过自动化的方法


５


北京邮电大学工学硕士学位论文


学习常识知识并回答问题
。
目前
，无监督常识问答任务的研宄点主要集中于生成


式为主的方法
，而其中也包括了当前比较主流的基于提示学习的思路
，下面将进


行详细说明
。


基于生成式的方法是指通过生成问题相关的文本语料
，然后利用这些常识知


识来回答自然语言问题
。这类研究工作主要利用生成模型得到与问题相关的文本


知识
，例如
一些补充背景知识或实体描述
，从而推理获得正确的答案
。早期的研


究采用了自监督思想
，
从非标注数据学习语义特征表示并得到常识知识
，
例如


Ｌｅｗｉｓ等人
则是采用随机遮蔽实体或其他名词短语来生成问题
，并将遮蔽部分


作为问题的答案
，
模型通过填补句子中缺失的单词
，
进而回答自然语言问题
。


Ｂｏｓｓｅｌｕｔ等人
提出的ＤｙｎａＧｅｎ模型使用ＣＯＭＥＴ
［４６
］生成中间推论
，然后用于对


选择进行评分
，但是该工作使用的ＣＯＭＥＴ作为生成器限制了其在通用常识领域


的适用性与迁移能力
。此外
，Ｎｉｕ等人
［９７
］利用生成式预训练模型产生数百个伪答


案
，然后计算候选答案与这些生成的伪答案之间的语义相似度
，并对答案进行投


票
，但是由于没有要提示的任务示例
，这种方案为获得良好的结果可能需要生成


数百个伪答案来辅助决策
。


提示学习
（Ｐｒｏｍｐｔｌｅａｒｎｉｎｇ）作为生成式方法的
一个新兴思路
，
在多种无监


督任务上取得了良好的表现
［４８］
。
其主体思想是通过给下游任务数据添加
一段合


适的提示模版
，使其与预训练任务的形式相近
，从而在可以使得模型只需了解少


量当前下游任务的数据示例便能进行合理地文本生成及预测
，本文所提出的针对


无监督常识问答的研究工作也是基于此分支展开
。其中
，Ｋｈａｓｌｉａｂｉ等人
［４９］等根据


回答问题所需的背景信息创建相关的模版提示
，将多类领域的问答任务转化为文


本生成问题
，从而构建
一般性的问答系统
。Ｂｒｏｗｎ等人
ｔ５叫吏用手工方式创建前缀


提示来处理问答
、翻译和常识推理等多种任务
。而Ｓｈｗａｒｔｚ等人
［９５］提出了
一种自


主对话模式的提示学习策略
，并通过任务特定的模版以生成相关的常识知识
，进


而完成答案的预测
。虽然手工创建提示模版的方式比较简明且直观
，但模版的构


建过程需要积累
一定先验知识
，
即对于不同的下游任务
，
需要结合相关领域的关


键特征以挖掘发现最佳模版类型
。为解决该问题
，Ｌｉ等人
提出
“
ＰｒｅｆｉｘＴｕｎｉｎｇ
”


策略则在输入前添加
一串连续向量
，
同时保持预训练语言模型的参数固定不动
，


仅训练前缀对应的网络参数
，通过自动化生成连续型提示
，设计生成式自动问答


模型
，
由此实现高维嵌入空间中的自动化提示学习
；并且为了使生成答案不受文


本长度限制
，该方法采用自回归的语言模型
ＧＰＴ
－方
７９］作为网络骨架
，并将外部知


识的融合过程以增量训练的方式实现
。
Ｌｈｉ等人
则进
一步提出了
一种更为通用


的两阶段模型
，该方法从任务数据中随机选择
一些样本
，在第
一阶段中借助指令


６


第
一章绪论


模版以提示〇？丁
－３
［８
（）
］模型生成问题相关的知识集合
，并且在第二阶段中将每个生


成的知识和问题拼接到文本匹配模型中进行评分及预测
。


综上所述
，无监督的常识问答任务的研宄工作主要集中于生成式的方法开展
，


利用了自监督学习
、生成模型
、提示学习等模型及策略
。然而
，
如何更有效地从


非结构化的文本数据中获取常识知识
，以及如何设计易迁移的自适应提示模版等


方面仍然面临着较大的问题与挑战
。


１
．３研究内容和工作贡献


本节将针对有监督和无监督常识问答任务
，
结合当前研究的问题要点
，
阐


述本文的主要研究内容与工作贡献
。


１
．３
，
１现存问题


通过对任务研宄现状的梳理
，
可以总结出以下两点亟待解决的问题
：


（
１
）有监督常识问答模型存在的知识覆盖不完备及知识噪声问题


常识问题所覆盖的知识范围普遍比较广泛
（如数字信息
、地理文化
、历史事


件等
）
，模型在进行常识推理时需要大量的证据信息来完成答案预测
，
因而模型


所依赖的外部知识源的覆盖面则尤为重要
。但目前的研究工作往往只采用单
一的


某种结构化或者非结构化的知识源作为推理依据
，其所能够覆盖的知识信息往往


不足以涵盖全部证据信息
，模型在推理过程中可能会因缺失部分关键常识而致使


预测偏差
。


Ｋｎｏｗ
ｌｅｄｇｅＧｒａｐｈｆｒｏｍＣｏｎｃｅｐ
ｔＮｅｔＣｏｎｔｅｘｔｕａ
ｌＤｅｓｃｒｉｐ
ｔｉｏｎｓｆｒｏｍＶＶｉｋｉｔｉｏｎａｒｙ


Ｒｅｓｔａｕｒａｎｔ
：Ｑ
－ＳｔｅａｋＨｏｕｓｅｒｅｆｅｒｓｔｏａｒｅｓｔａｕｒａｎ
ｔｔｈａｔ


：
ｓｐｅｃ
ｉａ
ｌ
ｉｚｅｓｉｎｓｔｅａｋｓａｎｄｃｈｏｐｓ
，ｆｏｕｎｄｍａｉｎｌｙ


秘ｄ
｜
ｉｎＮｏｎｈＡｍｅｒｉｃａ
．


ＳｔａｔｅｓＡ
．［Ｊａｐａｎ］
ｉｓａｎｉｓｌａｎｄｃｏｕｎｔｒｙ
ｉｎＥａｓｔＡｓ
ｉａ
．


＼／
；Ｂ
．Ｒｅｓｔａｕｒａｎｔｉｓａｂｕｓｉｎｅｓｓｔｈａｔ
ｐｒｅｐａｒｅｓａｎｄ


＼／
丨ｓｅｒｖｅｓｆｏｏｄａｎｄｄｒｉｎｋｓｔｏｃｕｓｔｏｍｅｒｓ
．


Ｍｅｘｉｃｏ＼／
：Ｔｅｘａｓ
？Ｃ
．Ｍｅｘｉｃｏｉｓａｃｏｕｎｔｒｙ
ｉｎｔｈｅｓｏｕｔｈｅｒｎ
ｐｏｒｔｉｏｎ


Ｘ＼／Ｊ
：ｏｆ
ＮｏｒｔｈＡｍｅｒ
ｉｃａ
．


／／
：Ｄ
．
［ＵｎｉｔｅｄＳｔａｔｅｓｉｓａｃｏｕｎｔｒｙｐｒ
ｉｍａｃｙ
ｌｏｃａｔｅｄ


■
■ＲｃｉａｉａｉＴｏＳｔｅａｋ
：ｉｎＮｏｒｔｈＡｍｅｒｉｃａ
．


？ＡｔＬｏｃａａｏｎＨｏｕｓｅ
：Ｅ
．
［Ｔｅｘａｓ
ｉｓａｓｔａｔｅｉｎｔｈｅｓｏｕｔｈｃｅｎｔｒａ
ｌｒｅｇ
ｉｏｎｏｆ


—
ＵｓｅｄＦｏｒ
；ｔｈｅＵｎ
ｉｔｅｄＳ
ｔａｔｅｓ
．


Ｑｕｅｓｔ
ｉｏｎ
：Ｗｈｅｒｅｎｅａｒｓｏｕｔｈｏｆ
ｔｈｅＵ
．Ｓ
．ｃａｎ
ｙｏｕｆｍｄａＳｔｅａｋＨｏｕｓｅｓｅｒｖ
ｉｎｇ？


Ａ
．
［ｊａｐａｎＢ
．ＲｅｓｔａｕｒａｎｔＣ
．Ｍｅｘｉｃｏ（）Ｄ
．Ｕｎｔ
ｏｄＳｔ
ｅｔｅｓＥ
．ｒｉｅｘａｓ
ｊ


图
１
－３
有监督常识问答任务中存在的问题


７


北京邮电大学工学硕士学位论文


此外
，
目前的外部知识获取手段通常采用检索与匹配等方式从大型开源知识


库中抽取相关知识
，但这些方法获得的外部知识往往包含大量的噪声
，也就是大


量无关或者是相似但错误的干扰信息
，这些噪声数据作为知识引入并被模型学习


后便会带来严重的性能折损
。
如图
１
－３所示的案例
，
“
ＳｔｅａｋＨｏｕｓｅ
” 虽然具备


“Ｒｅｓｔａｕｒａｎｔ
” 的属性
，但
“Ｒｅｓｔａｕｒａｎｔ
” 和当前的问题的询问指向
“Ｗｈｅｒｅ
” 基本不


相关
；而对于和
“
ＳｔｅａｋＨｏｕｓｅ
” 拥有相同关系
“ＡｔＬｏｃａｔ
ｉｏｎ
” 的其它四个不同选项
，


模型也很难去辨别选项间的区别
，换句话说
，这些在拓扑连接关系上与正确答案


相似的选项其实构成了强千扰噪声
。这些无关或强千扰的冗余噪声对常识推理并


没有帮助
，而由于缺少判断知识源与问题相关性的标注数据集
，故很难去直接训


练
一个用于噪声过滤模型以辅助推理决策
。


（２）无监督常识问答模型缺乏易迁移的自适应方法


无监督常识问答任务不依赖任何标记的下游任务数据
，因此无法对预训练语


言模型进行任务特定的微调操作
。当前的主流研究方法是使用人工精细定义的预


设模版
，指导预训练语言模型生成特定领域的知识或有关问题的补充
，并设计评


分函数来对答案选择进行排序
，例如采用互信息来衡量每个选项与问题的相关程


度
。
由此可见
，
当前的方法产生的知识类型往往较为固定
，在面对新的未知领域


数据时
，
虽然采用更大规模的语言模型可以在
一定程度上减小性能波动
，但该方


式对计算资源的要求较高
，其可操作性不强
，而如果模型借助原有模型则很难再


去成功设计
一个较优的模版方案以生成高质量的跨领域常识知识
，以辅助模型完


成合理预测
。
因此
，
在无监督方向上
，
ＣＱＡ任务在如何生成高质量知识
，
以及


如何灵活地进行自适应的任务迁移等方面均面临着较大的挑战
。


１
．３
．２研究内容


为了解决上述问题
，本文开展了基于有监督和无监督的常识问答研宄
，在设


计和训练模型时
，充分考虑知识源的多样性和有效性
，并探索与问题适配的知识


获取与融合策略
，
以便获取更为全面和准确的常识知识
，并提高模型的推理能力


和性能表现
。


本文的主要研宄内容如下
：


（１）基于知识增强型图对比学习的常识问答算法研究


针对问题
１
，本文进
一步拆解为研究如何解决知识覆盖不全
，
以及研究如何


减少推理过程知识噪声两个子方向
。故而
，本文首先设计了
一种异构知识增强的


方法
，在引入结构化常识知识图谱的基础上
，额外增设了问答对中的所含相关实


体的描述性文本信息以捕捉不同实体间的语义细微差别
，从而能较好地缓解常识


推理过程中的知识覆盖不完备问题
。为缓解知识噪声
，本文设计了
一种自适应采


８


第
一章绪论


样的图对比学习框架
，从拓扑连通度和语义关联性两个角度对图谱的节点与边进


行带权采样
，
并通过正负图例的构建进
一步优化目标损失
。


（２）基于提示型知识生成的常识问答算法研究


针对问题２
，本文设计了
一种通用型的提示型知识生成框架
，其核心思路可


以分为三个主要部分
，首先
，针对问题本身采用了无监督对比学习策略以进行继


续预训练
，从而拉大问题间的差异并学到更好的问题语义表示
；接着
，设计了
一


种通用格式的提示模版指导模型生成问题相关的知识描述
；最后则将生成知识与


候选答案进行匹配推理
，选择最大化预测概率对应的答案作为输出结果
。与之前


方法中采用特定模版或微调生成模型来获取知识及答案的策略不同
，当前框架在


任务迁移环节中仅需根据数据样式进行少量的示例设计
，在
一定程度上也提升了


模型开发及部署的工程效率
。


１
．３
．３贡献及创新点


本文的主要贡献以及创新点总结如下
：


（
１
）在有监督的常识问答任务中
，
本文提出了
一种基于图对比学习的端到


端模型框架
，通过自适应采样原始图的边和节点特征得到增强视图
，并设置批样


本中的难负例以增强图对比学习的训练效果及作用阈
；
并采用多源的知识增强
，


将问答对的实体描述性文本作为补充节点信息插入到当前子图中
，以充分完成知


识的融合与对齐
，
并通过图注意力网络完成图谱的更新与推理
。


（２）在无监督的常识问答任务中
，
本文提出了
一种基于提示型的知识生成


模型框架
，该方法通过无监督对比学习策略对问题语义进行继续预训练
，从而捕


获常识问题间的细微语义差异并学到更好的特征表示
；在知识生成阶段采用具备


指令及示例的提示模版产生
一系列知识描述的集合
；在知识推理阶段通过文本匹


配模型将生成的知识联合候选答案进行匹配推理
，并选择最高置信度的选项及其


支撑知识作为输出结果
。


（３
）在ＣｏｍｍｏｎｓｅｎｓｅＱＡ
［２
］
、
ＯｐｅｎｂｏｏｋＱＡ
［３］
、
以及ＳｏｃｉａｌＩＱＡ
［４
］这三个基准


数据集上设计了完备的实验分析流程（性能指标对比
、模型组件消融
、案例研究
、


可视化及关键参数分析等
）
，并在常识问答的有监督和无监督两个方向分别对比


了目前主要的基线方法
，通过大量的定性与定量实验充分证明了本文所提出的模


型的有效性
。


９


北京邮电大学工学硕士学位论文


１
．４文章组织结构


本论文总共包含五个章节
，章节结构如图
１
－４所示
，每
一章的核心要点如下
：


第
一章
，绪论
。该章节首先概述了常识问答任务的研究背景和应用价值
，并


对相关研究工作进行了详细阐述
；从无监督和有监督两个方向进
一步分析了当前


常识问答任务面临的问题
；
最后对本文的研究内容与工作贡献进行了总结
。


第二章
，相关理论与技术
。该章节详细介绍了本文提出方法所涉及的自然语


言处理及深度学习的基础理论知识
。首先阐述了文本向量化的内容
；随后对知识


表示的主流模型算法进行了梳理
；最后对本文用到的
一些关键的深度学习技术进


行了详尽说明
。


第三章
，基于知识增强的图对比学习常识问答算法研究
。该章节首先介绍了


本文所提出的端到端图对比学习有监督模型
；并从输入侧的知识增强到输出侧的


目标损失
，逐次介绍了各模块的设计思路与技术细节
；最后通过大量的实验分析
，


验证了该方法的可行性与有效性
。


第四章
，基于提示型知识生成的常识问答算法研究
。该章节首先介绍了本文


所提出的基于提示型知识生成的模型框架
，并从对问题的继续预训练到基于模版


提示的知识生成
，再到知识推理与答案选项的预测
，逐次介绍了各功能模块的实


现细节
。
最后借助大量的实验分析
，
证明了该方法的有效性与鲁棒性
。


第五章
，
总结与展望
。该章节对本文的研究工作进行了概括性总结
，并对未


来工作进行了展望
，
在有监督和无监督方向分别提出了进
一步的研宄改进思路
。


１０


第
一章绪论


第
一輋绪论


１


第二簟相关理论


与技术


：１
１
，


？
：｜
挑战１
：单
一知识源
｜
｜
挑战２
：知识子图的
｜
｜
挑战３
：如何生成
｜
挑战４
：如何增强模
：


覆盖不完备躁音信息高质置常识知识型的任务迁移能力


｝

［Ｊ


｜
ｍ鹏軒齡酬赃成
｜


；
；
图对比
５ＳＳ
删
°的常识问荅算法研究


第五章总结与展望


图
１
－４
本文的章节组织结构


１
．５本章小结


本章首先对常识问答的研宄背景与意义
、研宄现状进行了详细地介绍与分析
，


接着从有监督与无监督常识问答两个方向论述了当前研宄工作存在的问题与挑


战
，针对这些问题依次阐述了本文的主要研宄内容与工作重点
，并以这两个研宄


方向作为并行脉络
，
列出了本文各章节的核心要点及组织架构
。


１
１


北京邮电大学工学硕士学位论文


第二章相关理论与技术


２
．
１文本向量化


近年来
，随着自然语言处理技术的快速发展
，常识问答的研究方向也从早期


的复杂语法规则定义转化为了基于数据驱动的方法
，而文本向量化则是其中非常


重要的
一环
，
因为它负责将自然语言文本转换为计算机可以理解的数值化向量
，


并且能够输入到各种深度学习模型中以学到更高层次的语义信息表示
，以更好地


完成多种下游的文本处理与分析任务
。本节将详细介绍文本向量化的发展历程中


最具代表的几类方法
，
即从基于特征工程思想的独热编码
，发展到以概率分布为


核心的神经概率语言模型
，再到后来广泛采用的以Ｗ〇ｒｄ２ＶｅＣ为代表的词嵌入表


示模型
。


２
．
１
．
１独热编码


最基本的文本向量表示方法是独热编码
［５３
］（Ｏｎｅ
－ＨｏｔＥｎｃｏｄｉｎｇ）
，
其编码思


想如图
２
－
１所示
，
它会将每个词都编码为
一个二进制向量
，
向量的长度等于词典


大小
，
向量中每个维度对应
一个二进制位取值
，且每个词对应的向量只有
一个二


进制位为
１
，其它二进制位均设置为０
。比如
，某词表定义为
｛
“
ｒｅｄ
”
，
“
ｇｒｅｅｎ
”
，
“
ｂｌｕｅ
”
｝
，


则
“
ｒｅｄ
”
的独热编码表示为
［
１
，
０
，０
］
，
“
ｇｒｅｅｎ
” 的独热编码表示为
［０
，１
，０
］
，
同理
，


“
ｂ
ｌｕｅ
” 则编码为
［０
，
０
，１
］
。


ｗｌ
！ｗ２ｗ３ｗ４ｗｎ


ｎａｎ


￣
Ｔ
￣
１ｆ〇
￣
］
｜
￣
〇］００


０１
０００


００１
００


０００１０


０００
［
￣
０
￣０


ｏｏ］
￣
〇ｌ
［ｏ
｜Ｔ
＂


图
２
－
１
独热编码的思路说明


１２


第二章
相关理论与技术


独热编码的优势在于表示直观且可消除不同类别之间的大小关系
，避免了因


数值的偏置差异而影响模型的训练效果
。然而
，正是因为该编码方式过于简易与


直接
，
所以可能导致在处理大规模数据时
，
随着词表大小激增
，
所编码的词向量


将会更加趋于稀疏
，
由此会浪费大量的存储空间而造成维度灾难
［５４
］
。
同时
，独热


编码的向量表示在各个维度间都是相互独立的
，模型在学习过程中可能会很容易


捕捉到词间的差异规律而导致过拟合问题
。


２
．
１
．２神经概率语言模型


Ｂｅｎｇ
ｉｏ等人
［：
＞５
］发表的神经概率语言模型
（ＮｅｕｒａｌＰｒｏｂａｂｉ
ｌｉｓｔｉｃＬａｎｇｕａｇｅＭｏｄｅｌ
，


ＮＰＬＭ）可以看成是将祌经网络结构融入概率语言模型的开山之作
，
通过学习大


量的无标注语料数据
，从而能够捕捉到自然语言中的概率统计规律
，进而预测下


一个单词的可能性
。


７
－
ｔｌｉｏｕｔｐｕｔ＝巧叫
＝
？
｜ｃｃｗｒｅｖ＂


｜ｓｏｌｔｍａｘ


（
＃＃
？
：
－４？黎參嫌
】


７７ｘ


／／ｍｏｓｔｃｏｍｐｕｔａ
ｔｉｏｎｈｅｒｅ＼


ｆｉ
＼


＾／
＼


ｆ
＼


／ｉ
ｉ


Ｉ
１
ｔａｉｉｌｉ
１


ｉ
＊ｆ爨＃鬱
：
１


ｆ參
參？
＃
）（０？ｍ］


Ｔａｂｌｅ、
、ＭａｔｉｉｘＣ，


１００广
’
？
＊… …


Ｕ
ｉｃ
ａｃｒｏｓｓｗｏｒｄｓ


ｍｓｍ


ｉｎｄｅｘｆｏｉｕｖ
－ｈ＋
！
ｉｎｄｅｘｆｏｒｉｎｄｅｘｆｏｒｉ
ｔ＞
＿
ｊ


图
２
＿２
神经概率语言模型的网络结构
［５５
］


ＮＰＬＭ模型的网络结构如图
２
－２所示
，
包括了输入层
、
隐藏层
、
以及输出层


这三层神经网络
，同时输入是
一组前面的ｎ
－
１个单词
，输出是当前词的概率分布
，


如下公式所示
：


／（ｗ
ｔ
，ｗ
ｔ
＿１
，
－
－
－
，ｗ
ｔ
＿ｎ＋１）
＝Ｐ（ｗ
ｉ
＝ｉ｜ｃｏｎｔｅｘｔ
）（２
－
１）


１３


北京邮电大学工学硕士学位论文


ＮＰＬＭ模型的训练目标是通过最大化训练集中所有句子的生成概率
，
同时


使用反向传播和梯度下降算法来更新模型的权重和偏置
，并且在训练过程中加入


了正则化技术
，如常见的
Ｌ１和Ｌ２正则化手段
［５６］
，来避免模型过拟合
。其损失


函数如下所示
：


Ｌ＝
ｙ
＇
Ｙｊ
Ｉｏｇ
－－ｗ
ｔ
＿ｎ＋１
；９）＋Ｒ
（９）（２
－２）


ｔ


其中
，
０表示网络的参数
，
代表正则化项
。在训练完成后可得到
一个


词向量矩阵作为中间产物
，
其每行代表了模型学到的对应单词的稠密向量表达
。


至此
，
文本向量化模型开启了语义表征的新阶段
，
之后出现的被广泛应用的


Ｗｏｒｄ２Ｖｅｃ词嵌入模型
［５８］的灵感也来源其中
。


值得注意的是
，ＮＰＬＭ模型虽然避免了独热编码中的数据稀疏和维度灾难问


题
，但是由于模型在训练阶段采用固定局部窗口大小
，
因而无法完全理解文本篇


章的全部上下文信息
，
因而只能处理定长或者有限长度的序列
，无法捕捉长期的


上下文依赖关系
，
如若后续搭配ＲＮＮ等具备循环结构的深层时序模型＠］时
，
仍


然存在较大的性能瓶颈
。


２
．
１
．３词嵌入表示


词嵌入表示是将词汇对应的高维实数域
Ｏｎｅ
－Ｈｏｔ
向量嵌入到
一个低维的连


续向量空间中
，使得每个词都可以用
一个维数较小的密集向量表示
，可通过捕捉


词汇之间的语义和语法关系
，为自然语言处理任务提供有效的文本特征表示。因


而
，词嵌入方法在降低模型的计算复杂度的同时
，还可以让语义相似或相关的词


在向量空间中彼此靠近或方向
一致
，
在众多研究领域得到了广泛的应用
。


其中
，最具代表性方法则是Ｇｏｏｇ
ｌｅ在
２０１３年提出的Ｗｏｒｄ２Ｖｅｃ
［５８
ｌ
，即
“ Ｗｏｒｄ


ｔｏＶｅｃｔｏｒ
” 的简称
，
这是
一种采用浅层神经网络来学习词向量的方法。Ｗｏｒｄ２Ｖｅｃ


网络结构包括了输入层
、投影层和输出层
。输入层和输出层都是Ｏｎｅ
－Ｈｏｔ编码的


向量
，隐藏层是
一个不含激活函数的全连接层
，模型通过最大化词出现的概率来


训练神经网络
，
从而得到各个层之间的权重矩阵
。
按照训练不同的训练目标
，


Ｗｏｒｄ２Ｖｅｃ方法可主要分为两种类型
：连续词袋模型
（ＣＢＯＷ）和跳字模型
（Ｓｋｉｐ
－


Ｇｒａｍ）〇


１４


第二章
相关理论与技术


ＣＢＯＷ模型的核心是利用上下文的词语信息来预测目标词
，
其模型结构如


图
２
－３所示
。


ＩｎｐｕｔＰｒｏ
ｊｅｃｔ
ｉｏｎＯｕｔｐｕｔ


ｗ
（ｔ
－２
）


？
１
）ＳＵＭ


ｗ
（ｔ＋ｌ
）


ｗ（ｔ＋２
）〇ｒ
＾


图
２
－３ＣＢＯＷ模型的网络结构


具体而言
，
对于目标中心词Ｗ（ｔ）
，
输入为其上下文的［
＝
｛
１＾〇
—
〇
，
１＾（亡
－


ｃ＋１）
，
＂
．
，ｗ（ｔ＋ｃ
—
１）
，ｗ（ｔ＋ｃ）
｝通过
Ｏｎｅ
－Ｈｏｔ编码后的求和平均表不
投


影层则利用嵌入矩阵Ｗ
ｅＫＴ
ｘｎ（ｖ为词典大小
，
ｎ为特征维度
）与向量
进行


乘法运算
，从而获得隐藏状态向量
；输出层则将该隐藏向量与对应的权重矩阵Ｑｅ


Ｍ
ｎｘｖ相乘
，并经过
一个
Ｓｏｆｔｍａｘ层获得当前词在词表上的条件概率分布
，
公式如


下所示
，
模型选取最大概率对应的词作为中心词的预测输出
。


ｅｘｐ
（ＱＷ
ＴＸ
ｔ）
ｒ、


Ｐ（ｗ（ｔ）
｜〇＝
＾
．ｅｘｐ＾ＱＷＴＸ
ｉ）
（２
＿３）


Ｓｋｉｐ
－Ｇｒａｍ模型则是
ＣＢＯＷ模型的逆向过程
，
即利用目标词来预测上下文


的词语信息
，
其模型结构如图
２
－４所示
。


ＩｎｐｕｔＰｒｏ
ｊｅｃｔｉｏｎＯｕｔｐｕｔ


峰２
）


二＾
—〇ｗ
（
ｔ
－ｌ
）


＿°

ｗ
（
ｔ＋１
）


〇ｗ
（
ｔ＋２
）


图
２
－４Ｓｋｉｐ
－Ｇｒａｍ模型的网络结构


类似地
，
Ｓｋｉｐ
－Ｇｒａｍ模型的输入为当前词对应的独热编码
，
而上下文窗口大


小设置为ｍ
，
具体过程则是对当前词和窗口大小内的各个目标词进行组合连乘
。


因而
，
Ｓｋｉｐ
－Ｇｒａｍ模型预测的上下文Ｃ对应的条件概率可形式化表不为
：


１５


北京邮电大学工学硕士学位论文


Ｐ（Ｃ
｜ｗ（ｔ））
＝Ｙ＼
Ｐ（ｗ（ｔ＋ｊ）
＼ｗ（ｔ
））（２
－４）


－ｍｓ
ｊ
＜
ｍ
ｔｊ＾ｏ


由于Ｗｏｒｄ２Ｖｅｃ模型中的
Ｓｏｆ
ｔｍａｘ函数需要对词表中的所有词进行指数运算


与归
一化操作
，
因而在词表大小扩增的情况下会使得模型的计算复杂度陡增
。为


了解决此问题
，
出现了类似于层次
Ｓｏｆｔｍａｘ（ＨｉｅｒａｒｃｈｉｃａｌＳｏｆｔｍａｘ）和负采样


（ＮｅｇａｔｉｖｅＳａｍｐ
ｌｉｎｇ）等改进思路＿以优化计算资源
。层次
Ｓｏｆｔｍａｘ的核心思路


是将词表构建为
一棵霍夫曼树
ｔ６
１
］
，每个叶子节点代表
一个词
，每个内部节点代表


一个二分类器
，计算任何
一个词的概率都只需要沿着该二叉树的路径逐次进行逻


辑回归判断
［６２
］
，而不需要遍历词表中的所有词
。负采样策略是将多分类问题转化


为二分类问题
，
即给定
一个中心词和
一个上下文背景词
，判断两者是否共现
，为


了训练这个分类器
，
需要从噪声分布中利用词频进行带权采样以获取负样本
。


然而
，由于Ｗｏｒｄ２Ｖｅｃ模型训练得到的嵌入矩阵是聚焦于词的局部上下文的
，


使得词向量表示缺乏整体的语义联系
，并且无法根据不同的上下文情况进行动态


调整
，
此外Ｗ〇ｒｄ２Ｖｅｃ模型在面对如机器翻译
、机器阅读理解等深层次的自然语


言理解任务时仍然存在较大的性能瓶颈
。后续Ｐｅｎｎｉｎｇｔｏｎ等人提出的Ｇｌｏｖｅ词嵌


入模型
［６３
］（ＧｌｏｂａｌＶｅｃｔｏｒｓｆｏｒＷｏｒｄＲｅｐｒｅｓｅｎｔａｔｉｏｎ）
则在局部上下文窗口的基础


上
，
引入全局的词共现统计信息来学习词向量表示
，从而在
一定程度上缓解了局


部上下文等问题
，
并在多类下游文本任务上取得显著的性能提升
。


２
．２知识表示


知识图谱是目前在常识问答任务中被广泛应用的数据类型
，其包含了丰富的


知识信息
，是
一种由实体
、关系和属性等组成的有向多关系图
，通常以海量的三


元组
，
即
［头实体
，
关系
，
尾实体］
，
的形式进行存储
，
且不同元组间通过实体


关系以建立拓扑连接
，从而能够以结构化的方式描述现实世界中的各种概念和事


物间的相互联系
。知识表示则是利用机器学习的方法来自动化地学习如何将知识


图谱中的实体和关系转换为数值化对象
（如向量或矩阵）的过程
，从而能将离散


的结构化数据映射到连续的稠密向量空间
，
以方便机器有效地存储、理解及应用


这些知识信息
。本节将详细介绍知识表示的两大类代表性方法
，
即以评分函数为


核心的神经网络模型与基于平移不变性的翻译模型
。


１６


第二章相关理论与技术


２
．２
．
１神经网络模型


Ｓｏｃｈｅｉ
？等人
首次提出了基于神经网络的知识表示方法
，
即使用
一个单层


神经网络
（Ｓｉｎｇ
ｌｅＬａｙｅｒＭｏｄｅｌ
，
ＳＬＭ
）来学习实体和关系之间的非线性复杂关系
。


具体地
，
ＳＬＭ将每个三元组
映射为
一个向量
，
并用单层模型的权重矩


阵和激活函数来计算三元组的得分
，以评价两个实体之间存在某个特定关系的可


能性
，
具体表达如下
：


ｆｒ（ｈ
，
ｔ）
＝Ｕｊ
ｔａｎｈ
（ＷｒｌＶｈ＋Ｗｒ
：２Ｖ
ｔ）（２
－５）


其中
，
Ｖ
／ｉ
、ｈ
、Ｗ分别表示头实体
、尾实体
、关系对应的表示向量
，而州
ｒＭ


和
则代表了对应的权重矩阵
。虽然
ＳＬＭ方法可以为非线性操作提升了知识


表示的能力
，但单层网络结构对于实体和关系之间的潜在联系则并不能很好地建


模
。


Ｋｎｏｗ
ｌｅｄｇｅＢａｓｅＷｏｒｄ
ＶｅｃｔｏｒＳｐａｃｅＲｅａｓｏｎｉｎｇａｂｏｕｔＲｅ
ｌａｔｉｏｎｓ



—ｆ梦Ｃｏｎｆ
ｉｄｅｎｃｅｆｏｒ
Ｔｒ
ｉｐ
ｌｅｔ


ｔａ
ｉ
ｌ
一
一偽
ｔ


ｌｅｇ
ｌｅｇ



…
．


綠禱琴響＿＿＾
…
—／
］Ｎｅｕｒａｌ


Ｔｅｎｓｏｒ


ｔｉｇｅｒａｔ＾ｇｅｒＫ［／Ｎｅｔｗｏｒｋ


ｌｅｇ
ｌ
ｉｍｂ／


ＬＺＬ：Ｉ
ｊｎｄ
．
ａ


Ｂｅｎｇａｌｌ
ｉｇｅｔｒ
：＾］＾！— 一ｉ
．
（
Ｂｅｎｇａ
ｌｄＪｅｒ，ｔ
ｅ
ｐａｎ．ｔａｉｌ
）


．
．
．
．
．
．
ＤｏｅｓａＢｅｎｇａ
ｌｔ
ｉｇｅｒｈａｖｅａｔａ
ｉ
ｌ
？


图
２
－５
张量神经网络模型的处理流程
［６４
］


张量神经网络模型
［６４
］（ＮｅｕｒａｌＴｅｎｓｏｒＮｅｔｗｏｒｋ
，ＮＴＮ
）
则在
ＳＬＭ的基础上进


行了改进
，
模型的核心思路如图
２
－５所示
。
ＮＴＮ模型首先引入了额外的新闻语


料库
，
利用实体中的单词信息来初始化实体的表示
；接着
，模型采用了
一个双线


性张量层代替
一个之前的标准线性网络层
，从而将两个实体向量在多个维度上建


立直接关联
，以提高知识图谱嵌入的精度和表达能力
。针对每个三元组
（匕ｒ
，
ｔ
）
，


其评分函数如下所示
：


ＵＫｔ
）
＝Ｕｊ
ｔａｎｈ
（ＶｈＷｒＶ
ｔ＋Ｗｒ
：１Ｖｈ＋Ｗｒ
：２Ｖ
ｔ＋ｂｒ）（２
－６）


其中
，
＼为
一个三阶的张量层对应的权重矩阵与偏置
，
通过增加张量


层的个数
，
ＮＴＮ模型的深度和非线性得到提升
，
进而可以捕获实体之间更复杂


的交互关联
。
然而
，
ＮＴＮ模型的计算复杂度很高
，
需要采用大规模的三元组图


１７


北京邮电大学工学硕士学位论文


谱数据来进行训练
，
对显存资源和时间成本的要求较高
；
此外
，
ＮＴＮ模型的可


扩展性和泛化性较为受限
，基于张量的结构使得模型在稀疏或不完整的知识图谱


上的训练效果不佳
，
需要采用模型剪枝或量化
［６５
］等策略来避免过拟合
。


２
．２
．２翻译模型


翻译模型是目前应用最为广泛的
一类知识表示方法
。受到Ｍｉｋｄｏｖ等人
［５８］发


现的词向量空间存在平移不变现象的启发
，翻译模型则认为头实体加上关系应该


等于尾实体
，在知识图谱的向量空间中对应的就是翻译操作
。
因此
，
该类方法使


用
一个翻译向量来表示每个关系
，
并用
Ｌ
１或
Ｌ２范数来计算头实体在加上对应


关系后
，
与尾实体之间的距离
。


奉


Ｌａｔｅｎｔｅｍｂｅｄｄ
ｉｎｇｓｐａｃｅ


广＞


图
２
－６ＴｒａｎｓＥ模型的核心思想


其中
，最经典的翻译模型则是ＴｒａｎｓＥ
［６６
］
，
如图
２
－６所示
，
该方法基于平移不


变性假设
，在隐向量空间中将实体之间的关系看作相关头尾向量之间的平移来表


示
。
对于每个三元组
（ｈ
，ｒ
，
ｔ
）
，
ＴｒａｎｓＥ模型的翻译思路是尽量能用关系向量


来衡量头实体向量
和尾实体向量Ｒ的平移距离
，
即为如下公式所示
：


Ｖｈ
＋ｕｒ
＾Ｖ
ｔ
（２
－７）


因此
，
ＴｒａｎｓＥ模型的目标则是最小化正确三元组的平移距离
，从而使得海量


知识三元组能在训练过程中逐渐优化表示以满足向量平移的翻译约束
。
因此
，对


于每个三元组
ＴｒａｎｓＥ模型给出的置信度函数／ｒ（
／ｉ
，
ｔ）定义如下
，
即采用


头尾向量的在Ｌ
１或者Ｌ２空间中的平移距离以反映当前三元组的事实可信度
：


ｆｒ（ｈ
，
ｔ）
＝
＼Ｖｈ＋Ｕｒ
－Ｖ
ｔ
＼Ｌｉ／Ｌ２（２
－８）


在
ＴｒａｎｓＥ模型的具体实现环节
，
为了增强知识表示的区分能力
，
ＴｒａｎｓＥ模


型引入了负样例
，
即那些错误的知识三元组
，并通过设置最大间隔来进
一步优化


基于正负样本的学习目标
。
因而
，
优化后的目标函数可定义如下
：


１８


第二章相关理论与技术


Ｌ
＝
２Ｚ
ｍａｘ（０
，ｆｒ（
＿ｈ
，ｔ）＋ｙ
－
ｆｒ
＇
｛ｈ
＇
，ｔ
＇
））（２
－９）


（ｈ
，ｒ
，ｔ）ｅｓ（ｈ
＇
，ｒ
＇
，ｔ
＇
）ｅｓ
－


这里
，
Ｓ表示正确的知识三元组集合
，而Ｓ
－为错误的知识三元组集合
，
ｙ为


该两者得分之间的间隔超参数
，因而模型在训练过程中不断优化损失的过程则是


不断最小化正确三元组的平移距离
，并同时最大化错误三元组的平移距离的过程
。


对于负样例集合５
＿
的构造
，
ＴｍｎｓＥ模型的生成策略是基于知识图谱中真实


三元组进行采样
。具体地
，模型将正样例中的头实体或者尾实体进行随机替换以


生成
一个新的知识三元组
，
并检查它是否存在于当前知识图谱中
。
如果不存在
，


则将其作为负样本
；
如果存在
，
则继续重新生成
。
因而
，该过程可以保证负样例


和正样例具备相同的关系类型
，并且可以有效地区分正确和错误的事实
，可形式


化表示如下
：


Ｓ
￣
＝
Ｋｈ
＇
，ｒ
，ｔ）｝Ｕ｛（／ｉ
，ｒ
＇
，ｔ）｝Ｕ｛（ｈ
，ｒ
，ｔ
＇
）｝（２
－
１０）


与基于高阶张量的神经网络模型相比
，
ＴｒａｎｓＥ模型的参数量较少
，
网络结构


简单且计算复杂度较低
，可以较好地建模实体和关系之间的复杂语义联系
（如具


备对称关系
、
从属关系的实体）
，
并且在大规模稀疏知识图谱上表现出色
。


然而
，
ＴｒａｎｓＥ模型在建模过程中没有考虑到多个实体在知识嵌入空间中竞


争
一个点的情况
，
因此不适合处理多对
一
、
一对多
、
以及多对多等实体关系
。后


续有大量的相关研究基于ＴｒａｎｓＥ的架构进行补充与拓展
：
ＴｒａｎｓＨ
［６７
］将每个关系


抽象成
一个超平面
，然后通过超平面的法向量将实体向量投影到该平面后再进行


平移
，从而可以保证各种类型的实体在不同的超平面上能进行差异化表示
，
以解


决多对多等实体关系表示
；
Ｔｒ
ａｎｓＲ
［６８
］将每个关系视作
一个关系语义空间
，训练时


则将实体向量从实体空间映射到关系空间后再进行平移
，这样可以保证每个关系


有自己独立的语义空间而不受到其它关系的干扰
，从而解决在具备异构实体和多


元关系条件下的表示局限性
；
Ｔｒ
ａｍＤＭｌ则引入了动态投影矩阵的概念
，即根据每


个实体和关系自身的信息来调整投影矩阵的表示
，
并分解矩阵为两个向量相乘
，


在降低模型的计算复杂度的同时
，也能保证每个实体与关系在动态投影后能考虑


到其所含的固有特征信息
，
以获取更为合理的知识嵌入表示
。


２
．３深度学习技术


目前
，
常识问答领域的相关研究工作广泛应用了多种类型的深度学习技术
，


例如采用预训练语言模型以理解常识问题的语义、利用图神经网络和注意力机制


以融合外部知识并增强模型的推理能力
、以及通过提示学习等方式来探索模型在


１９


北京邮电大学工学硕士学位论文


不同领域的常识知识下的适应性与鲁棒性
。本节将对以上提到的几类主流的深度


学习技术展开详细介绍
。


２
．３
．
１注意力机制


注意力机制
（ＡｔｅｎｔｉｏｎＭｅｃｈａｎｉｓｍ
）起源于对人类视觉的研宄
，
在认知和理


解图像的过程中
，人们往往会自动地关注感兴趣或重点聚焦的部分
，而忽略其它


无关的部分
，
并且这些焦点信息对于整个画面形象或风格的传导是至关重要的
。


由此可见
，在人类视觉理解中
，这种带有选择性局部关注的处理机制可以帮助人


类节省认知资源
，
同时提高信息处理效率
。


从人类的视觉研究迁移到机器的深度学习后
，便成为了更具象化的注意力机


制
，其核心思想是让模型能够更好地关注输入数据或特征中的关键部分
，
从而提


高模型的表达能力和性能
，可以帮助模型解决序列数据中的长距离依赖问题
，
同


时也能有效减少信息损失和噪声干扰
。
注意力机制最早是在
２０
１４年应用在计算


机视觉任务上
，例如图像分类
、
目标检测等领域
［７Ｇ
］
。而后逐渐也在自然语言任务


中得到了广泛应用
，
Ｂａｈｄａｎａｕ等人
首次在机器翻译任务中利用注意力机制对


编码器中不同位置的隐藏状态序列进行加权
，以实现源语言和目标语言的动态对


齐
，在避免信息丢失的前提下提高了翻译的准确性和流畅性
。在常识问答任务中
，


注意力机制则可以在推理阶段辅助模型更好地定位和捕捉与问题强关联的常识


知识信息
，
以提高回答的准确性和可靠性
，在比如ＫａｇＮｅｔ
［
１９
］
、ＭＨＧＲＮ
［２
１
ｌ
、
以及


ＱＡ
－ＧＮＮＭ等
一系列高质量的研究工作中均有相关应用
。


；Ｋｅｙ
１Ｋｅｙ２Ｋｅｙ３Ｋｅｙ４
？


－古
！
＇
．
ＨＢＭ


！


］Ｖａ
ｌｕｅｌＶａ
ｌｕｅ２Ｖａ
ｌｕｅ３Ｖａ
ｌｕｅ４
｜


！ＬＪＬＪＵ
｜


［Ｓｏｕｒｃｅ
ｉ


图
２
－７
软注意力机制的通用计算流程


注意力机制可主要分为硬注意力和软注意力
。其中
，硬注意力机制直接从输


入序列中选择
一个或几个向量进行处理
，通常用于处理输入序列中需要进行特定


向量的离散选择的情况
，而在选择的策略则是通过选择函数来决定
，
比如最大向


量、随机选择
、基于概率选择等方法
。
由此可见
，
硬注意力机制要求的计算量较


２０


第二章
相关理论与技术


小
，适合处理长文本序列的离散型任务
，但无法为每个输入向量分配不同的权重
，


其灵活性和表达能力有限
。


软注意力机制作为应用更广泛的
一类方法
，
其核心采用了权重分配的思想
，


通过计算输入向量和输出向量之间的相似度来为每个输入向量分配
一个权重
，用


以反映对输出向量的相关贡献程度
，这些权重在经过归
一化操作后便可用来计算


最终的带权向量表示
。这种机制可以高效地处理连续型的文本序列任务
，并且能


根据需要为不同的输入向量灵活分配不同的权重
，从而提高了模型的表达能力与


可解释性
。


软注意机制的处理流程如图
２
－７所示
，其具体的计算过程可简要概述为以下


三个阶段
。


（
１
）相似度计算
：
对于输入序列中的每个向量
Ｆ
，
此阶段会计算它和


输出向量
Ｕ之间的相似度得分
，而常用的相似度衡量方式包括点积
、线性变换、


拼接、
多层感知机等
，
对应公式表示如下
：


ｒｕ
Ｔｖｄｏｔ


，
ｇｅｎｅｒａｌ


ｓｉｍ
ｉｆ（Ｍ
＞ｖ
ｉ｝）ＶＫ２
ｔ
ｔａｎｈｆＷ
ｘ
ｉｕ
；ｖ
］）ｃｏｎｃａｔ
（２１
１）


ｍ（Ｗ
２ｔａｎｈｉＷ＾ｕ；ｖ］＋ｂｘ）＋ｂ２）ｐｅｒｃｅｐｔｒｏｎ


其中
，
＜７表示非线性层的激活函数
，
％
，撕２为权重矩阵
，
为偏置项
。


（２）权重归
一化
：
此阶段负责将每个输入向量％ｅｈ
对应的相似度得分


转化为归
一化的权重ａ
ｆ
，
一般采用Ｓｏ／ｔｍａｘ函数实现
，
具体公式如下
：


ｇ
Ｓｉｍ
ｉ


ａ
ｔ
＝Ｓｏｆｔｍａｘｉｓｉｒｒｉ
ｉ）
＝
工
己―
（２
－
１２）


（３
）带权向量计算
：
利用权重对所有输入向量进行加权求和
，
即将输入序


列中的每个向量与它对应的权重项相乘
，
并将结果进行相加以得到最终向量
＾；
，


其公式表示如下
：


Ｃ＝
＾
Ｕ
ｉＶ
ｉ
（２
－
１３）


２
．３
．２预训练语言模型


预训练语言模型通过在大规模文本语料上进行无监督的先行训练
，使得模型


能掌握丰富的自然语言的语义和语法规律
，并可应用到各类下游的文本任务中以


提升性能效果
。
其中
，
最具代表性预训练语言模型便是由ＯｐｅｎＡＩ于
２０１８年发


布了大规模生成式的预训练语言模型ＧＰＴ
［７３
］
（Ｇｅｎｅｒａｔ
ｉｖｅＰｒｅｔｒａｉｎｅｄＴｒａｎｓｆｏｒｍｅｒ）


和Ｇｏｏｇ
ｌｅ团队于同年发布的双向文本编码模型
ＢＥＲＴ
［７４
］（Ｂｉｄｉｒｅｃｔ
ｉｏｎａｌＥｎｃｏｄｅｒ


２
１


北京邮电大学工学硕士学位论文


ＲｅｐｒｅｓｅｎｔａｔｉｏｎｓｆｒｏｍＴｒａｎｓｆｏｒｍｅｒｓ
）
，
它们都是基于Ｔｒａｎｓｆｏｒｍｅｒ网络架构
［７３
］
，
分


别在自回归和自编码方向上对模型的训练范式进行了突破性创新
，
在机器翻译
、


文本分类
、序列标注
、
问答系统等多个领域上均表现优异并得到了广泛的应用
。


下面将针对上述模型展开详细介绍
。


（
１
）Ｔｒａｎｓｆｏｒｍｅｒ架构


Ｔｒａｎｓｆｏｒｍｅｒ
？
的模型结构如图
２
－８所示
，
主要由两个部分组成
，
即编码器


（Ｅｎｃｏｄｅｒ
）和解码器
（Ｄｅｃｏｄｅｒ
）
，
其中编码器负责将输入序列编码为隐藏状态


序列
，
解码器则负责从隐藏状态序列中生成输出序列
，
而它们均是由多层组块


（Ｂ
ｌｏｃｋ
）堆叠而成
，每个组块内部也包含了不同的层次结构
［７５
］
。以编码器为例
，


每个组块内部由三种类型的结构组成
，
即多头自注意力层
、残差及规范化层
、全


连接前馈层
。


Ｏｕｔｐｕｔ


Ｐｒｏｂａｂ
ｉ
ｌ
ｉｔ
ｉｅｓ


１Ｓｃｆｌｍａｘ１


ｔ


１ｌｉｎｅａｒ１


（｛
－－
Ｉ


ＡｄｄＳＮｏｒｍ


Ｆ＾ｄ


Ｆｏｒｗａｒｄ


＞—＾


／ｆ
－
－ｎＡｄｄ＆Ｎｏｒｍ


｜Ａｔｔｅｎｔ
ｉｏｎ


Ｆｏｒｗ＾ｄＩ

Ｊ

Ｙ
＊Ｎｘ



＞


．
ｊＡｄｄＮｏｒｍ


ｆ＾
ｄｄ＆
ｊ
ｓｉｏｒｍ
ｊ
？
．■
心丄Ｊ
．
＂
．


｜
Ｍｕ
ｌｔ卜Ｈｅａｄ
ＩＭｕｌｔ
ｉ
－ＨｅａｄＩ


｜Ａｔｔｅｎｔｉｏｎ｜Ａｔｔｅｎｔ
ｓｏｎ｜


１ｔ＞＾＞


＂
ｊ
ＪＶ七


Ｐｏｓ
ｉ
ｔ
ｉｏｎａ
ｌ／Ｃ
￣
＼１１
＿／Ｔ＼Ｐｏｓ
ｉｔ
ｉｏｎａｌ


Ｅｎｃｏｄ
ｉｎｇＷ
￣
ＹＥｎｃｏｄ
ｉｎｇ


ＩｎｐｕｔＯｕｔｐｕｔ


Ｅｍｂｅｄｄ
ｉｎｇＥｍｂｅｄｄ
ｉｎｇ


１！


ＩｎｐｕｔｓＯｕｔｐｕｔｓ


（ｓｈ
ｉｆｔｅｄｒ
ｉｇｈｔ
）


图
２
－８Ｔｒａｎｓｆｏｒｍｅｒ模型的网络结构
［７５
］


多头自注意力层是Ｔｒａｎｓｆｏｒｍｅｒ模型的核心所在
，
其公式定义如下
：


Ａｔｔｅｎｔｉｏｎ（Ｑ
，Ｋ
，Ｖ）
＝Ｓｏｆｔｍａｘ
（＾ｊ
＝
ｊ
Ｖ（２
－
１４）


对于
一个长度为Ｌ的输入序列来说
，
ＱｅＭ
ｉｘｄ
，
Ａ：ｅＭ
ｉｘｒ
ｆ
，
ｖｅＭ
ｉｘｄ分别表


示当前序列对应的查询向量
、
键向量
、
值向量
，
ｄ代表隐藏向量的维度
，
Ａ为键


向量的维度
。
随后模型将上述三类向量拆分为到维度为
多个特征空间中
，
每


２２


第二章
相关理论与技术


个空间上进行独立的自注意力机制以学到多个侧度的语义信息
，最后将各个空间


得到的向量结果在特征维度上进行拼接
，
该过程可形式化表达如下
：


ＭｕｌｉｔＨｅａｄ（Ｑ
，Ｋ，Ｖ）
＝ＣｏｎｃａｔＱｉｅａｄ
．＾
．
．
．
．ｈｅａｄ
－＾Ｗ
０（２
－
１５）


ｈｅａｄ
ｔ
＝Ａｔｔｅｎｔｉｏｎ＾ＱＷ＾
２
，ＫＷ
ｔ
Ｋ
，ＶＷ＾）（２
－
１６）


此处
，
ＧＷｐ６１Ｒ屯ｘｄｆ
ｃ
，
ｅ以及Ｊ＾
ｏ
ｅＲ
ｎｄｐ
＞ｃｄｍ
。
，


Ｃｏｎｍｔ表示多头向量的拼接操作
，
ｎ则表示拆分的特征子空间数
。
由此可见
，在


多头自注意力机制中
，
每个位置的向量都会被用来计算其他位置的注意力权重
，


同时不增加时间复杂度的条件下每个头都可以学习不同的注意力权重
，从而提高


了模型的表达能力
。


残差及规范化层作用在多头自注意力层和全连接前馈层之后
，通过对每个位


置的特征向量进行归
一化处理以避免数据偏移
，并利用残差结构缓解训练过程中


的梯度消失问题
。全连接前馈层则由两个线性变换和
一个激活函数组成
，其中第


一个线性变换将特征向量映射到
一个更高维的空间
，第二个线性变换将其映射回


原始的维度
，
而其中的激活函数则选用
ＲｅＬＵ
［７６］以增强模型的非线性表达能力
。


解码器与编码器的结构基本类似
，但其不同点在于每个组块中额外增加了
一


个掩码的多头自注意力层
，用以学习输入序列和目标序列之间的相关性
，并采用


了矩阵掩码的方式确保模型在任意时间步中只能单向解码
，即只有已生成的输出


序列标记才能用于解码器的自注意力计算
。


相比于ＧＲＵ
［７７＾ＬＳＴＭ
［７８＾传统的时间序列模型
，
Ｔｒａｎｓｆ
ｏｒｍｅｒ网络通过自


注意力机制对全局信息进行交互
，从而能更好地捕捉序列间的深层语义关系
，并


解决长依赖性建模的问题
。此外
，
Ｔｒａｎｓｆｏｒｍｅｒ网络对于输入序列中不同位置的编


码是并行计算的
，
从而在
一定程度上提高了模型的训练效率
。


２３


北京邮电大学工学硕士学位论文


（２
）ＧＰＴ预训练语言模型


ＧＰＴ是由
ＯｐｅｎＡＩ提出的
一种单向语言模型
［７３］
，
其模型结构如图
２
－９所示
，


由多个Ｔｒａｎｓｆｏｒｍｅｒ的解码器单向堆叠而成
，这意味着ＧＰＴ模型只能利用左侧的


上下文信息来生成下
一个词
，而多头自注意力机制则可以捕捉到左侧序列的语义


信息并将这些信息与当前的词进行交互
，
以辅助模型进行高质量的语言生成
。


＼
＼ＯｐｅｎＡＩＧＰＴ


Ｌ＆ｆｅｒ
ｆ４〇ｍ＼了２
？


ＦｅｅｄＲ？ｗａｎ３
…
Ｊ


－
－
＿ｔ＝＾
— Ｊ
！


１
；（
ＴｒｍＸ
Ｔｒｍ
）
？
？
？ｖ
ｒｍ
Ｊｉ


ＰＳＳｉｌＳｉＳｎ
ｊｗ


ｒｒ
＾ｚ—ＥＪＯＤ…
ｌｅｎｎ


Ｔｅｘｔ＆Ｐｏｓ？
ｉｗＥｉｎｂｅｄ
ｌ—


：：
■
■
…


图
２
－９ＧＰＴ模型的网络结构


预训练阶段收集了来自维基百科及浏览网页上的海量通用语料
，
ＧＰＴ模型


在采用无监督训练方法进行下
一个词的预测
，
即将当前词之后的序列屏蔽掉
，使


用
Ｓｏｆ
ｔｍａｘ函数预测文本序列中下
一个词的概率分布
，从而完成预训练的语言建


模
。


Ｃ
ｌａｓｓｉｆ
ｉｃａｔｉｏｎｓｔａｒｔＴｅｘｔＥｘｔｒａｃｔ
－？ＴｒａｎｓｆｏｒｍｅｒＬ
ｉｎｅａｒ


Ｊ




１

ｎ－
．
＿
＿
＿叫…
■
■
，
．


Ｅｎｔａ
ｉ
ｌｍｅｎｔ
ｉｓｔａｒｔＰｒｅｍ
ｉｓｅ
｜Ｄｅｉ
ｓｍＨｙｐｏｔｈｅｓｉｓ

［
ＥｘｔｒａｃｔＴｒａｎｓｆｏｒｍｅｒＬ
ｉｎｅａｒ


ＳｍｒｉＴｅｘｔ１｜Ｄｅ
ｌｉｍＴｅｘｔ２Ｅｘｕａｃｉ

｜
－？Ｔｒａｎｓｆｏｒｍｅｒ
￣
］
＿
＾


Ｓ
ｉｍ
ｉ
ｌａｒｉｔｙ＾＿＿＿＿＿Ｕｎｅａｒ
Ｊ


ｓｔａｒｔＴｅｘｔ２Ｄｅ
ｌ
ｉｍＴｅｘｔ１Ｅｘｔｒａｃｔ
！Ｔｒａｎｓｆｏｒｍｅｒ
一
ＳｔａｒｔＣｏｎｔｅｘｔＤｅ
ｌｉｍＡｎｓｗｅｒ１ＥｘｔｒａａＴｒａｎｓｆｏｒｍｅｒＬ
ｉｎｅａｒ
—


Ｍｕ
ｌ
ｔ
ｉｐ
ｌｅＣｈｏ
ｉｃｅ

｜ｓｔａｒｔＣｏｎｔｅｘｔＤｅ
ｌ
ｉｍＡｎｓｗｅｒ２ＥｘｔｒａｃｔＨＴｒａｎｓｆｏｒｍｅｒ
ｊ
￣
ＨＬ
ｉｎｅａｒ
Ｊ
丨
ｉ
１






＾
ＳｔａｒｔＣｏｎｔｅｘｔＤｅ
ｌ
ｉｍＡｎｓｗｅｒＮＥｘｔｒａａ

ｊ
Ｌ？ＴｒａｎｓｆｏｒｍｅｒＨＬｉｎｅａｒ
－Ｊ


图
２
－
１０ＧＰＴ模型在多种下游任务上的微调方法
［７３
］


２４


第二章
相关理论与技术


在针对下游任务的微调阶段
，
ＧＰＴ模型使用特定任务的标注数据对模型进


行有监督训练
，
如图
２
－
１０所示
，
微调策略仅需对输入数据的格式进行适配于当


前任务的改造
，并将预训练阶段的输出层替换为满足当前任务目标的输出层即可
，


例如在多选式问答任务上
，
模型将输入问题内容与不同的答案选项
，
以
“
［Ｓｔａｒｔ
］


Ｃｏｎｔｅｘｔ［Ｄｅｌｉｍ
］ＡｎｓｗｅｒＮ
［Ｅｘｔｒａｃｔ
］
” 的形式进行组合拼接
，
并将输出层改为答案


预测的线性层
，
即可以实现对问答数据集的微调
。值得注意的是
，在此阶段模型


的参数通常不会被重新初始化
，
而是继续使用预训练结束时的网络参数
。
因此
，


借助预训练阶段学到的大量文本先验
，
ＧＰＴ模型往往只需要使用相对小量级的


数据便能够达到不错的性能效果
。


正因模型强大的生成能力与扩展迁移能力
，
目前
ＧＰＴ模型已在故事生成
、


机器翻译
、对话系统等多项生成任务上刷新了性能指标
。此外
，
ＧＰＴ为后续的具


备更大网络参数和训练数据规模的
（｝！＾
＿２
［７９
］与
〇？丁
－３＿等模型的出现提供了启


发
，这些模型不仅可以在生成领域取得了更好的效果
，
同时还能以非常少的数据


量级完成多种任务方向上的迁移学习
，具备极强的泛化能力
。但由于其单向解码


结构的约束
，模型无法高效地获取上下文的全部语义信息
，
因此在
一些侧重文本


语义挖掘的自然语言理解任务上仍存在
一定的局限
。


（３
）ＢＥＲＴ模型


ＢＥＲＴ模型作为Ｔｒａｎｓｆｏｒｍｅｒ架构的另
一个主流分支
，其结构如图２
－
１
１所示
，


采用了编码器这
一部分进行双向的多层堆叠
［７４
］
，
其内部的多头自注意力机制可


以捕捉到每个词的完整上下文语义
，而最终的输出层则可以将输入序列转换成高


维度的语义特征表示
，
此表示向量可以应用于各种下游的判别式自然语言任务
。


ＴｅｘｔＴａｓｋ
．


Ｒ
＇
ａｄｃｎａｎ＼
ｖ
■
■
■
■■■■＿■
■— —
ｉ


－
：
＼ＢＥＲＴ


卜
了
１
？
？
？


ＦｅｅｄＦｏｒｗａｒｄ＼
Ｔｆｍ
）＼
Ｔｆｍ
｝＼
Ｔｒｍ


丨


ｊ
一
’



：
：￣￣」Ｍ１
ｅＴ［
…
１
Ｅｎ］
：


图
２
－
１
１ＢＥＲＴ模型的网络结构


２５


北京邮电大学工学硕士学位论文


在预训练阶段
，
ＢＥＲＴ模型仍是基于大规模的通用语料数据进行训练
。
如图


２
－
１２的左侧部分所示
，模型的训练目标有两个
，分别为
：
ＭＬＭ
（ＭａｓｋｅｄＬａｎｇｕａｇｅ


Ｍｏｄｅｌｉｎｇ
）和ＮＳＰ（ＮｅｘｔＳｅｎｔｅｎｃｅＰｒｅｄｉｃｔ
ｉｏｎ）
。
具体地
，
在ＭＬＭ任务中
，
模型


随机选择输入文本中的
一些单词进行掩码操作
，
即以
８０％的概率被特殊的掩码


符号
［ＭＡＳＫ
］替换
、
以
１０％的概率被随机替换为词典中的词
、
以
１０％的概率保持


原始词不变
，并利用当前掩码词的上下文序列来对掩码词进行预测恢复
。该阶段


的目的是让ＢＥＲＴ模型学会对输入文本进行双向建模
，即在预测被掩盖单词的同


时
，
也能够从上下文中获取更多信息
。而在ＮＳＰ任务中
，
ＢＥＲＴ模型的目标是预


测两个文本句子之间是否存在上下句的逻辑顺序
，例如因果
、蕴含关系等
，
任务


目标本质为
一个预测
“ 是
” 或
“ 否
” 的二分类任务中
。模型在训练过程中以
一半


的概率保持原始句子对的顺序不变
，以
一半的概率随机从语料库中采样
一个单句


以替换原始句子对中的后
一句
。该阶段的目的是让ＢＥＲＴ模型理解文本之间的关


系
，
进
一步提高模型的泛化能力
。


，
ＮＳＰＭａｓＫＬＭＭ３ＳＫ
ＬＭ＼
／Ｉ＾ＮＵ／
ｆ＾／ＳＱｕＡＤｓｔａｒ
ｔＣｎｔｉＳｐａｎ＇


ＱＱＤ
－
－ＱＤＧ３ＧＤ
… Ｓ
ｔＺＸＤ
…
Ｔ
－
１
－
？ＧＤ


ＢＥＲＴ
：：：
：
：
：
：
：
．
：
：＊
：
．
．
，＾ＢＥＲＴ


ｉｉ
ｉ
ｉ
．
．ｉ
ｉ
ｒ＾ｓＦ＾
Ｉ
［＾
Ｉ
Ｉ
ｉ
…
ｉ
ｊ
ｉ
ｒ
￣
＾
￣
ｉ…
Ｉ
Ｉ


Ｌ＾
ｒ￣〇〇
—〇Ｑ
＾
？—￣〇－〇Ｃｒ
—Ｋｊ
￣－￣Ｏ
￣


ｆ
！ＣＵＳ
；

ｊ

ｆ１
１
．
．
．［
＊
．
．
．ｆ滅
〕
〔料
）卜
，
］ｆ
］
．ｆ
１


５
？
ｉ
１
ＭａｓｋｅｄＳｅｎｔｅｎｃｅ
ＡＭａｓｋｅｄＳｅｎｔ
ｅｎｃｅＢ
Ｑｕｅｓｔ
ｉｏｎ
＊Ｐａｒａｇ
ｒａｐｈ


＼舍
／＼＼＼
含


Ｕｎ
ｉａ＆ｅ
ｉｅｄＳｅｎｔｅｎｃｅ
Ａａｎｄ８Ｐａ
ｉｒ＾
／
，
／Ｖ
、、、、
、
、、
Ｑｕｅｓｔ
ｉｏｎ
Ａｎｓｗｅｒ
Ｐａｉｒ



Ｐｒｅ
－
ｔｒａ
ｉｎ
ｉｎｇＦ
ｉｎｅ
－Ｔｕｎ
ｉｎｇ


图
２
－
１２ＢＥＲＴ模型在多种下游任务上的微调方法
［７４
］


在微调阶段中
，
ＢＥＲＴ模型已经在预训练阶段学习到了大量的文本知识和语


义信息
，
可以轻松应用到如文本分类
、序列标注
、
自动问答等各类自然语言理解


任务中
。
如图
２
－
１２的右侧所示
，
与ＧＰＴ模型的处理过程十分相似
，
ＢＥＲＴ模型


无需改变本身的结构本身
，仅需要根据下游任务的目标自适应地改变输入的数据


格式或进行输出层的替换即可
。


ＢＥＲＴ的成功之处在于它的双向编码结构以及先进的预训练方案
，这些技术


使得ＢＥＲＴ成为了
一种非常通用且强大的语言模型并得到广泛应用
。后续有许多


研究者在ＢＥＲＴ的基础上提出了
一系列相关变体以满足特定场景下的应用需求
，


例如为了提升模型的预测性能与泛化能力
，
在
２０１９年由
ＦａｃｅｂｏｏｋＡＩＲｅｓｅａｒｃｈ


推出的
Ｒｏｂｅｒｔ＃
１坤莫型使用了更多的预训练数据和更长的训练时间
，
在采用动


态掩码的同时也并移除了ＮＳＰ任务
；而后续Ｇｏｏｇ
ｌｅＲｅｓｅａｒｃｈ提出的ＡＬＢＥＲＴ模


２６


第二章相关理论与技术


型
［８２壞
ｌ
ｊ是从模型轻量化的角度出发
，
采用参数共享和跨层参数共享的训练策略
，


在保持与ＢＥＲＴ性能近似的同时缩小模型的网络参数与训练时间
。


总结而言
，
ＢＥＲＴ和ＧＰＴ作为基于Ｔｒａｎｓｆｏｒｍｅｒ结构的预训练语言模型
，
它


们开创了通用语料的预训练与特定语料的微调的训练范式
，不仅可以提高模型的


准确性和泛化能力
，还可以显著减少模型对标注数据的依赖并加速模型的训练过


程
，
是自然语言处理发展历程中重要的里程碑
。


２
．３
．３图神经网络


传统的深度学习模型在处理具备稳定欧式结构的数据
（如图像
、视频
）时表


现良好
。然而
，在处理具备复杂拓扑关系的不规则结构数据时
，例如在社交网络
、


知识图谱
、化学分子结构等
，传统方法可能并不能很好地处理此类信息
，
图神经


网络
（ＧｒａｐｈＮｅｕｒａｌＮｅｔｗｏｒｋ
，ＧＮＮ）
的出现解决了这个问题
［８３
］
，
模型将复杂数据


进行边与节点的抽象
，通过逐层的计算和聚合节点特征进行信息传播与更新
，进


而学习到整个图的表示和特征
。在对数据进行高效建模的同时
，
图神经网络也能


学习到不同结构之间的关系与差异
，
以获取更为细粒度的特征表示
。


具体地
，
图神经网络可以根据消息传递机制的不同可主要划分为五大类
：
图


卷积网络
［８４
］（ＧｒａｐｈＣｏｎｖｏｌｕｔ
ｉｏｎａｌＮｅｔｗｏｒｋ
，ＧＣＮ
）
、
图注意力网络
［８３
］（Ｇｒａｐｈ


ＡｔｅｎｔｉｏｎＮｅｔｗｏｒｋ
：ＧＡＴ
）
、
图米样聚合网络
［８６
］（ＧｒａｐｈＳａｍｐ
ｌｅａｎｄＡｇｇｒｅｇａｔｅ
，


ＧｒａｐｈＳＡＧＥ
）
、
图生成网络
（ＧｒａｐｈＧｅｎｅｒａｔｉｖｅＮｅｔｗｏｒｋ
，ＧＧＮ
）
、
图自编码网


络
［８８
】
（０瓜口
１１八说〇￡１１〇）（１￡１
＇
，０从
）
，
其中以前三类ＧＮＮ结构应用最为广泛
，
后文


将对此展开详细介绍
。


图结构的数据可形式化定义为
Ｇ＝
〇
／
，五）
，其中
１／是图上所有的结点的集合
，


而￡则表示所有边的集合
。


（
１
）ＧＣＮ模型


Ａ
－


－
、、、


Ｃ
，
］＾
吻


—
－
一
一
二多＞
＜＾
｛Ｚ２
；


／—／＼ｈｉｄｄｅｎ＼


祝Ｋｖｅ－＠广
、


？
＂
Ｉ


ｉｎｐｕｔｌａｙｅｒｏｕｔｐｕｔｌａｙｅｒ


图
２
－
１３ＧＣＮ模型的消息传递过程
［８４
］


２７


北京邮电大学工学硕士学位论文


ＧＣＮ模型通过Ｌａｐ
ｌａｃｅ矩阵进行谱分解
，采用傅里叶变换进行特征分解
，并


定义了在谱域上的卷积核运算
［８４
］
。如图
２
－
１３所示
，
ＧＣＮ模型在信息传递的过程


中机制
，利用每个节点及其邻居节点的特征来更新当前节点的信息表示
，并且在


不同层之间共享权重
，
具体地
，
ＧＣＮ模型从
Ｚ层到
丨＋１层中的信息传递过程可


形式化表达如下
：


＝ａ
［
ｄ
￣ＵＤ
￣Ｗ＾Ｗ＾
（２
－
１７）


其中
，
表示当前层的特征矩阵
，
表示当前层的特征维度
，


是当前层的权重矩阵
，
〇
？表示非线性层的激活函数
，
Ｚ为邻接矩阵Ｚ和单位


矩阵
／ｗ的和
，
３为度矩阵
，其对角线反映了图中各节点的度
，其计算公式如下
：


Ｄｕ
＝
＾
Ａ
．ｊ
（２
－
１８）


ｊ


经过重归
一化步骤
（即
得到的增广邻接矩阵
，可以有效防止模型


在反向传播过程中因数值不稳定而产生梯度爆炸或梯度消失的问题
。
由此可见
，


ＧＣＮ模型的卷积操作是基于邻接矩阵和特征矩阵的乘积实现的
，
这个操作可以


将节点之间的信息传递到下
一层
，
从而完成节点的信息聚集与表示更新
。


（２）ＧＡＴ模型


ＧＡＴ模型
［８５
］在〇（＾的基础上引入了注意力机制
，
如图
２
－１４所示
，
模型能


够自适应地学习不同节点之间的关系权重
，而并非赋予所有邻居节点以相同的权


值
，从而能够更精细地捕获节点之间的相互作用
，
以便让关键的邻居节点能以较


大的权重被模型重点关注和学习
。


？


Ｗ／ｉ
，ＷＡ
，（＾
Ｔ
）


图
２
－
１４ＧＡＴ模型的消息传递过程
［ｓ５
］


具体地
，
对于图中的任意节点Ｖ
Ｇ
Ｆ
，
从ＧＡＴ模型的
Ｚ层到
Ｚ＋１层
，
其特


征向量＼的信息聚集与表示更新过程可形式化定义如下
：


２８


第二章
相关理论与技术


ｈ
［
ｖ
ｌ
＋１］＝ａ（＾％，⑴
（２
－
１９）


其中
，
７＼ｒ〇）为节点
ｖ的所有邻居节点集合
，
表示当前层的权重矩阵
，


而
表示节点
ｖ对其邻居节点ｖ的权重关注
，
可通过下式进行计算
：


ａｖｕ
＝Ｓｏｆ
ｔｍａｘ｛ＬｅａｋｙＲｅＬＵ
［ｗｊ
？
［
｜＾
（２
－２０）


此处
，
ＶＫａ表示
一个可学习的权重矩阵
，
｜
｜表示向量拼接
。
由此可见
，
ＧＡＴ模


型在每个节点处计算注意力权重后
，对所有邻居节点的特征加权求和以更新当前


表示
，从而实现自适应的信息聚集
。ＧＡＴ模型相比于ＧＣＮ具备更强的表征能力
，


在处理动态图以及带权图等方面更有优势
。


（３
）ＧｍｐｈＳＡＧＥ模型


Ｚ
．


／严＾尸
、
＇ｐ
＼ＰＡｇｇｒｅｇａｔｏｒ２


／＼
Ａｇｇｒｅｇａｔｏｒ
！


＼丨°


－
－
－



ＳＴＰ／
．
？对邻居节点均匀采样幻ＴＰ２：将邻居节点的特征信息聚合


到当前节点


图
２
－
１５ＧｒａｐｈｓＡＧＥ模型的消息传递过程


ＧＣＮ和ＧＡＴ考虑了所有的邻居节点信息
，
而ＧｒａｐｈＳＡＧＥ模型
［８６测是通过


随机采样邻居节点并聚合邻居节点的特征向量来完成信息传递和特征更新
。


如图
２
－
１５
所示
，
ＧｒａｐｈＳＡＧＥ
的具体步骤可以概括为两步
，
即邻居采样


（ＮｅｉｇｈｂｏｒＳａｍｐ
ｌｉｎｇ
）
和信息聚合
（
ＩｎｆｏｒｍａｔｉｏｎＡｇｇｒｅｇａｔｉｏｎ
）
。
首先
，
邻居米样


表示对每个节点的所有邻居进行均匀的有放回采样
，并将邻居节点的特征向量合


并成
一个新的特征向量
，
以完成当前层和下
一层之间的信息传递
。对于图中的任


意节点
ｖｅ！／
，
该过程可形式化表示如下
：


＾ｖ）
＝Ａｇｇｒｅｇａｔｅ
ｌ
＾
＼ｖｕＧｉＶ（ｕ）
）
（２
－２１）


其中
，
ＪＶＣ
！；
）表示节点
ｐ所有邻居集合的统
一抽样函数
，即若当前节点有
３个


邻居
｛Ｌ２
，３
｝
，则
ｙＶ〇）抽样后的
一种可能输出将会是｛
１
，２
｝或｛３
｝
。由于图中的每个


２９


北京邮电大学工学硕士学位论文


节点的度是不相同的
，
Ｇｒ
ａｐｈＳＡＧＥ针对每个节点采样固定数量的邻居以便进行


统
一化的批量处理。


接着
，信息聚合则是在每个节点的邻居节点特征向量合并之后
，对新的特征


向量进行聚合操作以将采样得到的节点信息聚合到中心节点
，从而得到更新的节


点特征向量表示
，
具体公式如下所示
：


ｈ＾
＋１）
＝
ｃｒ
［
ｗｍ
－Ｆ（ｈ＾
＼
＼
ｈ＾＼ｖ
）））
（２
－２２）


此处
，
表示当前层的权重矩阵
，Ｆ表示聚合操作
，可以采用ＭｅａｒｕＭａｘ、


ＬＳＴＭ
［７８］等方式实现
，经过多层模型推理后得到的各节点向量表示便可供多种下


游任务使用
。
由此可见
，
ＧｒａｐｈＳＡＧＥ模型通过随机采样邻居节点的方式极大减


少了模型的计算量
，并通过局部特征的合并高效地利用节点之间的相互关系和共


享信息
，
能够很容易地扩展到复杂的图结构中
，在诸如节点分类
、
图谱表示等任


务中取得良好的性能表现
。


综上所述
，
图神经网络与传统网络相比
，更适合处理复杂的关系网数据
，它


能通过消息聚合与节点更新的方式捕捉局部和全局的潜在特征
，以学到节点和图


的特征表示
。在动态迭代的过程中
，
图神经网络能够借助注意力机制
、随机采样


等策略来降低图中固有噪声和异常值的影响
，具有较强的鲁棒性与泛化能力
，在


舆情分析、
生物信息学
、
智能交通等领域具备广泛的应用前景
。


２
．３
．４提示学习


提示学习是近年来在自然语言处理领域中兴起的
一种新型学习方法
［８９］
，
其


产生背景可以追溯到
２０２０年ＧＰＴ
－３模型
［８Ｑ］的发布
，该模型能够借助
一些关键字


或词的提示以自动生成各种相关的文本
，包括文章
、故事、代码等
。这激发了学


者们对基于提示信息的语言模型的研究热潮
，针对少样本与零样本等领域
，设计


某些特定的关键词或句子以提示模型进行相关的文本预测
，从而提高模型可解释


性和迁移能力
。


目前主流的提示学习框架为基于人工模版的方法
，例如
ＯｐｅｎＡＩ团队分布的


ＧＰＴ
－３模型關以及ＦａｂｉｏＰｅｔｒｏｎｉ等人＿提出的
ＬＡＭＡ框架
，其核心思想是将下


游的任务借助于特定的模版和规则来完成数据格式及预测目标的转换
，从而弥合


下游任务与预训练任务之间的训练鸿沟
。
图
２
－
１６展示了
一个基于人工模版的提


示学习示例
，此处的原始任务为单句分类
，而当前的模版设计为
“
ｘｘｘ
［ＳＥＰ］Ｗｈａｔ


ｉｓｎｅｗｓａｂｏｕｔ？
［ＭＡＳＫ
］
”
，
通过将当前单句填充进模版后
，
模型的训练目标便从


单句分类转化为了ＭＬＭ任务
，
进而减弱了和预训练阶段之间的训练偏差
。


３０


第二章
相关理论与技术


Ｌａｂｅｌ
：


＿＾Ｔ
￣￣￣￣￣
－
Ｓｃｉｅｎｃｅ
（Ｖ）


，严、


－Ｂｕｓｉｎｅｓｓ


，Ｐｒｅｄｉｃｔ
－Ｓｐｏｒｔｓ


：ＶＪ


Ｉ
＇：＾


Ｊ


ｉＰｒｅｔｒａｉｅｎｄＬａｎｇｕａｇｅＭｏｄｅｌ


＼


、ｉ
／


ｉ


广
ｉ
＼


Ｔｈｅ１ＡＵｄｏｗｎｇｎａｄｅＰｌｕｔｏａｓａｄｗａｒｆ
ｐ
ｌａｎｅｔ
［ＳＥＰ
］Ｗｈａｔｉｓ


ｎｅｗｓａｂｏｕｔ？
［ＭＡＳＫ
］


ｖ
／


图
２
－
１６
基于人工模版的提示学习示例


由此可见
，基于人工模版的提示学习方法在设计过程中需要考虑到任务的需


求和限制
，并且能够指导模型生成符合语法规则和预测目标的相关文本
。该类方


法的优势在于其简单易懂
，
易于实现和解释
，适用于
一些需要对生成结果进行精


细控制的特定场景
。但也正是因为模版需要进行大量的人工设计和调试
，
因而模


型的迁移能力受限
，
难以应对多种复杂的任务场景
。


因此
，后续有部分研究聚焦于自动化的模版生成
，其核心思想不再依赖人工


设计的模版或规则
，而是通过优化某种指标函数
（如ＢＬＥＵ
、ＲＯＵＧＥ
、ＭＥＴＥＯＲ
、


困惑度等评估指标
）来辅助模型完成高质量模版生成
。
例如
Ｓｃｈｉｃｋ等人
将文


本指令与示例相结合
，并自动确定模版的离散型标签词
；
Ｚｈｏｎｇ等人
［９２
］的研究工


作则选择将模版信息连续化
，不再关心模版的具体形态
，而是与输入
一并映射到


共同的连续向量空间后
，在固定其它的网络参数前提下
，仅对模版的相关参数进


行优化
；
Ｚｈｅｎｇ等人
［９３］提出的
Ｐ
－ｍｎｉｎｇ策略则在连续空间中自动搜索模版提示
，


通过提示模版和语言模型同时进行参数更新来增强模型的理解能力
，
并借助


ＬＳＴＭ
网络
以增强词嵌入的相关性和加速模型收敛
。


总而言之
，提示学习通过引入具备指向性的提示模版信息
，
从而控制和影响


模型生成的文本
，并使得输出的可解释性和可控性更强
；
同时在
一些缺乏大量数


据标签的场景下
，提示学习可以结合大规模的预训练语言模型
，
以较小的成本代


价在模版中构建领域相关的先验知识来弥补数据稀缺问题
，从而能在
一定程度上


提高模型的性能表现和泛化能力
。


３１


北京邮电大学工学硕士学位论文


２
．４本章小结


本章主要介绍了常识问答研宄所涉及的相关理论与技术
。本章首先阐述了文


本向量化的发展历程
，紧接着从知识表示的角度说明了目前的主要思路
，
即平移


距离和基于描述的模型这两个方向
，最后介绍了目前的常识问答研究中所涉及的


深度学习技术
，
包括注意力机制
、预训练语言模型
、
图神经网络
、
以及提示学习


等方向
。为本文开展无监督和有监督的常识问答研宄的展开奠定了理论与技术基


础
。


３２


第三章基于知识增强型图对比学习的常识问答算法研宄


第三章基于知识增强型图对比学习的常识问答算法研究


３
．
１引言


有监督常识问答任务旨在为常识相关的自然语言问题选择答案
，并且训练样


本均带有正确标签信息
，
由于常识所覆盖的知识面非常广泛
，模型需要结合相关


的外部知识进行推理并选出最佳答案
，比较考验模型的语义理解与常识推断能力
。


最近的
一些研宄工作通过联合图神经网络以建模结构化的知识图谱
，从而为答案


预测提供可解释的推理证据链
。然而
，单
一的知识源结构通常不足以涵盖足够的


常识信息
，并且通过实体检索与语义相关度匹配等方式获取知识的过程中也会引


入大量的噪声
，因而模型在如何进行有效的知识融合与合理的知识去噪方面仍然


存在挑战
。
针对以上问题
，
本文提出了
一种基于知识增强的图对比学习模型


（ＫｎｏｗｌｅｄｇｅＥｎｈａｎｃｅｄＧｒａｐｈＣｏｎｔｒａｓｔｉｖｅＬｅａｒ
ｎｉｎｇ
，
ＫＥ
－ＧＣＬ
）应用于有监督的常


识问答任务
，在利用知识图谱的基础上额外引入了问答对所含的实体描述性文本


作为非结构化的知识增强
，从而拓展常识推理的知识覆盖面
；并且设计了
一种端


到端的图对比学习框架以缓解知识噪声
，根据图谱中节点与边的重要性进行自适


应地带权采样
，
进而构建正负图例以优化对比损失
。


这里给出有监督常识问答任务的形式化定义
：
给定当前的常识问题…
以及


含有Ｍ个选项组成的候选答案集Ｃ
，
即
Ｃ＝
｛ｑ
，
Ｃ
２
，
．
．
．
，ｃＭ｝，模型需要结合相关的


外部知识ＪＣ进行推理
，
并从候选集合Ｃ中选择出与问题的意图最匹配的
一个选


项作为最终的预测结果
。


３
．２基于知识增强的图对比学习模型


３
．２
．
１模型总览


图
３
－
１展示了所提出的ＫＥ
－ＧＣＬ模型的整体结构
，主要包含了多源知识表示


与融合模块
（３
．２
．２节
）
、
自适应图增强模块
（３
．２
．３节）
、知识图推理模块
（３
．２
．４


３３


北京邮电大学工学硕士学位论文


节
）
、答案预测模块
（３
．２
．５节
）
、
正负图例构造与目标函数
（３
．２
．６节
）这五个


部分
。


Ｌ〇
ｊ
—
Ｉ
！ＶＷｈ？ｔ
ｉＫＷ？ｗｈ
ｏｆ
ｔｈｅＵ
Ｊｉ
ｃａｒ
－
ｙｏｏ
ｔｔ
ｅｄ
ｔｂｏｎｗ
？〇ｖ？ｇ
＇

；


—＾
■
；
？

ｊ
ａｉ
〇Ｍｒ％ｋ？
ｔ
／
Ｉ
；


一｜ｆＤ


Ｓ
）ｔ？ｃＶ
ｂｔ
？？ｗ
ｒ
ｅ
ｆｅｒ＊
ｔｏ
？ｒ
ｃｔｔ
ａｗｗａ
ｔａ
ｉ
＜ｐｖｏ？ｈＭ＞
ｗ
Ｍ？ａｌ
５
＊？＊
Ｊ
－
？Ｓｆ？＿ｔ
－
；
ｃＭｍ

Ｓｉｎｗｄ
（
ｉｏｍＫ
ｍ
＞４？ＢｉＡｍｅｎｏ
ｉ二ｊ
＇ｘ
一
言
（ｐｒｏｆ
ｉｒ
ｔ）


Ｕ
ｉｋ
ｔ
ｉＭＭｉｎ
？

ｉｉ
Ｍｔ
ｉＫ？

？ｖ
？ａｘｗ〇＼

ｉｅＯｔｅ
ｕ
？ｕ
ｉ
ｉ？ｅｆａ
ｔＫＫＢｔ
ｉｅＭ
Ｘｔｔ
ｆｆ
ｔ
Ａａｗｗ
ｔ
ｉａ
１
．
．
．
５Ａ
＇
孩



＊
？
Ｊ：：
：
：
：
：：
．
．
．
．
．
．ＭＵ
＊／
’＇
．
＊
＇
１
＾

二、
墨


＇


？
■Ｕｄ
’
：分
一１
，气
一唆
丨細
咖一衫


？
丨
ｉ
Ｉ
．
．Ｃ
＜
、。
一
６获
ｃ
■
一
Ｐ
；ｅ
．
〇
．ＡＵ？ａａｏｏ

，
．＞、


／＼／
：＾ｖ
－
ｎ
；

ｒ
＊
＊
ｒ
－１
：
＊
？？
＜
－
■
‘ 
：
；
ｒｚｎ


＾
？？？？
＜
；ｋＭＨｔＭｃｔ
ＦｗｉＭ〇
＊
＾
＾￣￣￣￣


Ｃ—Ｎｔｔ
？
＼一
，
、
、一
、丄ｒ
ｃＣ
；
，
：ｒ
－＾
＇
：
；


：
ｉ＿＿
？＾
ｉＴ◎６３Ｕ＿１
：
＾＾
－Ｖ
Ｃｌ
；
Ｉ
到
ｉ？ｌ〇

ｉ
（２７７７７＾
１


ｒＭＭ
，
／
，
？
＇
：
、
－
、
＿
＞ｙ
、
—，
ｌ
？
－８＊ｉｃｋ
．
ＸＭ
（Ｎ
－
Ｊ
）
？


■

．
、
— ＾
，
？
．＾
－
？— 一—
？
？


Ｋ？ｒｍ
ｉｅ４ｔ
＜
Ｋｔｐｍｍｔ
ｆ
ｉｍａ
＆Ｋｓｔｎ？Ｍ？ｖ
Ｆｍａｏａ
｜
Ｏｎｐｈ
Ａｔｅｍｍｓｕｉｍ
ｊ
Ｏｔａｔ＊
ＨｅａＭ
ｔｕｔ
ｅｓ
Ｊ
Ａａ？？？ｒ
Ｐｒｒ
ｆｌｔｔ
？＊ｉ
ｉ
Ｓｉ
Ｌｏｗ
ＣｖａｐＭａ
ｔ
ｉｏｆ
ｌ


图
３
－
１ＫＥ
－ＧＣＬ模型的结构总览


具体而言
，多源知识表示与融合模块首先将给定的问答对
（ｑ
，Ｃ３）与基于维基


词典的实体描述
进行文本拼接
，
以获得上下文的语义特征表示
同


时
，
该模块从
Ｃｏｎｃｅｐ
ｔＮｅｔ
中提取知识子图＾以获取嵌入表达
Ｇ３
，
并将上下文


特征表示ｇ作为节点插入到图中
，
通过注意力机制获得多元知识融合图


在自适应图增强模块中
，模型通过掩码不重要的节点特征与丢弃不相关的边


得到增强视图＆
，
而节点与边的采样概率由拓扑连通性和上下文相关性决定
。


知识图推理模块先采用散播连接边进行初始化
，并采用图注意力网络
（ＧＡＴ）


实现知识的聚合与更新
，原始的知识融合图及其增强视图以共享参数的方式执行


相同的推理
，
并获得对应的最终图谱表示ｚｆ
。


答案预测模块则采用门控计算上下文特征４与图谱特征＃的权重
，
进而输


出模型的置信度评分
，
并选择具有最高得分的选项
ｃ３作为预测答案
。


最后
，在正负图例构造与目标函数中
，对比学习的正图例来自当前样本的正


确选项相关的増强视图
６３
，
而负例则由非正确选项对应的增强视图和当前训练


批次中其它样本所含的图表示共同组成
；
因此
，模型最终通过标签分类损失


与图对比学习损失￡〇＿进行端到端地联合目标优化
。


３
．２
．２多源知识表示与融合模块


（
１
）
文本编码器


受到Ｘｕ等人
的研究工作启发
，此处将维基词典作为非结构化的文本知识


源
，
从而可获得当前问答对
（ｑ
．ｑ）
中蕴含的实体信息描述
，
这两个文本描述分


别表示为４
和
＾＾
。
紧接着
，
文本编码器使用预训练语言模型来提取语义特征


表示
，
当前问答对
（ｑ
，
与其对应的实体描述文本进行拼接
，
并通过前向编码


过程表示为上下文语义特征向量
ｚｆ
，
公式表示如下
。
其中
，
／ｃ代表了文本侧的


预训练语言模型
。


３４


第三章基于知识增强型图对比学习的常识问答算法研宄


蚌
＝／ｃ（ｑ？ｑ十
ｄ
ｑ？ｄＣ
ｉ）（３
－１）


（２）
图谱嵌入


从
Ｃｏｎｃｅｐ
ｔＮｅｔ结构化知识库中可以检索得到针对当前问答对
（ｇ，ｑ）
的知


识子图
仏
，
这是与
ｇ和
ｑ
中的主题实体相关的两跳范围内节点与边所构成的


子图
。对于节点嵌入
，此处使用
Ｆｅｎｇ等人
［２１
］提供的实体嵌入参数进行初始化
，


它将预训练语言模型应用于
Ｃｏｎｃｅｐ
ｔＮｅｔ中的所有三元组
，
同时为每个实体获得


一个池化表示
。
经过初始化后的节点嵌入表示如下
：


＝
Ｋｉ
＊ｖ
ｉ
，２
，
－
＞ｖ
ｉ
ｉｎ］
（３
－２）


对于边的嵌入表示
，首先将当前边的类型
ｒｓｔ
以及边所连接的两个节点的类


型ｕｓ
，
构成的三元组编码为
一个独热向量
［ｕｓ？ｒｓｔ十叫］
；
然后采用两层


ＭＬＰ结构将独热向量映射为了当前边的初始化嵌入表示
。
经过初始化后的ｍ条


有向边的嵌入表示如下
：


Ｅ
ｉ
＝
（３
－３）


因此
，
当前知识子图对应的图谱嵌入可表示为
＆
＝
（％氏）
，＾
［
１
，财
］
，
其


具备
ｎ个节点和
ｍ条有向边
。


（３
）知识融合


此部分通过节点插入和注意力机制进行多源的知识融合
。具体来说
，上下文


的语义特征向量被视为
一个新节点插入到当前的知识子图中
，插入后的节点嵌入


更新为
Ｋ
＝
，
新插入的上下文节点定义如下
，
其中
／Ｍ表示
一


个两层的ＭＬＰ。


＇〇
＝
（３
－４）


对于相关边的构建
，
这里将与当前问答对
（ｑ，ｑ）中的实体有直接拓扑关系


的实体与新插入的上下文节点
建立连接
，
由此图谱边数增加到
话
。
边的嵌


入表示更新为
眾
＝
｛ｅｕ
，ｅ
ｉ
，２
，
…
，
随后
，
模块采用注意力机制来进
一步融


合文本知识
，上下文表示４被视作查询
（Ｑｕｅｒｙ）作用于所有图谱节点
。对于每


个节点
融合后的表示定义如下
，
其中映射
／Ｑ表示ＭＬＰ模块
，％为


节点的嵌入维度
。


＾
ｉ
，ｑ
＝ｓｏｆｔｍａｘ
（
’Ｑ（
＾
＿Ｍ
）
？
ｖ
Ｕｑ（３
－５）


因此
，
多源知识的融合图可表示为
￥
＝
（％
ｉ￡［
１
，Ｍ
］
，
其具备
ｎ＋１


个节点和
衍＋
１
条边
。


３５


北京邮电大学工学硕士学位论文


３
．２
．３自适应图增强模块


针对多源知识的融合图
￥
，
当前模块通过自适应地进行节点特征掩码和边


丢弃以构建对应的增强视图
首先定义子图中的每个节点
ｅＲ
的影响为


如下公式所示
：


Ｐｖ
ｉ
，ｑ
＝
／ｒ（＾
ｉ
，ａ））（３
－６）


这里
／ｒ（
？
）和丸
（Ｙ）分别代表拓扑连通性和上下文相关性
。拓扑连通性是


通过
ＰａｇｅＲａｎｋ算法进行计算
，该算法会从拓扑结构的角度对那些具有更多入度


的节点进行加权
；而上下文相关性则通过当前节点与上下文语义表示ｚｆ之间的


语义相似性衡量
，公式定义如下
，其中
０（Ｙ）表示两个向量之间的佘弦相似度
。


ｆＲ｛ｖ
ｉ
，ｑ
，ｚ＾
＾
ｅ（ｖＵｑ
，ｚｆ）
＝
（３
－７）


此处存在
一个直观性假设
，即那些经常在有影响力的节点中有高频表达的维


度应该是更重要的
。
因此
，
对于芡中的任何节点
，
维度
ｄ
的重要性权重计算公


式定义如下
，
并将权重数值
归
一化后作为是否对维度进行掩码的概率
。


Ｙｉ
，ｄ
＝ｌ〇ｇ＾
＼ｖ［ｄ＼
＼
■
Ｐｙ（３
－８）


ｖ£ Ｖ
ｔ


对于Ｓ
；
中的每条边
ｅ
，其重要性取决于当前边所指向的尾节点珥
的重要


性权重
，
如下公式所示
。
类似节点的操作
，
权重Ｉ在归
一化后便作为边
ｅ
的


丢弃概率
。


Ｖｅ＝ｌ〇ｇＰｔＪ
￡
（３
－９）


因此
，
该模块基于归
一化的概率进行伯努利釆样
，
并获得
￥对应的增强视


图
￥＝炙）
。


３
．２
＿４知识图推理模块


此模块中
，融合知识子图
Ｓ
；及其增强视图
￥会并行执行相同的前向操作
，


故此处以前者５
；为例
，
通过关联边的散射和基于注意力的节点聚合进行知识推


理
。具体来说
，为了利用图谱中带有关系类型的边信息
，对于每个节点
圬
ｅＫ
，


通过对指向节点
珥
的那些边进行散射操作可获得对应节点的初始隐藏表示


具体公式如下所示
，
这里
表示圬节点的所有邻居
。


ｈｆ
＝
＾
ｅｓ￡＋ｖ
ｔ
（３
－１０）


ＳＥＫｔ


３６


第三章基于知识增强型图对比学习的常识问答算法研究


之后
，
当前模块使用
ＧＡＴ
网络用以传播和聚合节点的知识信息
。对于每
一


层
图神经网络
，
巧节点的隐藏表示的更新公式如下
，
其中
ｔ／是注意


力头的数量
，
Ｍ／ｕ是对应的线性投影矩阵
，
丨
丨是多个注意力头的拼接操作
。


Ｚ＜ｔｗ
）
ｊ
（３
－１
１）


＼
ｓｅＪＶｉｕ｛ｔ｝／


其中
，ａ品
的具体计算方式如下
，表示将信息从
圬传播到
圬的注意力权重
，


ｙ品
则反映了两个节点之间的相关性
，
计算方式如下所示
。其中
，
％则表示第


￡层的线性投影矩阵
。


＃
＝
ｙ
（３
－
１２）


ｙｓ
ｕ
ｔ
＝ＬｅａｋｙＲｅＬＵ（Ｗｊ［ｈ＾
，ｈｆ］）（３
－１３）


在经过
Ｌ层的图推理之后
，
模块选择将上下文节点的隐藏状态作为整个知


识子图的池化表示
，
即
：


ｚｆ
＝Ｐｏｏｌ（ｈ
（
０
Ｌ
＼ｈ
［
Ｌ
＼
—
，ｈ＾
＝（３
－１４）


３
．２
．５答案预测模块


对于答案选项
ｑ
，当前模块使用相应的上下文语义特征表示
和知识子图


表示
ｚｆ来计算其成为正确答案的概率
，
计算公式如下
：


Ｐｉｃ
ｉ
＼
ｑ
）
＝
ｇｌＱ
［ｚ
ｃ
ｉＷ
ｃ
，ｚ＾｝（３
－１５）


ｇｔ
＝ｓｏｆｔｍａｘ
（ＭＬＰ（［ｚ＾，ｚｆ］）＾
（３
－１６）


其中
，
门控仍
用以控制上下文文本特征和图谱特征的重要性权重
。对于答


案预测
，在得到所有候选项的预测概率后
，模型将具有最高分数的选项视为最合


理的答案
。


３
．２
．６正负图例构造与目标函数


（
１
）损失目标函数


ＫＥ
－ＧＣＬ采用端到端的有监督训练
，
其总目标损失
Ｘ
ｉ
■定义如下
，
其中


和
分别表示答案预测分类损失和图对比学习损失
，而
入则是
一个用于调节


图对比学习影响的超参数
，
同时也反映了对预测分类目标的惩罚程度
。


Ｌｔ
＝Ｌｃｅ＋ＸＬｃｌ
（３
－
１７）


３７


北京邮电大学工学硕士学位论文


答案预测分类损失使用标准的交叉熵以优化正确答案ｑ的预测概率
，
其公


式定义如下
，
而图对比损失则将结合正负图例的构造在下文中详细展开
。


一
＝
一岣
＿
，
（＾」）
（３
－
１８）


ＳＣ
ｉ
，ｅｅｅｘｐ（Ｐ（
ｃｖ
｜ｑ
））


（２
）
正负图例的构建及图对比学习损失


图对比学习损失定义如下公式所示
，
该损失是基于
ＩｎｆｏＮＣＥ函数
构建
，


并在此基础上增设了难负例项以增强对比学习的效果
。直观来说
，对于
一个给定


的问题
，候选选项相关的知识子图及其增强视图通常会具备较多相同的节点和边
，


这种相似性会导致模型难以分辨
。
因此
，
当前子图的难负例存在两个来源
，
一个


是那些带有错误选项的问答对相关的知识子图
；另
一个则是它们相应的增强视图
。


ｒ？
１
￣
－ｋ？ｏｖ
￣
ｒ…


Ｉ
一
＊？＊？？
？￣￣
Ｉ
Ｃｕ
￣￣
｜

？＞
－￣
叫
、０Ｏ
＼
＊＊
丨

ｌａｂｅ＊Ｖ？
１


ｍｎ
／
〇
￣
ｏｖ
－＾－
ｆ
￣


：Ｗ＼
＇
ｖ
ｖ


酒ｅｙ
＇
隸Ｃ＼
＇
＇
－
－
－Ｋｉｌ）
、


Ｇ／
；
：
｜
！ｌ＼＼ｎＧ
＇


洛ｍ
！
，Ｕ
＇－＾ｅｅ
；


／
／
ｉ
｜＼＼
—
—


／／／
？
＼＼


ｃｏ＞
ｙ
ｉｊ＼＼


／＼


＿＿Ｃ＆＞＾、
＿＿


ｒ

１


Ｑｊ
■
■
■？
－？
？
一
？

〇
Ｇｆ？ｐ
ｆ
？Ｅｎｃｏｄｅｒｆｏｒ
ＫＧＰｏｓｔ
ｉｖｏＰａ
ｉｆＨ？ｕＪＮｅｇａｔ
ｉｖｅＰａｉｒＣｏｍｍｏｎ
Ｎ？ｇｏｔ
ｒｖｅＰａ
ｉｒＤａｌ９Ｆ
ｌｏｗ
Ｉ


ｖ








－
ｉ


图
３
－２ＫＥ
－ＧＣＬ模型中的正
负图例构建


图
３
－２直观展示了在
一个训练批次中
，
ｑ：所代表的样本在图对比学习中对


应的正例
（Ｐｏｓｉｔｉｖｅ）
、难负例
（ＨａｒｄＮｅｇａｔｉｖｅ）
、以及普通负例
（ＣｏｍｍｏｎＮｅｇａｔｉｖｅ
）
。


该过程可形式化总结如下
，
对于
一个问题
ｑ及其正确答案
ｑ
，
其对应的知识


子图表示为
ｚｆ
，其增强视图表示为
ｚｆ
，故正例集合定义为
：Ｐ＝
ｉｚｆ
，ｚｆｊ
；同时
，


难负例集合定义为焉＾
＝
｛＞／
：＞＃
｜〇
１
１
｛＞／
：７矣
〖
｝
；
此外
，
除了当前的实例样本
，


当前批次的其它所有问答对相关的知识子图与增强视图均被视为普通负例
，则普


通负例集合可定义为
＝其中
４
的两个版本表示当前训练批次


３８


第三章基于知识增强型图对比学习的常识问答算法研宄


中第
ｆ
ｃ个问答对相关的图表示
，
并且
ｆ
ｅｅ
［
ｉ
，Ｍ（
ＪＶ
－
ｉ
）］
，
ｉｖ表示训练批次的大


小
。
最终的负例集合则是由难负例和普通负例所共同构成
。


由此
，
正例
、
难负例
、
普通难负例对应的贡献项可表示如下
：


ＴＰ
＝ｅｘｐ
ｅ
（＾
＇ｚ＾／ｒ
（３
－２０）


ｔ＾ｈ
＝
ｉ
ｅｘｐ
９＾
，ｚ＾／Ｔ
＋ｅｘｐ
９＾
ｌＺ＾／ｒ（３
－２１）


ＪＶ（Ｍ
－ｌ）Ｎ（Ｍ
－１）
＿


Ｔ＾Ｃ
＝
ｚ
ｅｘｐ＾Ｍ）／ｒ
＋Ｉ
ｅｘｐ
９＾
＇（３
－２２）


ｋ＝ｌｆ
ｅ＝ｌ


３
．３实验分析


３
．３
．
１数据集与评估指标


有监督常识问答任务米用
ＣｏｍｍｏｎｓｅｎｓｅＱＡ和
ＯｐｅｎｂｏｏｋＱＡ作为基准评测


数据集
，两者的数据类型均为多选式问答
。其中
，
ＣｏｍｍｏｎｓｅｎｓｅＱＡ由
Ｔａｌｍｏｒ等


人
［２
］构建
，
其中每个样本均来自
Ｃｏｎｃｅｐ
ｔＮｅｔ的
一个源实体及其密切相关的子图
，


通过人工众包的方式进行常识问题的编写
，并为每个问题设计
一个正确答案和四


个干扰选项
。该数据集的部分示例如图
３
－３所示
，模型需要结合多方面的常识知


识来进行排除相关干扰项并选出正确答案
，是
一个比较考验模型的语义理解及知


识推理能力数据集
。


Ｗｈａｔｉｓｉｔｃａｌｌｅｄｗｈｅｎ
ｙｏｕｓｂｗｆ
ｙｃｏｏｋｕｓｉｎｇａ
ｇｒｉｌｌ？


Ｓａｎ＾
ｌｅ１


Ａ
．ｂａｃｋｙａｒｄＢ．ｒｅｓｔａｕｒａｎｔＣ．ｃｒｏｃｈｐｏｔＤ
．ｎｅｉｇｈｂｏｒ
＇
ｓｈｏｕｓｅｂａｒｂｅｑｕｅ


Ｓｏｍｅｔｈｉｎｇｔｈａｔｈａｓａｌｏｎｇａｎｄ
ｓｈａｒｐｂｌａｄｅｉｓａ？


Ａ．ｃｕｐＢ．ｆ
ｉｍＣｘｋｕｎｋＱ＾ｓｗｏｒｄ＾Ｅ．ｓｐａｔｕｌａ


Ｗｈｏｗａｓｔｈｅｈｅｅｄｏｆｔｈｅｂｒａｎｃｈ
ｙｅｌｌｉｎｇａｔ？


Ｓａｍｐ
ｌｅ３


Ａ．ｏｗｎｅｒＢ．ｆｏｏｔＱ．ｓｕｂｏｒｄｉｎａｔｅ＾Ｄ．ｂａｓｅＥ，
ｐｏｉｎｔ


图
３
－３ＣｏｍｍｏｎｓｅｎｓｅＱＡ数据集的真实样本示例


ＯｐｅｎｂｏｏｋＱＡ则是由Ｍｉｈａｙ
ｌｏｖ等人
［３］提出的基础科学常识问答数据集
，其中


包含了开放域中的
１
，３２６个科学事实
（例如金属导电
、地球自转等
）
，其数据示


例如图
３
－４所示
，每个科学事实都与问题对应的知识直接相关
。较早的部分常识


３９


北京邮电大学工学硕士学位论文


问答数据集侧重于语言理解
，
往往会提供了回答问题所需的相关背景知识
，
而


ＯｐｅｎｂｏｏｋＱＡ则鼓励后续研究获取更加广泛的外源性知识
，
以辅助模型对问题的


．ＷｈｉｃｈｏｆｔｈｅｓｅｗｏｕｌｄＩ过ｔｈｅｍｏｓｔｈｅａｔｔｒａｖｅｌｔｈｒｏｕｇｈ？


＆Ｃｈｏｉｃｅｓ
Ａ
，ａｎｅｗｐａｉｒｏｆｊｅａｍ１ａｓｔｅｅｌ
ｓｐｏｏｎｉｎｃａｆ
ｅｔｅｒｉａｓｆ


Ｃ．ａｃｏＵｏｎｃａｎｄｙａｔａｓｔｏｒｅＤ．ａｃａｂｉｎｋｈｉｎｃｏｔｔｏｎｂａｔ


Ｓｃｉｅｎｃｅ


ＦａｃｔＭｅｔａｌｉｓａｔｈｅｒｍａｌｃｏｎｄｕｃｔｏｒ


Ｃｏｍｍｏｎ
ＳｔｅｅＵｓｍａｄｅｏｆｍｅｔａｌ．


Ｋｎｏｗｌｅｄｇｅ／ｆ
ｅａ／ｔｒａｖｅｌｓｔｈｒｏｕｇｈａｔｈｅｒｍａｌｃｏｎｄｕｃｔｏｒ


表达与主旨进行更深层次的推理。


图
３
－４ＯｐｅｎｂｏｏｋＱＡ数据集的真实样本示例


在ＣｏｍｍｏｎｓｅｎｓｅＱＡ数据集上
，
由于官方测试集的标签并未公开
，仅用于模


型提交后的自动评测
，
故此处参考ＫａｇＮｅｔ方法的数据划分
［
１９］
，
基于相关的内部


验证集
（ＩＨｄｅｖ）和内部测试集
（ＩＨｔｅｓｔ）开展模型评测
；而在ＯｐｅｎｂｏｏｋＱＡ数据


集上
，模型评测则是基于官方测试集开展
。这两个数据集的统计结果如表
３
－１所


示
，为了定量评估模型的性能
，
当前任务使用答案预测准确率
（Ａｃｃｕｒａｃｙ，ＡＣＣ）


作为指标
，训练过程中仅保证训练集的数据标签可见
，而其它集合的数据标签仅


用于模型的性能评估
。


表
３
－
１ＣｏｍｍｏｎｓｅｓｅＱＡ和ＯｐｅｎｂｏｏｋＱＡ数据集的统计结果


数据集＃训练集＃测试集＃验证集＃选项


ＣｏｍｍｏｎｓｅｎｓｅＱＡ９
，７４１１
＾２２１１
，１４０５


ＯｐｅｎＢｏｏｋＱＡ４
，９５７５００５００４


３
．３
．２对比基线设置


本节选取了近年在有监督常识问答任务上的多种研究方法作为对比基线
，并


对各自的研究工作进行简要地概括说明
：


（
１
）原始基线直接采用Ｒ〇ＢＥＲＴａ
－Ｌａｒｇｅ
［８
１
］
（ｗ／ｏＫＧ）进行训练
，
即在不引入


任何外部知识的前提下仅拼接问题和选项并作为输入
，
使用特殊标记
［ＣＬＳ
］
的


隐藏状态来进行答案预测
。


４０


第三章基于知识增强型图对比学习的常识问答算法研宄


（２）ＲＮ
［１７
］则是将知识图视为连接实体对的
一组关系路径
，在常识问答中适


用于基于路径的多关系图编码
，
即使用ＭＬＰ对知识三元组
（单跳路径
）进行编


码
，
并推理路径中的所有三元组嵌入向量组合以获取图表示
。


（３
）ＲＧＣＮＭ为了解决现实知识库中的数据多关系问题
，在ＧＣＮ结构的基


础上
，通过执行特定于关系的聚合来使其适用于多关系图
。然而
，
该模型并不能


区分不同邻居或关系类型的重要性
，因此很难为模型的关系推理过程做出明确的


可行性解释
。


（４）ＫａｇＮｅｔ
［
１９］则在ＲＮ的基础上增设了ＬＳＴＭ结构来完成从问题实体到答


案实体的所有非退化路径的编码
，并通过注意力机制聚合所有关系路径嵌入
，从


而实现对多跳路径关系的建模
。虽然该方法在关系路径上应用注意机制可以提供


较好的可解释性
，
但这也使得模型难以进
一步扩展
。


（５）Ｇｃ〇ｎＡｔｔｎ
［２Ｇ
］最开始应用于文本推理任务
（ＮＬＩ）
，充分利用开放知识库


中包含各类型推理信息以
，基于前提和假设进行相关的知识检索
，并通过语言模


型和结构化图模型的组合以提升任务性能
。


（６）ＭＨＧＲＮ
［２
１＾ＲＧＣＮ基础上提出了多跳图关系网络
，
利用结构化的关


系注意力机制
，将从单跳消息传递扩展为具有结构化关系的多跳消息传递
，在获


得路径级的推理和建模能力的同时
，
在保证了模型的高度可扩展性
。


（７）ＱＡ
－ＧＮＮ
［２２＾Ｕ是采用语言模型和图神经网络结合的方案
，先利用预训练


语言模型计算图谱节点与问答对上下文之间的相关性
，然后采用基于结点与关系


边感知的ＧＡＴ
网络实现问答对上下文和知识图谱的融合与联合推理
。


在后续的实验中
，除了原始基线Ｒ〇ＢＥＲＴａ
－Ｌａｒｇｅ
（ｗ／ｏＫＧ
）的指标来自手工复


现的测试结果
，其它对比方法所展示的结果均来自对应论文中所报道的官方性能


指标
。


４１


北京邮电大学工学硕士学位论文


３
．３
．３实验参数设置


本章提出的ＫＥ
－ＧＣＬ模型采用开源深度学习框架Ｐｙｔｏｒ
ｃｈ进行模型搭建与训


练
，并在２个常识问答基准数据集上进行了大量的定性和定量实验
。实验环境为


搭载了２张１２ＧＢＮＶＩＤＩＡＧｅＦｏｒｃｅＲＴＸ２０８０Ｔｉ显卡的Ｕｂｕｎｔｕ１６
．０４服务器
。


为了进行公平的性能比较
，
此处使用相同的预训练语言模型作为网络主干
，


ＲｏＢＥＲＴａ
－Ｌａｒｇｅ用
于ＣｏｍｍｏｎｓｅｎｓｅＱＡ数据集
，
ＲｏＢＥＲＴａ
－Ｌａｒｇｅ和


ＡｒｉｓｔｏＲｏＢＥＲＴａ用于
ＯｐｅｎＢｏｏｋＱＡ数据集
，
且均基于Ｈｕｇｇ
ｉｎｇｆａｃｅ框架部署实


现
。检索知识子图的限制跳数
（ＨｏｐＳｉｚｅ）设置为
２
。文本编码器的上下文特征


维度设置为
１０２４
，
而最大序列长度设置为
１２８
；
图推理模块中节点与边的嵌入


维度％设置为
２００
，
而
ＧＡＴ
的推理层数设置为
３
。
为了控制图对比学习中的


变量约束影响
，
按照前人经验将超参数
；Ｉ、
Ｐ和
ｔ分别设置为
０
．１
、
２和
０
．２
。


文本编码器的学习率设置为
１０
－５
，
而其它模型组件的学习率设置为
１０
－３
，


Ｄｒｏｐｏｕｔ的数值设置为
０
．２
。模型在训练过程中采用
ＲＡｄａｍ作为模型优化器
，


将Ｅｐｏｃｈ设置为
３０并采用早停策略（ＥａｒｌｙＳｔｏｐｐ
ｉｎｇ）进行端到端的有监督训练
，


一个完整的训练周期大约需要
１３小时
。
此外
，
梯度累积策略
（Ｍｉｎｉ
－ｂａｔｃｈＳｉｚｅ
＝


２）用于训练过程中以实现ＢａｔｃｈＳｉＺｅ
＝
１２８的等效训练效果
。
最终的主实验结果


是使用不同的随机种子进行五次运行后的平均指标
。


３
．３
．４实验结果分析


表
３
－２和表
３
－３分别展示了ＫＥ
－ＧＣＬ在两个数据集
ＣｏｍｍｏｎｓｅｎｓｅＱＡ和


ＯｐｅｎＢｏｏｋＱＡ上的主要实验结果
。可以看到
，ＫＥ
－ＧＣＬ模型在这两个数据集上均


优于其它的对比基线模型
，并且几乎所有具备知识感知理解的模型都比原始的预


训练语言模型的性能更好
，这也证实了常识问答任务引入外部知识的必要性与有


效性
。
与之前的最优模型
ＱＡ
－ＧＮＮ相比
，
ＫＥ
－ＧＣＬ模型在
ＣｏｍｍｏｎｓｅｎｓｅＱＡ


数据集上取得
１
．３５％和
１
．０８％
的性能提升
，
而在
ＯｐｅｎＢｏｏｋＱＡ数据集上则提


升了０
．８３％和
０
．６４％
，
这充分证明了ＫＥ
－ＧＣＬ模型采用非结构化的上下文描述


进行知识增强的可行性
。值得
一提的是
，
在
ＯｐｅｎＢｏｏｋＱＡ数据集中
，
将模型的


基础架构从
ＲｏＢＥＲＴａ
－Ｌａｒｇｅ更改为
ＡｒｉｓｔｏＲｏＢＥＲＴａ后
，
所有方法的性能均得


到明显提升
，原因在于ＡｒｉｓｔｏＲｏＢＥＲＴａ会为每个问题提供相应的基础类科学事实


文本
，
但ＫＥ
－ＧＣＬ模型的性能仍然优于其它的基线方法
，
这表明ＫＥ
－ＧＣＬ模型


具备
一定的性能鲁棒性
，能够有效地整合外源的科学事实以做出更好的答案预测
。


４２


第三章基于知识增强型图对比学习的常识问答算法研究


表
３
－２ＫＥ
＿ＧＣＬ模型在ＣｏｍｍｏｎｓｅｓｅＱＡ数据集上的性能指标对比



模型方法
ＩＨｄｅｖＩＨｔｅｓｔ


ＲｏＢＥＲＴａ
－Ｌａｒｇｅ
（ｗ／ｏＫＧ）７０
．７０（±０
．３２）６７
．２３（±０
．４８）


＋ＲＮ（１
－ｈｏｐ）７４
．５７（±０
．９
１）６９
．０８（±０
．２１）


＋ＲＮ
（２
－ｈｏｐ）７３
．６５（±３
．０９）６９
．５９（±３
．８０）


＋ＲＧＣＮ７２
．６９（±０
．１９）６８
．４１（±０
．６６）


＋ＧｃｏｎＡｔｔｎ７２
．６１（±０
．３９）６８
．５９（±０
．９６）


＋ＫａｇＮｅｔ７３
．４７（±０
．２２）６９
．０１（±０
．７６）


＋ＭＨＧＲＮ７４
．４５（±０
．
１０）７１
．１
１（±０
．８
１
）


＋ＱＡ
－ＧＮＮ７６
．５４
（±０
．２１
）７３
．４１（±０
．９２）


＋ＫＥ
－ＧＣＬ（Ｏｕｒｓ）７７＾９（±０３７）７４．４９（±０３１）


表
３
－３ＫＥ
－ＧＣＬ在ＯｐｅｎＢｏｏｋ（ｊＡ数据集上的性能指标对比


模型方法ＲｏＢＥＲＴａ
－ＬａｒｇｅＡｒ
ｉｓｔ
ｏＲｏＢＥＲＴａ


Ｆｉｎｅ
－ｔｕｎｅｄＬＭｓ（ｗ／ｏＫＧ）６４
．８０（±２
．３７
）６４
．８０（±２
．３７）


＋ＲＮ
（１
－ｈｏｐ）６３
．６５（±２
．３
１）７３
．１５（± １
．６３）


＋ＲＮ
（２
－ｈｏｐ）６５
．２０（± １
，１８）７５３５（± １
．３９）


＋ＲＧＣＮ６２
．４５（± １
．５７
）７４
．６０（±２
．５３）


＋ＧｃｏｎＡｔｔｎ６４
．７５（士１
．４８）７１
．８０（± １
．２１）


＋ＭＨＧＲＮ６６
．８５（± １
．１９
）８０
．６０（±０
．００）


＋
ＱＡ
－ＧＮＮ６７
．８０（±２
．７５
）８２
．７７（± １
．５６）


＋ＫＥ
－ＧＣＬ（Ｏｕｒｓ）６８．６３（±１２４）８３
．４１（± １
．９３）


３
．３
．５消融实验


为了进
一步研究
ＫＥ
－ＧＣＬ模型中各个功能组件的有效性
，
本节基于


ＣｏｍｍｏｎｓｅｎｓｅＱＡ数据集
（
ＩＨｄｅｖ）进行了详细的组件消融
，
其结果报告如表
３
－４


所示
。


４３


北京邮电大学工学硕士学位论文


表
３
－４ＫＥ
－ＧＣＬ模型的组件消融实验结果


消融组件具体设置模型指标


ＯｕｒＫＥ
－ＧＣＬＦｕＵＭｏｄｕｌｅｓ７７＾９


ｗ／ｏＣｏｎｃｅｐｔＮｅｔ７４
．４８（３
．４１１
）


ＫｎｏｗｌｅｄｇｅＥｎｈａｎｃｅｍｅｎｔｗ／ｏＷｉｋｔｉｏｎａｒｙ７５
．１４
（２
．７５１
）


ｗ／ｏＥｉｔｈｅｒ７０
．０３（７
．８６ｉ
）


ｗ／ｏＴＣ７７
．２７（０
．６２ｉ
）


ＧｒａｐｈＡｕｇｍｅｎｔａｔｉｏｎｗ／ｏＣＲ７７
．０２（０
．８７ｉ）


ｗ／ｏＥｉｔｈｅｒ７６
．７８（ｌ
．ｌｌｉ）


ｗ／ｏＥｄｇｅＳｃａｔｔｅｒ７７５５（０
．３４ｉ
）


ＧｒａｐｈＡｕｇｍｅｎｔａｔｉｏｎｗ／ｏＧＡＴ７７
．１４（０
．７５ｉ
）


ｗ／ｏＥｉｔｈｅｒ７５
．９６（１
．９３Ｊ
．
）


ｗ／ｏＬｃｌ７５
．６６
（２
．２３丄）


ＧｒａｐｈＣｏｎｔｒａｓｔｉｖｅＬｅａｒｎｉｎｇ


ｗ／ｏＨａｒｄＮｅｇ７７１２（０
．７７ｉ）


（
１
）多元知识表示模块


此模版针对知识增强
（ＫｎｏｗｌｅｄｇｅＥｎｈａｎｃｅｍｅｎｔ）进行消融
。具体地
，在移除


了来自
ＣｏｎｃｅｐｔＮｅｔ中的知识子图后
（
“
＼￥／〇
（１；〇１＾＾
１价１：
”
）
，模型性能下降了
３
．４１％。


此外
，
当进
一步去除来自维基词典的实体上下文描述
（
“ｗ／ｏＥｉｔｈｅｒ
”
）后
，性能显


著下降了７
．８６％
，
由此可见这些描述性文本对原始子图是有较大助益的
，这些描


述性的信息了赋予了结构化知识图谱以上下文理解能力
，从而有利于模型扩大知


识覆盖范围并执行更高效地推理
。


（２）
自适应图增强模块


此模块针对自适应釆样策略进行消融
，
“ｗ／ｏＴＣ
”和
“ｗ／ｏＣＲ
”分别代表在计算


节点和边的采样权重时去除拓扑连通性
（Ｔｏｐｏｌｏｇ
ｉｃａｌＣｏｎｎｅｃｔｉｖｉｔｙ）和上下文相关


性
（ＣｏｎｔｅｘｔｕａｌＲｅｌｅｖａｎｃｅ）
。可以观察到上下文相关性略微比拓扑连通性的影响


更大
，
一个可能的原因是那些具备上下文高相关度的节点更有可能与正确答案关


联紧密
。此外
，在
“ｗ／ｏＥｉｔｈｅｒ
” 的消融设置中
，模型对原始子图中的节点和边进行


随机采样
，
此方式导致了１
．
１
１％
的性能下降
，
进而验证了所提出的自适应采样


策略的有效性。


（３
）知识图推理模块


４４


第三章基于知识增强型图对比学习的常识问答算法研宄


此模块针对图推理模块中的边散射和
ＧＡＴ组件进行消融
，分别表示为
“ｗ／ｏ


ＥｄｇｅＳｃａｔｅｒ
”和
“ｗ／ｏＧＡＴ
”
。在
“ｗ／ｏＧＡＴ
”和
“ｗ／ｏＥｄｇｅＳｃａｔｅｒ
”消融设置中
，
模型


性能略有
０
．３％？０
．７％的下降
。然而
，当整个推理模块被移除时
（
“ｗ／ｏＥｉｔｈｅｒ
”
）
，


性能下降了１
．９３％
，这比简单地将这两个影响叠加的效果更严重
，从而说明了图


推理模块可以通过有效聚合来自节点和边的有价值信息来获取高质量的图表示
。


（４）正负图例构造与目标函数


此模块主要针对图对比学习进行消融
。
“ｗ／ｏ意味着从总损失函数中移


除图对比学习损失
，而
“ｗ／ｏＨａｒｄＮｅｇ
”则表示取消难负例的权重设置
。在
“ｗ／ｏ


设置中
，性能明显下降了２
．２３％
。从而证实了图对比学习策略对模型的重要作用
，


因为它可以通过对比正图对和负图来区分正确答案和其他干扰因素
。
此外
，
在


“
ｗ／〇ＨａｒｄＮｅｇ
” 设置下
，
模型指标也随之下降了０
．７７％
，
这也表明难负例的设置


可以为图对比学习的训练效果带来进
一步的性能增益
。


由此可见
，
ＫＥ
－ＧＣＬ
中的每个组件都具备
一定的功能
，并且进行完整组合后


能有助于提升模型在有监督常识问答任务上的性能表现
。


３
．３
．６案例分析


表
３
－５展示了四个从
ＣｏｍｍｏｎｓｅｎｓｅＱＡ数据集中选择的案例
，
这里横向对


比了三个不同的模型
（原始基线ＲｏＢＥＲＴａ
－Ｌａｒｇｅ
、
ＱＡ
－ＧＮＮ
、
ＫＥ
－ＧＣＬ
）
的预测


结果
。前两个案例展示了ＫＥ
－ＧＣＬ预测正确的情况
，在第
一个示例中
，
只有
ＫＥ
－


ＧＣＬ模型在做出了正确的预测
；
而原始基线ＲｏＢＥＲＴａ
－Ｌａｒｇｅ因为没有加入任何


相关的外部知识
，
从而产生了
一个几乎均匀分布的预测结果
；
而ＱＡ
－ＧＮＮ则借


助
Ｃｏｎｃｅｐ
ｔＮｅｔ提供的结构化知识
，模型为选项
Ａ（
ｉ４
ｔｒｅｅ
”
）给出了高置信度的错


误预测
，
原因可能是
“
ｔｒｅｅ
”实体与
“
ｔａｌｌｇｒａｓｓ
”实体在对应的子图中关系更密切
。


在第二个例子中
，
虽然ＫＥ
－ＧＣＬ和ＱＡ
－ＧＮＮ模型都给出了正确的预测
，但ＫＥ
－


ＧＣＬ进
一步扩大了正确选项
Ｄ（
“
ｓａｆｅｔｙ
”
）和强干扰选项
Ｃ（
“
ｂｅｉｎｇ
ｓａｆｅ
”
）之间


的差距
，
这表明图对比学习策略的确有助于模型捕捉相似选择间的细微差别
。


而其余两个示例则是ＫＥ
－ＧＣＬ的失败预测
，
在第三个示例中只有
ＱＡ
－ＧＮＮ


模型回答正确
，
可能原因在于
ＫＥ
－ＧＣＬ模型并没能够充分理解问题中蕴涵的否


定关系词
“
ｎｏ
ｌｏｎｇｅｒ
”
，
这种理解偏差导致了模型给出了选项
Ｂ（
“
ｈｏｍｅ
”
）的错误


预测
。在最后
一个例子中
，这三个模型都未能做出正确预测
，
常识语义为餐馆数


量与选项之间的隐藏关联性
，
这表明
ＫＥ
－ＧＣＬ模型对于数字区间的理解及常识


关系的转化上仍有改进空间
。


４５



＂
北京邮电大学工学硕士学位论文


表
３
－５ＫＥ
－ＧＣＬ模型在ＣｏｍｍｏｎｓｅｎｓｅＱＡ数据集上的预測案例分析


案例
：
问题／候选答案模型方法预测结果预测得分


Ｑ１
：Ｗｈｅｒｅｃａｎｙｏｕｆ
ｉｎｄａｓｎａｋｅｉｎ


ＪＲｏＢＥＲＴａ
－ＬａｒｇｅＢ
．（ｘ
）［０
，１８
，０
＊２４
，０
．１８
，０
．２１
，０
．１９］


ｔａｌｌｇｒａｓｓ？


ＱＡ
－ＧＮＮＡ
．
（ｘ）［０．４１
，０
．１１
，０
．０９
，０
．１８
，０
．２１］


Ａ
．ｔｒｅｅＢ
．ｉｎａｊａｒＣ
．
ｐｅｔｓｈｏｐｓＤ
．
，


ＫＥ
－ＧＣＬＤ
．（ｖ
）［０
．１３
，０
．１
１
，０
．１７
，０
．４６
，０
．１３］


ｆ
ｉｅｌｄＥ
．ｔｒｏｐ
ｉｃａｌｆｏｒｅｓｔ


Ｑ２
：Ｓａｌｌｙｗａｓａｆ
ｒａｉｄｏｆｄａｎｇｅｒａｎｄ


】ＢＲｏＢＥＲＴａ
－ＬａｒｇｅＣ
．
（ｘ
）［０
．１４
，０
．０１
，０．４５
，０
．２３
，０
．１７］


ａｌｗａｙｓｄｏｕｂｌｅｃｈｅｃｋｅｄｗｈａｔ？
．


ＪＱＡ
－ＧＮＮＤ
．（Ｖ）［０
．０９
，０
．１２
，０
．３４
，０
．４４
，０
＿００］


Ａ
．ｆｉｇｈｔｅｎｅｍｙＢ
．ｓｅｃｕｒｅＣ
．ｂｅｉｎｇ
，


６ＫＥ
－ＧＣＬＤ
．（＼
ｆ
）［０
．０２
，０
．０２
，０
．２８
，０
．６７
，０
．０１］


ｓａｆｅＤ
．ｓａｆｅｔｙＥ
．ｖｉｃｉｎｉｔｙ


Ｑ３
：Ｗｈａｔｋｉｎｄｏｆｓｅｒｖｉｃｅｉｓｍｙ


ｂｏｄｙａ
ｐａｒｔｏｆｗｈｅｎＦｍｎｏｌｏｎｇｅｒＲｏＢＥＲＴａ
－ＬａｒｇｅＡ
．（ｘ）［０＾３
，０
．０２
，０
．００
，０
．０８
，０
．０７］


ｈｅｒｅ？ＱＡ
－ＧＮＮＤ
．
（Ｖ）［０
．０５
，０
．２１
，０
．０８
，０３９
，０
．２７］


Ａ
．ｂｏｄｙｃａｍＢ
．ｈｏｍｅＣ
．ｃｏｆ
ｉｎＫＥ
－ＧＣＬＢ
．（ｘ
）［０
．１２
，０
．４０
，０
．１９
，０
．２３
，０
．０６］


ｆｕｎｅｒａｌＥ
．
ｇｒａｖｅｙａｒｄ


Ｑ４
：Ｗｈｅｒｅｃｏｕｌｄ
ｙｏｕ
ｇｏｔｏ


ｂｅｔｗｅｅｎ１０００ａｎｄ１００００ＲｏＢＥＲＴａ
－ＬａｒｇｅＤ
．（ｘ
）［０
．００
，０
．１２
，０
．００
，０．７３
，０
．１５］


ｒｅｓｔａｕｒａｎｔ？ＱＡ
－ＧＮＮＢ
．（ｘ
）［０
．０８
，０＾１
，０
，３３
，０
．０１
，０
．０７］


Ａ．ｂｉｇｃｉｔｖＢ
．ｔｏｗｎＣ
．ｓｍａｌｌｔｏｗｎＫＥ
－ＧＣＬＢ
．（ｘ
）［０
．２１
，０３３
，０
．２７
，０
．１９
，０
．００］


Ｄ
．ＣａｎａｄａＥ
．
ｙｅｌｌｏｗ
ｐａｇｅｓ


３
．３
．７可视化分析


为了验证
ＫＥ
－ＧＣＬ
中
自适应图增强策略的有效性
，
本小节针对


ＣｏｍｍｏｎｓｅｎｓｅＱＡ数据集中的案例展开注意力的权重可视化分析
，如图
３
－５所示
。


具体来说
，
可视化分析从ＧＡＴ网络的最后
一层取得由上下文节点到其所有的
一


跳邻居的信息权重进行了展示
，
当前常识问题
“Ｗｈｅｒｅ
ｉｓａｈｕｍａｎ
ｌｉｋｅｌｙ
ｔｏｇｏａｓａ


ｒｅｓｕｌｔｏｆｂｅｉｎｇｈｕｎｇｒｙ？
”
的正确答案为
“Ｅａｔｉｎｒｅｓｔａｕｒａｎｔ
”
，
而原始知识子图
（左
）


及其增强视图
（右
）的侧边栏显示了对应的注意力权重大小
。可以观察到
，在自


适应图增强后
，上下文节点为正确选项所对应的节点
“Ｅａｔ
ｉｎｒｅｓｔａｕｒａｎｔ
” 赋予了更


多的权重
，这说明了模型在自适应图增强后能进
一步朝着正确答案的方向进行知


识推理
；
同时上下文节点对于
“
Ｓａｔｅｙｏｕｒｈｕｎｇｅｒ
”和
“Ｍａｋｅｂｒｅａｄ
” 等节点给予了


更少的关注度
，
这些节点信息其实是模型在推理过程中的千扰噪声
，
这也表明


４６


第三章基于知识增强型图对比学习的常识问答算法研究


ＫＥ
－ＧＣＬ的自适应图增强算法能够有效地减少知识图中的噪声
，并且在图推理过


程中引导模型关注那些更有利的节点信息
，
以做出更好的答案预测
。


ｊｇ
—
０
．３５ｍｔｒ０
．３５


Ｍａｋｅｂｒｅａｄ＼幾
■Ｍａｋｅｂｒｅａｄ０
．
１
１


Ｂ
－
０
．５０■
－
〇Ｍ
ｉ


Ｅａｉｖｅｇｅｔａｂｌｅｓ０
，０７■
ｉｖｅｇｅｕｊｂｂｓ０
，０９


Ｓａｌｅｖｏｕｒｈｕｎｇｅｒ０
，０９＿
＆
ｓａｌｅ
ｙｏｍＩｎｍｇｃｊ０
．０２


Ｈａｔｉｎｒｅｓｌａｕｒａｎ
ｉ
Ｉ
；ｉ
！ｍｍｓｔａｕｒａｎ
ｉ
ＱＱｊｊ


Ｂｒｍｇｈｏｍｅｓｏｍｅｆｉｓｈ０
．０９一
〇１
；
ｌｉｒｕｉｇｈｏｍｅｓｏｍｅｆ
ｉｓｈ０
，０８－
ａ
！５


ＣＴ
Ｏｋ
ｄｍ？ｅｒ０
．
１
１＿
〇Ｊ〇
Ｃ＾
ｄｍｎｃ
ｒ０．１３＿
謂


Ｇｅｌａ
ｊｏｂ０
．０６
（
ｉｅｔａ
ｊｏｈ０
．０５


－
０
．０５
－
００５


Ｂａｋｅａｃａｋｅ
ｌｉａｋｃ＊ｉｃａｋｅ０Ｊ２


－
０
．００
－
（ＭＫ
Ｉ


图
３
－５
知识子图在
自适应性增强前后的注意力可视化分析


３
．３
．８关键参数分析


为了探宄图对比学习
中难负例对模型性能的影响
，
本小节在


ＣｏｍｍｏｎｓｅｎｓｅＱＡ
和
ＯｐｅｎｂｏｏｋＱＡ两个数据集上评估了在不同难负例权重
０


的设置下
（公式
３
－
１９中所采用
）
，
ＫＥ
－ＧＣＬ模型的性能指标变化
，其实验结果如


图
３
－６所示
。值得注意的是
，
当权重因子
／
？＝
２
．０时
，
ＫＥ
－ＧＣＬ方法在两个数据


集上都取得了最佳性能
。
说明在图对比学习中设置合适的难负例的确可以带来


积极的收益
。
此外
，
可观察到当Ｐ设置较小时
（即数值小于
２
．０
）
，
模型性能在


相对稳定中略有增加
；
而当权重因子Ｐ数值攀升至
２
．０
以上时
，
模型性能急剧


下降
。
一个可能的原因是当权重因子
／
？设置过大时
，
模型将会过于关注难负例
，


在反向传播的过程中
，
正例对的相关梯度便会被严重削弱
，
从而导致图对比学习


损失难以优化
。


４７


北京邮电大学工学硕士学位论文


气
（
＞ｐ
ｉ
＊ｎｂｉＨ
＞ｋＱＡ


：＼Ｔ


一
竹
■


＾
７０
－


ｍ


６２
＂

、


０００５Ｍ）
１
．５２Ａ）２Ｓ１０＾４０


Ｗｅｉｇｈｉｎｇｆｕｃｔｏｒ０


图
３
－６
图对比学习中难负例因子０的影响


３
．４本章小结


本章在有监督常识问答方向开展了基于知识増强型的图对比学习模型
（ＫＥ
－


ＧＣＬ
）的研宄工作
。首先
，
本章从缓解知识噪声问题和增加知识覆盖面的角度给


出了ＫＥ
－ＧＣＬ模型的设计动机与解决思路
，并概述了模型端到端的处理流程
。紧


接着
，本章详细介绍了ＫＥ
－ＧＣＬ的各个组件模块
，该框架将上下文描述集成到当


前的知识子图中
，形成知识增强图
；接着
，
本章提出了
一种自适应采样策略来生


成当前知识子图所对应的增强视图
；随后
，本章通过边散射和节点聚合的方式对


图谱的信息进行更新与推理
；
此外
，
为了进
一步増强图对比学习的训练效果
，模


型将当前样例中错误问答对相关的知识子图及其增强视图视作为难负例
。
最后
，


本章在
ＣｏｍｍｏｎｓｅｎｓｅＱＡ和
ＯｐｅｎｂｏｏｋＱＡ两个基准数据集上进行了大量的定量


与定性实验分析
，
从模型性能对比
、
消融实验、案例分析
、
可视化分析
、难负例


影响等方面充分验证了所提出的ＫＥ
－ＧＣＬ方法的可行性与有效性
。


４８


第四章基于提示型知识生成的常识问答算法研宄


第四章基于提示型知识生成的常识问答算法研究


４
．
１引言


无监督常识问答任务是指在不采用任何带标签数据的条件下
，通过挖掘问题


背后的相关常识知识
，进而实现模型推理并完成答案预测
。之前的研宄通常采用


检索或匹配的方式对特定任务的相关知识库进行大量的手工规则筛选
，并借助预


训练语言模型进行知识改写或生成
，从而进行答案的推理预测
。然而
，这种方法


产生的知识类型比较有限且固定
，并且特定的规则处理框架泛化能力较差
，应用


到新的任务领域的迁移成本较高。针对以上问题
，本文提出了
一种基于提示型的


知识生成网络ＰＫＧＮ（Ｐｒｏｍｐｔ
－ｂａｓｅｄＫｎｏｗｌｅｄｇｅＧｅｎｅｒａｔ
ｉｏｎＮｅｔｗｏｒｋ）应用于无监


督的常识问答任务
，该方法借助对比学习的策略完成了对问题语义表示的无监督


继续预训练
，以便后续阶段能高效利用提示模版来引导模型生成更为准确的相关


知识描述
，
并通过文本匹配进行知识选择与答案预测
。本文提出的
ＰＫＧＮ无监


督框架具备较好的泛化性与任务迁移能力
，模型仅需针对模版及预测目标进行部


分调整
，
便能够灵活地迁移到多种自然语言理解任务中
。


无监督常识问答任务的形式化定义与有监督方向保持
一致
（详见
３
．
１节
）
，


故此处不再赘述
。


４
．２基于提示学习的知识生成模型


４
．２
．
１模型总览


本文提出的
ＰＫＧＮ无监督模型的整体架构如图
４
－１所示
，
主要包括三个部


分
，
分别是
：
基于问题语义的对比学习模块
（４
．２
．２节
）
、
提示型知识生成模块


（４
．２Ｊ节
）
、
以及知识推理与答案预测模块
（４
．２
．４节）
。


具体来说
，首先
，基于问题语义的对比学习模块通过Ｄｒｏｐｏｕｔ噪声进行数据


增强
，从而构造当前样本的常识问题在训练批次中对应的正例
（Ｐｏｓｉｔｉｖｅ）和负例


（Ｎｅｇａｔｉｖｅ）
，
并通过
ＩｎｆｏＮＣＥ损失
［９９
］进行对比学习的目标优化
，
从而让知识生


成模型预先捕获到数据集中问题间的细微语义差异
；紧接着
，在提示型知识生成


模块中
，
多个常识问题及其相关背景知识构成常识模版提示
，
以引导模型生成
一


系列以问题为条件的知识描述
；最后
，知识推理与答案预测模块负责对多个知识


４９


北京邮电大学工学硬士学位论文


描述与问题进行拼接
，并利用知识推理模型与候选答案的匹配
，选择其置信度最


高的知识并给出对应的答案预测
。


１
ｊ
Ｉ
Ｊ


ＦｕｒｔｈｅｒＰｒｅ
－
ｔｒａ
ｉｎ
ｉｎｇｗ
ｉｔｈＣＬ
１
！Ｋｎｏｗｌｅｄｇｅｐｒｏｍｐｔ
？


Ｂ
＼
＊
ｊ
ｉｎｓｔｍｃｔｉｏｎ
＼


—Ｈ
．ｏｏｈＰ
ｉ
ｉ
＝
」


ｊ＾
＼
ｉ
ｉＣａｓｅｓｆｏｒＣＱＡｔａｓｋ
：


Ｋｎｏｗｌｅｄｇｅ
＼
＼
ｉｎｐｕｔ
：ｑｉ


＾Ｇｅｎｅｒａｔｉｖｅ
Ｈ
．ｗＱ／
］
ｊ
？Ｋｎｏｗ
ｌｅｄｇｅ
：Ｋ１


Ｍｏｄｅ
ｌ
ｉ
？
｜


ｉ
／
｛
？
Ｉｎｐｕｔ
：Ｑｎ
ｊ


｜
’
！
＞Ｋｎｏｗｌｅｄｇｅ
：Ｋｎ


ｉ
ｉ
｜Ｑｕｅｓｔ
ｉｏｎ
ｓ


！ｖ
■
＿


ｊＫｎｏｗｌｅｄｇｅ１
ｔ


Ｋｎｏｗｌｅｄｇｅ
＼
Ｋｎｏｗ
！ｅｄ９ｅ２
ｊ义Ｋｎｏｗ
ｌｅｄｇｅ


１＞Ｇｅｎｅｒａｔ
ｉｖｅ

＞Ｒｅａｓｏｎ
ｉｎｇ
〇Ａｎｓｗｅｒ


Ｍｏｄｅ
ｌ
！
－
｜Ｍｏｄｅ
ｌ


｜
Ｋｎｏｗ
ｌｅｄｇｅＳ
ｊ
（


Ｋｎｏｗ
ｌｅｄｇｅＧｅｎｅｒａｔ
ｉｏｎｗ
ｉｔｈＰｒｏｍｐ
ｔＫｎｏｗｌｅｄｇｅＲｅａｓｏｎ
ｉｎｇ＆ＡｎｓｗｅｒＰｒｅｄ
ｉｃｔ
ｉｏｎ


图
４
－
１ＰＫＧＮ模型的结构总览


当前无需进行下游任务的微调
，不依赖于特定的模型结构和外源的人工标注
，


能以较小代价快速实现领域及任务迁移
。
同时
，
与以往基于提示学习的常识问答


方法相比
，
本文提出的
ＰＫＧＮ模型利用无监督对比学习的方式
，
在不引入额外


网络参数的情况下进行了继续预训练的过程
，有效利用了预训练模型中的先验知


识
，拉开问题间差异的同时也能并引导模型在后续完成更高质量地知识生成
，实


现模型性能的进
一步提升
。


４
．２
．２基于问题语义的对比学习模块


基于问题语义的对比学习模块本质是对生成模型进行继续预训练的过程
，通


过对常识数据集中的问题语料进行无监督训练
，在获取常识问题的有效特征表示


的同时
，
也能利用对比学习的方式让模型更好地感知到问题间的细微语义差异
，


从而弥合预训练任务和下游任务之间的语料分布及训练目标的鸿沟
。后续的知识


生成过程将复用当前的训练参数
，
以使模型能够更好地集成
Ｐｒｏｍｐ
ｔ模版并生成


更有针对性的知识描述
。


传统的无监督对比学习的训练方式包括了离散型数据增强
（如单词的替换和


删除
）
，
以及基于双编码器
（ＤｕａｌＥｎｃｏｄｅｒ）
的当前句与下
一句预测等
，
但诸如


５０


第四章基于提示型知识生成的常识问答算法研宄


此类的方法可能会损害句子的上下文语义
，从而使得模型因为正样本的生成偏差


而导致对比学习的训练效果欠佳
。


受到
Ｇａｏ等人Ｐ研是出的
ＳｉｍＣＳＥ工作的启发
，
当前模块将预训练模型中的


Ｄｒ
ｏｐｏｕｔ视为噪声进行数据增强
，
因为Ｄｒｏｐｏｕｔ策略天然具备更好地对齐性和均


匀性
，在每次前向传播过程中随机丢弃网络中的神经元
，
属于隐藏表示层的噪声


扰动而并不会损害句子本身的语义
。



１
Ｒａｄｏｍｄｒｏｐｏｕｔｍａｓｋ
：


＿＿＿＿＿＿＿
ｉｎｔｗｏｆｏｒｗａｒｄｐａｓｓｅｓ


Ｑｕｅｓｔ
ｉｏｎ１＞￣＞
（〇〇ｋ＾
（＋
）


（
－
）
＇
？
＼＼


— －—
－—
ｉ＼＼


＿
：
‘ ｎ
２—＾＾
Ｉ＼


Ｋｎｏｗ
ｌｅｄｇｅ
．
．＼


＾Ｇｅｎｅｒａｔｉｖｅ
（
￣
：
／！


Ｑｕｅｓ
ｔ
ｉｏ
ｎ３Ｍ〇（ｊ０
ｌ＾Ｊ


／


／


（
－
）
／


／


／


乂


Ｑ
ｉ，ｎ
ｃ
，
‘

，
—
＿一


！

ｉ
（＋）
！


＼ＫＧＰＴ
－ｂａｓｅｄＰＬＭ？Ｐｏｓｔ
ｉｖｅｉｎｓｔａｎｃｅ
Ｉ


ｉｗ
ｉｔｈｄｒｏｐｏｕｔ
Ｉ


ｉ
ｉ


ｉ
（
一
）
＼


ｌ＞Ｄａｔａｆｌｏｗ？Ｎｅｇａｔ
ｉｖｅ
ｉｎｓｔａｎｃｅ


一
一一
一
纖一
一
一
一
一
一
－—
一
一
一
一
一
一— 一
一
一
一
《？？
一
一
一
一
靡
—
一
一
一
－—
一一ｗ


图
４
－２ＰＫＧＮ模型的无监督对比学习策略


当前的无监督对比学习的训练策略如图
４
－２所示
，
具体来说
，
对于每个常识


问题实例＆
，生成模型会进行两次前向传播
，通过应用两次Ｄｒｏｐｏｕｔ而获得当前


实例两个不同的隐藏特征向量并作为正样本对
（
／ｉ
ｉ
，／２＾）
，
由于Ｄｒｏｐｏｕｔ的随机特


性
，
相同的问题输入模型后得到的两个特征向量并不完全相等
，在
一定程度上能


提升模型的泛化能力
；而负样本则选择当前训练批次中的其它常识问题所编码后


的特征向量
。
此阶段采用
Ｉｎｆ
ｏＮＣＥ损失
［９９
］作为无监督对比学习的优化目标
，
其


公式定义如下
：


Ｑ
ｓｉｍｉｈ
ｉ
．
ｈ＾／Ｔ


ＬｃＬ
＝￣
Ｌ
ｌ〇９
＾
ｉ（ｅ
ｓ
ｉｒｎ＾
，＾）／ｒ
）
（４
＇０


其中
，
ｉＶ表示当前训练批次的大小
，
ｓｉｍ代表两个向量的余弦相似性
，
Ｔ为


温度超参数
，
用以反映模型对难负例的关注度
。
由此可见
，
生成模型在不断优化


５
１


北京邮电大学工学硕士学位论文


无监督对比损失的过程中
，常识问题本身和其经过Ｄｒｏｐｏｕｔ增强后正样本之间的


语义表征不断拉近
，而将不同常识问题的语义逐渐拉远
，
以此来对抗文本向量的


坍缩现象并学习到问题的有效特征表示
。


４
．２
．３提示型知识生成模块


提示型知识生成模块是利用常识模版提示来指导生成模型产生与问题中的


主题事件或者常识概念相关的知识
，
即
一系列具备连贯语义的句子
，其本质是填


补知识空白
，
以辅助后续模型进行知识推理与答案预测
。


具体而言
，该模块借助与问题相关知识来提示语言模型生成知识陈述
，提示


模版在问答任务中均使用相同的通用格式
，
即由
一个指令句
、多个问题知识对的


示例组成
，并以
一个
“ ＜ｑｕｅＳｔｉｃｍ＞
” 问题占位符结尾
。在生成特定问题的知识时
，


当前模块会将问题插入占位符
，并通过重复多次采样以生成当前提示的接续部分
，


从而获得
一组知识描述＆
，
可形式化表示如下
：


ｆ
ｅｓ
＝
／Ｇ（Ｐ
ｇ
，分）
，ｓ＝１… Ｓ
）（４
－２）


其中
，
每个知识陈述匕是可变长度的文本生成序列
，
包含与问题相关的信


息
，
Ｐ
ｑ代表了当前任务的提示模版
，
仏表示知识生成模型
。


此外
，
在相同的数据集中生成问题的文本知识时
，
模版提示是完全固定的
，


并且需要收集多个与任务密切相关的问题
，以及为每个问题设计能帮助推理出答


案的知识描述
。这里值得注意的是
，所提供的知识描述不应该直接包括最终的答


案
，
而应该是更为多元化的短句文本
，
以暗示问题中的概念与答案之间的关系
，


类似于人类的思考或解决方案的过程
。例如
，对于企鹅有多少只翅膀的问题
，可


以写出以下知识陈述
：鸟类有两个翅膀
，企鹅是
一种鸟
。知识陈述中的两句话可


以被视为模型进行演绎推理的
一整套前提
，但如果是类似
“ 企鹅有两只翅膀
” 这


种知识表述则需要进行规避
，
因为它直接解释了问题和答案
。


因此
，在用于评估的三个常识问答数据集上
，完整的知识生成模版提示的设


计分别如表４
－
１
、表４
－２、
以及表
４
－３所示
：


５２


第四章基于提示型知识生成的常识问答算法研宄


表
４
－
１ＣｏｍｍｏｎｓｅｎｓｅＱＡ数据集对应的提示模版


基准数据集ＣｏｍｍｏｎｓｅｎｓｅＱＡ


Ｉｎｓｔｒｕｃｔｉｏｎｓ
：Ｇｅｎｅｒａｔｅｓｏｍｅｋｎｏｗｌｅｄｇｅａｂｏｕｔｔｈｅｃｏｎｃｅｐ
ｔｓｉｎｔｈｅｉｎｐｕｔ
．


Ｅｘａｍｐ
ｌｅｓ
：


Ｉｎｐｕｔ：Ｇｏｏｇ
ｌｅＭａｐｓａｎｄｏｔｈｅｒｈｉｇｈｗａｙａｎｄｓｔｒｅｅｔＧＰＳｓｅｒｖｉｃｅｓｈａｖｅ


ｒｅｐ
ｌａｃｅｄｗｈａｔ？


Ｋｎｏｗｌｅｄｇｅ
：Ｅｌｅｃｔｒｏｎｉｃｍａｐｓａｒｅｔｈｅｍｏｄｅｍｖｅｒｓｉｏｎｏｆ
ｐａｐｅｒａｔｌａｓ
．


Ｉｎｐｕｔ
：Ｔｈｅｆｏｘｗａｌｋｅｄｆｒｏｍｔｈｅｃｉｔｙ
ｉｎｔｏｔｈｅｆｏｒｅｓｔ
，ｗｈａｔｗａｓｉｔｌｏｏｋｉｎｇ


ｆｏｒ？


Ｋｎｏｗｌｅｄｇｅ
：Ｎａｔｕｒａｌｈａｂｉｔａｔｓａｒｅｕｓｕａｌｌｙａｗａｙｆｒｏｍｃｉｔｉｅｓ
．


Ｉｎｐｕｔ
：Ｙｏｕｃａｎｓｈａｒｅｆ
ｉｌｅｓｗｉｔｈｓｏｍｅｏｎｅｉｆ
ｙｏｕｈａｖｅａｃｏｎｎｅｃｔｉｏｎｔｏａ


知识生成的ｗｈａｔ？


Ｋｎｏｗｌｅｄｇｅ
：ＦｉｌｅｓｃａｎｂｅｓｈａｒｅｄｏｖｅｒｔｈｅＩｎｔｅｒ
ｎｅｔ
．


Ｉｎｐｕｔ：Ｔｏｏｍａｎｙｐｅｏｐ
ｌｅｗａｎｔｅｘｏｔｉｃｓｎａｋｅｓ
．Ｔｈｅｄｅｍａｎｄｉｓｄｒｉｖｉｎｇｗｈａｔ


ｔｏｃａｒｒｙｔｈｅｍ？


Ｋｎｏｗｌｅｄｇｅ
：Ｓｏｍｅ
ｐｅｏｐ
ｌｅｒａｉｓｅｓｎａｋｅｓａｓ
ｐｅｔｓ
．


Ｉｎｐｕｔ
：Ｔｈｅｂｏｄｙｇｕａｒｄｗａｓｇｏｏｄａｔｈｉｓｄｕｔｉｅｓ
，ｈｅｍａｄｅｔｈｅ
ｐｅｒｓｏｎｗｈｏ


ｈｉｒｅｄｈｉｍｗｈａｔ？


Ｋｎｏｗｌｅｄｇｅ
：Ｔｈｅ
ｊｏｂｏｆ
ｂｏｄｙｇｕａｒｄｓｉｓｔｏｅｎｓｕｒｅｔｈｅｓａｆｅｔｙａｎｄｓｅｃｕｒｉｔｙ


ｏｆｔｈｅｅｍｐ
ｌｏｙｅｒ
．


Ｉｎｐｕｔ
：＜ｑｕｅｓｔｉｏｎ＞


Ｋｎｏｗｌｅｄｇｅ
：


５３


北京邮电大学工学硕士学位论文


表４
－２ＯｐｅｎｂｏｏｋＱＡ数据集对应的提示模版


基准数据集ＯｐｅｎｂｏｏｋＱＡ


Ｉｎｓｔｒｕｃｔ
ｉｏｎｓ
：Ｇｅｎｅｒａｔｅｓｏｍｅｅｌｅｍｅｎｔａｒｙｓｃｉｅｎｃｅｋｎｏｗｌｅｄｇｅａｂｏｕｔｔｈｅ


ｃｏｎｃｅｐｔｓｉｎｔｈｅｉｎｐｕｔ
．Ｅｘａｍｐ
ｌｅｓ
：


Ｉｎｐｕｔ：Ａｓ
ｙｏｕｌｏｏｋｄｅｅｐｅｒｉｎｔｏａｍａｒｂｅｌ
ｙｏｕｃａｎｓｅｅ？


Ｋｎｏｗｌｅｄｇｅ
：Ａｓｔｈｅｓｉｚｅｏｆａｎｏｂｊｅｃｔａｐｐｅａｒｓｌａｒｇｅｒ，ｔｈａｔｏｂ
ｊｅｃｔｗｉｌｌｂｅ


ｏｂｓｅｒｖｅｄｂｅｔｅｒ
．


Ｉｎｐｕｔ
：Ｉｎｔｈｅｗｉｌｄｅｒｎｅｓｓ
，ｌｉｇｈｔ
ｐｏｌｌｕｔ
ｉｏｎｉｓ？


Ｋｎｏｗｌｅｄｇｅ
：Ａｓｄｉｓｔａｎｃｅｔｏａｃｉｔｙｄｅｃｒｅａｓｅｓ
，ｔｈｅａｍｏｕｎｔｏｆｌｉｇｈｔ
ｐｏｌｌｕｔ
ｉｏｎ


ｗｉｌｌｉｎｃｒｅａｓｅ
．


知识生成的Ｉｎｐｕｔ
：Ｅａｒｔｈｒｏｔａｔｉｎｇｃａｕｓｅｓ？


提示模版Ｋｎｏｗｌｅｄｇｅ
：Ａ
ｐ
ｌａｎｅｔｒｏｔａｔｉｎｇｃａｕｓｅｓｃｙｃｌｅｓｏｆｄａｙａｎｄｎｉｇｈｔｏｎｔｈａｔ


ｐ
ｌａｎｅｔ
．


Ｉｎｐｕｔ：Ｒｅｎｅｗａｂｌｅｒｅｓｏｕｒｃｅｓ？


Ｋｎｏｗｌｅｄｇｅ
：Ｒｅｎｅｗａｂｌｅｒｅｓｏｕｒｃｅｓｃａｎｂｅｕｓｅｄｏｖｅｒａｇａｉｎ
．


Ｉｎｐｕｔ
：Ｔｈｅｒｅｍｏｖａｌｏｆｔｒｅｅｓｍａｙｃａｕｓｅｄａｍａｇｅｔｏｅｃｏｓｙｓｔｅｍｓｓｕｃｈａｓ？


Ｋｎｏｗｌｅｄｇｅ
：Ｃｕｔｉｎｇｄｏｗｎｔｒｅｅｓｈａｓａｎｅｇａｔ
ｉｖｅｉｍｐａｃｔｏｎａｎｏｒｇａｎｉｓｍｓ


ｌｉｖｉｎｇ
ｉｎａｎｅｃｏｓｙｓｔｅｍ
．


Ｉｎｐｕｔ
：＜ｑｕｅｓｔｉｏｎ＞


Ｋｎｏｗｌｅｄｇｅ
：


５４


第四章基于提示型知识生成的常识问答算法研宄


表４
－３ＳｏｃｉａｌｌＱＡ数据集对应的提示模版


基准数据集ＳｏｃｉａｉｌＱＡ


Ｉｎｓｔｒｕｃｔｉｏｎｓ
：Ｇｅｎｅｒａｔｅｓｏｍｅｋｎｏｗｌｅｄｇｅａｂｏｕｔｔｈｅｅｖｅｎｔｓｉｎｔｈｅｉｎｐｕｔ
．


Ｅｘａｍｐ
ｌｅｓ
：


Ｉｎｐｕｔ
：Ｋｅｎｄａｌｌｆ
ｒｉｇｈｔｅｎｅｄｔｈｅｄｏｇｓａｗａｙｂｙｙｅｌｌｉｎｇａｎｄｗａｖｉｎｇｈｉｓ


ｈａｎｄｓ
．ＷｈｙｄｉｄＫｅｎｄａｌｌｄｏｔｈｉｓ？


Ｋｎｏｗｌｅｄｇｅ
：Ｐａｒｅｎｔｓｗｉｌｌ
ｐｒｏｔｅｃｔｔｈｅｉｒｃｈｉｌｄｒｅｎｆｒｏｍｄｏｇａｔｔａｃｋｓ
．


Ｉｎｐｕｔ
：Ｃａｍｅｒｏｎｄｅｃｉｄｅｄｔｏｈａｖｅａｂａｒｂｅｃｕｅａｎｄ
ｇａｔｈｅｒｅｄｈｅｒｆｒｉｅｎｄｓ


ｔｏｇｅｔｈｅｒ
．Ｈｏｗｗｏｕｌｄｏｔｈｅｒｓｆｅｅｌａｓａｒｅｓｕｌｔ？


Ｋｎｏｗｌｅｄｇｅ
：Ｇａｔｈｅｒ
ｉｎｇｗｉｔｈｆ
ｒｉｅｎｄｓｆｏｒａｂａｒｂｅｃｕｅｉｓａ
ｇｒｅａｔ
ｐ
ｌｅａｓｕｒｅ
．


Ｉｎｐｕｔ
：ＫｅｎｄａｌｌｒａｎｂａｃｋａｎｄｔｈａｎｋｅｄＬｅｅｆｏｒｈｅｌｐ
ｉｎｇｈｅｒｆ
ｉｎｄｔｈｅｄｏｇ
．


知识生成的Ｈｏｗｗｏｕｌｄ
ｙｏｕｄｅｓｃｒ
ｉｂｅＫｅｎｄａｌｌ？


提示模版Ｋｎｏｗｌｅｄｇｅ
：Ｗｅａｒｅ
ｇｒａｔｅｆｕｌｔｈａｔｏｔｈｅｒｓｈａｖｅｈｅｌｐｅｄｕｓ
．


Ｉｎｐｕｔ
：Ｊａｎｎｅｅｄｅｄｔｏ
ｇ
ｉｖｅｏｕｔ
ｊｏｂｓｆｏｒａｎｕｐｃｏｍｉｎｇｐｒｏｊｅｃｔａｔｗｏｒｋ
．Ｗｈａｔ


ｗｉｌｌＯｔｈｅｒｓｗａｎｔｔｏｄｏｎｅｘｔ？


Ｋｎｏｗｌｅｄｇｅ
：Ａｆｔｅｒｂｅｉｎｇａｓｓｉｇｎｅｄａｎｅｗ
ｐｒｏｊｅｃｔ
ｐｅｏｐ
ｌｅｗｉｌｌｓｔａｒｔ


ｗｏｒｋｉｎｇ
．


Ｉｎｐｕｔ
：Ｋａｉｆｏｕｎｄｏｎｅｆｏｒｓａｌｅｏｎｌｉｎｅｂｕｔｉｔｗａｓｔｏｏｍｕｃｈｍｏｎｅｙｆｏｒｈｅｒ
．


ＷｈａｔｄｏｅｓＫａｉｎｅｅｄｔｏｄｏｂｅｆｏｒｅｔｈｉｓ？


Ｋｎｏｗｌｅｄｇｅ
：Ｐｅｏｐ
ｌｅｎｅｅｄｔｏｔｕｒｎｏｎｔｈｅｌａｐｔｏｐｂｅｆｏｒｅ
ｇｏｉｎｇｏｎｌｉｎｅ
．


Ｉｎｐｕｔ
：＜ｑｕｅｓｔｉｏｎ＞


Ｋｎｏｗｌｅｄｇｅ
：


可以看到
，模版提示中多个常识问题及其相关背景知识作为输入
，构成不同


的常识模版提示
，然后根据问题和提示的对应关系
，生成相应的知识描述作为输


出
。这种基于提示学习的方式可以有效地利用预训练语言模型中的先验知识
，并


能以生成的方式提高知识的质量和多样性
。


５５


北京邮电大学工学硕士学位论文


４
．２
．４知识推理与答案预测模块


在每个问题得到其对应的
一系列相关知识描述后
，知识推理与答案预测模块


则负责将当前的知识进行有效整合
，推理模型将其结合当前的所有候选答案进行


置信度相关的可靠预测
。


具体而言
，
对于每个问题ｇ生成的Ｓ个知识陈述
知识


推理模型会将知识陈述单独添加到问题前
，形成Ｓ＋１个带有知识描述的完整问


题表达
，
可表示如下
：


Ｒｏ
＝
Ｒ＞
ｃｈ
＝
［＾１
Ｉ
Ｉ
＞Ｒｓ
＝
Ｉ
Ｉ？］（４
－３）


这里［彳卜］表示文本拼接
。推理模型会对每个候选答案
进行匹配与评分操作
，


即采用最支持当前候选项的知识描述来确定
一个置信度分数
。
由此可见
，
当前方


法会倾向于选择唯
一的候选答案及其相关知识
，同时筛掉无法让推理模型做出果


断决策的知识
。因此
，推理模型会从知识描述中选择
一个给出最髙置信度预测的


候选答案作为最终输出
，
可表示为如下公式
：


石
＝
ａｒｇｍａｘ
＼ｑｇ）（４
－４）


其中
，
Ａ代表知识推理模型
，
采用基于交互式文本匹配的预训练语言模型


实现
。模型给出的最高预测得分是以某个确切的知识描述
为推理依据得


出
，
所选知识对应的下标可形式化表示如下
：


ｓ＝ａｒｇｍａｘｍａｘ
ｆＲ＾
＼ｑｓ）（４
－５）


０＜Ｓ＜ＳＣ
ｉｅｃ


４
．３实验对比与分析


４
．３
．１数据集与评估指标


无监督常识问答任务的实验在
ＣｏｍｍｏｎｓｅｎｓｅＱＡ和
ＯｐｅｎｂｏｏｋＱＡ数据集的


基础上
（详见
３
．３
．
１节
）
，
增设了ＳｏｃｉａｌｌＱＡ基准数据集
［４］
，这是
一个用以评估语


言模型对社交互动及日常事件的理解及推理能力的数据集
，涉及到了日常的各类


社交场景
，如情感理解、社交关系
、社交规则等多种角度
。具体地
，
ＳｏｃｉａｌｌＱＡ数


据集总共包含
３７
，５８８个问答对
，
训练集
、测试集
、验证集的数量分布如表４
－４所


不
。


５６


第四章基于提示型知识生成的常识间答算法研宄


表
４
－４ＳｏｃｉａｌｌＱＡ数据集的统计结果


数据集划分样本数量


训练集３３
，４１０


测试集
１
，９５４


ＳｏｃｉａｌｌＱＡ


验证集２
，２２４


总计３７
＞５８８


图
４
－３展示了ＳｏｃｉａｌｌＱＡ数据集的
一个真实示例
，每个问题具有三个候选项
，


包括了正确答案
、与问题相关但不正确的干扰选项
、
以及无关选项
。模型需要结


合每个问题所附带的社交情境段落进行常识推理与行为判断
，从而选择出最合适


的答案
。无监督常识问答任务仍然采用答案预测准确率作为模型性能的评估指标
。


为了进行合理地对比分析
，后续所有的在三个基准数据集上的实验报道均是


基于相关验证集开展
。值得注意的是
，
模型在训练过程中严格控制标签不可见
，


标签仅用于最终的性能评估
。


ＳｏｃｉａｌＡｌｅｘｓｐ
ｉｌＭｔｈｅｆｏｏｄｓｈｅｊｕｓｔ
ｐｒｅｐａｒｅｄａｌｌｏｖｅｒｔｈｅｆ
ｌｏｏｒ
，


Ｃｏｎｔｅｘｔａｎｄｉｔｍｏｈｅａｈｕｇｅｍｅｓｓ．


ＱｕｅｓｔｉｏｎＷｈａｔｗｉｌｌ
Ａｌｅｘｗａｎｔｔｏｄｏｎｅｘｔ？


＆ＣｈｏｉｃｅｓＡ、Ｕｉｓｔｅ
ｔｈｅｆ〇０ｄＩｍｏｐｕｐｓｓｆＣ，ｒｕｎａｒｏｕｎｄｉｎｔｈｅｍｅｓｓ


Ｒｅｕｓｉｎｇ


Ｗｈａｔｈａｐｐｅｎｓｎｅｘｔ


图
４
－３ＳｏｃｉａｌｌＱＡ数据集的真实样本示例


４
．３
．２对比基线设置


本节选取了近年在无监督常识问答任务上的多种研宄方法作为对比基线
，并


对各自的研究工作进行简要地概括说明
：


（
１
）原始基线直接釆用
ＧＰＴ
＿２＾进行训练
，
即在不加入任何的相关外部知


识的前提下
，输入侧具体构建为当前常识问题拼接上所有答案选项的上下文组合


表示
，
并通过ＧＰＴ
－２进行特征编码及评分预测
。


５７


北京邮电大学工学硕士学位论文


（２）Ｓｅ！ｆ
－ｔａｌｋ
［９５
］受到基于询问的发现式学习的启发
，
以查询的方式从预训练


语言模型中提取相关的背景知识上下文
，
以提示模型作为预测答案的澄清声明
。


虽然该方法对生成知识的查询类型并没有限制
，但预设模版并不通用
，需要针对


不同任务类型来进行大量尝试调整
。


（３
）ＤｙｎａＧｅｎＰ６
：！基于
ＣＯＭＥＴ
［４６］构建了上下文相关的符号知识结构
，
并利


用它来动态推理与评估选项和问题间的适配程度
，但是该方法因为生成架构固定


为ＣＯＭＥＴ而限制了知识的适用领域与模型的泛化能力
。


（４）ＳＥＱＡ
［９７
］Ｓ用生成模型产生多个伪答案
，并将其与每个候选答案进行语


义相似性比较
，
采用
ＳＲ〇ＢＥＲＴａ
［４５］作为特征提取器进行投票从而得到最终答案
。


但它只与预训练语言模型进行
一次比较
，并且没有充分利用预训练过程中的固有


先验知识
。


（５
）ＧＫＰ
［９８］Ｗ符号知识表示的角度提出了
一种通用的神经元推理方法
，
从


数据集中选择样本构造提示
，并利用ＧＰＴ
－３＠］和Ｔ５
［４Ｔ
１等模型来生成并集成知识
，


从而使得方法具备较好的自适应性且能高效地完成领域迁移
。


对于
Ｓｅｌｆ
－ｔａｌｋ和
ＳＥＱＡ的工作
，
这里采用了Ｓｕｎ等人
［５２］的复现结果进行报


道
，
而其它的对比方法所示结果均来自相应论文中所报道的官方性能指标
。


４
．３
．３实验参数设置


本章提出的ＰＫＧＮ模型采用开源深度学习框架Ｐｙｔ
ｏｒ
ｃｈ进行网络搭建与模型


训练
，并在
３个常识问答基准数据集上进行了定性和定量实验
。实验环境为搭载


了２张４８ＧＢＮＶＩＤＩＡＲＴＸＡ６０００显卡的Ｕｂｕｎｔｕ２０
．０４服务器
。


基于问题语义的对比学习模块和提示型知识生成模块以生成式预训练语言


模型ＧＰＴ
－２作为基础骨架
。
为了获得稳定可靠的性能结果
，
此处基于ＧＰＴ
－２的


四组不同参数规模开展实验
：
ＧＰＴ
－２Ｓｍａｌｌ（
１
１７Ｍ）
、
ＧＰＴ
－２Ｍｅｄｉｕｍ（３４５Ｍ）
、


ＧＰＴ
－２Ｌａｒｇｅ（７６２Ｍ）
。
具体地
，
在无监督对比学习阶段
，
ＩｎｆｏＮＣＥ损失函数所


采用的温度因子ｔ设置为
０
．１
；
在知识生成阶段
，
所设计的提示模版中包含的问


题示例通过对各常识数据集的训练集进行随机采样而产生
，从而保证了提示框架


的通用性与泛化能力
，
模型会为每个问题产生的
Ｓ＝２０条有效知识描述
，
此处设


置为内核采样
ｐ
＝０
．５
，
并丢弃重复项和空字符串
，
且当词长超过
６４或遇到


符号时
，
模型即停止当前知识描述的生成
。


在知识推理与答案预测模块中
，与
ＳＥＱＡ
［９７］方法保持相同的结构设置
，
即使


用
ＳＲｏＢＥＲＴａ
－Ｌａｒｇｅ
ｔ４５］作为基础架构
，
该模型以孪生网络的结构在ＮＬＩ（Ｎａｔｕｒａｌ


ＬａｎｇｕａｇｅＩｎｆｅｒｅｎｃｅ）相关的数据集上进行文本匹配的任务微调
，进而能更好地计


算知识问题对与候选答案之间的语义关联度
。


５８


第四章基于提示型知识生成的常识问答算法研究


４
．３
．４实验结果分析


表
４
－５展示了ＰＫＧＮ模型的无监督实验结果
。
在这三个基准常识问答数据


集上
，
所提出的
ＰＫＧＮ模型的性能均优于当前所有的对比基线
，
具体来说
，
以


ＧＰＴ２
－Ｍｅｄｉｕｍ
的配置为例
，
与之前的最优模型
ＳＥＱＡ相比
，
ＰＫＧＮ模型在


ＣｏｍｍｏｎｓｅｎｓｅＱＡ和ＯｐｅｎＢｏｏｋＱＡ数据集上的表现取得了显著的性能提升
，分别


高出了２
．６％和
９
．２％
；而在
ＳｏｃｉａｌｌＱＡ数据集上
，
ＰＫＧＮ模型相比
ＳＥＱＡ的方法


则高出
１
．５％
，
并且随着ＧＰＴ
－２架构的参数规模逐渐上升的同时
，
ＰＫＧＮ模型在


这三个基准数据集上的实验性能也在稳步提升
。
此外
，
ＧＫＰ方法配备了更大的


语言模型
（如ＧＰＴ
－３和Ｔ５
）
以生成常识知识
，
而ＤｙｎａＧｅｎ方法则额外引入外部


知识库ＣＯＭＥＴ生成推断以进行预测
。因此
，该类基线在公开排行榜（Ｐｕｂｌｉｓｈｅｄ）


上的性能表现较好
。


值得注意的是
，所提出的ＰＫＧＮ模型在从ＧＰＴ
－２Ｓｍａｌｌ（
１
１７Ｍ）逐渐递进到


ＧＰＴ
－２Ｌａｒｇｅ（７６２Ｍ）的过程中
，模型性能的增幅收益并没有像其它基线方法所


提升得明显
，
说明经过对比学习训练后的
ＰＫＧＮ模型在利用提示模版生成知识


描述时
，模型的生成质量基本是趋于稳定的
，
因而仅通过增加模型参数来提高语


言模型的性能
，
其存在的可上升空间是比较有限的
。
同时
，
可以观察到
Ｓｅｌｆ
－ｔａｌｋ


模型相比于
ＰＫＧＮ模型
，
其未能保持较为
一致有效性
，
其准确性在某些配置下


甚至略低于原始基线
，
尤其是在
ＣｏｍｍｏｎｓｅｎｓｅＱＡ数据集上
，
这表明
Ｓｅｌｆ
－ｔａｌｋ


在生成知识的质量并不稳定
，
可能因模型的结构与参数差异而导致性能的波动
。


５９


北京邮电大学工学硕士学位论文


表
４
－５ＰＫＧＮ模型在三个常识基准数据集上的主要实验结果对比


咖… 丄ＧＰＴ
－２ＧＰＴ
－２


模型方法ＧＰＴ
－２ＬａｒｇｅＰｕｂｌｉｓｈｅｄ


ＳｍａｌｌＭｅｄｉｕｍ


Ｂａｓｅｌｉｎｅ２５
．６２８
．２２８
．７
－


Ｓｅｌｆ
－Ｔａｌｋ２４
．８２７
．３３１
．５３２
．４


ＣｏｍｍｏｎｓｅｎｓｅＱＡＳＥＱＡ２６
．１３０
．７３４
．６
－


ＧＫＰ
－
－
－４７
．３


ＰＫＧＮ３０．７３３３３５
．１
－


Ｂａｓｅｌｉｎｅ１５２１７
．３２０
．９
－


Ｓｅｌｆ
－Ｔａｌｋ１７
．４２１
．０２３
．８
－


ＯｐｅｎＢｏｏｋＱＡ


ＳＥＱＡ２７
．６２８
．６３２
．０
－


ＰＫＧＮ３５．４３７
＜８３８２
－


Ｂａｓｅｌｉｎｅ３７
．１３９
．７４１
．６
－


Ｓｅｌｆ
－Ｔａｌｋ４１
．２４３３４５
．３４６
．２


ＳｏｃｉａＵＱＡＳＥＱＡ４４
．４４４
．６４６
．６４７
．５


ＤｙｎａＧｅｎ
－
－
－５０
．１


ＰＫＧＮ４５３４６．１４７２
－


４
．３
．５消融实验


为了研究ＰＫＧＮ模型中每个模块的有效性
，本小节基于ＣｏｍｍｏｎｓｅｎｓｅＱＡ和


ＯｐｅｎＢｏｏｋＱＡ两个基准数据集
，在ＧＰＴ
－２Ｍｅｄｉｕｍ的配置下
，对ＰＫＧＮ模型中的


各个组件进行了消融实验分析
，
实验结果如表４
－６所示
。


可以看到
，
当移除基于问题语义的对比学习模块
（ｗ／ｏＵｎｓｕｐ
．Ｃｏｎｔｒａｓｔ
ｉｖｅ


Ｌｅａｒ
ｎｉｎｇ）后
，
在不进行任何针对问题本身的学习训练后
，
模型性能在两个数据


集上均显著下降了２
．６％和
３
．７
°／。
，
这也证实了继续预训练对于后续知识生成的


重要性
，在利用对比学习掌握问题间的语义差异并学到更好的特征表示后
，生成


模型才能在后续利用提示模版生成更高质量的有效知识描述。


在移除提示型知识生成模块（ｗ／ｏＧｅｎｅｒａｔｉｖｅＫｎｏｗｌｅｄｇｅＰｒｏｍｐｔｉｎｇ）后
，ＰＫＧＮ


模型退化为了基于
ＳＲｏＢＥＲＴａ架构的自编码器
，
并直接以问题与候选答案的组


合拼接为输入进行打分预测
。因此
，模型性能在两个数据集上迎来了最大幅度的


６０


第四章基于提示型知识生成的常识问答算法研宄


下降
，分别为４
．
１％和
９
．５％
，这也反映了常识知识的关键性
，仅利用预训练语言


模型的固有先验是很难高效完成覆盖广泛的常识推理任务
。


在移除知识推理与答案预测模块
（ｗ／ｏＫｎｏｗｌｅｄｇｅＲｅａｓｏｎｉｎｇ）
的设置中
，


ＰＫＧＮ模型直接拼接问题和首条生成知识
，
并连同候选答案输入到
ＧＰＴ
－２中进


行预测打分
，模型性能在两个数据集上的下降幅度分别为
０
．９％和
１
．７％
，这证明


了该模块采用的问题知识拼接策略和文本匹配模型能较好地理解并筛选对答案


预测有助益的有效知识
。


表
４
－６ＰＫＧＮ模型的组件消融实验结果


消融组件ＣｏｍｍｏｎｓｅｎｓｅＱＡＯｐｅｎｂｏｏｋＱＡ


ＰＫＧＮ（ＧＰＴ
－２Ｍｅｄｉｕｍ）３３３３７５


ｗ／ｏＵｎｓｕｐ
．ＣｏｎｔｒａｓｔｉｖｅＬｅａｒｎｉｎｇ３０
．７（２
．６１）３４
．１（３
．７
＞１）


ｗ／ｏＧｅｎｅｒ
ａｔ
ｉｖｅＫｎｏｗｌｅｄｇｅＰｒｏｍｐ
ｔｉｎｇ２９２（４
．１
ｉ）２８
．３（９
．５ｉ）


ｗ／ｏＫｎｏｗｌｅｄｇｅＲｅａｓｏｎｉｎｇ３２
．４（０
．９ｉ
）３６
．１（１
．７ｉ
）


４
．３
．６模版敏感度分析


本小节对
ＰＫＧＮ模型中的预定义模版设置多种扰动策略
，
从而测试模型在


不同攻击力度下的鲁棒性与泛化能力
。扰动方式可分为四种类型
，分别是
：
改述


模版指令
（ＩｎｓｔｒｕｃｔｉｏｎＰａｒａｐｈｒａｓｅ）
、调换模版指令与示例顺序
（ＯｒｄｅｒＥｘｃｈａｎｇｅ）
、


同域随机替换模版示例
（Ｉｎ
－ｄｏｍａｉｎＲｅｐ
ｌａｃｅｍｅｎｔ）
、跨域随机替换模版示例
（Ｃｒｏｓｓ
？


ｄｏｍａｉｎＲｅｐ
ｌａｃｅｍｅｎｔ）。为避免偶然性与数据偏差
，在每个扰动设置下
，ＰＫＧＮ基


于ＧＰＴ
－２Ｍｅｄｉｕｍ的配置在ＣｏｍｍｏｎｓｅｎｓｅＱＡ数据集上进行
３次重复实验并取得


平均值进行报道
，
实验结果如表４
－７所示
。


表４
－７ＰＫＧＮ模型的模版扰动分析


＃扰动策略模型性能操作说明


０Ｎｏｎｅ３３．３
－


１Ｉｎｓｔｒｕｃｔ
ｉｏｎＰａｒａｐｈｒａｓｅ３２
．９（０
．４１
）对指令的动词或关键名词进行同义替换


２ＯｒｄｅｒＥｘｃｈａｎｇｅ３２
．
１（１
．２ｉ）对指令和示例进行前后调换


从ＣｏｍｍｏｎｓｅｎｓｅＱＡ中随机选取


３Ｉｎ
－ｄｏｍａｉｎＲｅｐ
ｌａｃｅｍｅｎｔ３３
．０（０
．３４
．
）八
一
姑；


一个不例进订替换


，从ＯｐｅｎｂｏｏｋＱＡ中随机选取


４Ｃｒｏｓｓ
－ｄｏｍａｉｎＲｅｐｌａｃｅｍｅｎｔ３０
．４（２
．９４〇
Ａ
—
—
，
、…
－林
…





一个不例进行替换


北京邮电大学工学硕士学位论文


由此可见
，
在改述模版指令和同域随机替换模版示例扰动下
，模型的性能基


本与原来保持
一致
，说明模型对于指示词或示例的扰动攻击是比较鲁棒的
，知识


生成的质量与方向并不会因此而改变
；在对指令和示例进行前后调换设置中
，模


型在合理范围出现了１
．２％
的小幅下降
，这可能是因为指令的重要性在顺序调换


的过程中被滞后了
，
因而生成模型对于指令本身语义的理解水平下降
，从而导致


一定的性能折损
；在跨域随机替换模版示例的设置中
，模型迎来了２
．９％
的显著


性能下降
，
因为两个基准数据集侧重的常识知识类型并不相同
，模型在借助模版


进行提示学习的过程中便会因为领域间的数据分布偏差而导致知识的生成质量


下降
。


４
．３
．７案例分析


表
４
－８展示了所提出的
ＰＫＧＮ模型和原始基线针对ＣｏｍｍｏｎｓｅｎｓｅＱＡ数据


集上的相同案例所做出的答案预测情况
，从而反映模型生成的知识质量以及所带


来的性能增益
。其中
，前两个示例为ＰＫＧＮ模型预测正确
，而原始基线预测错误


的情况
。在第
一个例子中
，
原始基线给出的错误预测Ｃ（
“
ｆａｌＵｎｇｄｏｗｎ
”
）与问


题中的
“
ｒ
ｉｄｉｎｇａｂｉｋｅ
” 有
一定的因果关系
，而ＰＫＧＮ生成的知识片段
“
ｒｅｄｕｃｅｔｈｅ


ｓｔｒｅｎｇｔｈ
” 却很好地注意到了问题的约束
“
ｆｏｒａ
ｌｏｎｇｔｉｍｅ
”
，进而以较高的置信度


选择了正确答案
Ｂ（
“
ｆａｔ
ｉｇｕｅ
”
）
。
对于第二个案例
，
原始基线选择错误答案Ａ


（
“
ｃｒｉｍｅ
”
）
的原因可能是问题中
“
ｃｏｍｍｉｔｔｉｎｇ
” 与当前选项所组成的搭配的共


现频率过高
，
而ＰＫＧＮ则借助生成语料所含的
“
ｎｏｒｅｌｉａｎｃｅ
” 背景知识
，
进
一步


理解了问题的条件限制
‘
＞却１１１７
．
．
．１１１＾１
１
〇３１１１
”
，从而做出了正确的预测０（
“
１匕
’ ’
）
。


后两个示例则展示了ＰＫＧＮ和原始基线均预测正确的情况
，
但ＰＫＧＮ模型


均在原始基线的基础上有了更高置信度的评分
，说明了当前模型产生的知识描述


的确能在
一定程度上为常识推理带来正向收益
。直观来看
，借助提示模版生成可


靠知识的方式上能将常识推理从隐式的黑盒模型转化为显式的推理程序
，在释义


归纳
、溯因判断等常识理解层面赋予了更有益推理的相关知识线索
，从而纠正模


型在预训练阶段带来的先验偏置
，并通过有效地知识整合来引导模型朝着正确答


案的方向进行合理预测
。


６２


第四章基于提示型知识生成的常识问答算法研宄


表
４
－８ＰＫＧＮ模型在ＣｏｍｍｏｎｓｅｎｓｅＱＡ数据集上的案例分析


案例
：
候选答案／生成知识ＧＰＴ
－２ＰＫＧＮ


Ｑ１
：Ｒｉｄｉｎｇａｂｉｋｅｆｏｒａｌｏｎｇｔｉｍｅｃａｎｃａｕｓｅｗｈａｔ？


Ａ
．ｅｎｊｏｙｍｅｎｔＢｉａｔｉｇｕｅＣ
．ｆａｌｌｉｎｇｄｏｗｎＤ
．ｇｅｔｔｉｎｇ
ｌｏｓｔＥ
．ｔｈｉｒｓｔＣ
．（ｘ）Ｂ
．
（Ｖ）


Ｋｎｏｗｌｅｄｇｅ
：ｈｉｇｈ
－
ｉｎｔｅｎｓｉｔｙｏｕｔｄｏｏｒｗｏｒｋｏｕｔｓｒｅｄｕｃｅｔｈｅＳｃｏｒｅ
：０
．６９Ｓｃｏｒｅ
：０
．７３


ｓｔｒｅｎｇｔｈａｎｄｅｎｅｒｇｙ
．


Ｑ２
：Ｉｆ
ｙｏｕａｒｅｃｏｍｍｉｔｔｉｎｇｐｅｉｊｕｒｙｙｏｕｈａｖｅｄｏｎｅｗｈａｔｗｈｉｌｅ


ｕｎｄｅｒｏａｔｈ？Ａ
．（ｘ
）Ｄ
．（ｖ
）


Ａ

．ｃｒｉｍｅＢ
．ｄｉｓｒｅｓｐｅｃｔ
ｊｕｄｇｅＣ
．ｅｍｂａｒｒａｓｓｍｅｎｔＤＪｉｅＥ

．ｉｎｄｉｃｔｍｅｎｔＳｃｏｒｅ
：０
．５２Ｓｃｏｒｅ
：０
．８１


Ｋｎｏｗｌｅｄｇｅ；ｐｅｉｊｕｒｙｃｒｉｍｅｈａｓｎｏｒｅｌｉａｎｃｅｔｏｂｅ
ｐ
ｌａｃｅｄ
．


Ｑ３
：ＰｉｅｃｅｏｆｌａｎｄｉｎＣａｎａｄａｗｈｅｒｅ
ｙｏｕｃａｎｆ
ｉｎｄｍａｒｍｏｔ？


Ａ
．ＮｏｒｔｈＡｍｅｒｉｃａＢ
．ＵｎｉｔｅｄＳｔａｔｅｓＣ＞ＶａｎｃｏｕｖｅｒＩｓｌａｎｄ
，


ｃ
ｃ
．
（Ｖ）Ｃ
．
（Ｖ）


Ｄ
．ＡｍｅｎｃａｎＥ
．ｃａｇｅ


Ｓｃｏｒｅ
：０
．４７Ｓｃｏｒｅ
：０
．６５


Ｋｎｏｗｌｅｄｇｅ
：ｔｈｅＶａｎｃｏｕｖｅｒｍａｒｍｏｔｓａｒｅｒａｒｅａｎｉｍａｌｓｉｎＣａｎａｄａ


ｄｓｏｉｎｆ
ｌｉｅｗｏｒｌｄ
．


Ｑ４
：Ｈｅｉｓｈｏｕｓｅｗａｓａｍｅｓｓ
，ｈｅｂｅｇａｎｄｏｉｎｇｈｏｕｓｅｗｏｒｋｔｏ
ｇｅｔ


ｗｈａｔ？


Ａ
．ｂｏｒｅｄｏｍＢ
．ｎｉｃｅｈｏｍｅＣ
．ＭｉｃｈｉｇａｎＤｉｅｅｌｉｎｇｓａｔｉｓｆｉｅｄＥＪｉｏｕｓｅＥ
．（Ｖ）Ｅ
．
ｆＶ）


ｃｌｅａｎ
Ｓｃｏｒｅ
：０
．５８Ｓｃｏｒｅ
：０
．９８


Ｋｎｏｗｌｅｄｇｅ：ｈｏｕｓｅｗｏｒｋｉｎｖｏｌｖｅｓｃｌｅａｎｉｎｇａｎｄｃｏｏｋｉｎｇ
ｉｎｍｏｓｔ


ｆａｍｉｌｉｅｓ


４
．４本章小结


本章在常识问答任务的无监督方向上
，提出了
一种基于提示型知识生成的模


型框架
（ＰＫＧＮ）
。在开始部分
，
本章从现有方法在泛化性及任务迁移方面存在


缺陷的角度出发
，
给出了ＰＫＧＮ模型的设计动机及对应的解决思路
，
并对模型


的主体处理流程进行了概述
。
随后
，
本章详细介绍了ＰＫＧＮ各个功能模块的实


现细节
，模型是首先采用基于Ｄｒｏｐｏｕｔ增强的对比学习策略完成了对常识问题的


继续预训练
；接着
，模型利用所设计的提示模版进
一步生成
一系列问题相关的知


识描述
；最终
，模型借助文本匹配模型对带有知识的问题和候选项之间进行置信


度评分与答案预测
。
在最后的实验分析部分
，
本章所提出的
ＰＫＧＮ模型在三个


６３


北京邮电大学工学硕士学位论文


基准常识数据集上进行了大量实验
，其性能指标均强于目前的基线方法
，并从组


件消融
、
模版敏感度分析
、
以及案例分析等角度验证了ＰＫＧＮ模型的有效性与


泛化能力
，
充分证实了ＰＫＧＮ模型在无监督常识问答任务上的优越性能表现
。


６４


第五章
总结与展望


第五章总结与展望


５
．
１工作总结


常识问答任务以知识为驱动核心
，
旨在研宄如何获取关联知识
，并通过分析


问题语义和执行知识推理来获取精准定位的答案
，在智能客服
、搜索引擎
、语音


助手等工业领域具有广泛的应用空间
。本文聚焦于常识问答任务的有监督和无监


督方向
，并针对各自存在的问题挑战给出相应的解决方案
，相继提出了基于知识


增强型图对比学习的模型方法
（ＫＥ
－ＧＣＬ
）
和基于提示型知识生成的模型方法


（ＰＫＧＮ）〇


在有监督场景下
，
当前的研宄工作集中于设计复杂的知识推理结构
，而忽视


知识源覆盖不足和知识噪声问题
。知识源覆盖不足在于单
一外部知识源的引入可


能遗失了部分与正确答案相关的重要信息来源
，模型在推理过程会因缺失完整的


“ 证据链
” 而致使推断偏差与性能折损
。此外
，
知识噪声则是在于忽视对知识源


本身的过滤处理
，在结构化知识图谱中
，
一些无关或干扰性的实体节点与关系边


均可能会阻碍模型的知识推理过程
。因此
，本文有针对性地采用了
一种多源知识


増强的方法
，在引入结构化常识知识图谱的基础上
，通过在语言模型侧增加有关


问答对相关实体的维基词典描述以捕捉不同实体之间的细微语义差别
，能够增强


模型在图谱推理过程中对不同实体差异的细粒度感知
。为缓解知识噪声
，本文设


计了
一种基于自适应采样增强的图对比学习策略
，从拓扑连通度和语义关联性两


个角度定义重要度
，并对图谱中的节点与边进行带权采样
，最后通过训练批次中


的正负图例构建完成有监督的对比学习目标
。


在无监督场景下
，常识问答任务无法使用带答案标签的数据
，当前的研宄方


法使用预设模版引导生成模型输出特定类型的知识
，并结合评分函数完成答案的


置信度排序及预测
。这类方法产生的知识类型有限
，且难以自适应地迁移到新的


数据领域
。故而
，本文侧重于通用性和泛化性的角度提出了
一种由提示学习和对


比学习结合的知识生成框架
。利用Ｄｒｏｐｏｕｔ策略进行问题的数据增强
，继续预训


练的对比学习目标能让模型捕捉更为精细的问题语义表示
，进而能在知识生成阶


段借助提示模版生成更差异化的高质量知识描述
，最后通过文本匹配模型完成知


识推理
。该框架灵活性与泛化能力强
，能以较小的代价迁移到多种不同领域的任


务中
。


综上所述
，
本文研究工作的主要贡献总结如下
：


（
１
）在常识问答研宄中首次针对知识子图采用了图对比学习的训练策略
。


６５


北京邮电大学工学硕士学位论文


（２）提出了
一种基于知识增强的端到端图对比学习有监督常识问答算法
。


（３
）提出了
一种基于提示型知识生成的无监督常识问答的通用算法框架
。


５
．２未来工作展望


尽管本文针对常识问答任务的有监督和无监督两个方向存在的挑战开展了


详细的分析研宄
，并取得了
一定的工作成果
。但经过进
一步的数据观测与思考总


结后发现
，
在组件结构和的训练方式上仍有部分内容有待深入探索和挖掘
：


（
１
）在有监督方向
，本文所提出的
ＫＥ
－ＧＣＬ模型虽然利用图对比学习能够


在
一定程度上缓解外部知识源的噪声
，但是该方法在实际进行端到端的训练过程


中
，损失函数的计算与反向传播过程需要借助Ｍｉｎｉ
－ｂａｔｃｈ的梯度累积来实现
，
因


而当前策略会较为依赖显存计算资源
，这在
一定程度上也限制了模型的性能上限
，


未来可以考虑结合模型剪枝或蒸馏的思想
，从降低网络参数的角度规避图对比学


习带来的显存依赖
。同时
，
当前模型针对带有否定语义关系或数值比较类的常识


问题预测仍然存在
一定欠缺
，可能的思路是持续优化图推理模块或在语言模型端


考虑加入极性判断的约束
。


（２
）在无监督方向
，
本文所提出的ＰＫＧＮ模型利用提示模版生成常识问题


相关的知识描述
，但是为了保证训练框架的泛化性
，本文所构造的示例是从对应


的常识问答数据集中随机采样得到的
，存在
一定的不确定性
，后续可以考虑在采


样过程中加入
一些通用规则判断或选择提示
。
同时
，本文所选取的文本匹配模型


是在文本蕴含任务上进行了微调
，与常识问答任务之间仍然存在
一定的领域鸿沟
，


未来可以考虑设计在继续预训练阶段设计相关的对齐策略以进
一步弥合差异
。


常识问答作为自然语言领域极具应用前景的研究方向之
一
，其仍然是
一个充


满未知并且极具挑战性的任务
，
需要持续摸索与勇敢试错
，
“ 行而不辍
，未来可


期
，、


６６


参考文献


参考文献


［
１
］
王守会
，覃飙
．知识库问答系统研究进展
［Ｊ
］
．小型微型计算机系


统，２０２１
，４２
（０９
）
：
１７９３
－
１８０１
．


［２
］ＴａｌｍｏｒＡ
，ＨｅｒｚｉｇＪ
？ＬｏｕｒｉｅＮ
？ｅｔａｌ．ＣｏｍｍｏｎｓｅｎｓｅＱＡ
：ＡＱｕｅｓｔｉｏｎＡｎｓｗｅｒｉｎｇ


ＣｈａｌｌｅｎｇｅＴａｒｇｅｔｉｎｇＣｏｍｍｏｎｓｅｎｓｅＫｎｏｗｌｅｄｇｅ
［Ｃ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２０１９


ＣｏｎｆｅｒｅｎｃｅｏｆｔｈｅＮｏｒｔｈＡｍｅｒｉｃａｎＣｈａｐ
ｔｅｒｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌ


Ｌｉｎｇｕｉｓｔｉｃｓ
：ＨｕｍａｎＬａｎｇｕａｇｅＴｅｃｈｎｏｌｏｇ
ｉｅｓ
，Ｖｏｌｕｍｅ１（ＬｏｎｇａｎｄＳｈｏｒｔＰａｐｅｒｓ）
．


２０
１９
：
４１４９
－４１５８
．


［３
］Ｍｉｈａｙ
ｌｏｖＴ
５ＣｌａｒｋＰ
？ＫｈｏｔＴ
？ｅｔａｌ
．ＣａｎａＳｕｉｔｏｆＡｒｍｏｒＣｏｎｄｕｃｔＥｌｅｃｔｒｉｃｉｔｙ？Ａ


ＮｅｗＤａｔａｓｅｔｆｏｒＯｐｅｎＢｏｏｋＱｕｅｓｔｉｏｎＡｎｓｗｅｒｉｎｇ［Ｃ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２０１８


ＣｏｎｆｅｒｅｎｃｅｏｎＥｍｐ
ｉｒｉｃａｌＭｅｔｈｏｄｓｉｎＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ
．２０１８
：２３８１
－


２３９１
．


［４
］ＳａｐＭ
５ＲａｓｈｋｉｎＨ
，ＣｈｅｎＤ
，ｅｔａｌ
．ＳｏｃｉａｌＩＱａ
：ＣｏｍｍｏｎｓｅｎｓｅＲｅａｓｏｎｉｎｇａｂｏｕｔ


ＳｏｃｉａｌＩｎｔｅｒａｃｔｉｏｎｓ
［Ｃ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆ
ｔｈｅ２０１９ＣｏｎｆｅｒｅｎｃｅｏｎＥｍｐ
ｉｒｉｃａｌＭｅｔｈｏｄｓ


ｉｎＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇａｎｄｔｈｅ９ｔｈＩｎｔｅｒ
ｎａｔｉｏｎａｌＪｏｉｎｔＣｏｎｆｅｒｅｎｃｅｏｎ


ＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ（ＥＭＮＬＰ
－ＩＪＣＮＬＰ）
．２０１９
：４４６３
－４４７３
．


［５］ＨｕａｎｇＬ
，ＬｅＢｒａｓＲ
，ＢｈａｇａｖａｔｕｌａＣ
５ｅｔａｌ
．ＣｏｓｍｏｓＱＡ
；ＭａｃｈｉｎｅＲｅａｄｉｎｇ


ＣｏｍｐｒｅｈｅｎｓｉｏｎｗｉｔｈＣｏｎｔｅｘｔｕａｌＣｏｍｍｏｎｓｅｎｓｅＲｅａｓｏｎｉｎｇ［Ｃ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆ
ｔｈｅ


２０１９ＣｏｎｆｅｒａｉｃｅｏｎＥｍｐ
ｉｒｉｃａｌＭｅｔｈｏｄｓｉｎＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇａｎｄｔｈｅ


９ｔｈＩｎｔｅｒ
ｎａｔｉｏｎａｌＪｏｉｎｔＣｏｎｆｅｒｅｎｃｅｏｎＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ（ＥＭＮＬＰ
－


ＩＪＣＮＬＰ
）
．２０１９
：
２３９１
－２４０１
．


［６
］ＬｉｎＢＹ
５ＬｅｅＳ
？ＫｈａｎｎａＲ
？ｅｔａｌ
．Ｂｉｒｄｓｈａｖｅｆｏｕｒｌｅｇｓ？
！ＮｕｍｅｒＳｅｎｓｅ
：Ｐｒｏｂｉｎｇ


ＮｕｍｅｒｉｃａｌＣｏｍｍｏｎｓｅｎｓｅＫｎｏｗｌｅｄｇｅｏｆＰｒｅ
－ＴｒａｉｎｅｄＬａｎｇｕａｇｅ


Ｍｏｄｅｌｓ
［Ｃ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２０２ＧＣｏｎｆｅｒｅｎｃｅｏｎＥｍｐ
ｉｒ
ｉｃａｌＭｅｔｈｏｄｓｉｎＮａｔｕｒａｌ


ＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ（ＥＭＮＬＰ）
．２０２０
：６８６２
－６８６８
．


［７
］ＳｕｃｈａｎｅｋＦＭ
，
ＫａｓｎｅｃｉＧ
，
ＷｅｉｋｕｍＧ
．Ｙａｇｏ
：ａｃｏｒｅｏｆｓｅｍａｎｔｉｃ


ｋｎｏｗｌｅｄｇｅ
［Ｃ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ１６ｔｈｉｎｔｅｒ
ｎａｔｉｏｎａｌｃｏｎｆｅｒｅｎｃｅｏｎＷｏｒｌｄＷｉｄｅ


Ｗｅｂ
．２００７
：６９７
－７０６
．


［８
］ＥｔｚｉｏｎｉＯ
，ＢａｎｋｏＭ
，ＳｏｄｅｒｌａｎｄＳ
，ｅｔａｌ
．Ｏｐｅｎｉｎｆｏｒｍａｔｉｏｎｅｘｔｒａｃｔｉｏｎｆｒｏｍｔｈｅ


ｗｅｂ
［Ｊ
］
．ＣｏｍｍｕｎｉｃａｔｉｏｎｓｏｆｔｈｅＡＣＭ
，２００８
，５１
（１２
）
：６８
－７４
．


６７


北京邮电大学工学硕士学位论文


［９
］ＣａｒｌｓｏｎＡ
，ＢｅｔｔｅｒｉｄｇｅＪ
？ＫｉｓｉｅｌＢ
，ｅｔａｌ
．Ｔｏｗａｒｄａｎａｒｃｈｉｔｅｃｔｕｒｅｆｏｒｎｅｖｅｒ
－ｅｎｄｉｎｇ


ｌａｎｇｕａｇｅｌｅａｍｉｎｇ［Ｃ］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＡＡＡＩｃｏｎｆｅｒｅｎｃｅｏｎａｒｔｉｆｉｃｉａｌ


ｉｎｔｅｌｌｉｇｅｎｃｅ
．２０１０
，２４
（
１
）
：１３０６
－１３
１３
．


［
１０
］ＡｕｅｒＳ
５ＢｉｚｅｒＣ
，ＫｏｂｉｌａｒｏｖＧ
５ｅｔａｌ
．Ｄｂｐｅｄｉａ
：Ａｎｕｃｌｅｕｓｆｏｒａｗｅｂｏｆｏｐｅｎ


ｄａｔａ
［Ｃ
］
／／ＴｈｅＳｅｍａｎｔｉｃＷｅｂ
：６ｔｈＩｎｔｅｒ
ｎａｔｉｏｎａｌＳｅｍａｎｔｉｃＷｅｂＣｏｎｆｅｒｅｎｃｅ
，２ｎｄ


ＡｓｉａｎＳｅｍａｎｔｉｃＷｅｂＣｏｎｆｅｒｅｎｃｅ
，ＩＳＷＣ２００７＋ＡＳＷＣ２００７，Ｂｕｓａｎ
，Ｋｏｒｅａ，


Ｎｏｖｅｍｂｅｒ１
１
－
１５
，２００７
．Ｐｒｏｃｅｅｄｉｎｇｓ
．ＳｐｒｉｎｇｅｒＢｅｒｌｉｎＨｅｉｄｅｌｂｅｒｇ，２００７
：７２２
－７３５
．


［
１ｌ
］ＢｏｌｌａｃｋｅｒＫ
，ＥｖａｎｓＣ
５ＰａｒｉｔｏｓｈＰ
５ｅｔａｌ
．Ｆｒｅｅｂａｓｅ
：ａｃｏｌｌａｂｏｒａｔｉｖｅｌｙｃｒｅａｔｅｄ
ｇｒａｐｈ


ｄａｔａｂａｓｅｆｏｒｓｔｒｕｃｔｕｒｉｎｇｈｕｍａｎｋｎｏｗｌｅｄｇｅ
［Ｃ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２００８ＡＣＭ


ＳＩＧＭＯＤｉｎｔｅｒ
ｎａｔｉｏｎａｌｃｏｎｆｅｒｅｎｃｅｏａＭａｎａｇｅｍｅｎｔｏｆｄａｔａ．２００８
：１２４７
－１２５０
．


［
１２
］ＴａｎｄｏｎＮ
５ＤｅＭｅｌｏＧ
５ＳｕｃｈａｎｅｋＦ
？ｅｔａｌ
．Ｗｅｂｃｈｉｌｄ
：Ｈａｒｖｅｓｔｉｎｇａｎｄｏｒｇａｎｉｚｉｎｇ


ｃｏｍｍｏｎｓｅｎｓｅｋｎｏｗｌｅｄｇｅｆ
ｒｏｍｔｈｅｗｅｂ
［Ｃ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ７ｔｈＡＣＭ


ｉｎｔｅｒ
ｎａｔｉｏｎａｌｃｏｎｆｅｒｅｎｃｅｏｎＷｅｂｓｅａｒｃｈａｎｄｄａｔａｍｉｎｉｎｇ
．２０１４
：５２３
－５３２
．


［
１３
］ＬｉｕＨ
，ＳｉｎｇｈＰ．Ｃｏｎｃｅｐ
ｔＮｅｔ
￣ａ
ｐｒａｃｔｉｃａｌｃｏｍｍｏｎｓｅｎｓｅｒｅａｓｏｎｉｎｇｔｏｏｌ
－ｋｉｔ
［Ｊ
］
．ＢＴ


ｔｅｃｈｎｏｌｏｇｙｊｏｕｒ
ｎａｌ
，２００４
，２２（４）
：２１
１
－２２６
．


［
１４
］ＬａｎＹ
，ＨｅＧ
？ＪｉａｎｇＪ
，ｅｔａｌ
．Ａｓｕｒｖｅｙｏｎｃｏｍｐ
ｌｅｘｋｎｏｗｌｅｄｇｅｂａｓｅｑｕｅｓｔｉｏｎ


ａｎｓｗｅｒｉｎｇ
：Ｍｅｔｈｏｄｓ
，ｃｈａｌｌｅｎｇｅｓａｎｄｓｏｌｕｔ
ｉｏｎｓ［Ｊ
］
．ａｒＸｉｖ
ｐｒｅｐｒ
ｉｎｔａｒＸｉｖ
：２１０５
．
１
１６４４
，


２０２１
．


［
１５
］ＤｕＣｈａｒｍｅＢ
．ＬｅａｒｎｉｎｇＳＰＡＲＱＬ
：ｑｕｅｒｙ
ｉｎｇａｎｄｕｐｄａｔｉｎｇｗｉｔｈＳＰＡＲＱＬ１
．
１
［Ｍ
］
．
Ｍ


Ｏ
＇ＲｅｉｌｌｙＭｅｄｉａ
，Ｉｎｃ
．
Ｍ
？２０
１３
．


［
１６
］ＲｅｄｄｙＳ
？ＬａｐａｔａＭ
，ＳｔｅｅｄｍａｎＭ．Ｌａｒｇｅ
－ｓｃａｌｅｓｅｍａｎｔｉｃ
ｐａｒｓｉｎｇｗｉｔｈｏｕｔ
ｑｕｅｓｔｉｏｎ
－


ａｎｓｗｅｒｐａｉｒｓ［Ｊ
］
．Ｔｒａｎｓａｃｔ
ｉｏｎｓｏｆｔｈｅＡｓｓｏｃｉａｔ
ｉｏｎｆｏｒＣｏｍｐｕｔａｔ
ｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ，


２０１４
，
２
：３７７
－３９２
．


［１７
］ＳａｎｔｏｒｏＡ
，ＲａｐｏｓｏＤ
，
ＢａｒｒｅｔＤＧ
，
ｅｔａｌ
．Ａｓｉｍｐ
ｌｅｎｅｕｒａｌｎｅｔｗｏｒｋｍｏｄｕｌｅｆｏｒ


ｒｅｌａｔｉｏｎａｌｒｅａｓｏｎｉｎｇ［Ｊ］
．Ａｄｖａｎｃｅｓｉｎｎｅｕｒａｌｉｎｆｏｒｍａｔｉｏｎ
ｐｒｏｃｅｓｓｉｎｇｓｙｓｔｅｍｓ
，２０１７
，


３０
．


［
１８
］ＳｃｈｌｉｃｈｔｋｒｕｌｌＭ，ＫｉｐｆＴＮ
，ＢｌｏｅｍＰ
５ｅｔａｌ
．ＭｏｄｅｌｉｎｇＲｅｌａｔｉｏｎａｌＤａｔａｗｉｔｈＧｒａｐｈ


ＣｏｎｖｏｌｕｔｉｏｎａｌＮｅｔｗｏｒｋｓ
［Ｃ
］／／１５ｔｈＩｎｔｅｒ
ｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＥｘｔｅｎｄｅｄＳｅｍａｎｔｉｃ


ＷｅｂＣｏｎｆｅｒｅｎｃｅ
，ＥＳＷＣ２０１８
．ＳｐｒｉｎｇｅｒＡ／ｅｒｌａｇ，２０１８
：５９３
－６０７
．


［１９
］ＬｉｎＢＹ
，ＣｈｅｎＸ
？ＣｈｅｎＪ
，ｅｔａｌ
．ＫａｇＮｅｔ：Ｋｎｏｗｌｅｄｇｅ
－ＡｗａｒｅＧｒａｐｈＮｅｔｗｏｒｋｓｆｏｒ


ＣｏｍｍｏｎｓｅｎｓｅＲｅａｓｏｎｉｎｇ［Ｃ
］
／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２０１９ＣｏｎｆｅｒｅｎｃｅｏｎＥｍｐ
ｉｒｉｃａｌ


ＭｅｔｈｏｄｓｉｎＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇａｎｄｔｈｅ９ｔｈＩｎｔｅｒ
ｎａｔｉｏｎａｌＪｏｉｎｔ


ＣｏｎｆｅｒｅｎｃｅｏｎＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ（ＥＭＮＬＰ
－ＩＪＣＮＬＰ
）
．２０１９
：２８２９
－２８３９
．


６８


参考文献


［２０
］ＷａｎｇＸ
５ＫａｐａｎｉｐａｔｈｉＰ
５ＭｕｓａＲ
，ｅｔａｌ
．Ｉｍｐｒｏｖｉｎｇｎａｔｕｒａｌｌａｎｇｕａｇｅｉｎｆｅｒｅｎｃｅｕｓｉｎｇ


ｅｘｔｅｒ
ｎａｌｋｎｏｗｌｅｄｇｅｉｎｔ
ｉｉｅｓｃｉｅｎｃｅ
ｑｕｅｓｔｉｏｎｓｄｏｍａｉｎ
［Ｃ
］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＡＡＡＩ


ＣｏｎｆｅｒｅｎｃｅｏｎＡｒｔｉｆｉｃｉａｌＩｎｔｅｌｌｉｇｅｎｃｅ
．２０１９
，３３
（０１
）
：７２０８
－７２
１５
．


［２
１
］ＦｅｎｇＹ
？ＣｈｅｎＸ
５ＬｉｎＢＹ
？ｅｔａｌ
．ＳｃａｌａｂｌｅＭｕｌｔｉ
－ＨｏｐＲｅｌａｔｉｏｎａｌＲｅａｓｏｎｉｎｇｆｏｒ


Ｋｎｏｗｌｅｄｇｅ
－ＡｗａｒｅＱｕｅｓｔｉｏｎＡｎｓｗｅｒｉｎｇ［Ｃ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２０２０Ｃｏｎｆｅｒｅｎｃｅ


ｏｎＥｍｐ
ｉｒｉｃａｌＭｅｔｈｏｄｓｉｎＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ（ＥＭＮＬＰ
）
．２０２０
：１２９５
－


１３０９
．


［２２
］ＹａｓｕｎａｇａＭ
，ＲｅｎＨ
，ＢｏｓｓｅｌｕｔＡ
５ｅｔａｌ
．ＱＡ
－ＧＮＮ
：ＲｅａｓｏｎｉｎｇｗｉｔｈＬａｎｇｕａｇｅ


ＭｏｄｅｌｓａｎｄＫｎｏｗｌｅｄｇｅＧｒａｐｈｓｆｏｒＱｕｅｓｔｉｏｎＡｎｓｗｅｒｉｎｇ［Ｃ
］
／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ


２０２１ＣｏｎｆｅｒｅｎｃｅｏｆｔｈｅＮｏｒｔｈＡｍｅｒｉｃａｎＣｈａｐｔｅｒｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒ


ＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ
：ＨｕｍａｎＬａｎｇｕａｇｅＴｅｃｈｎｏｌｏｇ
ｉｅｓ
．２０２
１
：５３５
－５４６
．


［２３
］ＧａｏＴ
，ＹａｏＸ
？ＣｈｅｎＤ
．ＳｉｍＣＳＥ
：Ｓｉｍｐ
ｌｅＣｏｎｔｒａｓｔｉｖｅＬｅａｒｎｉｎｇｏｆＳｅｎｔｅｎｃｅ


Ｅｍｂｅｄｄｉｎｇｓ［Ｃ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２０２１ＣｏｎｆｅｒｅｎｃｅｏｎＥｍｐ
ｉｒｉｃａｌＭｅｔｈｏｄｓｉｎ


ＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ
．２０２１
：６８９４
－６９１０
．


［２４
］ＵｎｇｅｒＣ
，ＢｉｉｈｍａｎｎＬ
，ＬｅｈｍａｎｎＪ
？ｅｔａｌ
．Ｔｅｍｐ
ｌａｔｅ
－ｂａｓｅｄ
ｑｕｅｓｔｉｏｎａｎｓｗｅｒｉｎｇｏｖｅｒ


ＲＤＦｄａｔａ
［Ｃ
］
／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆ
ｔｈｅ２１ｓｔｉｎｔｅｍａｔｉｏｎａｌｃｏｎｆｅｒｅｎｃｅｏｎＷｏｒｌｄＷｉｄｅＷｅｂ
．


２０１２
：６３９
－６４８
．


ＳＷ
，ＣｈａｎｇＭＷ
，ＨｅＸ
，
ｅｔａＬＳｅｍａｎｔ
ｉｃｐａｒｓｉｎｇｖｉａｓｔａｇｅｄｑｕｅｒｙｇｒａｐｈ


ｇｅｎｅｒａｔｉｏｎ
：Ｑｕｅｓｔｉｏｎａｎｓｗｅｒｉｎｇｗｉｔｈｋｎｏｗｌｅｄｇｅｂａｓｅ
［Ｃ
］
／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆ
ｔｈｅＪｏｉｎｔ


Ｃｏｎｆｅｒｅｎｃｅｏｆｔｈｅ５３ｒｄＡｎｎｕａｌＭｅｅｔｉｎｇｏｆ
ｔｈｅＡＣＬａｎｄｔｈｅ７ｔｈＩｎｔｅｍａｔｉｏｎａｌＪｏｉｎｔ


ＣｏｎｆｅｒｅｎｃｅｏｎＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇｏｆｔｈｅＡＦＮＬＰ．２０１５
．


［２６
］ＢｕｒｇｅｓＣ，ＦｒｏｍＲａｎｋＮｅｔｔｏＬａｍｂｄａＲａｎｋｔｏＬａｍｂｄａＭＡＲＴ：ＡｎＯｖｅｒｖｉｅｗ
［Ｊ
］
．


ｌｅａｒｎｉｎｇ，２０１０
．


［２７
］ＨｕＳ
５ＺｏｕＬ
？ＺｈａｎｇＸ
．Ａｓｔａｔｅ
－ｔｒａｎｓｉｔｉｏｎｆ
ｒａｍｅｗｏｒｋｔｏａｎｓｗｅｒｃｏｍｐ
ｌｅｘ
ｑｕｅｓｔｉｏｎｓ


ｏｖｅｒｋｎｏｗｌｅｄｇｅｂａｓｅ［Ｃ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２０１８ｃｏｎｆｅｒｅｎｃｅｏｎｅｍｐ
ｉｒｉｃａｌ


ｍｅｔｈｏｄｓｉｎｎａｔｕｒａｌｌａｎｇｕａｇｅ
ｐｒｏｃｅｓｓｉｎｇ
．２０１８
：２０９８
－２１０８
．


［２８
］ＤｏｎｇＬ
，ＬａｐａｔａＭ．ＬａｎｇｕａｇｅｔｏＬｏｇ
ｉｃａｌＦｏｒｍｗｉｔｈＮｅｕｒａｌ


Ａｔｔｅｎｔｉｏｎ
［Ｃ
］
／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ５４ｔｈＡｎｎｕａｌＭｅｅｔｉｎｇｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒ


ＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ
（Ｖｏｌｕｍｅ１
：ＬｏｎｇＰａｐｅｒｓ）
．２０１６
：３３
－４３
．


［２９
］ＸｕＫ
５ＷｕＬ
，ＷａｎｇＺ
，ｅｔａｌ
．Ｅｘｐ
ｌｏｉｔｉｎｇＲｉｃｈＳｙｎｔａｃｔｉｃＩｎｆｏｒｍａｔｉｏｎｆｏｒＳｅｍａｎｔｉｃ


ＰａｒｓｉｎｇｗｉｔｈＧｒａｐｈ
－ｔｏ
－ＳｅｑｕｅｎｃｅＭｏｄｅｌ
［Ｃ
］
／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２０
１８Ｃｏｎｆｅｒｅｎｃｅ


ｏｎＥｍｐ
ｉｒｉｃａｌＭｅｔｈｏｄｓｉｎＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ
．２０１８
：９
１８
－９２４
．


６９


北京邮电大学工学硕士学位论文


［３０］ＣｕｉＷ
ｓＸｉａｏＹ
５ＷａｎｇＨ
？ｅｔａｌ
．ＫＢＱＡ
：ｌｅａｒｎｉｎｇｑｕｅｓｔｉｏｎａｎｓｗｅｒｉｎｇｏｖｅｒＱＡ


ｃｏｒｐｏｒａａｎｄｋｎｏｗｌｅｄｇｅｂａｓｅｓ［Ｊ
］
．ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＶＬＤＢＥｎｄｏｗｍｅｎｔ
，２０１７
，


１０（５
）
：５６５
－５７６
．


［３
１
］ＬｉａｎｇＣ
ｓＢｅｒａｎｔＪ
ｓＬｅＱ，ｅｔａｌ
．Ｎｅｕｒａｌｓｙｍｂｏｌｉｃｍａｃｈｉｎｅｓ
：Ｌｅａｒｎｉｎｇｓｅｍａｎｔｉｃ


ｐａｒｓｅｒｓｏｎｆ
ｒｅｅｂａｓｅｗｉｔｈｗｅａｋｓｕｐｅｒｖｉｓｉｏｎ［Ｃ
］／／５５ｔｈＡｎｎｕａｌＭｅｅｔｉｎｇｏｆｔｈｅ


ＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ
，ＡＣＬ２０１７
，Ａｓｓｏｃｉａｔｉｏｎｆｏｒ


ＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ
（ＡＣＬ），２０１７
：２３
－３３
．


［３２
］ＭｃＣａｒｔｈｙＪ
．ＨｉｓｔｏｒｙｏｆＬＩＳＰ
［Ｍ
］
／／Ｈｉｓｔｏｒｙｏｆ
ｐｒｏｇｒａｍｍｉｎｇ
ｌａｎｇｕａｇｅｓ
．１９７８
：１７３
－


１８５
．


［３３
］陈子睿
５王鑫
５王林等
．开放领域知识图谱问答研究综述
［Ｊ
］
．计算机科学与探


索
，２０２１
，１５（
１０
）
：
１８４３
－
１８６９
．


１：３４
］郑泳智
，朱定局
，吴惠粦
５彭小荣
．知识图谱问答领域综述
［Ｊ］
．计算机系统应


用
５２０２２
，３
１（０４
）
：１
－１３
．００１
：１０
．１５８８８／
】
．〇１＾以＆００８４１８
．


［３５
］ＹａｏＸ
，ＶａｎＤｕｒｍｅＢ
．Ｉｎｆｏｒｍａｔｉｏｎｅｘｔｒａｃｔｉｏｎｏｖｅｒｓｔｒｕｃｔｕｒｅｄｄａｔａ：Ｑｕｅｓｔｉｏｎ


ａｎｓｗｅｒｉｎｇｗｉｔｈｊＢ
＊
ｅｅｂａｓｅ
［Ｃ
］
／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ５２ｎｄａｎｎｕａｌｍｅｅｔｉｎｇｏｆｔｈｅ


ａｓｓｏｃｉａｔｉｏｎｆｏｒｃｏｍｐｕｔａｔ
ｉｏｎａｌｌｉｎｇｕｉｓｔｉｃｓ
（ｖｏｌｕｍｅ１
：ｌｏｎｇｐａｐｅｒｓ）
．２０１４
：９５６
－９６６
．


［３６
］ＢｏｒｄｅｓＡ
，
ＣｈｏｐｒａＳ
，
ＷｅｓｔｏｎＪ
．ＱｕｅｓｔｉｏｎＡｎｓｗｅｒｉｎｇｗｉｔｈＳｕｂｇｒａｐｈ


Ｅｍｂｅｄｄｉｎｇｓ
［Ｃ
］
／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２０１４ＣｏｎｆｅｒｅｎｃｅｏｎＥｍｐ
ｉｒｉｃａｌＭｅｔｈｏｄｓｉｎ


ＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ（ＥＭＮＬＰ）
．２０１４
：６１５
－６２０
．


［３７
］ＤｏｎｇＬ
？ＷｅｉＦ
，ＺｈｏｕＭ
？ｅｔａｌ
．Ｑｕｅｓｔｉｏｎａｎｓｗｅｒｉｎｇｏｖｅｒｊｆ
ｒｅｅｂａｓｅｗｉｔｈｍｕｌｔｉ
－ｃｏｌｕｍｎ


ｃｏｎｖｏｌｕｔｉｏｎａｌｎｅｕｒａｌｎｅｔｗｏｒ
ｉｃｓ［Ｃ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆ
ｔｈｅ５３ｒｄＡｎｎｕａｌＭｅｅｔｉｎｇｏｆｔｈｅ


ＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓａｎｄｔｈｅ７ｔｈＩｎｔｅｒ
ｎａｔｉｏｎａｌＪｏｉｎｔ


ＣｏｎｆｅｒｅｎｃｅｏｎＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ（Ｖｏｌｕｍｅ１
：ＬｏｎｇＰａｐｅｒｓ）
．２０１５
；２６０
－


２６９
．


［３８
］ＢｏｒｄｅｓＡ
，ＵｓｕｎｉｅｒＮ
？ＣｈｏｐｒａＳ
，ｅｔａｌ
．Ｌａｒｇｅ
－ｓｃａｌｅｓｉｍｐ
ｌｅ
ｑｕｅｓｔ
ｉｏｎａｎｓｗｅｒｉｎｇｗｉｔｈ


ｍｅｍｏｒｙｎｅｔｗｏｒｋｓ
［Ｊ
］
．ａｒＸｉｖ
ｐｒｅｐｒｉｎｔａｒＸｉｖ
：１５０６
．０２０７５
，２０１５
．


［３９
］ＳｕｋｈｂａａｔａｒＳ？ＳｚｌａｍＡ
，ＷｅｓｔｏｎＪ，ｅｔａｌ
．ＷｅａｋｌｙＳｕｐｅｒｖｉｓｅｄＭｅｍｏｒｙ


Ｎｅｔｗｏｒｋｓ
［Ｊ］
，２０１５
．


［４０］ＣｈｅｎＹ
，ＷｕＬ
，ＺａｋｉＭＪ
．ＢｉｄｉｒｅｃｔｉｏｎａｌＡｔｔｅｎｔｉｖｅＭｅｍｏｒｙＮｅｔｗｏｒｋｓｆｏｒＱｕｅｓｔｉｏｎ


ＡｎｓｗｅｒｉｎｇｏｖｅｒＫｎｏｗｌｅｄｇｅＢａｓｅｓ
［Ｃ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆ
ｔｈｅ２０１９Ｃｏｎｆｅｒｅｎｃｅｏｆｔｈｅ


ＮｏｒｔｈＡｍｅｒｉｃａｎＣｈａｐ
ｔｅｒｏｆ
ｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ
：Ｈｕｍａｎ


ＬａｎｇｕａｇｅＴｅｃｈｎｏｌｏｇ
ｉｅｓ
，Ｖｏｌｕｍｅ１（ＬｏｎｇａｎｄＳｈｏｒｔＰａｐｅｒｓ）
．２０１９
：２９１３
－２９２３
．


７０


参考文献


［４１
］ＳａｘｅｎａＡ
，Ｔｒ
ｉｐａｔｈｉＡ
，ＴａｌｕｋｄａｒＰ．Ｉｍｐｒｏｖｉｎｇｍｕｌｔｉ
－ｈｏｐｑｕｅｓｔｉｏｎａｎｓｗｅｒｉｎｇｏｖｅｒ


ｋｎｏｗｌｅｄｇｅ
ｇｒａｐｈｓｕｓｉｎｇｋｎｏｗｌｅｄｇｅｂａｓｅｅｍｂｅｄｄｉｎｇｓ
［Ｃ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ５８ｔｈ


ａｎｎｕａｌｍｅｅｔｉｎｇｏｆｔｈｅａｓｓｏｃｉａｔｉｏｎｆｏｒｃｏｍｐｕｔａｔｉｏｎａｌｌｉｎｇｕｉｓｔｉｃｓ
．２０２０
：４４９８
－４５０７
．


［４２
］ＬｖＳ
５ＧｕｏＤ
？ＸｕＪ
，ｅｔａｌ
．Ｇｒａｐｈ
－ｂａｓｅｄｒｅａｓｏｎｉｎｇｏｖｅｒｈｅｔｅｒｏｇｅｎｅｏｕｓｅｘｔｅｒ
ｎａｌ


ｋｎｏｗｌｅｄｇｅｆｏｒｃｏｍｍｏｎｓｅｎｓｅｑｕｅｓｔｉｏｎａｎｓｗｅｒｉｎｇ［Ｃ
］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＡＡＡＩ


ｃｏｎｆｅｒｅｎｃｅｏｎａｒｔｉｆ
ｉｃｉａｌｉｎｔｅｌｌｉｇｅｎｃｅ．２０２０
，３４
（０５
）
：８４４９
－８４５６
．


［４３
］赵思洋
．基于联合训练和无监督方法的中文知识图谱问答研究ｐ］
．哈尔滨工


业大学，２０２０
．ＤＯＩ
：
１０
．２７０６１／ｄ
．ｃｎｋｉ
．ｇｈｇｄｕ．２０２０
．０００６４２
．


［４４
］ＬｅｗｉｓＰ
，ＤｅｎｏｙｅｒＬ
５ＲｉｅｄｅｌＳ
．Ｕｎｓｕｐｅｒｖｉｓｅｄｑｕｅｓｔｉｏｎａｎｓｗｅｒｉｎｇｂｙｃｌｏｚｅ


ｔａｎｓｌａｔｉｏｎ［Ｃ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ５７ｔｈＡｎｎｕａｌＭｅｅｔｉｎｇｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒ


ＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ
．Ｆｌｏｒｅｎｃｅ
，
Ｉｔａｌｙ
：ＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌ


Ｌｉｎｇｕｉｓｔｉｃｓ
，２０１９
：４８９６
－４９１０
．


［４５
］ＲｅｉｍｅｒｓＮ
？ＧｕｒｅｖｙｃｈＩ
．Ｓｅｎｔｅｎｃｅ
－ＢＥＲＴ：ＳｅｎｔｅｎｃｅＥｍｂｅｄｄｉｎｇｓｕｓｉｎｇＳｉａｍｅｓｅ


ＢＥＲＴ
－Ｎｅｔｗｏｒｋｓ
［Ｃ
］
／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２０１９ＣｏｎｆｅｒｅｎｃｅｏｎＥｍｐ
ｉｒｉｃａｌＭｅｔｈｏｄｓ


ｉｎＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇａｎｄｔｈｅ９ｔｈＩｎｔｅｒ
ｎａｔｉｏｎａｌＪｏｉｎｔＣｏｎｆｅｒｅｎｃｅｏｎ


ＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ（ＥＭＮＬＰ
－
ＩＪＣＮＬＰ
）
．２０１９
：３９８２
－３９９２
．


［４６
］ＢｏｓｓｅｌｕｔＡ
，ＲａｓｈｋｉｎＨ
，ＳａｐＭ
？ｅｔａｌ
．ＣＯＭＥＴ；ＣｏｍｍｏｎｓｅｎｓｅＴｒａｎｓｆｏｒｍｅｒｓｆｏｒ


ＫｎｏｗｌｅｄｇｅＧｒａｐｈＣｏｎｓｔｒｕｃｔ
ｉｏｎ
［Ｃ
］／／ＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ


（ＡＣＬ）
．２０１９
．


［４７
］ＲａｆｆｅｌＣ
５ＳｈａｚｅｅｒＮ
，ＲｏｂｅｒｔｓＡ
，ｅｔａｌ
．Ｅｘｐ
ｌｏｒｉｎｇｔｈｅｌｉｍｉｔｓｏｆ
ｔｒａｎｓｆｅｒｌｅａｒｎｉｎｇｗｉｔｈ


ａｕｎｉｆｉｅｄｔｅｘｔ
－
ｔｏ
－ｔｅｘｔｔｒａｎｓｆｏｒｍｅｒ
［Ｊ
］
．ＴｈｅＪｏｕｒ
ｎａｌｏｆＭａｃｈｉｎｅＬｅａｒｎｉｎｇＲｅｓｅａｒｃｈ
，


２０２０
，２１
（
１
）
：５４８５
－５５５１
．


［４８
］ＬｉｕＰ
，ＹｕａｎＷ
５ＦｕＪ
？ｅｔａｌ
．Ｐｒｅ
－ｔｒａｉｎ
，ｐｒｏｍｐｔ
，ａｎｄ
ｐｒｅｄｉｃｔ：Ａｓｙｓｔｅｍａｔｉｃｓｕｒｖｅｙｏｆ


ｐｒｏｍｐｔｉｎｇｍｅｔｈｏｄｓｉｎｎａｔｕｒａｌｌａｎｇｕａｇｅ
ｐｒｏｃｅｓｓｉｎｇ［Ｊ
］
．ＡＣＭＣｏｍｐｕｔｉｎｇＳｕｒｖｅｙｓ
５


２０２３
，５５（９）
：１
－３５
．


［４９
］ＫｈａｓｈａｂｉＤ
，ＭｉｎＳ
？ＫｈｏｔＴ
，ｅｔａｌ
．ＵＮＩＦＩＥＤＱＡ
：ＣｒｏｓｓｉｎｇＦｏｒｍａｔＢｏｕｎｄａｒｉｅｓｗｉｔｈ


ａＳｉｎｇ
ｌｅ
ＱＡＳｙｓｔｅｍ
［Ｃ
］／／ＦｉｎｄｉｎｇｓｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ
：


ＥＭＮＬＰ２０２０
．２０２０
：１８９６
－１９０７
．


［５０
］ＢｒｏｗｎＴ
，ＭａｎｎＢ
？ＲｙｄｅｒＮ
ｓｅｔａｌ
．Ｌａｎｇｕａｇｅｍｏｄｅｌｓａｒｅｆｅｗ
－ｓｈｏｔｌｅａｍｅｒｓ
［Ｊ
］
．


Ａｄｖａｎｃｅｓｉｎｎｅｕｒａｌｉｎｆｏｒｍａｔｉｏｎ
ｐｒｏｃｅｓｓｉｎｇｓｙｓｔｅｍｓ
，２０２０
，３３
：１８７７
－
１９０Ｌ


［５
１
］ＬｉＸＬ
，
ＬｉａｎｇＰ．Ｐｒｅｆｉｘ
－Ｔｕｎｉｎｇ
：ＯｐｔｉｍｉｚｉｎｇＣｏｎｔｉｎｕｏｕｓＰｒｏｍｐｔｓｆｏｒ


Ｇｅｎｅｒａｔｉｏｎ
［Ｃ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ５９ｔｈＡｎｎｕａｌＭｅｅｔｉｎｇｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒ


ＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓａｎｄｔｈｅ１
１ｔｈＩｎｔｅｒ
ｎａｔｉｏｎａｌＪｏｉｎｔＣｏｎｆｅｒｅｎｃｅｏｎＮａｔｕｒａｌ


ＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ（Ｖｏｌｕｍｅ１
：ＬｏｎｇＰａｐｅｒｓ）
．２０２１
：４５８２
－４５９７
．


７１


北京邮电大学工学硕士学位论文


［５２
］ＳｕｎＹ
，ＺｈａｎｇＹ
，Ｑ
ｉＬ
，ｅｔａｌ
．ＴＳＧＰ
：Ｔｗｏ
－ＳｔａｇｅＧｅｎｅｒａｔｉｖｅＰｒｏｍｐ
ｔｉｎｇｆｏｒ


ＵｎｓｕｐｅｒｖｉｓｅｄＣｏｍｍｏｎｓｅｎｓｅＱｕｅｓｔｉｏｎＡｎｓｗｅｒｉｎｇ［Ｊ］
．ａｒＸｉｖｐｒｅｐｒ
ｉｎｔ


ａｒＸｉｖ
：２２１１
．１３５１５
？２０２２
．


［５３
］ＣｈｒｅｎＷＡ
．Ｏｎｅ
－ｈｏｔｒｅｓｉｄｕｅｃｏｄｉｎｇｆｏｒｌｏｗｄｄａｙ
＿
ｐｏｗｅｒｐｒｏｄｕｃｔＣＭＯＳｄｅｓｉｇｎ


［Ｊ
］
．ＩＥＥＥＴｒａｎｓａｃｔｉｏｎｓｏｎｃｉｒｃｕｉｔｓａｎｄｓｙｓｔｅｍｓＩＩ
：ａｎａｌｏｇａｎｄｄｉｇ
ｉｔａｌｓｉｇｎａｌ


ｐｒｏｃｅｓｓｉｎｇ，１９９８
，４５（３）
：３０３
－３
１３
．


［５４
］ＫｏｐｐｅｎＭ
．Ｔｈｅｃｕｒｓｅｏｆｄｉｍｅｎｓｉｏｎａｌｉｔｙ
［Ｃ
］／／５ｔｈｏｎｌｉｎｅｗｏｒｌｄｃｏｎｆｅｒｅｎｃｅｏｎｓｏｆｔ


ｃｏｍｐｕｔｉｎｇ
ｉｎｉｎｄｕｓｔｒ
ｉａｌａｐｐ
ｌｉｃａｔｉｏｎｓ
（ＷＳＣ５）
．２０００
ｓ１
：４
－８
．


［５５
］Ｂｅｎｇ
ｉｏＹ
？ＤｕｃｈａｒｍｅＲ
，ＶｉｎｃｅｎｔＰ．Ａｎｅｕｒａｌｐｒｏｂａｂｉｌｉｓｔｉｃｌａｎｇｕａｇｅｍｏｄｅｌ
［Ｊ
］
．


Ａｄｖａｎｃｅｓｉｎｎｅｘｉｒａｌｉｎｆｏｒｍａｔｉｏｎ
ｐｒｏｃｅｓｓｉｎｇｓｙｓｔｅｍｓ
，２０００
，
１３
．


［５６
］ＶｉｄａｕｒｒｅＤ
？ＢｉｅｌｚａＣ
，ＬａｒｒａｎａｇａＰ．ＡｓｕｒｖｅｙｏｆＬＩｒｅｇｒｅｓｓｉｏｎ
［Ｊ
］
．Ｉｎｔｅｒ
ｎａｔｉｏｎａｌ


ＳｔａｔｉｓｔｉｃａｌＲｅｖｉｅｗ
，２０１３
，８１（３）
：３６１
－３８７
．


［５７
］Ｃｏｒ
ｔｅｓＣ
，
ＭｏｈｒｉＭ，ＲｏｓｔａｍｉｚａｄｅｈＡ
．Ｌ２ｒｅｇｕｌａｒ
ｉｚａｔｉｏｎｆｏｒｌｅａｒ
ｎｉｎｇ


ｋｅｍｅｌｓ
［Ｃ］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＴｗｅｎｔｙ
－ＦｉｆｔｉＣｏｎｆｅｒｅｎｃｅｏｎＵｎｃｅｒｔａｉｎｔ
ｙ
ｉｎ


Ａｒｔｉｆ
ｉｃｉａｌＩｎｔｅｌｌｉｇｅｎｃｅ
．２００９
：１０９
－１１６
．


［５８
］ＭｉｋｏｌｏｖＴ
，ＣｈｅｎＫ
，ＣｏｒｒａｄｏＧ
，ｅｔａｌ
．Ｅｆ
ｉｃｉｅｎｔＥｓｔｉｍａｔｉｏｎｏｆ
ＷｏｒｄＲｅｐｒｅｓｅｎｔａｔｉｏｎｓ


ｉｎＶｅｃｔｏｒＳｐａｃｅ［Ｍ
］
．２０１３
．


［５９
］ＳｈｅｒｓｔｉｎｓｋｙＡ
，Ｆｕｎｄａｍｅｎｔａｌｓｏｆｒｅｃｕｒｒｅｎｔｎｅｕｒａｌｎｅｔｗｏｒｋ（ＲＮＮ）ａｎｄｌｏｎｇｓｈｏｒｔ
－


ｔｅｒｍｍｅｍｏｒｙ（ＬＳＴＭ
）ｎｅｔｗｏｒｋ
［Ｊ
］
．ＰｈｙｓｉｃａＤ
：ＮｏｎｌｉｎｅａｒＰｈｅｎｏｍｅｎａ２０２０
，４０４
：


１３２３０６
．


［６０］ＭｉｋｏｌｏｖＴ
，ＳｕｔｓｋｅｖｅｒＩ
？ＣｈｅｎＫ
３ｅｔａｌ
．Ｄｉｓｔｒ
ｉｂｕｔｅｄｒｅｐｒｅｓｅｎｔａｔｉｏｎｓｏｆｗｏｒｄｓａｎｄ


ｐｈｒａｓｅｓａｎｄｔｈｅｉｒｃｏｍｐｏｓｉｔｉｏｎａｌｉｔｙ［Ｊ
］
．Ａｄｖａｎｃｅｓｉｎｎｅｕｒａｌｉｎｆｏｒｍａｔｉｏｎ
ｐｒｏｃｅｓｓｉｎｇ


ｓｙｓｔｅｍｓ，２０１３
，２６
．


［６１
］Ｈｎｆｆ
ｉｎａｎＤＡ．Ａｍｅｔｈｏｄｆｏｒｔｈｅｃｏｎｓｔｒｕｃｔｉｏｎｏｆｍｉｎｉｍｕｍ
－ｒｅｄｕｎｄａｎｃｙｃｏｄｅｓ
［Ｊ
］
．


Ｐｒｏｃｅｅｄｉｎｇｓｏｆ
ｔｈｅＩＲＥ
，１９５２
，４０（９
）
：１０９８
－１
１０１
．


［６２
］ＫｌｅｉｎｂａｕｍＤＧ
？ＤｉｅｔｚＫ
？ＧａｉｌＭ
，ｅｔａＬＬｏｇｉｓｔｉｃｒｅｇｒｅｓｓｉｏｎ
［Ｍ
］
．ＮｅｗＹｏｒｋ
：


Ｓｐｒｉｎｇｅｒ
－Ｖｅｒｌａｇ，２００２
．


［６３
］Ｐｅｎｎｉｎｇ
ｔｏｎＪ
，
ＳｏｃｈｅｒＲ
，ＭａｉｍｉｎｇＣＤ
．Ｇｌｏｖｅ
：Ｇｌｏｂａｌｖｅｃｔｏｒｓｆｏｒｗｏｒｄ


ｒｅｐｒｅｓｅｎｔａｔｉｏｎ［Ａ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２０１４ｃｏｎｆｅｒｅｎｃｅｏｎｅｍｐ
ｉｒｉｃａｌｍｅｔｈｏｄｓｉｎ


ｎａｔｕｒａｌｌａｎｇｕａｇｅ
ｐｒｏｃｅｓｓｉｎｇ（ＥＭＮＬＰ）［Ｃ］，２０１４
：
１５３２
－
１５４３
．


［６４
］ＳｏｃｈｅｒＲ
，ＣｈｅｎＤ
，ＭａｉｍｉｎｇＣＤ
，ｅｔａｌ
．Ｒｅａｓｏｎｉｎｇｗｉｔｈｎｅｕｒａｌｔｅｎｓｏｒｎｅｔｗｏｒｋｓｆｏｒ


ｋｎｏｗｌｅｄｇｅｂａｓｅｃｏｍｐ
！ｅｔｉｏｎ［Ｊ
］
．Ａｄｖａｎｃｅｓｉｎｎｅｕｒａｌｉｎｆｏｒｍａｔｉｏｎ
ｐｒｏｃｅｓｓｉｎｇｓｙｓｔｅｍｓ
，


２０１３
，
２６
．


７２


参考文献


［６５
］ＬｉｕＺ
５ＬｉＪ
５ＳｈｅｎＺ
５ｅｔａｌ
．Ｌｅａｒｎｉｎｇｅｆｆ
ｉｃｉｅｎｔｃｏｎｖｏｌｕｔｉｏｎａｌｎｅｔｗｏｒｋｓｔｈｒｏｕｇｈ


ｎｅｔｗｏｒｋｓｌｉｍｍｉｎｇ［Ｃ
］
／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＩＥＥＥｉｎｔｅｒ
ｎａｔｉｏｎａｌｃｏｎｆｅｒｅｎｃｅｏｎ


ｃｏｍｐｕｔｅｒｖｉｓｉｏｎ
．２０
１７
：２７３６
－２７４４
．


［６６
］ＢｏｒｄｅｓＡ
，ＵｓｕｎｉｅｒＮ
５Ｇａｒｃｉａ
－ＤｕｒａｎＡ
？ｅｔａｌ
．Ｔｒａｎｓｌａｔｉｎｇｅｍｂｅｄｄｉｎｇｓｆｏｒｍｏｄｅｌｉｎｇ


ｍｕｌｔｉ
－ｒｅｌａｔｉｏｎａｌｄａｔａ
［Ｊ］
．Ａｄｖａｎｃｅｓｉｎｎｅｕｒａｌｉｎｆｏｒｍａｔｉｏｎ
ｐｒｏｃｅｓｓｉｎｇｓｙｓｔｅｍｓ
，２０１３
，


２６
．


［６７
］ＷａｎｇＺ
５ＺｈａｎｇＪ
５ＦｅｎｇＪ
？ｅｔａｌ
．Ｋｎｏｗｌｅｄｇｅｇｒａｐｈｅｍｂｅｄｄｉｎｇｂｙｔｒａｎｓｌａｔｉｎｇｏｎ


ｈｙｐｅｒｐ
ｌａｎｅｓ
［Ｃ
］
／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＡＡＡＩｃｏｎｆｅｒｅｎｃｅｏｎａｒｔｉｆ
ｉｃｉａｌｉｎｔｅｌｌｉｇｅｎｃｅ
．


２０１４
，２８（１
）
．


［６８
］ＬｉｎＹ
，ＬｉｕＺ
，ＳｕｎＭ
，ｅｔａｌ
．Ｌｅａｒｎｉｎｇｅｎｔｉｔｙａｎｄｒｅｌａｔｉｏｎｅｍｂｅｄｄｉｎｇｓｆｏｒｋｎｏｗｌｅｄｇｅ


ｇｒａｐｈｃｏｍｐ
ｌｅｔｉｏｎ
［Ｃ
］
／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＡＡＡＩｃｏｎｆｅｒｅｎｃｅｏｎａｒｔ
ｉｆ
ｉｃｉａｌ


ｉｎｔｅｌｌｉｇｅｎｃｅ
．２０１５
，２９（１
）
．


［６９
］ＪｉＧ
５ＨｅＳ
５ＸｕＬ
，ｅｔａｌ
．Ｋｎｏｗｌｅｄｇｅｇｒａｐｈｅｍｂｅｄｄｉｎｇｖｉａｄｙｎａｍｉｃｍａｐｐ
ｉｎｇ


ｍａｔｒｉｘ［Ｃ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ５３ｒｄａｎｎｕａｌｍｅｅｔｉｎｇｏｆｔｈｅａｓｓｏｃｉａｔｉｏｎｆｏｒ


ｃｏｍｐｕｔａｔｉｏｎａｌｌｉｎｇｕｉｓｔｉｃｓａｎｄｔｈｅ７ｔｈｉｎｔｅｒ
ｎａｔｉｏｎａｌｊｏｉｎｔｃｏｎｆｅｒｅｎｃｅｏｎｎａｔｕｒａｌ


ｌａｎｇｕａｇｅ
ｐｒｏｃｅｓｓｉｎｇ（ｖｏｌｕｍｅ１
：Ｌｏｎｇｐａｐｅｒｓ）
．２０１５
：６８７
－６９６
．


［７０
］ＷａｎｇＦ
，
ＪｉａｎｇＭ
，Ｑ
ｉａｎＣ
，ｅｔａｌ．Ｒｅｓｉｄｕａｌａｔｔｅｎｔｉｏｎｎｅｔｗｏｒｋｆｏｒｉｍａｇｅ


ｃｌａｓｓｉｆｉｃａｔｉｏｎ
［Ｃ
］
／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＩＥＥＥｃｏｎｆｅｒｅｎｃｅｏｎｃｏｍｐｕｔｅｒｖｉｓｉｏｎａｎｄ


ｐａｔｔｅｒｎｒｅｃｏｇｎｉｔｉｏｎ
．２０１７
：３
１５６
－３
１６４
．


［７１
］ＯｌｉｖａＡ
，ＴｏｒｒａｌｂａＡ
，ＣａｓｔｅｌｈａｎｏＭＳ
？ｅｔａｌ
．Ｔｏｐ
－ｄｏｗｎｃｏｎｔｒｏｌｏｆ
ｖｉｓｕａｌａｔｔｅｎｔ
ｉｏｎｉｎ


ｏｂ
ｊ
ｅｃｔｄｅｔｅｃｔｉｏｎ［Ｃ
］
／／Ｐｒｏｃｅｅｄｉｎｇｓ２００３Ｉｎｔｅｒ
ｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＩｍａｇｅ


Ｐｒｏｃｅｓｓｉｎｇ（ＣａｔＮｏ
．０３ＣＨ３７４２９）
．ＩＥＥＥ
，２００３
，１
：
１
－２５３
．


［７２
］ＢａｈｄａｎａｕＤ
？ＣｈｏＫ
，Ｂｅｎｇ
ｉｏＹ．Ｎｅｕｒａｌｍａｃｈｉｎｅｔｒａｎｓｌａｔ
ｉｏｎｂｙｊｏｉｎｔｌｙ
ｌｅａｒｎｉｎｇ
ｔｏ


ａｌｉｇｎａｎｄｔｒａｎｓｌａｔｅ［Ｊ
］
，ａｒＸｉｖ
ｐｒｅｐｒｉｎｔａｒＸｉｖ
：
１４０９
．０４７３
ｓ２０
１４
．


［７３
］ＲａｄｆｏｒｄＡ
，ＮａｒａｓｉｍｈａｎＫ
５ＳａｌｉｍａｎｓＴ
，ｅｔａｌ
．Ｉｍｐｒｏｖｉｎｇ
ｌａｎｇｕａｇｅｕｎｄｅｒｓｔａｎｄｉｎｇ


ｂｙｇｅｎｅｒａｔｉｖｅ
ｐｒｅ
－ｔｒａｉｎｉｎｇ［Ｊ
］
．２０１８
．


［７４
］ＤｅｖｌｉｎＪ
５ＣｈａｎｇＭ
－Ｗ
，ＬｅｅＫ
，ｅｔａｌ
．ＢＥＲＴ
：Ｐｒｅ
－ｔｒａｉｎｉｎｇｏｆＤｅｅｐＢｉｄｉｒｅｃｔｉｏｎａｌ


ＴｒａｎｓｆｏｒｍｅｒｓｆｏｒＬａｎｇｕａｇｅＵｎｄｅｒｓｔａｎｄｉｎｇ［Ａ
］
／／Ｍｉｎｎｅａｐｏｌｉｓ
，Ｍｉｎｎｅｓｏｔａ：


ＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ
，２０１９
：４１７１
－４１８６
．


［７５
］ＶａｓｗａｎｉＡ
，ＳｈａｚｅｅｒＮ
５ＰａｒｍａｒＮ
，ｅｔａｌ
．Ａｔｔｅｎｔｉｏｎｉｓａｌｌ
ｙｏｕｎｅｅｄ
［Ａ
］
，Ｐｒｏｃｅｅｄｉｎｇｓ


ｏｆｔｈｅ３
１ｓｔＩｎｔｅｒ
ｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＮｅｕｒａｌＩｎｆｏｒｍａｔｉｏｎＰｒｏｃｅｓｓｉｎｇＳｙｓｔｅｍｓ


［Ｃ
］
．ＬｏｎｇＢｅａｃｈ
，Ｃａｌｉｆｏｒｎｉａ
，ＵＳＡ
；ＣｕｒｒａｎＡｓｓｏｃｉａｔｅｓＩｎｃ
．２０１７
：６０００
－６０１０
．


［７６
］ＧｌｏｒｏｔＸ
９ＢｏｒｄｅｓＡ
，Ｂｅｎｇ
ｉｏＹ．Ｄｅｅｐｓｐａｒｓｅｒｅｃｔｉｆｉｅｒｎｅｕｒａｌ


ｎｅｔｗｏｒｋｓ
［Ｃ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅｆｏｕｒｔｅｅｎｔｈｉｎｔｅｒ
ｎａｔｉｏｎａｌｃｏｎｆｅｒｅｎｃｅｏｎａｒｔｉｆｉｃｉａｌ


７３


北京邮电大学工学硕士学位论文


ｉｎｔｅｌｌｉｇｅｎｃｅａｎｄｓｔａｔｉｓｔｉｃｓ
．ＪＭＬＲＷｏｒｋｓｈｏｐａｎｄＣｏｎｆｅｒｅｎｃｅＰｒｏｃｅｅｄｉｎｇｓ
，２０１
１
：


３
１５
－３２３
．


［７７
］ＨｏｃｈｒｅｉｔｅｒＳ
５ＳｃｈｍｉｄｈｕｂｅｒＪ
．Ｌｏｎｇｓｈｏｒｔ
－ｔｅｒｍｍｅｍｏｒｙ［Ｊ］
．Ｎｅｕｒａｌｃｏｍｐｕｔａｔｉｏｎ
，


１９９７
，９（８）
：１７３５
－
１７８０
．


［７８
］ＣｈｏＫ
，ＭｅｒｒｉｅｎｂｏｅｒＢ
，ＧｕｌｃｅｈｒｅＣ
，ｅｔａｌ
．ＬｅａｒｎｉｎｇＰｈｒａｓｅＲｅｐｒｅｓｅｎｔａｔｉｏｎｓｕｓｉｎｇ


ＲＮＮＥｎｃｏｄｅｒ
－ＤｅｃｏｄｅｒｆｏｒＳｔａｔｉｓｔｉｃａｌＭａｃｈｉｎｅＴｒａｎｓｌａｔｉｏｎ
［Ｃ
］
／／ＥＭＮＬＰ．２０１４
．


［７９
］
ＲａｄｆｏｒｄＡ
，ＷｕＪ
？ＣｈｉｌｄＲ
ｓｅｔａｌ
．Ｌａｎｇｕａｇｅｍｏｄｅｌｓａｒｅｕｎｓｕｐｅｒｖｉｓｅｄｍｕｌｔｉｔａｓｋ


ｌｅａｍｅｒｓ［Ｊ
］
．ＯｐｅｎＡＩｂｌｏｇ，２０１９
，
１（８）
：９
．


［８０
］ＢｒｏｗｎＴ
？ＭａｎｎＢ
？ＲｙｄｅｒＮ
？ｅｔａｌ
．Ｌａｎｇｕａｇｅｍｏｄｅｌｓａｒｅｆｅｗ
－ｓｈｏｔｌｅａｍｅｒｓ
［Ｊ
］
．


Ａｄｖａｎｃｅｓｉｎｎｅｕｒａｌｉｎｆｏｒｍａｔｉｏｎ
ｐｒｏｃｅｓｓｉｎｇｓｙｓｔｅｍｓ
，２０２０
，３３
：１８７７
－１９０１
．


［８
１
］ＬｉｕＹ
？ＯｔｔＭ
，ＧｏｙａｌＮ
５ｅｔａｌ．Ｒｏｂｅｒｔａ
：Ａｒｏｂｕｓｔｌｙｏｐｔｉｍｉｚｅｄｂｅｒｔｐｒｅｔｘａｉｎｉｎｇ


ａｐｐｒｏａｃｈ
［Ｊ
］
．ａｒＸｉｖ
ｐｒｅｐｒｉｎｔａｒＸｉｖ：
１９０７
．
１
１６９２
，２０１９
．


［８２
］ＬａｎＺ
，ＣｈｅｎＭ
，ＧｏｏｄｍａｎＳ
？ｅｔａｌ
．Ａｌｂｅｒｔ：Ａｌｉｔｅｂｅｒｔｆｏｒｓｅｌｆ
－ｓｕｐｅｒｖｉｓｅｄｌｅａｒｎｉｎｇ


ｏｆｌａｎｇｕａｇｅｒｅｐｒｅｓｅｎｔａｔｉｏｎｓ
［Ｊ
］
．ａｒＸｉｖ
ｐｒｅｐｒｉｎｔａｒＸｉｖ；１９０９
．
１
１９４２
，２０１９
．


［８３］ＷｕＺ，ＰａｎＳ
，ＣｈｅｎＦ
？ｅｔａｌ
．ＡＣｏｍｐｒｅｈｅｎｓｉｖｅＳｕｒｖｅｙｏｎＧｒａｐｈＮｅｕｒａｌＮｅｔｗｏｒｋｓ
［Ｊ］
．


ＩＥＥＥＴｒａｎｓａｃｔｉｏｎｓｏｎＮｅｕｒａｌＮｅｔｗｏｒｋｓａｎｄＬｅａｒｎｉｎｇＳｙｓｔｅｍｓ
，２０２１
，３２（
１）
：４
－２４
．


［８４
］ＫｉｐｆＴＮ
，ＷｅｌｌｉｎｇＭ
．Ｓｅｍｉ
－ｓｕｐｅｒｖｉｓｅｄｃｌａｓｓｉｆｉｃａｔｉｏｎｗｉｔｈｇｒａｐｈｃｏｎｖｏｌｕｔｉｏｎａｌ


ｎｅｔｗｏｒｋｓ［Ｊ
］
，ａｒＸｉｖ
ｐｒｅｐｒｉｎｔａｒＸｉｖ
：１６０９０２９０７
，２０１６


［８５
］ＶｅｌｉｃｋｏｖｉｃＰ
，ＣｕｃｕｒｕｌｌＧ，ＣａｓａｎｏｖａＡ
，ｅｔａｌ
．Ｇｒａｐｈａｔｔｅｎｔｉｏｎｎｅｔｗｏｒｋｓ［Ｊ］
．ａｒＸｉｖ


ｐｒｅｐｒｉｎｔａｒＸｉｖ
：
１７１０１０９０３
？２０１７
．


［８６
］ＨａｍｉｌｔｏｎＷＬ
？ＹｉｎｇＲ
？ＬｅｓｋｏｖｅｃＪ
．Ｉｎｄｕｃｔｉｖｅｒｅｐｒｅｓｅｎｔａｔｉｏｎｌｅａｒｎｉｎｇｏｎｌａｒｇｅ


ｇｒａｐｈｓ［Ａ
］
．Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ３
１ｓｔＩｎｔｅｒ
ｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＮｅｕｒａｌ


ＩｎｆｏｒｍａｔｉｏｎＰｒｏｃｅｓｓｉｎｇＳｙｓｔｅｍｓ［Ｃ
］
．ＬｏｎｇＢｅａｃｈ
，Ｃａｌｉｆｏｒｎｉａ
，ＵＳＡ
；Ｃｕｒｒａｎ


ＡｓｓｏｃｉａｔｅｓＩｎｃ
．２０１７
：１０２５
－
１０３５
．


［８７
］ＺｈｏｕＤ
？ＺｈｅｎｇＬ
，ＨａｎＪ
，ｅｔａｌ
．Ａｄａｔａ
－ｄｒｉｖｅｎ
ｇｒａｐｈ
ｇｅｎｅｒａｔｉｖｅｍｏｄｅｌｆｏｒｔｅｍｐｏｒａｌ


ｉｎｔｅｒａｃｔｉｏｎｎｅｔｗｏｒｋｓ［Ａ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２６ｔｈＡＣＭＳＩＧＫＤＤＩｎｔｅｒ
ｎａｔｉｏｎａｌ


ＣｏｎｆｅｒｅｎｃｅｏｎＫｎｏｗｌｅｄｇｅＤｉｓｃｏｖｅｒｙ＆ＤａｔａＭｉｎｉｎｇ［Ｃ
］，２０２０
：４０１
－４１１
．


［８８
］ＫｉｐｆＴＮ
，ＷｅｌｌｉｎｇＭ
．Ｖａｒｉａｔｉｏｎａｌｇｒａｐｈａｕｔｏ
－ｅｎｃｏｄｅｒｓ［Ｊ］
．ａｒＸｉｖｐｒｅｐｒｉｎｔ


ａｒＸｉｖ：
１６１
１０７３０８
ｓ２０１６
．


［８９
］ＬｉｕＰ
５ＹｕａｎＷ
５ＦｕＪ
５ｅｔａｌＰｒｅ
－ｔｒａｉｎ
，ｐｒｏｍｐｔ
，ａｎｄ
ｐｒｅｄｉｃｔ
：Ａｓｙｓｔｅｍａｔｉｃｓｕｒｖｅｙｏｆ


ｐｒｏｍｐｔｉｎｇｍｅｔｈｏｄｓｉｎｎａｔｕｒａｌｌａｎｇｕａｇｅ
ｐｒｏｃｅｓｓｉｎｇ［Ｊ］
．ＡＣＭＣｏｍｐｕｔｉｎｇＳｕｒｖｅｙｓ
５


２０２３
，５５（９
）
：１
－３５
．


［９０］ＰｅｔｒｏｎｉＦ
，ＲｏｃｋｔａｓｃｈｅｌＴ
？ＲｉｅｄｅｌＳ
，ｅｔａｌ
．ＬａｎｇｕａｇｅＭｏｄｅｌｓａｓＫｎｏｗｌｅｄｇｅＢａｓｅｓ？
［Ａ
］


／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆ
ｔｈｅ２０１９ＣｏｎｆｅｒｅｎｃｅｏｎＥｍｐ
ｉｒｉｃａｌＭｅｔｈｏｄｓｉｎＮａｔｕｒａｌＬａｎｇｕａｇｅ


７４


参考文献


Ｐｒｏｃｅｓｓｉｎｇａｎｄｔｈｅ９ｔｈＩｎｔｅｒ
ｎａｔｉｏｎａｌＪｏｉｎｔＣｏｎｆｅｒｅｎｃｅｏｎＮａｔｕｒａｌＬａｎｇｕａｇｅ


Ｐｒｏｃｅｓｓｉｎｇ（ＥＭＮＬＰ
－ＩＪＣＮＬＰ）［Ｃ
］
，２０１９
：２４６３
－２４７３
．


［９１
］ＳｃｈｉｃｋＴ
，ＳｃｈｕｔｚｅＨ
．Ｉｔ＾ＮｏｔＪｕｓｔＳｉｚｅＴｈａｔＭａｔｔｅｒｓ
：ＳｍａｌｌＬａｎｇｕａｇｅＭｏｄｅｌｓＡｒｅ


ＡｌｓｏＦｅｗ
－ＳｈｏｔＬｅａｒ
ａｅｒｓ［Ｃ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２０２
１ＣｏｎｆｅｒｅｎｃｅｏｆｔｈｅＮｏｒｔｈ


ＡｍｅｒｉｃａｎＣｈａｐｔｅｒｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ
：Ｈｕｍａｎ


ＬａｎｇｕａｇｅＴｅｃｈｎｏｌｏｇ
ｉｅｓ
．２０２１
；２３３９
－２３５２
．


［９２
］ＺｈｏｎｇＺ
？Ｆｒ
ｉｅｄｍａｎＤ
？ＣｈｅｎＤ
．ＦａｃｔｕａｌＰｒｏｂｉｎｇ
Ｉｓ
［ＭＡＳＫ
］
：Ｌｅａｒｎｉｎｇｖｓ
．Ｌｅａｒｎｉｎｇ


ｔｏＲｅｃａｌｌ
［Ｃ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆ
ｔｈｅ２０２１ＣｏｎｆｅｒｅｎｃｅｏｆｔｈｅＮｏｒｔｈＡｍｅｒｉｃａｎＣｈａｐｔｅｒ


ｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ
：ＨｕｍａｎＬａｎｇｕａｇｅＴｅｃｈｎｏｌｏｇ
ｉｅｓ
．


２０２１
：
５０１７
－５０３３
．


［９３
］ＬｉｕＸ，
ＺｈｅｎｇＹ
，
ＤｕＺ
，
ｅｔａｌ
．ＧＰＴｕｎｄｅｒｓｔａｎｄｓ
，ｔｏｏ［Ｊ
］
．ａｒＸｉｖｐｒｅｐｒ
ｉｎｔ


ａｒＸｉｖ
：２１０３
．
１０３８５
５２０２１
．


［９４
］
ＸｕＹ
，ＺｈｕＣ
？ＸｕＲ
，ｅｔａｌ
．ＦｕｓｉｎｇＣｏｎｔｅｘｔＩｎｔｏＫｎｏｗｌｅｄｇｅＧｒａｐｈｆｏｒＣｏｍｍｏｎｓｅｎｓｅ


ＱｕｅｓｔｉｏｎＡｎｓｗｅｒｉｎｇ
［Ｃ
］

／／ＦｉｎｄｉｎｇｓｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌ


Ｌｉｎｇｕｉｓｔｉｃｓ
：ＡＣＬ
－ＩＪＣＮＬＰ２０２１
．２０２１
：１２０１
－
１２０７
．


［９５
］ＳｈｗａｒｔｚＶ
？ＷｅｓｔＰ
？ＬｅＢｒａｓＲ
５ｅｔａｌ
．ＵｎｓｕｐｅｒｖｉｓｅｄＣｏｍｍｏｎｓｅｎｓｅＱｕｅｓｔｉｏｎ


ＡｎｓｗｅｒｉｎｇｗｉｔｈＳｅｌｆ
－Ｔａｌｋ
［Ｃ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔ
ｉｉｅ２０２０ＣｏｎｆｅｒｅｎｃｅｏｎＥｍｐ
ｉｒｉｃａｌ


ＭｅｔｈｏｄｓｉｎＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ（ＥＭＮＬＰ）
．２０２０
：４６１５
－４６２９
．


［９６
］ＢｏｓｓｅｌｕｔＡ
，ＬｅＢｒａｓＲ
，ＣｈｏｉＹ．Ｄｙｎａｍｉｃｎｅｕｒｏ
－ｓｙｍｂｏｌｉｃｋｎｏｗｌｅｄｇｅｇｒａｐｈ


ｃｏｎｓｔｒｕｃｔｉｏｎｆｏｒｚｅｒｏ
－ｓｈｏｔｃｏｍｍｏｎｓｅｎｓｅｑｕｅｓｔｉｏｎａｎｓｗｅｒｉｎｇ［Ｃ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆ


ｔｈｅＡＡＡＩｃｏｎｆｅｒｅｎｃｅｏｎＡｒｔｉｆｉｃｉａｌＩｎｔｅｌｌｉｇｅｎｃｅ
．２０２１
？３５（６
）
：４９２３
－４９３
１
．


［９７
］ＮｉｕＹ
５ＨｕａｎｇＦ
，ＬｉａｎｇＪ
５ｅｔａｌ
．ＡＳｅｍａｎｔｉｃ
－ｂａｓｅｄＭｅｔｈｏｄｆｏｒＵｎｓｕｐｅｒｖｉｓｅｄ


ＣｏｍｍｏｎｓｅｎｓｅＱｕｅｓｔｉｏｎＡｎｓｗｅｒ
ｉｎｇ［Ｃ］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ５９ｔｈＡｎｎｕａｌＭｅｅｔｉｎｇ


ｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓａｎｄｔｈｅ１
１ｔｈＩｎｔｅｒ
ｎａｔｉｏｎａｌＪｏｉｎｔ


ＣｏｎｆｅｒｅｎｃｅｏｎＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ（Ｖｏｌｕｍｅ１
：ＬｏｎｇＰａｐｅｒｓ）
．２０２１
：


３０３７
－３０４９
．


［９８
］ＬｉｕＪ
５ＬｉｕＡ
５ＬｕＸ
？ｅｔａｌ
．ＧｅｎｅｒａｔｅｄＫｎｏｗｌｅｄｇｅＰｒｏｍｐ
ｔｉｎｇｆｏｒＣｏｍｍｏｎｓｅｎｓｅ


Ｒｅａｓｏｎｉｎｇ［Ｃ
］
／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ６０ｔｈＡｎｎｕａｌＭｅｅｔｉｎｇｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒ


ＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ（Ｖｏｌｕｍｅ１
：ＬｏｎｇＰａｐｅｒｓ
）
．２０２２
：３
１５４
－３
１６９
．


［９９
］ＯｏｒｄＡ
，ＬｉＹ
，ＶｉｎｙａｌｓＯ
．Ｒｅｐｒｅｓｅｎｔａｔｉｏｎｌｅａｒｎｉｎｇｗｉｔｈｃｏｎｔｒａｓｔｉｖｅｐｒｅｄｉｃｔｉｖｅ


ｃｏｄｉｎｇ［Ｊ
］
．ａｒＸｉｖ
ｐｒｅｐｒｉｎｔａｒＸｉｖ
：１８０７
．０３７４８
，２０
１８
．


７５


