Correspondence Autoencoders for Cross-Modal Retrieval

FANGXIANG FENG, XIAOJIE WANG, and RUIFAN LI, Beijing University of Posts and Telecommunications IBRAR AHMAD, Beijing University of Posts and Telecommunications and University of Peshawar

This article considers the problem of cross-modal retrieval, such as using a text query to search for images and vice-versa. Based on different autoencoders, several novel models are proposed here for solving this problem. These models are constructed by correlating hidden representations of a pair of autoencoders. A novel optimal objective, which minimizes a linear combination of the representation learning errors for each modality and the correlation learning error between hidden representations of two modalities, is used to train the model as a whole. Minimizing the correlation learning error forces the model to learn hidden representations with only common information in different modalities, while minimizing the representation learning error makes hidden representations good enough to reconstruct inputs of each modality. To balance the two kind of errors induced by representation learning and correlation learning, we set a speciﬁc parameter in our models. Furthermore, according to the modalities the models attempt to reconstruct they are divided into two groups. One group including three models is named multimodal reconstruction correspondence autoencoder since it reconstructs both modalities. The other group including two models is named unimodal reconstruction correspondence autoencoder since it reconstructs a single modality. The proposed models are evaluated on three publicly available datasets. And our experiments demonstrate that our proposed correspondence autoencoders perform signiﬁcantly better than three canonical correlation analysis based models and two popular multimodal deep models on cross-modal retrieval tasks.

Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval Models; I.2.6 [Artiﬁcial Intelligence]: Learning

General Terms: Algorithms, Design, Experimentation

Additional Key Words and Phrases: Cross-modal, retrieval, image and text, deep learning, autoencoder

ACM Reference Format: Fangxiang Feng, Xiaojie Wang, Ruifan Li, and Ibrar Ahmad. 2015. Correspondence autoencoders for cross- modal retrieval. ACM Trans. Multimedia Comput. Commun. Appl. 12, 1s, Article 26 (October 2015), 22 pages. DOI: http://dx.doi.org/10.1145/2808205

1. INTRODUCTION Inclusion of multimodal data in webpages has become a widespread trend. For example, an appealing webpage telling a story often contains some illustrations and other images accompanying the text, while a travel photo shared on the social web is usually tagged

This work was partially supported by the National Natural Science Foundation of China (No. 61273365), the National High Technology Research and Development Program of China (No. 2012AA011103), discipline building plan in 111 base (No. B08004), the Fundamental Research Funds for the Central Universities (No. 2013RC0304) and the Engineering Research Center of Information Networks, Ministry of Education. Author’s addresses: F. Feng, X. Wang (corresponding author), and R. Li (corresponding author), School of Computer Sciences, Beijing University of Posts and Telecommunications, Beijing, 100876, China; email: f.fangxiang@gmail.com, {xjwang, rﬂi}@bupt.edu.cn; I. Ahmad, School of Computer Sciences, Beijing Uni- versity of Posts and Telecommunications, Beijing, 100876, China, and Department of Computer Science, University of Peshawar, Pakistan; email: ibrar@upesh.edu.pk. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. Request permissions from permissions@acm.org. c⃝2015 ACM 1551-6857/2015/10-ART26 $15.00 DOI: http://dx.doi.org/10.1145/2808205

ACM Trans. Multimedia Comput. Commun. Appl., Vol. 12, No. 1s, Article 26, Publication date: October 2015.

26:2 F. Feng et al.

with words. Furthermore, the presence of massive multimodal data on the Internet brings huge cross-modal retrieval requirements, such as using an image query to search for text and using a text query to search for images. Unlike traditional information retrieval tasks performed in a single modality, such as using a text query to search for text, cross-modal retrieval focuses on mining the correlations between data from different modalities.

1.1. Previous Work Many approaches have been proposed to develop solutions to the challenging tasks associated with cross-modal retrieval. In general, the previous works have deﬁned two main strategies for modeling cross-modal correlations. One strategy involves modeling the correlation between different modalities with a shared code layer. A portion of them use topic models to achieve the goal. Corre- spondence Latent Dirichlet Allocation (Corr-LDA) [Blei and Jordan 2003] extended the LDA model [Blei et al. 2003] to ﬁnd the topic-level relationships between images and text annotations, where the distributions of topics act as the middle layer for both images and text. For images with loosely-coupled text, a mixture of directed and undi- rected probabilistic graphical model, called MDRF, was proposed in Jia et al. [2011]. This model was built using a Markov random ﬁeld over LDA. These LDA-based mod- els can be essentially understood as a two-layer architecture with only one hidden layer. Recently, there has been a trend of developing deep architecture for tackling complex AI problems [Bengio 2009], which is inspired by the architectural depth of the brain. Some representative models, such as deep autoencoders (DAE) [Hinton and Salakhut- dinov 2006], deep belief networks (DBN) [Hinton et al. 2006], and deep Boltzmann Machine (DBM) [Salakhutdinov and Hinton 2012], and their corresponding learning algorithms have been proposed. These models also have been extended to model multi- modal data. Ngiam et al. [2011] used multimodal DAE to learn a shared representation for both speech and visual inputs. Silberer et al. [Silberer and Lapata 2014] extended the multimodal DAE in a semi-supervised fashion by introducing the label information of the shared layer. Srivastava and Salakhutdinov [2012b] used multimodal DBM and DBN to learn a uniﬁed representation for both images and text. Recently we have also noticed several deep learning methods [Weston et al. 2010; Frome et al. 2013; Socher et al. 2013] for learning a joint embedding space of image and text. Those models have shown signiﬁcant advantages in several tasks, such as image annotation, objective classiﬁcation, and zero-shot learning. However, to the best of our knowledge, none of them has ever been used for cross-modal retrieval tasks. The other strategy involves a two-stage framework. It ﬁrst learns or extracts features from each modality, then utilizes canonical correlation analysis (CCA) [Hardoon et al. 2004] to build a lower dimensional common representation space. Compared with the former, this strategy is more straightforward for the task of cross-modal retrieval. Brieﬂy, CCA is a method of data analysis used to discover a subspace of multiple data spaces. Given the training pairs p and q, CCA ﬁnds matrices U and V such that Up and Vq have maximum correlations. The ﬁrst d canonical components of U and V could be used for projecting new input pairs into a d-dimensional space where cross- modal retrieval could be conducted by calculating a simple similarity metric, such as Euclidean or cosine distance. Rasiwasia et al. [2010] proposed correlation matching by mapping the features of images and text extracted by LDA into the same representation space using CCA. Ngiam et al. [2011] used DAE to extract features and suggested using CCA to form a shared representation of audio and video data. As in Ngiam et al.’s work, Kim et al. [2012] learned the shared semantic space of different languages with DAE and CCA.

ACM Trans. Multimedia Comput. Commun. Appl., Vol. 12, No. 1s, Article 26, Publication date: October 2015.

Correspondence Autoencoders for Cross-Modal Retrieval 26:3

Fig. 1. An image and its tags. The tags “sky” and “blue” are common information in both image and text modalities. The tags “nikon” and “nikkor” are text modality-speciﬁc information, while “ﬂowers” and “clouds” are image modality-speciﬁc information. The common information is the key to cross-modal retrieval tasks.

1.2. Motivation In the ﬁrst strategy, the shared layer was built by jointly learning from different modalities. We argue that the shared layer learnt in this way might not ﬁt the need of cross-modal retrieval. It is clear that perceived data from different modalities for the same object usually comprise of common information in all modalities and modality- speciﬁc information. Figure 1 gives an example where a picture accompanying several word tags is shown. The tags “sky” and “blue” are common information for both image and text modalities, while “nikon” and “nikkor” are text modality-speciﬁc information which, otherwise, can hardly be acquired from the image. On the other hand, “ﬂowers” and “clouds” are solely image modality-speciﬁc information which cannot be captured in text tags. Intuitively, common information like “sky” and “blue” is the key to the task of cross-modal retrieval. Both text and image modality-speciﬁc information are unnec- essary and could even harm the task of cross-modal retrieval. A shared representation learns both common and modality-speciﬁc information. Although the strength of learn- ing shared representation has been shown on complementarity of data from different modalities, it is not an exactly well-ﬁtting representation to the task of cross-modal retrieval. A representation which can learn only common information for different modalities is rather a better choice. This serves as the ﬁrst motivation to build our model in this article. The second strategy separates correlation learning from representation learning. This strategy is insufﬁcient to exploit the complex correlations of representations from different modalities, especially when autoencoders are used to learn representations. Clearly, autoencoders can be used to learn different level representations for inputs with different reconstruction errors. One problem for building correlations between data from two different modalities is that it is difﬁcult to decide which level is best for building correlations. In other words, two different modalities may be correlated at different abstract levels of representations. Therefore, correlation learning need be si- multaneously considered with representation learning, which is the second motivation to build our model in this article.

1.3. Contribution To overcome the disadvantages of these two strategies, in our previous work, we have already proposed correspondence autoencoder (Corr-AE) based on a pair of basic

ACM Trans. Multimedia Comput. Commun. Appl., Vol. 12, No. 1s, Article 26, Publication date: October 2015.

26:4 F. Feng et al.

Fig. 2. The difference between two-stage methods and our Corr-AE. Corr-AE incorporates representa- tion learning and correlation learning into a single process while two-stage methods separate these two procedures.

unimodal autoencoders [Feng et al. 2014]. The difference between the two-stage method and our Corr-AE is illustrated in Figure 2. The two-stage method ignores the correla- tion between different modalities when performing representation learning. Corr-AE incorporates representation learning and correlation learning into a single process. A novel loss function composed of two parts is designed: the loss of different autoen- coders for both modalities, and the loss of correlation between different modalities. This model is evaluated using three publicly available datasets [Rasiwasia et al. 2010; Farhadi et al. 2010; Chua et al. 2009]. Compared with several other multimodal deep learning models [Kim et al. 2012; Ngiam et al. 2011, 2011; Srivastava and Salakhut- dinov 2012a], our Corr-AE demonstrates increased effectiveness. Moreover, based on two other multimodal autoencoders, we extend Corr-AE to two correspondence mod- els including correspondence cross-modal autoencoder (Corr-Cross-AE) and correspon- dence full-modal autoencoder (Corr-Full-AE). Experimental results based on different autoencoders show that the combination of representation learning and correlation learning is more effective than the two-stage method. This article is an extension of our preliminary studies mentioned previously [Feng et al. 2014], where the three correspondence autoencoders reconstruct both the image and text modalities. We call them multimodal reconstruction correspondence autoen- codedrs. In this article, we propose two novel correspondence autoencoders, which only reconstruct a single modality. We therefore call them unimodal reconstruction cor- respondence autoencoders. Speciﬁcally, one model which only reconstructs the image modality is named Correspondence Image Autoencoder (Corr-Image-AE). And the other which only reconstructs the text modality is named Correspondence Text Autoencoder (Corr-Text-AE). These two unimodal reconstruction Corr-AEs provide us with two addi- tional choices on implementing the task of cross-modal retrieval. In addition, together with the three multimodal reconstruction Corr-AEs, they give us a clearer insight on how all Corr-AEs work. The details will be given in Section 3.5.2.

ACM Trans. Multimedia Comput. Commun. Appl., Vol. 12, No. 1s, Article 26, Publication date: October 2015.

Correspondence Autoencoders for Cross-Modal Retrieval 26:5

Fig. 3. Correspondence autoencoder.

The remainder of this article is organized as follows. In the next section, the details of three multimodal reconstruction Corr-AEs are given. The two unimodal reconstruc- tion Corr-AEs are then presented. Section 3 describes the experimental results and compares the performance of different models. Conclusions are presented in Section 4.

2. LEARNING ARCHITECTURE In this section, we ﬁrst introduce the three multimodal reconstruction correspondence autoencoders. Then we describe the two unimodal reconstruction correspondence au- toencoders. Lastly, we build the deep architecture based on the Corr-AEs and give the training procedure.

2.1. Multimodal Reconstruction Correspondence Autoencoder Here, we introduce the three multimodal reconstruction correspondence autoencoders. Firstly, the architecture of the basic Corr-AE is described. Then, a loss function suitable for training the Corr-AE aiming to learn similar representations of different modalities is proposed. Last, two extensions, cross-modal and full-modal Corr-AEs are described.

2.1.1. Correspondence Autoencoder. As illustrated in Figure 3, the Corr-AE architecture consists of two subnetworks, each a basic autoencoder. These two networks are con- nected by a predeﬁned similarity measure on the code layers. Each sub-network in the Corr-AE is responsible for a modality. In this way, the inputs to each sub-network are the features from one modality. During learning, the two sub-networks are coupled at their code layers using a similarity measure. After learning, the two sub-networks of the Corr-AE essentially contain different parameters, even if they have the same architecture. As a result, the codes for new inputs can be obtained using the learned network parameters. Formally, the mapping from the inputs of these two sub-networks to the code layers is denoted as f (p; WI) and g(q; WT ), in which, f is for image modality and g is for text modality; W denotes the weight parameters in these two sub-networks. The subscripts denote the corresponding modalities. f and g are logistic activation function. The similarity measure between the kth pair of image representation p(k) and the given text representation q(k) are here deﬁned as follows:

C(p(k), q(k); WI, WT ) =  f (p(k); WI) −g(q(k); WT ) 2 2, (1)

where ∥·∥2 is the L2 norm.

ACM Trans. Multimedia Comput. Commun. Appl., Vol. 12, No. 1s, Article 26, Publication date: October 2015.

26:6 F. Feng et al.

To learn the similar representations of these two modalities for one object, a loss function given input image representation p(k) and its text representation q(k) are established. To simplify the notation, the network parameters WI and WT are grouped as . The loss function on any pair of inputs can then be deﬁned as follows:

L(p(k), q(k); ) = (1 −α)  LI(p(k); ) + LT (q(k); ) 

+ αLC(p(k), q(k); ), (2)

LI(p(k); ) = p(k) −ˆp(k) I 2 2, (3a)

LT (q(k); ) = q(k) −ˆq(k) T 2 2, (3b)

LC(p(k), q(k); ) = C(p(k), q(k); ). (3c)

Here, ∥·∥2 is the L2 norm. LI and LT are the losses caused by data reconstruction errors for the given inputs (an image and its text) of two sub-networks, speciﬁcally image and text modalities. ˆp(k) I and ˆq(k) T are the reconstruction data from p(k) and q(k) respectively; LC is the correlation loss; α(0 < α < 1) in the total loss function (2) is a parameter used to trade off between two groups of losses: correlation losses and re- construction losses. An appropriate value for α is crucial. If α = 0, the loss function degenerates to the loss function of the autoencoders. It cannot then capture any cor- relations between inputs from different modalities. At the other extreme, considering only correlation loss, α is set to 1. This assumes that any pair of inputs has correlations, regardless of whether the image and text inputs match or not. An intuitive interpreta- tion for this is that the cost function only focuses on the constraints of correlations and completely ignores the characteristics of the data. The learning for Corr-AE can be performed using the standard back-propagation algorithm. The gradients for the parameters WI and WT are calculated as follows:

∂L ∂WI = (1 −α) ∂LI

∂WI + α ∂LC

∂WI , (4)

∂L ∂WT = (1 −α) ∂LT

∂WT + α ∂LC

∂WT . (5)

In summary, minimizing the loss function deﬁned in Equation (2) enables our Corr- AE to learn similar representations from bimodal feature representations.

2.1.2. Correspondence Cross-Modal Autoencoder. As illustrated in Figure 4, we propose the Corr-Cross-AE, which replace the basic autoencoders to cross-modal autoencoders. Unlike the basic autoencoders, which reconstruct the input itself, cross-modal autoen- coders reconstruct input from different modalities. The loss function on any pair of inputs of the Corr-Cross-AE is deﬁned as follows:

L(p(k), q(k); ) = (1 −α)  LI(p(k), q(k); ) + LT (p(k), q(k); ) 

+ αLC(p(k), q(k); ), (6)

LI(p(k), q(k); ) = q(k) −ˆq(k) I 2 2, (7a)

LT (p(k), q(k); ) = p(k) −ˆp(k) T 2 2, (7b)

LC(p(k), q(k); ) = C(p(k), q(k); ). (7c)

ACM Trans. Multimedia Comput. Commun. Appl., Vol. 12, No. 1s, Article 26, Publication date: October 2015.

Correspondence Autoencoders for Cross-Modal Retrieval 26:7

Fig. 4. Correspondence cross-modal autoencoder.

Fig. 5. Correspondence full-modal autoencoder.

Here, ˆq(k) I and ˆp(k) T are the reconstruction data from image and text subnet, respectively. The meanings of other symbols are the same as those in Equation (2). The representation learning of image modality in cross-modal autoencoder considers the information from the text modality and vice-versa. This causes some correlations to be captured in the reconstruction loss.

2.1.3. Correspondence Full-Modal Autoencoder. The full-modal autoencoder can be viewed as a combination of a basic autoencoder and cross-modal autoencoder. This autoencoder is proposed to model the audio and video data in Ngiam et al. [2011]. The basic Corr-AE are also easy to extend to Corr-Full-AE based on full-modal autoencoder. As illustrated in Figure 5, the autoencoder reconstruct not only the input itself but also input from different modalities. This “full” representation space contains information from both modalities. The loss function of any pair of inputs of the Corr-Full-AE is deﬁned as follows:

L(p(k), q(k); ) = (1 −α)  LI(p(k), q(k); ) + LT (p(k), q(k); ) 

+ αLC(p(k), q(k); ), (8)

LI(p(k), q(k); ) = p(k) −ˆp(k) I 2 2 + q(k) −ˆq(k) I 2 2, (9a)

LT (p(k), q(k); ) = p(k) −ˆp(k) T 2 2 + q(k) −ˆq(k) T 2 2, (9b)

LC(p(k), q(k); ) = C(p(k), q(k); ). (9c)

ACM Trans. Multimedia Comput. Commun. Appl., Vol. 12, No. 1s, Article 26, Publication date: October 2015.

26:8 F. Feng et al.

Fig. 6. Correspondence image autoencoder.

Here, ˆp(k) I and ˆq(k) I are the reconstruction data from p(k) and q(k) in the image subnet; ˆp(k) T and ˆq(k) T are the reconstruction data from p(k) and q(k) in the text subnet. The meanings of other symbols are the same as those in Equation (2).

2.2. Unimodal Reconstruction Correspondence Autoencoder

2.2.1. Correspondence Image Autoencoder. As illustrated in Figure 6, we propose the Corr-Image-AE. Unlike the three aforementioned correspondence autoencoders, which reconstruct both image and text, correspondence image autoencoder only reconstructs image in both image and text modalities. The loss function on any pair of inputs of the Corr-Image-AE is deﬁned as follows:

L(p(k), q(k); ) = (1 −α)  LI(p(k); ) + LT (p(k), q(k); ) 

+ αLC(p(k), q(k); ), (10)

LI(p(k); ) = p(k) −ˆp(k) I 2 2, (11a)

LT (p(k), q(k); ) = p(k) −ˆp(k) T 2 2, (11b)

LC(p(k), q(k); ) = C(p(k), q(k); ). (11c)

Here, ˆp(k) I and ˆp(k) T are the reconstruction data from image and text subnet, respectively. The meanings of other symbols are the same as those in Equation (2).

2.2.2. Correspondence Text Autoencoder. As illustrated in Figure 7, we propose the Corr- Text-AE, which only reconstructs text in both image and text modalities. The loss function on any pair of inputs of the Corr-Text-AE is deﬁned as follows:

L(p(k), q(k); ) = (1 −α)  LI(p(k); ) + LT (p(k), q(k); ) 

+ αLC(p(k), q(k); ), (12)

LI(p(k), q(k); ) = q(k) −ˆp(k) T 2 2, (13a)

LT (q(k); ) = q(k) −ˆq(k) T 2 2, (13b)

LC(p(k), q(k); ) = C(p(k), q(k); ). (13c)

ACM Trans. Multimedia Comput. Commun. Appl., Vol. 12, No. 1s, Article 26, Publication date: October 2015.

Correspondence Autoencoders for Cross-Modal Retrieval 26:9

Fig. 7. Correspondence text autoencoder.

Here, ˆp(k) T and ˆq(k) T are the reconstruction data from image and text subnet, respectively. The meanings of other symbols are the same as those in Equation (2).

2.3. Deep Architecture Data from different modalities may have different statistical properties. This makes it difﬁcult to capture correlations across modalities directly. To overcome this difﬁculty, here we propose a deep architecture. This architecture ﬁrst involves using some stacked modality-friendly models to learn higher-level representations to remove modality- speciﬁc properties. Then the Corr-AE is used to learn similar representations at a higher level. As illustrated in Figure 8, the deep architecture has three stacked components. The ﬁrst two components are all restricted Boltzmann machines (RBMs). There are two extended RBMs for the ﬁrst component and two basic RBMs for the second component. To be brief, RBM [Smolensky 1986] is an undirected graphical model with stochastic binary units in a visible layer and a hidden layer but without connections between the units within these two layers. Given that each unit is distributed by Bernoulli distribution with logistic activation function, a joint probabilistic distribution of visible units and hidden units can be deﬁned. The basic RBM can be extended to exponential family. All the models can be efﬁciently learned by using the contrastive divergence approximation (CD) [Hinton 2002]. For the ﬁrst layer, Gaussian RBM [Welling et al. 2004] and replicated softmax RBM [Salakhutdinov and Hinton 2009] can be used to model the real-valued feature vectors for image and the discrete sparse word count vectors for text, respectively. After learning, the hidden layers of the RBMs can then be used as the input for the second component. The second component involves two basic RBMs, which are used to learn higher-level features for image and text. The third com- ponent can involve any one of the ﬁve correspondence autoencoders given above. The learning for all correspondence autoencoders can be performed using standard back- propagation algorithm. The details of the training procedure for the deep architecture are summarized in Algorithm 1. Here, we just take the Corr-AE as an example. It is straightforward to use the deep architecture for cross-model retrieval tasks. Generally we perform the training and retrieval processes. During the training of models, the feature representations of training pairs of images and text are extracted, and then fed into these deep models to learn the well-ﬁt parameters. Thereafter, the models are prepared for cross-modal retrieval tasks. For the task of image retrieval using a text query, the system is expected to return the most relevant images when given a text query. After being ﬁrstly represented through the same feature extraction

ACM Trans. Multimedia Comput. Commun. Appl., Vol. 12, No. 1s, Article 26, Publication date: October 2015.

26:10 F. Feng et al.

Fig. 8. Deep architecture.

ALGORITHM 1: Learning algorithm

1: Train the Gaussian RBM and replicated softmax RBM in the ﬁrst component using the contrastive divergence learning rule. 2: Train the two Bernoulli RBMs in the second component using the contrastive divergence learning rule. 3: Update the parameters of Corr-AE in the third component using the gradient descent of the total loss. More speciﬁcally,

WI ←WI −ϵ · ∂L

∂WI (14)

WT ←WT −ϵ · ∂L ∂WT (15)

where ϵ is the learning rate, ∂L ∂WI and ∂L ∂WT are computed following (4), (5).

methods used in the training process, the text query and all the candidate images from test datasets are then fed into the deep models with learned parameters. This models outputs comparable representations in the joint space for the text query and candidate images. Finally, a ranked list with an increasing score is generated by using a cosine distance metric between each pair of comparable representations of an image and a text query.

3. EXPERIMENTS We evaluate our models on three publicly available real-world datasets. In this section, we ﬁrst give a detailed description of these datasets and their modal-speciﬁc features.

ACM Trans. Multimedia Comput. Commun. Appl., Vol. 12, No. 1s, Article 26, Publication date: October 2015.

Correspondence Autoencoders for Cross-Modal Retrieval 26:11

Then, two evaluation protocols used in the experiments are introduced. Next, the per- formance of several models is reported. Finally, we analyze the impact of the parameter α on the models.

3.1. Datasets and Feature Extraction Wikipedia. The dataset [Rasiwasia et al. 2010] was collected from “Wikipedia featured articles.” It contains 2,866 image/text pairs belonging to ten semantic categories. The dataset was split into three subsets: 2,173 cases as the training set, 231 cases as the validation set, and 462 cases as the test set. For image representation, we extract the following three types of features.

(1) Pyramid Histogram of Words (PHOW) [Bosch et al. 2007]. For PHOW features, dense SIFT descriptors is ﬁrst extracted by VLfeat [Vedaldi and Fulkerson 2010] from each training image and then a 1,000-dimensional visual word codebook is learned through K-means clustering. (2) Gist [Oliva and Torralba 2001]. The gist descriptor is extracted by the publicly available package1 with default parameters. This results in a 512-dimensional feature vector for each image. (3) MPEG-7 descriptors [Manjunath et al. 2001]. We use the publicly available software [Bastan et al. 2010] to extract four different visual descriptors (CSD, SCD, CLD, EHD) deﬁned in MPEG-7 for image representations. The dimensionality of the obtained MPEG-7 feature vector is 784.

Thus, each image is represented by a 2,296-dimensional feature vector. For text representation, we use bag-of-words model. A dictionary of 3,000 high-frequency words is built from all training text. We use Python Natural Language Toolkit [Bird 2006] to stem the text. This dataset is available at http://www.svcl.ucsd.edu/projects/ cross-modal/. Pascal. The dataset [Farhadi et al. 2010] contains 1,000 image/text pairs from twenty categories, ﬁfty cases per category. The images are randomly selected from 2008 PASCAL development kit. Each image is labeled with ﬁve sentences. We split the data into three subsets: 800 for training (40 cases per category), 100 for validation (ﬁve cases per category), and 100 for testing (ﬁve cases per category). The feature extraction methods for images and text are the same as those for the Wikipedia dataset except that the dimensionality of text representation is 1,000. This dataset is available at http://vision.cs.uiuc.edu/pascal-sentences/. NUS-WIDE-10k. This dataset is a subset of NUS-WIDE [Chua et al. 2009], which contains about 270k images with tag annotations from 81 categories. We only choose ten categories with the largest quantity and 1,000 image/text pairs per category from NUS-WIDE. Each pair of our NUS-WIDE-10k belongs to only a single category. The ten categories are animal, cloud, ﬂower, food, grass, person, sky, toy, water, and window. We randomly split the dataset into three subsets: 8,000 cases for training (800 cases per cat- egory), 1,000 for validation (100 cases per category), and 1,000 for testing (100 cases per category). Each image is represented by six descriptors, including 64-D color histogram, 144-D color correlogram, 73-D edge direction histogram, 128-D wavelet texture, 225-D block-wise color moments, and 500-D bag-of-words based on SIFT descriptions. Each text is represented by 1,000-dimensional bag-of-words. This dataset with extracted features is available at http://lms.comp.nus.edu.sg/research/NUS-WIDE.htm. Note that the three datasets Wikipedia, Pascal, and NUS-WIDE-10k have distinct properties. For example, their text modality is largely different. Text of Wikipedia is formed by short articles, that of Pascal by sentences, and that of NUS-WIDE-10k by

1http://people.csail.mit.edu/torralba/code/spatialenvelope/.

ACM Trans. Multimedia Comput. Commun. Appl., Vol. 12, No. 1s, Article 26, Publication date: October 2015.

26:12 F. Feng et al.

tags, respectively. In addition, the scales of these datasets range from 1,000 to 10k. And the numbers of the categories range from ten to twenty.

3.2. Evaluation Metric We consider two cross-modal retrieval tasks: text retrieval from an image query and image retrieval from a text query. Following Zhuang et al. [2013], the retrieval per- formance is evaluated using two metrics, mean average precision (mAP) and top 20% percentage. The former measures the ability of learning discriminative cross-modal mapping functions, while the later reveals the ability of learning corresponding latent concepts. mAP. Given one query and the ﬁrst Rtop-ranked retrieved data, the average precision is deﬁned as follows

r=1 p(r) · rel(r), (16)

where M is the number of relevant data in the retrieved result, p(r) is the precision at r, and rel(r) presents the relevance of a given rank (one if relevant and zero otherwise). The retrieved data is considered as relevant if it has the same semantic label as the query. Thereafter, the metric mAP is obtained by averaging AP of all the queries. We report mAP@50 (R = 50) in all experiments. Semantic labels are used only for evaluation. That is, the training of our models does not need any semantic label. Top 20%. Jia et al. [2011] proposed this evaluation metric for datasets without seman- tic label. Because in this case only one ground-truth is available for each image-text pair, the position of the ground-truth image/text in the ranked list could be used to evaluate the performance. Speciﬁcally, the top 20% percentage is the relative number of images/text, which are correctly retrieved in the ﬁrst 20% of the ranked list.

3.3. Baseline To make fair comparisons, the inputs to our models and baseline methods2 are all features learned by ﬁrst two components of deep architecture described in Section 2.3. And the only difference is the third component in the deep architecture. Besides, cosine distance is used to measure the similarity in all our experiments. We compare our models with three CCA based models and two multimodal models.

(1) CCA-AE [Kim et al. 2012]. We ﬁrst use two unimodal autoencoders to learn higher level image feature and text feature, respectively. And then we use CCA3 to build a common representation space on the learned features. (2) CCA-Cross-AE. Instead of using the unimodal autoencoders with CCA, this method directly combines cross-modal autoencoders with CCA. (3) CCA-Full-AE. Literally, this method combines full-modal autoencoders with CCA. (4) Bimodal AE [Ngiam et al. 2011]. We train a bimodal autoencoder to perform shared representation learning. Follow the training algorithm described in Ngiam et al. [2011], we add training examples that have zero values for one input modality (e.g., image) and original values for the other input modality (e.g., text), but still require the network to reconstruct both modalities (image and text). Thus, one-third of the training data has only image for input, another one-third of the data has only text, and the rest one-third of the data has both image and text. After learning the bimodal autoencoder, a single modality input (image or text) can be mapped into the shared representation space.

2The code for all baseline models are available online at https://github.com/nitishsrivastava/deepnet. 3Matlab code can be downloaded at http://www.davidroihardoon.com/Professional/Code.html.

ACM Trans. Multimedia Comput. Commun. Appl., Vol. 12, No. 1s, Article 26, Publication date: October 2015.

Correspondence Autoencoders for Cross-Modal Retrieval 26:13

(5) Bimodal DBN [Ngiam et al. 2011; Srivastava and Salakhutdinov 2012a]. A bimodal DBN is obtained by connecting image and text features with a joint layer. The model does not give a direct matching function of image and text inputs. However, the bimodal DBN can generate the unknown modality conditioned on a given modality. In other words, queries can be mapped into the feature space of the other modality. And then a suitable similarity measure can be used to retrieve results that are close to the query.

3.4. Model Architecture To determine the speciﬁc architecture of all the models, we perform grid search for the number of hidden neurons of each layer with the setting 32, 64, 128, 256, 512, and 1,024. In addition, to reduce the search space we keep the number of units identical for all hidden layers in the deep architecture. The validation sets are used to determine the best number of hidden units. Besides, the validation sets are used to determine the dimensionality of the CCA latent space and the iteration number of bimodal AEs in our ﬁve correspondence models. For these ﬁve models, we need to choose an appropriate value for the parameter α. Our experiments show that the parameter α is not sensitive to datasets. Therefore, for all datasets, the value of α for Corr-AE and Corr-Full-AE is set to 0.8. For Corr-Cross-AE, α is set to 0.2. The values for α of Corr-Image-AE and Corr-Text-AE are set to 0.3 and 0.7, respectively. A detailed analysis of α will be given at the end of this section. For the models involving CCA, to achieve their best performance, CCA is applied to learn the correlation between the image and text hidden layers with different number of units. Three copies of all training data are used in the training of bimodal AE. The ﬁrst copy is the data from both modalities (original data). The second is only image data with text dataset to zeros. And the third is only text data with image dataset to zeros. We do not weight the reconstruction errors from different modalities. For the bimodal DBN, variational mean ﬁeld with ten steps is adopted to generate the unknown modality conditioned on a given modality. Gibbs sampling does not show improvement in our experiments. The code with parameter speciﬁcations of our models and the baseline methods are available online4.

3.5. Results

3.5.1. Multimodal Reconstruction Corr-AEs vs. Baselines. Table I summarizes the mAP scores and top 20% of the two cross-modal retrieval tasks for Wikipedia, Pascal, and NUS-WIDE-10k datasets, respectively. On all datasets, our three multimodal recon- struction Corr-AEs signiﬁcantly outperform other models on both tasks of text and image retrievals. Taking Corr-Full-AE as an example, we compare it with the best results on each task achieved by the ﬁve baseline models. It improves mAP scores by 12.3% and 16.6% respectively on searching text by images and searching images by text on Wikipedia, improves by 12.4% and 2.2% respectively on the two tasks on Pascal, improves by 32.4% and 10.2% respectively on the two tasks on NUS-WIDE-10k. Compared with CCA-AE, our Corr-AE improves the average mAP value of two tasks by 53.6%, 81.5%, and 48.3% on Wikipedia, Pascal, and NUS-WIDE-10k datasets, respectively. Compared with CCA-Cross-AE, our Corr-Cross-AE improves by 57.9%, 73.6%, and 28.3% on the three datasets. Compared with CCA-Full-AE, our Corr-Full- AE improves by 12.8%, 71.2%, and 46.7% on the three datasets. Based on three dif- ferent multimodal autoencoders, our correspondence models signiﬁcantly outperform the two-stage methods. In the third component of deep models, the three CCA-AEs

ACM Trans. Multimedia Comput. Commun. Appl., Vol. 12, No. 1s, Article 26, Publication date: October 2015.

26:14 F. Feng et al.

Table I. Results on Three Datasets by Two Evaluation Metrics: mAP Scores and Top 20%

(a) Wikipedia mAP Top 20% Model Image Query Text Query Average Image Query Text Query Average

CCA-AE 0.213 0.235 0.224 28.35 23.59 25.97 CCA-Cross-AE 0.197 0.230 0.214 25.54 28.14 26.84 CCA-Full-AE 0.293 0.331 0.312 51.08 49.57 50.33 Bimodal AE 0.282 0.327 0.305 44.16 42.42 43.29 Bimodal DBN 0.189 0.222 0.206 26.19 31.6 28.90

Corr-AE 0.326 0.361 0.344 56.06 55.19 55.63 Corr-Cross-AE 0.336 0.341 0.338 55.41 58.66 57.04 Corr-Full-AE 0.335 0.368 0.352 57.36 57.79 57.58

Corr-Image-AE 0.314 0.284 0.299 58.23 53.46 55.85 Corr-Text-AE 0.311 0.416 0.364 53.03 59.09 56.06

(b) Pascal mAP Top 20% Model Image Query Text Query Average Image Query Text Query Average

CCA-AE 0.161 0.153 0.157 36 30 33 CCA-Cross-AE 0.137 0.182 0.159 18 16 17 CCA-Full-AE 0.148 0.177 0.163 32 35 33.5 Bimodal AE 0.250 0.270 0.260 68 68 68 Bimodal DBN 0.219 0.219 0.219 55 61 58

Corr-AE 0.290 0.279 0.285 72 67 69.5 Corr-Cross-AE 0.271 0.280 0.276 78 75 76.5 Corr-Full-AE 0.281 0.276 0.279 74 73 73.5

Corr-Image-AE 0.275 0.264 0.269 71 72 71.5 Corr-Text-AE 0.266 0.289 0.278 68 78 73

(c) NUS-WIDE-10k mAP Top 20% Model Image Query Text Query Average Image Query Text Query Average

CCA-AE 0.199 0.268 0.234 37 33.8 35.4 CCA-Cross-AE 0.199 0.344 0.272 29 47.7 38.35 CCA-Full-AE 0.241 0.242 0.242 37.1 38.2 37.65 Bimodal AE 0.250 0.297 0.274 30.2 35.4 32.8 Bimodal DBN 0.173 0.203 0.188 25.3 27 26.15

Corr-AE 0.319 0.375 0.347 47.1 53.5 50.3 Corr-Cross-AE 0.349 0.348 0.349 53.1 59.7 56.4 Corr-Full-AE 0.331 0.379 0.355 49.6 56.5 53.05

Corr-Image-AE 0.341 0.274 0.308 58 41.3 49.65 Corr-Text-AE 0.307 0.397 0.352 40.7 58.1 49.4

ﬁrst learn image and text representations using two unimodal autoencoders, and then use CCA to capture the correlation between the two modalities. The advantage of our correspondence models over two-stage models is that correspondence models com- bine representation learning with correlation learning into an entire process. In other words, compared with our correspondence models, the two-stage methods are subop- timal. It should also be noticed that the distinction within the three correspondence

ACM Trans. Multimedia Comput. Commun. Appl., Vol. 12, No. 1s, Article 26, Publication date: October 2015.

Correspondence Autoencoders for Cross-Modal Retrieval 26:15

autoencoders are found to be smaller than the difference among the three CCA-AEs. This also demonstrates the effectiveness of the combination of representation learning and correlation learning. Compared with Bimodal AE, our Corr-AE improves mAP scores by 15.6% and 10.4% respectively on searching text by images and searching images by text on Wikipedia, improves by 16.0% and 3.3% respectively on the two tasks on Pascal, and improves by 27.6% and 26.3% respectively on the two tasks on NUS-WIDE-10k. Both Bimodal AE and Bimodal DBN model the multimodal inputs with a shared hidden layer. The representation learned in the shared hidden layer should cover the difference between two modalities for achieving a good performance in both autoencoders and RBMs. Therefore, it focuses more on learning complementarity instead of the correlation across data from different modalities. Figure 9 shows three examples of text-based cross-modal retrieval results using our Corr-Full-AE and the best baseline method evaluated on three datasets. The text queries fall into three categories: sport, aeroplane, and grass. In those three cases, the top four images retrieved by Corr-Full-AE are all relevant to the corresponding text query. See more details in the ﬁgure’s caption. As for the top 20% percentage metric, our three multimodal reconstruction Corr- AEs also signiﬁcantly outperform the other baseline models. Only one ground-truth for each image-text pair is available under this evaluation metric. Figure 10 shows several top-one hit retrieval examples by our Corr-Full-AE. In these cases, the ﬁrst retrieved results by the query (image or text) are all ground truth both on image query text and on text query image tasks. Various image and text can be retrieved effectively. Figure 11 shows several failure cases using Corr-Full-AE evaluated on the NUS- WIDE-10k. We show the text/image query, the top-ﬁve retrieved images/text, and their corresponding semantic labels. See more details in this ﬁgure’s caption.

3.5.2. Unimodal Reconstruction Corr-AEs vs. Multimodal Reconstruction Corr-AEs. We compare the two unimodal reconstruction Corr-AEs with the three multimodal reconstruction Corr-AEs. The experimental results are shown in Table I. By inspection, we ﬁnd some interesting results. The performances of the multimodal reconstruction Corr-AEs are relatively consis- tent on cross-modal retrieval tasks. They achieve a similar mAP score on both image query and text query tasks. While the two unimodal Corr-AEs perform quite unevenly on those two tasks. Corr-Text-AE performs best on text query task, but performs worst on image query task among all correspondence autoencoders. On the contrary, Corr- Image-AE performs better on image query task, but performs worst on text query task among all correspondence autoencoders. We visualize the image and text representations learned by the models using tSNE [van der Maaten and Hinton 2008] in Figure 12. In Figure 12(a), “plus ” and “square” are, to a large extent, uniformly clustered by colors in the plane. This means represen- tations of image and text learned by Corr-Full-AE are well mixed according to their semantic labels. By contrast, the “plus” clusters in Figure 12(b) tend to spread to the periphery of the “square” clusters which tend to converge. Therefore, the inter-cluster distances of text are larger, compared with the inter-cluster distances of images. It means that, in Corr-Text-AE, textual representations are more distinguishable than the visual representations. This should be caused by the reconstruction of text on both sides instead of reconstruction of text and image individually on each side in mul- timodal reconstruction models. The situation in Figure 12(c) is on the opposite side. Representations for image learned by Corr-Image-AE are more distinguishable because of the reconstruction of images on both sides.

ACM Trans. Multimedia Comput. Commun. Appl., Vol. 12, No. 1s, Article 26, Publication date: October 2015.

26:16 F. Feng et al.

Fig. 9. Examples of text-based cross-modal us- ing our Corr-Full-AE and the best baseline method on (a) Wikipedia, (b) Pascal, and (c) NUS- WIDE-10k, respectively. In each example, the text query and its corresponding image are shown at the top line; the retrieved images by Corr-Full- AE are shown in the middle; those by the best baseline model are shown at the bottom. For re- trieved images the relevant ones are surrounded with green bounding boxes, the irrelevant with the red.

Fig. 10. Nine top-one hit examples of cross-modal re- trieval by our Corr-Full-AE evaluated on (a) Wikipedia, (b) Pascal, and (c) NUS-WIDE-10k, respectively. Three image-text pairs in (a) come from warfare, biology, and history categories, respectively. Three pairs in (b) come from bicycle, cow, and person categories, respectively. And three pairs in (c) come from animal, ﬂower, and food categories, respectively.

ACM Trans. Multimedia Comput. Commun. Appl., Vol. 12, No. 1s, Article 26, Publication date: October 2015.

Correspondence Autoencoders for Cross-Modal Retrieval 26:17

Fig. 11. Four failure cases using Corr-Full-AE evaluated on the NUS-WIDE-10k dataset: the subﬁgure (a) for text query image and the subﬁgure (b) for image query text. The words under the text/image are the corresponding semantic labels. For the task of text query image, the retrieved images are related to the subset of the text query. For example, in the ﬁrst case, the ﬁrst retrieved image is related to two words: “windows” and “glasses”. For the task of image query text, the image query is too difﬁcult to be deﬁnitely recognized. For example, in the second case, the image query is easily misidentiﬁed as a kind of ﬂower.

Returning to Table I, we ﬁnd the situations shown in Figure 12 help much in under- standing the aforementioned results. For the task of text query image, distinct query is the key to cross-modal retrieval when image-text pairs are the same. Therefore, Corr-Text-AE which best distinguishes textual clusters achieves the best performance in text query task among all models, but performs worst on image query task among all Corr-AEs. In contrast, Corr-Image-AE is at the opposite end. Corr-Full-AE which reconstructs both text and image is in the middle and shows relatively consistent performance on both cross-modal retrieval tasks. Furthermore, when we check the results in detail, we ﬁnd that Corr-Text-AE per- forms diversely with signiﬁcance on different datasets. It improves mAP scores by 13%, 3.2%, and 4.7% on Wikipedia, Pascal, and NUS-WIDE-10k datasets respectively com- pared with the best performance of multimodal Corr-AEs. We infer that the diversity of improvement among different datasets is caused by the sparsities of text in these datasets. Speciﬁcally, the number of words in Wikipedia exceeds those of Pascal and NUS-WIDE-10k. On average, there are 219 words in one example of Wikipedia and only about six words in that of Pascal and NUS-WIDE-10k. The text modality impacts largely on the representation learning in Wikipedia dataset and make representations more distinguishable for text. Therefore, Corr-Text-AE achieves larger improvement on text query image task in Wikipedia. In conclusion, Corr-Text-AE, in practice, is a better choice for text query only cross- modal retrieval task. And multimodal reconstruction Corr-AEs is suitable for cross- modal retrieval tasks in both ways.

ACM Trans. Multimedia Comput. Commun. Appl., Vol. 12, No. 1s, Article 26, Publication date: October 2015.

26:18 F. Feng et al.

Fig. 12. Visualization of image and text representations obtained on the NUS-WIDE-10k test dataset by (a) Corr-Full-AE, (b) Corr-Text-AE, and (c) Corr-Image-AE, respectively. We use different shapes to denote different modalities: “square” for image and “plus” for text. In addition, we use different colors to denote different semantic categories.

ACM Trans. Multimedia Comput. Commun. Appl., Vol. 12, No. 1s, Article 26, Publication date: October 2015.

Correspondence Autoencoders for Cross-Modal Retrieval 26:19

Fig. 13. mAP values of three correspondence autoencoders with different values of α in all datasets. The X-axis denotes the value of α. The Y-axis denotes the mAP score. And the yellow line denotes the mAP score of the best baseline model, which does not change with the parameter of α.

3.6. Analysis of the Parameter α In this section, we take the three multimodal reconstruction Corr-AEs as an example to analyze the impact of the parameter α. Here, the mAP values of three multimodal re- construction Corr-AEs with different values of α in all datasets are shown in Figure 13. Both too small and too large values of α show poorer performance in the three datasets. This is consistent with the nature of α in the present models. Too small values of α overemphasize the “individuality” of data and ignore the correlations. On the contrary, too large values overemphasize the correlations and ignore the “individuality” of the data. To validate the hypothesis of the effect of α to the “individuality” of data, we also use tSNE [van der Maaten and Hinton 2008] to visualize the image and text repre- sentations learned by our Corr-Full-AE. As shown in Figure 14, when α equals 0.01, image and text representation space are almost disjoint. In this case, image and text have strong “individuality” such that the learned representations of image and text have no correlations. To the other extreme, when α equals 0.99, “individuality” of image and text representations is missing due to the confused representation space. In this case, any image-text pair has correlations, regardless of whether the image and text inputs match or not. When α is set to 0.2, portions of bimodal data belong- ing to the same category are clustered. For instance, the “green squares” and the “green pluses” are clustered. When α is set to 0.8 shown in Figure 12(a), the rep- resentation space is quite effective to the cross-modal retrieval task, since a large number of image-text pairs with the same semantic labels are clustered. For example, bimodal data belonging to “green”, “red”, and “black” categories are clustered quite well. Furthermore, as shown in Figure 13, the yellow line denotes the mAP scores of the best baseline model. On all datasets, the three multimodal reconstruction Corr-AEs outperform the best baseline even when α is in a quite large range.

4. CONCLUSION In this work, two groups of cross-modal learning models are presented. They inte- grate representation learning and correlation learning into a single process by combin- ing the autoencoder cost with the correlation cost. Out of them, one group including three multimodal reconstruction Corr-AEs reconstructs all modalities. Consequently, these models can learn the effective representations for all modalities. The other group including two unimodal reconstruction Corr-AEs only reconstructs one modal- ity. Hence, these models only learn the effective representation for the modality to be

ACM Trans. Multimedia Comput. Commun. Appl., Vol. 12, No. 1s, Article 26, Publication date: October 2015.

26:20 F. Feng et al.

Fig. 14. Visualization of image and text representations on the NUS-WIDE-10k test dataset with different values of α: (a) for 0.01, (b) for 0.2, and (c) for 0.99, respectively. The representation is learned by Corr-Full- AE, one of multimodal reconstruction Corr-AEs. The subﬁgure (a) in 12 shows the visual representations when α equals 0.8. The meanings of shapes and colors are the same as those in Figure 12.

reconstructed. These two groups of models are fully compared with the state-of-the-art CCA-based and multimodal deep learning models on three publicly available datasets. The experimental results demonstrate their effectiveness in two cross-modal retrieval tasks.

ACM Trans. Multimedia Comput. Commun. Appl., Vol. 12, No. 1s, Article 26, Publication date: October 2015.

Correspondence Autoencoders for Cross-Modal Retrieval 26:21

REFERENCES

Muhammet Bastan, Hayati Cam, Ugur Gdkbay, and ¨Ozg¨ur Ulusoy. 2010. Bilvideo-7: An MPEG-7-compatible video indexing and retrieval system. IEEE MultiMedia 17, 3, 62–73. Yoshua Bengio. 2009. Learning deep architectures for AI. Found. Trends Machine Learn. 2, 1, 1–127. Steven Bird. 2006. NLTK: the natural language toolkit. In Proceedings of the COLING/ACL on Interactive Presentation Sessions (COLING-ACL’06). 69–72. David M. Blei and Michael I. Jordan. 2003. Modeling Annotated Data. In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval (SIGIR’03). 127–134. David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. J. Machine Learn. Res. 3, 993–1022. Anna Bosch, Andrew Zisserman, and Xavier Mu˜noz. 2007. Image Classiﬁcation using Random Forests and Ferns. In Proceedings of the International Conference on Computer Vision (ICCV’07). 1–8. Tat-Seng Chua, Jinhui Tang, Richang Hong, Haojie Li, Zhiping Luo, and Yan-Tao Zheng. 2009. NUS-WIDE: A real-world web image database from National University of Singapore. In Proceedings of ACM Conference on Image and Video Retrieval (CIVR’09). 1–9. Ali Farhadi, Seyyed Mohammad Mohsen Hejrati, Mohammad Amin Sadeghi, Peter Young, Cyrus Rashtchian, Julia Hockenmaier, and David A. Forsyth. 2010. Every picture tells a story: Generating sentences from images. In Proceedings of the European Conference on Computer Vision (ECCV’10). 15–29. Fangxiang Feng, XiaojieWang, and Ruifan Li. 2014. Cross-modal retrieval with correspondence autoencoder. In Proceedings of the International Conference on Multimedia (MM’14). 7–16. Andrea Frome, Greg Corrado, Jon Shlens, Samy Bengio, Jeffrey Dean, Marc’Aurelio Ranzato, and Tomas Mikolov. 2013. DeViSE: A deep visual-semantic embedding model. In Neural Information Processing Systems (NIPS’13), 2121–2129. David R. Hardoon, Sndor Szedmk, and John Shawe-Taylor. 2004. Canonical correlation analysis; An overview with application to learning methods. Neural Comput. 16, 2639–2664. G. Hinton and R. Salakhutdinov. 2006. Reducing the dimensionality of data with neural networks. Science 313, 5786, 504–507. G. E. Hinton. 2002. Training products of experts by minimizing contrastive divergence. Neural Comput. 14, 8, 1771–1800. G. E. Hinton, S. Osindero, and Y. Teh. 2006. A fast learning algorithm for deep belief nets. Neural Comput. 18, 7, 1527–1554. Yangqing Jia, Mathieu Salzmann, and Trevor Darrell. 2011. Learning cross-modality similarity for multi- nomial data. In Proceedings of the International Conference on Computer Vision (ICCV’11). 2407–2414. Jungi Kim, Jinseok Nam, and Iryna Gurevych. 2012. Learning semantics with deep belief network for cross- language information retrieval. In Proceedings of the 25th International Conference on Computational Linguistics (COLING’12). 579–588. B. S. Manjunath, J. R. Ohm, V. V. Vinod, and A. Yamada. 2001. Color and texture descriptors. IEEE Trans. Circuits Syst. Video Technol. 11, 6, 703–715. J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng. 2011. Multimodal deep learning. In Proceedings of the 28th International Conference on Machine Learning (ICML’11). 689–696. A. Oliva and A. Torralba. 2001. Modeling the shape of the scene: A holistic representation of the spatial envelope. Int. J. Comput. Vision 42, 3, 145–175. Nikhil Rasiwasia, Jose Costa Pereira, Emanuele Coviello, Gabriel Doyle, Gert R. G. Lanckriet, Roger Levy, and Nuno Vasconcelos. 2010. A new approach to cross-modal multimedia retrieval. In Proceedings of the International Conference on Multimedia (MM’10). 251–260. R. Salakhutdinov and G. Hinton. 2009. Replicated Softmax: an Undirected Topic Model. In Neural Informa- tion Processing Systems (NIPS’09), 1607–1614. Ruslan R. Salakhutdinov and Geoffrey G. Hinton. 2012. An efﬁcient learning procedure for deep Boltzmann machines. Neural Comput. 24, 8, 1967–2006. Carina Silberer and Mirella Lapata. 2014. Learning grounded meaning representations with autoencoders. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 721–732. P. Smolensky. 1986. Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1. MIT Press, Cambridge, MA, Chapter Information processing in dynamical systems: foundations of harmony theory, 194–281.

ACM Trans. Multimedia Comput. Commun. Appl., Vol. 12, No. 1s, Article 26, Publication date: October 2015.

26:22 F. Feng et al.

Richard Socher, Milind Ganjoo, Christopher D. Manning, and Andrew Ng. 2013. Zero-shot learning through cross-modal transfer. In Neural Information Processing Systems (NIPS’13), 935–943. N. Srivastava and R. Salakhutdinov. 2012a. Learning representations for multimodal data with deep belief nets. In Proceedings of the International Conference on Machine Learning Representation Learning Workshop. N. Srivastava and R. Salakhutdinov. 2012b. Multimodal learning with deep Boltzmann machines. In Neural Information Processing Systems (NIPS’12), 2231–2239. L. J. P. van der Maaten and G. E. Hinton. 2008. Visualizing High-Dimensional Data Using t-SNE. J. Machine Learn. Res. 9, 2579–2605. Andrea Vedaldi and Brian Fulkerson. 2010. Vlfeat: An open and portable library of computer vision algo- rithms. In Proceedings of the International Conference on Multimedia (MM’10). 1469–1472. M. Welling, M. Rosen-Zvi, and G. Hinton. 2004. Exponential family harmoniums with an application to information retrieval. In Neural Information Processing Systems (NIPS’04), 501–508. Jason Weston, Samy Bengio, and Nicolas Usunier. 2010. Large scale image annotation: Learning to rank with joint word-image embeddings. In Proceedings of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD’10). 21–35. Yueting Zhuang, Yan Fei Wang, Fei Wu, Yin Zhang, and Weiming Lu. 2013. Supervised coupled dictionary learning with group structures for multi-modal retrieval. In Proceedings of the 27th AAAI Conference on Artiﬁcial Intelligence (AAAI’13). 1070–1076.

Received January 2015; revised May 2015, July 2015; accepted July 2015

ACM Trans. Multimedia Comput. Commun. Appl., Vol. 12, No. 1s, Article 26, Publication date: October 2015.