密级
：保密期限
：


避Ｉ部會夂壤


硕士学位论文


＿


题目
：基于提示学习的少样本


文本分类研究与应用


学号
：
２０１９１８０１
１０


姓名
：
魏志宇


专业
：计算机技术


导师
：
李睿凡


学院
：人工智能学院


２０２２年
６月
１
日


中国
■北京


密级：保密期限
：


却ｔ大聲


硕士学位论文


题目
：基于提示学习的少样本


文本分类研究与应用


学号
：
２０１９１８０１１０


姓
名
：
魏志宇


专业
：计職技术


导师
：
李睿凡


学院
人工智能学院


２０２２年６月
１
日


ＳｅｃｒｅｔＬｅｖｅｌ
：Ｃｏｎｆ
ｉｄｅｎｔｉａｌｉｔｙＰｅｒｉｏｄ
：


Ｂｅｉｊ
ｉｎｇＵｎｉｖｅｒｓｉｔｙｏｆＰｏｓｔｓａｎｄ


Ｔｅｌｅｃｏｍｍｕｎｉｃａｔｉｏｎｓ


ＴｈｅｓｉｓｆｏｒＭａｓｔｅｒＤｅｇｒｅｅ


ｍ


ＴＩＴＬＥ：ＲＥＳＥＡＲＣＨＡＮＤＡＰＰＬＩＣＡＴＩＯＮ


ＯＦＦＥＷ
－ＳＨＯＴＴＥＸＴＣＬＡＳＳＩＦＩＣＡＴＩＯＮ


ＢＡＳＥＤＯＮＰＲＯＭＰＴＬＥＡＲＮＩＮＧ


ＳｔｕｄｅｎｔＩＤ
：２０１９１８０１１０


Ｃａｎｄｉｄａｔｅ：ＷｅｉＺｈｉｙｕ


Ｓｕｂｊｅｃｔ：ＣｏｍｐｕｔｅｒＴｅｃｈｎｏｌｏｇｙ


Ｓｕｐｅｒｖｉｓｏｒ：ＬｉＲｕｉｆａｎ


Ｉｎｓｔｉｔｕｔｅ：ＳｃｈｏｏｌｏｆＡｒｔｉｆｉｃｉａｌＩｎｔｅｌｌｉｇｅｎｃｅ


Ｊｕｎｅ１
，２０２２


基于提示学习的少样本文本分类研究与应用


摘
要


近年来
，基于深度学习的方法在诸多自然语言处理任务上取得了


令人骄傲的成绩
。
然而
，
深度模型通常需要大量高质量的标注语料
，


以达到对网络中大规模参数的不断调优的目的
。
目前的数据标注工作


更多依赖于人工方式
，这使得想获取大量优质的标注数据依旧十分困


难
。
文本分类任务作为ＮＬＰ领域最常见的任务类型之
一
，
对每个新


的领域任务都去标注大规模的数据通常是不现实的
。少样本学习旨在


利用己学习的先验知识
，通过有限的带有标注的训练数据
，就可以快


速的在目标任务上完成模型的优化
。通过少样本学习
，既可以降低有


标注数据的获取成本
，
又可以加速模型的落地部署和缩短迭代周期
。


随着ＢＥＲＴ等预训练语言模型的提出
，基于预训练和微调的两阶


段训练范式逐渐成为新的趋势
，并在多数的自然语言处理任务上取得


了空前的成功
。但是在微调阶段
，模型的性能通常取决于任务和有标


注训练数据的数量
。然而在大多数情况下
，通常难以获取大量相关领


域的标注数据
，这使得当模型在面对仅含少量训练样本的下游任务时
，


往往表现不佳
。与微调方法不同的是
，基于提示学习的方法
，通过添


加灵活的提示信息
，
将具体下游任务与预训练任务在形式上相统
一
，


使得在低资源场景下也能够取得良好的效果
。提示学习的优势在于既


不需要大量相关领域的数据进行提前训练
，也无需显著的改变预训练


语言模型结构和参数
，仅通过对任务形式和输入形式的改变
，就可以


达到利用预训练语言模型中已经获得的通用领域知识的目的
，实现真


正的少样本学习
。


本文针对文本分类任务
，对基于提示学习的方法进行少样本学习


研究
，
开展如下工作
：


１
）
当前基于提示学习的少样本文本分类方法仅利用了预训练语


言模型中的通用知识
，而忽略了下游任务中的具体类别表征表示
。本


文提出了
一种基于提示学习和三元组损失的少样本文本分类算法
。该


算法将文本分类任务转换成基于自然语言推理的提示学习形式
，通过


任务形式的转换
，
实现在利用预训练语言模型的先验知识的基础上
，


达到隐式的数据增强
，
并通过两种不同粒度的损失进行优化
。
并且
，


为了捕获下游任务中丰富的类别表征信息
，通过三元组损失进行联合


Ｉ


优化
，同时引入掩码语言模型任务作为正则项
，提升模型的泛化能力
。


此外
，本文又在所提出方法的基础上设计了合适的预训练任务进行进


一步地预训练
。最终在中
、英文数据集中验证了本文提出方法的有效


性
。


２）设计并实现了基于少样本学习的文本分类任务智能标注工具
。


在本系统中
，保留了传统标注工具中通过自定义快捷操作方式进行的


人工标注形式
，
便于用户灵活标注
。
同时基于上述的算法研宄成果
，


用户仅需提供少量的标注数据
，通过简单地参数配置
，就可以完成模


型的在线训练以及对数据的智能标注工作
。此外
，用户可通过智能标


注结果的反馈
，更新训练数据
，
即通过主动学习策略
，持续提升模型


的性能和数据标注质量
。最后通过
一系列的系统测试
，表明本系统功


能满足设计需求
，
可稳定运行
。


关键词
：预训练语言模型少样本学习提示学习
文本分类


ｎ


ＲＥＳＥＡＲＣＨＡＮＤＡＰＰＬＩＣＡＴＩＯＮＯＦＦＥＷ
－ＳＨＯＴＴＥＸＴ


ＣＬＡＳＳＩＦＩＣＡＴＩＯＮＢＡＳＥＤＯＮＰＲＯＭＰＴＬＥＡＲＮＩＮＧ


ＡＢＳＴＲＡＣＴ


Ｉｎｒｅｃｅｎｔｙｅａｒｓ
，ｔｈｅｍｅｔｈｏｄｂａｓｅｄｏｎｄｅｅｐ
ｌｅａｒｎｉｎｇｈａｓａｃｈｉｅｖｅｄ


ｉｍｐｒｅｓｓｉｖｅｒｅｓｕｌｔｓｉｎｍａｎｙｎａｔｕｒａｌｌａｎｇｕａｇｅｐｒｏｃｅｓｓｉｎｇ
ｔａｓｋｓ
．Ｈｏｗｅｖｅｒ
，


ｄｅｅｐｍｏｄｅｌｓｕｓｕａｌｌｙｒｅｑｕｉｒｅａｌａｒｇｅａｍｏｕｎｔｏｆｈｉｇｈ
－
ｑｕａｌｉｔｙａｎｎｏｔａｔｅｄ


ｃｏｒｐｏｒａｆｏｒｔｈｅ
ｐｕｒｐｏｓｅｏｆｃｏｎｔｉｎｕｏｕｓｔｕｎｉｎｇｏｆｌａｒｇｅ
－ｓｃａｌｅｐａｒａｍｅｔｅｒｓｉｎ


ｔｈｅｎｅｔｗｏｒｋ
．Ｃｕｒｒｅｎｔｄａｔａａｎｎｏｔａｔｉｏｎｒｅｌｉｅｓｍｏｒｅｏｎｍａｎｕａｌｍｅｔｈｏｄｓ
，


ｗｈｉｃｈｍａｋｅｓｉｔｓｔｉｌｌｖｅｒｙｄｉｆ
ｉｃｕｌｔｔｏｏｂｔａｉｎａｌａｒｇｅａｍｏｕｎｔｏｆｈｉｇｈ
－
ｑｕａｌｉｔｙ


ａｎｎｏｔａｔｉｏｎｄａｔａ
．Ｉｔｉｓｕｎｒｅａｌｉｓｔｉｃｔｏｌａｂｅｌｌａｒｇｅ
－ｓｃａｌｅｄａｔａｆｏｒｅａｃｈｎｅｗ


ｄｏｍａｉｎｔａｓｋｆｏｒｔｅｘｔｃｌａｓｓｉｆ
ｉｃａｔｉｏｎｔａｓｋｓ
，ｏｎｅｏｆｔｈｅｍｏｓｔｃｏｍｍｏｎｔｙｐｅｓｏｆ


ｔａｓｋｓｉｎｔｈｅＮＬＰ．Ｆｅｗ
－ｓｈｏｔｌｅａｒｎｉｎｇａｉｍｓｔｏｕｓｅｔｈｅｌｅａｒ
ｎｅｄｐｒｉｏｒ


ｋｎｏｗｌｅｄｇｅｔｏｑｕｉｃｋｌｙｏｐ
ｔｉｍｉｚｅｔｈｅｍｏｄｅｌｏｎｔｈｅｔａｒｇｅｔｔａｓｋｗｉｔｈｌｉｍｉｔｅｄ


ｌａｂｅｌｅｄｄａｔａ
．Ｔｈｒｏｕｇｈｆｅｗ
－ｓｈｏｔｌｅａｒｎｉｎｇ，
ｉｔｃａｎｎｏｔｏｎｌｙｒｅｄｕｃｅｔｈｅｃｏｓｔｏｆ


ｏｂｔａｉｎｉｎｇ
ｌａｂｅｌｅｄｄａｔａ
，ｂｕｔａｌｓｏａｃｃｅｌｅｒａｔｅｔｈｅｄｅｐ
ｌｏｙｍｅｎｔｏｆ
ｔｈｅｍｏｄｅｌａｎｄ


ｓｈｏｒｔｅｎｔｈｅｉｔｅｒａｔｉｏｎｃｙｃｌｅ
．


Ｗｉｔｈｔｈｅ
ｐｒｏｐｏｓａｌｏｆ
ｐｒｅ
－ｔｒａｉｎｅｄｌａｎｇｕａｇｅｍｏｄｅｌｓｓｕｃｈａｓＢＥＲＴ
，ｔｈｅ


ｔｗｏ
－ｓｔａｇｅｔｒａｉｎｉｎｇｐａｒａｄｉｇｍｂａｓｅｄｏｎｐｒｅ
－
ｔｒａｉｎｉｎｇａｎｄｆｉｎｅ
－ｔｕｎｉｎｇｈａｓ


ｇｒａｄｕａｌｌｙｂｅｃｏｍｅａｎｅｗｔｒｅｎｄａｎｄｈａｓａｃｈｉｅｖｅｄｕｎｐｒｅｃｅｄｅｎｔｅｄｓｕｃｃｅｓｓｉｎ


ｍｏｓｔｎａｔｕｒａｌｌａｎｇｕａｇｅｐｒｏｃｅｓｓｉｎｇ
ｔａｓｋｓ
．Ｂｕｔｄｕｒｉｎｇ
ｔｈｅｆ
ｉｎｅ
－ｔｕｎｉｎｇｐｈａｓｅ
，


ｔｈｅ
ｐｅｒｆｏｒｍａｎｃｅｏｆｔｈｅｍｏｄｅｌｕｓｕａｌｌｙｄｅｐｅｎｄｓｏｎｔｈｅｔａｓｋａｎｄｔｈｅａｍｏｕｎｔ


ｏｆａｎｎｏｔａｔｅｄｄａｔａ
．Ｈｏｗｅｖｅｒ
，
ｉｎｍｏｓｔｃａｓｅｓ
，
ｉｔｉｓｏｆ
ｔｅｎｄｉｆ
ｉｃｕｌｔｔｏｏｂｔａｉｎａ


ｌａｒｇｅａｍｏｕｎｔｏｆｒｅｌｅｖａｎｔｄｏｍａｉｎａｎｎｏｔａｔｅｄｄａｔａ
，ｗｈｉｃｈｍａｋｅｓｔｈｅｍｏｄｅｌ


ｏｆ
ｔｅｎｐｅｒｆｏｒｍｂａｄｌｙｗｈｅｎｆａｃｅｄｗｉｔｈｄｏｗｎｓｔｒｅａｍｔａｓｋｓｗｉｔｈｌｉｍｉｔｅｄ


ｔｒａｉｎｉｎｇｓａｍｐ
ｌｅｓ
．Ｄｉｆｆｅｒｅｎｔｆ
ｒｏｍｆｍｅ
－
ｔｕｎｉｎｇ，ｔｈｅｍｅｔｈｏｄｂａｓｅｄｏｎ
ｐｒｏｍｐ
ｔ


ｌｅａｒｎｉｎｇ，ｂｙａｄｄｉｎｇｆｌｅｘｉｂｌｅｐｒｏｍｐ
ｔｉｎｆｏｒｍａｔｉｏｎ
，ｕｎｉｆｉｅｓｔｈｅｓｐｅｃｉｆｉｃ


ｄｏｗｎｓｔｒｅａｍｔａｓｋｓａｎｄ
ｐｒｅ
－ｔｒａｉｎｉｎｇｔａｓｋｓｉｎｆｏｒｍ
，ｓｏｔｈａｔ
ｇｏｏｄｒｅｓｕｌｔｓｃａｎ


ｂｅａｃｈｉｅｖｅｄｉｎｌｏｗ
－ｒｅｓｏｕｒｃｅｓｃｅｎａｒｉｏｓ
．Ｔｈｅａｄｖａｎｔａｇｅｏｆ
ｐｒｏｍｐｔｌｅａｒｎｉｎｇ


ｉｓｔｈａｔｉｔｄｏｅｓｎｏｔｒｅｑｕｉｒｅａｌａｒｇｅａｍｏｕｎｔｏｆｄａｔａｉｎｒｅｌａｔｅｄｆ
ｉｅｌｄｓｆｏｒｐｒｅ
－


ｉｉ
ｉ


ｔｒａｉｎｉｎｇ，ｎｏｒｄｏｅｓｉｔｎｅｅｄｔｏｓｉｇｎｉｆｉｃａｎｔｌｙｃｈａｎｇｅｔｈｅｓｔｒｕｃｔｕｒｅａｎｄ


ｐａｒａｍｅｔｅｒｓｏｆｔｈｅ
ｐｒｅ
－ｔｒａｉｎｅｄｌａｎｇｕａｇｅｍｏｄｅｌ
．Ｃｈａｎｇ
ｉｎｇ
ｔｈｅｔａｓｋｆｏｒｍａｎｄ


ｉｎｐｕｔｆｏｒｍａｌｏｎｅｍａｋｅｓｉｔ
ｐｏｓｓｉｂｌｅｔｏｕｓｅｔｈｅｇｅｎｅｒａｌｄｏｍａｉｎｋｎｏｗｌｅｄｇｅ


ｆｒｏｍｔｈｅ
ｐｒｅ
－ｔｒａｉｎｅｄｌａｎｇｕａｇｅｍｏｄｅｌｔｏａｃｈｉｅｖｅｆｅｗ
－ｓｈｏｔｌｅａｒｎｉｎｇ
．


Ｔｈｉｓｐａｐｅｒｃｏｎｄｕｃｔｓｆｅｗ
－ｓｈｏｔｌｅａｒｎｉｎｇｒｅｓｅａｒｃｈｏｎｐｒｏｍｐ
ｔｌｅａｒｎｉｎｇ


ｆｏｒｔｅｘｔｃｌａｓｓｉｆｉｃａｔｉｏｎｔａｓｋｓ
，ａｎｄｃａｒｒｉｅｓｏｕｔｔｈｅｆｏｌｌｏｗｉｎｇｗｏｒｋ
：


１
）Ｔｈｅｃｕｒｒｅｎｔｆｅｗ
－ｓｈｏｔｔｅｘｔｃｌａｓｓｉｆ
ｉｃａｔｉｏｎｍｅｔｈｏｄｓｂａｓｅｄｏｎｐｒｏｍｐ
ｔ


ｌｅａｒｎｉｎｇｏｎｌｙｕｔｉｌｉｚｅｔｈｅｇｅｎｅｒａｌｋｎｏｗｌｅｄｇｅｉｎｔｈｅｐｒｅ
－ｔｒａｉｎｅｄｌａｎｇｕａｇｅ


ｍｏｄｅｌ
，
ｉｇｎｏｒｉｎｇｓｐｅｃｉｆｉｃｃｌａｓｓｒｅｐｒｅｓｅｎｔａｔｉｏｎｓｉｎｄｏｗｎｓｔｒｅａｍｔａｓｋｓ
．Ｉｎｔｈｉｓ


ｐａｐｅｒ
，ｗｅ
ｐｒｏｐｏｓｅａｆｅｗ
－ｓｈｏｔｔｅｘｔｃｌａｓｓｉｆｉｃａｔｉｏｎａｌｇｏｒｉｔｈｍｂａｓｅｄｏｎ
ｐｒｏｍｐ
ｔ


ｌｅａｒｎ
ｉｎｇａｎｄｔｒｉｐ
ｌｅｔｌｏｓｓ
．Ｔｈｅｍｅｔｈｏｄｃｏｎｖｅｒｔｓｔｈｅｔｅｘｔｃｌａｓｓｉｆｉｃａｔｉｏｎｔａｓｋ


ｉｎｔｏｐｒｏｍｐ
ｔｌｅａｒ
ｎｉｎｇｂａｓｅｄｏｎｎａｔｕｒａｌｌａｎｇｕａｇｅｉｎｆｅｒｅｎｃｅ
．Ｔｈｒｏｕｇｈｔｈｅ


ｔｒａｎｓｆｏｒｍａｔｉｏｎｏｆｔａｓｋｆｏｒｍ
，ｔｈｅｉｍｐ
ｌｉｃｉｔｄａｔａｅｎｈａｎｃｅｍｅｎｔｉｓａｃｈｉｅｖｅｄ


ｂａｓｅｄｏｎｐｒｉｏｒｋｎｏｗｌｅｄｇｅｏｆｔｈｅｐｒｅ
－ｔｒａｉｎｉｎｇ
ｌａｎｇｕａｇｅｍｏｄｅｌａｎｄ


ｏｐ
ｔｉｍｉｚｅｄｂｙ
ｔｗｏｄｉｆｆｅｒｅｎｔ
ｇｒａｎｕｌａｒｉｔｙ
ｌｏｓｓｅｓ
．Ｍｏｒｅｏｖｅｒ
，
ｉｎｏｒｄｅｒｔｏｃａｐｔｕｒｅ


ｔｈｅｒｉｃｈｃａｔｅｇｏｒｙｒｅｐｒｅｓｅｎｔａｔｉｏｎｉｎｄｏｗｎｓｔｒｅａｍｔａｓｋｓ
，ｔｈｅｔｒｉｐ
ｌｅｔｌｏｓｓｉｓｕｓｅｄ


ｆｏｒ
ｊｏｉｎｔｏｐｔｉｍｉｚａｔｉｏｎ
，ａｎｄｔｈｅｍａｓｋｅｄ
－
ｌａｎｇｕａｇｅｍｏｄｅｌｉｓｉｎｔｒｏｄｕｃｅｄａｓａ


ｒｅｇｕｌａｒｔｅｒｍｔｏｉｍｐｒｏｖｅｔｈｅ
ｇｅｎｅｒａｌｉｚａｔｉｏｎａｂｉｌｉｔｙｏｆ
ｔｈｅｍｏｄｅｌ
．Ｉｎａｄｄｉｔｉｏｎ
，


ｗｅｄｅｓｉｇｎａｎａｐｐｒｏｐｒ
ｉａｔｅ
ｐｒｅ
－ｔｒａｉｎｉｎｇ
ｔａｓｋｆｏｒｆｕｒｔｈｅｒ
ｐｒｅ
－ｔｒａｉｎｉｎｇｂａｓｅｄｏｎ


ｔｈｅ
ｐｒｏｐｏｓｅｄｍｅｔｈｏｄ
．Ｆｉｎａｌｌｙ，ｔｈｅｅｆｆｅｃｔｉｖｅｎｅｓｓｏｆｔｈｅ
ｐｒｏｐｏｓｅｄｍｅｔｈｏｄｉｓ


ｖｅｒｉｆｉｅｄｉｎＣｈｉｎｅｓｅａｎｄＥｎｇ
ｌｉｓｈｄａｔａｓｅｔｓ
．


２
）Ａｎｉｎｔｅｌ
ｌｉｇｅｎｔａｎｎｏｔａｔｉｏｎｔｏｏｌｆｏｒｔｅｘｔｃｌａｓｓｉｆｉｃａｔｉｏｎｔａｓｋｓｂａｓｅｄｏｎ


ｆｅｗ
－ｓｈｏｔｌｅａｒｎｉｎｇ
ｉｓｄｅｓｉｇｎｅｄａｎｄｉｍｐ
ｌｅｍｅｎｔｅｄ
．Ｉｎｔｈｉｓｓｙｓｔｅｍ
，ｔｈｅｍａｎｕａｌ


ｌａｂｅｌｉｎｇｆｏｒｍｏｆｔｈｅｔｒａｄｉｔｉｏｎａｌｌａｂｅｌｉｎｇ
ｔｏｏｌｔｈｒｏｕｇｈｔｈｅｃｕｓｔｏｍｓｈｏｒｔｃｕｔ


ｏｐｅｒａｔｉｏｎｉｓｒｅｔａｉｎｅｄ
，ｗｈｉｃｈｉｓｃｏｎｖｅｎｉｅｎｔｆｏｒｕｓｅｒｓｔｏｌａｂｅｌｆｌｅｘｉｂｌｙ
．Ａｔｔｈｅ


ｓａｍｅｔｉｍｅ
，ｂａｓｅｄｏｎｔｈｅａｂｏｖｅａｌｇｏｒｉｔｈｍｒｅｓｅａｒｃｈｒｅｓｕｌｔｓ
，ｔｈｅｏｎｌｉｎｅ


ｔｒａｉｎｉｎｇｏｆｔｈｅｍｏｄｅｌａｎｄｔｈｅｉｎｔｅｌｌ
ｉｇｅｎｔｄａｔａａｎｎｏｔａｔｉｏｎｃａｎｂｅｃｏｍｐ
ｌｅｔｅｄ


ｕｓｉｎｇａｓｍａｌｌａｍｏｕｎｔｏｆｌａｂｅｌｅｄｄａｔａ
，ａｎｄｔｈｒｏｕｇｈｓｉｍｐ
ｌｅｐａｒａｍｅｔｅｒ


ｃｏｎｆｉｇｕｒａｔｉｏｎｂｙ
ｔｈｅｕｓｅｒ．Ｉｎａｄｄｉｔｉｏｎ
，ｕｓｅｒｓｃａｎｕｐｄａｔｅｔｒａｉｎｉｎｇｄａｔａ


ｔｈｒｏｕｇｈｔｈｅｆｅｅｄｂａｃｋｏｆｉｎｔｅｌｌｉｇｅｎｔａｎｎｏｔａｔｉｏｎｒｅｓｕｌｔｓ
．Ｉｎｏｔｈｅｒｗｏｒｄｓ
，ｗｅ


ｃａｎｃｏｎｔｉｎｕｏｕｓｌｙ
ｉｍｐｒｏｖｅｔｈｅ
ｐｅｒｆｏｒｍａｎｃｅｏｆｔｈｅｍｏｄｅｌａｎｄｔｈｅ
ｑｕａｌｉｔｙｏｆ


ｄａｔａａｎｎｏｔａｔｉｏｎｔｈｒｏｕｇｈａｃｔｉｖｅｌｅａｒｎｉｎｇｓｔｒａｔｅｇ
ｉｅｓ
．Ｆｉｎａｌｌｙ，ｔｈｒｏｕｇｈａｓｅｒ
ｉｅｓ


ｏｆｓｙｓｔｅｍｔｅｓｔｓ
，
ｉｔｓｈｏｗｓｔｈａｔｔｈｅｓｙｓｔｅｍｆｕｎｃｔｉｏｎｓｍｅｅｔｔｈｅｄｅｓｉｇｎ


ｒｅｑｕｉｒｅｍｅｎｔｓａｎｄｃａｎｒｕｎｓｔａｂｌｙ
．


ＩＶ


ＫＥＹＷＯＲＤＳ：ｐｒｅ
－ｔｒａｉｎｅｄｌａｎｇｕａｇｅｍｏｄｅｌ
，ｆｅｗ
－ｓｈｏｔｌｅａｒｎｉｎｇ，ｐｒｏｍｐ
ｔ


ｌｅａｒｎｉｎｇ，ｔｅｘｔｃｌａｓｓｉｆ
ｉｃａｔｉｏｎ


ｖ


目
录


第
一章绪论
１


１
．１研宄背景及意义
１


１
．２
国内外研究现状
２


１
．２
．
１基于度量学习的方法
２


１
．２
．２基于元学习的方法
３


１
．２
．３基于提示学习的方法
４


１
．３主要研宄内容
５


１
．４本文组织结构
６


１
．５本章小结
７


第二章
相关基础知识
９


２
．
１
少样本学习
９


２
．２ｎ元语法模型
９


２
．２
．
１概述
９


２
．２
．２拉普拉斯平滑算法
１０


２
．３
自然语言推理
１
１


２
．４度量学习相关技术
１
１


２
．４
．
１度量学
方法
１１


２
．４
．２度量学习中的损失函数

１２


２
．５词向量化表示技术
１３


２
．５
．
１Ｗｏｒｄ２Ｖｅｃ
１３


２
．５
．２ＥＬＭｏ
１５


２
．６预训练语言模型
１６


２
．６
．
１ＧＰＴ
１７


２
．６
．２ＢＥＲＴ
１８


２
．７提不学习—
２１


２
．７
．
１概述
２１


２
．７
．２提示学习形式
２２


２
．７
．３训练策略
２３


２
．８本章小结
２４


第三章基于提示学习的少样本文本分类算法研宄２５


３
．
１
引言
２５


３
．２基于提示学习和三元组损失的少样本文本分类算法２５


３
．２
．１模型架构
２５


３
．２
．２基于自然语言推理的提示学习模块
２６


３
．２
．３度量优化模块
２９


３
．２
．４基于自然语言推理的继续预训练过程
３０


３
．２
．５算法流程
３
１


３
．３实验设计和分析
３２


３
．３
．
１
评测数据集介绍
３２


３
．３
．２评测指标和基线方法

３４


３
．３
．３
网络训练参数设置


３５


３
．３
．４
中文数据集实验结果
３５


３
．３
．５英文数据集实验结果
３６


３
．３
．６组件有效性分析
３８


３
．３
．７提示模版分析
４０


３
．３
．８
负采样性能分析
４２


３
．３
．９可视化分析
４２


３
．４本章小结
，
４３


第四章基于少样本学习的文本分类智能标注工具４５


４
．
１
需求分析
４５


４
．１
．
１
系统功能性需求
４５


４
．１
．２
系统非功能性需求
５０


４
．２
系统概要设计
５０


４
．２
．１
总体功能设计
５０


４
．３
系统详细设计
５
１


４
．３
．
１
人工数据标注模块设计
５１


４
．３
．２
智能标注模块设计
５２


４
．３
．３主动学习模块设计
５２


４
．３
．４数据库设计
５３


４
．４
系统实现
５４


４
．４
．１
系统实现环境

５４


４
．４
．２标注任务管理模块实现
５４


４
．４
．３数据管理模块实现
５５


４
．４
．４标签管理模块实现
５６


４
．４
．５
智能标注模块实现
５７


４
．４
．６数据统计模块实现
５９


４
．５
系统测试
６０


４
．５
．
１测试环境
６０


４
．５
．２功能性测试
６１


４
．５
．３非功能性测试
６１


４
．６本章小结
６２


第五章
总结与展望
，






６３


５
．
１
工作总结
，
６３


５
．２
工作展望
６４


参考文献
６５


第
一章绪论


第
一章绪论


１
．１研究背景及意义


随着人工智能时代的到来
，
再加上半导体硬件和ＧＰＵ并行等技术的快速发


展
，
使得以数据驱动的深度学习成为现实
，
并在诸如自然语言理解
、
图像识别
、


语音识别和机器翻译等多种不同的任务上取得了引人瞩目的成绩
，甚至在
一些任


务达到了超越人类的水平
。


文本分类
任务作为自然语言处理
（ＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ，ＮＬＰ
）领域


中最常见的任务类型之
一
，
己经获得了广泛应用并取得了良好的效果
。然而
，在


取得如此优异成绩的背后往往有着大量高质量的标注数据在支撑
，而这些数据需


要花费大量的人力和物力资源
。当数据规模变少时
，深度模型容易发生过拟合风


险
，
使模型在未见过的样本上性能变差
。此外
，
在
一些特殊的工业应用场景中
，


诸如医疗
、安全等场景
，
通常也很难获取大量带有标注的数据
，这使得较难在拥


有大规模参数量的深度学习模型上取得令人满意的效果
。相比人类对于新鲜事物


的学习
，往往仅需要通过几个简单的样本
，甚至是不需要见过
，仅凭借以往的经


验知识
，
就能够快速且准确的识别出来
。


为了使机器具有人类快速学习新事物的特性
，少样本学习
（Ｆｅｗ
－ｓｈｏｔＬｅａｒｎｉｎｇ，


ＦＳＬ
）
的概念被提了出来
。
少样本学习任务的核心目标是面对新的领域任务时
，


仅通过少量的训练样本就可以实现快速且准确的学习
。这种从少量样本中快速学


习的能力
，不仅仅要求模型具备从先验知识中快速提取并适应目标任务领域知识


的能力
，
还要具备良好的泛化性能
。


然而传统的少样本文本分类方法需要先在大量相关联的领域数据上进行学


习
，然后在具体任务数据进行调整
。
当任务发生变化时
，
需要重复上述过程
。这


将造成了极大的资源浪费
，以及存在诸如实际任务类别数量与训练不
一致等很多


局限性的问题
。近年来
，
随着预训练语言模型取得的巨大成功
，预训练模型逐渐


成为ＮＬＰ领域上新的发展趋势
。
预训练语言模型在预训练过程中
，
已经学习到


了大量的通用领域知识
。基于提示学习的方法通过调整下游任务的形式与预训练


任务形式保持
一致
，减小了上下游任务训练方式不
一致带来的差异性
，并且达到


利用预训练模型中先验知识的目的
。


本文基于上述背景进行研究
，通过对提示学习方法的探索
，使少样本学习模


型在文本分类任务上具有快速学习和泛化的能力
。并且
，通过对少样本学习问题


１


北京邮电大学工程硕士学位论文


的研究
，既可以缓减深度模型的对大规模训练数据的依赖
，
同时又能够拓宽少样


本学习的应用场景
。


１
．２国内外研究现状


少样本学习研究起初主要集中于计算机视觉领域
，
由李飞飞等
人首次提


出相关概念
，
即单样本学习
（Ｏｎｅ
－ｓｈｏｔＬｅａｒｎｉｎｇ）
问题
，
也就是在给定类别中只


有单个的示例情况下
，让模型具有利用先验知识泛化到预测新类别的能力
。随着


深度学习和自然语言处理技术的不断发展
，国内外的研宄者开始转向研宄自然语


言处理领域的少样本学习问题
。本文将从度量学习
［４
］
、
元学习
［５］和提示学习等三


个方面具体介绍国内外对少样本学习问题的研究现状
。


１
．２
．１基于度置学习的方法


２０１５年
，
Ｋｏｃｈ等人
［６
］提出孪生网络
（ＳｉａｍｅｓｅＮｅｔｗｏｒｋｓ）模型
，
由两个结构


相同且部分共享权重的网络构成
。将数据以成对的形式输入到模型完成编码工作
，


最后通过欧式方法计算输入样本对的匹配程度判断是否属于同
一类别
。
２０１６年
，


Ｖｉｎｙａｌｓ等人
［７］提出
一种匹配网络模型
（ＭａｔｃｈｉｎｇＮｅｔｗｏｒｋｓ
）
，
相比于
Ｋｏｃｈ
的孪


生网络
，
该方法使用记忆网络
（ＭｅｍｏｒｙＮｅｔｗｏｒｋ）和注意力机制
（Ａｔｅｎｔｉｏｎ）
，


实现对以往知识的记忆存储以及快速学习新样本的特征
。
２０１７年
，
Ｓｎｅｌｌ等人％


提出原型网络模型
（Ｐｒｏｔｏｔｙｐ
ｉｃａｌＮｅｔｗｏｒｋｓ）
，
将不同类别的平均向量作为类别原


型的向量表示
，最后在推理阶段通过计算样本到类别原型向量的距离实现对类别


的预测
。随后在
２０１９年
，李涛涛等人
提出了多原型学习网络（Ｍｕｌｔｉｐ
ｌｅＰｒｏｔｏｔｙｐｅ


ＬｅａｒｎｉｎｇＮｅｔｗｏｒｋ
，ＭＰＬＮ）
，
他们认为用单
一的原型表征无法充分表不类别原型
，


通过自主学习实现对
一个类别进行多维度的类别原型表示
，使网络可以具有更好


的类原型表示能力
，
从而提高模型的性能
。


２０
１８年
，
Ｓｕｎｇ等人＿提出关系网络模型
（ＲｅｌａｔｉｏｎＮｅｔｗｏｒｋ）
，
该方法没有


使用诸如欧式距离度量
、余弦相似度度量等度量方式
，而是通过
一个神经网络关


系模块
，
实现自动学习特征间的距离度量关系表示
。
２０１９年
，
Ｇｅｎｇ等人
［
１
１
］提出


归纳网络模型
（
ＩｎｄｕｃｔｉｏｎＮｅｔｗｏｒｋ）
，在关系网络的基础上
，
引入胶囊网络中的动


态路由机制实现获取支撑实例的类别向量表示
，最后通过关系模块计算查询实例


与支撑实例的关系得分进行分类
。
随后在
２０２０年
，
他们又提出动态记忆归纳网


络
【
１２］（ＤｙｎａｍｉｃＭｅｍｏｒｙ
ＩｎｄｕｃｔｉｏｎＮｅｔｗｏｒｋｓ）
。通过引入二阶段训练范式
，在第
一


阶段进行有监督的训练
，实现为第二阶段的训练提供
一个具有良好初始化编码器


２


第
一章绪论


和记忆模块
，
同时利用动态路由机制为少样本学习提供更强大的灵活性
，让模型


更好
、
更好地适应训练数据
。


１
．２
．２基于元学习的方法


元学习
［
１３Ｉ
（Ｍｅｔａ
－Ｌｅａｍｉｎｇ）通过利用以往获取的经验知识指导模型获得
一种


学习能力
，
即面对新的任务时
，
使模型可以快速适应并泛化到目标任务
。元学习


通常包含元学习器
（ＭｅｔａＬｅａｍｅｒ）和基学习器
（ＢａｓｅＬｅａｒｎｅｒ）
，
目的是让元学习


器去学习基学习器在不同任务上学到的通用知识
，从而获得
一种学习能力
，通常


称之为元知识
（ＭｅｔａＫｎｏｗｌｅｄｇｅ）
。
根据上述定义
，
元学习的方法可以理解为从


大量不同的任务中进行元知识的学习
，使模型具备
一定的学习能力
，实现在仅有


少量样本的新任务上具备更好的学习性能
。


２０
１７年
，Ｍｕｎｋｈｄａｌａ等人
［
１４
］提出元网络模型
（ＭｅｔａＮｅｔ）
，除了包含基学习器


和元学习器两部分之外
，此外还有
一个外部存储器模块
，
用于存储元知识
。元网


络模型在训练过程中
，先由基学习器在不同任务上进行学习
，并将学习到的抽象


元信息反馈给元学习器
。元学习器负责在抽象元空间内通过不同任务学习到的元


知识进行持续学习
，并引入层级增强的方法实现将生成的慢速权重和快速权重进


行集成
，
用来不断提高学习器的性能
。


Ｆｉｎｎ等人＿提出ＭＡＭＬ算法
。
该算法是
一种与模型无关的通用优化算法
，


可用于任何以梯度下降为优化目标的算法框架中
，
即ＭＡＭＬ不依赖于特定的网


络模型结构
。
具体地
，
ＭＡＭＬ
的优化目标是要获得
一个具有良好初始化的模型


参数
，使得在面对新任务时
，仅需少量的梯度更新步骤就可以使模型具有良好的


性能
。针对ＭＡＭＬ需要计算二阶微分
，会导致新増大量计算操作的问题
，
Ｎｏｃｈｏｌ


等人
［
１６
］提出来Ｒｅｐ
ｉｔｌｅ元学习方法
，
该方法通过
一阶近似微分
，
只在每个任务上


执行随机梯度下降算法
（ＳＧＤ
）
，
因此Ｒｅｐ
ｉｔｌｅ将消耗更少的计算量与内存
。


Ｊｉａｎｇ等人Ｍ将ＭＡＭＬ迁移到自然语言处理领域中
，
提出基于注意力的任


务无关的元学习方法
（ＡｔｅｎｔｉｖｅＴａｓｋ
－ＡｇｎｏｓｔｉｃＭｅｔａ
－Ｌｅａｍｉｎｇ
，
ＡＴＡＭＬ）
。该方法


通过引入注意力机制
，使模型在学习任务不可知表征的同时
，又可以快速调整注


意力参数来区分不同的任务
。


大多数基于元学习的方法中过多依赖利用词汇特征及其在训练数据上的分


布特征
，而忽略了增强模型适应新任务的能力
。未解决此问题
，
Ｈａｎ等人
Ｉ
１８
』提出


融合对抗领域适应性的元学习方法
（ＭＬＡＤＡ）
。该方法通过将元学习与对抗性领


域适应网络相结合
，实现提高模型在新任务中的自适应能力
，
以及为新类别生成


高质量的文本嵌入表示的能力
。


３


北京邮电大学工程硕士学位论文


１
．２．３基于提示学习的方法


２０２０年ＯｐｅｎＡＩ发布ＧＰＴ
－３
［
１９
］自然语言深度模型
，该模型主要聚焦于成为更


通用的模型
。为此
，
ＧＰＴ
－３在训练中引入了自然语言提示的方式
，
如给定新闻标


题
，模型学习如何根据新闻标题生成新闻内容
。通过这种方式
，实现在下游任务


中
，
只需要输入特定的自然语言提示和少量的任务实例
，
ＧＰＴ
－３通过学习就能够


给出正确的预测结果
。
ＧＰＴ
－３在下游微调取得的成功
，
展现出了基于提示学习


（ｐｒｏｍｐ
ｔ）
的方法存在着巨大的潜能
。


基于提示学习的少样本学习方法旨在下游任务中
，通过调整下游任务的形式


与预训练任务形式保持
一致
，
充分发挥预训练模型中语言模型任务本身的优势
，


减小上下游任务训练方式不
一致带来的差异
。


Ｓｃｈｉｃｋ等人
［２Ｇ
，２
１
，２２］提出Ｐａｔｅｒ
ｎ
－Ｅｘｐ
ｌｏｉｔｉｎｇＴｒａｉｎｉｎｇ（ＰＥＴ
）方法用于少样本学


习
，
通过定义并添加人工构建的模版
，将文本分类任务转换为完形填空任务
。在


训练过程中
，
ＰＥＴ方法将分类标签转换为标签描述形式
，
并使用
［ＭＡＳＫ
］进行替


换填入到人工定义的模版当中
。接着通过语言模型还原
［ＭＡＳＫ
］位置的词
，
最后


使用标签映射策略完成文本分类任务
。
Ｔａｍ等人
［２３］在ＰＥＴ基础上提出ＡＤＡＰＥＴ


模型
，将模版中需要模型预测的词从有限候选词变成整个词表
，通过扩大其搜索


空间增加模型的泛化性能
。
以及通过正确标签反向预测原文中的字符
，进
一步提


升模型的性能
。
Ｇａｏ等人
［２４
］提出
ＬＭ
－ＢＦＦ模型
。
在该方法中
，
首先通过
Ｔ５模


型
［２５
］实现自动化的生成最优模版
，避免了繁杂的人工搜索模版这
一过程
。接下来


将提示示例通过上下文的形式添加到原始输入中
，通过利用更丰富的文本信息完


成语言模型的建模工作
。
Ｌｉｕ等人＿提出
Ｐ
－ｔｕｎｉｎｇ模型
，
他们考虑到不管是人工


选择模型
，还是通过机器自动生成的模版
，都是采用的离散自然语言形式
，并且


不同的提示模版对模型的效果影响很大
。为此
，
Ｐ
－
ｔｕｎｉｎｇ丢弃掉提示模版必须是


自然语言的假说
，采用让语言模型自动学习适合当前任务形式的最佳提示模版形


式
。在训练过程中
，通过使用预训练模型词表中未使用的字符去学习模版的连续


表示形式
，并且Ｐ
－ｔｕｎｉｎｇ通过只学习更新模版对应的参数
，从而极大程度的减小


了模型需要学习的参数量
。


Ｗａｎｇ等人
提出了ＥＦＬ模型
，与将文本分类任务转换为完形填空任务形式


不同的是
，
ＥＦＬ方法是将文本分类任务转换为文本蕴含任务形式
。即通过为标签


添加细粒度的描述作为模版
，判断原始输入与模版是否为蕴含关系
。在训练过程


中
，
ＥＦＬ对于每
一个原始输入
，都会与正确的标签描述生成新的正例
，
以及与其


余候选标签随机生成若干新的负例
。通过上述数据构造方式
，实现原始输入与正


确的标签描述模版构成蕴含关系
？
，
否则为非蕴含关系
。最后在推理阶段
，选择预


测为蕴含关系概率最高的标签描述模版所对应的真实标签
，
作为最终预测结果
。


４


第
一章
绪论
不同的提示模版对模型最终性能通常会产生较大的差异性
。为此
，研究者将


集成学习的思想应用在提示学习上
，通过集成多个提示模版进
一步减小模版带来


的影响并实现提高模型的性能
。
Ｊｉａｎｇ等人Ｐ叫是出两种不同的模版集成方法
。
首


先是概率平均的集成方法
，
通过训练集选择出
Ｋ个性能最好的提示模版
，
然后


在推理阶段中使用候选Ｋ个提示模版的概率平均作为最终预测结果
。
进
一步
，


考虑到不同的模版应该具有权重
，
比如某些提示模版对模型的性能提升更有效
。


又提出通过参数优化的方式
，
让模型直接学习不同提示模版的权重比例
。


基于提示学习的方法通常是将下游任务转换为语言模型
（ＭａｓｋｅｄＬａｎｇｕａｇｅ


Ｍｏｄｅｌ
，
ＭＬＭ）任务形式
，
Ｓｕｎ等人
［２９
］提出
ＢＳＰ
－ＢＥＲＴ模型方法
，
该方法将下游


任务转换为预训练任务中ＮＳＰ（ＮｅｘｔＳｅｎｔｅｎｃｅＰｒｅｄｉｃｔｉｏｎ）
任务形式
。
ＮＳＰ任务


作为ＢＥＲＴ模型在预训练过程中的第二个任务
，其目标是判断两个句子是否为相


邻的关系
。
在ＢＳＰ
－ＢＥＲＴ方法中
，
对于分类问题来说
，
将原始输入作为第
一句
，


把含有标签描述的模版文本作为第二句输入
，通过模型判断输入的两个句子是否


为
“ 上下文
” 的关系
。


１
．３主要研究内容


当前深度学习模型的成功大多依赖大量的高质量带标签的数据
，面对数据量


较少的情况时
，深度模型很容易发生过拟合现象
。所以
，
从少量样本数据中快速


学习和拥有较好的泛化性能对人工智能是
一个巨大地挑战
。因此
，本文针对基于


提示学习的少样本文本分类任务进行研究
，其核心目标是在预训练的背景下
，
当


深度模型面对有限的有标注数据时
，可以实现快速的学习
，并拥有
一定的泛化能


力
。
具体研宄内容如下
：


（
１
）本文基于提示学习的方法
，
通过提示学习达到利用预训练语言模型在


大规模无监督语料上的获得的先验知识的目的
。在少样本学习场景中
，很容易出


现类别的数量多于单
一类别样本量的现象
，使得模型在此类任务上性能降低
。本


文将下游任务转换成基于自然语言推理的提示学习形式
，通过对任务形式的转换
，


达到在有效利用预训练语言模型中己经学习到的先验知识的基础上
，实现隐式的


数据增强
，并通过两种不同粒度的损失进行优化
。
同时
，
为了捕获下游具体任务


中潜在的类别表征信息
，通过三元组损失优化
，使模型可以学习到下游任务的类


别表征表示
。此外
，针对预训练语言模型在预训练过程大多数引入的是词级
、实


体级等语义知识
，
而较少的涉及在预训练阶段引入推理知识
。为此
，
本文尝试进


行继续预训练
，将自然语言推理知识引入到预训练语言任务中
。最终
，
在公开的


中
、
英文数据集上验证了本文提出方法的有效性
。


５


北京邮电大学工程硕士学位论文


（２
）在上述算法研究的基础上
，
本文设计并实现了
一个文本分类任务智能


标注工具
。
系统以Ｗｅｂ技术为基础
，
采用
Ｂ／Ｓ架构
，
以及前后端分离的技术方


式进行构建
。
其中
，
前端采用
Ｖｕｅ
的单页
Ｗｅｂ应用框架技术
，
后端采用基于


Ｄｊａｎｇｏ的Ｗｅｂ框架技术
，以及数据库采用
ｐｙｔｈｏｎ内置的轻量级
ＳＱＬ
ｉｔｅ数据库
。


此外
，该系统引入主动学习策略
，
即在已经训练完成的模型基础上
，模型通过为


用户推荐可能进
一步提高模型性能的无标注数据
。用户再对这些数据进行校验和


修正工作
，并添加到训练数据中
，
通过迭代式的训练
，实现模型效果的提升和标


注数据质量的提升
。


１
．４本文组织结构


本文组织结构共包括五个章节
，
内容安排如下
：


第
一章
，
绪论
。本章讨论了少样本学习问题的研究背景和意义
，接着调研了


国内外少样本学习任务的研究现状和发展历程
，并从基于度量学习的方法
、基于


元学习的方法以及基于提示学习的方法等方面阐述了目前少样本学习任务的研


究现状
。
最后介绍了本文的主要工作
、
创新点以及组织结构
。


第二章
，
相关基础知识
。本章首先介绍了少样本学习的概念
，然后对相关基


础技术进行概述
。最后重点介绍了提示学习方法
，对提示学习的定义
、提示学习


形式和常见的训练策略等进行了详细的说明
。


第三章
，基于提示学习的少样本文本分类算法研究
。本章首先介绍现阶段在


少样本文本分类任务中存在的问题
，然后引出本文提出的基于提示学习的少样本


文本分类算法
。接着对本文提出的算法以及算法的实现细节等进行了详细的说明
。


最后详细介绍了实验所使用的数据集
、实验设计和实验结果
，并对结果进行了具


体的分析
。


第四章
，基于少样本学习的文本分类智能标注工具的设计和实现
。本章介绍


如何在本文提出算法模型的基础上
，设计和实现
一个基于少样本学习的文本分类


智能标注工具
。首先对智能标注工具进行需求分析
，并针对需求
，进行了详细的


概要设计和详细设计工作
，
主要包括主要整体功能的设计、
各功能模块的设计
、


以及数据库设计
。接下来是根据详细设计内容进行各功能模块的实现
。最后对系


统进行了功能和非功能性的测试
。


第五章
，
总结与展望
。本章总结在少样本学习任务上的主要工作
，深入分析


本课题研究所遇到的问题
，并对课题研究在未来研宄方向可能改进的点进行了展


望
。


６


第
一章绪论


１．５本章小结


本章围绕少样本学习展开介绍
。首先
，介绍了少样本学习任务的研究背景及


应用场景。并且通过对该任务国内外现状的描述
，包括基于度量学习的方法、基


于元学习的方法以及基于提示学习的方法等结构的算法
，对当前的少样本学习算


法现状进行了总结
。接下来介绍了本课题的研究目标和主要研究内容
，并且对整


个文章的章节组织内容进行了详细的介绍
。


７


第二章相关基础知识


第二章相关基础知识


本章节将介绍本文工作用的相关基础知识
。首先介绍少样本学习的定义
；然


后对诸如语言模型
、
自然语言推理、度量学习相关技术和词向量化表示技术等相


关技术进行介绍
；紧接着详细介绍了预训练语言模型和提示学习方法
，
以及提示


学习方法如何利用预训练语言模型从少量标注样本中学习和泛化目标任务的核


心思想以及主要方法。


２．１少样本学习


少样本学习属于机器学习领域
一个独立的任务问题
。传统的机器学习方法面


对有限的训练数据时
，模型的性能会严重下降
。而对于少样本学习问题
，带标注


的训练数据稀少通常是其显著特征之
一
。也就是说
，少样本学习的目标是面对非


常少量的经验数据ｅ和相对应的标签数据ｒ时
，
依然可以使得模型学习到从ｓ到ｒ


的最佳映射Ｉ
从而让模型拥有
一个良好的性能ｐ。


针对文本分类任务
，少样本学习的核心目标是可以从少量的训练样本中学习


到目标对象的类别信息
。它不仅强调了训练样本的少量
，更强调了模型在新的任


务上具有良好的泛化性能。


其次
，并非训练数据集中标注数据量少的问题才能称为少样本问题
，当任务


中单个类别的平均标注数据量与总类别数的比值小的场景下也可是视为少样本


学习问题
。
例如在中文少样本数据集
ＦｅｗＣＬＵＥ
［３Ｇ
】中的
ＩＦＬＹＴＥＫ任务中
，
虽然


用于训练的标注数据量达到了９２８条
，但是却有
１
１９个类别需要识别
，平均每个


类别只有不到
８条的标注数据
，其中还包括严重数据不平衡问题
，甚至有的类别


只有几个带标注的数据
，
这将更考验少样本学习模型的性能。


２．２ｎ元语法麵


２．２．１難


语言模型（ＬａｎｇｕａｇｅＭｏｄｅｌ
，ＬＭ
）在自然语言处理中扮演着重要的角色
，尤其


是在语音识别
、机器翻译以及命名实体识别等任务上起着非常重要的作用
。假设


一段长度为ｒ的字符串序列为１＾，ｗ２
，
？
，
则语言模型计算该序列的概率为
：


９


北京邮电大学工程硕士学位论文


Ｔ


Ｐ（ｗｉ，ｗ２，
．
．
．
，ｗＴ）
＝
Ｊ
￣
｜
Ｐ（ｗｔＩＷｘ，
．
．
．
，Ｗｆ
－ｉ）（２
－１）


ｔ
－１


从公式２
－
１可以看出
，随着字符串序列不断变长时
，会导致语言模型的计算


复杂度以指数级进行增长
，
于是基于马尔科夫假设的ｎ元语法被提了出来。
马尔


科夫假设具体是指第ｎ个词的出现概率只与前面的ｎ
－
１个词相关
，而与其他任何


词都无关。所以
，
语言模型可简化为：


Ｔ


Ｐ｛ｗｌｔｗ２
，
．
．
．
，ｗＴ）？Ｐ（ｗ
ｔ｜ｗｔＨｎ
＿ｘ＞
．
．
．
，Ｗｆ
－ｉ）（２
－２）


ｔ
－１


ｗ元语法模型如公式２
－２所示
，也被称为ｎ
—
１阶马尔科夫链
。具体地
，当ｎ＝
ｌ


时
，
为
一元语法模型
（ｕｎｉｇｒａｍ）
，
即用句子中每个词出现概率的乘积来表示
一个


句子的语言模型概率得分
。在ｕｎｉｇｒａｍ语法中
，
由于词与词之间相互独立
，
导致


失去语序这
一重要的语言行为信息
。当ｎ＝２时
，为二元语法模型
（ｂｉｇｒａｍ）
，也被


称为
一阶马尔科夫链，其中每个词出现的概率只与它前面的词有关
，而与其他词


无关
。此外
，
当ｎ＝３时被称为三元语法模型
（ｔｒ
ｉｇｒａｍ）
。
当ｎ大于
１时
，
此时的


语言模型就具有了
一定语序建模能力
。
当ｎ越大时
，
模型建模的语序能力越强
，


也就是说对下
一个词的约束力越强
，但是相应的会导致计算复杂度以及词表的总


数变得很大。在实际运用中
，为了平衡计算的复杂度和模型的性能
，通常选择ｎ＝２


或ｎ＝３计算语言模型概率。


２．２．２拉普■平滑算法


在实际应用中
，通常会遇到
一些句子中存在训练语料未包含的词
，这时候会


让整个句子的语言模型概率变为０
，
但这种情况通常不符合实际情况。所以
，
需


要使用平滑处理技术解决上述问题。


在实际使用中
，通常会选择拉普拉斯平滑算法避免上述问题
，例如在二元语


法中
：


（２
－３）


其中
，
表示词
的统计频率
，
表示词Ｗｍ的统计频率
。
Ｆ


表示词表大小
。


１０


第二章相关基础知识


２．３自然语言細


自然语言推理
［３
１
，３２
］（ＮａｔｕｒａｌＬａｎｇｕａｇｅｌｎｆｅｒｅｎｃｅ
，
ＮＬＩ）是自然语言处理领域


一个非常基础性的研究工作
。也常被称为文本蕴含
（ＴｅｘｔｕａｌＥｎｔａｉｌｍｅｎｔ
，
ＴＥ）
，该


任务的主要目标是判断两个句子或者两个词之间的语义关系
。


在自然语言推理中
，
通常给定
一个前提句
（Ｐｒｅｍｉｓｅ）
，
需要判断假设句


（Ｈｙｐｏｔｈｅｓｉｓ）与前提句的语义关系
，
这种语义关系
一般定义为三类
，
分别是蕴


含、矛盾和中立。具体地
，假设根据前提句Ｐ的语义信息推理出假设句Ｈ成立
，


则说明两句话间存在着蕴含关系
。假设根据前提句Ｐ的语义信息推理出假设句Ｈ


不成立
，
则表明两句话为矛盾关系
。此外
，
如果前提句Ｐ的语义与假设句Ｈ的


语义间无法推导出任何关系
，
则说明两句话为中立关系
。


为了更好的理解两个句子间的语义关系
，自然语言推理任务通常采用句子对


分类的思想解决该问题
。
即判断给定的两句话是否存在上述的三种语义关系
。


２．４度置学习相关技术


度量学习
（Ｍｅｔｒ
ｉｃＬｅａｒ
ｎｉｎｇ）在大多数表示学习问题中扮演着非常重要的角


色
。它的目标是通过度量样本在目标空间中的距离
，使属于同
一类别的不同样本


实例之间可以相互不断靠近
，属于不同类别的样本实例间彼此相互远离
。本小节


将介绍常用的度量学习方法和在度量学习中常见的损失函数。


２．４．１度置学习方法


（
１
）欧几里得距离


欧几里得距离
（ＥｕｄｉｄｅａｎＤｉｓｔａｎｃｅ）
，简称为欧氏距离
。用来度量位于欧氏空


间中两个点间的直线距离
。在机器学习中
，通常用来衡量在高维空间中特征向量


间的直线距离。
多维空间中两个点ａ＝
（＆
＆）
，
＆＝
（＾，
．
．
．
，７？）间的欧氏距离


如公式２
￣４所示
：


ｄ＝＿
ｙ＾
２（２
＿４）


（２）曼哈顿距离


曼哈顿距离
（ＭａｎｈａｔｔａｎＤｉｓｔａｎｃｅ）表示位于欧式空间中的两个点
，在南北方


向上的距离与东西方向上的距离之和
。在机器学习中
，可以表示为高维空间两个


点在各个维度下差值的绝对值之和
，
如公式２
－５所示
：


１１


北京邮电大学工程硕士学位论文


ｄ＝
ｙ
＼ｘ
ｉ
￣
ｙＡ（２
－５）


（３
）余弦相似度


余弦相似度
（ＣｏｓｉｎｅＳｉｍｉｌａｒ
ｉｔｙ
）是通过计算向量夹角的余弦值来评估两个向


量间的相似度
。如图
２
－
１所示
，
该度量方法与当前向量的长度无关
，
只与向量的


方向有关
。
即两个向量的夹角越小
，
表明相似性越高
；
反之则相反
。


Ａ


（ｘｌ
，ｙｌ
）


图
２
－
１
余弦相似度


对于高维空间中的向量
，
其余弦相似度计算入公式
２
－６所示
：


＾ｎ２ｊ￡＝ｉ＾
ｉＶｉ
／：、


ａ＝ｃｏｓＧ＝
丁
— 一
厂
―
－（２
－６）


２
．４．２度量学习中的损失函数


（
１
）
三元组损失


三元组损失
［４８
］（Ｔｒｉｐ
ｌｅｔＬｏｓｓ）是Ｇｏｏｇ
ｌｅ团队在
ＦａｃｅＮｅｔ中提出的基于度量


学习的损失函数
，
后来被逐渐推广到自然语言处理领域中
。


Ｎｅｇａｔ
ｉｖｅ


Ｎｅｇａｔ
ｉｖｅ


Ａｎｃ
＾
ｒＰ一
＿一一一，


？又ｓ


Ｐｏｓ
ｉｔ
ｉｖｅ
Ｐｏｓｉｔ
ｉｖｅ


图
２
－２
三元组损失


三元组损失通过构造三元组数据形式
，可以更细致的刻画类别特征表示
。此


外根据具体任务需求
，通过调控阈值可以灵活的控制类别间的距离
。如图
２
－２所


示
。
具体数据构造形式如下
．
？
从训练数据中选取
一个样本作为锚点
，
也被称作


Ａｎｃｈｏｒ。然后在与Ａｎｃｈｏｒ属于同类别的候选数据中随机抽取
一个样本作为正例
，


即
Ｐｏｓｉｔｉｖｅ
。
以及在非
Ａｎｃｈｏｒ所属类别的候选数据中随机抽取
一个样本作为负


１２


第二章相关基础知识


例
，
即Ｎｅｇａｔ
ｉｖｅ。最后则构成计算三元组
（Ａｎｃｈｏｒ，
Ｐｏｓｉｔ
ｉｖｅ
，
Ｎｅｇａｔｉｖｅ）损失的


输入形式。


此外，
三元组损失中还有
一个可调节的超参数ａ
，
用来保证不同实例间具有


一个最小的间隔
。
用公式
２
－７表示：


Ｎ


Ｌ＝
＾
ｍａｘ（０
，ｄ（Ａ
ｉｆＰ
ｔ）
－
ｄ（
＞４
￡，＋ａ）（２
－７）


ｉ＝ｌ


其中
，
ｄＣ４
ｉ，Ｐ
ｉ）表示
ａｎｃｈｏｒ与
Ｐｏｓｉｔｉｖｅ实例间的距离
，
表示
ａｎｃｈｏｒ与


Ｎｅｇａｔｉｖｅ实例间的距离
。


（２）对比损失


对比损失
［６
，３３］
（ＣｏｎｔｒａｓｔｉｖｅＬｏｓｓ）主要用于处理成对输入的数据形式
，如公式


２
－８表示。


Ｎ


Ｌ＝— ＾
ＹＤ
２
＋（１
－
７）ｍａｘ
（０，ｍ
－
Ｄｗｆ（２
－８）


ｎ＝ｌ


其中
，
ｙ表示两个样本是否匹配
，
当ｙ＝ｉ表示两个样本相似
；
当ｙ＝ｏ表示两个


样本不相似
。ｍ表示设定的间隔超参数
，￣表示当前输入两个样本的欧氏距离
。


具体地
，当输入的两个样本相似时
，如果样本的间的距离变大时
，损失也会变大
。


对应的
，
当输入的两个样本无关时
，距离越远反而相应的损失越小
，如果超过预


设的间隔
，
则没有损失。


２．５词向量化表示技术


２．５．１Ｗｏｒｄ２Ｖｅｃ


Ｗｏｒｄ２Ｖｅｃ
［３４
］是Ｇｏｏｇ
ｌｅ在
２０１３年推出的
一种对词的连续向量化表示技术。


该技术以无监督学习的方式
，
实现从大规模的文本语料中学习词的向量化表示，


并被广泛的应用到基于深度学习的词嵌入表示中
。它的核心思想是对目标词与上


下文词间的关系进行建模
，
通过该方法可以得到稠密低维的词向量表示。此外
，


Ｗ〇ｒｄ２Ｖｅｃ提供了两种构建词向量表示的模型方法
，
分别是ＣＢＯＷ模型和
Ｓｋｉｐ


Ｇｒａｍ模型
。


１
）ＣＢＯＷ模型


ＣＢＯＷ（ＣｏｎｔｉｎｕｏｕｓＢ％ｏｆＷｏｒｄｓ）模型
，也被称为连续词袋模型
，该方法旨在


通过中心词的周围环境词
（上下文）共同预测出中心词的概率
。如图
２
－３所示，


１３


北京邮电大学工程硕士学位论文


ＣＢＯＷ语言模型结构简单
。具体地
，对于目标中心词
，输入它的上下文词的独热


编码表示形式
，通过与投影层的共享权重矩阵做乘法运算
，并进行求和运算获得


隐层的向量表示
。最后与输出层的权重矩阵相乘
，并经过
一个
Ｓｏｆｔｍａｘ激活函数


获得每
一个词的预测概率分布
，
其中预测概率最大的索引即为预测的中心词
。


ＩＮＰＵＴＰＲＯＪＥＣＴＩＯＮＯＵＴＰＵＴ


ｗ（ｔ
－
ｌ
）ｓ．＼


ｎ


２？ｗ〇）


ｗｃ
ｉ－
ｌ
）／


ｗ
（ｔ＋２＞
ｆ


图
２
－３ＣＢＯＷ模型结构图


在ＣＢＯＷ模型中
，
由于每个词都有可能成为带预测的中心词
，
以及中文词


的周围环境词
，
即背景词
，
所以每个词都会表示成两个ｄ维向量参与运算
。
则在


背景词确定的情况下
，
生成中心词的条件概率为
：


１


、
ｅｘｐ
（Ｅ
７＾〇０：Ｌ＋
？
＋ｗｏ２ｍ））


Ｐ（ｗｃ
＼ｗｏ１
，
．
．
．
，Ｍ／
〇２ｒｎ）
＝ｊ
（２
－９）


２
ｉｅｖｅｘｐ＋
… ＋ｕｏ２ｍ））


其中
，
设滑动窗口大小
ｍ
，
ｃ表示中心词的索弓
丨
，
则化
£
Ｍ
ｄ表示索引为ｃ的中心


词的向量
，
以及〇１
〇２ｍ为背景词的索引
，
则
ｕ２ｍ
ＥＭ
ｄ＊别表示索引为


（＾
，
．
．
．
，
＾＾的背景词的向量
。


（２）Ｓｋｉｐ
－Ｇｒａｍ模型


ＩＮＰＵＴＰＲＯＪＥＣＴＩＯＮＯＵＴＰＵＴ


‘
ｗ（ｔ
－２
）


／Ｊｉ


／ＯＨ
卜
⑷


＼
＾ｗ（
ｔ＋ｌ
）


ｗＵ＋２）


图
２
－４Ｓｋｉｐ
－
ｇｒａｍ模型结构图


１４


第二章相关基础知识


Ｓｋｉｐ
－Ｇｍｍ模型
，也被称为跳字模型
。该模型正好与ＣＢＯＷ模型相反
，
即用


中心词去预测它的环境背景词
，
模型如图
２
－４所示
。


同理在
Ｓｋｉｐ
－Ｇｒａｍ模型中
，
中心词确定的情况下
，生成背景词的条件概率同


样可以表示为
：


ｅｘｐ（心ｃ）


Ｐ（．ｗ０
＼ｗｃ）
＝
－
Ｔ（２
－
１０）


Ｓｉｅｖｅｘｐ（ｕ
／
ｉ；
ｃ）


其中
，
ｃ表示中心词的索引
，
表示索引为ｃ的中心词的向量
。
ｏ为背景词的


索引
，
ｉ＾
ｅＭ
ｄ表示索引为〇的中心词的向量
。


２
．５．２ＥＬＭｏ


在自然语言处理领域中
，
一词多义是
一个很常见的问题
，
即同
一个词在不同


的上下文语境中有着不同含义
。但是Ｗ〇ｒｄ２ＶｅＣ为上下文无关的静态词嵌入技术
，


也就是说
，
同
一个词在不同的上下文语境中的表示都
一样
。为了解决上述问题
，


上下文相关联的词嵌入技术ＥＬＭ〇
ｆ３５
ｌ被提了出来
。如图
２
－５所示
，
它是
一个基于


长短期记忆网络
（ＬＳＴＭ）的两层模型结构
，
并且每
一层包含
一个前向ＬＳＴＭ语


言模型和
一个后向
ＬＳＴＭ语言模型
。其中
，采用两个单向模型训练语言模型可以


有效的避免
“ 看到自己
” 或者是
“ 看到未来
” 等问题
，
同时达到根据上下文语境


为每
一个词进行向量化表示的目的
。


Ｔ
１Ｔ２
．
．
．Ｔｎ


Ｅ
ｌ
［Ｅ２
｜
．
．
．
｜
Ｅｎ
Ｉ


图
２
－５ＥＬＭｏ模型结构


在前向语言模型中
，对于
一个长度为Ａ／的输入序列（ｂｈ
ｔｗ）
，需要根据前


Ｋ
－
１个时刻的文本序列
，预测第Ａ：个时刻ｈ的出现概率
。最终整个序列的概率是


每个时刻的条件概率的乘积
，
如公式
２
－
１
１所示
：


Ｎ


ＰＱｔｌ
，
它２
，
…
，
ｔＮ）
＝
口
Ｐ（？
｜ｔｉ
，Ｕ
－ｉ）（２
－
１
１）


ｉ＝ｌ


同理
，
对于后向语言模型
，
如公式２
－
１２所示
：


１５


北京邮电大学工程硕士学位论文


Ｎ


户⑷山
，
…
，
ｔｗ）（２
－１２）


ｉ＝ｌ


最终
，
模型的优化目标是前向模型与后向模型的联合最大似然
：


Ｎ


＾ＬＭ
－
－
？
ｔ
ｉ
－１
；＆Ｘ＇＾ＬＳＴＭ
ｆ＾
ｓ）＋


ｉ＝ｌ


ｌ〇ｇＰ（ｔ
ｔ
｜
ｔ
ｉ＋ｉ
，
ｔ
ｊ＋２
＞＾ｘ＞＾ＬＳＴＭ
ｆ＾ｓ）（２
－１３）


其中
，
表示前向ＬＳＴＭ隐层参数
；＆ＳＴＭ表示后向
ＬＳＴＭ隐层参数
；心表示


前向与后向
ＬＳＴＭ结合后的隐层参数
；
０Ｓ表示
Ｓｏｆ
ｔｍａｘ层参数
。


ＥＬＭｏ是
一个多层双向的语言模型结构
。假设共有Ｌ层
，
ＥＬＭｏ会输出２Ｌ个


ＬＳＴＭ最后隐层向量表示
，
同时再加上词嵌入层中的向量表示
，
则在
一个Ｌ层双


向语言模型中
，对于任意
一个词的向量化表示／？ｆ
ｃ
，总共含有２Ｌ＋１个
，
如公式
２
－


１４所示
：


只ｆ
ｃ
＝” ｜）
＝１
，


＝
ｂ〇
０”
”
，Ｌ
｝
（２
－
１４）


其中
，
／ｉ找表示词嵌入层
；
／ｉｇ
＝表示每
一层ＬＳＴＭ的隐层向量表示
。


有两种常用的方法可以将
ＥＬＭｏ
中学习的词向量表示应用到下游任务
。
第


一种是使用顶层中的隐层输出作为词向量表示
，
即Ｅ＠ｆ
ｃ：
）
＝，
这是最简单的


一种应用方法
。第二种是通过加权的方式计算各层中向量的表示
，获得最后的词


向量表示
：


Ｌ


Ｅ｛Ｒｋ
）Ｑ
ｔａｓｋ
）
＝
ｙ
ｔａｓｋ
＾
ｓｆ
ａｓｋ
ｈ＾
Ｍ
ｊ（２
－１５）


７
＝〇


其中
，
５
／
ａｓｆ
ｃ表示
ｓｏｆ
ｔｍａｘ后的规范化权重因子
，
ｙ
ｔａｓｆ
ｃ表示缩放因子
，可以根


据下游任务情况进行调整
。


２
．６预训练语言模型


基于预训练和微调范式的预训练语言模型对ＮＬＰ的发展产生了巨大的影响
，


无需使用带有标注的数据
，可以直接从海量的无标注数据中学习到通用的知识表


示
。随着预训练语言模型的不断发展
，根据预训练语言模型目标的不同
，进
一步


可以分为自回归语言模型
（ＡｕｔｏＲｅｇｒｅｓｓｉｖｅＬＭ
）和自编码语言模型
（ＡｕｔｏＥｎｃｏｄｅｒ


ＬＭ
）
〇


１６


第二章
相关基础知识


自回归语言模型是指根据上文中已经出现的文本信息预测下
一个词的出现


概率
；
或者反过来
，
根据下文内容预测前面
一个词的出现概率
。
〇？１＾，３７
，
１９］系列


就是典型的自回归语言模型
。通常用于自然语言生成任务
，
例如
：机器翻译、基


于生成式的文本摘要等
。


自编码语言模型则是同时利用上下文的信息预测中心词出现的概率
，实现对


上下文信息的充分利用
。ＢＥＲＴ
［３８
］就是经典的自编码语言模型
，通过［ＭＡＳＫ
］随机


替换输入中的部分词
，
然后根据上下文信息预测
［ＭＡＳＫ
］位置处的词
。
由于自编


码语言模型可以同时利用上下文信息
，
通过在自然语言理解任务表现优异
。


２
．６．１ＧＰＴ


ＧＰＴ
，
全称ＧｅｎｅｒａｌＰｒｅ
－Ｔｒａｉｎｉｎｇ
，
ＯｐｅｎＡＩ最早于
２０
１８年发布了第
一代大规


模通用生成式的预训练语言模型
，
如图
２
－６所示
。
ＧＰＴ采用两阶段的训练的方


式
，在第
一阶段为预训练阶段
，通过大规模的无标注语料训练
一个通用的语言模


型
；第二阶段为微调阶段
，在特定的下游任务上通过微调
，实现对模型参数的调


整来适应下游任务
。


丁
１丁
２
？
？
？丁
忖


｛ＴｒｍＨＴｒｍ）
…（
ＴｒｍＪ


１
＾
２
．
．
．Ｅ
ｎ


图
２
－６ＧＰＴ预训练模型
丨３８
】


ＧＰＴ在预训练过程中
，采用基于编码器
－解码器结构的Ｔｒａｎｓｆｏｒｍｅｒ＾网络代


替了传统的ＬＳＭＴ网络
。
Ｔｒａｎｓｆｏｒｍｅｒ是由Ｖａｓｗａｎｉ等人在
２０１７年提出的
，
它具


备更强的长距离语言特征捕获能力和并行性的优点
。
同时
，
ＧＰＴ在解码器


（Ｄｅｃｏｄｅｒ）端采用了ＭａｓｋＭｕｌｔｉ
－ＨｅａｄＡｔｅｎｔｉｏｎ的方式来避免预测当前词时会看


见后面的词
，
即是
一种从左到右
（Ｌｅｆ
ｔ
－ｔｏ
－Ｒｉｇｈｔ）的单向语言模型结构
，通常称之


为单向Ｔｒａｎｓｆｏｒｍｅｒ结构
。


在ＧＰＴ的微调阶段中
，
对于具体的下游任务
，
需要采用预训练相同的网络


结构
，
其区别仅在与不同任务的数据处理方式不同
。


１７


北京邮电大学工程硕士学位论文


Ｐ
ｒｅＳＳｏｎｃＳｅｆＣ
ｌａｓｓ
ｉｆ
ｉｃａＵｏｎ
｜Ｓｔａｎ
｜ＴｅｘｔＪ＾ｘｔｒａａ
［ＨＴ＾ｓｆ〇ｒｍｅｆ
＂
ＨＬｉｎｅａｒ

ｊ





＂
＂＂＂＂＂＂Ｔ

／

［
ｐ
Ｌａｙｐ
．
ｆ＾？ｎ．
Ｅｎｔａ
ｉ
ｉｍｅｎｔｊ
ｓ？ｔ
ｆｉＰｒｅｍｉｓｅ
ｊＤｅｆ
ａｎｊＨｙｐｏｔｈｅｓｉｓ
［ＥｘｕａａＴｒａｎｓｆｏｒｍｅｒｈ
－
［
Ｌ
ｉｎｅａｒ
）


４—
ｊ
．
…
．
ｓ




ＰｅｅｄＦｏｒｗａｒｄ
｜
Ｓｔａｒ
ｔ
ｊＴｅｘｔ１
｜Ｄｅｌｗｎ
］Ｔｅｘ！２！Ｅｘｔｒａｃｔ
ｌ
｜
－
＾Ｔｒａｎｓｆ
ｏｒｍｅｒｐｊ



１
Ｓ
ｉｍ
ｉ
ｌａｒｉｔｙ
ｄ
．
｛＾
－
－＊
ｊ
ｔｊｎｅａｆ
｜



ｊＳｔａｎ｜Ｔｅｘｔ２
｜
Ｄｅ
ｉ
ｉｍ
ｊＴｅｘｔ１ｊ
Ｅｘｔｒａ＾
￣
ｊＴｒａｎｓｆ
ｏｒｍｅｒ卜丁


Ｌａｙｅｒ
Ｎｏｏｎ


Ｍａｓｋ＾
？Ｊ
￣
ｌ
｜Ｓｔａｉ
￣
［Ｃｏｎｔｅｘｔ｜Ｄｅ
ｉ
ｉｍ
｜Ａｎｓｗｅｒ１［Ｅｘｔｒａｃｔ

［
［
－？ｐＴｒａｎｓｆ
ｏｒｍｅｒ［
＊
｜Ｌｉｎｅａｒ
｝
－
］


Ｚ
Ｉ

．



—
」Ｍｕ
ｌｔ
ｉｐ
ｌｅＣｈｏ
ｉｃｅ
｜
Ｓｔａｎ
［Ｃｏｎｔｅｘｔ
｜Ｄｅ＞
ｔｍ
［Ａｎｓｗｅｒ２｜
ＥｘｔｒａｃＴ
ｊＴｒａｎｓｆｏ
ｒｍｅｆ

ｊ
＊
｛
Ｌｉｎｅａｒ
Ｉ


Ｔｅｘｔ
＆Ｐｏｓ
ｉｔｉｏｎＥｍｂｅｄ
｜
ｓｔａｒ
ｔ
［Ｃｏｎｔｅｘｔ
［Ｄｅｉｔｍ
ｊＡｎｓｗｅｒＮ
［Ｅｘｔｒａ〇
￣
｜
＞
ｊＴｒａｎｓｆｏｍｗ
［
—
ｊＵｎｅａＴ


图
２
－７
下游任务数据改造形式
丨３６
】


如图
２
－７所示
，对于不同类型的下游任务
，仅需要对输入给模型的数据进行


适当的改造
，就可以利用预训练阶段产出的初始化好的模型参数
，实现快速的在


下游任务中微调
，大大提高了下游任务的训练效率
。对于文本分类任务
，
只需要


在输入文本的起始位置和结束位置分别添
标识符
，最后通过


添加分类层就可以实现文本分类任务
。对于文本蕴含任务
，另外需要在前提句和


假设句间添加句子分隔符＜Ｄｅｌｉｍ＞
。对于文本相似度任务
，
由于ＧＰＴ是单向语言


模型
，为了提高模型的抗干扰性
，对于两个句子
ｔｅｘｔｌ和
ｔｅｘｔ２
，需要分别以
“ ＜Ｓｔａｒｔ＞


ｔｅｘｔｌ
＜Ｄｅｌｉｍ＞
ｔｅｘｔ２〈Ｅｘｔｒａｃｔ〉
”
和
“ ＜Ｓｔａｒｔ＞
ｔｅｘｔ２＜Ｄｅｌｉｍ＞
ｔｅｘｔｌ
〈Ｅｘｔｒａｃｔ〉
”
的形式


输入到模型中
。对于多项选择任务
，需要将内容与不同的选项组合分别输入到模


型中
，
完成任务的建模
。


ＧＰＴ模型釆用的单向的Ｔｒａｎｓｆｏｒｍｅｒ结构
，可以使它很好的完成生成式任务
。


但由于只能利用上文中的信息
，而无法有效的利用下文中的信息
，导致在大多数


需要上下文语境信息的自然语言理解任务上表现较为
一般
。


２．６
．２ＢＥＲＴ


２０１８年谷歌推出了ＢＥＲＴ预训练语言模型
。
该模型
一经发布
，
就刷新了包


括文本推理
、问答理解和命名实体识别等多项自然语言任务的记录
，开启ＮＬＰ领


域新的篇章
。


如图
２
－８所示
，
ＢＥＲＴ同样采用
Ｔｒａｎｓｆｏｒｍｅｒ网络结构进行的特征提取表示
，


并且采用了多层堆叠的编码器结构
，
使其具备捕获不同维度信息的能力
。其中
，


编码器的核心是自注意力机制
（Ｓｅｌｆ
－Ａｔｅｎｔｉｏｎ）
，使模型具备既关注自身的信息
，


还可以关注到文本中其他词信息的能力
，
公式如下
：


Ａｔｔｅｎｔｉｏｎ（Ｑ
，Ｋ
，Ｖ）＝ｓｏｆｔｍａｘ
＾
￣＝＾
ｊ
Ｖ（２
－
１６）


１８


第二章
相关基础知识


其中
，
是查询向量
，
是键向量
，
是值向量
，
Ｌ是


序列的长度
，
ｄ是向量维度
。


丁
，丁
２
…Ｊ
Ｎ


（
Ｔｒｍｊ
ｆＴｍｉ）
…（
ＴｒｍＪ


ＣＴｒｍＴｒｍｊ
？
＂ＣＴｒｍｊ


ｅＴ］
１＾
２
１…
１
Ｅ
ｎ


图
２
－８ＢＥＲＴ模型结构
Ｉ３８
】


另外
，
如图
２
－９所示
，为了捕获更丰富的特征
，
Ｔｒａｎｓｆ
ｏｒｍｅｒ模型采用了多头


注意力机制
。将Ｑ
、
Ｋ和Ｖ向量进行拆分成Ｎ份
，如在ＢＥＲＴ中向量维度为
７６８


维
，通过将其拆分成
１２个头
，
则每个头为
６４维的向量
，完成自注意力计算后再


拼接在
一起保持特征维度不变
。通过这种方式
，实现了在不增加时间复杂度的条


件下
，
形成了多个独立的特征子空间
，
可以学习到更丰富的特征信息
。


ＭｕｌｉｔＨｅａｄ｛Ｑ
，Ｋ
，Ｖ）
＝Ｃｏｎｃａｔｉｈ＾
，
．
．
．
．ｈ＾（２
－
１７）


其中
，
ｎ表不划分的子空间数
，
Ｃｏｎｃａｔ表示向量的拼接操作
。


ｔ


Ｌｉｎｅａｒ


｜Ｃｏｎｃａｔ
］


■
Ｉ
，


Ｓｃａ
ｌｅｄＤｏｔ
－Ｐｒｏｄｕｃｔ么


Ａｔｔｅｎｔ
ｉｏｎ、广
’


ｊ
：｜ｊ：＝，＿Ｕ
：
Ｊ


ｊｕｎｅａｒ
｜ｉ
｜ｕｎｅａｒ
ｊＪ
［
＂
ｕｎｅａｒ
］ｉ


■
ｒｒ


ＶＫ〇


图
２
－９
多头注意力机制
ｌ３９
】


如图
２
－
１０所示
，
ＢＥＲＴ同样采用了预训练
－微调的两阶段训练范式
。
第
一阶


段
，是在大规模的无标注文本语料上以语言模型为主的预训练过程
。第二阶段是


在第
一阶段的基础上
，
以微调的形式在具体的下游任务上进行有监督的训练
。其


１９


北京邮电大学工程硕士学位论文


次
，在第
一阶段预训练过程中
，包含了两个任务
：基于掩码的语言模型
（Ｍａｓｋｅｄ


ＬａｎｇｕａｇｅＭｏｄｅｌ
，ＭＬＭ
）任务和下
一句预测（ＮｅｘｔＳｅｎｔｅｎｃｅＰｒｅｄｉｃｔｉｏｎ
，ＮＳＰ）任务
。


ＭＬＭ任务的本质通过掩盖住
一个词后
，利用它的上下文信息来恢复当前词
，


使其具备良好的语言能力
。
具体做法是通过以
１５％的机率随机掩盖住句子中的


一部分词
，
其中
８０％的几率会被掩盖掉
，
即通过
［ＭＡＳＫ
］替换
；
１０％的机率会通


过词典中的词进行随机替换
；
〗〇％的机率保持原来的词不变
。


其次
，
为了更好的理解自然语言中句子对关系
，
ＢＥＲＴ
引入了ＮＳＰ任务
。


ＮＳＰ任务是建模给定的两句话是否为上下句关系
。也就说
，
当输入的两句话为上


下句关系
，
则判定为
“
Ｔｒｕｅ
”
，
否则判定为
“
Ｆａｌｓｅ
”
。在处理训练数据时
，
会分两


种情况进行构造句子对
，
５０％的情况下
，
保持原始的句子对关系不变
；
５０％的情


况下
，
会随机从语料库中抽取
一句话替换原始句子对中的第二句
。


ｒ
ａｍＱＱ３３Ｄ
－
－ｇｄｍｍ
￣
ｉ％


？＊？


ＢＥＲＴ
？严
妒ＢＥＲＴ


Ｉｉ
ｉ
—
＇
Ｉ—ｎｉ
ｉ＾
ｉｆ
￣
ｉ
ｔｉ—ｉｉ


￣￣〇

〇
￣０
￣￣０
￣￣〇
－
■
■
〇


ＱＥＤ
琴因Ｃｐ审
Ｃｐ


ＭｅｓｋｓｓＪ
Ｓｅｎｓｅｎｃｅ
ＡＭａｓｋｓｃＳｅｎｔｅ
？＞ｃｓ
＊
８
Ｃｆ
ｃｅｓｔ
ｉｏｎＰａｒａｍ３ｐｂ


、含ｉ＼＼
、
＇含／


＼、
Ｕｎｗａｅ＾Ｓｅｎｆ
ｅｎａｓＡａｎｄＳＰａｉ
ｒ／
、
＼、＼、
＇
、
、
Ｏ＾ｏｎ
Ａｎｓｗｅｒ一／


Ｐｒｅ
－
ｔｒａｉｎ
ｉｎｇ
Ｆ
ｉｎｅ
－Ｔｕｎ
ｉｎｇ


图２
－
１０ＢＥＲＴＰｒｅ
－
ｔｒａｉｎ和ｆ
ｉｎｅ
－
ｔｕｎｉｎｇ
１３８
】


在微调阶段中
，
ＢＥＲＴ模型与ＧＰＴ模型的处理过程十分相似
，都不需要改变


ＢＥＲＴ模型结构本身
，
仅需要根据下游任务形式改变数据输入格式即可
。


／ｖ．／ｓ
．？ｖ
．／ｖ／
－ｖ
．／
■ｖｙｖ？ｖ
／ｙｖ／ｓ
．


，ｎＰｕｔ
［ｃｌｓ
］ｍｙｄｏｇ
ｉｓｃｕｔｅ［ｓｅｐ
］ｈｅ
ｌ
ｉｋｅｓ
ｊ
ｐ
ｌａｙ＃＃
ｉｎｇ
［ｓｅｐ
］


Ｅｍｂｅｄｄ
ｉｎｇｓ
＾
ｉｃｌｓ
］
已
ｍｙ
Ｅ
ｄ〇ｇ
Ｅ
ｉｓ＾
ｃｕｔｅ＾
［ｓｅｐｊ
Ｅ
ｈｌ£ 
Ｅ
ｌ
ｉｋｅｓ
Ｅ
ｐ
ｉａｙ
＾
＊
＞
ｉｒ，ｇ
＾
（ｓｅｐ
ｊ


＋＋＋＋Ｈｈ＋＋＋＋＋＋


Ｓｅｇｍｅｎｔ
ｊ
—ｒ
—ｐｐｐ
＞
—
ｉ
￣ｒ
￣「「


Ｅｍｂｅｄｄ
ｉｎｇｓ
匕
ａ
丨
仁
ａ
丨
亡
ｂ
丨
亡
ｂ


＋＋＋＋＋＋＋＋＋＋＋


Ｐｏｓ
ｉｔ
ｉｏｎ
ｒ－ｒ
—ｐｐ
ｒｚｐｐ
ｊ
ｐ
ｚｐｚｔｚ「


Ｅｍｂｅｄｄ
ｉｎｇｓ
ｊ
匚
ｉ
丨
仁
２
丨
仁
３
ｊ
匚
４Ｃ
５｜
亡
６
丨
仁
７
丨
仁
８
｜
仁
９
Ｃ
１０


图
２
－
１
１ＢＥＲＴ输入表征示意图
［３８
］


在模型的输入方面
，ＢＥＲＴ在使用基于ＷｏｒｄＰｉｅｃｅ
［４Ｇ
】方法的ＴｏｋｅｎＥｍｂｅｄｄｉｎｇ


作为词向量表征的基础上
，
同时加入了位置向量表征
（ＰｏｓｉｔｉｏｎＥｍｂｅｄｄｉｎｇ）和句


子切分向量
（ＳｅｇｍｅｎｔＥｍｂｅｄｄｉｎｇ）
，
如图
２
－
１
１所不
。


２０


第二章
相关基础知识


基于掩码形式的预训练语言模型
，通过对上下文信息的双向编码
，更适合处


理自然语言理解类问题
。但同时也由于该编码方式
，导致预训练过程中的数据形


式与微调阶段存在不匹配的问题
，
使得难以处理生成类任务
。


研究者们在
ＢＥＲＴ的基础上
，
进行了进
一步的探索研究工作
，
先后出现了


Ｒｏｂｅｒｔａ
［４
１
］
、
ＥＲＢＩＥ
［４２
］以及ＭａｃＢＥＲＴ
［４３＿不同的掩码形式的预训练语言模型
，极


大地提升了预训练模型性能
。


２．７提示学习


２０２０年ＯｐｅｎＡＩ提出的ＧＰＴ
－３大规模通用预训练语言模型
，
达到了惊人的


１７５０亿个参数量
，
实现了不经过微调的情况下
，
就己经在多个ＮＬＰ任务上达到


了最先进的水平
。
ＧＰＴ
－３的训练中引入了自然语言提示学习
（ｐｒ
ｏｍｐ
ｔ）方式
，
如


任务描述和触发词等
。这样在下游任务中
，
只需要输入简单的自然语言提示和少


量的任务实例
，
ＧＰＴ
－３就可以给出正确的预测结果
。预训练模型与提示学习的结


合
，
展示出了提示学习
［４４
］方法在通用任务的存在巨大潜能
，
成为了ＮＬＰ史上又


一重要里程碑
。


随着自然语言提示在ＧＰＴ
－３中的成功
，基于预训练
、提示学习和预测的方法


成为
一种新的ＮＬＰ范式
。
在这种范式下
，
不再需要通过微调使预训练语言模型


适应下游任务
，而是通过添加提示模版
，调整下游任务形式与预训练模型任务
一


致
，从而缓解了下游任务与预训练任务间存在巨大的鸿沟
。通过选择适合的提示


模版
，利用预训练模型己经学习到的先验知识
，实现让模型输出正确的预测结果
。


从而在仅依靠少量的训练数据前提下
，
就可以取得更好模型效果
。


２．７．１概述


对于传统的基于深度学习的监督方法
，
在给定的输入ｘ的情况下
，
需要通过


大量的有标注数据训练模型对Ｐ（ｙ
｜ｘ；０）进行建模
。但是对于大多数的任务
，通常


难以找到大量的有标注数据
。
基于提示学习的方法
，
尝试通过建模输入文本ｘ自


身的语言模型概率
，从而实现避免对大规模训练数据集的依赖
。具体地
，在提示


学习中
，
原始输入ｘ会通过提示模版修改成
一个含有待填充槽位的新输入Ｖ
，
随


后通过语言模型去建模Ｐ
（ｘ
＇
；０）
，
最后通过解析并获取提示模版中槽位的最高预


测概率
，
实现对最后预测标签ｙ的输出
。


基于提示学习方法的基本形式化表示如下
：


对于输入的文本；ｃ
，有函数
可以将输入ｘ转换为基于提示学习的形


式Ｙ
，
即
：


２
１


北京邮电大学工程硕士学位论文


尤
’
＝／ｐｒｏｍｐｔ
００
（２
－１８）


对于新闻分类任务
，
假设给定的模版为
：
“
［Ｘ
］
，
这是
｜Ｚ
］新闻
。
”
。其中
，
对于


原始输入Ｘ
，需要将其填充到
［Ｘ
］
；
以及标签的描述性信息Ｚ
，
同时需要将其填充到


［Ｚ
］
。例如对于原始输入文本Ｘ＝
“ 因痛失比赛而落泪的职业选手
，让人感到心疼
。
”
，


通过函数／ｐｒ
ａ７ｎｐｔ〇〇函数映射
，可以转换为基于提示学习方法的形式
，
即
：
ｘ
＇＝
“ 因


痛失比赛而落泪的职业选手
，
让人感到心疼
，
这是阂新闻
。
”


基于提示学习的方法
，将下游任务转化为建模语言模型的问题
，实现将预训


练阶段与下游微调阶段的统
一
。


２
．７．２提示学习形式


在基于提示学习的方法中
，
其核心是设计适合任务的映射函数／ｐｉｗｎｐｔ（ｘ）
，


使下游任务获得更好的性能
。提示学习的形式主要分为两种类型
：在文本字符串


中填充空白的完型填空形式
（ｃｌｏｚｅｐ
ｒｏｍｐ
ｔ）
，
以及前缀式形式
（ｐｒｅｆｉｘｐｒｏｍｐｔ）
。


提示学习形式的选择通常取决于任务类型和预训练模型本身
。通常来说
，对于自


然语言生成任务或者使用自回归预训练模型
，
如ＧＰＴ系列
，
通常选择前缀式提


示学习形式
。而对于自然语言理解任务或者使用基于掩码
（ＭＡＳＫ）的自编码预


训练模型
，如ＢＥＲＴ
、
Ｒｏｂｅｒ
ｔａ等
，
通常选择完型填空形式的提示学习形式
。
因为


基于掩码的自编码预训练任务本质上就是完型填空形式
，通过被掩盖住的词的上


下文去预测当前词
，
这样就与预训练语言模型任务形式相
一致
。


对于提示学习形式的构建方式
，通常包含手工构建提示模版方式以及自动化


构建提示模版方式
。


（
１
）手工自定义提示模版


手工自定义提示模版通常是最简单和最常用的方式
。具体地
，对于自然语言


理解任务
，通常选择掩码式的预训练语言模型
。
同时根据具体的下游任务
，
手工


设计完型填空形式的提示模版
，通过利用预训练模型己经学习到的先验知识
，实


现解决目标下游任务
。如图
２
－
１２所示
，
对于新闻分类任务
，
输入文本
：
“ 因痛失


比赛而落泪的职业选手
，
让人感到心疼
。
”
，
它的分类标签为
“ 体育
”
。
通过手工


构造提示模版
：
“ 这是
［ＭＡＳＫ
］新闻
”
，
实现将文本分类任务转化为ＭＬＭ任务
。


即构造成新的输入形式
，
“ 因痛失比赛而落泪的职业选手
，
让人感到心疼
，
这是


［ＭＡＳＫ
］新闻
。
”
。


基于手工构造的方式
，可以快速构造出大量的手工自定义提示模版
。但不同


的模版对模型的性能有较大的区别
，需要反复进行试验选择更适合下游任务的提


示模版
。


２２


第二章
相关基础知识


「因
痛
｜失
】
＾
｜
赛２而
「落＂沼
｜
的＾职
业
｜
选
Ｈ手
… …
｜岡
｜＾】
｜― ＾闻
：「
｜


预训练语言模型
（ＢＥＲＴ
）


ｍＷ
Ｉ


图
２
－
１２手工模版构建


（２）
自动化构建提示模版


虽然手工创建提示模版较为简单和直观
，但是由于提示模版间性能差异较大
，


需要花费大量的时间和精力去探索适合目标任务的提示模版形式
，造成不必要的


资源浪费
。
为了解决上述问题
，
很多研究者尝试自动化构建提示模版
。


自动化构建提示模版可以进
一步分为离散式提示模版和连续式提示模版
。其


中
，
离散式提示模版通常使用目标预训练语言模型或者其他生成式的语言模型
，


实现生成若干个合适目标任务的自然语言形式的提示模版
，也就是说
，模版的依


然是离散的字符串形式
。


罕平
［
＾
］
［
＾
］
［
＾
］
［
＾
］亨
［
＾
］
［
＾
］竿
丨业
选
手
Ｉ
丨
…
…
１
—
３
亨早
１
＾
１亨甲


预训练语言模型
（ＢＥＲＴ
）


去ｉ


图
２
－
１３连续式自动化模版构建


对于连续式提示模版
，研宄人员己经不再关心提示模版的具体形态
，此时提


示模版可以是非自然语言的表达形式
。在该方法中
，研究人员希望模型可以自己


学习到适合当前任务形式的连续式提示模版形式
，
如图
２
－
１３所示
，
输入非自然


语言形式［Ｕ
１
］
？
［Ｕ４
］
，
通过模型学习生成连续式提示模版形式
，
实现预测被遮盖


住的字符
，
完成对当前的输入实例的标签预测
。


２
．７．３训练策略


将提示学习应用于具体下游任务的训练过程中时
，模型结构中通常包含如下


两种参数形式
：预训练模型参数和提示模版参数
。在少样本学习场景下
，
当对部


分或者全部模型参数更新时
，将会对整个模型的性能产生重大的影响
，所以产生


了不同的训练策略
。
具体训练策略如表
２
－
１所示
。


２３


北京邮电大学工程硕士学位论文


表
２
－
］
训练策略


策略ＬＭ参数提示模版参数典型方法


策略
一固定固定ＧＰＴ
－３


策略二固定更新Ｐｒｏｍｐ
ｔ
－Ｔｕｎｉｎｇ


策略三更新固定ＬＭ
－ＢＦＦ


策略四更新更新Ｐ
－Ｔｕｎｉｎｇ


使用训练策略
一的方法
，此时不需要对语言模型和提示模版参数进行训练更


新
，
就可以直接通过模型预测结果
，通常适用于零样本学习问题
。典型的方法包


括ＧＰＴ
－３和
ＬＡＭＡＰ
Ｉ等
。
在策略二方法的使用场景中
，
通过固定预训练模型参


数部分
，
仅对提示模版参数进行更新
。在少量训练实例的情况下
，通过仅更新部


分模型参数
，
实现灵活的适应下游任务
。
典型的方法包括
Ｐｒｅｆ
ｉｘ
－Ｔｉｍｉｎｇ
ｌＷ和


Ｐｒｏｍｐｔ
－ＴｕｎｉｎｇＷ
。在训练策略三中
，通过固定提示模版
，仅更新预训练语言模型


参数部分
，
实现避免设计复杂的提示模版
。在该方法中
，通常使用简单的离散形


式自然语言提示模版
。典型的方法包括ＬＭ
－ＢＦＦ和
ＰＥＴ
。对于训练策略四
，通常


适用于数据量较大的场景中
。通过对预训练语言模型和提示模版参数同时进行更


新
，
可以使模型获得更好的性能
。
典型的方法有Ｐ
－Ｔｉｍｉｎｇ
。


２
．８本章小结


本章主要介绍了基于提示学习的少样本任务中涉及到的各种相关理论基础


知识
，首先介绍了少样本学习的定义以及其他基础知识
，然后详细介绍了预训练


语言模型
，
最后重点介绍了提示学习的原理
、
常用形式以及训练策略等
。


２４


第三章基于提示学习的少样本文本分类算法研究
第三章基于提示学习的少样本文本分类算法研究


３
．１引言


对于少样本文本分类任务
，在预训练模型的基础上采用直接对下游任务进行


微调的方法
，往往会因为样本数据量少使得模型发生过拟合的风险
，导致模型效


果不如预期
。此外
，对于以往的少样本学习方法
，会出现对于不同的下游任务或


是包含不同类别数量的任务
，都需要在相关领域数据上重新进行训练
，造成不必


要的资源浪费
。


目前深度预训练语言模型技术已经取得了巨大的成功
，预训练语言模型在预


训练任务阶段己经融入了各种形式的通用领域知识
，诸如Ｒｏｂｅｒｔａ融入实体知识
、


ＥＲＮＩＥ融入三元组知识等
。通过融合多种不同粒度的先验知识
，使得预训练语言


模型已经学习到了丰富的先验知识表不
。在此背景下
，基于提示学习的方法提供


了
一种新颖有效的解决思路
。基于提示学习的方法通过调整下游任务的形式与预


训练任务形式保持
一致
，实现对预训练语言模型中的先验知识的激活
，从而在下


游任务中表现出良好的性能
。


另
一方面
，在少样本学习场景中
，很容易出现类别数量远多于单
一类别样本


量的现象
。针对此问题
，本文基于提示学习方法将下游任务转换成基于自然语言


推理的提示学习形式
，通过对任务形式的转换
，达到在有效利用预训练语言模型


中己经学习到的先验知识的基础上
，实现对数据的隐式增强
。
同时
，
针对以往基


于提示学习的少样本文本分类算法中
，仅利用了预训练模型中的通用知识
，没有


考虑到具体下游任务中类别等信息
。本文通过三元组损失进行联合优化
，让模型


可以隐式地学习到具体下游任务中的类别表征表示
。此外
，本文将基于自然语言


推理的提示学习引入到预训练任务中
，
实现进
一步提升模型的性能
。


３．２基于提示学习和三元组损失的少样本文本分类算法


３．２
．１模型架构


在基于提示学习的少样本文本分类算法中
，面对少量训练样本时
，通常将下


游任务转化成预训练任务中基于掩码的语言模型任务形式
。通过任务形式的转化
，


实现既可以挖掘预训练过程中己经学习到的先验知识
，又能够减小模型需要优化


的参数量规模
。


２５


北京邮电大学工程硕士学位论文


Ｉ
￣
—


Ｐ〇５ｉｔｉｖｔ


ｒ…
一
ｖ
ｉ
ｉ
？
？
；



、
」
：
；
－
，

：
；
｜


ｖ？
＂
；
—
■
■
■
■－
？
■
ｆ
ｔｓｐ
ｉｅｔ
Ｍａｒｇｉｅ
Ｌｏｓｓ

— —
Ａｎｃｈｏｒ


Ｓａｉｕｐ
ｌｉｎｇ
ｊ＿
－
ｌＯ？
－
Ｇ


ｌＮｅｇａｔｉｖｅ度蛋优化镆块
；
ｊ
＿
＿
ｓ
Ｆｕａｎｉ
、ｎ


Ｔｒａ
ｉｎ
ｉｎｇＤａ
ｔａｓｅｔ４＾
＇
＾
Ｅｎｃｏｄｅｒ




”ｔ


ｆｆ
？刻
｜
ｊ

１
，
！
Ｉ


ｊ

；？
ＳｅｏＭｎｃｅ
Ｌｎ
＇
ｅｌ
Ｌ〇ｉ￥
：
！


ｒ
£
？
＿；
＊
〇ＭＬＭＬｅａｄ



－
，
．
．
．
．
．


；
／
ｖ
：
：
．
ｘ
！
Ｓ＾ｎＴｅｏｃ＊
－Ｇｒｏｕｐ


Ｈ
－Ｌ＾ａＭＳ


１１Ｔｒｍｐ
ｌａ
ｌｒ
？
；
ｊ


ｉ


图
３
－
１
模型整体架构图


本文提出了
一种基于提示学习和三元组损失的少样本文本分类算法
，其模型


的整体架构如图
３
－
１所示
。该模型的网络结构由基于自然语言推理的提示学习模


块和度量优化模块两个部分组成
，并且两个模块共享编码层
（Ｅｎｃｏｄｅｒ）参数
。其


中
，
基于自然语言推理的提示学习模块通过ＭＬＭＨｅａｄＬａｙｅｉ
？计算输入句子中的


［ＭＡＳＫ
］位置处的概率
，
并通过
ＳｅｎｔｅｎｃｅＬｅｖｅｌ和
Ｓｅｎｔｅｎｃｅ
－ＧｒｏｕｐＬｅｖｅｌ两种不同


粒度的损失方法进行模型优化
。度量优化模块随机对训练集样本进行抽样
，通过


共享编码层编码后
，
使用Ｔｒ
ｉｐ
ｌｅｔＬｏｓｓ计算销点
（Ａｎｃｈｏｒ）与正例
（Ｐｏｓｉｔｉｖｅ）
和


负例
（Ｎｅｇａｔｉｖｅ）
间的损失
。
最后
，
通过对两个模块进行联合优化学习
，
并利用


ＡｄａｍＷ优化器进行反向传播实现对模型的参数更新
。


与以往基于提示学习的少样本文本分类算法相比
，本文提出的算法
，既能够


对原始的数据进行数据增强
，又实现在不引入额外参数量的基础上有效利用预训


练模型中的先验知识
，同时通过三元组损失实现让模型隐式地学习到具体下游任


务的类别表征表示
，
达到少样本学习的目的
。


３．２
．２基于自然语言推理的提示学习模块


如图
３
－２所示
，基于自然语言推理的提示学习模块
，是整个算法的核心模块
。


该模块负责将文本分类任务转换为基于自然语言推理形式的完型填空任务
。具体


地
，对于原始输入文本
，将真实标签通过模版映射转化为自然语言推理形式
。其


中推理词使用预训练语言模型中
［ＭＡＳＫ
］字符进行替代
，
通过模型建模上下文间


的关系
，
实现推理出［ＭＡＳＫ
］位置上真实的推理词
。


下面给出基于自然语言推理的提示学习方法的形式化表达形式
：


对于给定的输入文本Ｘ
，所对应的真实标签为Ｚ
，
以及需要推理判断的标签描


述ｒ
ｆ
，
通过函数／ｐ
（ｍｐｔ
，实现将输入ｘ转换为基于提示学习新的输入形式Ｖ
，
即
：


Ｘ
— ｆｐｒｏｎｖｐｔｄ）
（３
－
１）


２６


第三章
基于提示学习的少样本文本分类算法研究


其中
，
ｚ表示通过ｖｅｒｂａ
ｌ
ｉｚｅ映射将真实标签与需要推理判断的标签描述的关


系转为逻辑推理词
，
具体表示为
：


ｚ＝ｖｅｒｂａ
ｌ
ｉｚｅ（
Ｚ
，ｄ）（３
－２）


这
［ＭＡＳｉｑ体育新闻
ｅ［ＳＥＰ
］
ｉ
＿
｜斌
［ＭＡＳｉｇ误乐新闻
。
［ＳＥＰ
］
｜
｜
这
ＩＭＡＳＫ
ｊ
科技新闻
。
［ＳＥＰ
］


＇
ｖ
－—
＞
－Ｋ
、
■
■

？


ＭＬＭＨｅａｄ


＞
，


▲
？＊
Ａ


！ｌｓ
ｉ茨麫
！ｌｓ领淘
ｉｉｓ


１
！、
；
１
；
１
Ｉ


，
■
… （ｎ概Ｌ
．
．
．
．｜
］．
餐
？乐
■紐
；… …二
］





？
？
？（
？猶細
）
．
．
？＇


一
．
，
，
．ｍ





’ｍ，



一ｍ




Ｖ
Ｊ＼／
Ｖ，／


图
３
－２基于自然语言推理的提示学习结构图


定义模版的
一般形式为
：
［ｘ
＇
］
＝
“
［ｘ
］
，
［ｚ
｜
［ｄ
］
。
”
。对于原始输入Ｘ
，
将其填充到


Ｍ
，
以及需要推理判断的标签描述ｄ填充到［ｄ
］中
；
接下来
，
将输入；Ｃ的真实标签


描述
〖
，与当前填充需要推理判断的标签描述ｄ
，通过映射函数ｖｅｒｂａ
ｌｉｚｅ（
〖
，ｄ
）获得


当前输入的逻辑推理词２
。其中
，
［ｚ
］将被预训练语言模型中
［ＭＡＳＫ
］字符进行替


代
，
以及逻辑推理关系词ｚ将作为
［ｚ
｜的真实标签参与模型的优化
。
在推理阶段
，


在数据处理中会对输入ｘ和所有的标签描述ｄ
，
通过映射函数／ｐｒ＾ｍｐｔ
，
转化为基


于提示学习Ｘ
＇的形式
。最后通过计算［幻处的自然语言推理词概率
，选取预测为蕴


含关系最大概率的标签描述ｄ所对应的真实标签
，
作为最后的预测结果
。


当采用自然语言形式的逻辑推理词时
，
也就是说
，
使用自然语言中的
“ 是
”


表示蕴含推理关系
，
“ 不是
” 表示非蕴含推理关系
。


例如对于给定原始输入文本
“ 因痛失比赛而落泪的职业选手
，
让人感到


心疼
。
”
，所对应的真实标签描述为
：
ｄ＝
“ 体育
”
。通过函数／ｐｒ＾ｍｐｔ
，
可以实现其


转换为基于提示学习新的输入文本
因痛失比赛而落泪的职业选手
，让人感到


心疼
，
这
［ｚ
］体育新闻
。
”
，
通过对模型参数的优化
，
使模型学习到［ｚ
］处填入推理


词
“ 是
” 的概率远远大于
“ 不是
” 的概率
。


进
一步
，为了让语言模型能够学到更通用自然语言推理的表示
，本文对于推


理词采用连续式的提示模版形式
。
即使用词表中未使用过的字符
“
［Ｕ１
］
”代表蕴


含推理关系
；
“
［Ｕ２
］
”代表非蕴含推理关系
。


在自然语言推理形式的提示学习训练过程中
，本文通过构造
一定比例的负例


样本实现数据增强
，也就是通过扩增非蕴含推理的实例
，提升基于自然语言推理


的提示学习建模效果
。此外
，针对单样本输入形式
、
以及通过数据增强形式扩增


２７


北京邮电大学工程硕士学位论文


负样本形成样例集合的形式
，本文设计并实现了两种不同粒度损失函数优化建模


效果
：


（
１
）ＳｅｎｔｅｎｃｅＬｅｖｅｌ损失函数
：


如图
３
－３所示
，对于每
一个通过
映射函数构成新的输入实例
，需要模


型完成建模上下文信息并预测推理出
［ＭＡＳＫ
］位置处的真实推理词
，
并通过交叉


熵进行优化
。


＇
四痛失比赛而落泪的职业选手／


＞让人感
心疼
。



ｆ
，
－


：
ｆ因痛失比赛而落
＇
泪的职业选手
，


；
｜
让人感到心疼
＝
这
ＦＮＩＡＳＫＩ体育


；
！



＜Ｉ
＊
＊＂
＂Ｌｑｓ
．


＾ＭＬＭＨｅａｄ
！／


１Ｌ￣￣－
ｐ
￣１
＊；


％
＾
＾ｔ


？


体育新闻
：


ＶＡ：０．８
－
－
－一


不是
：
０＊２


Ｖ．Ｊ


图３
－３ＳｅｎｔｅｎｃｅＬｅｖｅｌ优化过程


具体地
，
在给定输入ｘ情况下
，
定义［ＭＡＳＫ
］处推理词ｚ的概率分布如公式
３
－


３所示
：


ｅ
ｓ（ｚ
＼ｘ）


￣
Ｅｚｅｚｅ
Ｓ（－Ｚ
，Ｘ）
（３
＿３）


其中
，
Ｚ表不候选推理词集合
，
５（２
丨尤）
＝财／＾（２
丨／？
？７
＞
。？＾办））表示在
｜＞１＼３１＜＾位置


处对候选推理词集合的语言模型得分
。


最后
，
通过交叉摘
（ＣｒｏｓｓＥｎｔｒｏｐｙ）损失计算
ＳｅｎｔｅｎｃｅＬｅｖｅｌ损失
，
如公式


３
－４所示
：


￡ｓ
＝ＣＥ（ｑ（Ｈｘ）
，ｚ）
（３
－４）


（２）Ｓｅｎｔｅｎｃｅ
－ＧｒｏｕｐＬｅｖｅｌ损失函数
：


上述的
ＳｅｎｔｅｎｃｅＬｅｖｅｌ损失函数仅考虑对实例本身进行优化
，
没有考虑到同


一组正负样本间的关系
。
所以
，
这里通过定义
Ｓｅｎｔｅｎｃｅ
－ＧｒｏｕｐＬｅｖｅｌ损失函数
，


从而实现对
一组中的正负样本间关系进行优化
。


具体地
，本文在对输入的实例进行数据构造时
，会通过输入实例与所对应的


类别生成
一个正例
。此外
，
将输入实例与其他的类别进行数据构造
，
生成ｎ
－
１个


２８


第三章基于提示学习的少样本文本分类算法研究



负例
，最终总共为每
一条输入样本生成ｎ个实例样本
。最后
，
同样采用交叉熵损


失对ＳｅｎｔｅｎｃｅｒＧｒｏｕｐＬｅｖｅｌ进行优化。


＝ＣＥ（ｇ｛ｓＣｚ
＼ｘ）ｌＩｅｎｔａａ）（３
－５）


其中
，
定义４ｎｔ〇ａ表示在当前样例组中真实标签为蕴含关系的位置索引
，


沒（ｓ（ｚＷ）表示语言模型对＿８幻位置处的推理词在蕴含关系维度上的预测得


分
。


最后
，
基于自然语言推理的提示学习模块的损失函数定义如下
：


Ｌ
ｐ
＝
｛ｌ
－ｄ）
－Ｌｓ＋ａ
＾Ｌ
ｑ（３
－６）


其中
，
ａ为可调节超参
。


３．２．３度？优化微


提示学习通过利用预训练语言模型在预训练任务中学习到的先验知识，在具


体下游任务中可以取得
一个良好的性能
。但是对于文本分类任务来说
，类别特征


的表示也是至关重要的
。基于度量学习的训练框架通常是解决诸多表示学习问题


的基础结构
，通过度量学习
，可以实现将原始语义空间的实例映射到目标任务上


语义空间的表示
，使得实例在目标任务上的语义空间的表示具有更强的区分能力
。


度量优化模块的优化目标是使在语义空间中属于同
一类别的实例的距离更


接近
，
而不同类别的实例的距离远离
。本文通过三元组损失函数
（Ｔｒ
ｉｐ
ｌｅｔＬｏｓｓ）


进行有监督的度量学习
，
使得模型可以更好学习不同类别间的距离关系信息
［４９］
。


此外
，
使用带间隔的损失函数可以提升模型的泛化性能
，
公式如
３
－７所示
。具体


地
，在采用三元组损失构造数据时
，本文在某个类别中选定
一个实例作为Ａｎｃｈｏｒ
，


同类别的的实例作为Ｐｏｓｉｔ
ｉｖｅ
，
其他类别的实例作为Ｎｅｇａｔ
ｉｖｅ。


Ｎ


Ｊ＾ｔｍｌ
＝ｍａＸ（〇
＊ｄ＾＞Ｐｎ＞
￣Ｎｎ）＋Ａ）（３
－７）


ｎ＝ｌ


其中
，
表示
ａｎｃｈｏｒ与
Ｐｏｓｉｔｉｖｅ实例间的距离
，
ｄ
（ｙ４ｎ，
ｉＶｎ）表示
ａｎｃｈｏｒ与


Ｎｅｇａｔ
ｉｖｅ实例间的距离
，
Ａ表示是设定的间隔值。


此外
，在少样本学习场景中
，通常用于训练的数据量是有限的
。为了防止过


拟合问题以及缓解灾难性遗忘等问题
，
这里使用掩码语言模型任务
（Ｍａｓｋｅｄ


ＬａｎｇｕａｇｅＭｏｄｅｌ
，
ＭＬＭ）作为正则项进行建模。所以
，度量优化模型的损失函数


表示为
：


＾ａｖｘ
＝
（１
￣
／＾）
？
＾ｔｍｌ＋
Ｐ
＇
＾ｍｌｍ〇８）


其中
，
表示语言模型损失
，
沒表示语言模型损失的权重参数
。


最后
，
整体的损失函数由提示学习损失、和度量优化损失Ｘａｕｘ的加权构成
，


具体如下
：


２９


北京邮电大学工程硕士学位论文


＾■
ｔｏｔａｌ
＝
（１
—
ｙ）
■＋ｙ
■
＾ａｕｘ（３
－９）


其中
，
Ｘ
ｐ表示基于自然语言推理的提示学习模块的损失
，
￡ａｕｘ表示度量优化模块


的损失
，
ｙ表示权重参数
。


３．２
．４基于自然语言推理的继续预训练过程


预训练模型通过在大规模无监督语料上进行预训练
，学习到了丰富的通用知


识表示
。更进
一步地
，本文尝试将基于自然语言推理的提示学习方式融入到预训


练任务中
。
通过这种方式
，
一方面将自然语言推理的知识融入到预训练任务中
；


另
一方面探索将提示学习形式融入到预训练语言模型中
。


本文整理多个不同领域的有标注的中文文本分类任务数据集
，通过自定义基


于自然语言推理的提示模版
，
构造预训练任务的语料
，进行继续预训练
（Ｆｕｒｔｈｅｒ


Ｐｒｅ
－ｔｍｉｎｉｎｇ）
。同时为了保证提示模版文本表达形式的多样性
，在符合语句通顺的


情况下
，
构造了多种形式的提示模版
。
具体的数据构造方式如图
３
－４所示
：


ｉ接下来＜是＞体育新闻
：
因痛失比赛而落泪的职业选手让人感到心疼。


下面
＜
不是＞姨乐新闻
５
而＜是＞体育新闻
：
因痛失比赛而落泪的职业选手让人感到心疼。


因痛失比赛而落泪的职Ａ选手让人感到心疼
，
体育新闻？＜是的＞
？

，


１
因痛失比赛而落泪的职止选手让人感到心疼
，
娱乐新闻？＜不是＞的
，
而＜是＞体育新闻
。
＇


图
３
－４数据构造示例


本文基于自然语言推理形式的提示模版的数据构造形式主要分为两种
：直接


推理形式和两阶段推理形式
。直接推理提示模版的主要形式为
：
＜Ｍ＞＜ｌａｂｅｌｄｅＳＣ＞
。


其中
：
＜Ｍ＞为推理词部分
，
ｃｌａｂｅｌｄｅｓＯ为推理描述
。
例如
：＜是＞体育新闻
。两阶


段推理提不模版的主要形式为
：＜？Ｍ＞〈ｌａｂｅｌｄｅｓｃ
ｌ＞
，＜Ｍ＞＜ｌａｂｅｌｄｅｓｃ２＞
。
例如
：


＜不是＞娱乐新闻
，
＜是＞体育新闻
。进
一步地
，根据基于自然语言推理形式的提示


模版位置
，
可以将提示模版分为前缀式
、后缀式
。其中
，前缀式是指模版位于句


子的开始位置处
；后缀式是指模版位于句子尾部
一种形式
。此外
，模版的构造形


式和模版的位置关系可以相互组合
，形成更多
、更灵活的组合形式
，
具体如下表


３
－
１所示
：


表
３
－
１
模版构造形式与位置组合


＾
模版位置
７


模版＾
前缓式

后缓式


直接推理形式前缀直接推理形式后缀直接推理形式


两阶段推理形式
前缀两阶段推理形式
后缀两阶段推理形式


３０


第三章
基于提示学习的少样本文本分类算法研究


本文在中文
１１〇１３￡１１丁３
－＼胃１１＾＼＾
３４
］预训练语言模型的基础上进行继续预训练
。


具体的预训练方法如下
：


对于推理词
（＜Ｍ＞或者＜？Ｍ＞
，假设＜Ｍ＞表示肯定推理词
，则＜￣＼１＞表示否定


推理词
）采用静态ＭＡＳＫ方案
，
随机选择
６０％的句子
，
并随机对其中的
一个推


理词
（＜Ｍ＞或者＜？Ｍ＞
）进行基于掩码的语言模型预测
。其中
９０％的推理词通过


［ＭＡＳＫ］字符进行替换操作
，
剩余
１０％的推理词维持不变
，
通过基于掩码的语言


模型完成对推理词的预测
。


对于其他的字符采用字符级别的动态随机ＭＡＳＫ方案
。
也就是对于输入的


一个句子
，
随机选择句子中
１５％的字符
，
然后将其中
８０％的字符使用
［ＭＡＳＫ
］符


号进行替换
，其中
１０％使用随机的其他字符进随机替换
，剩下的
１０％保持不变
。


通过上述继续预训练方案
，既保持原始掩码式的语言预训练任务不变
，
同时


又实现对自然语言推理的强化
。


３．２．５算法流程


本文提出的算法将文本分类任务转化成自然语言推理任务
，即转化后的任务


本质上变成
一个二分类任务
。因此
，
当
一个原始分类任务存在Ｎ个类别时
，则该


算法需要进行
Ｎ次的推理
，
最后选择推理概率最大所对应的标签类别作为最终


预测结果
。


所以
，
在模型训练过程中
，为了提升模型的性能与泛化性
，
同时降低模型训


练的成本
，本文通过负采样的方式对下游任务进行训练
。对于
一个包含多个类别


分类任务
，本文会将每
一个实例与之对应的类别
，
作为正例
；
同时
，
会随机选择


Ｋ个其他类别与当前实例构成负例
。通过上述数据构造方式
，不但能够提升模型


的性能
，
而且相比使用全部类别构造负例进
一步缩短了模型训练所需时间
。


在模型推理阶段
，本文提出的算法仅激活基于自然语言推理的提示学习模块
。


具体地
，
对于包含Ｎ个标签的文本分类任务
，
在数据处理阶段会对每
一个实例


生成包含自然语言推理提示模版的
Ｎ条新的输入实例
。
通过模型预测每
一个实


例中
［ＭＡＳＫ
］位置处蕴含推理词的概率
，在Ｎ个预测结果中选择预测概率最大所


对应的标签作为当前原始输入实例的预测结果
。


详细的模型推理过程如算法
３
－
１所示
，其中
，第
１
？５行是通过／ｐＴｗｎｐｔ映射函


数
，将需要推理的原始实例转化为包含提示模版的新输入实例
，
？第
６行通过模型


预测每
一个新实例在［ＭＡＳＫ
］位置处蕴含推理词的概率
；
第
７
？
１
１
行按照标签列


表长度
，
将预测结果分成Ｍ份
，
在每
一分中
，
选择预测结果概率最大的标签索


引作为预测结果
，
并将预测结果返回
。


３
１


北京邮电大学工程硕士学位论文


＿
算法３
－
１
：
模型推理过程
｜


输入
：
需要推理的数据
ｄｍ｝
，
标签列表［＝
，
Ｚｎ｝
，


模型输入数据
＜？
＝
｛七
，
模型输出结果ｉ？＝
｛ｎ
，
．
．
．
，ｒｍ＋ｎ｝


输出
：
推理结果Ｓ＝
｛＆
，
．
．
．
，ｓｍ｝


１
：ｆｏｒｉｎｄｅｘｉｏｆＤ
：


２
：ｆｏｒｉｎｄｅｘｊｏｆＬ
：


３
－Ｑｉ＋ｊ
卜ｆｐｒｏｍｐｔ（／＾
ｉ
，ｎｕｌｌ
，
ｌ
ｊ）


４
：ｅｎｄｆｏｒ


５
：ｅｎｄｆｏｒ


６
：Ｒ＜
－ｍｏｄｅｌＥｖａｌ＾Ｑ）


Ｔ
．Ｎ—ｌｅｎ（Ｌ）
，ｊ— Ｑ


８
：ｆｏｒｉｉｎ
ｒａｎｇｅ（０
，
／ｅｎ（／？）
，Ｎ）
：


９
：＾
ａｒｇｍａｘＰｒｏｂｄ
＂Ｕ


１０
：ｅｎｄｆｏｒ


１
１
：ｒｅｎｔｕｍＳ


３
．３实验设计和分析


本节将在中文公开文本分类数据集和英文公开文本分类数据集上进行实验
。


通过与基线方法对比
，
以及使用组件有效性分析
、负采样性能分析和可视化分析


等方法
，
验证本文提出方法的有效性
。


３．３．１评测数据集介绍


（
１
）
中文数据集


中文数据集使用公开少样本评测数据集ＦｅｗＣＬＵＥ作为实验数据集
。该数据


集包含多个不同类型的子任务
，
具有
一定的代表性
。
同时
，有些任务包含了众多


类别
（如
５０＋、
１００＋的多分类任务
）
，
以及在部分任务中存在
一定的类别不均衡问


题
。
本文选取ＦｅｗＣＬＵＥ中的文本分类任务作为评测数据集
。


表
３
－２
中文ＦｅｗＣＬＵＥ文本分类数据集


ＣｏｒｐｕｓＴｒａｉｎＤｅｖＴｅｓｔ（ｐｕｂｌｉｃ）＃ＬａｂｅＩｓＴａｓｋＳｏｕｒｃｅ


ＥＰＲＳＴＭＴ３２３２６１０２ＳｅｎｔｉｍｅｎｔＡｎａｌｙｓｉｓＥ
－ＣｏｍｍｒｃｅＲｅｖｉｅｗ


ＣＳＬＤＣＰ５３６５３６１７８０６７ＬｏｎｇＴｅｘｔＣｌａｓｓｉｆｙＡｃａｄｅｍｉｃＣＮＫｌ


ＴＮＥＷＳ２４０２４０２０
１０１５ＳｈｏｔＴｅｘｔＣｌａｓｓｉｆ
ｙＮｅｗｓＴｉｔｌｅ


ＩＦＬＹＴＥＫ９２８６９０１７４９１
１９ＬｏｎｇＴｅｘｔＣｌａｓｓｉｆ
ｙＡｐｐＤｅｓｃ


３２


第三章基于提示学习的少样本文本分类算法研宄


如表３
－２所示
，本文在四个不同的领域任务上对评测数据集进行实验。其中
，


ＥＰＲＳＴＭＴ为分别电商评论情感分析任务
，是典型的包含正向情感和负向情感的


二分类任务
。
ＣＳＬＤＣＰ是针对科学文献学科领域的长文本多分类任务，包含了多


达６７个类别
。
ＴＮＥＷＳ
ｆＭ是针对新闻标题的短文本分类任务
，包含了教育、娱乐


和文化等
１５个类别
。其次
，
ＩＦＬＹＴＥＫ是根据ＡＰＰ应用的长文本主题描述信息
，


对超过
１００多个应用类别进行分类的任务
。最后
，本文对中文评测数据集总结了


以下几点特点
：


１
）文本长度多样
。包含短文本分类任务
（ＴＮＥＷＳ）
，
又包含长文本分类任


务
（ＣＳＬＤＣＰ、
ＩＦＬＹＴＥＫ）
。


２）任务类别多样
。包含不同分类类别数据的任务形式。
ＥＰＲＳＴＭＴ（２个类


别
）
、ＴＮＥＷＳ
Ｃ
１５个类别）
、ＣＳＬＤＣＰ
（６７个类别）以及
ＩＦＬＹＴＥＫ
（
１１９个类别）
。


３
）任务类别数据量设置更具有
一般性。类别少的任务
一般每个类别包含
１６


个实例（ＥＰＲＳＴＭＴ、ＴＮＥＷＳ）
。类别较多的任务每个类别包含８个实例（ＣＳＬＤＣＰ）
。


此外
，
在
ＩＦＬＹＴＥＫ任务中
，
分类的类别数不仅超过
１００
，
而且单个类别的平均


实例数７
．８个
，更是存在着
一定地类别不均衡现象
，个别类别的数据量不足５个
。


２）英文数据集


目前暂没有公开的英文少样本文本分类评测数据集
，本文在
３个流行的英文


文本分类数据集
（ＡＧＮｅｗｓ
，Ｔｒｅｅ
，ＹｅｌｐＲｅｖｉｅｗ）上进行评测
。英文数据集的详细


介绍如表
３
－３所示。


ＡＧＮｅｗ＃
２朦据集：是学术新闻搜索引擎从多个新闻来源中搜集超过了１００


万篇新闻文章构成的数据集
。主要用于非商业活动的研究目的
。总共包含４类新


闻主题
，
分别是
：
Ｗｏｒｌｄ、
Ｓｐｏｒｔｓ、
Ｂｕｓｉｎｅｓｓ和Ｓｃｉ／Ｔｅｃｈ。


Ｔｒｅ＃
１
］数据集：
中文名叫文本检索会议
（ＴＲＥＣ）
问题分类数据集。该数据


集包含６个
一级标签
，４７个二级标签。每个句子的平均长度为
１０
，词汇量是８７００
。


ＹｅｌｐＲｅｖｉｅｗ数据集：
ＹｅｌｐＲｅｖｉｅｗ数据集来自于Ｙｅｌｐ的用户评论
。它是从


Ｙｅｌｐ的
Ｃｈａｌｌｅｎｇｅ２０１５数据集中提取出来的
。
它的标签是用户对商品的星级打


分
，
总共分为
５档
，
分别是
１星、
２星
、
３星、
４星和５星
。


表３
－３
英文数据集详情


ＣｏｒｐｕｓＴｒａｉｎＴｅｓｔ＃Ｌａｂｅｌｓ


ＡＧＮｅｗｓ１２００００７６００４


Ｔｒｅｅ５４５２５００６


ＹｅｌｐＲｅｖｉｅｗ
６５００００
５００００
５


３３


北京邮电大学工程硕士学位论文


本文用于少样本学习的英文评测数据集具体如表３
－４所示
。具体地
，
用于评


测的英文数据集将从上述数据集中抽样完成。对每
一个原始英文数据集中随机抽
一


取
８个实例
、
１６个实例
、
３２个实例等
，形成对多个不同规模的数据集用于训练
，


测试集依然使用默认数据集。


表
３
￣４
少样本英文评測数据集详情


ＣｏｒｐｕｓＴｒａｉｎＴｅｓｔ＃ＬａｂｅＩｓ


ＡＧＮｅｗｓ８３２７６００４


ＡＧＮｅｗｓ１６６４７６００４


ＡＧＮｅｗｓ３２
１２８７６００４


Ｔｒｅｅ８４８５００６


Ｔｒｅｅ１６９７５００６


Ｔｒｅｅ３２
１９２５００６


ＹｅｌｐＲｅｖｉｅｗ８４０５００００５


ＹｅｌｐＲｅｖｉｅｗ１６８０５００００５


ＹｅｌｐＲｅｖｉｅｗ３２１６０５００００５


３．３．２评澜指赫基线方法


（
１
）评测指标


对于少样本学习问题
，
通常使用准确率
（Ａｃｃｕｒａｃｙ）作为评测指标
。它表示


模型预测正确的样本数量占所有的样本数量的比例
。如公式
３
－
１０所示
：


ＣＯＵＴｌｔｔｒｕｅ
，、


Ａｃｃｕｒａｃｙ
＝（３
－
１０）


ｃｏｖｎｔ
ｆ０ｆ；ａｉｓ


其中
，
ＣＯＭｎｔ
ｆｒｕｅ表示预测结果正确的样本数量
，
表示样本总数
。


（２）基线方法


本文的基线对比方法有基于微调
（Ｆｉｎｅ
－ｔｕｎｉｎｇ）的方法、
Ｚｅｒｏ
－ｓｈｏｔ、Ｚｅｒｏ
－ｓｈｏｔ


（ＧＰＴ）、
ＰＥＴ、
ＬＭ
－ＢＦＦ、
ＥＦＬ和Ｐ
－ｔｕｎｉｎｇＲ等方法。


基于微调
（Ｆｉｎｅ
－
ｔｕｎｉｎｇ）方法
：
在预训练语言模型的基础上
，
通过为模型添


加任务相关的分类器
，
达到使模型可以处理具体的下游任务的目的
。


Ｚｅｒｏ
－ｓｈｏｔ方法
：
基于
Ｒｏｂｅｒ
ｔａ等自编码预训练语言模型
，
通过Ｍａｓｋｅｄ


ＬａｎｇｕａｇｅＭｏｄｅｌ（ＭＬＭ）进行推理评测
。


Ｚｅｒｏ
－ｓｈｏｔ（ＧＰＴ）方法
：基于ＧＰＴ自回归预训练语言模型
，通过Ｌｅｆ
ｔ
－ｔｏ
－ｒ
ｉｇｈｔ


ＬａｎｇｕａｇｅＭｏｄｅｌ（Ｌ２ＲＬＭ）进行推理评测
。


ＰＥＴ方法
：通过添加人工自定义模版
，将下游任务转化成完成填空形式的任


务
，
然后在候选标签列表中选择合适的标签
。
－


３４


第三章
基于提示学习的少样本文本分类算法研究


ＡＤＡＰＥＴ方法
：对模版搜索正确答案时从有限候选词变成整个词表
，扩大了


模型的搜索空间
。此外
，对正确标签反向预测原文中的词
，实现模型性能的提升
。


ＬＭ
－ＢＦＦ方法
：通过自动化生成的离散化自然语言作为提示模版
，
同时通过


采样的形式
，
将实例以上下文的方式添加到每
一个输入中
。


ＥＦＬ方法
：通过添加人工自定义模版
，将下游任务转化成蕴含任务形式
，
并


添加额外的二分类器
，
实现对下游任务的微调
。


Ｐ
－ｔｕｎｉｎｇＲ方法
：
区别于自然语言形式的提示模版
，采用Ｒｏｂｅｒｔａ预训练语言


模型
，
实现让模型自动学习到最佳的连续式的非自然语言提示模版
。


３
．３
．３网络训练参数设置


本文的实验是在配有
ＣＵＤＡ环境的
Ｌｉｎｕｘ操作系统下进行的
，
并配置了两


张ＧＴＸ１０８０Ｔｉ显卡
。代码是使用基于
ＰｙＴｏｒｃｈ
［５３］框架的Ｈｕｇｇ
ｉｎｇＦａｃｅ的工具包


实现的
。
此外
，
对于中文数据集的评测
，
本文采用了１２层网络结构的中文


ＲｏＢＥＲＴａ
－ｗｗｍ
－ｅｘｔ预训练模型
。对于英文数据集的评测
，
则采用了
］２层结构的


ＢＥＲＴ
－ＢＡＳＥ预训练模型
。模型的参数设置如下
：
学习率为ｌｘ１（Ｔ
５
，超参ａ设置


为
０
．７
，
夕为
０
．０
１
，
为
〇
．〇２
，
三元损失间隔Ａ为
０
．
】５
，
并且使用入（１３１１１＼￥
［５５
］优化


器进行模型参数的优化
。


此外
，在继续预训练任务的实验中
，训练数据来自网络搜集的多个不同任务


类型的公开数据集Ｌ
具体地
，
模型的参数设置如下
：
批大小为
３２
，
学习率为


２ｘ１
ＣＴ
５
，权重衰减率为０
．０
１
，热启动的批数量大小为
５００
，句子最大长度为
５
１２
。


同时
，为了加快模型的训练速度
，实验中使用到了梯度累加技术
，梯度累加步骤


数为
６
。


３．３
．４中文数据集实验结果


在中文少样本ＦｅｗＣＬＵＥ数据集中
，本文与其他现有的基线方法进行了性能


对比实验
，
实验结果如表
３
－５所示
。


从评测实验结果可以看出
，
对于传统基于微调
（Ｒｎｅ
－
ｔｕｎｉｒｉｇ）
的方法
，
在真


实的小样本学习场景中模型性能通过表现不佳
。而对于采用基于提示学习的方法
，


通过使用
ＰＥＴ、
ＡＤＡＰＥＴ、
ＬＭ
－ＢＦＦ、
ＥＦＬ、
Ｐ
－ｔｕｎｉｎｇＲ方法
，
以及包括本文提出


的方法
，在小样本学习场景中模型的准确率都有了大幅度的提高
，也显示出了提


示学习方法具有强大潜能
。


通过对比本文提出的算法与其他基于提示学习的方法
（ＰＥＴ、ＡＤＡＰＥＴ、ＬＭ
－


ＢＦＦ
、
ＥＦＬ和
Ｐ
－ｔｕｎｉｎｇ等
）
，
可以看出本文提出的算法在ＥＰＲＳＴＭＴ、
ＣＳＬＤＣＰ和


ＴＮＥＷＳ等任务上取得了优异的成绩
。
此外
，
在
ＩＦＬＹＴＥＫ任务上
，
也取得了和


１ｈｔｔｐ
ｓ
：／／ｇ
ｉｔｈｕｂ
．ｃｏｍ／ＣＬＵＥｂｅｎｃｈｍａｒｋ／ＣＬＵＥＤａｔａｓｅｌＳｅａｒｃｈ


３５


北京邮电大学工程硕士学位论文


其他现有方法同等效果的性能
。并且本文提出的算法在中文文本分类任务的平均


准确率性能上取得了最高的成绩
。


表
３
－５
中文少样本数据集实验结果


ＭｅｔｈｏｄＥＰＲＳＴＭＴＣＳＬＤＣＰＴＮＥＷＳＩＦＬＹＴＥＫＭＥＡＮ


Ｈｕｍａｎ９０
．０６８
．０７
１
．０６６
．０７３
．８


Ｆｉｎｅ
－
ｔｕｎｉｎｇ６５
．４３５
．５４９
．０３２
．８４５
．７


Ｚｅｒｏ
－ｓｈｏｔ８５
．２１２
．６２５
．３２７
．７３７
．７


Ｚｅｒｏ
－ｓｈｏｔ（ＧＰＴ）５７
．５２６
．２３７
．０１９
．０３５
．０


ＰＥＴ８５．６５
１
．７５３
．７３５
．０５６
．５


ＡＤＡＰＥＴ８５
．
１４３
．５４６
．４３６
．６５２
．９


ＬＭ
－ＢＦＦ８４
．６５４
．４５３
．０４６
．
１５９
．５


ＥＦＬ８５
．０４５
．０５２
．
１４２
．７５６
．２


Ｐ
－
ｔｕｎ
ｉｎｇＲ８０
．６５４
．８５２
．２４８
．０５８
．８


Ｏｕｒ８５
．３５５．１５４．６４６
．４６０
．４


Ｏｕｒ
（＋ＦＰＴ
）８６．４５５．７５５．５４６
．８６１
．１


具体地
，
与转换为完形填空任务形式的
ＰＥＴ、ＡＤＡＰＥＴ等方法相比
，本文提


出的算法在利用预训练模型中已经学习到的通用知识的基础上
，通过引入下游具


有的任务类别表征信息
，
实现更好的建模效果
，
并在任务的平均准确率上高出


３
．９％
。
与转化为文本蕴含任务的
ＥＦＬ方法相比
，
本文提出的算法没有引入额外


需要学习的大规模参数
，并且与预训练语言模型任务保持相
一致
，有效减小的上


下游任务间的差异性
，
最终在任务的平均准确率上高出
４
．２％
。
与使用自动构建


模版或是非自然语言形式模版的ＬＭ
－ＢＦＦ
、
Ｐ
－ｔｕｎｉｎｇ方法相比
，
本文提出的算法


无需繁琐的模版构建形式
，
并且在任务的平均准确率上高出
１
．６％
。


此外
，
本文对中文预训练语言模型进行了基于自然语言推理的继续预训练


（ＦｕｒｔｈｅｒＰｒｅ
－Ｔｒａｉｎｉｎｇ
，
ＦＰＴ）
。其中
，
Ｏｕｒ
（＋ＦＰＴ
）是指在人工构造的自然语言推理


语料上完成继续预训练后
，
重新在下游任务上进行微调
。从实验可以看出
，通过


在人工构造的基于自然语言推理的语料上进行继续预训练后
，对下游任务具有
一


定的性能提升
。


３
．３．５英文数据集实验结果


在英文少样本评测数据集中
，本文对训练集中每
一类别包含不同规模的实例


数量
（Ｋ＝８
，１６和
３２）
，
与现有基于提示学习的典型方法
（ＰＥＴ
、
ＡＤＡＰＥＴ
、
ＥＦＬ


和Ｐ
－
ｔｕｎｉｎｇ等
）进行了性能对比实验
，
实验结果如表
３
－６所示
。


从实验的结果可以看出
，
对于不同的实例数量
（Ｋ＝８
，１６和
３２）
，
基于微调


的方法
、
ＰＥＴ
、
ＡＤＡＰＥＴ、
ＥＦＬ
、
Ｐ
－ｔｕｎｉｎｇ以及本文提出的算法
，都表现出随着实


３６


第三章基于提示学习的少样本文本处类算法研究


例数量的不断增多
，模型的准确率都有着明显的提升
，表明在基于深度模型的少


样本学习场景中
，
训练数据的规模对模型性能有着较大影响
。


表３
－６英文少样本数据集实验结果
．


ＭｅｔｈｏｄＡＧＮｅｗｓＴｒｅｅＹｅＪｐＲｅｖｉｅｗＭＥＡＮ


Ｆｅｗ
－ｓｈｏｔ
（Ｋ
＝８）


Ｆｉｎｅ
－ｔｕｎｉｎｇ５２．５２９．２１８．７３３
．５


ＰＥＴ７６
．０３８
．８２５
．０４６
．６


ＡＤＡＰＥＴ７８
．８２１
．６２４．２４１
．５


ＥＦＬ７８
．３４１
．６２１
．２４７．０


Ｐ
－ｔｕｎｉｎｇＲ６８
．３３
１
．８２２
．２４０
．８


Ｏｕｒ７９．５５５．８２６．１５３．８


Ｆｅｗ
－ｓｈｏｔ
（Ｋ＝
ｌ
６
）


Ｆｉｎｅ
－
ｔｕｎｉｎｇ６６
．４４６
．２２６．５４６．４


ＰＥＴ８３
．３６２
．４３０
．３５８
．７


ＡＤＡＰＥＴ８３
．６６２
．７３１
．７５９．３


ＥＦＬ８４
．６５５
．４３０
．０５６，７


Ｐ
－
ｔｕｎｉｎｇＲ８３
．２６６．４３０．０５９．９


Ｏｕｒ
８４９
６８＾
３２３
６１．９


Ｆｅｗ
－ｓｈｏｔ
（Ｋ＝３２）


Ｆｉｎｅ
－
ｔｕｎｉｎｇ８０
．８６６．８３２
．９６０
．２


ＰＥＴ８４
．７８０．３３９
．０６８．０


ＡＤＡＰＥＴ８５
．８８０
．１３
１
．５６５
．８


ＥＦＬ８６．０８１
．０３５
．９６７
．６


Ｐ
－ｔｕｎｉｎｇＲ８６
．
１８１
．８３９．２６８
．４


Ｏｕｒ８６．２８０
．８４０．０６９．０


其次在实例数Ｋ＝８时
，
从实验结果可以看出
，
虽然ＰＥＴ、
ＡＤＡＰＥＴ
、
ＥＦＬ


和
Ｐ
－ｔｕｎｉｎｇ等基于提示学习的方法比基于微调的方法模型的准确率有很大的提


升
，但本文提出的算法却表现出更加出众的性能
，即模型的准确率远远高于其他


方法。表明在给定较少实例的情况下
，本文提出的算法能够有效地对下游任务进


行建模，
也进
一步说明了本文提出模型方法的有效性。


进
一步
，
随着实例数的增加
（Ｋ＝
１６
，３２）
，
虽然其他基于提示学习方法的性


能也在提升
，但可以看出本文提出方法的模型准确率相比于其他方法
，依然保持


较高水平。
即使在实例数Ｋ＝３２的情况下
，本文提出的算法也与现有模型在性能


上保持在同
一水平
，
并在任务的平均准确率性能上保持最佳。


此外
，
为了可以更直观的观察到模型性能随实例的变化趋势
，
本文对
ＡＧ


Ｎｅｗｓ任务上不同模型的准确率进行了可视化
，如图
３
－５所示。从图上可以看出
，


基于提示的方法在少样本情况下
，模型准确率远远高于基于微调的方法。基于微


３７


北京邮电大学工程硕士学位论文


调的方法随着实例数的增多
（Ｋ＜＝３２）
，准确率基本呈线性变化
。并且本文提出的


算法在实例较少
（Ｋ
＝８
）
的情况下
，
具有良好的模型性能
。


１００
１


Ｆ
ｉｎｅ
－
ｔｕｎ
ｉｎｇ


ｐｅｔ


麵議
ＡＤＡＰＥＴ


９０
－ｗｓｍＥＦＬ


Ｐ
－
ｔｕｎ

ｉ
ｎｇＲ


晒＾ｍＭ§１


ｉｉｉ


８
１６３２


实例数００


图
３
－５ＡＧＮｅｗｓ任务上模型间性能对比


３
．３
．６组件有效性分析


（
１
）度量优化模块有效性分析


在度量优化模块的有效性分析实验中
，本文首先对比了基于度量学习不同的


损失优化方法
，
实验结果如表
３
－７（中文数据集）和表
３
－８（英文数据集）所示
。


接下来通过对本文所使用的度量优化模块进行消融实验
，观察并评估模型在移除


该模块前后的分类准确率性能变化情况
，
实验结果如表
３
－９（中文数据集）和表


３
－
１０（英文数据集
）所示
。


表
３
－７
中文数据集不同损失优化方法对比实验


ＭｅｔｈｏｄＥＰＲＳＴＭＴＣＳＬＤＣＰＴＮＥＷＳＩＦＬＹＴＥＫＭＥＡＮ


ＢＣＥＬｏｓｓ
（ＣｏｓｉｎｅＳｉｍ
ｉｌａｒｉｔｙ）７７
．６５４
．３５４
．
１４５
．２５７
．８


ＢＣＥＬｏｓｓ
（ＥｕｃｌｉｄｅａｎＤｉｓｔａｎｃｅ）８０
．６５５．５５２
．８４５
．
１５８
．５


ＣｏｎｔｒａｓｔｉｖｅＬｏｓｓ８３
．２５４
．５５３
．９４４
．４５９
．０


Ｔｒｉｐ
ｌｅｔＬｏｓｓ８５．３５５
．
１５４
．６４６
．４６０．４


表
３
－８
荚文数据集不同损失优化方法对比实验


ＭｅｔｈｏｄＡＧＮｅｗｓＴｒｅｅＹｅ
ｌｐＲｅｖ
ｉｅｗＭＥＡＮ


ＢＣＥＬｏｓｓ（ＣｏｓｉｎｅＳｉｍｉ
ｌａｒｉｔｙ）７２
．２５３
．４２５
．３５０
．３


ＢＣＥＬｏｓｓ（Ｅｕｃ
ｌｉｄｅａｎＤ
ｉｓｔａｎｃｅ）７２
．
１５
１
．６２５
．
１４９
．６


ＣｏｎｔｒａｓｔｉｖｅＬｏｓｓ７２
．９５４
．０２６
．６５
１
．２


Ｔｒｉｐ
ｌｅｔＬｏｓｓ
７９．５５５．８２６
．
１５３
．８


３８


第三章基于提示学习的少样本文本分类算法研究


在对比基于度量学习的不同的损失优化方法实验中
，本文对比了使用欧式距


离和余弦相似度作为度量方法的二元交叉熵损失
（ＢＣＥＬｏｓｓ）、对比损失以及三


元组损失的优化方法＊


具体地
，在使用二元交叉熵损失作为损失优化的实验中
，采用欧式距离作为


度方法。由于该方法的度量值域范围是［０
，＋叫
，为了便于计算二元交叉熵损失
，


本文将其映射到值域空间［０
，１
）的范围
，
具体如公式
３
－
１
１所示。


／⑷
＝
ｔａｎｈ
（３
－１
１）


其中
，
ｅ为超参数
，
目的是防止分母出现０的情况
。


其次
，在使用基于余弦相似度的度量方法中
，其值域范围是［
－１
，＋１
］
。同理
，


本文将其映射到值域空间［０
，１
］的范围
，
具体如公式
３
－
１２所示。


／（ｒ）
＝ｉ
（ｒ＋ｌ）（３
－１２）


从实验结果可以看出
，使用欧式距离或者余弦相似度作为度量方法的二元交


叉熵损失优化方法的性能相对较差
，而对比损失和三元组损失优化方法，在性能


上有较大提升
，而这得益于在这两个损失优化方法中引入间隔的策略
，使得模型


有了
一定的容错空间
，进而提升了模型泛化性能
。更进
一步地
，三元组损失通过


同时对比三组不同的实例
，使得它可以同时获取到更多的信息帮助模型进行优化
，


提升模型的性能
。与对比损失相比
，三元组损失在不同任务的平均准确率性能有


了１
．４％的提升。


表
３
－９
中文数据集度量优化模块消融实验


ＭｅｔｈｏｄＥＰＲＳＴＭＴＣＳＬＤＣＰＴＮＥＷＳＩＦＬＹＴＥＫＭＥＡＮ






Ｏｕｒ
（
－Ｔｒ
ｉｐ
ｌｅｔＬｏｓｓ
－ＭＬＭ）８３
．４５４
．６５４
．０４３
．３５８
．８


Ｏｕｒ
（
－
３ＭＤＬＭ）８５
．２５３
．０５４
．２４５
．７５９．５


ＯｕｒＬｏｓｓ８５
．３５５
．
１５４
．６４６．４６０．４


表
３
－
１０英文数据集度量优化模块消融实验
（Ｋ＝８）


ＭｅｔｈｏｄＡＧＮｅｗｓＴｒｅｅＹｅｌｐＲｅｖｉｅｗＭＥＡＮ


Ｏｕｒ
（
－Ｔｒ
ｉｐ
ｌｅｔＬｏｓｓ
－ＭＬＭ）７５
．５５４．４２１
．７５０．６


Ｏｕｒ
（
－ＭＬＭ）７８
．８５０
．０２６．３５Ｌ７


ＯｕｒＬｏｓｓ７９．５５５
．８２６
．１５３
．８


接下来
，本文对采用三元组损失的度量优化模块进行了消融实验。从实验结


果可以看出当移除完整的度量优化模块后
，模型的准确率有明显的性能下降
，在


中文数据集中平均下降了１
．６％
，
而在英文数据集中平均下降了３
．２％
，
从而验证


了度量优化模块的有效性。该模块通过学习下游任务中的类别信息
，实现了对模


型性能的提升
。


３９


北京邮电大学工程硕士学位论文


近
一步
，
本文在度量优化模块中将语言模型
（ＭＬＭ）损失作为三元组损失


的正则项引入
。为了验证ＭＬＭ正则项的有效性
，
本文仅保留了三元组损失
，
并


移除了ＭＬＭ损失正则项
。从实验结果可以看出
，
当移除ＭＬＭ正则项后
，
模型


的准确率性能在大部分任务上都有明显的下降
，
在中文
ＣＳＬＤＣＰ任务下降了


２
．
１％
，甚至在英文Ｔｒ
ｅｅ任务上下降
５
．８％
。这也说明了通过引入ＭＬＭ损失作为


正则项
，
对模型的性能提升是有效的
。


（２
）Ｓｅｎｔｅｎｃｅ
－ＧｒｏｕｐＬｅｖｅｌ损失有效性分析


在基于自然语言推理的提示学习模块中
，
本文通过
Ｓｅｎｔｅｎｃｅ
－ＧｒｏｕｐＬｅｖｅｌ损


失实现对
一组正负实例间的优化
。同样为了确定对该损失优化方法的有效性
，本


文对其进行消融实验
。实验结果如表
３
－
１
１（中文数据集
）
、表
３
－１２（英文数据集
）


所示
。


在中文数据集中通过实验可以发现
，
Ｓｅｎｔｅｎｃｅ
－ＧｒｏｕｐＬｅｖｅｌ损失优化方法对


不同的任务都有明显的性能提升
。尤其对于
ＩＦＬＹＴＥＫ任务
，更是有
３％的性能提


升
。对于英文数据集
，本小节依然在实例数Ｋ
＝８上进行试验
。可以发现
，相比于


中文数据集
，
在英文数据集中
Ｓｅｎｔｅｎｃｅ
－ＧｒｏｕｐＬｅｖｅｌ损失对模型的准确率具有更


大的提升作用
。在ＡＧＮｅｗｓ任务上更是提升了３８
．６％
，
即使在ＹｅｌｐＲｅｖｉｅｗ任务


中也有
５
．９％的提升
。


表
３
－
１
１
中文数据集
Ｓｅｎｔｅｎｃｅ
－ＧｒｏｕｐＬｅｖｅｌ损失有效性分析


ＭｅｔｈｏｄＥＰＲＳＴＭＴＣＳＬＤＣＰＴＮＥＷＳＩＦＬＹＴＥＫ


Ｏｕｒ（
－Ｌ
ｇ
ｒｏｕｐ）８４
．９５４
．２５４
．
１４３
．４


Ｏｕｒ８５
．３５５
．
１５４
．６４６
．４


表
３
－
１２
英文数据集
Ｓｅｎｔｅｎｃｅ
－ＧｒｏｕｐＬｅｖｅｌ损失有效性分析
（Ｋ＝８
）


ＭｅｔｈｏｄＡＧＮｅｗｓＴｒｅｅＹｅｌｐＲｅｖｉｅｗ


Ｏｕｒ
（
－Ｌ
ｇｒ〇ｕｐ）４０
．９３９
．６２０
．２


Ｏｕｒ７９
．５５５
．８２６
．
１


通过该实验表明
，
针对组内优化
Ｓｅｎｔｅｎｃｅ
－ＧｒｏｕｐＬｅｖｅｌ损失方法是有效的
。


它通过对比组内正负间的实例
，
可以学习到更好的知识表示
。


３
．３．７提示模版分析


（
１
）推理词形式性能分析


本文提出的算法将文本分类任务转换为基于自然语言推理形式的完型填空


任务
，
同时受到Ｐ
－
ｔｕｎｉｎｇ方法的启发
，推理词不仅可以是自然语言形式
，
也可以


４０


第三章基于提示学习的少样本文本分类算法研究


是非自然语言形式
。为此，本小节将对这两种形式的推理词进行性能评估
。实验


结果如表
３
－１３（中文数据集）、表３
－１４（英文数据集）所示。


表
３
－１３
中文数据集推理词形式性能分析


Ｍｅｔｈｏｄ

ＥＰＲＳＴＭＴＣＳＬＤＣＰＴＮＥＷＳＩＦＬＹＴＥＫＭＥＡＮ


自然语言推理词８６．０５４
．３５３
．５４５．４５９．８


非自然语言推理词８５
．３５５．１５４，６４６．４６０．４


表
３
－
１４英文数据集推理词形式性能分析
（Ｋ＝８）


ＭｅｔｈｏｄＡＧＮｅｗｓＴｒｅｅＹｅｌｐＲｅｖｉｅｗＭＥＡＮ


自然语言推理词７７
．３５９．０２６．１５４．１


非自然语言推理词７９．５５５
．８２６
．
１５３
．８


从中文
、英文数据集的实验结果可以看出
，非自然语言形式的推理词较为稳
？


定
，模型性能较好。具体地
，对于形式简单、数据区分度高的任务
，如ＥＰＲＳＴＭＴ


和
Ｔｒｅｅ等任务
，
自然语言形式的推理词表现较为出众
。而对于类别数较多
、复


杂的任务
，
例如ＴＮＥＷＳ、
ＩＦＬＹＴＥＫ和ＣＳＬＤＣＰ等任务
，
非自然语言形式的推


理词具备更好的性能
，这是由于它可以从具体任务中自主学习到更适合当前模版


的推理词形式
，而不受自然语言形式的限制
。也就是说
，非自然语言形式的推理


词可以从众多的上下文信息中学习到推理词的连续化表达形式
，从而有效避免了


单
一推理词的影响
。


（２）提示模版性能分析


在基于提示学习的方法中
，手工设计的提示模版通常会对模型的效果产生
一


定的波动
，本小节评估手工设计的模版对最终模型性能产生的影响
。实验结果如


表
３
－
１５所示。


表
３
－１５提示模版对模型准确率的影响


ＰｒｏｍｐｔＡｃｃｕｒａｃｙ


ＴＮＥＷＳＣｌａｓｓｉｆ
ｉｃａｔｉｏｎ


下面＜ＭＡＳＫｘｄｅｓｃ＞新闻
：
＜ｔｅｘｔ？＞
。
５４
．２


ｉｔ＜ＭＡＳＫｘｄｅｓｃ＞新闻报道
：
＜ｔｅｘｔ＞。
５３
．５


＜ｔｅｘｔ＞
，＜ｄｅｓＯ新闻
？＜ＭＡＳＫ＞？
５４
．６


＜ｔｅｘｔ＞
，
ｉｌ＜ＭＡＳＫｘｄｅｓＯ新闻
。
５＾
９


Ｔｒｅｅ
ＱｕｅｓｔｉｏｎＣｌａｓｓｉｆ
ｉｃａｔｉｏｎ
（Ｋ＝８）


ＴＴｉｉｓ＜ＭＡＳＫ＞ｔｈｅ＜ｄｅｓｃ＞
ｑｕｅｓｔｉｏｎ：＜ｔｅｘｔ＞
．
５０．０


＜ｔｅｘｔ＞
，
ｉｔ＜ＭＡＳＫ＞ａｂｏｕｔ＜ｄｅｓ〇
ｑｕｅｓｔｉｏｎ．
５６
．４


＜ｔｅｘｔ＞
．ｉｔ＜ＭＡＳＫ＞＜ｄｅｓｃ＞
ｑｕｅｓｔｉｏｎ．
５５
．８


４１


北京邮电大学工程硕士学位论文


从实验结果可以看出
，不管是中文还是对于英文数据集
，模型的性能都会受


到提示模版较大的影响
。
具体地
，
在中文
ＴＮＥＷＳ和英文
Ｔｒｅｅ任务上
，
本小节


对模版采用了前缀式与后缀式的形式进行评测
。相比之下
，在中文数据集上
，模


型的性能差异性相对较小
，
最大与最小的差值为
１
．
１％
。
而在英文数据上
，
模型


的性能却表现出较大的差异性
，
最大与最小的差值为
６
．４％
。
表明提示模版对模


型的准确率性能的影响
，与具体的下游任务有较大的关系
。通过优化模版的形式
，


可以较大程度提升模型的性能
。


３
．３
．８负采样性能分析


在本小节中
，本文尝试对中文ＴＮＥＷＳ数据集
，通过控制不同的负采样数据


比例
，
分析负采样对模型性能的影响
。


０
．５４５
－／＼


０
．
５４０
－厂
、＊
／＼


０
．５３５
－
／


雲
０
．
５３。
．


０
．
５２５
－／＼／


０
．５２０
－／


０
．
５
１５
－ｊ


１
Ｉ
Ｉ
Ｉ
Ｉ
Ｉ
Ｉ


２４６８１０１２１４


负采样数量


图
３
－６
中文ＴＮＥＷＳ任务准确率与负采样数量关系图


如图
３
－６所示
，
随着负采样的不断变大
，
模型的整体性能
（准确率）也在逐


渐提升
。
通过实验发现
，
在负采样在等于
２、
４和
８的时候
，
模型的性能有
一个


迅速的提升
。其中
，
当负采样在
８
？
１３之间的性能相对稳定
。当负采样达到
１４时


（ＴＮＥＷＳ任务包含
１５个类别
）
，
模型的准确率有
一个急速下降的趋势
，表明使


用全部类别作为负例
，
并不会提升模型的性能
。


３．３．９可视化分析


为了评估在引入度量优化模块后
，本文所提出算法获得任务类别表征表示的


有效性
，本小节将通过
ｔ
－ＳＮＥ
［５６
］对中文ＴＮＥＷＳ数据集通过随机采样进行了可视


化分析
，
结果如图
３
－７所示
。


４２


苐三章基于提示学习的少样本文本分类算法研究


本文在度量优化模块中采用三元组损失优化类别间的距离
。具体地
，本文将


预训练模型中
ｐｏｏｌｅｒ层的输出通过度量优化模块进行度量学习
，
图
３
－７（ａ
）展示了


实例经该模块编码输出后的向量分布情况
。从中可以看出
，
同
一类别的实例间都


较为紧凑
，
同时不同类别间也存在着较为明显的间隔距离
，
说明了模型在ｐｏｏｌｅｒ


层中己经学习到了非常好的类别表示
。


，
５
？
樣乐


？籲
汽￥


ｍ＊％＊
房产


蒺＊＃後
文化


，０
＊
？
＊
？＊
／
＊．－？
故事
？．
＊


１發
备
畚＃
２０
－
擊


？
珍
韌爹？
电竞
？《？


＊？
．？


？

＇？？
？
齡
？
：
？？
？
？
／＊
？＊
＊＊
＊＊


參
费
》
科技
＊春
聲＃？？


＊？＂％＊
？
农並？
承摩？＊


５
＇？
？
＇Ｉ＾？？
＊达游游
？


？■
签
？
？
＊＊＊ｒｅｓｕ＞
．
？？


？？？％？
军搴
？？？？窃


．，
？
？
？
？？
？
？
？
＊
？
？
？
：
？
？
？


？？
＃錄
务？？？？，


〇■
？
擊
？
雜
：擊
、
资１


？－？？
＊４
＾
＊＊
＊＊


ｉｊ
％？？？
？
？
？？
？
？
＊，
？
：？
？
？＊．？
＊


．
？？？
？
？
？
？
＊
，
＊
？
＊
？
？？
？＊
？＊
？？？
？
？
？
－
＂


．
？？
＊？？＜
＞
？？？＊


？
？
《＊
？
？
？
？
？
？
：
＊
＊
＊
？
？
？
．：２？


＊
？
沙
？？？
？？＊
＊？
＊．＾
＊够
文化


／
＂
？：－〇
？，
？
兔
？？％
令：
ＩＩ


？？
參
？像？？鲁
故系


－
，０
．，？
？？，？
？
＊
：
？？？
？
？
？
＾
ＩＩ


？＊
．
？
？
？
？
？
？
？
？？
？
？
？
？
？
？ｆ？？
…


？
？：？
？
．
？
？？
？？
＾
：
：２


％
？
２０
＇？
？？
＊？
締


－
？？？
＊
＊？．ＳＩ


１０
－
５０５
＞０
１５
－
１５
－
３０
－５０
５
１０


（ａ）ｐｏｏ
ｌｅ
ｒ层向Ｓｔ
－
ＳＭＥ分布表示
Ｑ＞）０＿Ｓ向
Ｓｉｔ
ＳＮＥ分右表示


图
３
－７
实例向量
ｒ
－ＳＮＥ分布可视化


更进
一步
，
为了验证模型的编码层是否有效学习到任务中的类别表征表示
，


本文将预训练模型中编码层
ＣＬＳ位的输出作为当前整个实例的向量化表示
，
如


图
３
－７（ｂ
）展示了编码后的分布情况
。从图中可以看出
，
实例类别的分布依然保持


着与
ｐｏｏｌｅｒ层相似的分布情况
。对于简单的新闻类别
，诸如股票
、娱乐
、
电竞和


汽车等依旧保持着较为紧凑的聚集现象
。而对于文化和故事等较为抽象或是涵盖


范围较广的新闻类别
，虽分布别较为分散
，但也有着
一定的区域性
。表明ＣＬＳ作


为整个句子的编码表示
，
己经学习到了
一定的实例类别表征信息
。


通过对实例类别的分布可视化分析
，表明度量优化模块可以为整个模型提供


更多额外的类别知识等信息
。


３
．４本章小结


本章详细介绍了本文提出的基于提示学习和三元组损失的少样本文本分类


算法
。首先阐述本文提出该算法的动机
，接着介绍了模型的整体网络结构
，并详


４３


北京邮电大学工程硕士学位论文


细介绍了基于自然语言推理的提示学习模块和度量优化模块
。其中
，基于自然语


言推理的提示学习模块将下游文本分类任务通过添加提示模版转化为掩码式的


完型填空形式
，使模型能够更好的利用预训练模型中的先验知识
，并通过两种不


同粒度的损失进行优化
。同时
，度量优化模块通过三元组损失优化
，实现捕获下


游具体的任务类别信息
。此外
，介绍了将基于自然语言推理形式的提示学习融入


到继续预训练任务中的方法。接着详细介绍了模型的训练与推理过程。最后通过


在中文、英文数据集上进行模型性能的评测
，
以及对模型的各个组件进行有效性


分析等实验
，
验证了本文提出模型的有效性。


４４


第四章基于少样本学习的文本分类智能标注工具


第四章基于少样本学习的文本分类智能标注工具


目前大多数的标注工具都是基于人工进行的
，
同时为了提高标注的准确性
，


通常每条数据都需要多人进行标注
，最后通过投票或者复审的方式对结果标注进


行确认
，而这
一过程将消耗大量的人力和物力
。当前的少样本学习已经有了
一定


的发展
，但是在自然语言处理领域才刚刚起步
，更多地停留在在科学研宄中
，较


少的在实际工业生产生活中被使用
。本文对此进行需求分析，将本文提出的基于


提示学习和三元组损失的少样本文本分类算法与文本标注工具相结合
，利用少量


样本学习帮助标注人员实现快速的智能标注。同时引入主动学习策略
，既可以通


过模型的迭代实现持续提升模型效果
，又可以帮助标注人员和工程师识别
一些较


难的样本
，协助进行
一系列的分析。为了满足不同的需求
，该智能标注工具不仅


包括了少样本学习的智能标注模块
，还包括传统的人工标注模块
，标注人员可以


通过简单的操作
，快速的进行人工标注或者人工校验等工作
。此外
，该工具支持


多种格式的文件导入和导出
，满足用户对数据格式的多种需求。本章内容主要包


括对智能标注工具的需求分析、系统概要设计、系统详细设计以及系统的实现与


测试。


４．１需求分析


本系统的目标是设计和实现
一个基于少样本学习的文本分类任务智能标注


工具
。该系统既可以通过人工形式进行传统方式的人工数据标注
，也可以基于少


样本学习实现高效的数据智能标注。同时引入主动学习的方式
，实现持续迭代提


升模型
，进而提升数据标注质量。为此
，系统应包括用户管理模块、标注任务管


理模块、数据管理模块、标签管理模块、
智能标注模块以及数据统计模块等
。


４．１．１系统功能性霑求


（
１
）用户管理模块


用户管理的主要功能保证用户只有完成登录后
，
才能正常使用本标注工具
，


确保工具使用者的信息核数据的安全性。如图
４
－
１所示
，
具体功能需求为
：


１
）登陆功能
：
所有用户都需要通过登陆功能来进入系统。


２）注册功能
：
所有用户需要完成注册才能使用本工具
。


３）个人信息管理功能
：
用户可以对个人信息进行管理
，
包括查看和修改个


人信息等
，
如
：
邮箱、登录密码等
。


４５


北京邮电大学工程硕士学位论文


八＼
个人信＾）


用户＾
＜
ｉｎｄｕｄｅ＞


？＜
ｉｎｃ
ｉｕｄｅ＞


个人


图
４
－
１
用户管理模块用例图


（２
）标注任务管理模块


标注任务管理是方便用户对个人的数据标注任务进行管理
。主要包括任务创


建
、任务修改、
任务详情和任务删除等功能
。
如图
４
－２所示
：


用户＼
、—
．一
＂


图
４
－２标注任务管理模块用例图


（３
）数据管理模块


数据管理模块主要提供对标注任务数据的导入
、导出和管理功能
，其中包括


人工数据标注功能
。
如图
４
－３所示
：


１
）数据导入
：用户可以导入多种格式的文件
，用于人工标注或者智能标注
。


支持用户导入
ｔｘｔ、
ｃｓｖ和
ｊｓｏｎｌ等文件格式
。


２）数据导出
：
用户可以导出标注完成的数据
。
当用户选择部分数据作为智


能标注模型的训练和评测数据后
，用户也可单独导出该部分数据文件
。支持用户


导出
ｃｓｖ和
ｊｓｏｎｌ等文件格式
。


４６


第四章
基于少样本学习的文本分类智能标注工具


３
）数据管理
：
用户可以对当前任务的全部数据
、
已标注数据和未标注数据


进行查看
、查询和删除等操作
。此外
，为方便用户查看
，
还可以灵活定义每页显


示数据量
。


４）人工标注
：
支持用户对当前任务进行人工的数据标注操作
。
同时支持通


过快捷键的方式进行标注
，
可以极大提高标注效率和审查效率
。


＜ｉｎｃＪｕｄｅ＞


．＜ｉｎｃｌｕｄｅ＞
－
－


／＜ｉｎｃｌｕｄｅ＞


／腿蒯除）


。
—
－＜
ｉｎｃ
ｌｕｄｅ＞
－
－


周户＼＾

＜
丨ｎｃｌｕｄｅ＞
一＾’


＼导入


＼一
—
＜
ｉｎｃｉｕｄｅ＞
、
一——＾


＜
ｉｎｃｌｕｄｅ＞一＾


￣
＞
ｉ４
（＾
ＡＪｓｏｎ
ｌ＾＾）


图
４
－３数据管理模块用例图


（４）标签管理模块


标签管理模块的主要功能是帮助用户方便的进行管理当前任务数据的标签
。


如图
４
－４所示
：


１
）标签定义管理
：
主要包括数据标签创建
、
查看
、
修改以及删除功能
。


２）快捷键管理
：
为方便人工数据标注
，
用户可以为标签进行快捷键添加
、


查看、
修改和删除功能
。


３
）标识色管理
：
用户可以为标签添加自定义颜色
，
方便用户可以快速的识


别标签
。


４７


北京邮电大学Ｔ程硕士学位论文


／签创


／
＜
ｉｎｃ
ｌｕｄｅ＞产


／
／
＊乂标签查看）


〇／，
’
．
，＜
ｉｎｃ
ｌｕｄｅ＞＾乂


＾
标签管理
）
：
：：
？
？＾
＾


＼
＇
＜
ｉｎｃ
ｉｕｄｅ＞＼


人
＼

、
、＜
标签修改
）


／＼＼
＜
ｉｎｃ
ｉｕｄｅ＞＾


用户＼
’
、
＇、Ｘ


＼＾
标签删除ｊ


图
４
－４标签模块用例图


（５）
智能标注模块


智能标注模块主要功能是通过用户提供少量的标注数据
，就可以创建
一个智


能标注任务
，并且可以进行模型评测和对需要标注数据的进行智能标注
，达到自


动化的数据标注的目的
。同时用户可以根据智能标注结果的反馈
，对训练数据的


进行调整
，
提升模型的能力
，
进而提升标注效果
。
如图
４
－５所示
：


１
）模型数据管理
：
主要用于管理智能标注模型的训练数据和评测数据
，
以


及模型标注结果的导出
。
用户可以选定已标注的数据加入到训练集和评测集中
，


进行模型训练和性能的提升
。


２）智能标注任务管理
：
提供对基于少样本学习模型的智能标注任务的管理


功能
，包含了对任务的配置
、修改
、删除
、查看等功能
，以及包括对模型的训练
、


评测和自动化标注功能
。此外
，还包括对模型训练
、评测的日志
、
以及评测结果


在线查看等功能
。


３
）智能标注结果管理
：
主要用于对智能标注结果的管理
。
用户可以对智能


标注结果进行人工的校准和标注结果的确认
。以及提供基于信息熵的主动学习方


式
，
实现为智能标注任务提供更有利模型优化的数据
，
提升数据标注效果
。


４８


第四章
基于少样本学习的文本分类智能标注工具


＜
ｉｎｃ
ｉｕｄｅ＞


／结果


／


／
＜
ｉｎｄｕｄｅ＞、ｘ．


Ｑ／，－＾
．ｙｉｉｎａｕａｅ＞
－＞Ｃ＾
ｍｍｍ
Ｊ）


ｔ
＾
（
－ＵＭｍｓ＾ｓｍｍｙｒ
．
：＾＝＝＝＝＝Ｃ


人＼＾—


／＼＼
＜ｉｎｄｕｄｅ＞


用
丨户＼
＜
ｉｎｄｕｄｅ＞
？＾


＼
、
＇
＊＊
（＾輕磁
Ｊ


＼
＜ｍｃ
ｉｕｄｅ＞＾乂


雜果￥＞妥一


〈
ｉｎｃｌｕｄｅ〉
、－＇＾


图
４
－５
智能标注用例图


（６）数据统计模块


数据统计模块的主要功能是为用户提供当前标注任务的统计信息
，包含数据


标注情况统计
、数据的标签分布统计
、智能标注任务的评测指标统计以及数据统


计等
。
如图
４
－６所示
：


标注


用户＼
指獅


＼＾＾
ＥＥｖＴ＾
）


图
４
－６数据统计糢块用例图


４９


北京邮电大学工程硕士学位论文


４Ｘ２系统非功能性需求


（
１
）
易用性需求


本系统应以提高用户效率、方便用户使用为设计目的
，保证用户可以获得最


佳的体验效果
。同时
，应保证系统的各模块内部的操作过程应符合大众使用习惯
，


并保证界面设计的简单和便捷性
。


同时作为
一款简单易用的标注工具
，应保证本系统可方便用户快速部署和使


用
。


（２
）可扩展性需求


本文设计系统的可扩展性需求主要体现在
：在智能标注模块中
，未来可灵活


的接入更多模型用于快速进行智能标注
。其次
，
随着标注数据量的上升
，
系统可


扩展支持处理
一定规模数据的能力
。


（３
）兼容性需求


本系统的前端需适配
ＩＥ
、
Ｆｉｒｅｆｏｘ和Ｃｈｒｏｍｅ等各种主流浏览器内核
。在各种


主流屏幕尺寸和不同分辨率下都能正常显示
，并且前端元素保持
一致
，不会出现


错位
、
丢失等现象
。


（４
）
安全性需求


本系统在安全性方面应该满足以下几个方面
：
用户密码采用带有
ＳＨＡ２５６


哈希的
ＰＢＫＤＦ２算法加密方式进行存储
，
保证用户信息的安全性
。
当用户对数


据进行各种访问
、修改以及删除操作时
，都应该核对用户的身份信息
、操作权限


等进行验证
，
保证数据操作的安全性
。


此外
，本系统应具备对系统的访问
、操作等日志记录功能
，
可用于快速的发


现问题和定位问题
。


４．２系统概要设计


４．２
．１总体功能设计


本节根据４
．
１节对本系统进行的需求分析
，对智能标注工具的功能进行模块


化设计
。
如图
４
－７所示
，
该系统分为用户管理和标注任务管理两大部分
；
其中
，


标注任务管理又细分为数据管理、标签管理
、智能标注和标注统计等四个大的模


块
；
每个模块又细分为若干更小的模块
。


５０


第四章基于少样本学习的文本分类智能标注工具


基于少样本学习的自动标注工＾




＞
ｒ


用户營理
标注Ｍ管理



＞

［
＾
ｒＸ
］
［



｜雜顏
［
丨
磁難
￣￣
｜
｜ｗｍｍ
｜
丨
膨充计
—


＇
■
＞
■ｊ￡
．
￣
ＪｌＪｌＪｌ１ＪｌＪｌ．
￣￣
！
＞ｌＪｌ
＿￡
．
￣
ｉ
￣
１


ｓ智
自


用Ｓ人标
＃Ｋ模
能
标数
标
动


用ｍ户ｆ工
数签型
标
注据
签
标


户户信￡数
据定２Ｓ数
注
结标
分
注


登注息泛
据
查义２
２据
任
梁注
布
评


录ｍ管空
标
看管
葚５管
努
管统
统
测１


理ｉ注理
ｉＳＳ理
管
理计
计
统


」ＬＩＵ
Ｈ
」
ＬＪＬＩＬＩ」
＆
ＬＪ」
１＿
计


图
４
－７
系统功能图


４．３系统详细设计


４．３
．１人工数据标注模块设计


人工数据标注模块是数据标注工具最基本的模块
。用户完成需要标注数据的


导入工作后
，并为数据添加需要分类的标签
，
以及标签的自定义快捷键
。便可实


现快速的为每条数据进行人工标注
。
具体流程如图
４
－８所示
：


开始
—



＞
ｊ
ｒ


ｔ縮
上传



＞
Ｊｒ


添加标签


为标签添加



＞
ｊｒ


ＡＸ标注


ｙ


飞束
｜


图
４
－８
人工标注流程图


５
１


北京邮电大学工程硕士学位论文


４．３．２智能标注模块设计


智能标注模块是该系统的核心模块
。用户利用已标注或者导入少量的训练数


据和评测数据
，通过配置少样本学习模型
，便可实现对需要标注数据的智能标注
。


智能标注流程如图
４
－９所示
，该模块使用的模型是第三章设计提出的基于提示学


习的少样本文本分类算法模型
。


１
开始


ｉ


选倒
丨
丨练和评测魏





：Ｘ
ｊ智雛注
：


；
１ｍｍｍｍ
：


：ＹＴ
！


；
丨
＾训练
］
１ｍｍｉｍ１
；


ｊｄＳＨＯ／／智齡
碑
搜果ｙ
；


结束


图
４
－９智能标ｖ主流程图


４．３
．３主动学习模块设计


主动学习模块是用户可以根据智能标注结果的反馈
，
进行人工的数据修正
，


同时也可以选择加入到训练数据中
，
实现迭代式的提升模型效果和标注的质量
。


其中
，模型会给每
一条数据计算
一个推理结果反馈分
，称为不确定度（Ｕｎｃｅｒｔａｉｎｔｙ）
。


该值含义如下
：
如果不确定度越高
，表明模型对当前数据越难以判断
，不同类别


间的打分越接近
；不确定度越低
，模型对当前数据的打分越置信
，不同类别间的


打分相差越大
。


，
，
…
Ｅｆ＝１Ｐ（〇
ｚ〇５Ｐ（
ｉ）…
、


Ｕｎｃｅｒｔａｉｎｔｙ
＝
＾
（４
－
１）


１０９
Ｎ


其中
，
Ｎ代表类别数
。


用户可以根据模型给出的不确定度信息
，从智能标注数据中选择对模型有利


的数据
，让模型重新进行训练
，实现迭代式的提升模型效果和标注的质量
。对于
，


不确定度高的数据样本
，表明当前数据属于模型没有学习到的难样本
，可以帮助


模型进
一步提升
。对于不确定度低的数据样本
，如果模型给出标注结果与真实不


一致
，说明模型对当前数据的学习存在过拟合现象
，对当前数据进行纠正并提供


给模型进行重新训练
，
可以进
一步帮助模型提升性能
。
如图
４
－
１０所示
：


５２


第四章
基于少样本学习的文本分类智能标注工具


＜


ｉ


離川练和评测觀


．ｒ


智〒注
１


ｉ


基于不确定度ＡＴ修
｜


正标避果


图
４
－
１０主动学习流程图


４．３
．４数据库设计


本系统的数据存储分为两部分
：
一个是用于存储用户信息
、标注数据等的传


统关系型数据库
ＳＱＬｉｔｅ
；另
一个是用来存储模型日志信息的文件存储方式
。该系


统的目标是为用户提供
一个简单
、易用的文本分类智能数据标注工具
，为实现用


户可以无感知的使用本标注工具
，
本系统釆用了无需配置的
ＳＱＬｉｔｅ轻量级数据


库用于存储用户数据和文本数据等信息
。


（Ｓ）
／
／＜＾
＞ｘｇ
％）


＿＾
ｉ—
７
￣
ｒ

！Ｍ
＝＊＝
＾＞
￣
：
驗


＜ｓｒ／＾＾７ｖ＿


图
４
－
１
丨
ＳＱＬｉｔｅ数据库Ｅ
－Ｒ图


如图４
－
１
１所示的是
ＳＱＬｉｔｅ关系型数据库Ｅ
－Ｒ图
。在本系统中
，
包含了多张


关系数据表
，
其中包括
：
用户表
（Ｕｓｅｒ表
）
、标注任务表
（Ｐｒｏｊｅｃｔ表）
、标签表


（
ｌａｂｅｌｓ表
）
、
数据表
（Ｄｏｃｕｍｅｎｔ表
）
、
智能标注任务表
（Ｔａｓｋ表）
、
数据标注表


（Ｄｏｃｕｍｅｎｔ
＿
ｌａｂｅｌ表）
、
智能标注表
（Ｔａｓｋ
＿ｄａｔａ表
）等
。


用户表
（Ｕｓｅｉ
？表）
：
用于存储用户个人数据
。


标注任务表
（Ｐｒｏｊｅｃｔ表
）
：
用于存储当前标注任务的信息
。


５３


北京邮电大学工程硕士学位论文


标签表
（
ｌａｂｅｌｓ表）
：
用于存储标注任务的标签信息
。


数据表
（Ｄｏｃｕｍｅｎｔ表）
：
用于存储标注任务的文本数据信息
。


智能标注任务表
（Ｔａｓｋ表）
：
用于存储智能标注任务的配置信息
、任务状态


信息等。


数据标注表
（Ｄｏｃｕｍｅｎｔｊａｂｅｌ表）
：
用于存储标注任务中数据的人工标注结


果等信息
。


智能标注表
（Ｔａｓｋ
＿ｄａｔａ表）：
用于存储智能标注任务对标注任务中数据的智


能标注结果等信息。


４．４系统实现


４．４．１系统实现环境


本系统采用
Ｂ／Ｓ架构方式
，
以及采用了目前流行的前后端分离技术进行实


现。其中
，
前端使用Ｖｕｅ的单页Ｗｅｂ应用框架技术
，
后端采用了支持ｐｙ
ｔｈｏｎ语


言的ＤｊａｎｇｏＷｅｂ框架
，同时在数据上也釆用ｐｙ
ｔｈｏｎ内置的轻量级数据库ＳＱＬｉｔｅ
。


此外
，
在智能标注模块中采用Ｐｙｔ
ｏｒ
ｃｈ深度学习框架
，
实现模型的训练和推理等


工作
。如表４
－１所示
。


表４
－１
系统实现环境


＾￡＃＾


操作系统
Ｗｉｎｄｏｗｓ１０Ｃｏｒ
ｅＩ５＋１６Ｇ



Ｐｙ
ｔｈｏｎ３
．６．２后端开发语言


开发环境Ｎｏｄｅ．ｊｓ１４
．１５
．０ＪａｖａＳｃｒｉｐｔ运行环境


Ｃｈｒｏｍｅ浏览器


Ｄｊａｎｇｏ３
．１
．３后台框架


系统框架
Ｖｕｅ２
．６
．１
１前端框架



ＳＱＬｉｔｅ数据库


深度学习框架
Ｐｙｔｏｒｃｈ１
．７．０深度学习框架


其他工具Ｇｉｔ／ｇＭｉｕｂ版本控制


４．４．２


用户完成登录后
，便可进入标注任务管理界面。在本界面下
，用户可以创建


新的标注任务
，也可以进入己创建的标注任务进项管理
，
以及删除不需要的标注


任务等操作
。如图本１２所示。


５４


第四章
基于少样本学习的文本分类智能标注工具


ｉ
丨


ｓｍ茗梦槎这
胃黎＿轻級
费激ｉｓ黎
珍？《


５楼避３讀
餅ｉ鮮衆
５ｅ？ｆＯ赖游
Ｊ减ｔ傅勒Ｓｆ
ｅｊｆ
ｃＭ？
３２？？ｔ
－ｅ＞
－
ｊ
５ｓＪ９
；６


￡文終突Ｍ＾ｙｍ＾ｅｆｉ＃？
＝ｆ
ｅｓ？
：ｃ
ｊｎ３域
！
１
？ＨＳ构？
：捃汐
游
Ｉ舞Ｋ３７顧２


图
４
－
１２
标注任务管理模块实现


４．４
．３数据管理模块实现


当用户进行己创建的标注任务后
，系统会默认进行数据管理模块
。通过数据


管理模块
，
用户可以完成数据的导入和导出
、数据详情查看
、
以及数据的人工标


注等工作
。


在数据的导入模块中
，
支持导入文本
ｔｘｔ、
ｃｓｖ和
ｊｓｏｎｌ格式的数据
；
同样在


数据的导出模块中
，
支持导出
ｃｓｖ和
ｊｓｏｎｌ格式的数据
。
其中
，
图
４
－
１３展示了数


据导入模块的实现
。


————＾？


＿＿？
°


５ａｂｂ


ｃ


＾Ｂｍ


造
：卿〇


＊＊＊
ｔ？Ｋ？
－ｕ
＞？？
ｉ


ｆｌ
？
海灣


图
４
－
１３数据导入模块实现


点击数据列表
，
便可进入数据查看模块中
，
如图
４
－
１４所示
。
在该模块中
，


用户可以对数据按照标注状态进行查看
；也同时支持通过关键词对数据进行搜索


查看等方式
；
同时对数据也可以进行删除操作
。


５５


北京邮电大学工程硕士学位论文


… 对
？和ｗ


Ｑ
：
：：
：
：


Ｕ
＞
ｊ
ｔ？
ＵＭ
：


ｉｔ？＾ｚ７ｔＨ
：ｊｒｓ
￣ｎ＾
－


？
＞２
－？＾
＊
ｓ
ｒ
＊
－
＞？
ｉ？ｒ
－
．ｃ
＊ｎ＾，？＾＊ｒｅ
－３
－＾


？ｉ６？ａＩ；＾ｆ？


＊
？ｍｘ＊￡ｔａｈ＾ｚ
－＇
．ｕｎｉ


ｍｎｘ＾＜ｉ－？＊ｃｉ
ｉ？２一议＞ｆｎ
＾ｕ
－


Ｈｉｍ


图
４
－
１４数据查看模块实现


当点击标注数据后
，
便进入到人工数据标注模块
，
如图
４
－
１５所示
。
为了提


升人工标注的效率
，用户可以通过预定义标签的快捷键方式
，实现仅通过键盘操


作
，就可以对每
一条数据进行快速的标注操作
。如图
，
用户可以通过鼠标点击或


是通过预定义的快捷键实现数据标注行为
。


ｂｂｈｓ＾ｂｂｂｂｈｈｈｈｈｈｈｈｈｈｈｈｂｈｉｈｈｈｈｈｈｈｈｂｂｈ＾ｓｈ


一一
。
一一
Ｑ
Ｅ＾３
ｅ＊？＊？



－
ｍ－ｍ－ｍ
－


跑致事
：
舌Ｌｔ钱途
，
盛纖玉


３０３
１５＾３３３４－２２７０＾３２？￡


图
４
－
１５
人工数据标注模块实现


４．４
．４标签管理模块实现


点击标签管理面板后
，
就进入到标签管理模块中
，
如图
４
－
１６所示
。
该模块


可以实现数据标签的创建
、修改
、删除和查看等功能
。为了在标注过程中
，
使每


一个标签更有辨识度
，
用户可以为每
一个标签添加自定义颜色
。
同时
，
为了提高


人工标注效率
，
用户可以为每
一个标签添加自定义快捷键
，
实现快速操作
。


５６


第四章
基于少样本学习的文本分类智能标注工具


．


ｉａｂｅＳ

ｆ
＇
ｊａｍｅ
Ｓｈｏｒｔｃｕｔ
Ｃ〇ｊ？



Ｖ


＾ｍ
．


ｗｓ
ｒａｓｍ
ｍ？ｆ


＇１３付
＜ｆ
ｅ
；
：
ａ８＾＾３＾＾３


－－１


＾断咖■ｓａｓｓ


？ｍ－＊ｉｒ
ａｒ
ａ


图
４
－
１６标签管理模块实现


４．４．５智能标注模块实现


在智能标注模块中
，用户可以对模型训练和评测数据进行管理、对智能标注


任务进行管理以及对模型标注的结果进行管理等操作
。


点击智能标注选项卡
，
便可进入到智能标注模块
。
如图
４
－
１７所示
，
在训练


数据管理模块中
，用户可以选择己有标签的数据加入到训练数据中
。
同理
，在评


测数据模块中
，用户也可以选择己有标签的数据加入到评测数据中
。其中
，
训练


数据集与评测数据的交集为空
。


？


■■■■■■■
￣


ｔ？
Ｊ
ｉｔ
ｌ？ｂ？
？
？


１Ｂ
．？
ｊ＊
ｒ±？
ｆ９


ｉ
？
－ｆ
＊
－？＾３？？
ｔｒｒｆ
ｅ￡Ｓ＾


３
＾ｏｊｔＪＢＡｉｇｆ
ｌ
ｉｓａ
？ａ


‘ｋｌＭＺ
Ｋ
！？３？ｘｒ


Ｈ
？Ｈｔ＆ｓ殘资ｓｅａｓ減链是棘爱券钱絲
职


—
，
…


图
４
－
１７训练数据管理模块实现


通过点击创建任务选项
，用户可以实现创建
一个智能标注任务
，如图
４
－
１８所


示
。
在该页面中
，
用户可以配置模型的参数
。
如图
４
－
１９所示
，
当用户完成智能


５７


北京邮电大学工程硕士学位论文


标注任务的创建后
，便可对智能标注任务进行管理
，包括模型的训练
、评测
、推


理
（自动标注）
以及参数
、
日志
、
和评测结果的查看等操作
。


０


— ＿
￣
＊


＇
通


麵链


Ｉ
ｌｌｉｉ
ｉｉＩｌＨ


蠢


Ｇ？
，ａ＊
＿
：


Ｉ


ｔ＼
—


＿霸


一《？？＜？
１


？


图
４
－
１８
创建智能标注任务模块实现


■■■■■■■■■■■■■■■■■


￣
一


ｉｒｌｉ＾
．
Ｉ
＇


－
＼
＊＊￣
１


圍漏丨
『
ｉ


■


■■■■■■Ｉ：


图
４
－
１９
智能标注任务管理模块实现


点击模型标注结果选项卡
，
可进入到模型标注结果管理模块
，
如图
４
－２０所


示
。在该模块中
，通过下拉菜单搜索已完成智能标注的任务
，可实现对模型标注


结果的管理
。其中
，对于模型标注结果与人工标注结果不
一致的情况
，
系统会为


模型标注结果添加黄色背景进行提示
；
如果模型标注结果与人工标注结果
一致
，


５８


第四章基于少样本学习的文本分类智能标注工具


则会为模型标注结果添加绿色背景
；如果数据没有人工标注
，则会为模型标注结


果添加蓝色背景
。


其次
，
模型会为每条数据给出预测结果标签的概率和不确定度的得分结果
。


用户可以根据模型预测结果的概率或者不确定度进行排序
，选择对模型有用的数


据加入到训练数据中
，
通过主动学习的方式去提升模型效果和标注质量
。


最后
，
用户可以通过编辑操作
，修改模型预测结果
，并将数据批量加入到已


标注数据中
，
实现对数据批量的确认
。


參


隨灘＊


－ｒ
ａａ


－議


交本
＞ｉ
＊＊Ｑｔ＾４？＾ｍ＾ｆｓｓ兹本ｗｔ


一


ｖｍｍｓｍ


Ｓ
丨
：＞
－ＨＶ
＊
Ｃ＊？
〇？？


，
、
，、邏＿


ａｒ＾ｘｔｖｉ
？ｉ
＊ａｓｒｏｎ％ＫＣ９


■


图
４
－２０模型标注结果管理模块实现


４．４．６数据统计模块实现


点击数据统计选项卡
，
可进入到数据统计模块中
，
如图
４
－２
１所示
。
在该模


块中
，对数据标注情况
、标签分布情况
、不同数据集的数据量以及任务的评测结


果进行了统计
。


标注统计
：
统计了当前数据的已标注数据量和未标注数据量占比情况
。


标签分布统计
：
统计了不同标签下数据量分布情况
。


数据统计
：
统计了数据在训练、
评测和标注数据中的分布情况
。


任务评测结果统计
：统计了所有完成训练和评测的智能标注任务的评测结果


情况
。


５９


北京邮电大学工程硕士学位论文


０
．


核嫌ｉｔ
樣


Ｈｉ丨．ｆ％ｉｍ


■■■


Ｂａａ
＂
：
？Ｓ？？
ｉｔ
ｔｎ？？？ａ＊


｜ＨＢ


ｉ？＿


？…
ｍＢ
ｊＢＢｍ



ｔ
：
３？？
雄
图
４
－２
１
数据统计模块实现


４
．５系统测试


４
．５．１测试环境


为了保证本系统可以正常运行使用
，本节将对系统进行详细的测试
。测试分


类功能性测试和非功能性测试
，所有测试通过后
，才能确保系统的稳定运行
。如


表
４
－２所示
，
展示了具体的测试环境说明
。


表
４
－２测试环境说明


环境
｜名称具体信息


操作系统Ｕｂｕｎｔｕ１６
．０４ＬＴＳ


处理器Ｉｎｔｅｌ（Ｒ）Ｘｅｏｎ（Ｒ）ＣＰＵＥ５
－２６７８ｖ３＠２
．５０ＧＨｚ


远程服务端

—




内存４７ＧＢ


硬盘２
．０ＴＢ


操作系统Ｗｉｎｄｏｗｓ１０
系统


处理器Ｉｎｔｅｌ（Ｒ）Ｃｏｒｅ
（ＴＭ）
ｉ５
－
１０３５Ｇ
ｌＣＰＵ１
．００ＧＨｚ１
．
１９ＧＨｚ


本地服务端


内存１６ＧＢ


硬盘５００ＧＢ


操作系统Ｗ
ｉｎｄｏｗｓ１０系统


处理器ＩｎｔｅＩ（Ｒ）Ｃｏｒｅ（ＴＭ）
ｉ５
－１０３５Ｇ
ｌＣＰＵ１
．００ＧＨｚ１
．
１９ＧＨｚ


客户端内存１６ＧＢ


硬盘５００ＧＢ


浏览器Ｍ
ｉｃｒｏｓｏｆ
ｔＥｄｇｅ９８
．０
．
１
１０８
．４３
；Ｆｉｒｅｆｏｘ９４
．０
．２


６０


第四章基于少样本学习的文本分类智能标注工具


４．５．２功能糊试


在功能测试中
，
本小节主要针对４．１
．１节中功能性需求进行测试。具体地，


本文对每
一个功能模块进行了测试。在表４
－３中
，展示了各个功能模块的测试结


果
，
具体如下
：


表４
￣３功能性测试结果



功ｆ
ｔ模块
｜功ｆ
ｅ
丨
ａｎ试結果
｜具休信息


．
ｗ登录通过登录功能正常


用尸管理



ａ＃通过用户名
、
邮箱
、
密码校验有效



．
．ｔｘｔ、
ｃｓｖ和ｊｓｏｎｌ格式文件数据上传正


数据上传通过＾


＾


数据下载通过ｃｓｖ和ｊｓｏｎ丨格式文件数据下载正常


数据管理Ｉｉ数据分页查看功能正常
；
关键字搜索结


数据查看通过＾＿


果正常且显示正常


＾ｔ１通过鼠标点击
、
快捷键进行人工数据标


人工数据标注通过＆


注正常


标签定义通过标签创建、
修改、
删除功能正常


标签管理快捷键功能通过快捷键定义有效


标签标识功能通过标签颜色手动、
随机切换正常


模型数据管理通过
職据和评测数据的添加
、
删除功能






Ａｗ＾智能标注任务创建、
刪除和查看功能正


知故拉
－
－智能任务管理通过杏


智能标；王常


智能标注结果查看功能正常
；
结果排序


智能标注结果管理通过正常
；
关键字搜索功能正常
；
标签修正



功能正常


数据标注统计通过
数据标注情况统计结果显示正常



标签分布统计
通过标签分布情况统计结果显示正常


标注统计



智能标注评測统计通过智能标注评測统计结果显示正常



数据统计通过
数据在不同集合中的统计结果显示正常


４．５．３非功能糊试


在非功能测试中
，
本小节主要针对４．１
．２节中非功能性需求中提出的易用性


和兼容性需求进行测试。其中
，易用性中主要测试本系统的在不同环境下部署便


捷性
；兼容性主要测试不同的浏览器是否可以有效的支持本系统前端界面的正常


显示
。


在表
４
￣４
中
，
展示了后端部署易用性测试结果
。本系统后端釆用了成熟的


Ｄｊａｎｇｏ框架
，使得不管是在本地部署还是远程部署都会相对简单、容易
。同时
，


６１


北京邮电大学工程硕士学位论文


数据库采用了ｐｙｔｈｏｎ内置且无需安装的ＳＱＬｉｔｅ数据库
，使用户在部署时无需进


行额外的操作
。


表
４
＊４
后端部署易用性測试


部署方式名称是否部署方便


本地部署后端部署是


远程部署后端部署Ａ


在表４
－５中
，展示了不同浏览器对前端显示的兼容性测试结果
。具体描述如


下
：


表４
－５浏览器兼容性測试


篁器名称是否兼容


Ｍｉｃｒｏｓｏｆ
ｔＥｄｇｅ前端显示
是


Ｆｉｒｅｆｏｘ前端显示是


大于
ＩＥ９．０的
１Ｅ浏览器前端显示Ａ


４．６本章小结


本章详细介绍了基于少样本学习的文本分类智能标注工具的设计与实现工


作
。首先分析了系统的功能与非功能性需求
；
然后对系统的概要设计进行介绍
。


接下来具体介绍智能标注系统中各个核心模块的详细设计与实现过程
。最后通过


功能性测试与非功能性测试
，
确保了本系统的正常运行
。


６２


第五章
总结与展望


第五章总结与展望


５．１工作总结


少样本学习旨在利用已经获得的先验知识
，从少量的训练数据中进行快速学


习并使模型具有较好的泛化性能
。据此
，本文开展了基于提示学习的少样本文本


分类算法研究与应用工作
。提示学习方法通过任务形式转化的方式
，
即利用了预


训练语言模型中的先验知识
，又实现了两阶段训练形式的统
一
。
因此
，文本以自


然语言文本分类任务问题为研究背景
，通过对现有利用提示学习解决少样本学习


问题的方法进行深入的研究和改进
，
具体的研究成果和创新点如下
：


（
１
）基于提示学习和三元组损失的少样本文本分类算法
。基于自然语言推


理的提示学习方法通过将
一个多分类问题转化成Ｎ个二分类问题
，
实现了在数


据层面上隐式的扩增训练语料
，
从而缓减了过拟合问题发生的风险
。此外
，该方


法在保持了两阶段训练形式的统
一的同时
，
同时没有引入额外的参数
，避免了对


大量训练语料的依赖
。
并且
，
在该方法的基础上
，
本文通过设计
ＳｅｎｔｅｎｃｅＬｅｖｅｌ


和
Ｓｅｎｔｅｎｃｅ
－ＧｒｏｕｐＬｅｖｅｌ两种不同粒度的损失
，
实现了更有效的利用了有限的训


练数据样本
，使得模型可以提取更丰富的特征信息
。其次
，本文在基于提示学习


的方法中引入三元组损失
。当前大多数基于提示学习方法在文本分类任务上依然


存在识别率低等问题
。这主要是因为仅依靠预训练语言模型中的先验知识是远远


不够的
，忽视了文本分类任务中隐式的类别特征信息
。考虑到少样本学习特殊的


场景下
，应避免引入额外需要学习优化的模型参数
。为了解决这
一问题
，文本通


过引入三元组损失进行优化
。实现通过度量学习的方式
，让模型学习到下游任务


中类别表征
，
同时可以进
一步提升模型的泛化性能
。最后
，通过实验验证了上述


方法可以有效提升了模型的准确率
。


（２）
设计并实现了基于少样本学习的文本分类智能标注工具
。本文将基于


提示学习的少样本文本分类算法与文本分类标注工具相结合
，以及通过引入主动


学习方法
，
让少样本学习发挥出更大的价值
。本工具含有数据管理模块
、标签管


理模块
、智能标注模块和标注统计模块等４大核心模型
，其中每
一个模块又分为


若干小的模块
，共同构成了文本分类智能标注工具
。本工具在保留传统人工标注


形式的基础上
，通过引入本文设计的少样本学习算法模型实现智能标注
，可以节


省大量的人力和物力
。
同时
，
引入对智能标注结果的不确定度计算方法
，
帮助标


注人员更快的找到困难数据或是存在歧义的数据
，实现通过主动学习的方式
，提


升模型性能和数据的标注质量
。


６３


北京邮电大学工程碩士学位论文


５．２工作展望


本文以少样本文本分类任务为研究对象，提出了基于提示学习的少样本文本


分类算法
，并通过实验验证了本文提出的模型具有较好的性能
。虽然本模型
一定


程度上提高了少样本场景下文本分类的准确率
，但依然存在很大的改进和提升空


间
，
具体如下
：


（
１
）更高效的提示学习形式。大量实验证明
，
不管使用人工定义的提示学


习形式
，还是自动化的提示学习形式
，将下游任务转化成预训练任务形式
，通过


激活预训练过程中学习到的通用先验知识，可以在少样本学习
，甚至是在零样本


学习问题上取得比基于微调方法更好的模型性能
。但是
，当前的提示学习形式面


对大多数的下游任务
，依然需要依然大量的数据才能具有更好的性能。因此
，
需


要深入理解提示学习的内在运行原理
，
设计出更灵活、更高效的提示学习形式
，


才能在少样本学习问题上
，
发挥出更好的性能。


（２）有效利用大规模无标注数据进
一步提升模型性能
。在少样本学习场景


中
，有标注的数据是往往是有限的
，但通常情况下较容易挖掘出大量的无标注数


据
。当前大多数的自动引入方法是通过软标签的方法将无标注数据引入到少样本


学习中
，但是当模型的初始化学习效果不尽如人意时
，后续过程再通过软标签引


入数据会造成模型性能的严重下降
。因此
，如何更有效的利用下游任务场景中大


规模的无标注数据
，实现模型可以从少量的样本实例中快速学习到对任务有用的


信息提升模型性能也是研宄重点之
一
。


６４


参考文献


＃＃文献


［
１
］ＭｉｎａｅｅＳ
，
ＫａｌｃｈｂｒｅｎｎｅｒＮ
，Ｃａｍｂｒ
ｉａＥ
，
ｅｔａＬＤｅｅｐ
ｌｅａｒｎｉｎｇ
—ｂａｓｅｄｔｅｘｔ


ｃｌａｓｓｉｆｉｃａｔｉｏｎ：ａｃｏｍｐｒｅｈｅｎｓｉｖｅｒｅｖｉｅｗ［Ｊ
］
．ＡＣＭＣｏｍｐｕｔｉｎｇＳｕｒｖｅｙｓ（ＣＳＵＲ），


２０２１
，５４
（３）
：１
－４０．


［２
］Ｆｅ
－ＦｅｉＬ
．ＡＢａｙｅｓｉａｎａｐｐｒｏａｃｈｔｏｕｎｓｕｐｅｒｖｉｓｅｄｏｎｅ
－ｓｈｏｔｌｅａｒｎｉｎｇｏｆｏｂｊｅｃｔ


ｃａｔｅｇｏｒ
ｉｅｓ［Ａ
］／／ｐｒｏｃｅｅｄｉｎｇｓｎｉｎｔｈＩＥＥＥｉｎｔｅｒ
ｎａｔｉｏｎａｌｃｏｎｆｅｒｅｎｃｅｏｎｃｏｍｐｕｔｅｒ


ｖｉｓｉｏｎ［Ｃ
］，ｆ
ｉＥＥ
，２００３
：
１１３４
－１１４１
．


［３
］Ｆｅｉ
－ＦｅｉＬ
？ＦｅｒｇｕｓＲ，ＰｅｒｏｎａＰ．Ｏｎｅ
－ｓｈｏｔｌｅａｒｎｉｎｇｏｆｏｂｊｅｃｔｃａｔｅｇｏｒ
ｉｅｓ
［Ｊ］
．ＩＥＥＥ


ｔｒａｎｓａｃｔｉｏｎｓｏｎ
ｐａｔｅｒｎａｎａｌｙｓｉｓａｎｄｍａｃｈｉｎｅｉｎｔｅｌｌｉｇｅｎｃｅ
，２００６
，２８（４）
：５９４
－６１
１
．


［４
］
刘鑫，周凯锐，何玉琳，景丽萍，于剑
．基于度量的小样本分类方法研究综述［Ｊ］
．模


式识别与人工智能，２０２１
，３４（１０）
．


［５］赵凯琳，靳小龙，王元卓
．小样本学习研究综述［Ｊ］
．软件学报
，２０２１
，３２（０２）
．


［６］ＫｏｃｈＧ．ＳｉａｍｅｓｅＮｅｕｒａｌＮｅｔｗｏｒｋｓｆｏｒＯｎｅ
－ＳｈｏｔＩｍａｇｅＲｅｃｏｇｎｉｔｉｏｎ
［Ｄ
］
．


ＵｎｉｖｅｒｓｉｔｙｏｆＴｏｒｏｎｔｏ
，２０１５
．


［７
］＾ｎｙａｌｓＯ
，ＢｌｕｎｄｅｌｌＣ
，ＬｉｌｌｉｃｒａｐＴ
？ｅｔａｌ
．Ｍａｔｃｈｉｎｇｎｅｔｗｏｒｋｓｆｏｒｏｎｅｓｈｏｔｌｅａｍｉｎｇ［Ｊ］
．


Ａｄｖａｎｃｅｓｉｎｎｅｕｒａｌｉｎｆｏｒｍａｔｉｏｎ
ｐｒｏｃｅｓｓｉｎｇｓｙｓｔｅｍｓ
，２０１６
，２９．


［８
］ＳｎｅｌｌＪ
，ＳｗｅｒｓｋｙＫ
，ＺｅｍｅｌＲ，Ｐｒｏｔｏｔｙｐ
ｉｃａｌｎｅｔｗｏｒｋｓｆｏｒｆｅｗ
－ｓｈｏｔｌｅａｍｉｎｇ［Ｊ］
．


Ａｄｖａｎｃｅｓｉｎｎｅｕｒａｌｉｎｆｏｒｍａｔｉｏｎ
ｐｒｏｃｅｓｓｉｎｇｓｙｓｔｅｍｓ，２０１７
，３０
．


［９
］李涛涛
．基于局部描述子的小样本学习方法研宄
［Ｄ
］
．南京大学，２０１９
．


［
１０
］ＳｕｎｇＦ
，ＹａｎｇＹ，ＺｈａｎｇＬ
，ｅｔａｌ
．Ｌｅａｒ
ｎｉｎｇｔｏｃｏｍｐａｒｅ
：Ｒｅｌａｔ
ｉｏｎｎｅｔｗｏｒｋｆｏｒｆｅｗ
－


ｓｈｏｔｌｅａｍｉｎｇ［Ａ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔ
ｆｉｅＩＥＥＥｃｏｎｆｅｒｅｎｃｅｏｎｃｏｍｐｕｔｅｒｖｉｓｉｏｎａｎｄ


ｐａｔｔｅｒｎｒｅｃｏｇｎｉｔｉｏｎ［Ｃ］，２０１８
：１１９９
－１２０８
．


［
１
１
］ＧｅｎｇＲ
，ＬｉＢ
，ＬｉＹ
，ｅｔａｌ．ＩｎｄｕｃｔｉｏｎＮｅｔｗｏｒｋｓｆｏｒＦｅｗ
－ＳｈｏｔＴｅｘｔＣｌａｓｓｉｆｉｃａｔｉｏｎ
［Ａ
］


／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆ
ｔ
ｉｂｉｅ２０１９ＣｏｎｆｅｒｅｎｃｅｏｎＥｍｐ
ｉｒｉｃａｌＭｅｔｈｏｄｓｉｎＮａｔｕｒａｌＬａｎｇｕａｇｅ


Ｐｒｏｃｅｓｓｉｎｇａｎｄｔｈｅ９ｔｈＩｎｔｅｒ
ｎａｔｉｏｎａｌＪｏｉｎｔＣｏｎｆｅｒｅｎｃｅｏｎＮａｔｕｒａｌＬａｎｇｕａｇｅ


Ｐｒｏｃｅｓｓｉｎｇ（ＥＭＮＬＰ
－ＩＪＣＮＬＰ）［Ｃ
］？２０１９
：３９０４
－３９１３
．


［
１２
］ＧｅｎｇＲ
，ＬｉＢ
，ＬｉＹ
，ｅｔａｌ
．ＤｙｎａｍｉｃＭｅｍｏｒｙＩｎｄｕｃｔｉｏｎＮｅｔｗｏｒｋｓｆｏｒＦｅｗ
－Ｓｈｏｔ


ＴｅｘｔＣｌａｓｓｉｆｉｃａｔｉｏｎ［Ａ］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ５８ｔｈＡｎｎｕａｌＭｅｅｔｉｎｇｏｆｔｈｅ


ＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ
［Ｃ
］，２０２０：１０８７
－
１０９４．


［
１３
］ＴｈｒｕｎＳ，ＰｒａｔｔＬ
．Ｌｅａｒｎｉｎｇｔｏｌｅａｍ：Ｉｎｔｒｏｄｕｃｔｉｏｎａｎｄｏｖｅｒｖｉｅｗ
ｊＭ］
，Ｂｏｓｔｏｎ
，ＭＡ：


Ｓｐｒｉｎｇｅｒ
，１９９８
：３
－１７
．


６５


北京邮电大学工程硕士学位论文


［
１４
］ＭｕｎｋｈｄａｌａｉＴ
，ＹｕＨ
．ＭｅｔａＮｅｔｗｏｒｋｓ
［Ｊ
］
．Ｐｒｏｃｅｅｄｉｎｇｓｏｆ
ｍａｃｈｉｎｅｌｅａｒｎｉｎｇｒｅｓｅａｒｃｈ
，


２０
１７
，７０
：
２５５４
－２５６３
．


［
１５
］ＦｉｎｎＣ
，ＡｂｂｅｅｌＰ
，ＬｅｖｉｎｅＳ
．Ｍｏｄｅｌ
－ａｇｎｏｓｔｉｃｍｅｔａ
－
ｌｅａｒｎｉｎｇｆｏｒｆａｓｔａｄａｐ
ｔａｔｉｏｎｏｆ


ｄｅｅｐｎｅｔｗｏｒｋｓ［Ａ
］／／Ｉｎｔｅｒｎａｔｉｏｎａｌｃｏｎｆｅｒｅｎｃｅｏｎｍａｃｈｉｎｅｌｅａｍｉｎｇ［Ｃ
］
？２０
１７
：１
１２６
－


１
１３５
．


［
１６
］ＮｉｃｈｏｌＡ
，ＡｃｈｉａｍＪ
，ＳｃｈｕｌｍａｎＪ
．Ｏｎｆ
ｉｒｓｔ
－ｏｒｄｅｒｍｅｔａ
－
ｌｅａｍｉｎｇａｌｇｏｒｉｔｈｍｓ
［Ｊ
］
．ａｒＸｉｖ


ｐｒｅｐｒ
ｉｎｔａｒＸｉｖ
：１８０３
．０２９９９
，２０１８
．


［
１７
］ＪｉａｎｇＸ
，ＨａｖａｅｉＭ
，ＣｈａｒｔｒａｎｄＧ
，ｅｔａｌ
．Ｏｎｔｈｅｉｍｐｏｒｔａｎｃｅｏｆａｔｔｅｎｔｉｏｎｉｎｍｅｔａ
－


ｌｅａｒ
ａｉｎｇｆｏｒｆｅｗ
－ｓｈｏｔｔｅｘｔｃｌａｓｓｉｆ
ｉｃａｔｉｏｎ
［Ｊ
］
．ａｒＸｉｖ
ｐｒｅｐｒ
ｉｎｔａｒＸｉｖ
：１８０６
．００８５２
，
２０
１８
．


［
１８
］ＨａｎＣ
，ＦａｎＺ
，ＺｈａｎｇＤ
，ｅｔａｌ
．Ｍｅｔａ
－ＬｅａｒｎｉｎｇＡｄｖｅｒｓａｒ
ｉａｌＤｏｍａｉｎＡｄａｐ
ｔａｔｉｏｎ


ＮｅｔｗｏｒｋｆｏｒＦｅｗ
－ＳｈｏｔＴｅｘｔＣｌａｓｓｉｆ
ｉｃａｔｉｏｎ
［Ａ
］／／ＦｉｎｄｉｎｇｓｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒ


ＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ
：ＡＣＬ
－
ＩＪＣＮＬＰ２０２１
［Ｃ
］
，２０２
１
：１６６４
－
１６７３
．


［
１９
］ＢｒｏｗｎＴ
，ＭａｎｎＢ
，ＲｙｄｅｒＮ
，ｅｔａｌ
．Ｌａｎｇｕａｇｅｍｏｄｅｌｓａｒｅｆｅｗ
－ｓｈｏｔｌｅａｍｅｒｓ
［Ｊ
］
．


Ａｄｖａｎｃｅｓｉｎｎｅｕｒａｌｉｎｆｏｒｍａｔｉｏｎ
ｐｒｏｃｅｓｓｉｎｇｓｙｓｔｅｍｓ
，２０２０
，３３
：１８７７
－
１９０
１
．


［２０
］ＳｃｈｉｃｋＴ
，ＳｃｈｕｔｚｅＨ
．Ｆｅｗ
－ｓｈｏｔｔｅｘｔ
ｇｅｎｅｒａｔｉｏｎｗｉｔｈ
ｐａｔｔｅｒｎ
－ｅｘｐ
ｌｏｉｔｉｎｇ
ｔｒａｉｎｉｎｇ［Ｊ
］
．


ａｒＸｉｖ
ｐｒｅｐｒ
ｉｎｔａｒＸｉｖ
：２０
１２
．
１
１９２６
，２０２０
．


［２
１
］ＳｃｈｉｃｋＴ
？ＳｃｈｕｔｚｅＨ
．Ｅｘｐ
ｌｏｉｔｉｎｇＣｌｏｚｅ
－ＱｕｅｓｔｉｏｎｓｆｏｒＦｅｗ
－ＳｈｏｔＴｅｘｔＣｌａｓｓｉｆ
ｉｃａｔｉｏｎ


ａｎｄＮａｔｕｒａｌＬａｎｇｕａｇｅＩｎｆｅｒｅｎｃｅ
［Ａ
］
／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ１６ｔｈＣｏｎｆｅｒｅｎｃｅｏｆｔｈｅ


ＥｕｒｏｐｅａｎＣｈａｐｔｅｒｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ
：Ｍａｉｎ


Ｖｏｌｕｍｅ
［Ｃ
］
？２０２１
：
２５５
－２６９
．


［２２
］ＳｃｈｉｃｋＴ
，
ＳｃｈｉｉｔｚｅＨ
．Ｉｔ
’
ｓＮｏｔＪｕｓｔＳｉｚｅＴｈａｔＭａｔｅｒｓ
：ＳｍａｌｌＬａｎｇｕａｇｅＭｏｄｅｌｓＡｒｅ


ＡｌｓｏＦｅｗ
－ＳｈｏｔＬｅａｍｅｒｓ
［Ａ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２０２
１ＣｏｎｆｅｒｅｎｃｅｏｆｔｈｅＮｏｒｔｈ


ＡｍｅｒｉｃａｎＣｈａｐｔｅｒｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ
：Ｈｕｍａｎ


ＬａｎｇｕａｇｅＴｅｃｈｎｏｌｏｇ
ｉｅｓ
［Ｃ
］
？２０２
１
：２３３９
－２３５２
．


［２３
］ＴａｍＤ
，ＭｅｎｏｎＲＲ
，ＢａｎｓａｌＭ
，ｅｔａｌ
．ＩｍｐｒｏｖｉｎｇａｎｄＳ
ｉｍｐ
ｌ
ｉｆｙ
ｉｎｇＰａｔｔｅｒｎＥｘｐ
ｌｏｉｔｉｎｇ


Ｔｒａｉｎｉｎｇ［Ａ
］
／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２０２１ＣｏｎｆｅｒｅｎｃｅｏｎＥｍｐ
ｉｒｉｃａｌＭｅｔｈｏｄｓｉｎ


ＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ［Ｃ
］
，２０２
１
：４９８０
－４９９１
．


［２４
］ＧａｏＴ
，ＦｉｓｃｈＡ
，ＣｈｅｎＤ
．ＭａｋｉｎｇＰｒｅ
－ｔｒａｉｎｅｄＬａｎｇｕａｇｅＭｏｄｅｌｓＢｅｔｔｅｒＦｅｗ
－ｓｈｏｔ


Ｌｅａｍｅｒｓ
［Ａ
］
／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ５９ｔｈＡｎｎｕａｌＭｅｅｔｉｎｇｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒ


ＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓａｎｄｔｈｅ１
１
ｔｈＩｎｔｅｒｎａｔｉｏｎａｌＪｏｉｎｔＣｏｎｆｅｒｅｎｃｅｏｎＮａｔｕｒａｌ


ＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ（Ｖｏｌｕｍｅ１
：ＬｏｎｇＰａｐｅｒｓ）［Ｃ
］
，２０２
１
；３８
１６
－３８３０
．


［２５
］ＲａｆｆｅｌＣ
，ＳｈａｚｅｅｒＮ
？ＲｏｂｅｒｔｓＡ
，ｅｔａｌ
．Ｅｘｐ
ｌｏｒｉｎｇ
ｔｈｅＬｉｍｉｔｓｏｆＴｒａｎｓｆｅｒＬｅａｒｎｉｎｇ


ｗｉｔｈａＵｎｉｆ
ｉｅｄＴｅｘｔ
－ｔｏ
－ＴｅｘｔＴｒａｎｓｆｏｒｍｅｒ
［Ｊ
］
．Ｊｏｕｒ
ｎａｌｏｆＭａｃｈｉｎｅＬｅａｒｎｉｎｇ


Ｒｅｓｅａｒｃｈ
，２０２０
，２
１
：１
－６７
．


６６


参考文献


［２６
］
ＬｉｕＸ
，ＺｈｅｎｇＹ
，ＤｕＺ
？ｅｔａｌ
．ＧＰＴｕｎｄｅｒｓｔａｎｄｓ，ｔｏｏ［Ｊ］
．ａｒＸｉｖｐｒｅｐｒ
ｉｎｔ


ａｒＸｉｖ：２１０３
．１０３８５
，２０２１
．


［２７
］ＷａｎｇＳ？ＦａｎｇＨ
，ＫｈａｂｓａＭ
？ｅｔａｌ
．Ｅｎｔａｉｌｍｅｎｔａｓｆｅｗ
－ｓｈｏｔｌｅａｍｅｒ
［Ｊ］
，ａｒＸｉｖ
ｐｒｅｐｒ
ｉｎｔ


ａｒＸｉｖ：２１０４．
１４６９０
，２０２１
．


［２８］
ＪｉｍｇＺ
？ＸｕＦＦ
，ＡｒａｋｉＪ５ｅｔａｌ
．Ｈｏｗｃａｎｗｅｋｎｏｗｗｈａｔｌａｎｇｕａｇｅｍｏｄｅｌｓｋｎｏｗ？［Ｊ
］
，


Ｔｒａｎｓａｃｔｉｏｎｓｏｆ
ｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ
，２０２０
，８
：４２３
－４３８
．


［２９］ＳｕｎＹ
？ＺｈｅｎｇＹ
，ＨａｏＣ，ｅｔａｌ
．ＮＳＰ
－ＢＥＲＴ：ＡＰｒｏｍｐｔ
－ｂａｓｅｄＺｅｒｏ
－ＳｈｏｔＬｅａｒ
ｎｅｒ


ＴｈｒｏｕｇｈａｎＯｒ
ｉｇｉｎａｌＰｒｅ
－ｔｒａｉｎｉｎｇＴａｓｋ
—ＮｅｘｔＳｅｎｔｅｎｃｅＰｒｅｄｉｃｔｉｏｎ
［Ｊ］
．ａｒＸｉｖ


ｐｒｅｐｒｉｎｔａｒＸｉｖ：２１０９．０３５６４
，２０２１
．


［３０
］
ＸｕＬ
，ＬｕＸ
，ＹｕａｎＣ，ｅｔａｌ
．Ｆｅｗｃｌｕｅ：ＡＣｈｉｎｅｓｅｆｅｗ
－ｓｈｏｔｌｅａｒｎｉｎｇｅｖａｌｕａｔｉｏｎ


ｂｅｎｃｈｍａｒｋ［Ｊ］
．ａｒＸｉｖ
ｐｒｅｐｒ
ｉｎｔａｒＸｉｖ：２１０７．０７４９８
，２０２１
．


［３
１
］ＤａｇａｎＩ
，ＧｌｉｃｋｍａｎＯ
．Ｐｒｏｂａｂｉｌｉｓｔｉｃｔｅｘｔｕａｌｅｎｔａｉｌｍｅｎｔ：Ｇｅｎｅｒｉｃａｐｐ
ｌｉｅｄｍｏｄｅｌｉｎｇ


ｏｆｌａｎｇｕａｇｅｖａｒ
ｉａｂｉｌｉｔ
ｙ［Ｊ］
．Ｌｅａｒ
ｎｉｎｇＭｅｔ
ｉｉｏｄｓｆｏｒＴｅｘｔＵｎｄｅｒｓｔａｎｄｉｎｇａｎｄＭｉｎｉｎｇ，


２００４
，２００４
：２６
－２９．


［３２
］ＤａｇａｎＩ
，ＧｌｉｃｋｍａｎＯ
，ＭａｇｎｉｎｉＢ
．Ｔｈｅｐａｓｃａｌｒｅｃｏｇｎｉｓｉｎｇｔｅｘｔｕａｌｅｎｔａｉｌｍｅｎｔ


ｃｈａｌｌｅｎｇｅ［Ａ
］／／ＭａｃｈｉｎｅＬｅａｒｎｉｎｇＣｈａｌｌｅｎｇｅｓＷｏｒｋｓｈｏｐ［Ｃ］，Ｂｅｒｌｉｎ，Ｈｅｉｄｅｌｂｅｒｇ
：


Ｓｐｒ
ｉｎｇｅｒ
，２００５
：１７７
－
１９０．


［３３
］ＨａｄｓｅｌｌＲ，ＣｈｏｐｒａＳ
，ＬｅＣｕｎＹ
．Ｄｉｍｅｎｓｉｏｎａｌｉｔｙｒｅｄｕｃｔｉｏｎｂｙ
ｌｅａｒｎｉｎｇａｎｉｎｖａｒ
ｉａｎｔ


ｍａｐｐ
ｉｎｇ［Ａ
］／／２００６ＩＥＥＥＣｏｍｐｕｔｅｒＳｏｃｉｅｔｙＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄ


Ｐａｔｅｒ
ｎＲｅｃｏｇｎｉｔｉｏｎ
（ＣＶＰＲ
＇
Ｏｅ）［Ｃ
］，２００６
，２
：１７３５
－１７４２．


ｐ４
］ＭｉｋｏｌｏｖＴ
，
ＳｕｔｓｋｅｖｅｒＩ
，ＣｈｅｎＫ
，
ｅｔａＬＤｉｓｔｒｉｂｕｔｅｄｒｅｐｒｅｓｅｎｔａｔ
ｉｏｎｓｏｆｗｏｒｄｓａｎｄ


ｐｈｒａｓｅｓａｎｄｔｈｅｉｒｃｏｍｐｏｓｉｔｉｏｎａｌｉｔｙ［Ｊ
］
．Ａｄｖａｎｃｅｓｉｎｎｅｕｒａｌｉｎｆｏｒｍａｔｉｏｎ
ｐｒｏｃｅｓｓｉｎｇ


ｓｙｓｔｅｍｓ，２０１３
，２６．


［３５
］
ＰｅｔｅｒｓＭ
，ＮｅｕｍａｎｎＭ？ＩｙｙｅｒＭ
？ｅｔａｌ
．ＤｅｅｐＣｏｎｔｅｘｔｕａｌｉｚｅｄＷｏｒｄ


Ｒｅｐｒｅｓｅｎｔａｔｉｏｎｓ［Ｊ］
．Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２０１８ＣｏｎｆｅｒｅｎｃｅｏｆｔｈｅＮｏｒｔｈＡｍｅｒｉｃａｎ


Ｃｈａｐ
ｔｅｒｏｆｔｈｅＡｓｓｏｃｉａｔ
ｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ：ＨｕｍａｎＬａｎｇｕａｇｅ


Ｔｅｃｈｎｏｌｏｇ
ｉｅｓ，Ｖｏｌｕｍｅ１（ＬｏｎｇＰａｐｅｒｓ），２０１８
．


［３
６］
ＲａｄｆｏｒｄＡ
，ＮａｒａｓｉｍｈａｎＫ
，
ＳａｌｉｍａｎｓＴ，
ｅｔａｌ．Ｉｍｐｒｏｖｉｎｇ
ｌａｎｇｕａｇｅｕｎｄｅｒｓｔａｎｄｉｎｇ


ｂｙｇｅｎｅｒａｔｉｖｅ
ｐｒｅ
－ｔｒａｉｎｍｇ［Ｊ］
．２０１８－


［３
７
］
ＲａｄｆｏｒｄＡ
，ＷｕＪ
？ＣｈｉｌｄＲ
，ｅｔａｌ
．Ｌａｎｇｕａｇｅｍｏｄｅｌｓａｒｅｕｎｓｕｐｅｒｖｉｓｅｄｍｕｌｔｉｔａｓｋ


ｌｅａｍｅｒｓｆＪ
］
．ＯｐｅｎＡＩｂｌｏｇ，２０１９
，
１（８）
：９．


［３８
］
ＤｅｖｌｉｎＪ
，ＣｈａｎｇＭＷ，ＬｅｅＫ
？ｅｔａｌ．ＢＥＲＴ：Ｐｒｅ
－ｔｒａｉｍｎｇｏｆＤｅｅｐＢｉｄｉｒｅｃｔｉｏｎａｌ


ＴｒａｎｓｆｏｒｍｅｒｓｆｏｒＬａｎｇｕａｇｅＵｎｄｅｒｓｔａｎｄｉｎｇ［Ａ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２０１９


ＣｏｎｆｅｒｅｎｃｅｏｆｔｈｅＮｏｒｔｈＡｍｅｒｉｃａｎＣｈａｐｔｅｒｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌ


６７


北京邮电大学工程硕士学位论文


Ｌｉｎｇｕｉｓｔｉｃｓ
：ＨｕｍａｎＬａｎｇｕａｇｅＴｅｃｈｎｏｌｏｇ
ｉｅｓ
，Ｖｏｌｕｍｅ１（ＬｏｎｇａｎｄＳｈｏｒｔＰａｐｅｒｓ）


［Ｃ
］
，
２０１９
：４１７
１
－４１８６
．


［３９
］ＶａｓｗａｎｉＡ
？ＳｈａｚｅｅｒＮ
，ＰａｒｍａｒＮ
，ｅｔａｌ
．Ａｔｔｅｎｔｉｏｎｉｓａｌｌ
ｙｏｕｎｅｅｄ
［Ｊ
］
．Ａｄｖａｎｃｅｓｉｎ


ｎｅｕｒａｌｉｎｆｏｒｍａｔｉｏｎ
ｐｒｏｃｅｓｓｉｎｇｓｙｓｔｅｍｓ
，２０
１７
，３０
．


［４０
］ＳｃｈｕｓｔｅｒＭ
，Ｎａｋａｊ
ｉｍａＫ
．ＪａｐａｎｅｓｅａｎｄＫｏｒｅａｎｖｏｉｃｅｓｅａｒｃｈ
［Ａ
］
／／Ａｃｏｕｓｔｉｃｓ
，


ＳｐｅｅｃｈａｎｄＳｉｇｎａｌＰｒｏｃｅｓｓｉｎｇ（
ＩＣＡＳＳＰ
）
，２０
１２ＩＥＥＥＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅ


ｏｎ
［Ｃ
］
，
２０
１２
．


［４
１
］ＬｉｕＹ
，ＯｔｔＭ
，ＧｏｙａｌＮ
，ｅｔａｌ
．Ｒｏｂｅｒｔａ
：Ａｒｏｂｕｓｔｌｙｏｐ
ｔｉｍｉｚｅｄｂｅｒｔｐｒｅｔｒａｉｎｉｎｇ


ａｐｐｒｏａｃｈ
［Ｊ
］
．ａｒＸｉｖ
ｐｒｅｐｒ
ｉｎｔａｒＸｉｖ
：
１９０７
．
１
１６９２
？２０１９
．


［４２
］ＳｕｎＹ
，ＷａｎｇＳ
，ＬｉＹ
，ｅｔａｌ
．Ｅｒｎｉｅ
：Ｅｎｈａｎｃｅｄｒｅｐｒｅｓｅｎｔａｔｉｏｎｔｈｒｏｕｇｈｋｎｏｗｌｅｄｇｅ


ｉｎｔｅｇｒａｔｉｏｎ
［Ｊ
］
．ａｒＸｉｖ
ｐｒｅｐｒｉｎｔａｒＸｉｖ
：１９０４
．０９２２３
，２０
１９
．


［４３
］ＣｕｉＹ
，
ＣｈｅＷ
，
ＬｉｕＴ
，
ｅｔａｌ
．ＲｅｖｉｓｉｔｉｎｇＰｒｅ
－ＴｒａｉｎｅｄＭｏｄｅｌｓｆｏｒＣｈｉｎｅｓｅＮａｔｕｒａｌ


ＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ［Ａ
］
／／ＦｉｎｄｉｎｇｓｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌ


Ｌｉｎｇｕｉｓｔｉｃｓ
：ＥＭＮＬＰ２０２０
［Ｃ
］
？２０２０
：６５７
－６６８
．


［４４
］ＬｉｕＰ
，ＹｕａｎＷ
５ＦｕＪ
ｓｅｔａｌ
．Ｐｒｅ
－ｔｒａｉｎ，ｐｒｏｍｐ
ｔ
，ａｎｄ
ｐｒｅｄｉｃｔ
：Ａｓｙｓｔｅｍａｔｉｃｓｕｒｖｅｙｏｆ


ｐｒｏｍｐ
ｔｉｎｇｍｅｔｈｏｄｓｉｎｎａｔｕｒａｌｌａｎｇｕａｇｅｐｒｏｃｅｓｓｉｎｇ
［Ｊ］
．ａｒＸｉｖｐｒｅｐｒ
ｉｎｔ


ａｒＸｉｖ
：２
１０７
．
１３５８６
，２０２
１
．


［４５
］ＰｅｔｒｏｎｉＦ
？ＲｏｃｋｔａｓｃｈｅｌＴ
，
ＲｉｅｄｅｌＳ
，ｅｔａｌ
．ＬａｎｇｕａｇｅＭｏｄｅｌｓａｓＫｎｏｗｌｅｄｇｅ


Ｂａｓｅｓ？
［Ａ
］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆ
ｔｈｅ２０１９ＣｏｎｆｅｒｅｎｃｅｏｎＥｍｐ
ｉｒｉｃａｌＭｅｔｈｏｄｓｉｎＮａｔｕｒａｌ


ＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇａｎｄｔｈｅ９ｔｈＩｎｔｅｒ
ｎａｔｉｏｎａｌＪｏｉｎｔＣｏｎｆｅｒｅｎｃｅｏｎＮａｔｕｒａｌ


ＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ（ＥＭＮＬＰ
－ＩＪＣＮＬＰ
）［Ｃ
］
，２０１９
：２４６３
－２４７３
．


［４６
］ＬｉＸＬ
，ＬｉａｎｇＰ
．Ｐｒｅｆ
ｉｘ
－Ｔｕｎｉｎｇ
：Ｏｐ
ｔｉｍｉｚｉｎｇＣｏｎｔｉｎｕｏｕｓＰｒｏｍｐ
ｔｓｆｏｒＧｅｎｅｒａｔｉｏｎ
［Ａ
］


／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ５９ｔｈＡｎｎｕａｌＭｅｅｔｉｎｇｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌ


Ｌｉｎｇｕｉｓｔｉｃｓａｎｄｔｈｅ１
１ｔｈＩｎｔｅｒｎａｔｉｏｎａｌＪｏｉｎｔＣｏｎｆｅｒｅｎｃｅｏｎＮａｔｕｒａｌＬａｎｇｕａｇｅ


Ｐｒｏｃｅｓｓｉｎｇ（Ｖｏｌｕｍｅ１
：ＬｏｎｇＰａｐｅｒｓ）［Ｃ
］，２０２
１
：４５８２
－４５９７
．


［４７
］ＬｅｓｔｅｒＢ
，Ａｌ
－ＲｆｏｕＲ
？ＣｏｎｓｔａｎｔＮ
．ＴｈｅＰｏｗｅｒｏｆＳｃａｌｅｆｏｒＰａｒａｍｅｔｅｒ
－Ｅｆｆｉｃｉｅｎｔ


Ｐｒｏｍｐ
ｔＴｕｎｉｎｇ［Ａ
］
／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ２０２
１ＣｏｎｆｅｒｅｎｃｅｏｎＥｍｐ
ｉｒｉｃａｌＭｅｔｈｏｄｓ


ｉｎＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ［Ｃ
］
？２０２
１
：３０４５
－３０５９
．


［４８
］ＳｃｈｒｏｆｆＦ
，
ＫａｌｅｎｉｃｈｅｎｋｏＤ
，
ＰｈｉｌｂｉｎＪ
．Ｆａｃｅｎｅｔ
：Ａｕｎｉｆ
ｉｅｄｅｍｂｅｄｄｉｎｇｆｏｒｆａｃｅ


ｒｅｃｏｇｎｉｔｉｏｎａｎｄｃｌｕｓｔｅｒｉｎｇ［Ａ
］／／ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＩＥＥＥｃｏｎｆｅｒｅｎｃｅｏｎｃｏｍｐｕｔｅｒ


ｖｉｓｉｏｎａｎｄ
ｐａｔｔｅｒｎｒｅｃｏｇｎｉｔｉｏｎ
［Ｃ
］
？２０
１５
：８
１５
－８２３
．


［４９
］ＷｅｉｎｂｅｒｇｅｒＫＱ，ＳａｕｌＬＫ
．ＤｉｓｔａｎｃｅＭｅｔｒｉｃＬｅａｒｎｉｎｇｆｏｒＬａｒｇｅＭａｒｇ
ｉｎＮｅａｒｅｓｔ


ＮｅｉｇｈｂｏｒＣｌａｓｓｉｆ
ｉｃａｔｉｏｎ
［Ｊ
］
．ＪｏｕｒｎａｌｏｆＭａｃｈｉｎｅＬｅａｒｎｉｎｇＲｅｓｅａｒｃｈ
，２００９
？
１０
（
１）
：


２０７
－２４４
．


６８


参考文献


［５０
］
ＸｕＬ
，ＨｕＨ
，ＺｈａｎｇＸ
？ｅｔａｌ
．ＣＬＵＥ：ＡＣｈｉｎｅｓｅＬａｎｇｕａｇｅＵｎｄｅｒｓｔａｎｄｉｎｇ


ＥｖａｌｕａｔｉｏｎＢｅｎｃｈｍａｒｋ
［Ａ］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆ
ｔｈｅ２８ｔ
ｉｉＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎ


ＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔ
ｉｃｓ［Ｃ］，２０２０
：４７６２
－４７７２．


［５１］
ＬｉＸ
，ＲｏｔｈＤ．Ｌｅａｒｎｉｎｇｑｕｅｓｔｉｏｎｃｌａｓｓｉｆ
ｉｅｒｓ［Ａ］／／ＣＯＬＩＮＧ２００２：Ｔｈｅ１９ｔｈ


Ｉｎｔｅｒ
ｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ［Ｃ］５２００２
．


［５２］
ＺｈａｎｇＸ
，ＺｈａｏＪ
，ＬｅＣｕｎＹ．Ｃｈａｒａｃｔｅｒ
－
ｌｅｖｅｌｃｏｎｖｏｌｕｔｉｏｎａｌｎｅｔｗｏｒｋｓｆｏｒｔｅｘｔ


ｃｌａｓｓｉｆｉｃａｔｉｏｎ［Ｊ
］
．Ａｄｖａｎｃｅｓｉｎｎｅｕｒａｌｉｎｆｏｒｍａｔｉｏｎ
ｐｒｏｃｅｓｓｉｎｇｓｙｓｔｅｍｓ
，２０１５
，２８
．


［５３
］ＰａｓｚｋｅＡ，ＧｒｏｓｓＳ
，ＭａｓｓａＦ
，ｅｔａｌ
．Ｐｙｔｏｒｃｈ：Ａｎｉｍｐｅｒａｔｉｖｅｓｔｙ
ｌｅ
，ｈｉｇｈ
－
ｐｅｒｆｏｒｍａｎｃｅ


ｄｅｅｐ
ｌｅａｒｎｉｎｇ
ｌｉｂｒａｒｙ［Ｊ］
．Ａｄｖａｎｃｅｓｉｎｎｅｕｒａｌｉｎｆｏｒｍａｔｉｏｎ
ｐｒｏｃｅｓｓｉｎｇｓｙｓｔｅｍｓ，２０１９
，


３２
．


［５４
］ＣｕｉＹ
？ＣｈｅＷ
？ＬｉｕＴ
，ｅｔａｌ
．Ｐｒｅ
－ｔｒａｉｎｉｎｇｗｉｔｈｗｈｏｌｅｗｏｒｄｍａｓｋｉｎｇｆｏｒＣｈｉｎｅｓｅ


ｂｅｒｔ
［Ｊ
］
．ＩＥＥＥ／ＡＣＭＴｒａｎｓａｃｔｉｏｎｓｏｎＡｕｄｉｏ，Ｓｐｅｅｃｈ
，ａｎｄＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ，


２０２１
，２９
：３５０４
－３５１４．


［５５
］ＬｏｓｈｃｈｉＩｏｖＩ
？ＨｕｔｔｅｒＦ．Ｄｅｃｏｕｐ
ｌｅｄＷｅｉｇｈｔＤｅｃａｙＲｅｇｕｌａｒ
ｉｚａｔｉｏｎ
［Ａ
］／／


Ｉｎｔｅｒ
ｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＬｅａｒ
ｎｉｎｇＲｅｐｒｅｓｅｎｔａｔｉｏｎｓ［Ｃ］，２０１８
．


［５６
］
ＶａｎｄｅｒＭａａｔｅｎＬ
？ＨｉｎｔｏｎＧ．Ｖｉｓｕａｌｉｚｉｎｇｄａｔａｕｓｉｎｇ
ｔ
－ＳＮＥ［Ｊ
］
．Ｊｏｕｒ
ｎａｌｏｆ
ｍａｃｈｉｎｅ


ｌｅａｒｎｉｎｇｒｅｓｅａｒｃｈ
，２００８
，９（
１
１）
．


６９


