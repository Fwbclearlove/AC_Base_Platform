FreeGraftor: Training-Free Cross-Image Feature Grafting for
Subject-Driven Text-to-Image Generation
Zebin Yao1, Lei Ren2, Huixing Jiang2, Chen Wei2, Xiaojie Wang1, Ruifan Li1, Fangxiang Feng1
1Beijing University of Posts and Telecommunications
2Li Auto Inc.
“... on top
of a mirror”
“... on the
desk”
“... on a
race track”
“... lifting
weights”
“... riding a
bike”
“... playing
skateboard”
Reference
Reference
(a) Pixel-level Preservation
(b) Flexible Control
Reference
“... on a
beach”
“... in Times
Square”
“A cat sitting on a
chair and a dog
sitting on the floor”
(c) Multi-subject Support
Figure 1: Subject-driven generation results of our FreeGraftor, achieved without any training or tuning process. These results
demonstrate the superior properties of FreeGraftor, including (a) pixel-level detail preservation, (b) flexible text-guided control,
and (c) support for multiple reference subjects.
Abstract
Subject-driven image generation aims to synthesize novel scenes
that faithfully preserve subject identity from reference images while
adhering to textual guidance, yet existing methods struggle with
a critical trade-off between fidelity and efficiency. Tuning-based
approaches rely on time-consuming and resource-intensive subject-
specific optimization, while zero-shot methods fail to maintain
adequate subject consistency. In this work, we propose FreeGraftor,
a training-free framework that addresses these limitations through
cross-image feature grafting. Specifically, FreeGraftor employs se-
mantic matching and position-constrained attention fusion to trans-
fer visual details from reference subjects to the generated image.
Additionally, our framework incorporates a novel noise initializa-
tion strategy to preserve geometry priors of reference subjects for
robust feature matching. Extensive qualitative and quantitative ex-
periments demonstrate that our method enables precise subject
identity transfer while maintaining text-aligned scene synthesis.
Without requiring model fine-tuning or additional training, Free-
Graftor significantly outperforms existing zero-shot and training-
free approaches in both subject fidelity and text alignment. Fur-
thermore, our framework can seamlessly extend to multi-subject
generation, making it practical for real-world deployment. Our code
is available at https://github.com/Nihukat/FreeGraftor.
1
Introduction
The democratization of text-to-image generation has ignited wide-
spread interest in personalized content creation, where users seek
to synthesize novel scenes containing specific visual concepts—be
it a cherished pet, a unique artifact, or a fictional character. While
diffusion models[25, 28, 30, 32, 37] excel at generating diverse im-
agery from textual prompts, adapting them to preserve subject
identity without costly per-subject optimization remains a critical
challenge.
Current solutions for subject-driven generation[10, 17, 19, 35, 49]
face a critical dilemma between fidelity and efficiency. Tuning-
based methods, such as DreamBooth[35] and Textual Inversion[10],
capture subject identity by fine-tuning models or learning em-
beddings on user-provided subject datasets. These methods rely
on subject-specific optimization to achieve high subject consis-
tency, which are resource-intensive and time-consuming, prone
to catastrophic forgetting, and struggle with multi-subject com-
position. Zero-shot methods, such as IP-Adapter[49] and BLIP-
Diffusion[19], extract subject identities from reference images via
additional trainable modules (e.g., image encoders or adapters) and
integrate them into the text-to-image generation process. While
these methods directly generate images containing reference sub-
jects, they often fail to retain fine visual details, especially for novel
concepts, as the limited parameter capacity of auxiliary modules
fundamentally restricts their ability to reconstruct high-fidelity
visual details.
1
arXiv:2504.15958v2  [cs.CV]  26 Apr 2025
Reference
Target
Reference
Target
Reference
Target
Reference
Target
Figure 2: Cross-image semantic correspondence in FLUX.1.
For each specified pixel in the reference image, we identify
the most similar pixel in the target image within FLUX.1‘s
feature space. These corresponding pixel pairs are then visu-
ally connected by colored lines.
It is worth noting that pre-trained text-to-image models inher-
ently possess powerful visual representation capabilities, as evi-
denced by their ability to synthesize intricate textures and struc-
tures from textual prompts. However, current approaches fail to
fully exploit this potential. Tuning-based methods modify the im-
age generation distribution through fine-tuning, while zero-shot
approaches offload visual representation learning to auxiliary mod-
ules. In contrast, directly tapping into the base model’s own feature
space for cross-image knowledge transfer offers a promising way
to bridge the gap between fidelity and efficiency. Recent advances
in appearance transfer[1, 8, 12, 22, 23] lend credence to this idea,
demonstrating that diffusion models can transplant visual attributes
such as style and texture between images through semantic feature
matching. For instance, Eye-for-an-eye[12] successfully transfers
localized appearances by establishing cross-image correspondences
in U-Net’s feature space. By computing pixel-wise similarity (de-
tailed in Sec. 3.2), we demonstrate that such cross-image semantic
correspondences can also be identified in the feature space of mod-
ern MM-DiT-based[9] models (e.g., FLUX.1[18]). As illustrated in
Figure 2, two pixels that are similar in FLUX.1’s feature space are
also semantically corresponding.
Building on this insight, we propose FreeGraftor, a training-free
framework that resolves the fidelity-efficiency dilemma in subject-
driven generation via cross-image feature grafting. Our approach
extracts visual features from reference images using rectified flow
inversion[5, 34, 44] and injects them into the generation process.
Specifically, we introduce a novel cross-image feature grafting
mechanism that transfers appearance details through semantic
matching and position-constrained attention fusion in the feature
space of text-to-image diffusion models. To bind reference patches
to their corresponding generated patches, we implement a posi-
tion embedding replication strategy, enabling semantically matched
patches to share positional information during attention compu-
tation. Furthermore, we identify that proper noise initialization is
crucial for robust semantic matching and structural preservation
of reference subjects. Thus, we propose a structure-consistent
initialization strategy: first constructing a collage containing the
reference subject, then inverting it as the initialization noise for gen-
eration. Our method achieves pixel-level detail preservation (e.g.,
text, patterns) with fast inference and low computational overhead,
requiring no additional training or optimization.
Extensive experiments demonstrate that our method outper-
forms existing approaches in both subject consistency and text
alignment while maintaining competitive efficiency. When combin-
ing multiple subjects, FreeGraftor preserves individual identities
and harmoniously integrates all subjects into a coherent scene
without layout conflicts or attribute confusion.
Our contributions are summarized as follows:
• We introduce FreeGraftor, a plug-and-play framework for
subject-driven image generation that achieves pixel-level
detail preservation and flexible text-guided control without
fine-tuning or training.
• We develop a cross-image feature grafting technique that
transfers visual characteristics of reference subjects to se-
mantically corresponding regions in generated images via
semantic matching and position-constrained attention fu-
sion.
• We propose a novel noise initialization strategy that pre-
serves reference geometry through automated collage con-
struction and rectified flow inversion, seamlessly extending
our method to multi-subject scenarios.
2
Related Work
2.1
Multimodal-Diffusion Transformer
Text-to-image diffusion models have achieved remarkable success in
generating high-quality and diverse images, exemplified by DALL-E
2[30], Imagen[37], GLIDE[25], and eDiff-I[4]. Early popular open-
source models, such as Stable Diffusion[32] and SDXL[28], are
based on U-Net[33] architecture and utilize cross-attention between
image and text to introduce textual conditions. In contrast, Diffu-
sion Transformer (DiT)[27] replaces the U-Net backbone with a
Transformer[42], demonstrating superior scalability and perfor-
mance. Recent advanced open-source models, including Stable Dif-
fusion 3[9] and FLUX.1[18], follow this design and project text
and image tokens into a unified space to compute joint attention,
thereby better integrating complex relationships between text and
image modalities. This architecture is termed Multimodal-Diffusion
Transformer (MM-DiT). In this work, we implement our method
on FLUX.1, where joint attention is computed as:
𝑄=
h
𝑄txt;𝑄imgi
,
𝐾=
h
𝐾txt;𝐾imgi
,
𝑉=
h
𝑉txt;𝑉imgi
(1)
˜𝑄= 𝑃𝐸⊙𝑄,
˜𝐾= 𝑃𝐸⊙𝐾
(2)
𝐴= softmax
 ˜𝑄˜𝐾⊤
√
𝑑

,
𝐻out = 𝐴𝑉
(3)
Here, [; ] denotes concatenation, 𝑄, 𝐾, 𝑉represent queries, keys,
values, 𝑃𝐸is position embedding, 𝑑denotes the dimension of the
input features, 𝐴denotes attention scores, and 𝐻out is the output
of the attention layer.
2
2.2
Subject-Driven Generation
Subject-driven generation[10, 17, 19, 35, 49] aims to synthesize
novel scenes containing user-specified subjects. Existing approaches
fall into two categories: tuning-based and zero-shot. Tuning-based
methods[2, 7, 10, 13, 17, 21, 35, 43] achieve subject-driven gener-
ation by customizing models, typically requiring per-subject fine-
tuning[13, 17, 35] or iterative learning of unique embeddings[2,
10, 21, 43]. While subject-specific tuning effectively captures vi-
sual details of the reference subject, it is resource-intensive and
time-consuming, risks overfitting and catastrophic forgetting, and
struggles with multi-subject composition. Zero-shot methods[3,
11, 14, 15, 19, 36, 38, 45–47, 49] introduce additional learnable mod-
ules to process extra image inputs, seamlessly integrating visual
information of the reference subject into the generation process.
These methods enable personalization at inference time but sacri-
fice fine visual details, especially for novel concepts. Furthermore,
they require large-scale retraining when switching to new base
models. Moreover, both categories can degrade the performance of
base models, resulting in reduced quality of generated images.
Beyond these categories, several training-free approaches[6, 39]
aim to harness the inherent capabilities of pre-trained text-to-
image models for plug-and-play subject-driven generation. For
example, FreeCustom[6] injects reference features by concatenat-
ing key and value pairs of reference and generated images in at-
tention layers, emphasizing visual characteristics of the reference
subject through attention score editing. However, this approach
relies on layout alignment across multiple reference images and
suffers from visual distortion due to coarse attention manipulation.
DiptychPrompting[39] creatively reframes the subject-driven gen-
eration task as a diptych inpainting process, utilizing the visual con-
text processing capabilities of pre-trained models to achieve high
subject consistency. Unfortunately, pixel-space diptych construc-
tion incurs significant memory overhead, limiting its applicability
to high-resolution images (e.g., 1024×1024 or larger). Additionally,
this paradigm does not support generation with multiple refer-
ence subjects. In contrast, our work identifies cross-image semantic
correspondences in MM-DiT’s feature space and binds matched
patches via position embedding manipulation, enabling efficient
subject feature injection with minimal computational cost.
2.3
Appearance Transfer
Appearance transfer methods[1, 8, 12, 22, 23] aim to combine the
appearance of one image with the structure of another. Cross-
Image[1] inject key-value pairs across images to transfer appear-
ance from reference to target positions in the generated image.
Dragon-diffusion[22] and DiffEditor[23] integrate noise guidance
with key-value injection. However, these methods struggle to pre-
serve structure because attention scores do not necessarily reflect
semantic correspondences between reference and target images,
potentially leading to erroneous matches. DIFT[41] identifies fine-
grained semantic correspondences in U-Net feature space by com-
puting pixel-pair similarities. Building on this, Eye-for-an-Eye[12]
achieves appearance transfer by directly replacing features of cor-
responding pixels. Nevertheless, these U-Net-based approaches are
incompatible with modern MM-DiT architectures. Moreover, these
methods focus on style transfer rather than identity preservation,
which will cause geometric inconsistencies (e.g., shape, limb propor-
tions) between reference and generated subjects if directly applied
to subject-driven generation. In this work, we explore semantic
matching and fine-grained feature transfer in MM-DiT while em-
ploying a noise initialization strategy to preserve the geometric
structure of reference subjects.
3
Method
3.1
Overview
Our framework comprises three stages: collage construction, inver-
sion, and generation (illustrated in Figure 3). First, we construct a
collage using the given reference image and text prompt. Next, we
invert the collage to obtain initialization noise for the generation
phase while extracting reference features. Finally, these features are
injected into the generation process to synthesize the final image
containing the reference subject within the text-described scene.
3.2
Semantic-Aware Feature Grafting
Most subject-driven generation methods rely on learnable parame-
ters to inject reference features into the generation process. Tuning-
based approaches embed subject-specific information into pre-
trained models, while zero-shot methods employ additional mod-
ules for feature extraction and injection—both incurring significant
computational overhead.
Inspired by appearance transfer techniques[1, 8, 12, 22, 23], we
leverage the inherent capabilities of text-to-image diffusion mod-
els to transfer visual characteristics of reference subjects to corre-
sponding regions in generated images without incurring additional
training costs. To achieve this, we propose the Semantic-Aware
Feature Grafting (SAFG) module, which first establishes semantic
correspondences between reference and generated patches via se-
mantic matching, then fuses features through position-constrained
attention fusion (Figure 4).
3.2.1
Semantic Matching. Early appearance transfer methods[1, 22,
23] directly concatenate keys and values of reference and generated
images to compute cross-image attention. However, high attention
scores do not necessarily indicate semantic correspondences across
images, and erroneous matches may lead to structural distortions
and visual artifacts. DIFT[41] revealed that U-Net-based diffusion
models contain rich semantic information in their feature space
and established cross-image correspondences via cosine similarity-
based semantic matching. We extend this principle to MM-DiT-
based diffusion models.
Given a reference image 𝐼ref and a predefined binary mask 𝑀ref, pre
indicating the reference subject, we extract attention features us-
ing rectified flow inversion[5, 34, 44] and match reference 𝐻ref
and generated 𝐻gen features at the same timestep and attention
layer. For each reference patch 𝑖within 𝑀ref, pre, we compute cosine
similarity with all generated patches 𝑗:
𝑠𝑖𝑗=
𝐻img,ref
𝑖
· 𝐻img,gen
𝑗
∥𝐻img,ref
𝑖
∥2∥𝐻img,gen
𝑗
∥2
,
𝑚(𝑖) = arg max
𝑗
𝑠𝑖𝑗
(4)
3
DiT
DiT
SAFG
Erase
Segment
& Paste
Reference
DiT
SAFG
…
Prompt:
“A cat is sitting
on a chair,
wearing a
birthday hat.”
DiT
Output
Prompt
DiT
Copy
…
Stage 1: Collage Construction (Sec. 3.3)
Stage 2: Inversion (Sec. 3.2)
Stage 3: Generation (Sec. 3.2)
Prompt
Prompt
Gaussian
Figure 3: Overview of FreeGraftor. First, we construct a collage based on the given text prompt and the reference subject image
(Stage 1). Next, we invert this collage and record its diffusion trajectory (Stage 2). Finally, using the inverted noise as the initial
latent representation, our FreeGraftor synthesizes the output image through iterative denoising. During this process, the
Semantic-aware Feature Grafting (SAFG) module integrates features from the collage to ensure alignment with the reference
subject (Stage 3).
where 𝐻img,ref
𝑖
and 𝐻img,gen
𝑗
denote features of the 𝑖-th reference
patch and 𝑗-th generated patch at the attention layer input. To
ensure reliable matches, we apply two filtering strategies:
i. Similarity Threshold Filtering. For each reference patch
𝑖, if its maximum cosine similarity 𝑠𝑖,𝑚(𝑖) falls below a predefined
threshold𝜏, the match is discarded. This is represented by the binary
mask:
𝑀ref, sim
𝑖
=
(
1,
𝑠𝑖,𝑚(𝑖) ≥𝜏
0,
𝑠𝑖,𝑚(𝑖) < 𝜏
(5)
ii. Cycle Consistency Filtering. Following DreamMatcher[24],
for each reference patch 𝑖(satisfying 𝑀ref, pre
𝑖
= 1), we find the most
similar reference patch 𝑘to its matched generated patch 𝑚(𝑖):
𝑚−1(𝑚(𝑖)) = arg
max
𝑘:𝑀ref,pre
𝑘
=1
𝐻img,gen
𝑚(𝑖)
· 𝐻img,ref
𝑘
∥𝐻img,gen
𝑚(𝑖)
∥2∥𝐻img,ref
𝑘
∥2
(6)
Let pref
𝑖
and pref
𝑚−1(𝑚(𝑖)) denote the 2D coordinates of patch 𝑖and
its reverse-matched counterpart. Matches are rejected if their L2
distance exceeds 𝛿:
𝑑𝑖= ∥pref
𝑖
−pref
𝑚−1(𝑚(𝑖)) ∥2,
𝑀ref, consi
𝑖
=
(
1,
𝑑𝑖≤𝛿
0,
𝑑𝑖> 𝛿
(7)
Combining these filters with the predefined mask yields the final
reference image mask:
𝑀ref = 𝑀ref, pre ⊙𝑀ref, sim ⊙𝑀ref, consi
(8)
3.2.2
Position-Constrained Attention Fusion. After obtaining the
filtered reference mask 𝑀ref, we transfer features from the retained
reference patches. A naive approach is to directly replace the fea-
tures of the corresponding patches. However, this may introduce
artifacts along object boundaries or even cause image distortion
(as shown in Figure 7). Unlike U-Net, which models positional rela-
tionships through convolutional operations, MM-DiT distinguishes
patch proximity and orientation via position embeddings (PE). This
enables us to align reference patches with their target positions in
the generated image by editing position embeddings.
Specifically, we concatenate both the keys and values of the refer-
ence patch to those of the generated patch, along with the position
embeddings corresponding to the generated patch associated with
the reference patch. This design enables the reference patch and
its corresponding generated patch to share identical positional in-
formation. Let 𝐾txt and 𝑉txt denote the text embeddings’ keys and
values, respectively; 𝐾img, gen and𝑉img, gen represent the generated
image’s keys and values; and 𝐾img, ref
𝑀
and 𝑉img, ref
𝑀
correspond to
the keys and values of the reference patches satisfying 𝑀ref. We
concatenate these as follows:
𝐾cat = [𝐾txt;𝐾img,gen;𝐾img,ref
𝑀
],
𝑉cat = [𝑉txt;𝑉img,gen;𝑉img,ref
𝑀
]
(9)
4
(0,0)
(0,1)
(1,0)
(1,1)
(0,0)
(0,1)
(1,0)
(1,1)
(0,0)
Text
Text
…
…
Q
PE
K
PE
Reference
Generated
match
copy
vanilla
T2T
attn
vanilla
I2T
attn
vanilla
T2I
attn
vanilla
I2I
attn
new
T2I
attn
new
I2I
attn
Figure 4: Illustration of the Proposed Semantic-aware Fea-
ture Grafting (SAFG) Module. For each patch in the reference
image, it first establishes semantic correspondence in the
generated image via feature matching. The module then (1)
concatenates the key and value pairs of the reference patch
with those of the corresponding generated patch, while (2)
copying and applying the position embedding from the gen-
erated patch to the reference patch’s key. This mechanism
enables effective positional information sharing between cor-
responding patches while maintaining semantic alignment.
For each retained reference patch 𝑖, we extract the corresponding
position embedding 𝑃𝐸img, gen
𝑚(𝑖)
based on the matching relationship
𝑚(𝑖). The new position embedding is then formed by concatenating:
𝑃𝐸cat = [𝑃𝐸txt; 𝑃𝐸img,gen; {𝑃𝐸img,gen
𝑚(𝑖)
| 𝑀ref
𝑖
= 1}]
(10)
The revised attention computation is formulated as:
˜𝑄= 𝑃𝐸⊙𝑄,
˜𝐾cat = 𝑃𝐸cat ⊙𝐾cat
(11)
𝐴+ = softmax
 ˜𝑄( ˜𝐾cat)⊤
√
𝑑

,
𝐻+
out = 𝐴+𝑉cat
(12)
Here, 𝐴+ represents the updated attention scores, and 𝐻+
out de-
notes the output of the attention layer. By integrating semantic
matching and the sharing of position embeddings, our Semantic-
Aware Feature Grafting module effectively binds features between
matching patches in the reference and generated images, facilitating
cross-image feature grafting.
3.3
Structure-Consistent Initialization
Attention-based feature migration can replicate the appearance
of reference subjects (e.g., color, texture) but may not preserve
structural details (e.g., shape, body proportions). Additionally, lay-
outs generated from Gaussian noise exhibit significant randomness,
leading to discrepancies in shape and size between generated sub-
jects and those in the reference image, potentially causing semantic
matching failures. To maintain structural integrity and enhance the
robustness of semantic matching, we propose a structure-consistent
initialization strategy. This involves creating a collage containing
the reference subject and then employing an inversion technique
to derive the initial noise required for the generation phase.
As illustrated in Figure 3, given a text prompt (e.g., "a cat sitting
on a chair wearing a birthday hat"), we first use a base text-to-image
model to generate a template image that faithfully represents the
described scene. To preserve the structure of the reference subject,
we replace the corresponding subject in the template image with
the reference subject. Specifically, a grounding model localizes
the target subject in the template, a segmentation model extracts
its mask, and an inpainting model removes the subject from the
template. The reference subject is then cropped (using the same
grounding and segmentation models), resized, and pasted into the
erased region to form a collage. This collage replaces the original
reference image as the new reference. Finally, we invert this collage
to obtain the initial noise for the generation phase and record the
diffusion trajectory during inversion to facilitate feature grafting.
To prevent the generated subject from overfitting the pose of the
reference subject, we introduce a dynamic feature dropout strategy.
Since structural layouts are predominantly determined during early
diffusion steps[1, 4], we reduce reference feature injection in early
steps while retaining late-step features to preserve visual details.
At timestep 𝑡, for each reference patch, we apply dropout with
probability 𝑝= 𝜔𝑡, where 𝜔∈[0, 1] controls dropout intensity.
The dropout mask is represented by a Bernoulli distribution:
𝑀drop
𝑖,𝑗
∼Bernoulli(1 −𝜔𝑡)
∀𝑖, 𝑗
(13)
The final reference mask is computed by applying dropout to
the reference image mask obtained from semantic matching (Sec.
3.2):
𝑀ref′ = 𝑀ref ⊙𝑀drop
(14)
This strategy preserves the reference subject’s structure while
aligning it with the text-driven layout and maintaining pose flexi-
bility. For multi-subject generation, we iteratively replace subjects
in the template and invert the multi-subject collage, avoiding simul-
taneous processing of multiple reference images and eliminating
computational overhead.
4
Experiments
4.1
Experimental Setup
4.1.1
Implementation Details. Our method is implemented on FLUX.1-
dev. For collage construction, we use Grounding DINO[20] to local-
ize subjects, SAM[16] to segment subjects, and LaMa[40] to erase
the original subject in the template image. For rectified flow inver-
sion, FireFlow[5] is employed as the inversion method. Template
generation, inversion, and final generation each take 25 diffusion
steps. During semantic matching, we set the similarity threshold
𝜏= 0.2, cycle consistency threshold 𝛿= 1.5, and dropout intensity
𝜔= 0.5.
4.1.2
Baselines. We compare our method with previous subject-
driven generation approaches, including zero-shot methods (IP-
Adapter[49], MS-Diffusion[45], OmniGen[47]) and training-free
methods (FreeCustom[6], DiptychPrompting[39]). Detailed descrip-
tions of these methods are provided in Appendix B.
5
IP-Adapter
FreeCustom
MS-Diffusion
OmniGen
DiptychPrompting
Ours
Reference
A can on top of a wooden floor.
A clock on the beach.
A backpack in the snow.
A bear plushie is playing skateboard on street.
A dog is swimming underwater.
Figure 5: Generation results with single reference subject using different methods. Our FreeGraftor achieves pixel-level detail
preservation (e.g., text and patterns) while allowing flexible text-guided control (e.g., poses of teddy bears and dogs).
4.1.3
Datasets and Evaluation. For qualitative evaluation, we use
reference images from DreamBench[35], CustomConcept101[17],
and Mix-of-Show[13] for subject-driven generation. For quantita-
tive evaluation, we generate 30×25×4=3,000 images using 30 sub-
jects and 25 prompt templates from DreamBench[35], with 4 ran-
dom seeds per subject-prompt pair. We evaluate performance from
two perspectives: text alignment and image alignment. For text
alignment, we report results using CLIP[29] and ImageReward[48]
metrics. For image alignment, we use GroundedSAM[31] to extract
segments of reference and generated subjects, respectively, and
compute similarity scores between these segments using CLIP[29]
and DINOv2[26].
Image-alignment
Text-alignment
Method
CLIP-I
DINO
CLIP-T
ImageReward
FreeCustom
0.8308
0.6107
0.3246
0.6223
IP-Adapter
0.8920
0.7696
0.3048
0.7444
MS-Diffusion
0.9023
0.7977
0.3254
1.3405
OmniGen
0.9113
0.8167
0.3256
1.4926
DiptychPrompting
0.8924
0.7971
0.3291
1.5728
Ours
0.9527
0.9042
0.3308
1.6481
Table 1: Quantitative Comparison of Subject-Driven Genera-
tion Methods. Without requiring model fine-tuning or addi-
tional training, our FreeGraftor significantly outperforms ex-
isting zero-shot and training-free approaches in both image-
alignment and text-alignment.
6
A car parked in front of a barn.
Reference
Ours
OmniGen
MS-Diffusion
FreeCustom
A cup is placed on a table.
A cat and a dog is sitting in the forest.
A man and a woman on street, drinking coffee, smiling.
Figure 6: Generation results with multiple reference subjects using different methods. Our FreeGraftor preserves the visual
details of all reference subjects while producing high-quality images well-aligned with the text.
Image-alignment
Text-alignment
Variant
CLIP-I
DINO
CLIP-T
ImageReward
w/o Init
0.8529
0.7138
0.3236
1.6320
w/o Graft
0.8463
0.6572
0.3306
1.6434
w/o Match
0.9504
0.8974
0.3232
1.5258
w/ replace
0.8476
0.7010
0.3174
1.4551
Ours
0.9527
0.9042
0.3308
1.6481
Table 2: Quantitative Comparison of Ablation Variants. Ab-
sence of structure-consistent initialization or feature grafting
leads to degraded image alignment, while omitting semantic
matching or replacing patch features results in deteriorated
text alignment.
4.2
Quantitative Results
As shown in Table 1, FreeGraftor demonstrates superior image
alignment and text alignment compared to prior methods. The sig-
nificant improvement in image alignment indicates our method
effectively preserves the identity of reference subjects: structure-
consistent initialization retains geometric priors, while feature graft-
ing successfully transfers fine-grained visual details. The high text
alignment stems from leveraging the base text-to-image model with-
out fine-tuning or training auxiliary modules, thereby preserving
its original capabilities. Furthermore, FreeGraftor employs a gen-
tle attention fusion strategy to inject subject features, minimizing
perturbations to vanilla attention computation.
4.3
Qualitative Results
4.3.1
Single-Subject Generation. Figure 5 presents the generation
results for single-subject generation using FreeGraftor and other
methods. FreeCustom and IP-Adapter fail to retain fine visual details
of the reference subject effectively. While MS-Diffusion, OmniGen,
and DiptychPrompting partially recover reference characteristics,
they struggle with pixel-level fidelity. FreeGraftor achieves pixel-
accurate detail preservation, such as logos on the can, numerals
on the clock, and text on the teddy bear. For non-rigid subjects
7
A cat is running in the grass.
A dog is standing in snow.
Reference
(a) w/o Init (b) w/o Graft (c) w/o Match
(e) Ours (Full)
(d) w/ Replace
Figure 7: Generation results of different ablation variants. (a)
Without structure-consistent initialization. (b) Without fea-
ture grafting. (c) Using direct key-value concatenation in at-
tention layers instead of semantic matching. (d) Using direct
patch feature replacement instead of position-constrained
attention fusion. (e) Our full method.
0.1
0.2
0.15
0.25
0.3
1.5
0.5
1.0
2.0
2.5
Prompt:
A backpack
on top of a
purple rug in a
forest.
Reference
τ
δ
Figure 8: Generation results under different similarity thresh-
old (𝜏) and cycle-consistency threshold (𝛿) settings. Higher 𝜏
and lower 𝛿lead to more stringent filtering.
A dog is running in the grass.
A teddy bear is standing on the stage.
Reference
ω
0
0.2
0.4
0.6
0.8
1.0
Reference
Figure 9: Generation results under different dropout intensity
(𝜔) settings. Higher 𝜔improves text alignment but degrades
subject consistency.
(e.g., dogs, plush toys), FreeGraftor generates natural pose varia-
tions aligned with text guidance, demonstrating the flexibility and
robustness of our feature grafting technique.
4.3.2
Multi-Subject Generation. As IP-Adapter and DiptychPrompt-
ing do not support multi-subject generation, we compare Free-
Graftor with FreeCustom, MS-Diffusion, and OmniGen. As shown
in Figure 6, FreeCustom exhibits severe attribute confusion (e.g.,
facial structures of the cat and the dog, hairstyles of the man and the
woman). MS-Diffusion and OmniGen show higher fidelity but still
lack visual faithfulness for small objects (e.g., the cup). FreeGraftor
preserves all reference details while enabling flexible text-guided
control, highlighting its effectiveness in multi-subject generation.
4.4
Ablation Study
4.4.1
Component Ablation. To validate the effectiveness of each
component in our method, we compare several ablation variants,
as shown in Figure 7. (a) When structure-consistent initialization
is removed, the generated images inherit the reference subject’s
appearance (e.g., colors, patterns) but fail to maintain structural
consistency (e.g., distorted body proportions in dogs). (b) With-
out feature grafting, the generated subjects lack identifiable cor-
respondence with the reference subjects. (c) If semantic matching
is omitted and reference keys and values are directly appended
during attention computation, the generated subjects exhibit rigid
poses and artifacts. (d) Replacing position-constrained attention
fusion with direct feature replacement introduces incoherent edges
and distortions. Quantitative results in Table 2 confirm that both
structure-consistent initialization and feature grafting significantly
improve image alignment, as they are critical for preserving subject
identity. Skipping semantic matching or positional fusion degrades
text alignment due to reduced image quality.
4.4.2
Hyperparameter Analysis. We conduct additional ablation
experiments to analyze the impact of similarity threshold 𝜏and
cycle consistency threshold 𝛿on semantic matching. Appropriate
thresholds alleviate mismatches (e.g., unnatural expansion of the
backpack), while overly strict thresholds (𝜏↑or 𝛿↓) discard critical
visual details (e.g., missing badges on the backpack, Figure 8).
We further verify the effectiveness of dropout in mitigating struc-
tural overfitting. As shown in Figure 9, moderate dropout enhances
pose flexibility while preserving subject identity, whereas excessive
dropout intensities (𝜔> 0.6) erode identity retention. These find-
ings demonstrate that through careful hyperparameter selection,
our method achieves an optimal balance between pose flexibility
and detail preservation.
5
Conclusion
We present FreeGraftor, a novel training-free framework for subject-
driven generation that bridges the critical gap between subject fi-
delity and computational efficiency. By leveraging Semantic-Aware
Feature Grafting, FreeGraftor seamlessly injects the appearance of
reference subjects into generated scenes. Additionally, our Structure-
Consistent Initialization strategy preserves reference geometry and
enhances the robustness of semantic matching. Extensive exper-
iments demonstrate that FreeGraftor achieves pixel-level detail
preservation and flexible text-guided control without requiring any
training or test-time optimization. Our framework can be seam-
lessly extended to multi-subject generation, faithfully retaining
visual details of all reference subjects while incurring no additional
computational overhead.
8
References
[1] Yuval Alaluf, Daniel Garibi, Or Patashnik, Hadar Averbuch-Elor, and Daniel
Cohen-Or. 2024. Cross-image attention for zero-shot appearance transfer. In
ACM SIGGRAPH 2024 Conference Papers. 1–12.
[2] Yuval Alaluf, Elad Richardson, Gal Metzer, and Daniel Cohen-Or. 2023. A neural
space-time representation for text-to-image personalization. ACM Transactions
on Graphics (TOG) 42, 6 (2023), 1–10.
[3] Moab Arar, Rinon Gal, Yuval Atzmon, Gal Chechik, Daniel Cohen-Or, Ariel
Shamir, and Amit H. Bermano. 2023. Domain-agnostic tuning-encoder for fast
personalization of text-to-image models. In SIGGRAPH Asia 2023 Conference
Papers. 1–10.
[4] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng
Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, et al. 2022. ediff-i:
Text-to-image diffusion models with an ensemble of expert denoisers. arXiv
preprint arXiv:2211.01324 (2022).
[5] Yingying Deng, Xiangyu He, Changwang Mei, Peisong Wang, and Fan Tang.
2024. FireFlow: Fast Inversion of Rectified Flow for Image Semantic Editing.
arXiv preprint arXiv:2412.07517 (2024).
[6] Ganggui Ding, Canyu Zhao, Wen Wang, Zhen Yang, Zide Liu, Hao Chen, and
Chunhua Shen. 2024. Freecustom: Tuning-free customized image generation
for multi-concept composition. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. 9089–9098.
[7] Ziyi Dong, Pengxu Wei, and Liang Lin. 2022. Dreamartist: Towards controllable
one-shot text-to-image generation via positive-negative prompt-tuning. arXiv
preprint arXiv:2211.11337 (2022).
[8] Dave Epstein, Allan Jabri, Ben Poole, Alexei Efros, and Aleksander Holynski.
2023. Diffusion self-guidance for controllable image generation. Advances in
Neural Information Processing Systems 36 (2023), 16222–16239.
[9] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller,
Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. 2024.
Scaling rectified flow transformers for high-resolution image synthesis. In Forty-
first international conference on machine learning.
[10] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal
Chechik, and Daniel Cohen-Or. 2022. An image is worth one word: Personalizing
text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618
(2022).
[11] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H Bermano, Gal Chechik, and Daniel
Cohen-Or. 2023. Encoder-based domain tuning for fast personalization of text-
to-image models. ACM Transactions on Graphics (TOG) 42, 4 (2023), 1–13.
[12] Sooyeon Go, Kyungmook Choi, Minjung Shin, and Youngjung Uh. 2024. Eye-for-
an-eye: Appearance transfer with semantic correspondence in diffusion models.
arXiv preprint arXiv:2406.07008 (2024).
[13] Yuchao Gu, Xintao Wang, Jay Zhangjie Wu, Yujun Shi, Yunpeng Chen, Zihan
Fan, Wuyou Xiao, Rui Zhao, Shuning Chang, Weijia Wu, et al. 2023. Mix-of-
show: Decentralized low-rank adaptation for multi-concept customization of
diffusion models. Advances in Neural Information Processing Systems 36 (2023),
15890–15902.
[14] Miao Hua, Jiawei Liu, Fei Ding, Wei Liu, Jie Wu, and Qian He. 2023. Dream-
tuner: Single image is enough for subject-driven generation. arXiv preprint
arXiv:2312.13691 (2023).
[15] Xuhui Jia, Yang Zhao, Kelvin CK Chan, Yandong Li, Han Zhang, Boqing Gong,
Tingbo Hou, Huisheng Wang, and Yu-Chuan Su. 2023. Taming encoder for zero
fine-tuning image customization with text-to-image diffusion models. arXiv
preprint arXiv:2304.02642 (2023).
[16] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura
Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al.
2023. Segment anything. In Proceedings of the IEEE/CVF international conference
on computer vision. 4015–4026.
[17] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan
Zhu. 2023. Multi-concept customization of text-to-image diffusion. In Proceedings
of the IEEE/CVF conference on computer vision and pattern recognition. 1931–1941.
[18] Black Forest Labs. 2024. FLUX. https://github.com/black-forest-labs/flux.
[19] Dongxu Li, Junnan Li, and Steven Hoi. 2023. Blip-diffusion: Pre-trained subject
representation for controllable text-to-image generation and editing. Advances
in Neural Information Processing Systems 36 (2023), 30146–30166.
[20] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing
Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. 2024. Grounding dino: Marry-
ing dino with grounded pre-training for open-set object detection. In European
Conference on Computer Vision. Springer, 38–55.
[21] Zhiheng Liu, Yifei Zhang, Yujun Shen, Kecheng Zheng, Kai Zhu, Ruili Feng,
Yu Liu, Deli Zhao, Jingren Zhou, and Yang Cao. 2023. Cones 2: Customizable
image synthesis with multiple subjects. In Proceedings of the 37th International
Conference on Neural Information Processing Systems. 57500–57519.
[22] Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and Jian Zhang. 2023.
Dragondiffusion: Enabling drag-style manipulation on diffusion models. arXiv
preprint arXiv:2307.02421 (2023).
[23] Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and Jian Zhang. 2024.
Diffeditor: Boosting accuracy and flexibility on diffusion-based image editing. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.
8488–8497.
[24] Jisu Nam, Heesu Kim, DongJae Lee, Siyoon Jin, Seungryong Kim, and Seung-
gyu Chang. 2024.
Dreammatcher: appearance matching self-attention for
semantically-consistent text-to-image personalization. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. 8100–8110.
[25] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin,
Bob McGrew, Ilya Sutskever, and Mark Chen. 2021. Glide: Towards photorealistic
image generation and editing with text-guided diffusion models. arXiv preprint
arXiv:2112.10741 (2021).
[26] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec,
Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-
Nouby, et al. 2023. Dinov2: Learning robust visual features without supervision.
arXiv preprint arXiv:2304.07193 (2023).
[27] William Peebles and Saining Xie. 2023. Scalable diffusion models with transform-
ers. In Proceedings of the IEEE/CVF international conference on computer vision.
4195–4205.
[28] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas
Müller, Joe Penna, and Robin Rombach. 2023. Sdxl: Improving latent diffusion
models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952
(2023).
[29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
et al. 2021. Learning transferable visual models from natural language supervision.
In International conference on machine learning. PMLR, 8748–8763.
[30] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
2022. Hierarchical text-conditional image generation with clip latents. arXiv
preprint arXiv:2204.06125 1, 2 (2022), 3.
[31] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen,
Xinyu Huang, Yukang Chen, Feng Yan, et al. 2024. Grounded sam: Assembling
open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159
(2024).
[32] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn
Ommer. 2022. High-resolution image synthesis with latent diffusion models. In
Proceedings of the IEEE/CVF conference on computer vision and pattern recognition.
10684–10695.
[33] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolu-
tional networks for biomedical image segmentation. In Medical image computing
and computer-assisted intervention–MICCAI 2015: 18th international conference,
Munich, Germany, October 5-9, 2015, proceedings, part III 18. Springer, 234–241.
[34] Litu Rout, Yujia Chen, Nataniel Ruiz, Constantine Caramanis, Sanjay Shakkottai,
and Wen-Sheng Chu. 2024. Semantic image inversion and editing using rectified
stochastic differential equations. arXiv preprint arXiv:2410.10792 (2024).
[35] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and
Kfir Aberman. 2023. Dreambooth: Fine tuning text-to-image diffusion models for
subject-driven generation. In Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition. 22500–22510.
[36] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch,
Neal Wadhwa, Michael Rubinstein, and Kfir Aberman. 2024. Hyperdreambooth:
Hypernetworks for fast personalization of text-to-image models. In Proceedings
of the IEEE/CVF conference on computer vision and pattern recognition. 6527–6536.
[37] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L
Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim
Salimans, et al. 2022. Photorealistic text-to-image diffusion models with deep
language understanding. Advances in neural information processing systems 35
(2022), 36479–36494.
[38] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. 2024. Instantbooth: Personal-
ized text-to-image generation without test-time finetuning. In Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition. 8543–8552.
[39] Chaehun Shin, Jooyoung Choi, Heeseung Kim, and Sungroh Yoon. 2024. Large-
Scale Text-to-Image Model with Inpainting is a Zero-Shot Subject-Driven Image
Generator. arXiv preprint arXiv:2411.15466 (2024).
[40] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova,
Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park,
and Victor Lempitsky. 2022. Resolution-robust large mask inpainting with fourier
convolutions. In Proceedings of the IEEE/CVF winter conference on applications of
computer vision. 2149–2159.
[41] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath
Hariharan. 2023. Emergent correspondence from image diffusion. Advances in
Neural Information Processing Systems 36 (2023), 1363–1389.
[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing systems 30 (2017).
[43] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir Aberman. 2023. p+:
Extended textual conditioning in text-to-image generation.
arXiv preprint
arXiv:2303.09522 (2023).
9
[44] Jiangshan Wang, Junfu Pu, Zhongang Qi, Jiayi Guo, Yue Ma, Nisha Huang, Yuxin
Chen, Xiu Li, and Ying Shan. 2024. Taming rectified flow for inversion and
editing. arXiv preprint arXiv:2411.04746 (2024).
[45] Xierui Wang, Siming Fu, Qihan Huang, Wanggui He, and Hao Jiang. 2024. Ms-
diffusion: Multi-subject zero-shot image personalization with layout guidance.
arXiv preprint arXiv:2406.07209 (2024).
[46] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo.
2023. Elite: Encoding visual concepts into textual embeddings for customized
text-to-image generation. In Proceedings of the IEEE/CVF International Conference
on Computer Vision. 15943–15953.
[47] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran
Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. 2024. Omnigen:
Unified image generation. arXiv preprint arXiv:2409.11340 (2024).
[48] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang,
and Yuxiao Dong. 2024. Imagereward: Learning and evaluating human prefer-
ences for text-to-image generation. Advances in Neural Information Processing
Systems 36 (2024).
[49] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. 2023. Ip-adapter: Text
compatible image prompt adapter for text-to-image diffusion models. arXiv
preprint arXiv:2308.06721 (2023).
10
A
Pseudo Code
Algorithm 1: FreeGraftor Pipeline
Input: Reference image 𝐼ref, text prompt 𝑃
Output: Generated image 𝐼gen
Stage 1: Collage Construction
1. Template Generation
𝐼tmp ←T2I(𝑃) ;
// Base text-to-image model
2. Template Subject Processing
𝐵tmp ←GroundingDINO(𝐼tmp, 𝑃) ;
// Bounding box
𝑀tmp ←SAM(𝐼tmp, 𝐵tmp) ;
// Segmentation mask
𝐼erase ←LaMa(𝐼tmp, 𝑀tmp) ;
// Inpainting
3. Reference Subject Extraction
𝐵ref ←GroundingDINO(𝐼ref, "subject")
𝑀ref ←SAM(𝐼ref, 𝐵ref)
𝐼crop ←CropWithMask(𝐼ref, 𝑀ref) ;
// Mask-based
cropping
4. Collage Assembly
𝐼collage ←ResizeAndPaste(𝐼erase, 𝐼crop, 𝐵tmp) ; // Scale
& align
Stage 2: Inversion
5. 𝜖init, F ref ←FireFlow(𝐼collage) ;
// Noise & feature
extraction
Stage 3: Generation
6. 𝐼gen ←SAFG(𝜖init, F ref, 𝑃) ;
// Feature grafting
return 𝐼gen
B
Baselines
In both qualitative and quantitative comparisons, we only com-
pared our FreeGraftor with training-free approaches and publicly
available zero-shot models. Detailed descriptions of these methods
are provided below.
• IP-Adapter[49] introduces a lightweight module that projects
visual features from an image encoder directly into the cross-
attention layers of a pre-trained text-to-image diffusion model.
This enables the seamless integration of image prompts with
textual conditions without the need for extra fine-tuning.
• MS-Diffusion[45] tackles multi-subject personalization by
using layout guidance and grounding tokens to ensure each
subject’s details are preserved and positioned in defined re-
gions. Its specialized cross-attention mechanism effectively
minimizes subject conflicts while achieving high visual fi-
delity in a zero-shot setting.
• OmniGen[47] unifies diverse image generation tasks, in-
cluding text-to-image synthesis, editing, subject-driven gen-
eration and even classical vision operations—within a single
diffusion framework based solely on a VAE and transformer.
Trained on a large-scale unified dataset, it avoids additional
encoders and transfers knowledge robustly across tasks and
domains.
• FreeCustom[6] provides a tuning-free approach for gener-
ating multi-concept images by leveraging a multi-reference
self-attention mechanism that integrates features from dif-
ferent reference images. With its weighted masking strategy
Algorithm 2: Semantic-Aware Feature Grafting
Input: Reference features F ref, noise 𝜖init, text prompt 𝑃
Output: Modified attention features 𝐻+
out
Semantic Matching
1. For each timestep 𝑡:
foreach reference patch 𝑖∈𝑀ref, pre do
foreach generated patch 𝑗do
𝑠𝑖𝑗←
𝐻img,ref
𝑖
·𝐻img,gen
𝑗
∥𝐻img,ref
𝑖
∥2∥𝐻img,gen
𝑗
∥2
𝑚(𝑖) ←arg max𝑗𝑠𝑖𝑗
2. Apply filtering:
𝑀ref ←ThresholdFilter(𝑠𝑖𝑗,𝜏) ⊙CycleConsistency(𝑑𝑖,𝛿)
Position-Constrained Fusion
3. Concatenate keys/values:
𝐾cat ←[𝐾txt;𝐾img,gen;𝐾img,ref
𝑀
]
𝑉cat ←[𝑉txt;𝑉img,gen;𝑉img,ref
𝑀
]
4. Bind positional embeddings:
𝑃𝐸cat ←Concat(𝑃𝐸txt, 𝑃𝐸img,gen, {𝑃𝐸img,gen
𝑚(𝑖)
})
5. Compute revised attention:
˜𝑄←𝑃𝐸⊙𝑄
˜𝐾cat ←𝑃𝐸cat ⊙𝐾cat
𝐴+ ←Softmax( ˜𝑄( ˜𝐾cat)⊤/
√
𝑑)
𝐻+
out ←𝐴+𝑉cat
return 𝐻+
out
to suppress extraneous details, it efficiently composes high-
quality images without further training.
• DiptychPrompting[39] reinterprets subject-driven gener-
ation as a text-conditioned inpainting task using a diptych
layout, where the left panel holds a reference subject (with
background removed) and the right panel is generated from a
textual prompt. By enhancing the attention between panels,
it captures fine-grained subject details in a zero-shot manner.
C
Additional Experiments
C.1
Efficiency Analysis
Table 3: Efficiency Comparison of Subject-Driven Generation
Methods
Method
Base Model
Time (s)
Memory (MiB)
FreeCustom
Stable Diffusion v1.5
22.02
14670
IP-Adapter
FLUX.1-dev
9.58
40788
MS-Diffusion
SDXL
8.89
14654
OmniGen
Phi-3
46.13
12638
DiptychPrompting
FLUX.1-dev
52.11
46858
Ours
FLUX.1-dev
42.62
41106
We compare the efficiency of FreeGraftor with test-time subject-
driven generation methods (including zero-shot and training-free
approaches) on an NVIDIA A40 GPU. All methods generate 512×512
resolution images under identical conditions, with measurements
of peak GPU memory consumption and inference time.
11
As shown in Table 3, among methods built on the FLUX.1-dev
base model, FreeGraftor significantly outperforms DiptychPrompt-
ing in both time and memory efficiency, while demonstrating com-
parable memory usage to IP-Adapter. Across all evaluated methods,
OmniGen achieves the lowest memory footprint but requires sub-
stantially longer inference time. MS-Diffusion attains the fastest
inference speed and second-lowest memory consumption after
OmniGen, though these efficiency gains come at the expense of
reduced subject fidelity and degraded visual quality. Notably, our
FreeGraftor maintains an optimal balance between generation qual-
ity and computational efficiency, delivering robust performance
without compromising subject faithfulness or visual details.
Method
Text-alignment
Image-alignment
FreeCustom
1.813
1.638
IP-Adapter
3.090
2.183
MS-Diffusion
3.458
2.905
OmniGen
3.905
3.368
DiptychPrompting
4.633
4.280
Ours
4.815
4.773
Table 4: Results of user study. Our method is the most pre-
ferred by users, achieving the highest ratings in both text
and image alignment.
C.2
User Study
We conduct a user study assessing human preference of text-to-
image correspondence and subject identity consistency in generated
results. Participants are presented with images synthesized by com-
peting methods alongside their corresponding textual prompts and
reference subjects. For text alignment evaluation, participants rate
the semantic consistency between generated images and prompts
on a 5-point Likert scale (1: minimal correspondence, 5: perfect
alignment). For image alignment, they evaluate the visual similar-
ity between generated subjects and reference counterparts using
an identical scale, with higher scores indicating stronger identity
preservation. The study involves a comprehensive evaluation com-
prising 40 participants, each assessing 60 randomly sampled images.
As detailed in Table 4, our method demonstrated significant superi-
ority over baselines in both metrics, validating its effectiveness in
balancing textual faithfulness and subject fidelity.
C.3
More visualization results
Figure 10 presents additional qualitative results comparing our
method with the baselines. As shown in these results, our method
generates text-aligned novel scenes while maintaining subject fi-
delity. Our FreeGraftor consistently preserves pixel-level visual
details, even for challenging text and patterns, highlighting its ef-
fectiveness in subject-driven generation.
Extended single-subject generation results with various prompts
are visualized in Figure 11. Our method demonstrates comprehen-
sive prompt adaptability, dynamically modulating subject attributes,
poses, and scene configurations to synthesize high-fidelity images
that achieve both visual harmony and precise semantic alignment
with input textual descriptions.
D
Limitations
While FreeGraftor achieves robust subject-driven generation in
most scenarios, its performance partially depends on the reliabil-
ity of external models (e.g., GroundingDINO, SAM, LaMa) during
the collage construction stage. For instance, rare failures in sub-
ject detection, imperfect segmentation masks, or residual artifacts
from inpainting could propagate to subsequent stages, occasionally
leading to suboptimal initialization. Additionally, the current im-
plementation builds upon the FLUX.1 base model, which demands
significant GPU memory (e.g., 40+ GB for 1024×1024 resolution),
limiting accessibility for users with constrained hardware. While
this choice ensures compatibility with state-of-the-art architectures,
adapting our framework to lighter MM-DiT variants or exploring
model distillation and quantization techniques could broaden its
practical applicability.
E
Social Impacts
The proposed FreeGraftor framework has the potential to democra-
tize personalized content creation by enabling users to synthesize
high-fidelity images of specific subjects without requiring extensive
training. This capability could empower creators in fields such as
digital art, advertising, and education to rapidly prototype visual
concepts while preserving intricate details (e.g., logos, textures),
thereby lowering barriers to professional-grade content generation.
However, the ease of transferring visual identities also raises con-
cerns about misuse, such as generating deceptive imagery for dis-
information campaigns or unauthorized replication of copyrighted
subjects. Additionally, while our method avoids the computational
costs of per-subject fine-tuning, the reliance on large pre-trained
diffusion models still necessitates significant energy consumption
during inference, highlighting the need for ongoing research into
efficient deployment strategies. We advocate for ethical guidelines
that balance creative freedom with safeguards against malicious ap-
plications, such as watermarking synthesized content or restricting
access to sensitive reference data.
12
IP-Adapter
FreeCustom
MS-Diffusion
OmniGen
DiptychPrompting
Ours
Reference
A backpack on a cobblestone street.
A bowl on top of a dirt road.
A sneaker on top of a wooden floor.
A dog on top of pink fabric.
A robot toy in the jungle.
A sloth plushie with a tree and autumn leaves in the background.
Figure 10: More qualitative comparisons
13
A dog with angel wings.
Reference
A dog is playing with a ball.
A dog is eating a banana.
A dog wearing a birthday hat.
A dog in a box.
A dog is getting a haircut.
A dog wearing headphones.
A dog wearing pink glasses.
A purple dog on the floor.
A dog in the rain.
A dog in a doghouse.
A dog wearing a rainbow scarf.
A dog is riding a skateboard.
A dog in a bucket.
A dog wearing sunglasses.
A dog wearing a yellow shirt.
Figure 11: Single-subject generation results
14
