June 2022, 29(3):

25 - 33

The Journal of China Universities of Posts and

詨詨詨詨詨詨詨詨詨詨詨

Telecommunications

Multi鄄level fusion with deep neural networks for

multimodal sentiment classification

Zhang Guangwei1, Zhao Bing2, Li Ruifan3(苣)

1. School of Computer Sciences, Beijing University of Posts and Communications, Beijing 100876, China

2. School of Science, Yanshan University, Qinhuangdao 066004, China

3. School of Artificial Intelligence, Beijing University of Posts and Communications, Beijing 100876, China

Abstract

The task of multimodal sentiment classification aims to associate multimodal information, such as images and texts with appropriate sentiment polarities. There are various levels that can affect human sentiment in visual and textual modalities. However, most existing methods treat various levels of features independently without having effective method for feature fusion. In this paper, we propose a multi鄄level fusion classification (MFC) model to predict the sentiment polarity based on the fusing features from different levels by exploiting the dependency among them. The proposed architecture leverages convolutional neural networks (CNNs) with multiple layers to extract levels of features in image and text modalities. Considering the dependencies within the low鄄level and high鄄level features, a bi鄄directional (Bi) recurrent neural network (RNN) is adopted to integrate the learned features from different layers in CNNs. In addition, a conflict detection module is incorporated to address the conflict between modalities. Experiments on the Flickr dataset demonstrate that the MFC method achieves comparable performance compared with strong baseline methods.

Keywords摇multimodal fusion, sentiment analysis, deep learning

Received date: 28鄄02鄄2022

Corresponding author: Li Ruifan, E鄄mail: rfli@ bupt. edu. cn

1摇Introduction

摇 Online social networks provide multiple forms of being available to their users. For instance, people can post a tweet attached with images or videos. Social networks play a more and more important role in our daily life for acquiring information and sharing experiences[1]. Meanwhile, online users love to

express their opinions on subjects they are interested

social media platforms, such as video, pictures, and audio. Interestingly, statistics have shown that the use of pictures in a tweet can increase the click rate, forwarding rate and collection rate of the tweet[3], which further encourages users to publish more visual content. 摇In recent years, deep neural networks have achieved remarkable performance in various fields, especially in compute vision[4 - 5] and natural language processing[6]. Inspired by the enormous success of deep learning, researches on sentiment analysis have applied deep learning algorithms. However, most of these mainly focused on one single modality of user content instead of the closely鄄related modalities. In fact, a large number of posted images do not contain any sentiment words in text at all; or the text sentiment is obvious but the image sentiment is inconspicuous. 摇 In this paper, we focus on the task of sentiment prediction using the joint textual and visual information of online posts. Deep neural networks employed in previous work[7 - 10] have been shown effective in solving the tasks of image or text sentiment analysis. In addition, the fusion of multimodal data cannot provide additional information with an increase in overall accuracy[11]. Therefore, to solve the challenging problem, we propose a feature fusion method based on multiple neural networks. Moreover, to further improve MFC method, we design a conflict detection module to extend our model, i. e. , rectified multi鄄level fusion classification ( R鄄MFC ). Experimental results demonstrate the effectiveness of the proposed method for the task of joint vision and text sentiment analysis.

2摇Related work

decision vector to obtain the final decision. Hybrid multimodal fusion[20 - 21] is the combination of both feature鄄level and decision鄄level fusion methods. Model鄄 level fusion[22 - 23] is a technique that uses the correlation between data observed under different modalities, with a relaxed fusion of the data. Classification鄄based fusion[24 - 25] uses a range of classification algorithms to classify the multimodal information into pre鄄defined classes. In the rule鄄based fusion methods[26 - 27], multimodal information is fused by statistical rule鄄based methods such as linear weighted fusion, majority voting and custom鄄defined rules. Estimation鄄based fusion methods[28 - 29] are usually employed to estimate the state of moving object using multimodal information, especially audio and video. 摇Previous work only utilized the high鄄level features for fusion, such as using the last outputs of different modal model layers[17]. Due to the complexity of emotions and the differences between text sentiment and visual sentiment, more and more work were proposed to exploit multiple levels features fusion. CNN have achieved good results in many areas, including visual sentiment[10, 30] and textual sentiment[31]. RNNs can effectively model the long鄄term dependency in sequential data. Rao et al. [32] utilized three different networks to capture different levels of visual features with high expense of parameters. Inspired by this work, we try to explicitly exploit the dependency between low鄄level features and high鄄level features to improve the sentiment classification model.

3摇MFC model

text feature extractor and bi鄄directional gated recurrent unit ( Bi鄄GRU) feature fusion. The input image and text are first fed to the two different CNN models to extract multiple levels of features at different branches. We expect that those features from different layers represent different parts of modals, such as line, color, texture, and object in image model, word,

subject in text model, which characterize different levels of features from the local view to global view. Bi鄄GRU fusion aims to integrate the different levels of features by exploiting the dependency between low鄄 level and high鄄level features. The integrated features from Bi鄄GRU model are concatenated for textual and visual sentiment classification.

Fig. 1摇Structure of the proposed MFC model

3. 1摇Image CNN with multiple branches

middle鄄level features. The image CNN used to extract image features in this paper has multiple branches, which are used to extract a variety of different levels of image features. The pre鄄processed image is sent to the pre鄄trained image CNN to obtain image features {vm} M

Table 1摇Details of image CNN

Layer Channel Kernel size/ Stride Respective field

Conv 1 64 11 伊11 / 4 11

Pooling 1 64 3 伊3 / 2 19

Branch 1 128 1 伊1 / 1 19

Conv 2 128 5 伊5 / 1 51

Pooling 2 128 3 伊3 / 2 67

Branch 2 128 1 伊1 / 1 67

Conv 3 256 3 伊3 / 1 99

Branch 3 128 1 伊1 / 1 99

Conv 4 512 3 伊3 / 1 131

Branch 4 128 1 伊1 / 1 131

Conv 5 512 3 伊3 / 1 163

Pool 5 512 3 伊3 / 2 195

Branch 5 128 1 伊1 / 1 195

3. 2摇Text CNN with multiple branches

摇 In the field of natural language processing, CNNs have been successfully applied. The advantage of CNN is that it can easily use filters to extract local features in sentences, and can reduce the influence of irrelevant words on the final result. The multi鄄branch CNN can also extract key features in different ranges. And the size of the convolution kernel can be arbitrarily changed as needed, which is more flexible. This paper uses CNN to extract text features, and also uses a multi鄄branch design. 摇Specifically, MFC uses the pre鄄processed dictionary to index the text to obtain { wk } K

k = 1 ( k is the word index), and then input these words into the embedding module to convert it into a word vector representation {ek} K

k = 1. Then the word vector representation of each textual description is sent to the text CNN, and the text feature vector {tm} M

dimension transformation and rectified linear activation. Those five branches from the convolutional layers and different levels of features extracted from multiple branches will also be fed into the Bi鄄GRU fusion module. We pre鄄train the text CNN on the task of text sentiment classification.

Table 2摇Details of text CNN

Layer Channel Kernel size/ Stride

Conv 1 64 3 伊3 / 1

Max鄄pooling in Branch 1 64 3 伊3 / 1

Mean鄄pooling in Branch 1 64 3 伊3 / 1

Conv 2 128 3 伊3 / 1

Max鄄pooling in Branch 2 128 3 伊3 / 1

Mean鄄pooling in Branch 2 128 3 伊3 / 1

Conv 3 256 3 伊3 / 1

Max鄄pooling in Branch 3 256 3 伊3 / 1

Mean鄄pooling in Branch 3 256 3 伊3 / 1

Conv 4 512 3 伊3 / 1

Max鄄pooling in Branch 4 512 3 伊3 / 1

Mean鄄pooling in Branch 4 512 3 伊3 / 1

Conv 5 512 3 伊3 / 1

Max鄄pooling in Branch 5 512 3 伊3 / 1

Mean鄄pooling in Branch 5 512 3 伊3 / 1

3. 3摇Multiple layer fusion

摇On the basis of the extracted features of image and text, the information interaction of image and text features is carried out through a Bi鄄GRU and the features are further extracted. We first use pre鄄trained image CNN and text CNN to extract different levels features, i. e. , image features {vm} M

m = 1 for image, text

features { tm } M

m = 1 for text. Here, M represents the branch number, and its default value is 5. Then we concatenate them in sequence, and feed them into Bi鄄 GRU fusion module as the inputs of different time steps. This process is formulated in Eq. (1). We use G to denote the gated recurrent unit (GRU), in which the arrow over G denotes the direction.

寅G([vm;tm] M

m = 1)

饮G([vm;tm] M

m = 1)

in which, outputs yL and yR of Bi鄄GRU are concatenated with the concatenation operation [ ·; ·], obtaining the final representation yB. Then we train the Bi鄄GRU module with a logistic regression classifier.

3郾4摇R鄄MFC module

摇In popular social networks, due to the randomness and subjectivity of blog posts, there could be sentimental conflicts between modalities. To solve this problem, we incorporate a conflict detection in the MFC model obtaining R鄄MFC. Specifically, before feature fusion, we firstly calculate the sentimental polarity of a single modality, and then, according to whether the conflict exits we choose different fusion strategies. We suppose that emotional conflicts between visual and textual modalities may be more complex. This situation needs to be addressed by the fusion of complex network. 摇The rectified conflict detection first uses a Sigmoid activation function to calculate the probability distribution. For the visual feature { vm} M

m = 1, we use

p兹v(·) under the parameter 兹v. For the textual feature

{tm} M

m = 1, we use p兹t(t) under the parameter 兹t. Then

we choose the fusion strategy according to whether distributions are consistent. This process is given as 鬃(·);摇if (p兹v(·) - 0郾5)(p兹t(·) - 0郾5) > 0

准(·); } 摇else

where 鬃(·) represents the feature fusion using the feature concatenation method, 准(·) represents the feature fusion using Bi鄄GRU. Through such a design, the relatively easy situation where the image and text have the same emotional tendency, is assigned to a simple feature splicing combined with a single鄄layer fully connected neural network. The complex task situation in which images and texts have conflicting sentiment across modalities, is assigned to a complex RNN fusion network.

4摇Experiments and results

performance of MFC with four baseline methods.

4. 1摇Dataset and metric

4. 2摇Implementation details

摇 For the textual part, we employ the classic pre鄄 trained Word2Vec model to obtain the distributed word representations. The Word2Vec is pre鄄trained on the Stanford Twitter Sentiment corpus, and has a fixed size of 200 dimensions. Words not in the pre鄄training dataset are initialized randomly. The size of embedded word matrix is 100 伊200, due to the maximum length of description is 100 words. For the visual part, the input images are first resized to 224 伊224 before feeding into the image CNN. The mini鄄batch size of 32 is adopted. In addition, adaptive weight decay (AdamW) optimization is adopted. The MFC model is trained on a workstation with i7鄄8700 central processing unit (CPU), 16 GB random access memory ( RAM), and NVDIA 1060 graphic processing unit (GPU).

4郾3摇Experimental baselines

adaptively selecting the modality having stronger sentiment information. 摇 7 ) Image鄄text interaction graph neural network (ITIGNN) [39]. ITIGNN is a graph neural network for image鄄text sentiment analysis. A text graph neural network of the text features and a pre鄄trained CNN of image features are used for image鄄text interaction graph network. 摇Furthermore, to verify the contributions of different components in MFC, we design different variants as follows. 摇 1 ) MFC ( Concat ). This method simply concatenates five branch outputs from image CNN and text CNN. 摇2) MFC (GRU). This method puts the five branch outputs of image CNN and text CNN into a uni鄄 directional GRU as the inputs of different time steps. 摇3) MFC (3 branches). This variant puts the first, the third, and the last branch outputs of image CNN and text CNN to a Bi鄄GRU as the inputs of different time steps. 摇4) MFC (4 branches). This variant puts the first, the third, the fourth, and the last branch outputs of image CNN and text CNN to a Bi鄄GRU as the inputs of different time steps. 摇 5) MFC ( Bi鄄LSTM). This method puts the five branch outputs of image CNN and text CNN to a Bi鄄 LSTM as the inputs of different time steps. 摇6) MFC. This method puts the five branch outputs of image CNN and text CNN to a Bi鄄GRU as the inputs of different time steps. 摇7) R鄄MFC. This method first performs the sentiment classification of image and text independently, then puts the five branch outputs of image CNN and text CNN to a Bi鄄GRU as the inputs of different time steps, if the two sentiments are different. Or else,we take the five branch outputs of image CNN and text CNN as inputs of a fully鄄connected neural network.

4. 4摇Results and analysis

the results of ablation studies. First, we observe that MFC outperforms DCN, CCR, DFC, DMAF, and AMGN approaches. This shows that the multi鄄level fusion strategy with multimodal information could improve the model蒺s performance. However, ITIGNN method achieves the best performance.

Table 3摇Experimental results on Flickr dataset

Method Accuracy Recall F1 score

Image CNN 0. 783 0. 799 0. 790

Text CNN 0. 712 0. 722 0. 715

DCN 0. 805 0. 817 0. 811

CCR 0. 810 0. 836 0. 820

DFC 0. 824 0. 851 0. 840

DMAF 0. 859 0. 845 0. 850

AMGN 0. 870 0. 862 0. 868

ITIGNN 0. 919 0. 917 0. 922

MFC (Concat) 0. 846 0. 857 0. 850

MFC (GRU) 0. 851 0. 870 0. 862

MFC (3 branches) 0. 870 0. 886 0. 877

MFC (4 branches) 0. 871 0. 890 0. 879

MFC (Bi鄄LSTM) 0. 873 0. 890 0. 880

R鄄MFC 0. 884 0. 890 0. 888

MFC 0. 885 0. 897 0. 891

Table 4摇Experimental results on human labeled

Flickr dataset

Method Accuracy Recall F1 score

Image CNN 0. 675 0. 710 0. 682

Text CNN 0. 633 0. 660 0. 647

DCN 0. 705 0. 721 0. 811

CCR 0. 691 0. 710 0. 696

DFC 0. 720 0. 732 0. 728

DMAF 0. 753 0. 751 0. 759

AMGN 0. 781 0. 785 0. 779

ITIGNN 0. 828 0. 820 0. 829

MFC (Concat) 0. 730 0. 800 0. 741

MFC (GRU) 0. 777 0. 793 0. 780

MFC (3 branches) 0. 730 0. 764 0. 739

MFC (4 branches) 0. 735 0. 729 0. 733

MFC (Bi鄄LSTM) 0. 773 0. 780 0. 775

R鄄MFC 0. 801 0. 799 0. 800

We suppose that ITIGNN uses a pre鄄trained CNN with graph neural networks for textual and visual analysis. In comparison, our MFC method has fewer scale in parameters. 摇 Furthermore, in these six variant models, the one with five branches and Bi鄄GRU fusion approach achieves the best performance. This verifies that better multi鄄level fusion could improve the multimodal sentiment classification performance. Furthermore, compared with MFC with 3 and 4 branches, the variant of 5 branches has a significant increase due to the non鄄 linear effect of fusion information and local鄄to鄄global features of high鄄level CNNs. MFC shows better performance compared with R鄄MFC. This could be caused by underestimated trade鄄off between the conflict detection and without detection in MFC. In addition, this also indicates that the fusing of multiple middle layers of two modalities improves the performance of sentiment prediction.

5摇Conclusions and future work

摇In this paper, we present a new fusion method for visual and textual sentiment analysis. Our MFC leverages different levels of features from multiple branches in image CNN and text CNN, and effectively integrates these features by exploiting the dependencies among them with the Bi鄄GRU approach. Extensive experimental results demonstrate that the proposed multi鄄level fusion method achieves comparable performance on multimodal sentiment analysis compared with strong baseline approaches. In the future, we will explore the effect of trade鄄off between conflict detection and without detection.

Acknowledgements

This work was supported in part by the National Key Research

and Development ( R&D ) Program of China

(2018YFB1403003).

References

[1]摇REN F J, WU Y. Predicting user鄄topic opinions in twitter with

[2]摇PENG L, CUI G, ZHUANG M Z, et al. What do seller manipulations of online product reviews mean to consumers. HKIBS / WPS / 070 - 1314. Hong Kong, China: Hong Kong Institute of Business Studies (HKIBS), 2014. [3]摇ASUR S, HUBERMAN B A. Predicting the future with social

media. Proceedings of the 2010 IEEE / WIC / ACM International Conference on Web Intelligence and Intelligent Agent Technology: Vol. 1, 2010, Aug 31 - Sept 3,Toronto, Canada. Piscataway, NJ, USA: IEEE, 2010: 492 - 499. [4]摇KRIZHEVSKY A, SUTSKEVER I, HINTON G E. ImageNet classification with deep convolutional neural networks. Advances in Neural Information Processing Systems, 2012, 25(2): 1097 - 1105. [5]摇LECUN Y, BOSER B, DENKE J S, et al. Backpropagation applied to handwritten zip code recognition. Neural Computation, 1989, 1(4): 541 - 551. [6]摇 GRAVES A, MOHAMED A R, HINTON G. Speech recognition with deep recurrent neural networks. Proceedings of the 2013 IEEE International Conference on Acoustics, Speech and Signal Processing ( ICASSP蒺13 ), 2013, May 26 - 31, Vancouver, Canada. Piscataway, NJ, USA: IEEE, 2013: 6645 - 6649. [7]摇SANTOS C D, GATTI M. Deep convolutional neural networks for

sentiment analysis of short texts. Proceedings of the 25th International Conference on Computational Linguistics: Technical Papers ( COLING蒺14 ), 2014, Aug 23 - 29, Dublin, Ireland. Dublin, Ireland: Dublin City University and Association for Computational Linguistics, 2014: 69 - 78. [8]摇KIM Y. Convolutional neural networks for sentence classification.

Proceedings of the 19th Conference on Empirical Methods in Natural Language Processing ( EMNLP蒺14), 2014, Oct 25 - 29, Doha, Qatar. Stroudsburg, PA, USA: Association for Computational Linguistics, 2014: 1746 - 1751. [9]摇XU C, CETINTAS S, LEE K C, et al. Visual sentiment prediction with deep convolutional neural networks. arXiv Preprint, arXiv:1411. 5731, 2014. [10]摇YOU Q Z, LUO J B, JIN H L, et al. Robust image sentiment

analysis using progressively trained and domain transferred deep networks. Proceedings of the 29th AAAI Conference on Artificial Intelligence ( AAAI蒺15 ), 2015, Jan 25 - 30, Austin, TX, USA. Menlo Park,CA, USA: American Association for Artificial Intelligence (AAAI), 2015: 381 - 388. [11]摇D蒺MELLO S K, KORY J. A review and meta鄄analysis of multimodal affect detection systems. ACM Computing Survey, 2015, 47(3): 1 - 36. [12]摇MONKARESI H, HUSSAIN M S, CALVO R A. Classification of

affects using head movement, skin color features and physiological signals. Proceedings of the 2012 IEEE International Conference on Systems, Man, and Cybernetics ( SMC蒺12 ), 2012, Oct 14 - 17, Seoul, Republic of Korea. Piscataway, NJ, USA: IEEE, 2012: 2664 - 2669. [13]摇PORIA S, CAMBRIA E, HUSSAIN A, et al. Towards an intelligent framework for multimodal affective data analysis. Neural Networks, 2015, 63: 104 - 116. [14]摇SARKAR C, BHATIA S, AGARWAL A, et al. Feature analysis

[15]摇WANG S F, ZHU Y C, WU G B, et al. Hybrid video emotional

tagging using users蒺EEG and video content. Multimedia Tools and Applications, 2014, 72(2): 1257 - 1283. [16]摇ALAM F, RICCARDI G. Predicting personality traits using multimodal information. Proceedings of the 2014 Workshop on Computational Personality Recognition (WCPR蒺14), 2014, Nov 7, Orlando, FL, USA. New York, NY, USA: ACM, 2014: 15 - 18. [17]摇CAI G Y, XIA B B. Convolutional neural networks for multimedia sentiment analysis. Natural Language Processing and Chinese Computing: Proceedings of the 4th CCF International Conference on Natural Language Processing and Chinese Computing (NLPCC蒺15), 2015, Oct 9 - 13, Nanchang, China. LNCS 9362. Berlin, Germany: Springer, 2015: 159 - 167. [18]摇DOBRI譒EK S, GAJ譒EK R, MIHELI 觺C F, et al. Towards efficient multi鄄modal emotion recognition. International Journal of Advanced Robotic Systems, 2013, 10(1): 1 - 10. [19]摇GLODEK M, REUTER S, SCHELS M, et al. Kalman filter

based classifier fusion for affective state recognition. Multiple Classifier Systems: Proceedings of the 11th International Workshop on Multiple Classifier Systems (MCS蒺13), 2013, May 15 - 17, Nanjing, China. LNIP 7872. Berlin, Germany: Springer, 2013: 85 - 94. [20]摇PORIA S, CAMBRIA E, GELBUKH A. Deep convolutional neural network textual features and multiple kernel learning for utterance鄄level multimodal sentiment analysis. Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 2015, Sept 17 - 21, Lisbon, Portugal. Stroudsburg, PA, USA: Association for Computational Linguistics, 2015: 2539 - 2544. [21]摇PORIA S, CAMBRIA E, HOWARD N, et al. Fusing audio,

visual and textual clues for sentiment analysis from multimodal content. Neurocomputing, 2016, 174 (Part A): 50 - 59. [22]摇BALTRU譒AITIS T, BANDA N, ROBINSON P. Dimensional affect recognition using continuous conditional random fields. Proceedings of the 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG蒺13), 2013, Apr 22 - 26, Shanghai, China. Piscataway, NJ, USA: IEEE, 2013: 1 - 8. [23]摇METALLINOU A, WOLLMER M, KATSAMANIS A, et al.

Context鄄sensitive learning for enhanced audiovisual emotion classification. IEEE Transactions on Affective Computing 2012, 3(2): 184 - 198. [24]摇ADAMS W H, IYENGAR G, LIN C Y, et al. Semantic indexing

of multimedia content using visual, audio, and text cues. EURASIP Journal on Advances in Signal Processing, 2003: 1 - 16. [25]摇NEFIAN A V, LIANG L H , PI X B, et al. Dynamic Bayesian

networks for audio鄄visual speech recognition. EURASIP Journal on Advances in Signal Processing, 2002: 1 - 15. [26]摇CORRADINI A, MEHTA M, BERNSEN N O, et al. Multimodal

input fusion in human鄄computer interaction. NATO Science Series Sub Series III: Computer and Systems Sciences 198. Odense, Denmark: University of Southern Denmark, 2005. [27]摇IYENGAR G, NOCK H J, NETI C. Audio鄄visual synchrony for

772. [28]摇NICKEL K, GEHRIG T, STIEFELHAGEN R, et al. A joint

particle filter for audio鄄visual speaker tracking. Proceedings of the 7th International Conference on Multimodal Interfaces (ICMI蒺05), 2005, Oct 4 - 6, Torento, Italy. New York, NY, USA: ACM, 2005: 61 - 68. [29]摇POTAMITIS I, CHEN H M, TREMOULIS G. Tracking of multiple moving speakers with multiple microphone arrays. IEEE Transactions on Speech and Audio Processing, 2004, 12 (5): 520 - 529. [30]摇CAMPOS V, JOU B, GIR魷鄄I鄄NIETO X. From pixels to sentiment: Fine鄄tuning CNNs for visual sentiment prediction. Image and Vision Computing, 2017, 65: 15 - 22. [31]摇WANG J, YU L C, LAI K R, et al. Dimensional sentiment

analysis using a regional CNN鄄LSTM model. Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics: Vol 2, Short Papers, 2016, Aug 7 - 12, Berlin, Germany. Stroudsburg, PA, USA: Association for Computational Linguistics, 2016: 225 - 230. [32]摇RAO T R, LI X X, XU M. Learning multi鄄level deep representations for image emotion classification. Neural Processing Letters, 2020, 51: 2043 - 2061. [33]摇 JOU B, CHEN T, PAPPAS N, et al. Visual affect around the world: a large鄄scale multilingual visual sentiment ontology. Proceedings of the 23rd ACM International Conference on Multimedia (MM蒺15), 2015, Oct 26 - 30, Brisbane, Australia. New York, NY, USA: ACM, 2015: 159 - 168.

[34]摇YOU Q Z, LUO J B, JIN H L, et al. Cross鄄modality consistent

regression for joint visual鄄textual sentiment analysis of social multimedia. Proceedings of the 9th ACM International Conference on Web Search and Data Mining (WSDM蒺16), 2016, Feb 22 - 25, San Francisco, CA, USA. New York, NY, USA: ACM, 2016: 13 - 22. [35]摇YU Y H, LIN H F, MENG J N, et al. Visual and textual

sentiment analysis of a microblog using deep convolutional neural networks. Algorithms, 2016, 9(2): 1 - 11. [36]摇CHEN X Y, WANG Y H, LIU Q J. Visual and textual sentiment

analysis using deep fusion convolutional neural networks. Proceedings of the 2017 IEEE International Conference on Image Processing ( ICIP蒺17 ), 2017, Sept 17 - 20, Beijing, China. Piscataway, NJ, USA: IEEE, 2017: 1557 - 1561. [37]摇HUANG F R, ZHANG X M, ZHAO Z H, et al. Image鄄text

sentiment analysis via deep multimodal attentive fusion. Knowledge鄄Based Systems, 2019, 167: 26 - 37. [38]摇HUANG F R, WEI K M, WENG J, et al. Attention鄄based modality鄄gated networks for image鄄text sentiment analysis. ACM Transactions on Multimedia Computing, Communications, and Applications, 2020, 16(3): 1 - 19. [39]摇LIAO W X, ZENG B, LIU J Q, et al. Image鄄text interaction

graph neural network for image鄄text sentiment analysis. Applied Intelligence, 2022, DOI:10. 1007 / s10489 - 021 - 02936 - 9.

(Editor: Luo Lang)

From p. 24

[23]摇 RAGHU A, RAGHU M, BENGIO S, et al. Rapid learning or feature reuse? towards understanding the effectiveness of MAML. Proceeding of the 8th International Conference on Learning Representations (ICLR蒺20), 2020, Apr 27 - 30, Addis Ababa, Ethiopia. 2020: 1 - 6. [24]摇 OH J, YOO H, KIM C H, et al. Boil: towards representation change for few鄄shot learning. Proceeding of the 9th International Conference on Learning Representations (ICLR蒺21), 2021, May 3 - 7, Vienna, Austria. 2021: 1 - 15. [25]摇LUO Y D, HUANG Z, ZHANG Z, et al. Learning from the

past: continual meta鄄learning with Bayesian graph neural networks. Proceeding of the 34th AAAI Conference on Artificial Intelligence ( AAAI蒺20), 2020, Feb 7 - 12, New York, NY, USA. Menlo Park, CA, USA: American Association for Artificial Intelligence (AAAI), 2020: 5021 - 5028. [26]摇SANTORO A, BARTUNOV S, BOTVINICK M, et al. Meta鄄

learning with memory鄄augmented neural networks. Proceeding of the 33rd International Conference on Machine Learning (ICML蒺16), 2016, Jun 19 - 24, New York, NY, USA. 2016: 1842 - 1850. [27]摇HUANG H X, ZHANG J J, ZHANG J, et al. Low鄄rank pairwise

alignment bilinear network for few鄄shot fine鄄grained image classification. IEEE Transactions on Multimedia, 2020, 23: 1666 - 1680.

[28]摇 WELINDER P, BRANSON S, MITA T, et al. Caltech鄄UCSD Birds 200. CNS鄄TR鄄2010鄄001. Pasadena, CA, USA: California Institute of Technology, 2010. [29]摇KRAUSE J, STARK M, DENG J, et al. 3D object representations for fine鄄grained categorization. Proceedings of the 2013 IEEE International Conference on Computer Vision Workshops ( ICCV蒺13), 2013, Dec 2 - 8, Sydney, Australia. Piscataway, NJ, USA: IEEE, 2013: 554 - 561. [30]摇KHOSLA A, JAYADEVAPRAKASH N, YAO B P, et al. Novel

dataset for fine鄄grained image categorization: Stanford Dogs. https:/ / people. csail. mit. edu / khosla/ papers/ fgvc2011. pdf. 2011. [31]摇RUSSAKOVSKY O, DENG J, SU H, et al. ImageNet large scale

visual recognition challenge. International Journal of Computer Vision, 2015, 115: 211 - 252. [32]摇ZHANG X T, QIANG Y T, SUNG F, et al. RelationNet2: deep

comparison columns for few鄄shot learning. Proceedings of the 2020 International Joint Conference on Neural Networks (IJCNN蒺20), 2020, Jul 19 - 24, Glasgow, UK. Piscataway, NJ, USA: IEEE, 2020: 1 - 15. [33]摇ZHANG H G, LI H D, KONIUSZ P. Multi鄄level second鄄order

few鄄shot learning. IEEE Transactions on Multimedia, 2022, Early Access Article, DOI:10. 1109 / TMM. 2022. 3142955.

(Editor: Luo Lang)