# src/05_generate_fusion_academic_profiles.py
import json
import os
from collections import Counter, defaultdict
from pathlib import Path
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from wordcloud import WordCloud
import networkx as nx
import pandas as pd
from datetime import datetime

from config import PROCESSED_DATA_DIR

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# è¾“å‡ºè·¯å¾„
PROFILES_DIR = PROCESSED_DATA_DIR / "fusion_academic_profiles"
PROFILES_DIR.mkdir(exist_ok=True)

# ç»“æœæ–‡ä»¶è·¯å¾„
FUSION_PROFILES_PATH = PROFILES_DIR / "fusion_academic_profiles.json"
DEEP_AUTHOR_ANALYSIS_PATH = PROFILES_DIR / "deep_author_analysis.json"
INNOVATION_LANDSCAPE_PATH = PROFILES_DIR / "innovation_landscape.json"
TECHNICAL_NETWORK_PATH = PROFILES_DIR / "technical_network.json"
VISUALIZATIONS_DIR = PROFILES_DIR / "visualizations"
VISUALIZATIONS_DIR.mkdir(exist_ok=True)


class FusionAcademicProfileGenerator:
    """èåˆå­¦æœ¯ç”»åƒç”Ÿæˆå™¨ - æ•´åˆæ¦‚è¦å±‚å’Œç»“æ„åŒ–å±‚ä¿¡æ¯"""

    def __init__(self):
        self.summaries = {}  # æ¦‚è¦å±‚ä¿¡æ¯
        self.structural_insights = {}  # ç»“æ„åŒ–ä¿¡æ¯
        self.fusion_profiles = {}

    def load_data_sources(self):
        """åŠ è½½ä¸¤ä¸ªæ•°æ®æº"""
        print("åŠ è½½æ•°æ®æº...")

        # åŠ è½½æ¦‚è¦å±‚æ•°æ®
        summaries_path = PROCESSED_DATA_DIR / "document_summaries.json"
        try:
            with open(summaries_path, 'r', encoding='utf-8') as f:
                self.summaries = json.load(f)
            print(f"æˆåŠŸåŠ è½½æ¦‚è¦å±‚æ•°æ®: {len(self.summaries)} ä¸ªæ–‡æ¡£")
        except FileNotFoundError:
            print("æ‰¾ä¸åˆ°document_summaries.jsonæ–‡ä»¶")
            return False
        except Exception as e:
            print(f"åŠ è½½æ¦‚è¦å±‚æ•°æ®å¤±è´¥: {e}")
            return False

        # åŠ è½½ç»“æ„åŒ–æ•°æ®
        structural_path = PROCESSED_DATA_DIR / "structural_insights" / "structural_insights.json"
        try:
            with open(structural_path, 'r', encoding='utf-8') as f:
                self.structural_insights = json.load(f)
            print(f"æˆåŠŸåŠ è½½ç»“æ„åŒ–æ•°æ®: {len(self.structural_insights)} ä¸ªæ–‡æ¡£")
        except FileNotFoundError:
            print("æ‰¾ä¸åˆ°structural_insights.jsonæ–‡ä»¶")
            return False
        except Exception as e:
            print(f"åŠ è½½ç»“æ„åŒ–æ•°æ®å¤±è´¥: {e}")
            return False

        # åˆ†ææ•°æ®è¦†ç›–æƒ…å†µ
        summary_docs = set(self.summaries.keys())
        structural_docs = set(self.structural_insights.keys())

        overlap = summary_docs & structural_docs
        only_summary = summary_docs - structural_docs
        only_structural = structural_docs - summary_docs

        print(f"æ•°æ®è¦†ç›–åˆ†æ:")
        print(f"   ä¸¤ç§æ•°æ®éƒ½æœ‰: {len(overlap)} ä¸ªæ–‡æ¡£")
        print(f"   ä»…æœ‰æ¦‚è¦æ•°æ®: {len(only_summary)} ä¸ªæ–‡æ¡£")
        print(f"   ä»…æœ‰ç»“æ„åŒ–æ•°æ®: {len(only_structural)} ä¸ªæ–‡æ¡£")
        print(f"   æ€»è®¡å¯ç”¨æ–‡æ¡£: {len(summary_docs | structural_docs)} ä¸ª")

        return True

    def clean_field_data(self, field_value, invalid_values=None):
        """æ¸…æ´—å­—æ®µæ•°æ®ï¼Œç§»é™¤æ— æ•ˆå€¼"""
        if invalid_values is None:
            invalid_values = ["åŸæ–‡æ— æ­¤ä¿¡æ¯", "åŸæ–‡æœªæ˜ç¡®æåŠ", "æœªåœ¨åŸæ–‡ä¸­æ˜ç¡®æåŠ", "", "æœªçŸ¥"]

        if not field_value:
            return []

        if isinstance(field_value, str):
            if field_value.strip() in invalid_values:
                return []
            return [field_value.strip()]

        elif isinstance(field_value, list):
            cleaned = []
            for item in field_value:
                if isinstance(item, str) and item.strip():
                    if item.strip() not in invalid_values:
                        cleaned.append(item.strip())
                elif isinstance(item, dict):
                    # å¤„ç†ç»“æ„åŒ–æ•°æ®ä¸­çš„å­—å…¸æ ¼å¼
                    cleaned.append(item)
            return cleaned

        return []

    def extract_authors_from_document(self, doc_id):
        """ä»ä¸¤ä¸ªæ•°æ®æºä¸­æå–ä½œè€…ä¿¡æ¯"""
        authors_info = {
            'authors': [],
            'institutions': [],
            'source': 'none'
        }

        # ä¼˜å…ˆä»æ¦‚è¦å±‚è·å–åŸºæœ¬ä½œè€…ä¿¡æ¯
        if doc_id in self.summaries:
            summary = self.summaries[doc_id]
            doc_type = summary.get('document_type', 'unknown')

            if doc_type == 'patent':
                authors = self.clean_field_data(summary.get('inventors', []))
            else:
                authors = self.clean_field_data(summary.get('authors', []))

            if authors:
                authors_info['authors'] = authors
                authors_info['source'] = 'summary'

        # ä»ç»“æ„åŒ–æ•°æ®ä¸­è¡¥å……æœºæ„ä¿¡æ¯
        if doc_id in self.structural_insights:
            structural = self.structural_insights[doc_id]
            metadata = structural.get('document_metadata', {})

            # å¦‚æœæ¦‚è¦å±‚æ²¡æœ‰ä½œè€…ä¿¡æ¯ï¼Œä»ç»“æ„åŒ–æ•°æ®è·å–
            if not authors_info['authors']:
                if 'authors' in metadata:
                    authors = self.clean_field_data(metadata.get('authors', []))
                elif 'inventors' in metadata:
                    authors = self.clean_field_data(metadata.get('inventors', []))
                else:
                    authors = self.clean_field_data(metadata.get('authors_or_creators', []))

                if authors:
                    authors_info['authors'] = authors
                    authors_info['source'] = 'structural'

            # è·å–æœºæ„ä¿¡æ¯
            institutions = self.clean_field_data(metadata.get('institutions', []))
            if institutions:
                authors_info['institutions'] = institutions

        return authors_info

    def analyze_deep_author_profiles(self):
        """æ·±åº¦åˆ†æç ”ç©¶è€…ç”»åƒ"""
        print(" æ·±åº¦åˆ†æç ”ç©¶è€…ç”»åƒ...")

        author_profiles = defaultdict(lambda: {
            'basic_info': {
                'total_papers': 0,
                'document_types': Counter(),
                'institutions': set(),
                'collaboration_count': 0
            },
            'research_focus': {
                'domains': Counter(),
                'methods': Counter(),
                'concepts': Counter(),
                'keywords': Counter()
            },
            'innovation_analysis': {
                'stated_contributions': [],
                'technical_novelty': [],
                'limitations_acknowledged': [],
                'datasets_used': set(),
                'evaluation_metrics': set()
            },
            'technical_relationships': {
                'builds_upon': Counter(),
                'compared_with': Counter(),
                'extends': Counter()
            },
            'collaboration_network': {
                'collaborators': set(),
                'institutional_networks': set()
            },
            'paper_list': []
        })

        collaboration_pairs = Counter()

        # éå†æ‰€æœ‰æ–‡æ¡£
        all_docs = set(self.summaries.keys()) | set(self.structural_insights.keys())

        for doc_id in all_docs:
            # è·å–ä½œè€…ä¿¡æ¯
            authors_info = self.extract_authors_from_document(doc_id)
            authors = authors_info['authors']

            if not authors:
                continue

            # ä»æ¦‚è¦å±‚è·å–åŸºç¡€ä¿¡æ¯
            summary = self.summaries.get(doc_id, {})
            doc_type = summary.get('document_type', 'unknown')

            # ä»ç»“æ„åŒ–å±‚è·å–è¯¦ç»†ä¿¡æ¯
            structural = self.structural_insights.get(doc_id, {})

            for author in authors:
                profile = author_profiles[author]

                # åŸºç¡€ä¿¡æ¯
                profile['basic_info']['total_papers'] += 1
                profile['basic_info']['document_types'][doc_type] += 1
                profile['basic_info']['institutions'].update(authors_info['institutions'])
                profile['paper_list'].append(doc_id)

                # ç ”ç©¶é‡ç‚¹ï¼ˆèåˆä¸¤ä¸ªæ•°æ®æºï¼‰
                self._extract_research_focus(profile['research_focus'], summary, structural, doc_type)

                # åˆ›æ–°åˆ†æï¼ˆä¸»è¦æ¥è‡ªç»“æ„åŒ–æ•°æ®ï¼‰
                self._extract_innovation_analysis(profile['innovation_analysis'], structural, doc_type)

                # æŠ€æœ¯å…³ç³»ï¼ˆä¸»è¦æ¥è‡ªç»“æ„åŒ–æ•°æ®ï¼‰
                self._extract_technical_relationships(profile['technical_relationships'], structural)

            # åˆä½œå…³ç³»åˆ†æ
            if len(authors) > 1:
                for i, author1 in enumerate(authors):
                    author_profiles[author1]['basic_info']['collaboration_count'] += len(authors) - 1
                    for author2 in authors[i + 1:]:
                        pair = tuple(sorted([author1, author2]))
                        collaboration_pairs[pair] += 1
                        author_profiles[author1]['collaboration_network']['collaborators'].add(author2)
                        author_profiles[author2]['collaboration_network']['collaborators'].add(author1)

                # æœºæ„ç½‘ç»œ
                institutions = authors_info['institutions']
                for author in authors:
                    author_profiles[author]['collaboration_network']['institutional_networks'].update(institutions)

        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼å¹¶è®¡ç®—è¡ç”ŸæŒ‡æ ‡
        serializable_profiles = self._process_author_profiles(author_profiles, collaboration_pairs)

        return serializable_profiles

    def _extract_research_focus(self, focus_dict, summary, structural, doc_type):
        """æå–ç ”ç©¶é‡ç‚¹ä¿¡æ¯"""
        # ä»æ¦‚è¦å±‚è·å–é¢†åŸŸå’Œæ–¹æ³•
        if doc_type == 'patent':
            domains = self.clean_field_data(summary.get('application_domain', ''))
            methods = self.clean_field_data(summary.get('technical_solution', ''))
        else:
            domains = self.clean_field_data(summary.get('main_topic', ''))
            methods = self.clean_field_data(summary.get('methodology', ''))

        concepts = self.clean_field_data(summary.get('technical_concepts', []))
        keywords = self.clean_field_data(summary.get('keywords', []))

        # æ›´æ–°è®¡æ•°
        for domain in domains:
            focus_dict['domains'][domain] += 1
        for method in methods:
            focus_dict['methods'][method] += 1
        for concept in concepts:
            focus_dict['concepts'][concept] += 1
        for keyword in keywords:
            focus_dict['keywords'][keyword] += 1

        # ä»ç»“æ„åŒ–æ•°æ®è¡¥å……
        if structural:
            # ä»ä¸åŒæ–‡æ¡£ç±»å‹çš„ç»“æ„åŒ–æ•°æ®ä¸­æå–
            if doc_type == 'experimental_paper':
                exp_setup = structural.get('experimental_setup', {})
                datasets = exp_setup.get('datasets_used', [])
                for dataset in datasets:
                    if isinstance(dataset, dict):
                        name = dataset.get('dataset_name', '')
                        if name and name != 'åŸæ–‡æ— æ­¤ä¿¡æ¯':
                            focus_dict['concepts'][f"æ•°æ®é›†:{name}"] += 1

    def _extract_innovation_analysis(self, innovation_dict, structural, doc_type):
        """æå–åˆ›æ–°åˆ†æä¿¡æ¯"""
        if not structural:
            return

        # æå–è´¡çŒ®å£°æ˜
        innovation_analysis = structural.get('innovation_analysis', {})
        contributions = self.clean_field_data(innovation_analysis.get('stated_contributions', []))
        novelty = self.clean_field_data(innovation_analysis.get('stated_novelty', []))

        innovation_dict['stated_contributions'].extend(contributions)
        innovation_dict['technical_novelty'].extend(novelty)

        # ç†è®ºæ€§è®ºæ–‡çš„ç‰¹æ®Šå¤„ç†
        if doc_type == 'theoretical_paper':
            theoretical_contrib = structural.get('theoretical_contributions', {})
            math_contrib = self.clean_field_data(theoretical_contrib.get('mathematical_contributions', []))
            innovation_dict['technical_novelty'].extend(math_contrib)

        # å±€é™æ€§
        limitations = structural.get('limitations', {})
        if limitations:
            ack_limitations = self.clean_field_data(limitations.get('acknowledged_limitations', []))
            innovation_dict['limitations_acknowledged'].extend(ack_limitations)

        # å®éªŒä¿¡æ¯
        if doc_type == 'experimental_paper':
            exp_setup = structural.get('experimental_setup', {})
            datasets = exp_setup.get('datasets_used', [])
            metrics = exp_setup.get('evaluation_metrics', [])

            for dataset in datasets:
                if isinstance(dataset, dict):
                    name = dataset.get('dataset_name', '')
                    if name and name != 'åŸæ–‡æ— æ­¤ä¿¡æ¯':
                        innovation_dict['datasets_used'].add(name)

            for metric in self.clean_field_data(metrics):
                innovation_dict['evaluation_metrics'].add(metric)

    def _extract_technical_relationships(self, tech_rel_dict, structural):
        """æå–æŠ€æœ¯å…³ç³»ä¿¡æ¯"""
        if not structural:
            return

        tech_relationships = structural.get('technical_relationships', {})

        # åŸºäºçš„æ–¹æ³•
        base_methods = tech_relationships.get('base_methods', [])
        for method_info in base_methods:
            if isinstance(method_info, dict):
                method_name = method_info.get('method_name', '')
                if method_name and method_name != 'åŸæ–‡æ— æ­¤ä¿¡æ¯':
                    tech_rel_dict['builds_upon'][method_name] += 1
            elif isinstance(method_info, str) and method_info != 'åŸæ–‡æ— æ­¤ä¿¡æ¯':
                tech_rel_dict['builds_upon'][method_info] += 1

        # å¯¹æ¯”çš„æ–¹æ³•
        compared_methods = tech_relationships.get('compared_methods', [])
        for method_info in compared_methods:
            if isinstance(method_info, dict):
                method_name = method_info.get('method_name', '')
                if method_name and method_name != 'åŸæ–‡æ— æ­¤ä¿¡æ¯':
                    tech_rel_dict['compared_with'][method_name] += 1

        # æ‰©å±•çš„ç†è®ºï¼ˆä¸»è¦æ¥è‡ªç†è®ºæ€§è®ºæ–‡ï¼‰
        extends = tech_relationships.get('extends', [])
        for theory in self.clean_field_data(extends):
            tech_rel_dict['extends'][theory] += 1

    def _process_author_profiles(self, author_profiles, collaboration_pairs):
        """å¤„ç†ä½œè€…ç”»åƒï¼Œè½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼"""
        processed_profiles = {}

        for author, profile in author_profiles.items():
            processed_profile = {
                'basic_info': {
                    'total_papers': profile['basic_info']['total_papers'],
                    'document_types': dict(profile['basic_info']['document_types']),
                    'institutions': list(profile['basic_info']['institutions']),
                    'collaboration_count': profile['basic_info']['collaboration_count'],
                    'productivity_level': self._classify_productivity(profile['basic_info']['total_papers'])
                },
                'research_focus': {
                    'top_domains': [{'domain': k, 'count': v} for k, v in
                                    profile['research_focus']['domains'].most_common(5)],
                    'top_methods': [{'method': k, 'count': v} for k, v in
                                    profile['research_focus']['methods'].most_common(5)],
                    'top_concepts': [{'concept': k, 'count': v} for k, v in
                                     profile['research_focus']['concepts'].most_common(10)],
                    'top_keywords': [{'keyword': k, 'count': v} for k, v in
                                     profile['research_focus']['keywords'].most_common(10)]
                },
                'innovation_profile': {
                    'total_contributions': len(profile['innovation_analysis']['stated_contributions']),
                    'technical_novelty_count': len(profile['innovation_analysis']['technical_novelty']),
                    'limitations_awareness': len(profile['innovation_analysis']['limitations_acknowledged']),
                    'datasets_expertise': list(profile['innovation_analysis']['datasets_used']),
                    'evaluation_metrics_used': list(profile['innovation_analysis']['evaluation_metrics']),
                    'innovation_level': self._classify_innovation_level(profile['innovation_analysis'])
                },
                'technical_network': {
                    'builds_upon': [{'method': k, 'count': v} for k, v in
                                    profile['technical_relationships']['builds_upon'].most_common(5)],
                    'compared_with': [{'method': k, 'count': v} for k, v in
                                      profile['technical_relationships']['compared_with'].most_common(5)],
                    'extends': [{'theory': k, 'count': v} for k, v in
                                profile['technical_relationships']['extends'].most_common(5)]
                },
                'collaboration_network': {
                    'collaborators': list(profile['collaboration_network']['collaborators']),
                    'institutional_networks': list(profile['collaboration_network']['institutional_networks']),
                    'collaboration_strength': len(profile['collaboration_network']['collaborators']),
                    'network_level': self._classify_collaboration_level(
                        len(profile['collaboration_network']['collaborators']))
                },
                'paper_list': profile['paper_list']
            }

            processed_profiles[author] = processed_profile

        return processed_profiles

    def _classify_productivity(self, paper_count):
        """åˆ†ç±»ç ”ç©¶è€…äº§å‡ºæ°´å¹³"""
        if paper_count >= 8:
            return "é«˜äº§ç ”ç©¶è€…"
        elif paper_count >= 5:
            return "æ´»è·ƒç ”ç©¶è€…"
        elif paper_count >= 3:
            return "ç¨³å®šç ”ç©¶è€…"
        elif paper_count >= 2:
            return "æ–°å…´ç ”ç©¶è€…"
        else:
            return "åˆçº§ç ”ç©¶è€…"

    def _classify_innovation_level(self, innovation_analysis):
        """åˆ†ç±»åˆ›æ–°æ°´å¹³"""
        contrib_count = len(innovation_analysis['stated_contributions'])
        novelty_count = len(innovation_analysis['technical_novelty'])
        dataset_count = len(innovation_analysis['datasets_used'])

        score = contrib_count * 2 + novelty_count + dataset_count * 0.5

        if score >= 10:
            return "åˆ›æ–°å¼•é¢†è€…"
        elif score >= 6:
            return "åˆ›æ–°æ´»è·ƒè€…"
        elif score >= 3:
            return "åˆ›æ–°å‚ä¸è€…"
        else:
            return "åˆ›æ–°åˆå­¦è€…"

    def _classify_collaboration_level(self, collaborator_count):
        """åˆ†ç±»åˆä½œæ°´å¹³"""
        if collaborator_count >= 10:
            return "åˆä½œç½‘ç»œæ ¸å¿ƒ"
        elif collaborator_count >= 5:
            return "åˆä½œæ´»è·ƒè€…"
        elif collaborator_count >= 2:
            return "å›¢é˜Ÿåˆä½œè€…"
        else:
            return "ç‹¬ç«‹ç ”ç©¶è€…"

    def analyze_innovation_landscape(self):
        """åˆ†æåˆ›æ–°æ ¼å±€"""
        print("åˆ†æåˆ›æ–°æ ¼å±€...")

        innovation_stats = {
            'contribution_types': Counter(),
            'novelty_areas': Counter(),
            'limitation_patterns': Counter(),
            'technical_evolution': defaultdict(list),
            'experimental_trends': {
                'datasets': Counter(),
                'metrics': Counter(),
                'baseline_methods': Counter()
            }
        }

        for doc_id in self.structural_insights:
            structural = self.structural_insights[doc_id]

            # åˆ›æ–°è´¡çŒ®åˆ†æ
            innovation = structural.get('innovation_analysis', {})
            contributions = self.clean_field_data(innovation.get('stated_contributions', []))
            novelty = self.clean_field_data(innovation.get('stated_novelty', []))

            for contrib in contributions:
                innovation_stats['contribution_types'][contrib] += 1

            for nov in novelty:
                innovation_stats['novelty_areas'][nov] += 1

            # å±€é™æ€§æ¨¡å¼
            limitations = structural.get('limitations', {})
            if limitations:
                ack_limitations = self.clean_field_data(limitations.get('acknowledged_limitations', []))
                for limitation in ack_limitations:
                    innovation_stats['limitation_patterns'][limitation] += 1

            # å®éªŒè¶‹åŠ¿
            exp_setup = structural.get('experimental_setup', {})
            if exp_setup:
                datasets = exp_setup.get('datasets_used', [])
                metrics = exp_setup.get('evaluation_metrics', [])
                baselines = exp_setup.get('baseline_methods', [])

                for dataset in datasets:
                    if isinstance(dataset, dict):
                        name = dataset.get('dataset_name', '')
                        if name and name != 'åŸæ–‡æ— æ­¤ä¿¡æ¯':
                            innovation_stats['experimental_trends']['datasets'][name] += 1

                for metric in self.clean_field_data(metrics):
                    innovation_stats['experimental_trends']['metrics'][metric] += 1

                for baseline in self.clean_field_data(baselines):
                    innovation_stats['experimental_trends']['baseline_methods'][baseline] += 1

        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
        landscape_analysis = {
            'contribution_landscape': {
                'total_unique_contributions': len(innovation_stats['contribution_types']),
                'top_contribution_types': [
                    {'contribution': k, 'frequency': v}
                    for k, v in innovation_stats['contribution_types'].most_common(15)
                ]
            },
            'novelty_landscape': {
                'total_novelty_claims': len(innovation_stats['novelty_areas']),
                'top_novelty_areas': [
                    {'area': k, 'frequency': v}
                    for k, v in innovation_stats['novelty_areas'].most_common(15)
                ]
            },
            'limitation_awareness': {
                'total_limitation_types': len(innovation_stats['limitation_patterns']),
                'common_limitations': [
                    {'limitation': k, 'frequency': v}
                    for k, v in innovation_stats['limitation_patterns'].most_common(10)
                ]
            },
            'experimental_ecosystem': {
                'popular_datasets': [
                    {'dataset': k, 'usage_count': v}
                    for k, v in innovation_stats['experimental_trends']['datasets'].most_common(10)
                ],
                'common_metrics': [
                    {'metric': k, 'usage_count': v}
                    for k, v in innovation_stats['experimental_trends']['metrics'].most_common(10)
                ],
                'baseline_methods': [
                    {'method': k, 'usage_count': v}
                    for k, v in innovation_stats['experimental_trends']['baseline_methods'].most_common(10)
                ]
            }
        }

        return landscape_analysis

    def analyze_technical_network(self):
        """åˆ†ææŠ€æœ¯ç½‘ç»œå’Œæ¼”åŒ–"""
        print("ğŸ”— åˆ†ææŠ€æœ¯ç½‘ç»œ...")

        technical_network = {
            'method_relationships': defaultdict(lambda: {
                'builds_upon': Counter(),
                'compared_with': Counter(),
                'extended_by': Counter()
            }),
            'domain_method_map': defaultdict(set),
            'method_evolution': defaultdict(list)
        }

        for doc_id in self.structural_insights:
            structural = self.structural_insights[doc_id]
            summary = self.summaries.get(doc_id, {})

            # æŠ€æœ¯å…³ç³»
            tech_rel = structural.get('technical_relationships', {})

            # åŸºäºå…³ç³»
            base_methods = tech_rel.get('base_methods', [])
            for method_info in base_methods:
                if isinstance(method_info, dict):
                    method_name = method_info.get('method_name', '')
                    if method_name and method_name != 'åŸæ–‡æ— æ­¤ä¿¡æ¯':
                        # è¿™ä¸ªæ–‡æ¡£åŸºäºè¿™ä¸ªæ–¹æ³•
                        technical_network['method_relationships'][method_name]['extended_by'][doc_id] += 1

            # å¯¹æ¯”å…³ç³»
            compared_methods = tech_rel.get('compared_methods', [])
            current_methods = self.clean_field_data(summary.get('methodology', ''))

            for method_info in compared_methods:
                if isinstance(method_info, dict):
                    method_name = method_info.get('method_name', '')
                    if method_name and method_name != 'åŸæ–‡æ— æ­¤ä¿¡æ¯':
                        for current_method in current_methods:
                            technical_network['method_relationships'][current_method]['compared_with'][method_name] += 1

        # è½¬æ¢ä¸ºç½‘ç»œåˆ†æç»“æœ
        network_analysis = {
            'method_influence_ranking': [],
            'method_comparison_patterns': [],
            'technical_evolution_paths': []
        }

        # æ–¹æ³•å½±å“åŠ›æ’åï¼ˆè¢«å¤šå°‘è®ºæ–‡åŸºäºæˆ–æ‰©å±•ï¼‰
        method_influence = {}
        for method, relationships in technical_network['method_relationships'].items():
            influence_score = len(relationships['extended_by'])
            comparison_count = sum(relationships['compared_with'].values())
            method_influence[method] = {
                'influence_score': influence_score,
                'comparison_count': comparison_count,
                'total_mentions': influence_score + comparison_count
            }

        network_analysis['method_influence_ranking'] = [
            {
                'method': method,
                'influence_score': info['influence_score'],
                'comparison_count': info['comparison_count'],
                'total_mentions': info['total_mentions']
            }
            for method, info in sorted(method_influence.items(),
                                       key=lambda x: x[1]['total_mentions'],
                                       reverse=True)[:20]
        ]

        return network_analysis

    def generate_fusion_visualizations(self, fusion_data):
        """ç”Ÿæˆèåˆæ•°æ®çš„å¯è§†åŒ–"""
        print("ç”Ÿæˆèåˆå¯è§†åŒ–...")

        try:
            # 1. ç ”ç©¶è€…åˆ›æ–°æ°´å¹³åˆ†å¸ƒ
            author_profiles = fusion_data['deep_author_analysis']
            innovation_levels = Counter()
            for author, profile in author_profiles.items():
                level = profile['innovation_profile']['innovation_level']
                innovation_levels[level] += 1

            if innovation_levels:
                plt.figure(figsize=(12, 8))
                levels = list(innovation_levels.keys())
                counts = list(innovation_levels.values())
                colors = ['#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4']

                plt.pie(counts, labels=levels, autopct='%1.1f%%', colors=colors, startangle=90)
                plt.title('ç ”ç©¶è€…åˆ›æ–°æ°´å¹³åˆ†å¸ƒ', fontsize=16, fontweight='bold')
                plt.axis('equal')
                plt.tight_layout()
                plt.savefig(VISUALIZATIONS_DIR / 'innovation_level_distribution.png', dpi=300, bbox_inches='tight')
                plt.close()

            # 2. çƒ­é—¨è´¡çŒ®ç±»å‹æŸ±çŠ¶å›¾
            contrib_data = fusion_data['innovation_landscape']['contribution_landscape']['top_contribution_types'][:10]
            if contrib_data:
                plt.figure(figsize=(15, 10))
                contributions = [item['contribution'][:50] + '...' if len(item['contribution']) > 50
                                 else item['contribution'] for item in contrib_data]
                frequencies = [item['frequency'] for item in contrib_data]

                plt.barh(contributions, frequencies, color='lightcoral')
                plt.title('çƒ­é—¨è´¡çŒ®ç±»å‹ç»Ÿè®¡', fontsize=16, fontweight='bold')
                plt.xlabel('å‡ºç°é¢‘æ¬¡')
                plt.gca().invert_yaxis()
                plt.tight_layout()
                plt.savefig(VISUALIZATIONS_DIR / 'top_contributions.png', dpi=300, bbox_inches='tight')
                plt.close()

            # 3. å®éªŒç”Ÿæ€ç³»ç»Ÿåˆ†æ
            exp_ecosystem = fusion_data['innovation_landscape']['experimental_ecosystem']

            # æ•°æ®é›†ä½¿ç”¨æƒ…å†µ
            dataset_data = exp_ecosystem['popular_datasets'][:8]
            if dataset_data:
                plt.figure(figsize=(12, 8))
                datasets = [item['dataset'] for item in dataset_data]
                usage_counts = [item['usage_count'] for item in dataset_data]

                plt.bar(datasets, usage_counts, color='skyblue')
                plt.title('çƒ­é—¨æ•°æ®é›†ä½¿ç”¨ç»Ÿè®¡', fontsize=16, fontweight='bold')
                plt.ylabel('ä½¿ç”¨æ¬¡æ•°')
                plt.xticks(rotation=45, ha='right')
                plt.tight_layout()
                plt.savefig(VISUALIZATIONS_DIR / 'popular_datasets.png', dpi=300, bbox_inches='tight')
                plt.close()

            # 4. åˆä½œç½‘ç»œå¼ºåº¦åˆ†å¸ƒ
            collaboration_levels = Counter()
            for author, profile in author_profiles.items():
                level = profile['collaboration_network']['network_level']
                collaboration_levels[level] += 1

            if collaboration_levels:
                plt.figure(figsize=(10, 6))
                levels = list(collaboration_levels.keys())
                counts = list(collaboration_levels.values())
                colors = ['#ffeaa7', '#fab1a0', '#fd79a8', '#e17055']

                plt.bar(levels, counts, color=colors)
                plt.title('ç ”ç©¶è€…åˆä½œç½‘ç»œå¼ºåº¦åˆ†å¸ƒ', fontsize=16, fontweight='bold')
                plt.ylabel('ç ”ç©¶è€…æ•°é‡')
                plt.xticks(rotation=45)
                plt.tight_layout()
                plt.savefig(VISUALIZATIONS_DIR / 'collaboration_strength.png', dpi=300, bbox_inches='tight')
                plt.close()

            print(f"èåˆå¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: {VISUALIZATIONS_DIR}")

        except Exception as e:
            print(f"ç”Ÿæˆå¯è§†åŒ–æ—¶å‡ºç°é”™è¯¯: {e}")

    def generate_fusion_profiles(self):
        """ç”Ÿæˆèåˆå­¦æœ¯ç”»åƒ"""
        print("=" * 80)
        print("å¼€å§‹ç”Ÿæˆèåˆå­¦æœ¯ç”»åƒ")
        print("=" * 80)

        if not self.load_data_sources():
            return None

        # 1. æ·±åº¦ç ”ç©¶è€…åˆ†æ
        print("\n" + "=" * 50)
        deep_author_analysis = self.analyze_deep_author_profiles()

        # 2. åˆ›æ–°æ ¼å±€åˆ†æ
        print("\n" + "=" * 50)
        innovation_landscape = self.analyze_innovation_landscape()

        # 3. æŠ€æœ¯ç½‘ç»œåˆ†æ
        print("\n" + "=" * 50)
        technical_network = self.analyze_technical_network()

        # 4. ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        statistics = {
            'data_sources': {
                'summary_documents': len(self.summaries),
                'structural_documents': len(self.structural_insights),
                'total_unique_documents': len(set(self.summaries.keys()) | set(self.structural_insights.keys()))
            },
            'analysis_scope': {
                'total_authors': len(deep_author_analysis),
                'innovation_contributions': len(
                    innovation_landscape['contribution_landscape']['top_contribution_types']),
                'technical_methods': len(technical_network['method_influence_ranking'])
            },
            'generation_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }

        # æ•´åˆæ‰€æœ‰åˆ†æç»“æœ
        fusion_profiles = {
            'metadata': {
                'generation_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'data_sources': 'Summary + Structural Insights',
                'analysis_version': '2.0-Fusion',
                'total_documents_analyzed': statistics['data_sources']['total_unique_documents']
            },
            'statistics': statistics,
            'deep_author_analysis': deep_author_analysis,
            'innovation_landscape': innovation_landscape,
            'technical_network': technical_network
        }

        # ä¿å­˜ç»“æœ
        with open(FUSION_PROFILES_PATH, 'w', encoding='utf-8') as f:
            json.dump(fusion_profiles, f, indent=2, ensure_ascii=False)

        with open(DEEP_AUTHOR_ANALYSIS_PATH, 'w', encoding='utf-8') as f:
            json.dump(deep_author_analysis, f, indent=2, ensure_ascii=False)

        with open(INNOVATION_LANDSCAPE_PATH, 'w', encoding='utf-8') as f:
            json.dump(innovation_landscape, f, indent=2, ensure_ascii=False)

        with open(TECHNICAL_NETWORK_PATH, 'w', encoding='utf-8') as f:
            json.dump(technical_network, f, indent=2, ensure_ascii=False)

        # ç”Ÿæˆå¯è§†åŒ–
        self.generate_fusion_visualizations(fusion_profiles)

        # æ‰“å°æ‘˜è¦
        self.print_fusion_summary(fusion_profiles)

        print(f"\n" + "=" * 80)
        print("èåˆå­¦æœ¯ç”»åƒç”Ÿæˆå®Œæˆï¼")
        print(f"å®Œæ•´ç”»åƒ: {FUSION_PROFILES_PATH}")
        print(f"æ·±åº¦ä½œè€…åˆ†æ: {DEEP_AUTHOR_ANALYSIS_PATH}")
        print(f"åˆ›æ–°æ ¼å±€: {INNOVATION_LANDSCAPE_PATH}")
        print(f"æŠ€æœ¯ç½‘ç»œ: {TECHNICAL_NETWORK_PATH}")
        print(f"å¯è§†åŒ–å›¾è¡¨: {VISUALIZATIONS_DIR}")
        print("=" * 80)

        return fusion_profiles

    def print_fusion_summary(self, profiles):
        """æ‰“å°èåˆåˆ†ææ‘˜è¦"""
        print(f"\n" + "=" * 60)
        print("èåˆå­¦æœ¯ç”»åƒæ‘˜è¦")
        print("=" * 60)

        stats = profiles['statistics']
        print(f"æ•°æ®è§„æ¨¡:")
        print(f"   æ¦‚è¦å±‚æ–‡æ¡£: {stats['data_sources']['summary_documents']}")
        print(f"   ç»“æ„åŒ–æ–‡æ¡£: {stats['data_sources']['structural_documents']}")
        print(f"   æ€»è®¡æ–‡æ¡£: {stats['data_sources']['total_unique_documents']}")

        print(f"\nç ”ç©¶è€…åˆ†æ:")
        print(f"   æ€»ç ”ç©¶è€…: {stats['analysis_scope']['total_authors']}")

        # æ˜¾ç¤ºé¡¶çº§ç ”ç©¶è€…
        author_analysis = profiles['deep_author_analysis']
        top_productive = sorted(author_analysis.items(),
                                key=lambda x: x[1]['basic_info']['total_papers'],
                                reverse=True)[:5]
        print(f" é«˜äº§ç ”ç©¶è€…TOP5:")
        for i, (author, profile) in enumerate(top_productive, 1):
            papers = profile['basic_info']['total_papers']
            level = profile['basic_info']['productivity_level']
            print(f"     {i}. {author}: {papers}ç¯‡ ({level})")

        print(f"\nåˆ›æ–°åˆ†æ:")
        innovation = profiles['innovation_landscape']
        contrib_count = innovation['contribution_landscape']['total_unique_contributions']
        novelty_count = innovation['novelty_landscape']['total_novelty_claims']
        print(f"   ç‹¬ç‰¹è´¡çŒ®ç±»å‹: {contrib_count}")
        print(f"   æ–°é¢–æ€§å£°æ˜: {novelty_count}")

        print(f"   çƒ­é—¨è´¡çŒ®TOP3:")
        top_contribs = innovation['contribution_landscape']['top_contribution_types'][:3]
        for i, contrib in enumerate(top_contribs, 1):
            print(f"     {i}. {contrib['contribution'][:50]}... ({contrib['frequency']}æ¬¡)")

        print(f"\næŠ€æœ¯ç½‘ç»œ:")
        tech_network = profiles['technical_network']
        methods_count = len(tech_network['method_influence_ranking'])
        print(f" æŠ€æœ¯æ–¹æ³•æ•°: {methods_count}")

        if tech_network['method_influence_ranking']:
            print(f"   å½±å“åŠ›æœ€å¤§çš„æ–¹æ³•TOP3:")
            for i, method_info in enumerate(tech_network['method_influence_ranking'][:3], 1):
                method = method_info['method'][:40] + '...' if len(method_info['method']) > 40 else method_info[
                    'method']
                mentions = method_info['total_mentions']
                print(f"     {i}. {method} ({mentions}æ¬¡æåŠ)")


def main():
    """ä¸»å‡½æ•°"""
    generator = FusionAcademicProfileGenerator()
    fusion_profiles = generator.generate_fusion_profiles()
    return fusion_profiles


if __name__ == "__main__":
    main()