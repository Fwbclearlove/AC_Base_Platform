# src/ai_visualization_data_generator.py
import json
import os
import time
from zhipuai import ZhipuAI
from pathlib import Path
from collections import defaultdict, Counter
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import pandas as pd
import numpy as np

from config import PROCESSED_DATA_DIR

# APIé…ç½®
API_KEY = "838cc9e6876a4fea971b3728af105b56.1KDgfLzNHnfllnhb"
CLIENT = ZhipuAI(api_key=API_KEY)
MODEL_NAME = "glm-4"

# è¾“å‡ºè·¯å¾„
VIZ_DATA_DIR = PROCESSED_DATA_DIR / "visualization_data"
VIZ_DATA_DIR.mkdir(exist_ok=True)

VIZ_CHARTS_DIR = VIZ_DATA_DIR / "charts"
VIZ_CHARTS_DIR.mkdir(exist_ok=True)


class VisualizationDataGenerator:
    """ä¸“é—¨ç”Ÿæˆå¯è§†åŒ–æ•°æ®çš„AIåˆ†æå™¨"""

    def __init__(self):
        self.summaries = {}
        self.structural_insights = {}
        self.author_name_map = {}
        self.viz_data = {}

    def load_data_sources(self):
        """åŠ è½½æ•°æ®æº"""
        # åŠ è½½æ¦‚è¦å±‚æ•°æ®
        summaries_path = PROCESSED_DATA_DIR / "document_summaries.json"
        try:
            with open(summaries_path, 'r', encoding='utf-8') as f:
                self.summaries = json.load(f)
            print(f"âœ… åŠ è½½æ¦‚è¦å±‚æ•°æ®: {len(self.summaries)} ä¸ªæ–‡æ¡£")
        except Exception as e:
            print(f"âŒ åŠ è½½æ¦‚è¦å±‚æ•°æ®å¤±è´¥: {e}")
            return False

        # åŠ è½½ç»“æ„åŒ–æ•°æ®
        structural_path = PROCESSED_DATA_DIR / "structural_insights" / "structural_insights.json"
        try:
            with open(structural_path, 'r', encoding='utf-8') as f:
                self.structural_insights = json.load(f)
            print(f"âœ… åŠ è½½ç»“æ„åŒ–æ•°æ®: {len(self.structural_insights)} ä¸ªæ–‡æ¡£")
        except Exception as e:
            print(f"âŒ åŠ è½½ç»“æ„åŒ–æ•°æ®å¤±è´¥: {e}")
            return False

        return True

    def generate_team_radar_data_with_ai(self):
        """AIç”Ÿæˆå›¢é˜Ÿé›·è¾¾å›¾æ•°æ®ï¼ˆåŸºäºæ¯ç¯‡æ–‡ç« çš„è¯¦ç»†æ•°æ®ï¼‰"""

        # å‡†å¤‡æ¯ç¯‡æ–‡ç« çš„è¯¦ç»†æ•°æ®ï¼ˆå‰20ç¯‡ä½œä¸ºæ ·æœ¬ï¼‰
        detailed_papers = self._prepare_detailed_papers_data(limit=20)

        prompt = f"""
ä½ æ˜¯å­¦æœ¯å›¢é˜Ÿè¯„ä¼°ä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹æ¯ç¯‡è®ºæ–‡çš„è¯¦ç»†æ•°æ®ï¼Œä¸ºå›¢é˜Ÿèƒ½åŠ›è¯„ä¼°ç”Ÿæˆé›·è¾¾å›¾æ•°æ®ã€‚

æ¯ç¯‡è®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯:
{json.dumps(detailed_papers, ensure_ascii=False, indent=1)}

è¯·ä»”ç»†åˆ†ææ¯ç¯‡è®ºæ–‡çš„å†…å®¹ï¼Œä»ä»¥ä¸‹6ä¸ªç»´åº¦ä¸ºå›¢é˜Ÿæ‰“åˆ†ï¼ˆ1-10åˆ†ï¼Œä¿ç•™1ä½å°æ•°ï¼‰ï¼š

1. ç ”ç©¶äº§å‡ºï¼šåŸºäºè®ºæ–‡æ•°é‡ã€è´¨é‡å’Œå‘è¡¨é¢‘ç‡
2. æŠ€æœ¯åˆ›æ–°ï¼šåŸºäºåˆ›æ–°ç‚¹çš„åŸåˆ›æ€§å’ŒæŠ€æœ¯çªç ´
3. åˆä½œç½‘ç»œï¼šåŸºäºä½œè€…åˆä½œæ¨¡å¼å’Œè·¨æœºæ„åˆä½œ
4. å­¦æœ¯å½±å“ï¼šåŸºäºæ–¹æ³•å½±å“åŠ›å’Œè¢«å¯¹æ¯”æƒ…å†µ
5. äººæ‰åŸ¹å…»ï¼šåŸºäºä½œè€…æ¢¯é˜Ÿå’Œç ”ç©¶æ·±åº¦
6. å›½é™…åŒ–ï¼šåŸºäºå›½é™…åˆä½œå’Œç ”ç©¶è§†é‡

è¿”å›æ ¼å¼ï¼š
{{
  "radar_data": {{
    "ç ”ç©¶äº§å‡º": 8.5,
    "æŠ€æœ¯åˆ›æ–°": 7.8,
    "åˆä½œç½‘ç»œ": 6.9,
    "å­¦æœ¯å½±å“": 7.2,
    "äººæ‰åŸ¹å…»": 6.5,
    "å›½é™…åŒ–": 5.8
  }},
  "evidence_analysis": {{
    "ç ”ç©¶äº§å‡º": "åŸºäº{len(detailed_papers)}ç¯‡è®ºæ–‡åˆ†æ...",
    "æŠ€æœ¯åˆ›æ–°": "å‘ç°Xä¸ªåˆ›æ–°ç‚¹...",
    "åˆä½œç½‘ç»œ": "å¹³å‡æ¯ç¯‡Xä½ä½œè€…åˆä½œ..."
  }}
}}

åªè¾“å‡ºJSONæ ¼å¼ã€‚
"""

        try:
            response = CLIENT.chat.completions.create(
                model=MODEL_NAME,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=800,
            )

            result_str = response.choices[0].message.content.strip()
            result_str = result_str.replace("```json", "").replace("```", "").strip()

            radar_data = json.loads(result_str)
            print("âœ… å›¢é˜Ÿé›·è¾¾å›¾æ•°æ®ç”ŸæˆæˆåŠŸ")
            return radar_data

        except Exception as e:
            print(f"âŒ å›¢é˜Ÿé›·è¾¾å›¾æ•°æ®ç”Ÿæˆå¤±è´¥: {e}")
            return None

    def generate_author_comparison_data_with_ai(self):
        """AIç”Ÿæˆä½œè€…å¯¹æ¯”æ•°æ®ï¼ˆåŸºäºæ¯ä½ä½œè€…çš„å…·ä½“è®ºæ–‡ï¼‰"""

        # è·å–æ¯ä½ä½œè€…çš„è¯¦ç»†è®ºæ–‡æ•°æ®
        author_papers_data = self._prepare_author_papers_data(limit=5)

        prompt = f"""
ä½ æ˜¯å­¦æœ¯äººæ‰è¯„ä¼°ä¸“å®¶ã€‚åŸºäºä»¥ä¸‹æ¯ä½ç ”ç©¶è€…çš„å…·ä½“è®ºæ–‡å†…å®¹ï¼Œç”Ÿæˆèƒ½åŠ›å¯¹æ¯”æ•°æ®ã€‚

ç ”ç©¶è€…åŠå…¶è®ºæ–‡è¯¦æƒ…:
{json.dumps(author_papers_data, ensure_ascii=False, indent=1)}

è¯·ä»”ç»†åˆ†ææ¯ä½ç ”ç©¶è€…çš„æ‰€æœ‰è®ºæ–‡ï¼Œåœ¨5ä¸ªç»´åº¦ä¸Šä¸ºæ¯äººæ‰“åˆ†ï¼ˆ1-10åˆ†ï¼Œä¿ç•™1ä½å°æ•°ï¼‰ï¼š

1. ç ”ç©¶äº§å‡ºï¼šè®ºæ–‡æ•°é‡ã€å‘è¡¨è´¨é‡
2. åˆ›æ–°èƒ½åŠ›ï¼šåˆ›æ–°ç‚¹åŸåˆ›æ€§ã€æŠ€æœ¯çªç ´åº¦
3. æŠ€æœ¯æ·±åº¦ï¼šæ–¹æ³•å¤æ‚åº¦ã€ç†è®ºæ·±åº¦
4. åˆä½œèƒ½åŠ›ï¼šåˆä½œç½‘ç»œå¹¿åº¦ã€å›¢é˜Ÿåä½œ
5. å½±å“åŠ›ï¼šè¢«å¼•ç”¨ã€æ–¹æ³•è¢«å¯¹æ¯”æƒ…å†µ

è¿”å›æ ¼å¼ï¼š
{{
  "comparison_matrix": [
    {{"name": "ç ”ç©¶è€…å§“å", "ç ”ç©¶äº§å‡º": 9.2, "åˆ›æ–°èƒ½åŠ›": 8.8, "æŠ€æœ¯æ·±åº¦": 8.5, "åˆä½œèƒ½åŠ›": 7.9, "å½±å“åŠ›": 8.1}},
    ...
  ],
  "dimensions": ["ç ”ç©¶äº§å‡º", "åˆ›æ–°èƒ½åŠ›", "æŠ€æœ¯æ·±åº¦", "åˆä½œèƒ½åŠ›", "å½±å“åŠ›"],
  "analysis_basis": {{
    "ç ”ç©¶è€…å§“å": "åŸºäºXç¯‡è®ºæ–‡ï¼Œå‘ç°Yä¸ªåˆ›æ–°ç‚¹...",
    ...
  }}
}}

åªè¾“å‡ºJSONæ•°æ®ã€‚
"""

        try:
            response = CLIENT.chat.completions.create(
                model=MODEL_NAME,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2,
                max_tokens=1200,
            )

            result_str = response.choices[0].message.content.strip()
            result_str = result_str.replace("```json", "").replace("```", "").strip()

            comparison_data = json.loads(result_str)
            print("âœ… ä½œè€…å¯¹æ¯”æ•°æ®ç”ŸæˆæˆåŠŸ")
            return comparison_data

        except Exception as e:
            print(f"âŒ ä½œè€…å¯¹æ¯”æ•°æ®ç”Ÿæˆå¤±è´¥: {e}")
            return None

    def generate_research_trend_data_with_ai(self):
        """AIç”Ÿæˆç ”ç©¶è¶‹åŠ¿æ—¶é—´åºåˆ—æ•°æ®"""

        # æ”¶é›†æŒ‰å¹´ä»½çš„ç ”ç©¶æ•°æ®
        yearly_stats = self._collect_yearly_research_data()

        prompt = f"""
ä½ æ˜¯è¶‹åŠ¿åˆ†æä¸“å®¶ã€‚åŸºäºç ”ç©¶æ•°æ®ç”Ÿæˆæ—¶é—´åºåˆ—å›¾è¡¨æ•°æ®ã€‚

å¹´åº¦æ•°æ®: {json.dumps(yearly_stats, ensure_ascii=False)}

è¯·ç”Ÿæˆ2020-2024å¹´çš„ç ”ç©¶è¶‹åŠ¿æ•°æ®ï¼š

{{
  "time_series_data": [
    {{"year": 2020, "è®ºæ–‡æ•°é‡": 15, "åˆ›æ–°æŒ‡æ•°": 6.8, "åˆä½œå¼ºåº¦": 4.2}},
    {{"year": 2021, "è®ºæ–‡æ•°é‡": 23, "åˆ›æ–°æŒ‡æ•°": 7.2, "åˆä½œå¼ºåº¦": 5.1}},
    {{"year": 2022, "è®ºæ–‡æ•°é‡": 31, "åˆ›æ–°æŒ‡æ•°": 7.8, "åˆä½œå¼ºåº¦": 6.3}},
    {{"year": 2023, "è®ºæ–‡æ•°é‡": 28, "åˆ›æ–°æŒ‡æ•°": 8.1, "åˆä½œå¼ºåº¦": 7.0}},
    {{"year": 2024, "è®ºæ–‡æ•°é‡": 21, "åˆ›æ–°æŒ‡æ•°": 8.4, "åˆä½œå¼ºåº¦": 7.5}}
  ],
  "trend_analysis": {{
    "è®ºæ–‡æ•°é‡_trend": "å…ˆå¢åå‡ï¼Œ2022å¹´è¾¾å³°",
    "åˆ›æ–°æŒ‡æ•°_trend": "ç¨³æ­¥ä¸Šå‡",
    "åˆä½œå¼ºåº¦_trend": "æŒç»­å¢å¼º"
  }},
  "future_prediction": {{
    "2025_forecast": {{"è®ºæ–‡æ•°é‡": 25, "åˆ›æ–°æŒ‡æ•°": 8.6, "åˆä½œå¼ºåº¦": 8.0}}
  }}
}}

åªè¾“å‡ºJSONæ•°æ®ã€‚
"""

        try:
            response = CLIENT.chat.completions.create(
                model=MODEL_NAME,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=1000,
            )

            result_str = response.choices[0].message.content.strip()
            result_str = result_str.replace("```json", "").replace("```", "").strip()

            trend_data = json.loads(result_str)
            print("âœ… ç ”ç©¶è¶‹åŠ¿æ•°æ®ç”ŸæˆæˆåŠŸ")
            return trend_data

        except Exception as e:
            print(f"âŒ ç ”ç©¶è¶‹åŠ¿æ•°æ®ç”Ÿæˆå¤±è´¥: {e}")
            return None

    def generate_collaboration_network_data_with_ai(self):
        """AIç”Ÿæˆåˆä½œç½‘ç»œå›¾æ•°æ®"""

        # æ”¶é›†åˆä½œå…³ç³»æ•°æ®
        collab_stats = self._collect_collaboration_data()

        prompt = f"""
ä½ æ˜¯ç½‘ç»œåˆ†æä¸“å®¶ã€‚åŸºäºåˆä½œæ•°æ®ç”Ÿæˆç½‘ç»œå›¾æ•°æ®ã€‚

åˆä½œç»Ÿè®¡: {json.dumps(collab_stats, ensure_ascii=False)}

è¯·ç”Ÿæˆç½‘ç»œå›¾çš„èŠ‚ç‚¹å’Œè¾¹æ•°æ®ï¼š

{{
  "nodes": [
    {{"id": "æç¿å‡¡", "size": 25, "group": "æ ¸å¿ƒ", "papers": 65, "centrality": 0.85}},
    {{"id": "Wang Xiaojie", "size": 18, "group": "æ´»è·ƒ", "papers": 25, "centrality": 0.62}},
    {{"id": "Feng Fangxiang", "size": 15, "group": "æ´»è·ƒ", "papers": 19, "centrality": 0.48}}
  ],
  "edges": [
    {{"source": "æç¿å‡¡", "target": "Wang Xiaojie", "weight": 8, "papers": 8}},
    {{"source": "æç¿å‡¡", "target": "Feng Fangxiang", "weight": 5, "papers": 5}},
    {{"source": "Wang Xiaojie", "target": "Feng Fangxiang", "weight": 3, "papers": 3}}
  ],
  "network_metrics": {{
    "density": 0.65,
    "avg_clustering": 0.73,
    "core_nodes": ["æç¿å‡¡"],
    "bridge_nodes": ["Wang Xiaojie"]
  }}
}}

åªè¾“å‡ºJSONæ•°æ®ã€‚
"""

        try:
            response = CLIENT.chat.completions.create(
                model=MODEL_NAME,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2,
                max_tokens=1200,
            )

            result_str = response.choices[0].message.content.strip()
            result_str = result_str.replace("```json", "").replace("```", "").strip()

            network_data = json.loads(result_str)
            print("âœ… åˆä½œç½‘ç»œæ•°æ®ç”ŸæˆæˆåŠŸ")
            return network_data

        except Exception as e:
            print(f"âŒ åˆä½œç½‘ç»œæ•°æ®ç”Ÿæˆå¤±è´¥: {e}")
            return None

    def generate_research_domain_pie_data_with_ai(self):
        """AIç”Ÿæˆç ”ç©¶é¢†åŸŸé¥¼å›¾æ•°æ®"""

        domain_stats = self._collect_domain_statistics()

        prompt = f"""
ä½ æ˜¯é¢†åŸŸåˆ†æä¸“å®¶ã€‚åŸºäºç ”ç©¶é¢†åŸŸæ•°æ®ç”Ÿæˆé¥¼å›¾æ•°æ®ã€‚

é¢†åŸŸç»Ÿè®¡: {json.dumps(domain_stats, ensure_ascii=False)}

è¯·ç”Ÿæˆç ”ç©¶é¢†åŸŸåˆ†å¸ƒçš„é¥¼å›¾æ•°æ®ï¼š

{{
  "pie_data": [
    {{"domain": "è®¡ç®—æœºè§†è§‰", "papers": 32, "percentage": 27.1, "color": "#FF6B6B"}},
    {{"domain": "è‡ªç„¶è¯­è¨€å¤„ç†", "papers": 28, "percentage": 23.7, "color": "#4ECDC4"}},
    {{"domain": "æœºå™¨å­¦ä¹ ", "papers": 25, "percentage": 21.2, "color": "#45B7D1"}},
    {{"domain": "æ•°æ®æŒ–æ˜", "papers": 18, "percentage": 15.3, "color": "#96CEB4"}},
    {{"domain": "å…¶ä»–", "papers": 15, "percentage": 12.7, "color": "#FFEAA7"}}
  ],
  "total_papers": 118,
  "domain_analysis": {{
    "dominant_field": "è®¡ç®—æœºè§†è§‰",
    "emerging_field": "è‡ªç„¶è¯­è¨€å¤„ç†",
    "diversity_index": 0.78
  }}
}}

åªè¾“å‡ºJSONæ•°æ®ã€‚
"""

        try:
            response = CLIENT.chat.completions.create(
                model=MODEL_NAME,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2,
                max_tokens=800,
            )

            result_str = response.choices[0].message.content.strip()
            result_str = result_str.replace("```json", "").replace("```", "").strip()

            pie_data = json.loads(result_str)
            print("âœ… ç ”ç©¶é¢†åŸŸé¥¼å›¾æ•°æ®ç”ŸæˆæˆåŠŸ")
            return pie_data

        except Exception as e:
            print(f"âŒ ç ”ç©¶é¢†åŸŸé¥¼å›¾æ•°æ®ç”Ÿæˆå¤±è´¥: {e}")
            return None

    def clean_field_data(self, field_value, invalid_values=None):
        """æ¸…æ´—å­—æ®µæ•°æ®ï¼Œç§»é™¤æ— æ•ˆå€¼"""
        if invalid_values is None:
            invalid_values = [
                "åŸæ–‡æ— æ­¤ä¿¡æ¯", "åŸæ–‡æœªæ˜ç¡®æåŠ", "æœªåœ¨åŸæ–‡ä¸­æ˜ç¡®æåŠ",
                "æœªåœ¨åŸæ–‡ä¸­æ˜ç¡®åˆ—å‡º", "", "æœªçŸ¥", "unknown", "æ— "
            ]

        if not field_value:
            return []

        if isinstance(field_value, str):
            if field_value.strip() in invalid_values:
                return []
            return [field_value.strip()]

        elif isinstance(field_value, list):
            cleaned = []
            for item in field_value:
                if isinstance(item, str) and item.strip():
                    if item.strip() not in invalid_values:
                        cleaned.append(item.strip())
            return cleaned
        return []

    def normalize_author_name(self, author_name):
        """æ ‡å‡†åŒ–ä½œè€…å§“å"""
        if not author_name or author_name.strip() in ["", "æœªçŸ¥", "unknown"]:
            return None

        name = author_name.strip()

        # æç¿å‡¡çš„å„ç§å†™æ³•ç»Ÿä¸€
        ruifan_variations = [
            "Ruifan Li", "æç¿å‡¡", "Li Ruifan", "Li, Ruifan",
            "Ruifan, Li", "ruifan li", "li ruifan"
        ]

        for variation in ruifan_variations:
            if name.lower() == variation.lower():
                return "æç¿å‡¡"

        # å…¶ä»–å§“åæ ‡å‡†åŒ–å¤„ç†
        name = name.replace(",", " ").replace(".", " ")
        name = " ".join(name.split())

        if any('\u4e00' <= char <= '\u9fff' for char in name):
            return name

        parts = name.split()
        if len(parts) == 2:
            return f"{parts[1]} {parts[0]}" if parts[0][0].isupper() and parts[1][0].isupper() else name

        return name

    def build_author_name_mapping(self):
        """æ„å»ºä½œè€…å§“åæ ‡å‡†åŒ–æ˜ å°„"""
        all_authors = set()

        # ä»æ¦‚è¦å±‚æ”¶é›†æ‰€æœ‰ä½œè€…å
        for doc_id, summary in self.summaries.items():
            doc_type = summary.get('document_type', 'unknown')
            if doc_type == 'patent':
                authors = self.clean_field_data(summary.get('inventors', []))
            else:
                authors = self.clean_field_data(summary.get('authors', []))
            all_authors.update(authors)

        # ä»ç»“æ„åŒ–å±‚æ”¶é›†ä½œè€…å
        for doc_id, structural in self.structural_insights.items():
            metadata = structural.get('document_metadata', {})
            authors = self.clean_field_data(metadata.get('authors', []))
            inventors = self.clean_field_data(metadata.get('inventors', []))
            creators = self.clean_field_data(metadata.get('authors_or_creators', []))
            all_authors.update(authors + inventors + creators)

        # å»ºç«‹æ˜ å°„
        for author in all_authors:
            normalized = self.normalize_author_name(author)
            if normalized:
                self.author_name_map[author] = normalized

    def _prepare_detailed_papers_data(self, limit=20):
        """å‡†å¤‡æ¯ç¯‡è®ºæ–‡çš„è¯¦ç»†æ•°æ®ä¾›AIåˆ†æ"""
        detailed_papers = {}
        all_docs = list(set(self.summaries.keys()) | set(self.structural_insights.keys()))

        # é™åˆ¶æ•°æ®é‡ï¼Œé€‰æ‹©å‰Nç¯‡è®ºæ–‡
        selected_docs = all_docs[:limit]

        for doc_id in selected_docs:
            paper_data = {"doc_id": doc_id}

            # ä»æ¦‚è¦å±‚è·å–æ•°æ®
            if doc_id in self.summaries:
                summary = self.summaries[doc_id]
                paper_data["æ¦‚è¦å±‚"] = {
                    "document_type": summary.get('document_type', ''),
                    "title": summary.get('title', ''),
                    "authors": self.clean_field_data(summary.get('authors', [])) or
                               self.clean_field_data(summary.get('inventors', [])),
                    "main_topic": summary.get('main_topic', '') or summary.get('application_domain', ''),
                    "methodology": summary.get('methodology', '') or summary.get('technical_solution', ''),
                    "key_innovations": self.clean_field_data(summary.get('key_innovations', [])),
                    "keywords": self.clean_field_data(summary.get('keywords', [])),
                    "technical_concepts": self.clean_field_data(summary.get('technical_concepts', []))
                }

            # ä»ç»“æ„åŒ–å±‚è·å–æ•°æ®
            if doc_id in self.structural_insights:
                structural = self.structural_insights[doc_id]
                paper_data["ç»“æ„åŒ–å±‚"] = {
                    "document_metadata": structural.get('document_metadata', {}),
                    "innovation_analysis": {
                        "stated_contributions": self.clean_field_data(
                            structural.get('innovation_analysis', {}).get('stated_contributions', [])
                        ),
                        "stated_novelty": self.clean_field_data(
                            structural.get('innovation_analysis', {}).get('stated_novelty', [])
                        )
                    },
                    "technical_relationships": structural.get('technical_relationships', {}),
                    "experimental_setup": structural.get('experimental_setup', {})
                }

            detailed_papers[doc_id] = paper_data

        return detailed_papers

    def _prepare_author_papers_data(self, limit=5):
        """å‡†å¤‡æ¯ä½ä½œè€…çš„å…·ä½“è®ºæ–‡æ•°æ®"""
        # å…ˆç»Ÿè®¡æ¯ä¸ªä½œè€…çš„è®ºæ–‡æ•°
        author_papers = defaultdict(list)
        all_docs = set(self.summaries.keys()) | set(self.structural_insights.keys())

        for doc_id in all_docs:
            # è·å–ä½œè€…
            doc_authors = []
            if doc_id in self.summaries:
                summary = self.summaries[doc_id]
                doc_type = summary.get('document_type', 'unknown')
                if doc_type == 'patent':
                    authors = self.clean_field_data(summary.get('inventors', []))
                else:
                    authors = self.clean_field_data(summary.get('authors', []))
                doc_authors.extend(authors)

            # ä¸ºæ¯ä¸ªä½œè€…è®°å½•è¿™ç¯‡è®ºæ–‡
            for author in doc_authors:
                normalized = self.author_name_map.get(author, author)
                if normalized:
                    author_papers[normalized].append(doc_id)

        # é€‰æ‹©å‰Nä¸ªæœ€æ´»è·ƒçš„ä½œè€…
        top_authors = sorted(author_papers.items(), key=lambda x: len(x[1]), reverse=True)[:limit]

        # ä¸ºæ¯ä¸ªä½œè€…å‡†å¤‡è¯¦ç»†çš„è®ºæ–‡æ•°æ®
        author_detailed_data = {}
        for author_name, paper_ids in top_authors:
            author_detailed_data[author_name] = {
                "total_papers": len(paper_ids),
                "papers": {}
            }

            # ä¸ºæ¯ç¯‡è®ºæ–‡å‡†å¤‡è¯¦ç»†æ•°æ®
            for paper_id in paper_ids:
                paper_data = {}

                # æ¦‚è¦å±‚æ•°æ®
                if paper_id in self.summaries:
                    summary = self.summaries[paper_id]
                    paper_data["æ¦‚è¦"] = {
                        "title": summary.get('title', ''),
                        "main_topic": summary.get('main_topic', '') or summary.get('application_domain', ''),
                        "methodology": summary.get('methodology', '') or summary.get('technical_solution', ''),
                        "key_innovations": self.clean_field_data(summary.get('key_innovations', []))
                    }

                # ç»“æ„åŒ–å±‚æ•°æ®
                if paper_id in self.structural_insights:
                    structural = self.structural_insights[paper_id]
                    paper_data["ç»“æ„åŒ–"] = {
                        "contributions": self.clean_field_data(
                            structural.get('innovation_analysis', {}).get('stated_contributions', [])
                        ),
                        "novelty": self.clean_field_data(
                            structural.get('innovation_analysis', {}).get('stated_novelty', [])
                        ),
                        "technical_relationships": structural.get('technical_relationships', {})
                    }

                author_detailed_data[author_name]["papers"][paper_id] = paper_data

        return author_detailed_data

    def _collect_yearly_research_data(self):
        """æ”¶é›†çœŸå®çš„å¹´åº¦ç ”ç©¶æ•°æ®"""
        # æ³¨æ„ï¼šæ¦‚è¦æ•°æ®ä¸­å¯èƒ½æ²¡æœ‰å¹´ä»½ä¿¡æ¯ï¼Œè¿™é‡Œè¿”å›æç¤º
        return {"note": "éœ€è¦ä»æ–‡æ¡£ä¸­æå–å¹´ä»½ä¿¡æ¯ï¼Œå½“å‰æ•°æ®ä¸­å¹´ä»½ä¿¡æ¯æœ‰é™"}

    def _collect_collaboration_data(self):
        """æ”¶é›†çœŸå®çš„åˆä½œæ•°æ®"""
        collaboration_pairs = 0
        total_collaborations = 0
        all_docs = set(self.summaries.keys()) | set(self.structural_insights.keys())

        for doc_id in all_docs:
            doc_authors = []
            if doc_id in self.summaries:
                summary = self.summaries[doc_id]
                doc_type = summary.get('document_type', 'unknown')
                if doc_type == 'patent':
                    authors = self.clean_field_data(summary.get('inventors', []))
                else:
                    authors = self.clean_field_data(summary.get('authors', []))
                doc_authors.extend(authors)

            if len(doc_authors) > 1:
                collaboration_pairs += 1
                total_collaborations += len(doc_authors)

        return {
            "collaboration_papers": collaboration_pairs,
            "total_papers": len(all_docs),
            "avg_authors_per_paper": total_collaborations / len(all_docs) if all_docs else 0
        }

    def _collect_domain_statistics(self):
        """æ”¶é›†çœŸå®çš„é¢†åŸŸç»Ÿè®¡"""
        domain_counts = Counter()

        for doc_id, summary in self.summaries.items():
            doc_type = summary.get('document_type', 'unknown')
            if doc_type == 'patent':
                domains = self.clean_field_data(summary.get('application_domain', ''))
                domains.extend(self.clean_field_data(summary.get('application_scenarios', [])))
            else:
                domains = self.clean_field_data(summary.get('main_topic', ''))
                domains.extend(self.clean_field_data(summary.get('application_domains', [])))

            for domain in domains:
                domain_counts[domain] += 1

        return dict(domain_counts.most_common(10))

    def create_visualizations_from_ai_data(self):
        """åŸºäºAIç”Ÿæˆçš„æ•°æ®åˆ›å»ºå¯è§†åŒ–"""

        print("\nğŸ¨ å¼€å§‹åˆ›å»ºå¯è§†åŒ–å›¾è¡¨...")

        # 1. å›¢é˜Ÿé›·è¾¾å›¾
        if 'radar_data' in self.viz_data:
            self._create_team_radar_chart()

        # 2. ä½œè€…å¯¹æ¯”çƒ­å›¾
        if 'comparison_data' in self.viz_data:
            self._create_author_comparison_heatmap()

        # 3. ç ”ç©¶è¶‹åŠ¿çº¿å›¾
        if 'trend_data' in self.viz_data:
            self._create_research_trend_chart()

        # 4. åˆä½œç½‘ç»œå›¾
        if 'network_data' in self.viz_data:
            self._create_collaboration_network_chart()

        # 5. ç ”ç©¶é¢†åŸŸé¥¼å›¾
        if 'pie_data' in self.viz_data:
            self._create_research_domain_pie_chart()

    def _create_team_radar_chart(self):
        """åˆ›å»ºå›¢é˜Ÿèƒ½åŠ›é›·è¾¾å›¾"""
        radar_data = self.viz_data['radar_data']['radar_data']

        categories = list(radar_data.keys())
        values = list(radar_data.values())

        # é—­åˆé›·è¾¾å›¾
        categories += categories[:1]
        values += values[:1]

        fig = go.Figure()

        fig.add_trace(go.Scatterpolar(
            r=values,
            theta=categories,
            fill='toself',
            name='å›¢é˜Ÿèƒ½åŠ›',
            line=dict(color='rgb(255, 99, 132)', width=3),
            fillcolor='rgba(255, 99, 132, 0.25)'
        ))

        fig.update_layout(
            polar=dict(
                radialaxis=dict(
                    visible=True,
                    range=[0, 10],
                    tickfont=dict(size=12)
                )),
            showlegend=True,
            title="å›¢é˜Ÿç»¼åˆèƒ½åŠ›é›·è¾¾å›¾",
            font=dict(family="Microsoft YaHei", size=14),
            width=600,
            height=600
        )

        fig.write_html(VIZ_CHARTS_DIR / "team_radar_chart.html")
        print("âœ… å›¢é˜Ÿé›·è¾¾å›¾å·²ç”Ÿæˆ")

    def _create_author_comparison_heatmap(self):
        """åˆ›å»ºä½œè€…èƒ½åŠ›å¯¹æ¯”çƒ­å›¾"""
        comparison_data = self.viz_data['comparison_data']['comparison_matrix']
        dimensions = self.viz_data['comparison_data']['dimensions']

        # æ„å»ºæ•°æ®çŸ©é˜µ
        authors = [item['name'] for item in comparison_data]
        matrix = []

        for author_data in comparison_data:
            row = [author_data[dim] for dim in dimensions]
            matrix.append(row)

        fig = px.imshow(
            matrix,
            labels=dict(x="èƒ½åŠ›ç»´åº¦", y="ç ”ç©¶è€…", color="è¯„åˆ†"),
            x=dimensions,
            y=authors,
            color_continuous_scale="RdYlBu_r",
            aspect="auto"
        )

        fig.update_layout(
            title="ç ”ç©¶è€…èƒ½åŠ›å¯¹æ¯”çƒ­å›¾",
            font=dict(family="Microsoft YaHei", size=12),
            width=800,
            height=500
        )

        fig.write_html(VIZ_CHARTS_DIR / "author_comparison_heatmap.html")
        print("âœ… ä½œè€…å¯¹æ¯”çƒ­å›¾å·²ç”Ÿæˆ")

    def _create_research_trend_chart(self):
        """åˆ›å»ºç ”ç©¶è¶‹åŠ¿å›¾"""
        trend_data = self.viz_data['trend_data']['time_series_data']

        df = pd.DataFrame(trend_data)

        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=('è®ºæ–‡æ•°é‡è¶‹åŠ¿', 'åˆ›æ–°æŒ‡æ•°è¶‹åŠ¿', 'åˆä½œå¼ºåº¦è¶‹åŠ¿', 'ç»¼åˆè¶‹åŠ¿'),
            specs=[[{"secondary_y": False}, {"secondary_y": False}],
                   [{"secondary_y": False}, {"secondary_y": False}]]
        )

        # è®ºæ–‡æ•°é‡
        fig.add_trace(
            go.Scatter(x=df['year'], y=df['è®ºæ–‡æ•°é‡'], name='è®ºæ–‡æ•°é‡', line=dict(color='blue', width=3)),
            row=1, col=1
        )

        # åˆ›æ–°æŒ‡æ•°
        fig.add_trace(
            go.Scatter(x=df['year'], y=df['åˆ›æ–°æŒ‡æ•°'], name='åˆ›æ–°æŒ‡æ•°', line=dict(color='red', width=3)),
            row=1, col=2
        )

        # åˆä½œå¼ºåº¦
        fig.add_trace(
            go.Scatter(x=df['year'], y=df['åˆä½œå¼ºåº¦'], name='åˆä½œå¼ºåº¦', line=dict(color='green', width=3)),
            row=2, col=1
        )

        # ç»¼åˆè¶‹åŠ¿
        fig.add_trace(
            go.Scatter(x=df['year'], y=df['è®ºæ–‡æ•°é‡'], name='è®ºæ–‡æ•°é‡', line=dict(color='blue')),
            row=2, col=2
        )
        fig.add_trace(
            go.Scatter(x=df['year'], y=df['åˆ›æ–°æŒ‡æ•°'] * 5, name='åˆ›æ–°æŒ‡æ•°Ã—5', line=dict(color='red')),
            row=2, col=2
        )

        fig.update_layout(
            title="ç ”ç©¶å‘å±•è¶‹åŠ¿åˆ†æ",
            font=dict(family="Microsoft YaHei", size=12),
            width=1000,
            height=700
        )

        fig.write_html(VIZ_CHARTS_DIR / "research_trend_chart.html")
        print("âœ… ç ”ç©¶è¶‹åŠ¿å›¾å·²ç”Ÿæˆ")

    def _create_collaboration_network_chart(self):
        """åˆ›å»ºåˆä½œç½‘ç»œå›¾"""
        network_data = self.viz_data['network_data']
        nodes = network_data['nodes']
        edges = network_data['edges']

        # ä½¿ç”¨plotlyåˆ›å»ºç½‘ç»œå›¾
        edge_x = []
        edge_y = []

        # ç®€å•çš„åœ†å½¢å¸ƒå±€
        import math
        n = len(nodes)
        positions = {}

        for i, node in enumerate(nodes):
            angle = 2 * math.pi * i / n
            x = math.cos(angle)
            y = math.sin(angle)
            positions[node['id']] = (x, y)

        for edge in edges:
            x0, y0 = positions[edge['source']]
            x1, y1 = positions[edge['target']]
            edge_x.extend([x0, x1, None])
            edge_y.extend([y0, y1, None])

        edge_trace = go.Scatter(x=edge_x, y=edge_y,
                                line=dict(width=2, color='#888'),
                                hoverinfo='none',
                                mode='lines')

        node_x = []
        node_y = []
        node_text = []
        node_size = []

        for node in nodes:
            x, y = positions[node['id']]
            node_x.append(x)
            node_y.append(y)
            node_text.append(f"{node['id']}<br>è®ºæ–‡æ•°: {node['papers']}")
            node_size.append(node['size'])

        node_trace = go.Scatter(x=node_x, y=node_y,
                                mode='markers+text',
                                hoverinfo='text',
                                text=[node['id'] for node in nodes],
                                hovertext=node_text,
                                marker=dict(size=node_size,
                                            color='lightblue',
                                            line=dict(width=2, color='darkblue')))

        fig = go.Figure(data=[edge_trace, node_trace],
                        layout=go.Layout(
                            title='ç ”ç©¶è€…åˆä½œç½‘ç»œå›¾',
                            titlefont_size=16,
                            showlegend=False,
                            hovermode='closest',
                            margin=dict(b=20, l=5, r=5, t=40),
                            annotations=[dict(
                                text="èŠ‚ç‚¹å¤§å°è¡¨ç¤ºè®ºæ–‡æ•°é‡",
                                showarrow=False,
                                xref="paper", yref="paper",
                                x=0.005, y=-0.002)],
                            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                            font=dict(family="Microsoft YaHei", size=12)))

        fig.write_html(VIZ_CHARTS_DIR / "collaboration_network.html")
        print("âœ… åˆä½œç½‘ç»œå›¾å·²ç”Ÿæˆ")

    def _create_research_domain_pie_chart(self):
        """åˆ›å»ºç ”ç©¶é¢†åŸŸé¥¼å›¾"""
        pie_data = self.viz_data['pie_data']['pie_data']

        domains = [item['domain'] for item in pie_data]
        papers = [item['papers'] for item in pie_data]
        colors = [item['color'] for item in pie_data]

        fig = go.Figure(data=[go.Pie(
            labels=domains,
            values=papers,
            marker=dict(colors=colors),
            textinfo='label+percent',
            textfont_size=12,
            hole=.3
        )])

        fig.update_layout(
            title="ç ”ç©¶é¢†åŸŸåˆ†å¸ƒ",
            font=dict(family="Microsoft YaHei", size=14),
            width=600,
            height=600
        )

        fig.write_html(VIZ_CHARTS_DIR / "research_domain_pie.html")
        print("âœ… ç ”ç©¶é¢†åŸŸé¥¼å›¾å·²ç”Ÿæˆ")

    def run_complete_visualization_pipeline(self):
        """è¿è¡Œå®Œæ•´çš„å¯è§†åŒ–æ•°æ®ç”Ÿæˆæµç¨‹"""
        print("=" * 80)
        print("AIå¯è§†åŒ–æ•°æ®ç”Ÿæˆå™¨")
        print("ä¸“é—¨ç”Ÿæˆå¯ç”¨äºå›¾è¡¨å¯è§†åŒ–çš„æ•°æ®")
        print("=" * 80)

        # 1. åŠ è½½æ•°æ®æº
        if not self.load_data_sources():
            return None

        # 2. ç”Ÿæˆå„ç±»å¯è§†åŒ–æ•°æ®
        print("\nğŸ¤– ä½¿ç”¨AIç”Ÿæˆå¯è§†åŒ–æ•°æ®...")

        # å›¢é˜Ÿé›·è¾¾å›¾æ•°æ®
        radar_data = self.generate_team_radar_data_with_ai()
        if radar_data:
            self.viz_data['radar_data'] = radar_data

        # ä½œè€…å¯¹æ¯”æ•°æ®
        comparison_data = self.generate_author_comparison_data_with_ai()
        if comparison_data:
            self.viz_data['comparison_data'] = comparison_data

        # ç ”ç©¶è¶‹åŠ¿æ•°æ®
        trend_data = self.generate_research_trend_data_with_ai()
        if trend_data:
            self.viz_data['trend_data'] = trend_data

        # åˆä½œç½‘ç»œæ•°æ®
        network_data = self.generate_collaboration_network_data_with_ai()
        if network_data:
            self.viz_data['network_data'] = network_data

        # ç ”ç©¶é¢†åŸŸé¥¼å›¾æ•°æ®
        pie_data = self.generate_research_domain_pie_data_with_ai()
        if pie_data:
            self.viz_data['pie_data'] = pie_data

        # 3. ä¿å­˜å¯è§†åŒ–æ•°æ®
        with open(VIZ_DATA_DIR / "ai_visualization_data.json", 'w', encoding='utf-8') as f:
            json.dump(self.viz_data, f, indent=2, ensure_ascii=False)

        # 4. åˆ›å»ºå›¾è¡¨
        self.create_visualizations_from_ai_data()

        print(f"\n" + "=" * 80)
        print("AIå¯è§†åŒ–æ•°æ®ç”Ÿæˆå®Œæˆï¼")
        print(f"ğŸ“Š å¯è§†åŒ–æ•°æ®: {VIZ_DATA_DIR / 'ai_visualization_data.json'}")
        print(f"ğŸ“ˆ å›¾è¡¨æ–‡ä»¶: {VIZ_CHARTS_DIR}")
        print("=" * 80)

        return self.viz_data


def main():
    """ä¸»å‡½æ•°"""
    generator = VisualizationDataGenerator()
    viz_data = generator.run_complete_visualization_pipeline()
    return viz_data


if __name__ == "__main__":
    main()